<!DOCTYPE html> <html lang="en" class="" id="rgw26_56ab9f3e7a026"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="hp+nKXUTWQ9/b8gHqWVe6/Vtpm0Sf/YNoGKTPE1FLVxB9CkrBCI7Bj0x/zE4VgsCW7ILKiBQDyshDrmr9ahUNSLa5r2M3Fzb1eXPNexN3eUoJXw8xb3tnVNFUqKaDKj4qeyBkp5QkrhZxXywtX49Q8yK3HhNs9u+gCx2ZiQBEn2xY2CwLzbGeXaeYrVxezTJLt6LUOI+VZJJTzg0VQbidDO1+CMTL5BYcRhyOr/S4g/7Wle1z10gvWLhbIDle1W1D/et7t0lXC+sMS66OFFOHvaYbMDfIM/ftxoKOiNKUik="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-3258d2b1-e861-4c07-a15f-7632f92dfc2a",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/45865922_The_Infinite_Hierarchical_Factor_Regression_Model" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="The Infinite Hierarchical Factor Regression Model" />
<meta property="og:description" content="We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors. To accomplish this, we propose a sparse..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/45865922_The_Infinite_Hierarchical_Factor_Regression_Model/links/0f6460e138294e886aa150c6/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/45865922_The_Infinite_Hierarchical_Factor_Regression_Model" />
<meta property="rg:id" content="PB:45865922" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="The Infinite Hierarchical Factor Regression Model" />
<meta name="citation_author" content="Piyush Rai" />
<meta name="citation_author" content="Hal Daumé III" />
<meta name="citation_publication_date" content="2009/08/04" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/45865922_The_Infinite_Hierarchical_Factor_Regression_Model" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/45865922_The_Infinite_Hierarchical_Factor_Regression_Model" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>The Infinite Hierarchical Factor Regression Model</title>
<meta name="description" content="The Infinite Hierarchical Factor Regression Model on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab9f3e7a026" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab9f3e7a026" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab9f3e7a026">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=The%20Infinite%20Hierarchical%20Factor%20Regression%20Model&rft.date=2009&rft.au=Piyush%20Rai%2CHal%20Daum%C3%A9%20III&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">The Infinite Hierarchical Factor Regression Model</h1> <meta itemprop="headline" content="The Infinite Hierarchical Factor Regression Model">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/45865922_The_Infinite_Hierarchical_Factor_Regression_Model/links/0f6460e138294e886aa150c6/smallpreview.png">  <div id="rgw7_56ab9f3e7a026" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab9f3e7a026" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Piyush_Rai2" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Piyush Rai" alt="Piyush Rai" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Piyush Rai</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw9_56ab9f3e7a026" data-account-key="Piyush_Rai2">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Piyush_Rai2"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Piyush Rai" alt="Piyush Rai" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Piyush_Rai2" class="display-name">Piyush Rai</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_of_Texas_at_Austin" title="University of Texas at Austin">University of Texas at Austin</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56ab9f3e7a026"> <a href="researcher/45250047_Hal_Daume_III" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Hal Daumé III" alt="Hal Daumé III" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Hal Daumé III</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw11_56ab9f3e7a026">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/45250047_Hal_Daume_III"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Hal Daumé III" alt="Hal Daumé III" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/45250047_Hal_Daume_III" class="display-name">Hal Daumé III</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">        <meta itemprop="datePublished" content="2009-08">  08/2009;               <div class="pub-source"> Source: <a href="http://arxiv.org/abs/0908.0570" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw12_56ab9f3e7a026" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors. To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman's coalescent. We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis.</div> </p>  </div>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw25_56ab9f3e7a026">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw24_56ab9f3e7a026"  itemprop="articleBody">  <p>Page 1</p> <p>arXiv:0908.0570v1  [cs.LG]  5 Aug 2009<br />The Infinite Hierarchical Factor Regression Model<br />Piyush Rai and Hal Daum´ e III<br />School of Computing, University of Utah<br />{piyush,hal}@cs.utah.edu<br />Abstract<br />We propose a nonparametric Bayesian factor regression model that accounts for<br />uncertainty in the number of factors, and the relationship between factors. To<br />accomplish this, we propose a sparse variant of the Indian Buffet Process and<br />couplethiswith ahierarchicalmodeloverfactors,basedonKingman’scoalescent.<br />We apply this model to two problems (factor analysis and factor regression) in<br />gene-expression data analysis.<br />1 Introduction<br />Factor analysis is the task of explaining data by means of a set of latent factors. Factor regression<br />couples this analysis with a predictiontask, where the predictionsare made solely on the basis of the<br />factorrepresentation. The latent factorrepresentationachievestwo-foldbenefits: (1)discoveringthe<br />latent process underlyingthe data; (2)simpler predictivemodelingthrougha compactdata represen-<br />tation. Inparticular,(2) is motivatedbythe problemof predictionin the “large P small N” paradigm<br />[1], wherethe numberof featuresP greatlyexceedsthe numberof examplesN, potentiallyresulting<br />in overfitting.<br />We address three fundamental shortcomings of standard factor analysis approaches [2, 3, 4, 1]: (1)<br />we do not assume a known number of factors; (2) we do not assume factors are independent; (3)<br />we do not assume all features are relevant to the factor analysis. Our motivation for this work stems<br />from the task of reconstructing regulatory structure from gene-expression data. In this context, fac-<br />tors correspond to regulatory pathways. Our contributions thus parallel the needs of gene pathway<br />modeling. In addition, we couple predictive modeling (for factor regression) within the factor anal-<br />ysis framework itself, instead of having to model it separately.<br />Our factor regression model is fundamentally nonparametric. In particular, we treat the gene-to-<br />factor relationship nonparametrically by proposing a sparse variant of the Indian Buffet Process<br />(IBP) [5], designed to account for the sparsity of relevant genes (features). We couple this IBP with<br />a hierarchical prior over the factors. This prior explains the fact that pathways are fundamentally<br />related: some are involvedin transcription,some in signaling, some in synthesis. Thenonparametric<br />nature of our sparse IBP requires that the hierarchical prior also be nonparametric. A natural choice<br />is Kingman’s coalescent [6], a popular distribution over infinite binary trees.<br />Since ourmotivationis an applicationin bioinformatics,our notationand terminologywill be drawn<br />from that area. In particular, genes are features, samples are examples, and pathways are factors.<br />However, our model is more general. An alternative application might be to a collaborative filtering<br />problem, in which case our genes might correspond to movies, our samples might correspond to<br />users and our pathways might correspond to genres. In this context, all three contributions of our<br />model still make sense: we do not know how many movie genres there are; some genres are closely<br />related (romance to comedy versus to action); many movies may be spurious.<br />1</p>  <p>Page 2</p> <p>2Background<br />OurmodelusesavariantoftheIndianBuffetProcesstomodelthefeature-factor(i.e.,gene-pathway)<br />relationships. We further use Kingman’s coalescent to model latent pathway hierarchies.<br />2.1Indian Buffet Process<br />The Indian Buffet Process [7] defines a distribution over infinite binary matrices, originally moti-<br />vated by the need to model the latent factor structure of a given set of observations. In the standard<br />form it is parameterized by a scale value, α. The distribution can be explained by means of a simple<br />culinary analogy. Customers (in our context, genes) enter an Indian restaurant and select dishes<br />(in our context, pathways) from an infinite array of dishes. The first customer selects Poisson(α)<br />dishes. Thereafter, each incoming customer i selects a previously-selected dish k with a probability<br />mk/(i − 1), where mkis the number of previous customers who have selected dish k. Customer i<br />then selects an additional Poisson(α/i) new dishes. We can easily define a binary matrix Z with<br />value Zik = 1 precisely when customer i selects dish k. This stochastic process thus defines a<br />distribution over infinite binary matrices.<br />It turn out [7] that the stochastic process defined above corresponds to an infinite limit of an<br />exchangeable process over finite matrices with K columns.<br />p(Z | α) =?K<br />k=1<br />Γ(P+1+α<br />K)<br />tomers. Taking K → ∞ yields the IBP. The IBP has several nice properties, the most important<br />of which is exchangeablility. It is the exchangeablility (over samples) that makes efficient sam-<br />pling algorithms possible. There also exists a two-parametergeneralizationto IBP where the second<br />parameter β controls the sharability of dishes.<br />This distribution takes the form<br />iZikand P is the total number of cus-<br />α<br />KΓ(mk+α<br />K)Γ(P−mk−1)<br />, where mk=?<br />2.2Kingman’s Coalescent<br />Ourmodelmakes use ofa latenthierarchicalstructureoverfactors; we use Kingman’scoalescent[6]<br />as a convenient prior distribution over hierarchies. Kingman’s coalescent originated in the study of<br />population genetics for a set of single-parent organisms. The coalescent is a nonparametric model<br />over a countable set of organisms. It is most easily understood in terms of its finite dimensional<br />marginal distributions over n individuals, in which case it is called an n-coalescent. We then take<br />the limit n → ∞. In our case, the individuals are factors.<br />The n-coalescent considers a population of n organisms at time t = 0. We follow the ancestry of<br />these individuals backward in time, where each organism has exactly one parent at time t &lt; 0. The<br />n-coalescent is a continuous-time, partition-valued Markov process which starts with n singleton<br />clusters at time t = 0 and evolves backward, coalescing lineages until there is only one left. We<br />denote by tithe time at which the ith coalescent event occurs (note ti ≤ 0), and δi = ti−1−<br />tithe time between events (note δi &gt; 0). Under the n-coalescent, each pair of lineages merges<br />indepentently with exponential rate 1; so δi∼ Exp??n−i+1<br />from the n-coalescent is a binary tree with a single root at t = −∞ and n individuals at time t = 0.<br />We denote the tree structure by π. The marginal distribution over tree topologies is uniform and<br />independent of coalescent times; and the model is infinitely exchangeable. We therefore consider<br />the limit as n → ∞, called the coalescent.<br />2<br />??. With probability one, a random draw<br />Once the tree structure is obtained, one can define an additional Markov process to evolve over the<br />tree. One common choice is a Brownian diffusion process. In Brownian diffusion in D dimensions,<br />we assume an underlying diffusion covariance of Λ ∈ RD×Dp.s.d. The root is a D-dimensional<br />vector drawn z. Each non-root node in the tree is drawn Gaussian with mean equal to the value of<br />the parent, and variance δiΛ, where δiis the time that has passed.<br />Recently, Teh et al. [8] proposed efficient bottom-up agglomerative inference algorithms for the<br />coalescent. These (approximately)maximize the probability of π and δs, marginalizing out internal<br />nodes by Belief Propagation. If we associate with each node in the tree a mean y and variance v<br />message, we update messages as Eq (1), where i is the current node and li and ri are its children.<br />vi=?(vli+ (tli− ti)Λ)−1+ (vri+ (tri− ti)Λ)−1?−1<br />yi=?yli(vli+ (tli− ti)Λ)−1+ yri(vri+ (tri− ti)Λ)−1?−1vi<br />(1)<br />2</p>  <p>Page 3</p> <p>3 Nonparametric Bayesian Factor Regression<br />Recall the standard factor analysis problem: X = AF + E, for standardized data X. X is a P × N<br />matrix consisting of N samples [x1,...,xN] of P features each. A is the factor loading matrix of<br />size P × K and F = [f1,...,fN] is the factor matrix of size K × N. E = [e1,...,eN] is the matrix<br />of idiosyncratic variations. K, the number of factors, is known.<br />Recall that our goal is to treat the factor analysis problem nonparametrically, to model feature rele-<br />vance, and to model hierarchical factors. For expository purposes, it is simplest to deal with each of<br />these issues in turn. In our context, we begin by modeling the gene-factor relationship nonparamet-<br />rically (using the IBP). Next, we propose a variant of IBP to model gene relevance. We then present<br />the hierarchical model for inferring factor hierarchies. We conclude with a presentation of the full<br />model and our mechanism for modifying the factor analysis problem to factor regression.<br />3.1<br />We begin by directly using the IBP to infer the number of factors. Although IBP has been applied<br />to nonparametric factor analysis in the past [5], the standard IBP formulation places IBP prior on<br />the factor matrix (F) associating samples (i.e. a set of features) with factors. Such a model assumes<br />that the sample-fctor relationship is sparse. However, this assumption is inappropriate in the gene-<br />expression context where it is not the factors themselves but the associations among genes and<br />factors (i.e., the factor loading matrix A) that are sparse. In such a context, each sample depends on<br />all the factors but each gene within a sample usually depends only on a small number of factors.<br />Nonparametric Gene-Factor Model<br />Thus, it is more appropriate to model the factor loading matrix (A) with the IBP prior. Note that<br />since A andF arerelated witheach othervia the numberof factorsK, modelingA nonparametrically<br />allows our model to also have an unboundednumber of factors.<br />For most gene-expressionproblems [1], a binary factor loadings matrix (A) is inappropriate. There-<br />fore, we instead use the Hadamard (element-wise) product of a binary matrix Z and a matrix V<br />of reals. Z and V are of the same size as A. The factor analysis model, for each sample i, thus<br />becomes: xi = (Z ⊙ V )fi+ ei. We have Z ∼ IBP(α,β). α and β are IBP hyperparameters<br />and have vague gamma priors on them. Our initial model assumes no factor hierarchies and hence<br />the prior over V would simply be a Gaussian: V ∼ Nor(0,σ2<br />σv. F has a zero mean, unit variance Gaussian prior, as used in standard factor analysis. Finally,<br />ei= Nor(0,Ψ) models the idiosyncratic variations of genes where Ψ is a P × P diagonal matrix<br />(diag(Ψ1,...,ΨP)). Each entry ΨPhas an inverse-gammaprior on it.<br />vI) with an inverse-gamma prior on<br />3.2Feature Selection Prior<br />Typical gene-expression datasets are of the order of several thousands of genes, most of which<br />are not associated with any pathway (factor). In the above, these are accounted for only by the<br />idiosyncratic noise term. A more realistic model is that certain genes simply do not participate in<br />the factor analysis: for a culinary analogy, the genes enter the restaurant and leave before selecting<br />anydishes. Thosegenesthat“leave”,weterm“spurious.” Weaddanadditionalpriortermtoaccount<br />for such spurious genes; effectively leading to a sparse solution (over the rows of the IBP matrix).<br />It is important to note that this notion of sparsity is fundamentally different from the conventional<br />notion of sparsity in the IBP. The sparsity in IBP is over columns, not rows. To see the difference,<br />recall that the IBP contains a “rich get richer” phenomenon: frequently selected factors are more<br />likely to get reselected. Consider a truly spurious gene and ask whether it is likely to select any<br />factors. If some factor k is already frequently used, then a priori this gene is more likely to select it.<br />The only downside to selecting it is the data likelihood. By setting the corresponding value in V to<br />zero, there is no penalty.<br />Our sparse-IBP prioris identical to the standardIBP priorwith one exception. Each customer(gene)<br />p is associated with Bernoulli random variable Tpthat indicates whether it samples any dishes. The<br />T vector is given a parameter ρ, which, in turn, is given a Beta prior with parameters a,b.<br />3.3Hierarchical Factor Model<br />In our basic model, each column of the matrix Z (and the correspondingcolumn in V ) is associated<br />with a factor. These factors are considered unrelated. To model the fact that factors are, in fact, re-<br />3</p>  <p>Page 4</p> <p>lated, we introduce a factor hierarchy. Kingman’s coalescent [6] is an attractive prior for integration<br />with IBP for several reasons. It is nonparametric and describes exchangeable distributions. This<br />means that it can model a varying number of factors. Moreover, efficient inference algorithms exist<br />[8].<br />Figure 1: The graphical model for nonparametric<br />Bayesian Factor Regression. X consists of response<br />variables as well.<br />Figure 2: Training and test data are combined to-<br />gether and testresponses aretreatedasmissingvalues<br />to be imputed<br />3.4Full Model and Extension to Factor Regression<br />Our proposed graphical model is depicted in Figure 1. The key aspects of this model are: the IBP<br />prior over Z, the sparse binary vector T, and the Coalescent prior over V.<br />In standard Bayesian factor regression [1], factor analysis is followed by the regression task. The<br />regression is performed only on the basis of F, rather than the full data X. For example, a simple<br />linear regression problem would involve estimating a K-dimensional parameter vector θ with re-<br />gression value θ⊤F. Our model, on the other hand, integrates factor regression component in the<br />nonparametric factor analysis framework itself. We do so by prepending the responses yito the<br />expression vector xiand joining the training and test data (see figure 2). The unknown responses<br />in the test data are treated as missing variables to be iteratively imputed in our MCMC inference<br />procedure. It is straightforward to see that it is equivalent to fitting another sparse model relating<br />factors to responses. Our model thus allows the factor analysis to take into account the regression<br />task as well. In case of binary responses, we add an extra probit regression step to predict binary<br />outcomes from real-valued responses.<br />4 Inference<br />We use Gibbs sampling with a few M-H steps. The Gibbs distributions are summarized here.<br />Sampling the IBP matrix Z: Sampling Z consists of sampling existing dishes, proposing new<br />dishes and accepting or rejectingthem based on the acceptanceratio in the associated M-H step. For<br />sampling existing dishes, an entry in Z is set as 1 according to p(Zik = 1|X,Z−ik,V,F,Ψ) ∝<br />m−i,k<br />(P+β−1)p(X|Z,V,F,Ψ) whereas it is set as 0 according to p(Zik = 0|X,Z−ik,V,F,Ψ) ∝<br />P+β−1−m−i,k<br />(P+β−1)<br />p(X|Z,V,F,Ψ). m−i,k=?<br />For sampling new dishes, we use an M-H step where we simultaneously propose η<br />(Knew,Vnew,Fnew) where Knew∼ Poisson(αβ/(β + P − 1)). We accept the proposal with<br />an acceptance probability (following [9]) given by a = min{1,p(rest|η∗)<br />likelihood of the data given parameters η. We propose Vnewfrom its prior (either Gaussian or<br />Coalescent) but, for faster mixing, we propose Fnewfrom its posterior.<br />j?=iZjkis how many other customers chose dish k.<br />=<br />p(rest|η)}. Here, p(rest|η) is the<br />Sampling Vnewfrom the coalescent is slightly involved. As shown pictorially in figure 3, proposing<br />a new column of V corresponds to adding a new leaf node to the existing coalescent tree. In<br />particular, we need to find a sibling (s) to the new node y′and need to find an insertion point on the<br />branch joining the sibling s to its parent p (the grandparent of y′). Since the marginal distribution<br />over trees under the coalescent is uniform, the sibling s is chosen uniformly over nodes in the tree.<br />We then use importance sampling to select an insertion time for the new node y′between tsand<br />tp, according to the exponential distribution given by the coalescent prior (our proposal distribution<br />is uniform). This gives an insertion point in the tree, which corresponds to the new parent of y′.<br />4</p>  <p>Page 5</p> <p>We denote this new parent by p′and the time of insertion as t. The predictive density of the newly<br />inserted node y′can be obtained by marginalizing the parent p′. This yields Nor(y0,v0), given by:<br />v0= [(vs+ (ts− t)Λ)−1+ (vp+ (t − tp)Λ)−1]−1<br />y0= [ys/(vs+ (ts− t)Λ) + yp/(vp+ (tp− t)Λ)]v0<br />Here, ysand vsare the messages passed up through the tree, while ypand vpare the messages<br />passed down through the tree (compare to Eq (1)).<br />Figure 3: Adding a<br />new node to the tree<br />Sampling the sparse IBP vector T: In the sparse IBP prior, recall that we<br />have an additional P-many variables Tp, indicating whether gene p “eats”<br />any dishes. Tpis drawn from Bernoulli with parameter ρ, which, in turn, is<br />given a Bet(a,b) prior. For inference, we collapse ρ and Ψ and get Gibbs<br />posterior over Tpof the form p(Tp= 1|.) ∝ (a +?<br />Vp)F,g/h,g)) and p(Tp= 0|.) ∝ (b + P −?<br />where Stu is the non-standardStudent’s t-distribution. g,h are hyperparam-<br />eters of the inverse-gammaprior on the entries of Ψ.<br />q?=pTp)Stu(xp|(Zp⊙<br />q?=pTq)Stu(xp|0,g/h,g),<br />Sampling the real valued matrix V: For the case when V has a Gaus-<br />sian prior on it, we sample V from its posterior p(Vg,j|X,Z,F,Ψ) ∝<br />Nor(Vg,j|µg,j,Σg,j), where Σg,j<br />Σg,j(?N<br />?K<br />prior and posterior also has the same form. For the case with coalescent prior on V, we have<br />Σg,j = (?N<br />i=1<br />v0are the Gaussian posteriors of the leaf node added in the coalescent tree (see Eq (1)), which<br />corresponds to the column of V being sampled.<br />=(?N<br />i=1<br />F2<br />Ψg<br />j,i<br />+<br />1<br />σ2<br />=<br />v)−1<br />and<br />µg,j<br />=<br />i=1Fj,iX∗<br />g,j)Ψ−1<br />g. We define X∗<br />g,j<br />Xg,i −<br />l=1,l?=j(Ag,lVg,l)Fl,i, and A = Z ⊙ V. The hyperparameter σv on V has an inverse-gamma<br />F2<br />Ψg+<br />j,i<br />1<br />v0j)−1and µg,j = Σg,j(?N<br />i=1Fj,iX∗<br />g,j)(Ψg+<br />y0g,j<br />v0j)−1, where y0and<br />Sampling the factormatrixF:We sample forF fromits posteriorp(F|X,Z,V,Ψ) ∝ Nor(F|µ,Σ)<br />where µ = AT(AAT+ Ψ)−1X and Σ = I − AT(AAT+ Ψ)−1A, where A = Z ⊙ V<br />Sampling the idiosyncratic noise term: We place an inverse-gamma prior on the diagonal entries<br />of Ψ and the posterior too is inverse-gamma: p(Ψp|.) ∝ IG(g +N<br />X − (Z ⊙ V)F.<br />2,<br />h<br />1+h<br />2tr(ETE)), where E =<br />Sampling IBP parameters: We sample the IBP parameter α from its posterior: p(α|.) ∼<br />Gam(K++ a,<br />?P<br />Sampling the Factor Tree: Use the Greedy-Rate1 algorithm [8].<br />b<br />1+bHP(β)), where K+is the number of active features at any moment and HP(β) =<br />i=11/(β + i − 1). β is sampled from a prior proposal using an M-H step.<br />5Related Work<br />A number of probabilistic approaches have been proposed in the past for the problem of gene-<br />regulatory network reconstruction [2, 3, 4, 1]. Some take into account the information on the prior<br />network topology [2], which is not always available. Most assume the number of factors is known.<br />To get around this, one can perform model selection via Reversible Jump MCMC [10] or evolu-<br />tionary stochastic model search [11]. Unfortunately, these methods are often difficult to design and<br />may take quite long to converge. Moreover, they are difficult to integrate with other forms of prior<br />knowledge (eg., factor hierarchies). A somewhat similar approach to ours is the infinite indepen-<br />dent component analysis (iICA) model of [12] which treats factor analysis as a special case of ICA.<br />However, their model is limited to factor analysis and does not take into account feature selection,<br />factor hierarchyand factor regression. As a generalizationto the standardICA model, [13] proposed<br />a model in which the components can be related via a tree-structured graphical model. It, however,<br />assumes a fixed number of components.<br />Structurally, our model with Gaussian-V (i.e. no hierarchy over factors) is most similar to the<br />Bayesian Factor Regression Model (BFRM) of [1]. BFRM assumes a sparsity inducing mixture<br />prior on the factor loading matrix A. Specifically, Apk∼ (1 − πpk)δ0(Apk) + πpkNor(Apk|0,τk)<br />5</p>  <p>Page 6</p> <p>where δ0() is a point mass centered at zero. To complete the model specification, they define πpk∼<br />(1−ρk)δ0(πpk)+ρkBet(πpk|sr,s(1−r)) and ρk∼ Bet(ρk|av,a(1−v)). Now, integratingout πpk<br />gives: Apk∼ (1−vρk)δ0(Apk)+vρkNor(Apk|0,τk). Itis interestingtonotethatthenonparametric<br />prior of our model (factor loading matrix defined as A = Z ⊙ V) is actually equivalent to the<br />(parametric) sparse mixture prior of the BFRM as K → ∞. To see this, note that our prior on the<br />factor loading matrix A (composedof Z havingan IBP prior, and V having a Gaussian prior), can be<br />written as Apk∼ (1−ρk)δ0(Apk)+ρkNor(Apk|0,σ2<br />to see that, for BFRM where ρk∼ Bet(av,a(1−v)), setting a = 1+αβ/K and v = 1−αβ/(aK)<br />recovers our model in the limiting case when K→∞.<br />v), if we define ρk∼ Bet(1,αβ/K). It is easy<br />6Experiments<br />In this section, we report our results on synthetic and real datasets. We compare our nonparametric<br />approach with the evolutionary search based approach proposed in [11], which is the nonparametric<br />extension to BFRM.<br />We used the gene-factor connectivity matrix of E-coli network (described in [14]) to generate a<br />synthetic dataset having 100 samples of 50 genes and 8 underlying factors. Since we knew the<br />ground truth for factor loadings in this case, this dataset was ideal to test for efficacy in recovering<br />the factor loadings (binding sites and number of factors). We also experimented with a real gene-<br />expression data which is a breast cancer dataset having 251 samples of 226 genes and 5 prominent<br />underlying factors (we know this from domain knowledge).<br />6.1Nonparametric Gene-Factor Modeling and Variable Selection<br />ForthesyntheticdatasetgeneratedbytheE-colinetwork,theresultsareshowninfigure4comparing<br />the actual network used to generate the data and the inferred factor loading matrix. As shown in<br />figure 4, we recovered exactly the same number (8) of factors, and almost exactly the same factor<br />loadings (binding sites and number of factors) as the ground truth. In comparison, the evolutionary<br />search based approach overestimated the number of factors and the inferred loadings clearly seem<br />to be off from the actual loadings (even modulo column permutations).<br />Factors<br />Genes<br />True Factor Loadings<br />12345678<br />5<br />10<br />15<br />20<br />25<br />30<br />35<br />40<br />45<br />50<br />Factors<br />Genes<br />Inferred Factor Loadings<br /> <br /> <br />12345678<br />5<br />10<br />15<br />20<br />25<br />30<br />35<br />40<br />45<br />50<br />Factors<br />Genes<br />Factor Loadings Inferred by BFRM<br /> <br /> <br />123456789 10<br />5<br />10<br />15<br />20<br />25<br />30<br />35<br />40<br />45<br />Figure 4: (Left and middle) True and inferred factor loadings (with our approach) for the synthetic data<br />with P=50, K=8 generated using connectivity matrix of E-coli data. (Right) Inferred factor loadings with the<br />evolutionary search based approach. White rectangles represent active sites. The data also has added noise with<br />signal-to-noise-ratio of 10<br />Our results on real data are shown in figure 5. To see the effect of variable selection for this data,<br />we also introduced spurious genes by adding 50 random features in each sample. We observe the<br />following: (1)Withoutvariableselectionbeingon,spuriousgenesresult inanoverestimatednumber<br />of factors and falsely discovered factor loadings for spurious genes (see figure 5(a)), (2) Variable<br />selection, when on, effectively filters out spurious genes, without overestimating the number of<br />factors (see figure 5(b)). We also investigated the effect of noise on the evolutionary search based<br />approach and it resulted in an overestimated number of factor, plus false discovered factor loadings<br />for spurious genes (see figure 5(c)). To conserve space, we do not show here the cases when there<br />are no spurious genes in the data but it turns out that variable selection does not filter out any of 226<br />relevant genes in such a case.<br />6.2 Hierarchical Factor Modeling<br />Our results with hierarchical factor modeling are shown in figure 6 for synthetic and real data. As<br />shown, the model correctly infers the gene-factor associations, the number of factors, and the factor<br />6</p>  <p>Page 7</p> <p>Factors<br />(a)<br />Noise                             Genes<br /> <br /> <br />2468<br />50<br />100<br />150<br />200<br />250<br />10<br />20<br />30<br />40<br />50<br />60<br />Factors<br />(b)<br />Noise                             Genes<br /> <br /> <br />12345<br />50<br />100<br />150<br />200<br />250<br />10<br />20<br />30<br />40<br />50<br />60<br />Factors<br />Noise                    Genes<br /> <br /> <br />12345678<br />50<br />100<br />150<br />200<br />250<br />10<br />20<br />30<br />40<br />50<br />60<br />(c)<br />Figure5: Effect of spurious genes (heat-plots of factor loading matrix shown): (a) Standard IBP (b) Our model<br />with variable selection (c) The evolutionary search based approach<br />hierarchy. Thereare severalways to interpretthe hierarchy. Fromthe factorhierarchyforE-coli data<br />(figure 6), we see that column-2 (corresponding to factor-2) of the V matrix is the most prominent<br />one (it regulates the highest number of genes), and is closest to the tree-root, followed by column-<br />2, which it looks most similar to. Columns corresponding to lesser prominent factors are located<br />further down in the hierarchy (with appropriate relatedness). Figure 6 (d) can be interpreted in a<br />similar manner for breast-cancer data. The hierarchy can be used to find factors in order of their<br />prominence. The higher we chop off the tree along the hierarchy, the more prominent the factors,<br />we discover, are. For instance, if we are only interested in top 2 factors in E-coli data, we can<br />chop off the tree above the sixth coalescent point. This is akin to the agglomerative clustering sense<br />which is usually done post-hoc. In contrast, our model discovers the factor hierarchies as part of the<br />inference procedure itself. At the same time, there is no degradationof data reconstruction(in mean<br />squared error sense) and the log-likelihood, when compared to the case with Gaussian prior on V<br />(see figure 7 - they actually improve). We also show in section 6.3 that hierarchical modeling results<br />in better predictiveperformancefor the factor regressiontask. Empirical evidences also suggest that<br />the factor hierarchy leads to faster convergence since most of the unlikely configurations will never<br />be visited as they are constrained by the hierarchy.<br /> <br /> <br />12345678<br />5<br />10<br />15<br />20<br />25<br />30<br />35<br />40<br />45<br />50<br />(a)<br />35874612<br />0.04<br />0.045<br />0.05<br />0.055<br />0.06<br />0.065<br />0.07<br />0.075<br />0.08<br />0.085<br />(b)<br />Factors<br />Genes<br /> <br /> <br />1  2 3  4   5  <br />20<br />40<br />60<br />80<br />100<br />120<br />140<br />160<br />180<br />200<br />220<br />10<br />20<br />30<br />40<br />50<br />60<br />(c)<br />12354<br />0.07<br />0.08<br />0.09<br />0.1<br />0.11<br />0.12<br />(d)<br />Figure 6: Hierarchical factor modeling results. (a) Factor loadings for E-coli data. (b) Inferred hierarchy for<br />E-coli data. (c) Factor loadings for breast-cancer data. (d) Inferred hierarchy for breast-cancer data..<br />6.3 Factor Regression<br />We report factor regression results for binary and real-valued responses and compare both variants<br />of our model (Gaussian V and Coalescent V) against 3 different approaches: logistic regression,<br />BFRM, and fitting a separate predictive model on the discovered factors (see figure 7 (c)). The<br />breast-cancer dataset had two binary response variables (phenotypes) associated with each sample.<br />For this binary prediction task, we split the data into training-set of 151 samples and test-set of 100<br />samples. This is essentially a transduction setting as described in section 3.4 and shown in figure 2.<br />For real-valuedprediction task, we treated a 30x20 block of the data matrix as our held-out data and<br />predicted it based on the rest of the entries in the matrix. This method of evaluation is akin to the<br />task of image reconstruction [15]. The results are averaged over 20 random initializations and the<br />low error variances suggest that our method is fairly robust w.r.t. initializations.<br />7</p>  <p>Page 8</p> <p>0100 200 300 400500 600 700800 9001000<br />0.25<br />0.3<br />0.35<br />0.4<br />0.45<br />0.5<br />0.55<br />0.6<br />0.65<br />0.7<br />0.75<br />Iterations<br />MSE<br /> <br /> <br />Coalescent V<br />Gaussian V<br />Post Convergence<br />MSE of BFRM<br />0 100 200300 400500600700 800900 1000<br />−8.5<br />−8<br />−7.5<br />−7<br />−6.5<br />−6<br />−5.5<br />−5<br />−4.5x 10<br />4<br />log likelihood<br />Iterations<br /> <br /> <br />Gaussian V<br />Coalescent V<br />ModelBinary Real<br />(%error,std dev) (MSE)<br />17.5 (1.6)<br />19.8 (1.4)<br />15.8 (0.56)<br />14.6 (0.48)<br />18.1 (2.1)<br />LogReg<br />BFRM<br />Nor-V<br />Coal-V<br />PredModel<br />-<br />0.48<br />0.45<br />0.43<br />-<br />Figure 7: (a) MSE on the breast-cancer data for BFRM (horizontal line), our model with Gaussian (top red<br />curved line) and Coalescent (bottom blue curved line) priors. This MSE is the reconstruction error for the data<br />- different from the MSE for the held-out real valued responses (fig 7 c) (b) Log-likelihoods for our model with<br />Gaussian (bottom red curved line) and Coalescent (top blue curved line) priors. (c) Factor regression results<br />7Conclusions and Discussion<br />We have presented a fully nonparametric Bayesian approach to sparse factor regression, modeling<br />the gene-factor relationship using a sparse variant of the IBP. However, the true power of nonpara-<br />metric priors is evidenced by the ease of integration of task-specific models into the framework.<br />Both gene selection and hierarchical factor modeling are straightforward extensions in our model<br />that do not significantly complicate the inference procedure, but lead to improved model perfor-<br />mance and more understandable outputs. We applied Kingman’s coalescent as a hierarhical model<br />on V, the matrix modulating the expression levels of genes in factors. An interesting open question<br />is whether the IBP can, itself, be modeled hierarchically.<br />References<br />[1] M. West. Bayesian Factor Regression Models in the “Large p, Small n” Paradigm. In Bayesian Statistics<br />7, 2003.<br />[2] C. Sabatti and G. James. Bayesian Sparse Hidden Components Analysis for Transcription Regulation<br />Networks,. Bioinformatics 22, 2005.<br />[3] G. Sanguinetti, N. D. Lawrence, and M. Rattray. Probabilistic Inference of Transcription Factor Concen-<br />trations and Gene-specific Regulatory Activities. Bioinformatics, 22(22), 2006.<br />[4] M. J. Beal, F. Falciani, Z. Ghahramani, C. Rangel, and D. L. Wild. A Bayesian Approach to Reconstruct-<br />ing Genetic Regulatory Networks with Hidden Factors. Bioinformatics, 21(3), 2005.<br />[5] Z. Ghahramani, T.L. Griffiths, and P. Sollich.Bayesian Nonparametric Latent Feature Models.<br />Bayesian Statistics 8. Oxford University Press, 2007.<br />[6] J. F. C. Kingman. The coalescent. Stochastic Processes and their Applications, 1982.<br />[7] T. Griffithsand Z. Ghahramani. InfiniteLatent FeatureModels and theIndian Buffet Process. In Advances<br />in Neural Information Processing Systems 18, 2006.<br />[8] Y. W. Teh, H. Daum´ e III, and D. M. Roy. Bayesian Agglomerative Clustering with Coalescents. In<br />Advances in Neural Information Processing Systems, volume 20, 2008.<br />[9] E. Meeds, Z. Ghahramani, R. M. Neal, and S. T. Roweis. Modeling Dyadic Data with Binary Latent<br />Factors. In Advances in Neural Information Processing Systems 19. 2007.<br />[10] P. Green. Reversible jump markov chain monte carlo computation and bayesian model determination.<br />Biometrica 82, 1995.<br />[11] C. Carvalho, J. Lucas, Q. Wang, J. Chang, J. Nevins, and M. West. High-Dimensional Sparse Factor<br />Modelling - Applications in Gene Expression Genomics. In JASA, 2008.<br />[12] D. Knowles and Z. Ghahramani. Infinite Sparse Factor Analysis and Infinite Independent Components<br />Analysis. In ICA 2007, 2007.<br />[13] Francis R. Bach and Michael I. Jordan. Beyond independent components: trees and clusters. Journal of<br />Machine Learning Research, pages 1205–1233, 2003.<br />[14] I. Pournara and L. Wernisch. Factor Analysis for Gene Regulatory Networks and Transcription Factor<br />Activity Profiles. BMC Bioinformatics, 2007.<br />[15] J. J. Verbeek, S. T. Roweis, and N. Vlassis. Non-linear CCA and PCA by Alignment of Local Models. In<br />Advances in Neural Information Processing Systems 16. 2004.<br />In<br />8</p>   </div> <div id="rgw17_56ab9f3e7a026" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw18_56ab9f3e7a026">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw19_56ab9f3e7a026"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://arxiv.org/pdf/0908.0570.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="The Infinite Hierarchical Factor Regression Model">The Infinite Hierarchical Factor Regression Model</a> </div>  <div class="details">   Available from <a href="http://arxiv.org/pdf/0908.0570.pdf" target="_blank" rel="nofollow">ArXiv</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw21_56ab9f3e7a026" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw22_56ab9f3e7a026">  </ul> </div> </div>   <div id="rgw13_56ab9f3e7a026" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw14_56ab9f3e7a026"> <div> <h5> <a href="publication/287107903_Spatial_distribution_and_potential_sources_of_trace_elements_in_PM10_monitored_in_urban_and_rural_sites_of_Piedmont_Region" class="color-inherit ga-similar-publication-title"><span class="publication-title">Spatial distribution and potential sources of trace elements in PM10 monitored in urban and rural sites of Piedmont Region</span></a>  </h5>  <div class="authors"> <a href="researcher/2089020442_Elio_Padoan" class="authors ga-similar-publication-author">Elio Padoan</a>, <a href="researcher/38602886_Mery_Malandrino" class="authors ga-similar-publication-author">Mery Malandrino</a>, <a href="researcher/33408853_Agnese_Giacomino" class="authors ga-similar-publication-author">Agnese Giacomino</a>, <a href="researcher/2044108035_Mauro_M_Grosa" class="authors ga-similar-publication-author">Mauro M. Grosa</a>, <a href="researcher/2089111501_Francesco_Lollobrigida" class="authors ga-similar-publication-author">Francesco Lollobrigida</a>, <a href="researcher/2089078858_Sara_Martini" class="authors ga-similar-publication-author">Sara Martini</a>, <a href="researcher/39549095_Ornella_Abollino" class="authors ga-similar-publication-author">Ornella Abollino</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw15_56ab9f3e7a026"> <div> <h5> <a href="publication/291520751_How_to_choose_the_right_statistical_software-a_method_increasing_the_post-purchase_satisfaction" class="color-inherit ga-similar-publication-title"><span class="publication-title">How to choose the right statistical software?-a method increasing the post-purchase satisfaction</span></a>  </h5>  <div class="authors"> <a href="researcher/2095275351_Roberto_Cavaliere" class="authors ga-similar-publication-author">Roberto Cavaliere</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw16_56ab9f3e7a026"> <div> <h5> <a href="publication/47462362_The_Impact_of_Different_Teaching_Approaches_and_Languages_on_Student_Learning_of_Introductory_Programming_Concepts" class="color-inherit ga-similar-publication-title"><span class="publication-title">The Impact of Different Teaching Approaches and Languages on Student Learning of Introductory Programming Concepts</span></a>  </h5>  <div class="authors"> <a href="researcher/2036408922_Wanda_M_Kunkle" class="authors ga-similar-publication-author">Wanda M Kunkle</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw27_56ab9f3e7a026" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw28_56ab9f3e7a026">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw29_56ab9f3e7a026" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=TNcaU3BSwGh4JM5E9UeRv2swoirCVyA2mxmdgMDAa_TUoSf2hXMh_LXfBSXhCn8l" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="zH7q5qhkRwE60e3fwW1w92bGkgjZm5kzLoayNnkS0+RXbF2Qm11EGkkOEuLzApSLKkZ4rE9NSID/v8MEbZZqb57M9CiYk0p9sBvuSwSIsI7zJO1nA1TeOzrFA6hc4Xfu41rt6+yVlG4mEQAq2ZcAo2emeezTkEIANMRuO94bvzvEdRaUWLootVC/ftrgf4M34uUpI5Nnlk+AfJRlrMXpB6Ko1fyTaLArKjcfxXOpJXyh4b7fduiBDUaThTgdQPxqDKORtY8RnOMyNefRA57+dFkqaO+ZCxbDpbIaN+pJGO0="/> <input type="hidden" name="urlAfterLogin" value="publication/45865922_The_Infinite_Hierarchical_Factor_Regression_Model"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vNDU4NjU5MjJfVGhlX0luZmluaXRlX0hpZXJhcmNoaWNhbF9GYWN0b3JfUmVncmVzc2lvbl9Nb2RlbA%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vNDU4NjU5MjJfVGhlX0luZmluaXRlX0hpZXJhcmNoaWNhbF9GYWN0b3JfUmVncmVzc2lvbl9Nb2RlbA%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vNDU4NjU5MjJfVGhlX0luZmluaXRlX0hpZXJhcmNoaWNhbF9GYWN0b3JfUmVncmVzc2lvbl9Nb2RlbA%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw30_56ab9f3e7a026"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 783;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Piyush Rai","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Piyush_Rai2","institution":"University of Texas at Austin","institutionUrl":false,"widgetId":"rgw4_56ab9f3e7a026"},"id":"rgw4_56ab9f3e7a026","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=2429152","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab9f3e7a026"},"id":"rgw3_56ab9f3e7a026","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=45865922","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":45865922,"title":"The Infinite Hierarchical Factor Regression Model","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"08\/2009;","publicationDateRobot":"2009-08","article":""}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/0908.0570","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"The Infinite Hierarchical Factor Regression Model"},{"key":"rft.date","value":"2009"},{"key":"rft.au","value":"Piyush Rai,Hal Daum\u00e9 III"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56ab9f3e7a026"},"id":"rgw6_56ab9f3e7a026","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=45865922","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":45865922,"peopleItems":[{"data":{"authorNameOnPublication":"Piyush Rai","accountUrl":"profile\/Piyush_Rai2","accountKey":"Piyush_Rai2","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Piyush Rai","profile":{"professionalInstitution":{"professionalInstitutionName":"University of Texas at Austin","professionalInstitutionUrl":"institution\/University_of_Texas_at_Austin"}},"professionalInstitutionName":"University of Texas at Austin","professionalInstitutionUrl":"institution\/University_of_Texas_at_Austin","url":"profile\/Piyush_Rai2","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Piyush_Rai2","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw9_56ab9f3e7a026"},"id":"rgw9_56ab9f3e7a026","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=2429152&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University of Texas at Austin","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":2,"accountCount":1,"publicationUid":45865922,"widgetId":"rgw8_56ab9f3e7a026"},"id":"rgw8_56ab9f3e7a026","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=2429152&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=2&accountCount=1&publicationUid=45865922","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/45250047_Hal_Daume_III","authorNameOnPublication":"Hal Daum\u00e9 III","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Hal Daum\u00e9 III","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/45250047_Hal_Daume_III","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw11_56ab9f3e7a026"},"id":"rgw11_56ab9f3e7a026","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=45250047&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw10_56ab9f3e7a026"},"id":"rgw10_56ab9f3e7a026","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=45250047&authorNameOnPublication=Hal%20Daum%C3%A9%20III","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab9f3e7a026"},"id":"rgw7_56ab9f3e7a026","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=45865922&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":45865922,"abstract":"<noscript><\/noscript><div>We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors. To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman's coalescent. We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw12_56ab9f3e7a026"},"id":"rgw12_56ab9f3e7a026","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=45865922","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/45865922_The_Infinite_Hierarchical_Factor_Regression_Model\/links\/0f6460e138294e886aa150c6\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw5_56ab9f3e7a026"},"id":"rgw5_56ab9f3e7a026","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=45865922&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2089020442,"url":"researcher\/2089020442_Elio_Padoan","fullname":"Elio Padoan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38602886,"url":"researcher\/38602886_Mery_Malandrino","fullname":"Mery Malandrino","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":33408853,"url":"researcher\/33408853_Agnese_Giacomino","fullname":"Agnese Giacomino","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2044108035,"url":"researcher\/2044108035_Mauro_M_Grosa","fullname":"Mauro M. Grosa","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":3,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2016","journal":"Chemosphere","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/287107903_Spatial_distribution_and_potential_sources_of_trace_elements_in_PM10_monitored_in_urban_and_rural_sites_of_Piedmont_Region","usePlainButton":true,"publicationUid":287107903,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"3.34","url":"publication\/287107903_Spatial_distribution_and_potential_sources_of_trace_elements_in_PM10_monitored_in_urban_and_rural_sites_of_Piedmont_Region","title":"Spatial distribution and potential sources of trace elements in PM10 monitored in urban and rural sites of Piedmont Region","displayTitleAsLink":true,"authors":[{"id":2089020442,"url":"researcher\/2089020442_Elio_Padoan","fullname":"Elio Padoan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38602886,"url":"researcher\/38602886_Mery_Malandrino","fullname":"Mery Malandrino","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":33408853,"url":"researcher\/33408853_Agnese_Giacomino","fullname":"Agnese Giacomino","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2044108035,"url":"researcher\/2044108035_Mauro_M_Grosa","fullname":"Mauro M. Grosa","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089111501,"url":"researcher\/2089111501_Francesco_Lollobrigida","fullname":"Francesco Lollobrigida","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089078858,"url":"researcher\/2089078858_Sara_Martini","fullname":"Sara Martini","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39549095,"url":"researcher\/39549095_Ornella_Abollino","fullname":"Ornella Abollino","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Chemosphere 02\/2016; 145:495-507. DOI:10.1016\/j.chemosphere.2015.11.094"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/287107903_Spatial_distribution_and_potential_sources_of_trace_elements_in_PM10_monitored_in_urban_and_rural_sites_of_Piedmont_Region","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/287107903_Spatial_distribution_and_potential_sources_of_trace_elements_in_PM10_monitored_in_urban_and_rural_sites_of_Piedmont_Region\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw14_56ab9f3e7a026"},"id":"rgw14_56ab9f3e7a026","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=287107903","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2095275351,"url":"researcher\/2095275351_Roberto_Cavaliere","fullname":"Roberto Cavaliere","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Journal of Thoracic Disease","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291520751_How_to_choose_the_right_statistical_software-a_method_increasing_the_post-purchase_satisfaction","usePlainButton":true,"publicationUid":291520751,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.78","url":"publication\/291520751_How_to_choose_the_right_statistical_software-a_method_increasing_the_post-purchase_satisfaction","title":"How to choose the right statistical software?-a method increasing the post-purchase satisfaction","displayTitleAsLink":true,"authors":[{"id":2095275351,"url":"researcher\/2095275351_Roberto_Cavaliere","fullname":"Roberto Cavaliere","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of Thoracic Disease 01\/2016; 7(12):E585-98. DOI:10.3978\/j.issn.2072-1439.2015.11.57"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291520751_How_to_choose_the_right_statistical_software-a_method_increasing_the_post-purchase_satisfaction","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291520751_How_to_choose_the_right_statistical_software-a_method_increasing_the_post-purchase_satisfaction\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw15_56ab9f3e7a026"},"id":"rgw15_56ab9f3e7a026","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291520751","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2036408922,"url":"researcher\/2036408922_Wanda_M_Kunkle","fullname":"Wanda M Kunkle","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"ACM Transactions on Computing Education","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/47462362_The_Impact_of_Different_Teaching_Approaches_and_Languages_on_Student_Learning_of_Introductory_Programming_Concepts","usePlainButton":true,"publicationUid":47462362,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/47462362_The_Impact_of_Different_Teaching_Approaches_and_Languages_on_Student_Learning_of_Introductory_Programming_Concepts","title":"The Impact of Different Teaching Approaches and Languages on Student Learning of Introductory Programming Concepts","displayTitleAsLink":true,"authors":[{"id":2036408922,"url":"researcher\/2036408922_Wanda_M_Kunkle","fullname":"Wanda M Kunkle","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["ACM Transactions on Computing Education 01\/2016; 16(1). DOI:10.1145\/2785807"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/47462362_The_Impact_of_Different_Teaching_Approaches_and_Languages_on_Student_Learning_of_Introductory_Programming_Concepts","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/47462362_The_Impact_of_Different_Teaching_Approaches_and_Languages_on_Student_Learning_of_Introductory_Programming_Concepts\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56ab9f3e7a026"},"id":"rgw16_56ab9f3e7a026","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=47462362","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw13_56ab9f3e7a026"},"id":"rgw13_56ab9f3e7a026","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=45865922&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":45865922,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":45865922,"publicationType":"article","linkId":"0f6460e138294e886aa150c6","fileName":"The Infinite Hierarchical Factor Regression Model","fileUrl":"http:\/\/arxiv.org\/pdf\/0908.0570.pdf","name":"ArXiv","nameUrl":"http:\/\/arxiv.org\/pdf\/0908.0570.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw19_56ab9f3e7a026"},"id":"rgw19_56ab9f3e7a026","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=45865922&linkId=0f6460e138294e886aa150c6&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw18_56ab9f3e7a026"},"id":"rgw18_56ab9f3e7a026","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=45865922&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":0,"valueFormatted":"0","widgetId":"rgw20_56ab9f3e7a026"},"id":"rgw20_56ab9f3e7a026","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=45865922","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw17_56ab9f3e7a026"},"id":"rgw17_56ab9f3e7a026","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=45865922&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":45865922,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw22_56ab9f3e7a026"},"id":"rgw22_56ab9f3e7a026","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=45865922&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":0,"valueFormatted":"0","widgetId":"rgw23_56ab9f3e7a026"},"id":"rgw23_56ab9f3e7a026","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=45865922","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw21_56ab9f3e7a026"},"id":"rgw21_56ab9f3e7a026","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=45865922&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"arXiv:0908.0570v1  [cs.LG]  5 Aug 2009\nThe Infinite Hierarchical Factor Regression Model\nPiyush Rai and Hal Daum\u00b4 e III\nSchool of Computing, University of Utah\n{piyush,hal}@cs.utah.edu\nAbstract\nWe propose a nonparametric Bayesian factor regression model that accounts for\nuncertainty in the number of factors, and the relationship between factors. To\naccomplish this, we propose a sparse variant of the Indian Buffet Process and\ncouplethiswith ahierarchicalmodeloverfactors,basedonKingman\u2019scoalescent.\nWe apply this model to two problems (factor analysis and factor regression) in\ngene-expression data analysis.\n1 Introduction\nFactor analysis is the task of explaining data by means of a set of latent factors. Factor regression\ncouples this analysis with a predictiontask, where the predictionsare made solely on the basis of the\nfactorrepresentation. The latent factorrepresentationachievestwo-foldbenefits: (1)discoveringthe\nlatent process underlyingthe data; (2)simpler predictivemodelingthrougha compactdata represen-\ntation. Inparticular,(2) is motivatedbythe problemof predictionin the \u201clarge P small N\u201d paradigm\n[1], wherethe numberof featuresP greatlyexceedsthe numberof examplesN, potentiallyresulting\nin overfitting.\nWe address three fundamental shortcomings of standard factor analysis approaches [2, 3, 4, 1]: (1)\nwe do not assume a known number of factors; (2) we do not assume factors are independent; (3)\nwe do not assume all features are relevant to the factor analysis. Our motivation for this work stems\nfrom the task of reconstructing regulatory structure from gene-expression data. In this context, fac-\ntors correspond to regulatory pathways. Our contributions thus parallel the needs of gene pathway\nmodeling. In addition, we couple predictive modeling (for factor regression) within the factor anal-\nysis framework itself, instead of having to model it separately.\nOur factor regression model is fundamentally nonparametric. In particular, we treat the gene-to-\nfactor relationship nonparametrically by proposing a sparse variant of the Indian Buffet Process\n(IBP) [5], designed to account for the sparsity of relevant genes (features). We couple this IBP with\na hierarchical prior over the factors. This prior explains the fact that pathways are fundamentally\nrelated: some are involvedin transcription,some in signaling, some in synthesis. Thenonparametric\nnature of our sparse IBP requires that the hierarchical prior also be nonparametric. A natural choice\nis Kingman\u2019s coalescent [6], a popular distribution over infinite binary trees.\nSince ourmotivationis an applicationin bioinformatics,our notationand terminologywill be drawn\nfrom that area. In particular, genes are features, samples are examples, and pathways are factors.\nHowever, our model is more general. An alternative application might be to a collaborative filtering\nproblem, in which case our genes might correspond to movies, our samples might correspond to\nusers and our pathways might correspond to genres. In this context, all three contributions of our\nmodel still make sense: we do not know how many movie genres there are; some genres are closely\nrelated (romance to comedy versus to action); many movies may be spurious.\n1"},{"page":2,"text":"2Background\nOurmodelusesavariantoftheIndianBuffetProcesstomodelthefeature-factor(i.e.,gene-pathway)\nrelationships. We further use Kingman\u2019s coalescent to model latent pathway hierarchies.\n2.1Indian Buffet Process\nThe Indian Buffet Process [7] defines a distribution over infinite binary matrices, originally moti-\nvated by the need to model the latent factor structure of a given set of observations. In the standard\nform it is parameterized by a scale value, \u03b1. The distribution can be explained by means of a simple\nculinary analogy. Customers (in our context, genes) enter an Indian restaurant and select dishes\n(in our context, pathways) from an infinite array of dishes. The first customer selects Poisson(\u03b1)\ndishes. Thereafter, each incoming customer i selects a previously-selected dish k with a probability\nmk\/(i \u2212 1), where mkis the number of previous customers who have selected dish k. Customer i\nthen selects an additional Poisson(\u03b1\/i) new dishes. We can easily define a binary matrix Z with\nvalue Zik = 1 precisely when customer i selects dish k. This stochastic process thus defines a\ndistribution over infinite binary matrices.\nIt turn out [7] that the stochastic process defined above corresponds to an infinite limit of an\nexchangeable process over finite matrices with K columns.\np(Z | \u03b1) =?K\nk=1\n\u0393(P+1+\u03b1\nK)\ntomers. Taking K \u2192 \u221e yields the IBP. The IBP has several nice properties, the most important\nof which is exchangeablility. It is the exchangeablility (over samples) that makes efficient sam-\npling algorithms possible. There also exists a two-parametergeneralizationto IBP where the second\nparameter \u03b2 controls the sharability of dishes.\nThis distribution takes the form\niZikand P is the total number of cus-\n\u03b1\nK\u0393(mk+\u03b1\nK)\u0393(P\u2212mk\u22121)\n, where mk=?\n2.2Kingman\u2019s Coalescent\nOurmodelmakes use ofa latenthierarchicalstructureoverfactors; we use Kingman\u2019scoalescent[6]\nas a convenient prior distribution over hierarchies. Kingman\u2019s coalescent originated in the study of\npopulation genetics for a set of single-parent organisms. The coalescent is a nonparametric model\nover a countable set of organisms. It is most easily understood in terms of its finite dimensional\nmarginal distributions over n individuals, in which case it is called an n-coalescent. We then take\nthe limit n \u2192 \u221e. In our case, the individuals are factors.\nThe n-coalescent considers a population of n organisms at time t = 0. We follow the ancestry of\nthese individuals backward in time, where each organism has exactly one parent at time t < 0. The\nn-coalescent is a continuous-time, partition-valued Markov process which starts with n singleton\nclusters at time t = 0 and evolves backward, coalescing lineages until there is only one left. We\ndenote by tithe time at which the ith coalescent event occurs (note ti \u2264 0), and \u03b4i = ti\u22121\u2212\ntithe time between events (note \u03b4i > 0). Under the n-coalescent, each pair of lineages merges\nindepentently with exponential rate 1; so \u03b4i\u223c Exp??n\u2212i+1\nfrom the n-coalescent is a binary tree with a single root at t = \u2212\u221e and n individuals at time t = 0.\nWe denote the tree structure by \u03c0. The marginal distribution over tree topologies is uniform and\nindependent of coalescent times; and the model is infinitely exchangeable. We therefore consider\nthe limit as n \u2192 \u221e, called the coalescent.\n2\n??. With probability one, a random draw\nOnce the tree structure is obtained, one can define an additional Markov process to evolve over the\ntree. One common choice is a Brownian diffusion process. In Brownian diffusion in D dimensions,\nwe assume an underlying diffusion covariance of \u039b \u2208 RD\u00d7Dp.s.d. The root is a D-dimensional\nvector drawn z. Each non-root node in the tree is drawn Gaussian with mean equal to the value of\nthe parent, and variance \u03b4i\u039b, where \u03b4iis the time that has passed.\nRecently, Teh et al. [8] proposed efficient bottom-up agglomerative inference algorithms for the\ncoalescent. These (approximately)maximize the probability of \u03c0 and \u03b4s, marginalizing out internal\nnodes by Belief Propagation. If we associate with each node in the tree a mean y and variance v\nmessage, we update messages as Eq (1), where i is the current node and li and ri are its children.\nvi=?(vli+ (tli\u2212 ti)\u039b)\u22121+ (vri+ (tri\u2212 ti)\u039b)\u22121?\u22121\nyi=?yli(vli+ (tli\u2212 ti)\u039b)\u22121+ yri(vri+ (tri\u2212 ti)\u039b)\u22121?\u22121vi\n(1)\n2"},{"page":3,"text":"3 Nonparametric Bayesian Factor Regression\nRecall the standard factor analysis problem: X = AF + E, for standardized data X. X is a P \u00d7 N\nmatrix consisting of N samples [x1,...,xN] of P features each. A is the factor loading matrix of\nsize P \u00d7 K and F = [f1,...,fN] is the factor matrix of size K \u00d7 N. E = [e1,...,eN] is the matrix\nof idiosyncratic variations. K, the number of factors, is known.\nRecall that our goal is to treat the factor analysis problem nonparametrically, to model feature rele-\nvance, and to model hierarchical factors. For expository purposes, it is simplest to deal with each of\nthese issues in turn. In our context, we begin by modeling the gene-factor relationship nonparamet-\nrically (using the IBP). Next, we propose a variant of IBP to model gene relevance. We then present\nthe hierarchical model for inferring factor hierarchies. We conclude with a presentation of the full\nmodel and our mechanism for modifying the factor analysis problem to factor regression.\n3.1\nWe begin by directly using the IBP to infer the number of factors. Although IBP has been applied\nto nonparametric factor analysis in the past [5], the standard IBP formulation places IBP prior on\nthe factor matrix (F) associating samples (i.e. a set of features) with factors. Such a model assumes\nthat the sample-fctor relationship is sparse. However, this assumption is inappropriate in the gene-\nexpression context where it is not the factors themselves but the associations among genes and\nfactors (i.e., the factor loading matrix A) that are sparse. In such a context, each sample depends on\nall the factors but each gene within a sample usually depends only on a small number of factors.\nNonparametric Gene-Factor Model\nThus, it is more appropriate to model the factor loading matrix (A) with the IBP prior. Note that\nsince A andF arerelated witheach othervia the numberof factorsK, modelingA nonparametrically\nallows our model to also have an unboundednumber of factors.\nFor most gene-expressionproblems [1], a binary factor loadings matrix (A) is inappropriate. There-\nfore, we instead use the Hadamard (element-wise) product of a binary matrix Z and a matrix V\nof reals. Z and V are of the same size as A. The factor analysis model, for each sample i, thus\nbecomes: xi = (Z \u2299 V )fi+ ei. We have Z \u223c IBP(\u03b1,\u03b2). \u03b1 and \u03b2 are IBP hyperparameters\nand have vague gamma priors on them. Our initial model assumes no factor hierarchies and hence\nthe prior over V would simply be a Gaussian: V \u223c Nor(0,\u03c32\n\u03c3v. F has a zero mean, unit variance Gaussian prior, as used in standard factor analysis. Finally,\nei= Nor(0,\u03a8) models the idiosyncratic variations of genes where \u03a8 is a P \u00d7 P diagonal matrix\n(diag(\u03a81,...,\u03a8P)). Each entry \u03a8Phas an inverse-gammaprior on it.\nvI) with an inverse-gamma prior on\n3.2Feature Selection Prior\nTypical gene-expression datasets are of the order of several thousands of genes, most of which\nare not associated with any pathway (factor). In the above, these are accounted for only by the\nidiosyncratic noise term. A more realistic model is that certain genes simply do not participate in\nthe factor analysis: for a culinary analogy, the genes enter the restaurant and leave before selecting\nanydishes. Thosegenesthat\u201cleave\u201d,weterm\u201cspurious.\u201d Weaddanadditionalpriortermtoaccount\nfor such spurious genes; effectively leading to a sparse solution (over the rows of the IBP matrix).\nIt is important to note that this notion of sparsity is fundamentally different from the conventional\nnotion of sparsity in the IBP. The sparsity in IBP is over columns, not rows. To see the difference,\nrecall that the IBP contains a \u201crich get richer\u201d phenomenon: frequently selected factors are more\nlikely to get reselected. Consider a truly spurious gene and ask whether it is likely to select any\nfactors. If some factor k is already frequently used, then a priori this gene is more likely to select it.\nThe only downside to selecting it is the data likelihood. By setting the corresponding value in V to\nzero, there is no penalty.\nOur sparse-IBP prioris identical to the standardIBP priorwith one exception. Each customer(gene)\np is associated with Bernoulli random variable Tpthat indicates whether it samples any dishes. The\nT vector is given a parameter \u03c1, which, in turn, is given a Beta prior with parameters a,b.\n3.3Hierarchical Factor Model\nIn our basic model, each column of the matrix Z (and the correspondingcolumn in V ) is associated\nwith a factor. These factors are considered unrelated. To model the fact that factors are, in fact, re-\n3"},{"page":4,"text":"lated, we introduce a factor hierarchy. Kingman\u2019s coalescent [6] is an attractive prior for integration\nwith IBP for several reasons. It is nonparametric and describes exchangeable distributions. This\nmeans that it can model a varying number of factors. Moreover, efficient inference algorithms exist\n[8].\nFigure 1: The graphical model for nonparametric\nBayesian Factor Regression. X consists of response\nvariables as well.\nFigure 2: Training and test data are combined to-\ngether and testresponses aretreatedasmissingvalues\nto be imputed\n3.4Full Model and Extension to Factor Regression\nOur proposed graphical model is depicted in Figure 1. The key aspects of this model are: the IBP\nprior over Z, the sparse binary vector T, and the Coalescent prior over V.\nIn standard Bayesian factor regression [1], factor analysis is followed by the regression task. The\nregression is performed only on the basis of F, rather than the full data X. For example, a simple\nlinear regression problem would involve estimating a K-dimensional parameter vector \u03b8 with re-\ngression value \u03b8\u22a4F. Our model, on the other hand, integrates factor regression component in the\nnonparametric factor analysis framework itself. We do so by prepending the responses yito the\nexpression vector xiand joining the training and test data (see figure 2). The unknown responses\nin the test data are treated as missing variables to be iteratively imputed in our MCMC inference\nprocedure. It is straightforward to see that it is equivalent to fitting another sparse model relating\nfactors to responses. Our model thus allows the factor analysis to take into account the regression\ntask as well. In case of binary responses, we add an extra probit regression step to predict binary\noutcomes from real-valued responses.\n4 Inference\nWe use Gibbs sampling with a few M-H steps. The Gibbs distributions are summarized here.\nSampling the IBP matrix Z: Sampling Z consists of sampling existing dishes, proposing new\ndishes and accepting or rejectingthem based on the acceptanceratio in the associated M-H step. For\nsampling existing dishes, an entry in Z is set as 1 according to p(Zik = 1|X,Z\u2212ik,V,F,\u03a8) \u221d\nm\u2212i,k\n(P+\u03b2\u22121)p(X|Z,V,F,\u03a8) whereas it is set as 0 according to p(Zik = 0|X,Z\u2212ik,V,F,\u03a8) \u221d\nP+\u03b2\u22121\u2212m\u2212i,k\n(P+\u03b2\u22121)\np(X|Z,V,F,\u03a8). m\u2212i,k=?\nFor sampling new dishes, we use an M-H step where we simultaneously propose \u03b7\n(Knew,Vnew,Fnew) where Knew\u223c Poisson(\u03b1\u03b2\/(\u03b2 + P \u2212 1)). We accept the proposal with\nan acceptance probability (following [9]) given by a = min{1,p(rest|\u03b7\u2217)\nlikelihood of the data given parameters \u03b7. We propose Vnewfrom its prior (either Gaussian or\nCoalescent) but, for faster mixing, we propose Fnewfrom its posterior.\nj?=iZjkis how many other customers chose dish k.\n=\np(rest|\u03b7)}. Here, p(rest|\u03b7) is the\nSampling Vnewfrom the coalescent is slightly involved. As shown pictorially in figure 3, proposing\na new column of V corresponds to adding a new leaf node to the existing coalescent tree. In\nparticular, we need to find a sibling (s) to the new node y\u2032and need to find an insertion point on the\nbranch joining the sibling s to its parent p (the grandparent of y\u2032). Since the marginal distribution\nover trees under the coalescent is uniform, the sibling s is chosen uniformly over nodes in the tree.\nWe then use importance sampling to select an insertion time for the new node y\u2032between tsand\ntp, according to the exponential distribution given by the coalescent prior (our proposal distribution\nis uniform). This gives an insertion point in the tree, which corresponds to the new parent of y\u2032.\n4"},{"page":5,"text":"We denote this new parent by p\u2032and the time of insertion as t. The predictive density of the newly\ninserted node y\u2032can be obtained by marginalizing the parent p\u2032. This yields Nor(y0,v0), given by:\nv0= [(vs+ (ts\u2212 t)\u039b)\u22121+ (vp+ (t \u2212 tp)\u039b)\u22121]\u22121\ny0= [ys\/(vs+ (ts\u2212 t)\u039b) + yp\/(vp+ (tp\u2212 t)\u039b)]v0\nHere, ysand vsare the messages passed up through the tree, while ypand vpare the messages\npassed down through the tree (compare to Eq (1)).\nFigure 3: Adding a\nnew node to the tree\nSampling the sparse IBP vector T: In the sparse IBP prior, recall that we\nhave an additional P-many variables Tp, indicating whether gene p \u201ceats\u201d\nany dishes. Tpis drawn from Bernoulli with parameter \u03c1, which, in turn, is\ngiven a Bet(a,b) prior. For inference, we collapse \u03c1 and \u03a8 and get Gibbs\nposterior over Tpof the form p(Tp= 1|.) \u221d (a +?\nVp)F,g\/h,g)) and p(Tp= 0|.) \u221d (b + P \u2212?\nwhere Stu is the non-standardStudent\u2019s t-distribution. g,h are hyperparam-\neters of the inverse-gammaprior on the entries of \u03a8.\nq?=pTp)Stu(xp|(Zp\u2299\nq?=pTq)Stu(xp|0,g\/h,g),\nSampling the real valued matrix V: For the case when V has a Gaus-\nsian prior on it, we sample V from its posterior p(Vg,j|X,Z,F,\u03a8) \u221d\nNor(Vg,j|\u00b5g,j,\u03a3g,j), where \u03a3g,j\n\u03a3g,j(?N\n?K\nprior and posterior also has the same form. For the case with coalescent prior on V, we have\n\u03a3g,j = (?N\ni=1\nv0are the Gaussian posteriors of the leaf node added in the coalescent tree (see Eq (1)), which\ncorresponds to the column of V being sampled.\n=(?N\ni=1\nF2\n\u03a8g\nj,i\n+\n1\n\u03c32\n=\nv)\u22121\nand\n\u00b5g,j\n=\ni=1Fj,iX\u2217\ng,j)\u03a8\u22121\ng. We define X\u2217\ng,j\nXg,i \u2212\nl=1,l?=j(Ag,lVg,l)Fl,i, and A = Z \u2299 V. The hyperparameter \u03c3v on V has an inverse-gamma\nF2\n\u03a8g+\nj,i\n1\nv0j)\u22121and \u00b5g,j = \u03a3g,j(?N\ni=1Fj,iX\u2217\ng,j)(\u03a8g+\ny0g,j\nv0j)\u22121, where y0and\nSampling the factormatrixF:We sample forF fromits posteriorp(F|X,Z,V,\u03a8) \u221d Nor(F|\u00b5,\u03a3)\nwhere \u00b5 = AT(AAT+ \u03a8)\u22121X and \u03a3 = I \u2212 AT(AAT+ \u03a8)\u22121A, where A = Z \u2299 V\nSampling the idiosyncratic noise term: We place an inverse-gamma prior on the diagonal entries\nof \u03a8 and the posterior too is inverse-gamma: p(\u03a8p|.) \u221d IG(g +N\nX \u2212 (Z \u2299 V)F.\n2,\nh\n1+h\n2tr(ETE)), where E =\nSampling IBP parameters: We sample the IBP parameter \u03b1 from its posterior: p(\u03b1|.) \u223c\nGam(K++ a,\n?P\nSampling the Factor Tree: Use the Greedy-Rate1 algorithm [8].\nb\n1+bHP(\u03b2)), where K+is the number of active features at any moment and HP(\u03b2) =\ni=11\/(\u03b2 + i \u2212 1). \u03b2 is sampled from a prior proposal using an M-H step.\n5Related Work\nA number of probabilistic approaches have been proposed in the past for the problem of gene-\nregulatory network reconstruction [2, 3, 4, 1]. Some take into account the information on the prior\nnetwork topology [2], which is not always available. Most assume the number of factors is known.\nTo get around this, one can perform model selection via Reversible Jump MCMC [10] or evolu-\ntionary stochastic model search [11]. Unfortunately, these methods are often difficult to design and\nmay take quite long to converge. Moreover, they are difficult to integrate with other forms of prior\nknowledge (eg., factor hierarchies). A somewhat similar approach to ours is the infinite indepen-\ndent component analysis (iICA) model of [12] which treats factor analysis as a special case of ICA.\nHowever, their model is limited to factor analysis and does not take into account feature selection,\nfactor hierarchyand factor regression. As a generalizationto the standardICA model, [13] proposed\na model in which the components can be related via a tree-structured graphical model. It, however,\nassumes a fixed number of components.\nStructurally, our model with Gaussian-V (i.e. no hierarchy over factors) is most similar to the\nBayesian Factor Regression Model (BFRM) of [1]. BFRM assumes a sparsity inducing mixture\nprior on the factor loading matrix A. Specifically, Apk\u223c (1 \u2212 \u03c0pk)\u03b40(Apk) + \u03c0pkNor(Apk|0,\u03c4k)\n5"},{"page":6,"text":"where \u03b40() is a point mass centered at zero. To complete the model specification, they define \u03c0pk\u223c\n(1\u2212\u03c1k)\u03b40(\u03c0pk)+\u03c1kBet(\u03c0pk|sr,s(1\u2212r)) and \u03c1k\u223c Bet(\u03c1k|av,a(1\u2212v)). Now, integratingout \u03c0pk\ngives: Apk\u223c (1\u2212v\u03c1k)\u03b40(Apk)+v\u03c1kNor(Apk|0,\u03c4k). Itis interestingtonotethatthenonparametric\nprior of our model (factor loading matrix defined as A = Z \u2299 V) is actually equivalent to the\n(parametric) sparse mixture prior of the BFRM as K \u2192 \u221e. To see this, note that our prior on the\nfactor loading matrix A (composedof Z havingan IBP prior, and V having a Gaussian prior), can be\nwritten as Apk\u223c (1\u2212\u03c1k)\u03b40(Apk)+\u03c1kNor(Apk|0,\u03c32\nto see that, for BFRM where \u03c1k\u223c Bet(av,a(1\u2212v)), setting a = 1+\u03b1\u03b2\/K and v = 1\u2212\u03b1\u03b2\/(aK)\nrecovers our model in the limiting case when K\u2192\u221e.\nv), if we define \u03c1k\u223c Bet(1,\u03b1\u03b2\/K). It is easy\n6Experiments\nIn this section, we report our results on synthetic and real datasets. We compare our nonparametric\napproach with the evolutionary search based approach proposed in [11], which is the nonparametric\nextension to BFRM.\nWe used the gene-factor connectivity matrix of E-coli network (described in [14]) to generate a\nsynthetic dataset having 100 samples of 50 genes and 8 underlying factors. Since we knew the\nground truth for factor loadings in this case, this dataset was ideal to test for efficacy in recovering\nthe factor loadings (binding sites and number of factors). We also experimented with a real gene-\nexpression data which is a breast cancer dataset having 251 samples of 226 genes and 5 prominent\nunderlying factors (we know this from domain knowledge).\n6.1Nonparametric Gene-Factor Modeling and Variable Selection\nForthesyntheticdatasetgeneratedbytheE-colinetwork,theresultsareshowninfigure4comparing\nthe actual network used to generate the data and the inferred factor loading matrix. As shown in\nfigure 4, we recovered exactly the same number (8) of factors, and almost exactly the same factor\nloadings (binding sites and number of factors) as the ground truth. In comparison, the evolutionary\nsearch based approach overestimated the number of factors and the inferred loadings clearly seem\nto be off from the actual loadings (even modulo column permutations).\nFactors\nGenes\nTrue Factor Loadings\n12345678\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\nFactors\nGenes\nInferred Factor Loadings\n \n \n12345678\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\nFactors\nGenes\nFactor Loadings Inferred by BFRM\n \n \n123456789 10\n5\n10\n15\n20\n25\n30\n35\n40\n45\nFigure 4: (Left and middle) True and inferred factor loadings (with our approach) for the synthetic data\nwith P=50, K=8 generated using connectivity matrix of E-coli data. (Right) Inferred factor loadings with the\nevolutionary search based approach. White rectangles represent active sites. The data also has added noise with\nsignal-to-noise-ratio of 10\nOur results on real data are shown in figure 5. To see the effect of variable selection for this data,\nwe also introduced spurious genes by adding 50 random features in each sample. We observe the\nfollowing: (1)Withoutvariableselectionbeingon,spuriousgenesresult inanoverestimatednumber\nof factors and falsely discovered factor loadings for spurious genes (see figure 5(a)), (2) Variable\nselection, when on, effectively filters out spurious genes, without overestimating the number of\nfactors (see figure 5(b)). We also investigated the effect of noise on the evolutionary search based\napproach and it resulted in an overestimated number of factor, plus false discovered factor loadings\nfor spurious genes (see figure 5(c)). To conserve space, we do not show here the cases when there\nare no spurious genes in the data but it turns out that variable selection does not filter out any of 226\nrelevant genes in such a case.\n6.2 Hierarchical Factor Modeling\nOur results with hierarchical factor modeling are shown in figure 6 for synthetic and real data. As\nshown, the model correctly infers the gene-factor associations, the number of factors, and the factor\n6"},{"page":7,"text":"Factors\n(a)\nNoise                             Genes\n \n \n2468\n50\n100\n150\n200\n250\n10\n20\n30\n40\n50\n60\nFactors\n(b)\nNoise                             Genes\n \n \n12345\n50\n100\n150\n200\n250\n10\n20\n30\n40\n50\n60\nFactors\nNoise                    Genes\n \n \n12345678\n50\n100\n150\n200\n250\n10\n20\n30\n40\n50\n60\n(c)\nFigure5: Effect of spurious genes (heat-plots of factor loading matrix shown): (a) Standard IBP (b) Our model\nwith variable selection (c) The evolutionary search based approach\nhierarchy. Thereare severalways to interpretthe hierarchy. Fromthe factorhierarchyforE-coli data\n(figure 6), we see that column-2 (corresponding to factor-2) of the V matrix is the most prominent\none (it regulates the highest number of genes), and is closest to the tree-root, followed by column-\n2, which it looks most similar to. Columns corresponding to lesser prominent factors are located\nfurther down in the hierarchy (with appropriate relatedness). Figure 6 (d) can be interpreted in a\nsimilar manner for breast-cancer data. The hierarchy can be used to find factors in order of their\nprominence. The higher we chop off the tree along the hierarchy, the more prominent the factors,\nwe discover, are. For instance, if we are only interested in top 2 factors in E-coli data, we can\nchop off the tree above the sixth coalescent point. This is akin to the agglomerative clustering sense\nwhich is usually done post-hoc. In contrast, our model discovers the factor hierarchies as part of the\ninference procedure itself. At the same time, there is no degradationof data reconstruction(in mean\nsquared error sense) and the log-likelihood, when compared to the case with Gaussian prior on V\n(see figure 7 - they actually improve). We also show in section 6.3 that hierarchical modeling results\nin better predictiveperformancefor the factor regressiontask. Empirical evidences also suggest that\nthe factor hierarchy leads to faster convergence since most of the unlikely configurations will never\nbe visited as they are constrained by the hierarchy.\n \n \n12345678\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n(a)\n35874612\n0.04\n0.045\n0.05\n0.055\n0.06\n0.065\n0.07\n0.075\n0.08\n0.085\n(b)\nFactors\nGenes\n \n \n1  2 3  4   5  \n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n220\n10\n20\n30\n40\n50\n60\n(c)\n12354\n0.07\n0.08\n0.09\n0.1\n0.11\n0.12\n(d)\nFigure 6: Hierarchical factor modeling results. (a) Factor loadings for E-coli data. (b) Inferred hierarchy for\nE-coli data. (c) Factor loadings for breast-cancer data. (d) Inferred hierarchy for breast-cancer data..\n6.3 Factor Regression\nWe report factor regression results for binary and real-valued responses and compare both variants\nof our model (Gaussian V and Coalescent V) against 3 different approaches: logistic regression,\nBFRM, and fitting a separate predictive model on the discovered factors (see figure 7 (c)). The\nbreast-cancer dataset had two binary response variables (phenotypes) associated with each sample.\nFor this binary prediction task, we split the data into training-set of 151 samples and test-set of 100\nsamples. This is essentially a transduction setting as described in section 3.4 and shown in figure 2.\nFor real-valuedprediction task, we treated a 30x20 block of the data matrix as our held-out data and\npredicted it based on the rest of the entries in the matrix. This method of evaluation is akin to the\ntask of image reconstruction [15]. The results are averaged over 20 random initializations and the\nlow error variances suggest that our method is fairly robust w.r.t. initializations.\n7"},{"page":8,"text":"0100 200 300 400500 600 700800 9001000\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\nIterations\nMSE\n \n \nCoalescent V\nGaussian V\nPost Convergence\nMSE of BFRM\n0 100 200300 400500600700 800900 1000\n\u22128.5\n\u22128\n\u22127.5\n\u22127\n\u22126.5\n\u22126\n\u22125.5\n\u22125\n\u22124.5x 10\n4\nlog likelihood\nIterations\n \n \nGaussian V\nCoalescent V\nModelBinary Real\n(%error,std dev) (MSE)\n17.5 (1.6)\n19.8 (1.4)\n15.8 (0.56)\n14.6 (0.48)\n18.1 (2.1)\nLogReg\nBFRM\nNor-V\nCoal-V\nPredModel\n-\n0.48\n0.45\n0.43\n-\nFigure 7: (a) MSE on the breast-cancer data for BFRM (horizontal line), our model with Gaussian (top red\ncurved line) and Coalescent (bottom blue curved line) priors. This MSE is the reconstruction error for the data\n- different from the MSE for the held-out real valued responses (fig 7 c) (b) Log-likelihoods for our model with\nGaussian (bottom red curved line) and Coalescent (top blue curved line) priors. (c) Factor regression results\n7Conclusions and Discussion\nWe have presented a fully nonparametric Bayesian approach to sparse factor regression, modeling\nthe gene-factor relationship using a sparse variant of the IBP. However, the true power of nonpara-\nmetric priors is evidenced by the ease of integration of task-specific models into the framework.\nBoth gene selection and hierarchical factor modeling are straightforward extensions in our model\nthat do not significantly complicate the inference procedure, but lead to improved model perfor-\nmance and more understandable outputs. We applied Kingman\u2019s coalescent as a hierarhical model\non V, the matrix modulating the expression levels of genes in factors. An interesting open question\nis whether the IBP can, itself, be modeled hierarchically.\nReferences\n[1] M. West. Bayesian Factor Regression Models in the \u201cLarge p, Small n\u201d Paradigm. In Bayesian Statistics\n7, 2003.\n[2] C. Sabatti and G. James. Bayesian Sparse Hidden Components Analysis for Transcription Regulation\nNetworks,. Bioinformatics 22, 2005.\n[3] G. Sanguinetti, N. D. Lawrence, and M. Rattray. Probabilistic Inference of Transcription Factor Concen-\ntrations and Gene-specific Regulatory Activities. Bioinformatics, 22(22), 2006.\n[4] M. J. Beal, F. Falciani, Z. Ghahramani, C. Rangel, and D. L. Wild. A Bayesian Approach to Reconstruct-\ning Genetic Regulatory Networks with Hidden Factors. Bioinformatics, 21(3), 2005.\n[5] Z. Ghahramani, T.L. Griffiths, and P. Sollich.Bayesian Nonparametric Latent Feature Models.\nBayesian Statistics 8. Oxford University Press, 2007.\n[6] J. F. C. Kingman. The coalescent. Stochastic Processes and their Applications, 1982.\n[7] T. Griffithsand Z. Ghahramani. InfiniteLatent FeatureModels and theIndian Buffet Process. In Advances\nin Neural Information Processing Systems 18, 2006.\n[8] Y. W. Teh, H. Daum\u00b4 e III, and D. M. Roy. Bayesian Agglomerative Clustering with Coalescents. In\nAdvances in Neural Information Processing Systems, volume 20, 2008.\n[9] E. Meeds, Z. Ghahramani, R. M. Neal, and S. T. Roweis. Modeling Dyadic Data with Binary Latent\nFactors. In Advances in Neural Information Processing Systems 19. 2007.\n[10] P. Green. Reversible jump markov chain monte carlo computation and bayesian model determination.\nBiometrica 82, 1995.\n[11] C. Carvalho, J. Lucas, Q. Wang, J. Chang, J. Nevins, and M. West. High-Dimensional Sparse Factor\nModelling - Applications in Gene Expression Genomics. In JASA, 2008.\n[12] D. Knowles and Z. Ghahramani. Infinite Sparse Factor Analysis and Infinite Independent Components\nAnalysis. In ICA 2007, 2007.\n[13] Francis R. Bach and Michael I. Jordan. Beyond independent components: trees and clusters. Journal of\nMachine Learning Research, pages 1205\u20131233, 2003.\n[14] I. Pournara and L. Wernisch. Factor Analysis for Gene Regulatory Networks and Transcription Factor\nActivity Profiles. BMC Bioinformatics, 2007.\n[15] J. J. Verbeek, S. T. Roweis, and N. Vlassis. Non-linear CCA and PCA by Alignment of Local Models. In\nAdvances in Neural Information Processing Systems 16. 2004.\nIn\n8"}],"widgetId":"rgw24_56ab9f3e7a026"},"id":"rgw24_56ab9f3e7a026","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=45865922&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw25_56ab9f3e7a026"},"id":"rgw25_56ab9f3e7a026","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=45865922&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":45865922,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/45865922_The_Infinite_Hierarchical_Factor_Regression_Model","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab9f3e7a026"},"id":"rgw2_56ab9f3e7a026","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":45865922},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=45865922&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab9f3e7a026"},"id":"rgw1_56ab9f3e7a026","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"hp+nKXUTWQ9\/b8gHqWVe6\/Vtpm0Sf\/YNoGKTPE1FLVxB9CkrBCI7Bj0x\/zE4VgsCW7ILKiBQDyshDrmr9ahUNSLa5r2M3Fzb1eXPNexN3eUoJXw8xb3tnVNFUqKaDKj4qeyBkp5QkrhZxXywtX49Q8yK3HhNs9u+gCx2ZiQBEn2xY2CwLzbGeXaeYrVxezTJLt6LUOI+VZJJTzg0VQbidDO1+CMTL5BYcRhyOr\/S4g\/7Wle1z10gvWLhbIDle1W1D\/et7t0lXC+sMS66OFFOHvaYbMDfIM\/ftxoKOiNKUik=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/45865922_The_Infinite_Hierarchical_Factor_Regression_Model\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"The Infinite Hierarchical Factor Regression Model\" \/>\n<meta property=\"og:description\" content=\"We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors. To accomplish this, we propose a sparse...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/45865922_The_Infinite_Hierarchical_Factor_Regression_Model\/links\/0f6460e138294e886aa150c6\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/45865922_The_Infinite_Hierarchical_Factor_Regression_Model\" \/>\n<meta property=\"rg:id\" content=\"PB:45865922\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"The Infinite Hierarchical Factor Regression Model\" \/>\n<meta name=\"citation_author\" content=\"Piyush Rai\" \/>\n<meta name=\"citation_author\" content=\"Hal Daum\u00e9 III\" \/>\n<meta name=\"citation_publication_date\" content=\"2009\/08\/04\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/45865922_The_Infinite_Hierarchical_Factor_Regression_Model\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/45865922_The_Infinite_Hierarchical_Factor_Regression_Model\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-3258d2b1-e861-4c07-a15f-7632f92dfc2a","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":766,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw26_56ab9f3e7a026"},"id":"rgw26_56ab9f3e7a026","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-3258d2b1-e861-4c07-a15f-7632f92dfc2a", "88dc5bcb3ac3f896eeec83b7e41ec8f95ccead73");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-3258d2b1-e861-4c07-a15f-7632f92dfc2a", "88dc5bcb3ac3f896eeec83b7e41ec8f95ccead73");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw27_56ab9f3e7a026"},"id":"rgw27_56ab9f3e7a026","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/45865922_The_Infinite_Hierarchical_Factor_Regression_Model","requestToken":"zH7q5qhkRwE60e3fwW1w92bGkgjZm5kzLoayNnkS0+RXbF2Qm11EGkkOEuLzApSLKkZ4rE9NSID\/v8MEbZZqb57M9CiYk0p9sBvuSwSIsI7zJO1nA1TeOzrFA6hc4Xfu41rt6+yVlG4mEQAq2ZcAo2emeezTkEIANMRuO94bvzvEdRaUWLootVC\/ftrgf4M34uUpI5Nnlk+AfJRlrMXpB6Ko1fyTaLArKjcfxXOpJXyh4b7fduiBDUaThTgdQPxqDKORtY8RnOMyNefRA57+dFkqaO+ZCxbDpbIaN+pJGO0=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=TNcaU3BSwGh4JM5E9UeRv2swoirCVyA2mxmdgMDAa_TUoSf2hXMh_LXfBSXhCn8l","encodedUrlAfterLogin":"cHVibGljYXRpb24vNDU4NjU5MjJfVGhlX0luZmluaXRlX0hpZXJhcmNoaWNhbF9GYWN0b3JfUmVncmVzc2lvbl9Nb2RlbA%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw29_56ab9f3e7a026"},"id":"rgw29_56ab9f3e7a026","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw28_56ab9f3e7a026"},"id":"rgw28_56ab9f3e7a026","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw30_56ab9f3e7a026"},"id":"rgw30_56ab9f3e7a026","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
