<!DOCTYPE html> <html lang="en" class="" id="rgw33_56ab9f8366cb1"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="Wimy1Ly7rO/tLmOERQ5fhFBLZbCvWcgwCl1FXe16iYXaZLKb9BF222yzwOH31qLdsl3uKp7HeuTZETcklzaoyAxFGwFnvfo2AyUjl+4+XM7g6V9qzeLNLvI2HomRo+VXR3zOrLGSG7sNtQc1fcEmEGXxUcT+pIoihaW8hkbyNQeT5z77Tj1om1aKfQFvDvuFtVL7D1Hw+V+5ZL9Drg8qbt58Meu3pEPLIFO1L7r/xWbmEOCBIQWaCuY/ovx5s3vA7cCiGQMT1+JsbQNMThRIV8iD3X43vrNjZzwEmqKzh2g="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-e2eaded6-c60f-47fb-9a97-93410b1265fd",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/220320618_Nonparametric_Bayesian_Matrix_Factorization_by_Power-EP" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Nonparametric Bayesian Matrix Factorization by Power-EP." />
<meta property="og:description" content="Many real-world applications can be modeled by matrix factorization. By approximating an observed data matrix as the product of two latent matrices, matrix factorization can reveal hidden..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/220320618_Nonparametric_Bayesian_Matrix_Factorization_by_Power-EP/links/02a85a480cf2fb757aff7589/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/220320618_Nonparametric_Bayesian_Matrix_Factorization_by_Power-EP" />
<meta property="rg:id" content="PB:220320618" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Nonparametric Bayesian Matrix Factorization by Power-EP." />
<meta name="citation_author" content="Nan Ding" />
<meta name="citation_author" content="Yuan (Alan) Qi" />
<meta name="citation_author" content="Rongjing Xiang" />
<meta name="citation_author" content="Ian Molloy" />
<meta name="citation_author" content="Ninghui Li" />
<meta name="citation_publication_date" content="2010/01/01" />
<meta name="citation_journal_title" content="Journal of Machine Learning Research" />
<meta name="citation_issn" content="1532-4435" />
<meta name="citation_volume" content="9" />
<meta name="citation_firstpage" content="169" />
<meta name="citation_lastpage" content="176" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/220320618_Nonparametric_Bayesian_Matrix_Factorization_by_Power-EP" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/220320618_Nonparametric_Bayesian_Matrix_Factorization_by_Power-EP" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Nonparametric Bayesian Matrix Factorization by Power-EP.</title>
<meta name="description" content="Nonparametric Bayesian Matrix Factorization by Power-EP. on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab9f8366cb1" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab9f8366cb1" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab9f8366cb1">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Nonparametric%20Bayesian%20Matrix%20Factorization%20by%20Power-EP.&rft.title=Journal%20of%20Machine%20Learning%20Research%20-%20Proceedings%20Track&rft.jtitle=Journal%20of%20Machine%20Learning%20Research%20-%20Proceedings%20Track&rft.volume=9&rft.date=2010&rft.pages=169-176&rft.issn=1532-4435&rft.au=Nan%20Ding%2CYuan%20(Alan)%20Qi%2CRongjing%20Xiang%2CIan%20Molloy%2CNinghui%20Li&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Nonparametric Bayesian Matrix Factorization by Power-EP.</h1> <meta itemprop="headline" content="Nonparametric Bayesian Matrix Factorization by Power-EP.">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/220320618_Nonparametric_Bayesian_Matrix_Factorization_by_Power-EP/links/02a85a480cf2fb757aff7589/smallpreview.png">  <div id="rgw8_56ab9f8366cb1" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56ab9f8366cb1"> <a href="researcher/70685224_Nan_Ding" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Nan Ding" alt="Nan Ding" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Nan Ding</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw10_56ab9f8366cb1">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/70685224_Nan_Ding"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Nan Ding" alt="Nan Ding" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/70685224_Nan_Ding" class="display-name">Nan Ding</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56ab9f8366cb1"> <a href="researcher/33206422_Yuan_Alan_Qi" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Yuan (Alan) Qi" alt="Yuan (Alan) Qi" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Yuan (Alan) Qi</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56ab9f8366cb1">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/33206422_Yuan_Alan_Qi"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Yuan (Alan) Qi" alt="Yuan (Alan) Qi" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/33206422_Yuan_Alan_Qi" class="display-name">Yuan (Alan) Qi</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw13_56ab9f8366cb1"> <a href="researcher/70272422_Rongjing_Xiang" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Rongjing Xiang" alt="Rongjing Xiang" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Rongjing Xiang</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw14_56ab9f8366cb1">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/70272422_Rongjing_Xiang"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Rongjing Xiang" alt="Rongjing Xiang" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/70272422_Rongjing_Xiang" class="display-name">Rongjing Xiang</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw15_56ab9f8366cb1"> <a href="researcher/45501099_Ian_Molloy" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Ian Molloy" alt="Ian Molloy" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Ian Molloy</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw16_56ab9f8366cb1">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/45501099_Ian_Molloy"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Ian Molloy" alt="Ian Molloy" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/45501099_Ian_Molloy" class="display-name">Ian Molloy</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw17_56ab9f8366cb1" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Ninghui_Li" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Ninghui Li" alt="Ninghui Li" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Ninghui Li</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw18_56ab9f8366cb1" data-account-key="Ninghui_Li">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Ninghui_Li"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Ninghui Li" alt="Ninghui Li" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Ninghui_Li" class="display-name">Ninghui Li</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Purdue_University" title="Purdue University">Purdue University</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/1532-4435_Journal_of_Machine_Learning_Research"><span itemprop="name">Journal of Machine Learning Research</span></a> </span>    (Impact Factor: 2.47).     <meta itemprop="datePublished" content="2010-01">  01/2010;  9:169-176.             <div class="pub-source"> Source: <a href="http://dblp.uni-trier.de/db/journals/jmlr/jmlrp9.html#DingQXML10" rel="nofollow">DBLP</a> </div>  </div> <div id="rgw19_56ab9f8366cb1" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>Many real-world applications can be modeled by matrix factorization. By approximating an observed data matrix as the product of two latent matrices, matrix factorization can reveal hidden structures embedded in data. A common challenge to use matrix factorization is determining the dimensionality of the latent matrices from data. Indian Buffet Processes (IBPs) enable us to apply the nonparametric Bayesian machinery to address this challenge. However, it remains a difficult task to learn nonparametric Bayesian matrix factorization models. In this paper, we propose a novel variational Bayesian method based on new equivalence classes of infinite matrices for learning these models. Furthermore, inspired by the success of nonnegative matrix factorization on many learning problems, we impose nonnegativity constraints on the latent matrices and mix variational inference with expectation propagation. This mixed inference method is unified in a power expectation propagation framework. Experimental results on image decomposition demonstrate the superior computational efficiency and the higher prediction accuracy of our methods compared to alternative Monte Carlo and variational inference methods for IBP models. We also apply the new methods to collaborative filtering and role mining and show the improved predictive performance over other matrix factorization methods.</div> </p>  </div>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw32_56ab9f8366cb1">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw31_56ab9f8366cb1"  itemprop="articleBody">  <p>Page 1</p> <p>Nonparametric Bayesian Matrix Factorization by Power-EP<br />Nan Ding+<br />Yuan (Alan) Qi?<br />+Department of Computer Science, Purdue University, West Lafayette, IN 47907<br />?Departments of Computer Science and Statistics, Purdue University, West Lafayette, IN 47907<br />Rongjing Xiang+<br />Ian Molloy+<br />Ninghui Li+<br />Abstract<br />Many real-world applications can be modeled<br />by matrix factorization. By approximating<br />an observed data matrix as the product of<br />two latent matrices, matrix factorization can<br />reveal hidden structures embedded in data.<br />A common challenge to use matrix factoriza-<br />tion is determining the dimensionality of the<br />latent matrices from data. Indian Buffet Pro-<br />cesses (IBPs) enable us to apply the nonpara-<br />metric Bayesian machinery to address this<br />challenge. However, it remains a difficult task<br />to learn nonparametric Bayesian matrix fac-<br />torization models. In this paper, we propose<br />a novel variational Bayesian method based<br />on new equivalence classes of infinite matri-<br />ces for learning these models. Furthermore,<br />inspired by the success of nonnegative matrix<br />factorization on many learning problems, we<br />impose nonnegativity constraints on the la-<br />tent matrices and mix variational inference<br />with expectation propagation.<br />inference method is unified in a power expec-<br />tation propagation framework. Experimental<br />results on image decomposition demonstrate<br />the superior computational efficiency and the<br />higher prediction accuracy of our methods<br />compared to alternative Monte Carlo and<br />variational inference methods for IBP mod-<br />els. We also apply the new methods to collab-<br />orative filtering and role mining and show the<br />improved predictive performance over other<br />matrix factorization methods.<br />This mixed<br />Appearing in Proceedings of the 13thInternational Con-<br />ference on Artificial Intelligence and Statistics (AISTATS)<br />2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of<br />JMLR: W&amp;CP 9. Copyright 2010 by the authors.<br />1Introduction<br />Matrix factorization models have been applied in many<br />areas of machine learning, information retrieval, and<br />computational biology. By approximating an observed<br />data matrix by a product of two (or three) latent ma-<br />trices, we can use matrix factorization to discover the<br />hidden structure embedded in observed data. If the<br />data is represented by a (N × D) matrix X where N is<br />the number of D-dimensional observations, the goal of<br />matrix factorization is to find latent matrices Z and A<br />such that X ≈ ZA. Each row of A can be viewed as a<br />basis vector for X and the loading matrix Z determines<br />how to combine these basis vectors together to recon-<br />struct observations in X.<br />rows of A is smaller than the number of observations,<br />suggesting the latent matrices Z and A offer compact<br />summary of the data.<br />Typically, the number of<br />Varying the dimensionality of latent matrices greatly<br />affects the performance of matrix factorization meth-<br />ods.Instead of fixing the dimensionality K, Grif-<br />fiths and Ghahramani (2005) propose nonparametric<br />Bayesian matrix factorization models based on Indian<br />buffet processes (IBPs). The IBP prior allows us to<br />model latent matrices of infinite sizes and learn the di-<br />mensionality of effective (nonzero) latent matrices au-<br />tomatically. Given massive data, however, it remains a<br />difficult task to efficiently estimate this nonparametric<br />matrix factorization model.<br />In this paper, we present two novel approximate<br />Bayesian inference methods to address this issue.<br />These approximate inference methods are based on<br />new equivalence classes for infinite matrices that con-<br />tain only non-zero columns of those infinite matri-<br />ces and are not in the left-ordered form proposed by<br />Griffiths and Ghahramani (2005). Without the left-<br />ordered constraint, these new equivalence classes al-<br />low us to approximate each column of the latent ma-<br />trix Z as independent factors, so that approximate in-<br />ference can be performed elegantly. Specifically, we<br />present in Section 2 the variational approximation<br />for these equivalence classes and adaptively select the</p>  <p>Page 2</p> <p>dimension of the latent matrices by maximizing the<br />marginal likelihood of these models. We also use the<br />variational inference to estimate observation noise and<br />hyper-parameters of this model.<br />this method as infinite matrix factorization for sim-<br />plicity. Recently, Doshi-Velez et al. (2009) proposed a<br />variational inference method for IBP. This method is<br />based on a truncated stick-break representation and<br />performs unfavorably compared to finite variational<br />approximation. Unlike this method, our new method<br />does not require any specification of a truncation level<br />and empirically achieves much higher prediction accu-<br />racy (with more robustness) than the finite variational<br />approximation.<br />Later we refer to<br />Inspired by the success of nonnegative matrix factor-<br />ization on many real-world applications such as im-<br />age decomposition (Lee and Seung, 2001) and com-<br />putational biology (Devarajan, 2008), we extend the<br />nonparametric Bayesian matrix factorization models<br />in Section 3 by imposing nonnegativity constraints on<br />elements of the latent matrices.<br />model infinite nonnegative matrix factorization. For<br />the efficient inference on this new model, we combine<br />the variational approximation inference with expecta-<br />tion propagation in the power expectation propagation<br />framework (Minka, 2004).<br />We call this new<br />In Section 4, we describe experimental results for im-<br />age decomposition, demonstrating the superior com-<br />putational efficiency and the improved prediction ac-<br />curacy of IMF and INMF compared to alternative<br />Monte Carlo and variational inference methods for<br />IBP models. In addition, we apply IMF and INMF<br />to collaborative filtering and role mining and demon-<br />strate the improved performance of INF and INMF<br />over other matrix factorization methods.<br />2 Infinite Matrix Factorization<br />Infinite matrix factorization models were proposed by<br />Griffiths and Ghahramani (2005).<br />nonparametric Bayesian prior distribution on an infi-<br />nite matrix by taking the limit for a prior distribution<br />on a finite Bayesian matrix and constructing equiva-<br />lence classes on infinite matrices. To develop the new<br />variational method for the infinite matrix factorization<br />model, we also start from this finite model.<br />They derive the<br />2.1 Finite Bayesian Matrix Factorization<br />Let us denote the (N ×D) data matrix by X. Our goal<br />is to decompose the data matrix X into a product of<br />two latent matrices Z (N × K) and A (K × D). The<br />factorization can be modeled by a likelihood function<br />p(X|Z,A), which represents a probabilistic generative<br />process for producing the data X. We also assign priors<br />over Z and A to capture our uncertainty in these latent<br />matrices.Using a hierarchical Bayesian model, the<br />problem of matrix factorization amounts to finding the<br />posterior distribution of Z and A:<br />p(Z,A|X) ∝ p(X,Z,A) = p(X|Z,A)p(A)p(Z|π)p(π)<br />where p(A), p(Z|π) are the prior distributions, π is the<br />parameter vector for the prior p(Z|π) , and p(π) is the<br />hyper-prior in this hierarchical Bayesian model.<br />We assign a factorized Gaussian prior on A = {akj}:<br />?<br />p(A) =<br />k,j<br />p(akj) =<br />?<br />k,j<br />1<br />?2πσ2<br />A<br />exp?−<br />a2<br />2σ2<br />kj<br />A<br />?<br />(1)<br />We use a binary matrix Z = {zik} and denote its kth<br />column by z:,k and its ithrow by zi,:. Using a factorized<br />discrete distribution on Z with the mean parameter<br />π = (π1,...,πK), we have<br />p(Z|π) =<br />?<br />k,i<br />p(zik|πk) =<br />?<br />k<br />π<br />?<br />izik<br />k<br />(1 − πk)N−?<br />izik<br />(2)<br />We assign a conjugate prior over π<br />p(π) =<br />?<br />k<br />Beta?α<br />K,1?=<br />?<br />k<br />α<br />Kπ<br />α<br />K−1<br />k<br />(3)<br />Note that α/K regularizes the sparsity of Z; if K<br />is large, πk is concentrated around small values and<br />therefore many elements of Z will be encouraged to be<br />zero.<br />We can choose a data likelihood function based on ap-<br />plications at hand. Here we use the Gaussian likeli-<br />hood function due to its popularity in practice.<br />p(X|Z,A) =<br />?<br />i,j<br />1<br />?2πσ2<br />X<br />exp?−<br />1<br />2σ2<br />X<br />(xij− zi,:a:,j)2?<br />(4)<br />where we denote the jthcolumn of A by a:,j. We as-<br />sume the variance parameter σX is known for the time<br />being.<br />2.2 Equivalence Classes<br />Instead of choosing K, the dimensionality of Z, to a<br />particular value, Griffiths and Ghahramani (2005) set<br />K → ∞ to obtain nonparametric Bayesian prior on Z.<br />In the infinite case, however, p(zk= 0,πk= 0) converges<br />to 1. Thus the probability of any nonzero matrix Z is<br />0. Apparently, such a model is not practically applica-<br />ble. To solve this issue, Doshi-Velez et al. (2009); Teh<br />et al. (2007) uses a truncated stick-breaking approx-<br />imation to an Indian buffet process. Their approach<br />requires a predefined truncation level T and, according<br />to Doshi-Velez et al. (2009); Teh et al. (2007), empir-<br />ically this approach does not outperform simple finite</p>  <p>Page 3</p> <p>matrix factorization models described in the previous<br />section.<br />Another approach to address this issue is using equiva-<br />lence classes of infinite matrices. By grouping the infi-<br />nite non-zero matrices into the equivalence classes, we<br />can make sure that the probability of an equivalence<br />class does not converge to 0 even though the prob-<br />ability of each infinite matrix in this class converges<br />to 0. For example, Griffiths and Ghahramani (2005)<br />defined equivalence classes on Z with respect to a func-<br />tion on Z that maps Z to left-ordered binary matrices<br />(so that the columns are sorted). Each of these equiva-<br />lence classes has a valid nonzero probability. However,<br />since the columns of the left-ordered binary matrices<br />are correlated in the left-ordered form, we cannot use<br />adopt variational inference with factorized approxima-<br />tions based on these equivalence classes.<br />To remove this coupling effect, we define new equiva-<br />lence classes on Z. The key observation we have is if<br />we remove all-zero columns from many infinite binary<br />matrices, many of these matrices can be reduced to<br />the same non-zero sub-matrix Z+. All the matrices Z<br />that can be reduced to the same sub-matrix Z+ have<br />the same effect in the data likelihood (4). To persevere<br />the data likelihood, we define equivalence classes on bi-<br />nary matrices by a many-to-one function: each binary<br />matrix Z is mapped to the representative of its class<br />¯Z = [Z+,Z0] where Z0 only contains all-zero columns.<br />This mapping moves the non-zero columns to the left<br />side of the all-zero columns without changing the or-<br />der between non-zero columns.<br />the columns in the left-ordered equivalence classes, the<br />non-zero columns of Z+ can be treated as independent<br />factors, which enables us to use factorized variational<br />approximations to the posterior distribution of Z+.<br />As a result, unlike<br />For a matrix with K columns and K+ non-zero<br />columns, we denote its equivalence class with a rep-<br />resentative matrix¯Z by [¯Z]K<br />trices that can be mapped to the same equivalence<br />class [¯Z]K<br />K<br />the joint distribution over [¯Z]K<br />K+. The number of the ma-<br />K+is simply C<br />K+<br />=<br />K!<br />K+!(K−K+)!. Considering<br />K+and π, we have<br />p([¯Z]K<br />K+,π) =C<br />K+<br />Kp(¯Z,π) = C<br />?<br />K+<br />Kp(Z+,π+)p(Z0,π0)<br />?<br />=C<br />K+<br />K<br />k≤K+<br />p(z:,k,πk)<br />k&gt;K+<br />p(z:,k= 0,πk) (5)<br />Note that in the above equation we partition π into<br />π+ and π0 according to the partition of¯(Z). It is easy<br />to derive that as K → ∞, although p(Z) converges to 0<br />for any particular Z ∈ [¯Z]∞<br />We also divide the rows of A into A+ and A0, such<br />that ZA = Z+A+. Now the task of learning IMFs is to<br />find the posterior distribution of [¯Z]∞<br />the exact posterior distributions of [¯Z]∞<br />K+, p([¯Z]∞<br />K+) does not.<br />K+, A and π. Since<br />K+, π and A are<br />computationally intractable, we describe a variational<br />method to approximate them in the next section.<br />2.3Variational inference for IMF<br />First, let us define the notation. We use ˜ zi,: to denote<br />the ithrow of Z+ and use ˜ a:,j to denote the jthcolumn<br />of A+.<br />We choose a factorized distribution to approximate the<br />posterior distributions of¯Z, A and π:<br />q(¯Z,A,π) =?<br />∞<br />?<br />k=1<br />q(πk)<br />?<br />i<br />q(zik)?<br />D<br />?<br />j=1<br />q(a:,j)<br />=q(π+)q(Z+)q(A+)q(Z0,π0)q(A0)<br />∝<br />?<br />k≤K+<br />?q(πk)<br />?<br />i<br />q(zik)??<br />D<br />?<br />j=1<br />q(˜ a:,j)?q(Z0,π0)q(A0)<br />Using Jensen’s inequality, we immediately obtain<br />lnp(X,K+)<br />?<br />≥<br />?<br />¯ Z<br />q(¯Z,π,A+,A0)ln<br />p(X,[¯Z]∞<br />K+,A,π)<br />q(¯Z,π,A)<br />dAdπ<br />= lim<br />K→∞<br />?<br />Z+<br />?<br />q(Z+,A+,π+)lnC<br />K+<br />K<br />p(X,Z+,A+,π+)<br />q(Z+,A+,π+)<br />dA+dπ+<br />?L(q;K+) (6)<br />The first equation holds since p(X,[¯Z]∞<br />Kp(X,Z+,A+,π+)p(Z0,π0)p(A0) and we set<br />q(Z0,π0) = p(Z0,π0) and q(A0) = p(A0). We denote the<br />above lower bound by L(q;K+) to emphasize that it<br />depends on the value of K+.<br />K+,A,π) =<br />limK→∞C<br />K+<br />The details for calculating the lower bound L(q;K+)<br />are shown in the appendix. Note that if we apply a<br />variational lower bound without using the equivalence<br />classes on Z, the lower bound becomes negatively in-<br />finite when K → ∞ since p(Z+,π+) → 0.<br />is defined to contain only non-zero columns but our<br />factorized q(zik) does not impose this constraint, the<br />above inequality holds only approximately. Since the<br />dataset that we apply IMF to has at least dozens of, if<br />not hundreds or thousands (or more) of, data points,<br />q(z:,k= 0) =?N<br />approximation to the exact lower bound. This is con-<br />firmed by our empirical results (e.g., see in Figure 3<br />where N=50).<br />Since Z+<br />i=1q(zik= 0) is a very small value. There-<br />fore, the approximate lower bound is a very accurate<br />Maximizing the lower bound L(q;K+), we obtain the<br />following iterative updates for k ≤ K+:<br />q(πk) = Beta(ˆ αk,ˆβk)<br />(7)<br />q(zik= 1) =<br />λik<br />λik+ 1<br />(8)<br />q(˜ a:,j) = N(˜ a:,j|mj,Vj)<br />(9)</p>  <p>Page 4</p> <p>Figure 1: Illustration of the synthetic image data. The<br />first row shows four latent images. The second row<br />are examples of the observed images that are random<br />combinations of the latent images with Gaussian noise<br />(variance =0.5).<br />where<br />ˆ αk=<br />?<br />i<br />q(zik= 1)<br />ˆβk= N − ˆ αk+ 1<br />?<br />Vj =(V0<br />j)−1+<br />1<br />σ2<br />x<br />?<br />i<br />Eq(Z+)[˜ zT<br />i,:˜ zi,:]<br />?−1<br />(10)<br />mj = Vj<br />?(V0<br />j)−1m0<br />j+<br />1<br />σ2<br />x<br />?<br />i<br />xijEq(Z+)[˜ zi,:]?<br />(11)<br />λik= exp?Ψ(ˆ αk) − Ψ(ˆβk)<br />−<br />2λ2<br />x<br />j<br />1<br />?<br />?2<br />?<br />r?=k<br />(q(zir= 1)Λj<br />rk) + Λj<br />kk− 2xijmj(k)??<br />where Λj= Vj+ mjmT<br />and Ψ(·) denotes the digamma function. For all k &gt; K+,<br />q(z:,k= 0,πk= 0) = 1.<br />j, mj(k) is the kthelement of mj<br />Now we address the issue of how to K+.<br />end, we maximize the lower bound L(q;K+) over K+.<br />Since L(q;K+) approximates the marginal likelihood<br />p(X|K+) ∝ p(K+|X)p(X), the maximization over K+ is<br />justified by Bayesian evidence maximization and does<br />not lead to overfitting. In practice, the algorithm is<br />initialized with K+ = 1. When the updates converge<br />for the current K+, we increase K+ by one and use<br />the current approximate posteriors to initialize the<br />next iterations. We stop increasing K+ when the ap-<br />proximate marginal model likelihood (i.e., evidence)<br />L(q;K+) stops to increase with a bigger K+.<br />To this<br />To illustrate how evidence maximization is used to<br />choose K+, we apply infinite matrix factorization on<br />the synthetic data that was used by Griffiths and<br />Ghahramani (2005). First, we define four latent im-<br />ages (each image corresponds a row of A and each<br />element of the latent image is either 1 or 0, as shown<br />in the first row of figure 1) and generate a 50 by 4<br />loading matrix Z by randomly sampling its elements<br />123<br />Number of Bases<br />4567<br />−500<br />−400<br />−300<br />−200<br />−100<br />0<br />L(q;K+)<br />Figure 2: The approximate marginal model likelihood<br />L(q;K+) on image data. By maximizing L(q;K+), the<br />variational infinite matrix factorization discovers the<br />true number of latent images, which is 4.<br />with p(zik = 0.5). Then we combine the latent images<br />A based on Z and add the Gaussian noise with mean<br />0 and variance 0.5 to generate 50 observed image data<br />X (see the second row of figure 1). We plot the vari-<br />ational lower bound L(q;K+) for different values of K+<br />in figure 2. The maximal value of L(q;K+) indicates<br />K+ = 4, which matches the true number of the latent<br />images. In other words, infinite matrix factorization<br />finds the true number of the latent images.<br />2.4Learning variance parameters σX and σA<br />In the previous section, we assume σX (the variance<br />of the observation noise in (4)) is known and we pre-<br />define the hyperparameter, σA, in the prior distribu-<br />tion (1). Now we extend our model to estimate σX and<br />σA from data. Specifically, we assign inverse-Gamma<br />prior distributions over σX and σA to represent the<br />uncertainty about them and approximate their poste-<br />rior distributions via variational inference. Using the<br />inverse-Gamma priors is equivalent to using Gamma<br />priors on the reciprocals of σX and σA, ηX ? 1/σ2<br />ηA? 1/σ2<br />Xand<br />A:<br />p(ηX) = Gamma(ηX|a0<br />X,b0<br />X)p(ηA) = Gamma(ηA|a0<br />A,b0<br />A)<br />where Gamma(·) is a Gamma distribution and a0<br />a0<br />mative.<br />X, a0<br />X,<br />X, and a0<br />Xare set to make the hyper-priors noninfor-<br />Given these priors, we obtain the following variational<br />updates for q(ηX) and q(ηA):<br />q(ηX) = Gamma(ηX|ˆ aX,ˆbX)q(ηA) = Gamma(ηA|ˆ aA,ˆbA)</p>  <p>Page 5</p> <p>where<br />ˆ aX = a0<br />X+ND<br />2<br />?<br />ˆbX = b0<br />X+1<br />2<br />ij<br />?x2<br />ij− 2xijEq[˜ zi,:]Eq[˜ a:,j]<br />+ Eq[˜ zi,:]Eq[˜ a:,j˜ aT<br />:,j]Eq[˜ zT<br />i,:]?<br />ˆ aA= a0<br />A+KD<br />2<br />?<br />ˆbA= b0<br />A+1<br />2<br />kj<br />Eq[˜ a2<br />kj]<br />The expectations of ηX and ηA are Eq[ηX] = ˆ aX/ˆbX and<br />Eq[ηA] = ˆ aA/ˆbA. For the expanded model, we change<br />the variational updates (10) and (11) by replacing 1/σX<br />and 1/σA with Eq[ηX] and Eq[ηA]. We also modify the<br />variational lower bound L(q;K+) accordingly.<br />3 Infinite nonnegative matrix<br />factorization<br />For certain applications such as image decomposition,<br />imposing nonnegativity constraints to the factorized<br />matrices leads to clearer model interpretation and im-<br />proved predictive power (Lee and Seung, 2001). To<br />increase the utility of Bayesian matrix factorization,<br />we extend the infinite matrix factorization model by<br />imposing nonnegative constraints on A. We call this<br />new model Infinite Nonnegative Matrix Factorization<br />(INMF).<br />For IMFs, we assign a factorized Gaussian prior over<br />A. For INMFs, we change this prior to a factorized<br />truncated Gaussian distribution:<br />p(A) ∝<br />?<br />k,j<br />N(akj|0,σ2<br />A)I(akj ≥ 0)<br />(12)<br />where I(·) is an indicator function.<br />Gaussian prior not only regularizes A to prevent over-<br />fitting as an L2-Regularizer but also effectively im-<br />poses nonnegative constraints on elements of A.<br />This truncated<br />Given the truncated Gaussian prior over A, we can-<br />not apply variational methods directly since the lower<br />bound (6) becomes negatively infinite.<br />this issue, we use the Power-Expectation Propagation<br />(Power-EP) framework (Minka, 2004) to approximate<br />the exact posterior.<br />To address<br />3.1Power-EP inference for INMF<br />For Power-EP, we choose the same form for the ap-<br />proximate posteriors q as before.Similar to the IMF<br />case, we match q(A0)q(Z0,π0) to the exact distri-<br />butions.Therefore, we only need to approximate<br />q(Z+)q(π+)q(A+) based on p(Z+,A+,π+|X). Now for<br />q(A+), we let<br />q(A+) ∝ ˜ p0(A+)˜ pX(A+)<br />(13)<br />where ˜ pX(A+) and ˜ p0(A+) are the Gaussian messages<br />from the likelihood p(X|Z+,A+,π+) and the prior p(A+)<br />to the variable A+, respectively. Since both messages<br />have the form of Gaussians, q(A+) is a Gaussian dis-<br />tribution.<br />Power-EP (Minka, 2004) generalizes expectation prop-<br />agation and variational inference (in particular, varia-<br />tional message passing (Winn and Bishop, 2004)) us-<br />ing a flexible α−divergence. This divergence includes<br />KL(p?q) and KL(q?p) as special cases. Using Power-<br />EP, we have three steps for processing a factor in the<br />joint distribution of the model: i) in the deletion step<br />we compute the partial posterior/belief after removing<br />the message from this factor; ii) in the projection step<br />we minimize an α-divergence (e.g., KL(p?q) or KL(q?P))<br />to obtain the new posterior q; and iii) in the message-<br />update step, we update the message to be the ratio of<br />the new posterior q and the partial belief computed in<br />the deletion step. We then iteratively process all the<br />factors in the joint distribution with these three steps.<br />For INMFs, we minimize KL(p?q) when processing the<br />prior factor p(A+), and minimize KL(q?p) when pro-<br />cessing the the likelihood and the other priors.<br />In the deletion step, to process the likelihood and the<br />priors on Z+ and π+, we compute the partial belief<br />q\X(A+) ∝ q(A+)/˜ px(A+). Because of (13), we have<br />q\X(A+) ∝ ˜ p0(A+)<br />(14)<br />Since q(Z+) and q(π+) are totally defined by the mes-<br />sages from the likelihood and the prior on Z, the partial<br />beliefs q\X(Z+) and q\X(π+) are 1 after removing these<br />messages.<br />Then in the projection step, we minimize the exclusive<br />KL divergence over the new approximate posteriors:<br />KL(q(Z+)q(π+)q(A+)?p(X|Z+,A+,π+)p(Z+,π+)q\X(A+)).<br />By replacing p(A+) with its approximation q\X(A+) in<br />the variational updates described in Section 2, we can<br />efficiently minimize the above KL divergence.<br />To update the message update, it normally requires<br />the computation of the messages from the likeli-<br />hood and the priors on Z+ and π+ to the variables<br />(A,Z+,&amp;π+. But since here we do not need the mes-<br />sages explicitly in the deletion step to obtain all the<br />partial beliefs, we can save the computation of these<br />messages.<br />When processing the truncated Gaussian prior p(A) =<br />?p(akj), we only update q(A+) since this prior does</p>  <p>Page 6</p> <p>not involve Z+ and π+. The deletion step computes<br />the partial belief<br />q\kj(0)(˜ a:,j) = N(˜ a:,j|m\kj(0),V\kj(0)) ∝ q(˜ a:,j)/˜ p0(akj)<br />where ˜ p0(akj ∝ N(mkj(0),Vkj(0)) (i.e., this message is a<br />K+× 1 dimensional Gaussian from akj to ˜ a:,j) and<br />V\kj(0)= (V−1<br />m\kj(0)= V\kj(0)(V−1<br />j<br />− Vkj(0))−1<br />j mj− V−1<br />(15)<br />(16)<br />kj(0)mkj(0))<br />The projection step gives the new posterior<br />qnew(˜ a:,j) ∝ N(˜ aj|mnew<br />j<br />,Vnew<br />j<br />),<br />by minimizing the inclusive<br />q\kj(0)(˜ a:,j)p(akj). The following moment matching<br />equations solve the minimization problem:<br />KL(ˆ p?q) where<br />ˆ p<br />∝<br />mnew<br />j<br />= mw+ αVwek<br />(17)<br />Vnew<br />j<br />= Vw−αmnew<br />j<br />(k)<br />vw(k,k)<br />VwekeT<br />kVT<br />w<br />(18)<br />where<br />Vw= ([V\kj(0)]−1+<br />1<br />σ2<br />A<br />ekeT<br />k)−1<br />mw= Vw([V\kj(0)]−1m\kj(0))<br />1<br />vw(k,k)1/2<br />α =<br />N(vw(k,k)−1/2mw(k)|0,1)<br />?vw(k,k)−1/2mw(k)<br />−∞<br />N(z|0,1)dz<br />To update the message, we take the ratio between<br />qnew(˜ a:,j) and q\kj(0)(˜ a:,j) and obtain the new message<br />˜ p0(akj with<br />Vnew<br />kj(0)=?(Vnew<br />mnew<br />j<br />)−1− (V\kj(0))−1?−1<br />j<br />)−1mnew<br />(19)<br />?(20)<br />kj(0)= Vnew<br />kj(0)<br />?(Vnew<br />j<br />− (V\kj(0))−1m\kj(0)<br />Note that the deletion step and message update can<br />be further simplified by low-rank matrix operations.<br />To learn K+ for the INMF model, we maximize the<br />approximate marginal likelihood L(q;K+). We also ex-<br />tend the model to estimate the posteriors of σX and<br />σA, just like what we have described in Section 2.4.<br />The IMF and INMF are summarized in Algorithm 1.<br />4 Experiments<br />To evaluate the proposed methods, we test them on<br />three tasks: synthetic image decomposition, collabo-<br />rative filtering, and role mining.<br />4.1Image Decomposition<br />First, we compare IMFs and INMFs with Gibbs sam-<br />pling (Wood and Griffiths, 2007), Particle Filtering<br />(PF) (Wood and Griffiths, 2007), Variational Finite<br />10<br />0<br />10<br />1<br />10<br />2<br />10<br />3<br />10<br />4<br />8<br />8.5<br />9<br />9.5<br />10<br />10.5<br />11<br />11.5<br />12<br />Time/s<br />Error<br /> <br /> <br />Gibbs<br />PF<br />IMF<br />INMF<br />VFMF<br />TVIMF<br />Figure 3: Performance comparison on image data.<br />Matrix Factorization (VFMF), and the variational infi-<br />nite matrix factorization (TVIMF) (Doshi-Velez et al.,<br />2009) on a synthetic dataset.<br />We use a similar data generation process as described<br />in Section 2.3 and by (Griffiths and Ghahramani,<br />2005). The only difference is that now we randomly<br />generate each of the 6-by-6 latent images that contain<br />8 elements of 1’s (the other elements are 0’s).<br />Figure 3 shows the performance of all these six algo-<br />rithms. For the evaluation, we interpret each row of<br />the latent matrix A as a base (i.e.,ak,:). The errors<br />in the figure are measured by the mean square differ-<br />ence between each latent base, ak,:, and the estimated<br />base that is closest to it. As shown in Figure 3, our<br />new methods converge much faster than the alterna-<br />tive methods, demonstrating their high computational<br />efficiency. The results of particle filtering (for which<br />we vary the number of particles from 100 to 400, 1000,<br />2500, 5000) fluctuate heavily. Sometimes the particle<br />filtering method even ends up with over 50 bases as<br />the final estimation, while the true number of bases<br />Algorithm 1 IMF and INMF<br />1. Initialize K+= 1.<br />2. Initialize all approximate factors.<br />3. For each variational inference iteration:<br />a) Loop over k = 1,...,K+:<br />Update q(πk) via (7)<br />b) Loop over i = 1,...,N,k = 1,...,K+:<br />Update q(zik) via (8)<br />c) Loop over j = 1,...,D:<br />For IMF: update q(aj) via (9)<br />For INMF:<br />Loop over j = 1,...,D,k = 1,...,K+:<br />Update q(aj) via (15) to (20)<br />d) Compute the evidence L(q;K+)<br />e) Increase K+ if L(q;K+) increases.</p>  <p>Page 7</p> <p>2468101214<br />0.169<br />0.17<br />0.171<br />0.172<br />0.173<br />0.174<br />0.175<br />0.176<br />0.177<br />K<br />NMAE<br /> <br /> <br />NMF<br />SVD<br />IMF<br />INMF<br />Figure 4: Prediction errors for collaborative filter-<br />ing. The plot shows Normalized Mean Absolute Er-<br />ror (NMAE) of four models on the Jester data. The<br />results are averaged over 10 experiments.<br />is only 4.<br />level T has to be sufficiently large (we varied it from<br />4 to 10) to achieve low estimate error. This makes<br />this variational approach less efficient compared to our<br />method. The performance of TVIMF and VFMF are<br />not stable in practice. Therefore, we run them multi-<br />ple times and pick the best solution from these runs.<br />By contrast, the IMF and INMF work stably and the<br />quality of the approximate lower bound is very good:<br />IMF and INMF never over-estimate K+ in our ex-<br />periments. IMF achieves the lowest prediction error<br />among all the approximate inference methods (except<br />INMF) for IBP models. Furthermore, the INMF leads<br />to highest prediction accuracy among all the methods,<br />demonstrating the effectiveness of the nonnegativity<br />constraints on latent matrices.<br />For TVIMF and VFMF, the truncation<br />4.2 Collaborative Filtering<br />We apply the new methods to a collaborative filtering<br />task to test how accurate they are when predicting the<br />preference of a specific user given his previous ratings.<br />We use a subset of the Jester dataset Goldberg et al.<br />(2001). The dataset contains 100 jokes with 73421 user<br />ratings. The density (or fraction of the rating matrix<br />that is filled) of the Jester set is about 0.5. We select<br />1000 users and for each selected user, 30 ratings are<br />held out for testing. We select 1000 users and for each<br />selected user, 30 ratings are held out for testing. The<br />experiment is repeated for 10 times, each time with a<br />different user subset.<br />IMF automatically discovers 7.8 latent bases averaged<br />over the 10 experiments. INMF gives 7.6 latent bases<br />on average. For comparison, VFMF, TVIMF, classical<br />nonnegative matrix factorization (NMF), and singular<br />value decomposition (SVD) are applied to the same<br />training and testing data. We did not apply Gibbs<br />samplers and particle filtes because of their limited<br />scalability for large datasets. NMF and SVD require<br />a predefined number of bases. We vary the number of<br />bases from 2 to 15 and choose this number by optimiz-<br />ing their performance. The truncation level of VFMF<br />and TVIMF is 30. Empirically, a smaller truncation<br />level leads to much worse prediction performance.<br />The results of NMF, SVD, IMF and INMF are shown<br />in figure 4. We do not plot the results of VFMF and<br />TVIMF since their results are out of the boundary of<br />this figure. VFMF achieves 0.1767 with 30 effective<br />bases, and TVIMF achieves 0.1761 with 30 effective<br />bases. IMF and INMF not only learn a more compact<br />representation of the data, but also give more accurate<br />predictions than the other methods.<br />4.3 Role Mining<br />Finally, we apply our new methods to the role min-<br />ing problem. Role mining is an active research area in<br />information security that aims at discovering a set of<br />roles for role-based access control (RBAC) from an ex-<br />isting user-permission assignment relation. In RBAC,<br />instead of assigning permissions directly to users, an<br />administrator assigns permissions to roles and autho-<br />rizes users to roles. To simplify administration, it is<br />desirable to keep the number of roles small.<br />We use two datasets, “Domino” and “Firewall” from<br />researchers at HP Labs. The “Domino” dataset, from<br />a Lotus Domino server, is a 79x231 binary matrix<br />where each row is a user and each column is a per-<br />mission for a given level of access to files, databases,<br />and custom applications. The “Firewall” dataset is a<br />709x365 binary matrix from a Cisco firewall used to<br />provide external users access to internal resources. In<br />our experiment, we first transpose the original matri-<br />ces, such that Z matrix becomes a meaningful binary<br />role matrix, which assigns permissions to roles.<br />We randomly hold out 20% elements of each matrix for<br />testing. Since the matrix is extremely sparse, we re-<br />port the Area Under the Curve (AUC) of IMF, INMF,<br />VFMF, TVIMF, NMF and SVD in Table 1. The trun-<br />cation level T of VFMF and TVIMF is 20. NMF and<br />SVD are chosent to have the optimal number of bases<br />that gives to their best AUC value.<br />We find that TFMF, TVIMF, IMF and SVD have<br />inferior performance on this dataset.<br />the AUC comparison shown in Table 1 suggests that<br />INMF outperforms all the other algorithms on both<br />datasets. For better illustration, we plot the ROC<br />curve of “domino” dataset. We observe that INMF<br />achieves the distinguishable True-Positive ratio when<br />the False-Positive ratio is small, which indicates the<br />desirable performance on the top of the list.<br />By contrast,</p>  <p>Page 8</p> <p>0 0.20.4 0.60.81<br />0.75<br />0.8<br />0.85<br />0.9<br />0.95<br />1<br />FALSE POSITIVE RATIO<br />TRUE POSITIVE RATIO<br /> <br /> <br />VFMF<br />TVIMF<br />SVD<br />NMF<br />IMF<br />INMF<br />Figure 5: The ROC curve for the “Domino” dataset.<br />Algorithm<br />DominoFirewall<br />K<br />20<br />20<br />5<br />4<br />8<br />9<br />AUC<br />0.9339<br />0.9382<br />0.9518<br />0.9755<br />0.9400<br />0.9869<br />K<br />20<br />20<br />10<br />6<br />7<br />9<br />AUC<br />0.9748<br />0.9732<br />0.9745<br />0.9801<br />0.9823<br />0.9853<br />VFMF<br />TVIMF<br />SVD<br />NMF<br />IMF<br />INMF<br />Table 1: AUC of VFMF, TVIMF, SVD, NMF, IMF<br />and INMF on “Domino” and “Firewall” datasets.<br />5 Conclusions<br />Using the new equivalence classes of infinite binary<br />matrices, we have developed two novel approximate<br />inference algorithms for infinite matrix factorization<br />and infinite nonnegative matrix factorization, respec-<br />tively. These two methods are unified in the power-<br />EP framework. They learn the model dimensional-<br />ity automatically from the data, as well as all the<br />model hyper-parameters.<br />ational method (Doshi-Velez et al., 2009), the new<br />methods do not use a truncated stick-breaking rep-<br />resentation (so no truncation levels to be set) and<br />achieve faster convergence and lower prediction error<br />rates compared to alternative inference methods.<br />Unlike the previous vari-<br />Acknowledgements<br />The work was supported by NSF IIS-0916443 and NSF<br />ECCS-0941533.<br />References<br />Griffiths TL, Ghahramani Z (2005) Infinite latent fea-<br />ture models and the Indian buffet process. Technical<br />Report 2005-001, Gatsby Computational Neuroscience<br />Unit, University College London.<br />Doshi-Velez F, Miller KT, Gael JV, Teh YW (2009) Vari-<br />ational inference for the Indian buffet process. In Pro-<br />ceedings of AISTATS.<br />Lee DD, Seung HS (2001) Algorithms for non-negative ma-<br />trix factorization. In Advances in Nerual Information<br />Processing 13. MIT Press.<br />Devarajan K (2008) Nonnegative matrix factorization: An<br />analytical and interpretive tool in computational biol-<br />ogy. PLoS Computational Biology 4: 1–12.<br />Minka T (2004) Power EP. Technical Report 2004-149,<br />Microsoft Research Ltd.<br />Teh YW, Gorur D, Ghahramani Z (2007) Stick-breaking<br />construction for the Indian buffet process. In Proceed-<br />ings of the International Conference on Artificial Intel-<br />ligence and Statistics. volume 11.<br />Winn J, Bishop CM (2004) Variational message passing.<br />Journal of Machine Learning Research 5.<br />Wood F, Griffiths TL (2007) Particle filtering for nonpara-<br />metric Bayesian matrix factorization. In Advances in<br />Nerual Information Processing 19. MIT Press.<br />Goldberg K, Roeder T, Gupta D, Perkins C (2001) Eigen-<br />taste: A constant time collaborative filtering algorithm.<br />Information Retrieval 4: 133–151.<br />A Lower bound of marginal likelihood<br />L(q;K+)<br />The lower bound L(q;K+) is given here.<br />L(q;K+) = T1+ T2+ T3− H1− H2− H3<br />where<br />T1 =<br />N<br />?<br />i=1<br />D<br />?<br />1<br />2σ2<br />j=1<br />?−1<br />K+<br />?<br />2ln(2πσ2<br />x) +xij<br />2σ2<br />x<br />K+<br />?<br />k=1<br />q(zik= 1)mj(k)<br />−<br />x<br />r=1<br />K+<br />?<br />k=1<br />Eq(Z+)(zirzik)Λj<br />rk−<br />1<br />2σ2<br />xx2<br />ij<br />?<br />T2 =<br />D<br />?<br />?<br />+ (<br />j=1<br />?−K+<br />?(<br />?<br />?−1<br />?<br />+ q(zik= 0)lnq(zik= 0)?<br />q(πk)lnq(πk)dπk<br />2<br />ln(2πσ2<br />A) −<br />1<br />2σ2<br />A<br />K+<br />?<br />k=1<br />Λj<br />kk<br />?<br />T3 =<br />k≤K+<br />?<br />q(zik= 1) − 1)?Ψ(ˆ αk) − Ψ(ˆ αk+ˆβk)?+ lnα<br />2ln|Vj| −K+<br />2<br />??<br />i<br />q(zik= 0))?Ψ(ˆβk) − Ψ(ˆ αk+ˆβk)?<br />i<br />k<br />?<br />H1 =<br />D<br />?<br />j=1<br />(1 + ln(2π))?<br />H2 =<br />k≤K+<br />i<br />q(zik= 1)lnq(zik= 1)<br />H3 =<br />?<br />=<br />k≤K+<br />?<br />?<br />k≤K+<br />+ (ˆβk− 1){ψ(ˆβk) − ψ(ˆ αk+ˆβk)}<br />− lnΓ(ˆ αk+ˆβk) + lnΓ(ˆ αk) + lnΓ(ˆβk)?<br />where Λj= Vj + mjmT<br />the covariance matrix of q(˜ aj) respectively, and mj(k) is<br />the k-th element of mj.<br />?(ˆ αk− 1){ψ(ˆ αk) − ψ(ˆ αk+ˆβk)}<br />j, mj and Vj are the mean and</p>   </div> <div id="rgw24_56ab9f8366cb1" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw25_56ab9f8366cb1">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw26_56ab9f8366cb1"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://www.cs.purdue.edu/homes/ding10/DinQiXia10.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Nonparametric Bayesian Matrix Factorization by Power-EP.">Nonparametric Bayesian Matrix Factorization by Pow...</a> </div>  <div class="details">   Available from <a href="http://www.cs.purdue.edu/homes/ding10/DinQiXia10.pdf" target="_blank" rel="nofollow">cs.purdue.edu</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw28_56ab9f8366cb1" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw29_56ab9f8366cb1">  </ul> </div> </div>   <div id="rgw20_56ab9f8366cb1" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw21_56ab9f8366cb1"> <div> <h5> <a href="publication/286413478_Estimating_size_and_scope_economies_in_the_Portuguese_water_sector_using_the_Bayesian_stochastic_frontier_analysis" class="color-inherit ga-similar-publication-title"><span class="publication-title">Estimating size and scope economies in the Portuguese water sector using the Bayesian stochastic frontier analysis</span></a>  </h5>  <div class="authors"> <a href="researcher/58482171_Pedro_Carvalho" class="authors ga-similar-publication-author">Pedro Carvalho</a>, <a href="researcher/15534211_Rui_Cunha_Marques" class="authors ga-similar-publication-author">Rui Cunha Marques</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw22_56ab9f8366cb1"> <div> <h5> <a href="publication/284122815_Probabilistic_approach_to_assessing_and_monitoring_settlements_caused_by_tunneling" class="color-inherit ga-similar-publication-title"><span class="publication-title">Probabilistic approach to assessing and monitoring settlements caused by tunneling</span></a>  </h5>  <div class="authors"> <a href="researcher/2007870883_Carles_Camos" class="authors ga-similar-publication-author">Carles Camós</a>, <a href="researcher/2048026281_Olga_Spackova" class="authors ga-similar-publication-author">Olga Špačková</a>, <a href="researcher/71768049_Daniel_Straub" class="authors ga-similar-publication-author">Daniel Straub</a>, <a href="researcher/21819000_Climent_Molins" class="authors ga-similar-publication-author">Climent Molins</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw23_56ab9f8366cb1"> <div> <h5> <a href="publication/291946547_A_Mobile_Sensing_Approach_for_Regional_Surveillance_of_Fugitive_Methane_Emissions_in_Oil_and_Gas_Production" class="color-inherit ga-similar-publication-title"><span class="publication-title">A Mobile Sensing Approach for Regional Surveillance of Fugitive Methane Emissions in Oil and Gas Production</span></a>  </h5>  <div class="authors"> <a href="researcher/18327462_John_D_Albertson" class="authors ga-similar-publication-author">John D Albertson</a>, <a href="researcher/2095802337_Tierney_A_Harvey" class="authors ga-similar-publication-author">Tierney A Harvey</a>, <a href="researcher/70947493_Greg_Foderaro" class="authors ga-similar-publication-author">Greg Foderaro</a>, <a href="researcher/2095834812_Pingping_Zhu" class="authors ga-similar-publication-author">Pingping Zhu</a>, <a href="researcher/2095911253_Xiaochi_Zhou" class="authors ga-similar-publication-author">Xiaochi Zhou</a>, <a href="researcher/13883831_Silvia_Ferrari" class="authors ga-similar-publication-author">Silvia Ferrari</a>, <a href="researcher/2089186854_M_Shahrooz_Amin" class="authors ga-similar-publication-author">M Shahrooz Amin</a>, <a href="researcher/2095905431_Mark_Modrak" class="authors ga-similar-publication-author">Mark Modrak</a>, <a href="researcher/2057660266_Halley_Lee_Brantley" class="authors ga-similar-publication-author">Halley Lee Brantley</a>, <a href="researcher/2095826696_Eben_Thoma" class="authors ga-similar-publication-author">Eben Thoma</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw34_56ab9f8366cb1" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw35_56ab9f8366cb1">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw36_56ab9f8366cb1" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=tMEnM0o9wDPXnmjXLkwyhinEwBtQ_u_ccFbbWhLg-dIHvzaZK6C2x0GzNhYRgHsA" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="YZY8vFngkqUM1Ole5PswgXbpchdlIcr+cBulpuqaSaSAcdj9ywVOQdq49QxnopsIkx14T8zDJeIPEycu42U3al+tGiU1ZhqRlp9RI6FrOwLiSdElSDPtezGKkNddQ/gBajAbVGFSrILAklNDKWK2iBxVVV27oTmK97Z32Xb8i30qt88skJMkW+//Gn9jvTcmaufFHy0Zj2UwKO/FRLTHPEyguYunF3x711vtG9afpG7ieuqFWp9CRAiEdQuRlOxwZApKQ0Rv8n3xz4Suc2hUQ8EnWzz/M2W7pfqMqfeQvaw="/> <input type="hidden" name="urlAfterLogin" value="publication/220320618_Nonparametric_Bayesian_Matrix_Factorization_by_Power-EP"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMzIwNjE4X05vbnBhcmFtZXRyaWNfQmF5ZXNpYW5fTWF0cml4X0ZhY3Rvcml6YXRpb25fYnlfUG93ZXItRVA%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMzIwNjE4X05vbnBhcmFtZXRyaWNfQmF5ZXNpYW5fTWF0cml4X0ZhY3Rvcml6YXRpb25fYnlfUG93ZXItRVA%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMzIwNjE4X05vbnBhcmFtZXRyaWNfQmF5ZXNpYW5fTWF0cml4X0ZhY3Rvcml6YXRpb25fYnlfUG93ZXItRVA%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw37_56ab9f8366cb1"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 565;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Ninghui Li","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Ninghui_Li","institution":"Purdue University","institutionUrl":false,"widgetId":"rgw4_56ab9f8366cb1"},"id":"rgw4_56ab9f8366cb1","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=2183428","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab9f8366cb1"},"id":"rgw3_56ab9f8366cb1","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=220320618","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":220320618,"title":"Nonparametric Bayesian Matrix Factorization by Power-EP.","journalTitle":"Journal of Machine Learning Research","journalDetailsTooltip":{"data":{"journalTitle":"Journal of Machine Learning Research","journalAbbrev":"J MACH LEARN RES","publisher":false,"issn":"1532-4435","impactFactor":"2.47","fiveYearImpactFactor":"4.77","citedHalfLife":"8.30","immediacyIndex":"0.31","eigenFactor":"0.03","articleInfluence":"3.23","widgetId":"rgw6_56ab9f8366cb1"},"id":"rgw6_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=1532-4435","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"01\/2010;","publicationDateRobot":"2010-01","article":"9:169-176.","journalTitle":"Journal of Machine Learning Research","journalUrl":"journal\/1532-4435_Journal_of_Machine_Learning_Research","impactFactor":2.47}},"source":{"sourceUrl":"http:\/\/dblp.uni-trier.de\/db\/journals\/jmlr\/jmlrp9.html#DingQXML10","sourceName":"DBLP"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Nonparametric Bayesian Matrix Factorization by Power-EP."},{"key":"rft.title","value":"Journal of Machine Learning Research - Proceedings Track"},{"key":"rft.jtitle","value":"Journal of Machine Learning Research - Proceedings Track"},{"key":"rft.volume","value":"9"},{"key":"rft.date","value":"2010"},{"key":"rft.pages","value":"169-176"},{"key":"rft.issn","value":"1532-4435"},{"key":"rft.au","value":"Nan Ding,Yuan (Alan) Qi,Rongjing Xiang,Ian Molloy,Ninghui Li"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56ab9f8366cb1"},"id":"rgw7_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=220320618","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":220320618,"peopleItems":[{"data":{"authorUrl":"researcher\/70685224_Nan_Ding","authorNameOnPublication":"Nan Ding","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Nan Ding","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/70685224_Nan_Ding","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw10_56ab9f8366cb1"},"id":"rgw10_56ab9f8366cb1","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=70685224&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw9_56ab9f8366cb1"},"id":"rgw9_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=70685224&authorNameOnPublication=Nan%20Ding","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/33206422_Yuan_Alan_Qi","authorNameOnPublication":"Yuan (Alan) Qi","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Yuan (Alan) Qi","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/33206422_Yuan_Alan_Qi","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56ab9f8366cb1"},"id":"rgw12_56ab9f8366cb1","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=33206422&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56ab9f8366cb1"},"id":"rgw11_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=33206422&authorNameOnPublication=Yuan%20%28Alan%29%20Qi","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/70272422_Rongjing_Xiang","authorNameOnPublication":"Rongjing Xiang","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Rongjing Xiang","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/70272422_Rongjing_Xiang","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw14_56ab9f8366cb1"},"id":"rgw14_56ab9f8366cb1","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=70272422&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw13_56ab9f8366cb1"},"id":"rgw13_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=70272422&authorNameOnPublication=Rongjing%20Xiang","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/45501099_Ian_Molloy","authorNameOnPublication":"Ian Molloy","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Ian Molloy","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/45501099_Ian_Molloy","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw16_56ab9f8366cb1"},"id":"rgw16_56ab9f8366cb1","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=45501099&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw15_56ab9f8366cb1"},"id":"rgw15_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=45501099&authorNameOnPublication=Ian%20Molloy","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Ninghui Li","accountUrl":"profile\/Ninghui_Li","accountKey":"Ninghui_Li","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Ninghui Li","profile":{"professionalInstitution":{"professionalInstitutionName":"Purdue University","professionalInstitutionUrl":"institution\/Purdue_University"}},"professionalInstitutionName":"Purdue University","professionalInstitutionUrl":"institution\/Purdue_University","url":"profile\/Ninghui_Li","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Ninghui_Li","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw18_56ab9f8366cb1"},"id":"rgw18_56ab9f8366cb1","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=2183428&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Purdue University","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":5,"accountCount":1,"publicationUid":220320618,"widgetId":"rgw17_56ab9f8366cb1"},"id":"rgw17_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=2183428&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=5&accountCount=1&publicationUid=220320618","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab9f8366cb1"},"id":"rgw8_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=220320618&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":220320618,"abstract":"<noscript><\/noscript><div>Many real-world applications can be modeled by matrix factorization. By approximating an observed data matrix as the product of two latent matrices, matrix factorization can reveal hidden structures embedded in data. A common challenge to use matrix factorization is determining the dimensionality of the latent matrices from data. Indian Buffet Processes (IBPs) enable us to apply the nonparametric Bayesian machinery to address this challenge. However, it remains a difficult task to learn nonparametric Bayesian matrix factorization models. In this paper, we propose a novel variational Bayesian method based on new equivalence classes of infinite matrices for learning these models. Furthermore, inspired by the success of nonnegative matrix factorization on many learning problems, we impose nonnegativity constraints on the latent matrices and mix variational inference with expectation propagation. This mixed inference method is unified in a power expectation propagation framework. Experimental results on image decomposition demonstrate the superior computational efficiency and the higher prediction accuracy of our methods compared to alternative Monte Carlo and variational inference methods for IBP models. We also apply the new methods to collaborative filtering and role mining and show the improved predictive performance over other matrix factorization methods.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw19_56ab9f8366cb1"},"id":"rgw19_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=220320618","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/220320618_Nonparametric_Bayesian_Matrix_Factorization_by_Power-EP\/links\/02a85a480cf2fb757aff7589\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw5_56ab9f8366cb1"},"id":"rgw5_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=220320618&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":58482171,"url":"researcher\/58482171_Pedro_Carvalho","fullname":"Pedro Carvalho","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":15534211,"url":"researcher\/15534211_Rui_Cunha_Marques","fullname":"Rui Cunha Marques","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2016","journal":"Science of The Total Environment","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/286413478_Estimating_size_and_scope_economies_in_the_Portuguese_water_sector_using_the_Bayesian_stochastic_frontier_analysis","usePlainButton":true,"publicationUid":286413478,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"4.10","url":"publication\/286413478_Estimating_size_and_scope_economies_in_the_Portuguese_water_sector_using_the_Bayesian_stochastic_frontier_analysis","title":"Estimating size and scope economies in the Portuguese water sector using the Bayesian stochastic frontier analysis","displayTitleAsLink":true,"authors":[{"id":58482171,"url":"researcher\/58482171_Pedro_Carvalho","fullname":"Pedro Carvalho","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":15534211,"url":"researcher\/15534211_Rui_Cunha_Marques","fullname":"Rui Cunha Marques","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Science of The Total Environment 02\/2016; 544:574-586. DOI:10.1016\/j.scitotenv.2015.11.169"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/286413478_Estimating_size_and_scope_economies_in_the_Portuguese_water_sector_using_the_Bayesian_stochastic_frontier_analysis","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/286413478_Estimating_size_and_scope_economies_in_the_Portuguese_water_sector_using_the_Bayesian_stochastic_frontier_analysis\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw21_56ab9f8366cb1"},"id":"rgw21_56ab9f8366cb1","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=286413478","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2007870883,"url":"researcher\/2007870883_Carles_Camos","fullname":"Carles Cam\u00f3s","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2048026281,"url":"researcher\/2048026281_Olga_Spackova","fullname":"Olga \u0160pa\u010dkov\u00e1","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":71768049,"url":"researcher\/71768049_Daniel_Straub","fullname":"Daniel Straub","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":21819000,"url":"researcher\/21819000_Climent_Molins","fullname":"Climent Molins","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Tunnelling and Underground Space Technology","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/284122815_Probabilistic_approach_to_assessing_and_monitoring_settlements_caused_by_tunneling","usePlainButton":true,"publicationUid":284122815,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.49","url":"publication\/284122815_Probabilistic_approach_to_assessing_and_monitoring_settlements_caused_by_tunneling","title":"Probabilistic approach to assessing and monitoring settlements caused by tunneling","displayTitleAsLink":true,"authors":[{"id":2007870883,"url":"researcher\/2007870883_Carles_Camos","fullname":"Carles Cam\u00f3s","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2048026281,"url":"researcher\/2048026281_Olga_Spackova","fullname":"Olga \u0160pa\u010dkov\u00e1","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":71768049,"url":"researcher\/71768049_Daniel_Straub","fullname":"Daniel Straub","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":21819000,"url":"researcher\/21819000_Climent_Molins","fullname":"Climent Molins","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Tunnelling and Underground Space Technology 01\/2016; 51:313-325. DOI:10.1016\/j.tust.2015.10.041"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/284122815_Probabilistic_approach_to_assessing_and_monitoring_settlements_caused_by_tunneling","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/284122815_Probabilistic_approach_to_assessing_and_monitoring_settlements_caused_by_tunneling\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw22_56ab9f8366cb1"},"id":"rgw22_56ab9f8366cb1","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=284122815","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":18327462,"url":"researcher\/18327462_John_D_Albertson","fullname":"John D Albertson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095802337,"url":"researcher\/2095802337_Tierney_A_Harvey","fullname":"Tierney A Harvey","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70947493,"url":"researcher\/70947493_Greg_Foderaro","fullname":"Greg Foderaro","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2095834812,"url":"researcher\/2095834812_Pingping_Zhu","fullname":"Pingping Zhu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":6,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Environmental Science & Technology","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291946547_A_Mobile_Sensing_Approach_for_Regional_Surveillance_of_Fugitive_Methane_Emissions_in_Oil_and_Gas_Production","usePlainButton":true,"publicationUid":291946547,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"5.33","url":"publication\/291946547_A_Mobile_Sensing_Approach_for_Regional_Surveillance_of_Fugitive_Methane_Emissions_in_Oil_and_Gas_Production","title":"A Mobile Sensing Approach for Regional Surveillance of Fugitive Methane Emissions in Oil and Gas Production","displayTitleAsLink":true,"authors":[{"id":18327462,"url":"researcher\/18327462_John_D_Albertson","fullname":"John D Albertson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095802337,"url":"researcher\/2095802337_Tierney_A_Harvey","fullname":"Tierney A Harvey","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70947493,"url":"researcher\/70947493_Greg_Foderaro","fullname":"Greg Foderaro","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095834812,"url":"researcher\/2095834812_Pingping_Zhu","fullname":"Pingping Zhu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095911253,"url":"researcher\/2095911253_Xiaochi_Zhou","fullname":"Xiaochi Zhou","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":13883831,"url":"researcher\/13883831_Silvia_Ferrari","fullname":"Silvia Ferrari","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089186854,"url":"researcher\/2089186854_M_Shahrooz_Amin","fullname":"M Shahrooz Amin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095905431,"url":"researcher\/2095905431_Mark_Modrak","fullname":"Mark Modrak","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2057660266,"url":"researcher\/2057660266_Halley_Lee_Brantley","fullname":"Halley Lee Brantley","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095826696,"url":"researcher\/2095826696_Eben_Thoma","fullname":"Eben Thoma","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Environmental Science & Technology 01\/2016;  DOI:10.1021\/acs.est.5b05059"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291946547_A_Mobile_Sensing_Approach_for_Regional_Surveillance_of_Fugitive_Methane_Emissions_in_Oil_and_Gas_Production","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291946547_A_Mobile_Sensing_Approach_for_Regional_Surveillance_of_Fugitive_Methane_Emissions_in_Oil_and_Gas_Production\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw23_56ab9f8366cb1"},"id":"rgw23_56ab9f8366cb1","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291946547","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw20_56ab9f8366cb1"},"id":"rgw20_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=220320618&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":220320618,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":220320618,"publicationType":"article","linkId":"02a85a480cf2fb757aff7589","fileName":"Nonparametric Bayesian Matrix Factorization by Power-EP.","fileUrl":"http:\/\/www.cs.purdue.edu\/homes\/ding10\/DinQiXia10.pdf","name":"cs.purdue.edu","nameUrl":"http:\/\/www.cs.purdue.edu\/homes\/ding10\/DinQiXia10.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw26_56ab9f8366cb1"},"id":"rgw26_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=220320618&linkId=02a85a480cf2fb757aff7589&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw25_56ab9f8366cb1"},"id":"rgw25_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=220320618&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":0,"valueFormatted":"0","widgetId":"rgw27_56ab9f8366cb1"},"id":"rgw27_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=220320618","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw24_56ab9f8366cb1"},"id":"rgw24_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=220320618&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":220320618,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw29_56ab9f8366cb1"},"id":"rgw29_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=220320618&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":0,"valueFormatted":"0","widgetId":"rgw30_56ab9f8366cb1"},"id":"rgw30_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=220320618","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw28_56ab9f8366cb1"},"id":"rgw28_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=220320618&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Nonparametric Bayesian Matrix Factorization by Power-EP\nNan Ding+\nYuan (Alan) Qi?\n+Department of Computer Science, Purdue University, West Lafayette, IN 47907\n?Departments of Computer Science and Statistics, Purdue University, West Lafayette, IN 47907\nRongjing Xiang+\nIan Molloy+\nNinghui Li+\nAbstract\nMany real-world applications can be modeled\nby matrix factorization. By approximating\nan observed data matrix as the product of\ntwo latent matrices, matrix factorization can\nreveal hidden structures embedded in data.\nA common challenge to use matrix factoriza-\ntion is determining the dimensionality of the\nlatent matrices from data. Indian Buffet Pro-\ncesses (IBPs) enable us to apply the nonpara-\nmetric Bayesian machinery to address this\nchallenge. However, it remains a difficult task\nto learn nonparametric Bayesian matrix fac-\ntorization models. In this paper, we propose\na novel variational Bayesian method based\non new equivalence classes of infinite matri-\nces for learning these models. Furthermore,\ninspired by the success of nonnegative matrix\nfactorization on many learning problems, we\nimpose nonnegativity constraints on the la-\ntent matrices and mix variational inference\nwith expectation propagation.\ninference method is unified in a power expec-\ntation propagation framework. Experimental\nresults on image decomposition demonstrate\nthe superior computational efficiency and the\nhigher prediction accuracy of our methods\ncompared to alternative Monte Carlo and\nvariational inference methods for IBP mod-\nels. We also apply the new methods to collab-\norative filtering and role mining and show the\nimproved predictive performance over other\nmatrix factorization methods.\nThis mixed\nAppearing in Proceedings of the 13thInternational Con-\nference on Artificial Intelligence and Statistics (AISTATS)\n2010, Chia Laguna Resort, Sardinia, Italy. Volume 9 of\nJMLR: W&CP 9. Copyright 2010 by the authors.\n1Introduction\nMatrix factorization models have been applied in many\nareas of machine learning, information retrieval, and\ncomputational biology. By approximating an observed\ndata matrix by a product of two (or three) latent ma-\ntrices, we can use matrix factorization to discover the\nhidden structure embedded in observed data. If the\ndata is represented by a (N \u00d7 D) matrix X where N is\nthe number of D-dimensional observations, the goal of\nmatrix factorization is to find latent matrices Z and A\nsuch that X \u2248 ZA. Each row of A can be viewed as a\nbasis vector for X and the loading matrix Z determines\nhow to combine these basis vectors together to recon-\nstruct observations in X.\nrows of A is smaller than the number of observations,\nsuggesting the latent matrices Z and A offer compact\nsummary of the data.\nTypically, the number of\nVarying the dimensionality of latent matrices greatly\naffects the performance of matrix factorization meth-\nods.Instead of fixing the dimensionality K, Grif-\nfiths and Ghahramani (2005) propose nonparametric\nBayesian matrix factorization models based on Indian\nbuffet processes (IBPs). The IBP prior allows us to\nmodel latent matrices of infinite sizes and learn the di-\nmensionality of effective (nonzero) latent matrices au-\ntomatically. Given massive data, however, it remains a\ndifficult task to efficiently estimate this nonparametric\nmatrix factorization model.\nIn this paper, we present two novel approximate\nBayesian inference methods to address this issue.\nThese approximate inference methods are based on\nnew equivalence classes for infinite matrices that con-\ntain only non-zero columns of those infinite matri-\nces and are not in the left-ordered form proposed by\nGriffiths and Ghahramani (2005). Without the left-\nordered constraint, these new equivalence classes al-\nlow us to approximate each column of the latent ma-\ntrix Z as independent factors, so that approximate in-\nference can be performed elegantly. Specifically, we\npresent in Section 2 the variational approximation\nfor these equivalence classes and adaptively select the"},{"page":2,"text":"dimension of the latent matrices by maximizing the\nmarginal likelihood of these models. We also use the\nvariational inference to estimate observation noise and\nhyper-parameters of this model.\nthis method as infinite matrix factorization for sim-\nplicity. Recently, Doshi-Velez et al. (2009) proposed a\nvariational inference method for IBP. This method is\nbased on a truncated stick-break representation and\nperforms unfavorably compared to finite variational\napproximation. Unlike this method, our new method\ndoes not require any specification of a truncation level\nand empirically achieves much higher prediction accu-\nracy (with more robustness) than the finite variational\napproximation.\nLater we refer to\nInspired by the success of nonnegative matrix factor-\nization on many real-world applications such as im-\nage decomposition (Lee and Seung, 2001) and com-\nputational biology (Devarajan, 2008), we extend the\nnonparametric Bayesian matrix factorization models\nin Section 3 by imposing nonnegativity constraints on\nelements of the latent matrices.\nmodel infinite nonnegative matrix factorization. For\nthe efficient inference on this new model, we combine\nthe variational approximation inference with expecta-\ntion propagation in the power expectation propagation\nframework (Minka, 2004).\nWe call this new\nIn Section 4, we describe experimental results for im-\nage decomposition, demonstrating the superior com-\nputational efficiency and the improved prediction ac-\ncuracy of IMF and INMF compared to alternative\nMonte Carlo and variational inference methods for\nIBP models. In addition, we apply IMF and INMF\nto collaborative filtering and role mining and demon-\nstrate the improved performance of INF and INMF\nover other matrix factorization methods.\n2 Infinite Matrix Factorization\nInfinite matrix factorization models were proposed by\nGriffiths and Ghahramani (2005).\nnonparametric Bayesian prior distribution on an infi-\nnite matrix by taking the limit for a prior distribution\non a finite Bayesian matrix and constructing equiva-\nlence classes on infinite matrices. To develop the new\nvariational method for the infinite matrix factorization\nmodel, we also start from this finite model.\nThey derive the\n2.1 Finite Bayesian Matrix Factorization\nLet us denote the (N \u00d7D) data matrix by X. Our goal\nis to decompose the data matrix X into a product of\ntwo latent matrices Z (N \u00d7 K) and A (K \u00d7 D). The\nfactorization can be modeled by a likelihood function\np(X|Z,A), which represents a probabilistic generative\nprocess for producing the data X. We also assign priors\nover Z and A to capture our uncertainty in these latent\nmatrices.Using a hierarchical Bayesian model, the\nproblem of matrix factorization amounts to finding the\nposterior distribution of Z and A:\np(Z,A|X) \u221d p(X,Z,A) = p(X|Z,A)p(A)p(Z|\u03c0)p(\u03c0)\nwhere p(A), p(Z|\u03c0) are the prior distributions, \u03c0 is the\nparameter vector for the prior p(Z|\u03c0) , and p(\u03c0) is the\nhyper-prior in this hierarchical Bayesian model.\nWe assign a factorized Gaussian prior on A = {akj}:\n?\np(A) =\nk,j\np(akj) =\n?\nk,j\n1\n?2\u03c0\u03c32\nA\nexp?\u2212\na2\n2\u03c32\nkj\nA\n?\n(1)\nWe use a binary matrix Z = {zik} and denote its kth\ncolumn by z:,k and its ithrow by zi,:. Using a factorized\ndiscrete distribution on Z with the mean parameter\n\u03c0 = (\u03c01,...,\u03c0K), we have\np(Z|\u03c0) =\n?\nk,i\np(zik|\u03c0k) =\n?\nk\n\u03c0\n?\nizik\nk\n(1 \u2212 \u03c0k)N\u2212?\nizik\n(2)\nWe assign a conjugate prior over \u03c0\np(\u03c0) =\n?\nk\nBeta?\u03b1\nK,1?=\n?\nk\n\u03b1\nK\u03c0\n\u03b1\nK\u22121\nk\n(3)\nNote that \u03b1\/K regularizes the sparsity of Z; if K\nis large, \u03c0k is concentrated around small values and\ntherefore many elements of Z will be encouraged to be\nzero.\nWe can choose a data likelihood function based on ap-\nplications at hand. Here we use the Gaussian likeli-\nhood function due to its popularity in practice.\np(X|Z,A) =\n?\ni,j\n1\n?2\u03c0\u03c32\nX\nexp?\u2212\n1\n2\u03c32\nX\n(xij\u2212 zi,:a:,j)2?\n(4)\nwhere we denote the jthcolumn of A by a:,j. We as-\nsume the variance parameter \u03c3X is known for the time\nbeing.\n2.2 Equivalence Classes\nInstead of choosing K, the dimensionality of Z, to a\nparticular value, Griffiths and Ghahramani (2005) set\nK \u2192 \u221e to obtain nonparametric Bayesian prior on Z.\nIn the infinite case, however, p(zk= 0,\u03c0k= 0) converges\nto 1. Thus the probability of any nonzero matrix Z is\n0. Apparently, such a model is not practically applica-\nble. To solve this issue, Doshi-Velez et al. (2009); Teh\net al. (2007) uses a truncated stick-breaking approx-\nimation to an Indian buffet process. Their approach\nrequires a predefined truncation level T and, according\nto Doshi-Velez et al. (2009); Teh et al. (2007), empir-\nically this approach does not outperform simple finite"},{"page":3,"text":"matrix factorization models described in the previous\nsection.\nAnother approach to address this issue is using equiva-\nlence classes of infinite matrices. By grouping the infi-\nnite non-zero matrices into the equivalence classes, we\ncan make sure that the probability of an equivalence\nclass does not converge to 0 even though the prob-\nability of each infinite matrix in this class converges\nto 0. For example, Griffiths and Ghahramani (2005)\ndefined equivalence classes on Z with respect to a func-\ntion on Z that maps Z to left-ordered binary matrices\n(so that the columns are sorted). Each of these equiva-\nlence classes has a valid nonzero probability. However,\nsince the columns of the left-ordered binary matrices\nare correlated in the left-ordered form, we cannot use\nadopt variational inference with factorized approxima-\ntions based on these equivalence classes.\nTo remove this coupling effect, we define new equiva-\nlence classes on Z. The key observation we have is if\nwe remove all-zero columns from many infinite binary\nmatrices, many of these matrices can be reduced to\nthe same non-zero sub-matrix Z+. All the matrices Z\nthat can be reduced to the same sub-matrix Z+ have\nthe same effect in the data likelihood (4). To persevere\nthe data likelihood, we define equivalence classes on bi-\nnary matrices by a many-to-one function: each binary\nmatrix Z is mapped to the representative of its class\n\u00afZ = [Z+,Z0] where Z0 only contains all-zero columns.\nThis mapping moves the non-zero columns to the left\nside of the all-zero columns without changing the or-\nder between non-zero columns.\nthe columns in the left-ordered equivalence classes, the\nnon-zero columns of Z+ can be treated as independent\nfactors, which enables us to use factorized variational\napproximations to the posterior distribution of Z+.\nAs a result, unlike\nFor a matrix with K columns and K+ non-zero\ncolumns, we denote its equivalence class with a rep-\nresentative matrix\u00afZ by [\u00afZ]K\ntrices that can be mapped to the same equivalence\nclass [\u00afZ]K\nK\nthe joint distribution over [\u00afZ]K\nK+. The number of the ma-\nK+is simply C\nK+\n=\nK!\nK+!(K\u2212K+)!. Considering\nK+and \u03c0, we have\np([\u00afZ]K\nK+,\u03c0) =C\nK+\nKp(\u00afZ,\u03c0) = C\n?\nK+\nKp(Z+,\u03c0+)p(Z0,\u03c00)\n?\n=C\nK+\nK\nk\u2264K+\np(z:,k,\u03c0k)\nk>K+\np(z:,k= 0,\u03c0k) (5)\nNote that in the above equation we partition \u03c0 into\n\u03c0+ and \u03c00 according to the partition of\u00af(Z). It is easy\nto derive that as K \u2192 \u221e, although p(Z) converges to 0\nfor any particular Z \u2208 [\u00afZ]\u221e\nWe also divide the rows of A into A+ and A0, such\nthat ZA = Z+A+. Now the task of learning IMFs is to\nfind the posterior distribution of [\u00afZ]\u221e\nthe exact posterior distributions of [\u00afZ]\u221e\nK+, p([\u00afZ]\u221e\nK+) does not.\nK+, A and \u03c0. Since\nK+, \u03c0 and A are\ncomputationally intractable, we describe a variational\nmethod to approximate them in the next section.\n2.3Variational inference for IMF\nFirst, let us define the notation. We use \u02dc zi,: to denote\nthe ithrow of Z+ and use \u02dc a:,j to denote the jthcolumn\nof A+.\nWe choose a factorized distribution to approximate the\nposterior distributions of\u00afZ, A and \u03c0:\nq(\u00afZ,A,\u03c0) =?\n\u221e\n?\nk=1\nq(\u03c0k)\n?\ni\nq(zik)?\nD\n?\nj=1\nq(a:,j)\n=q(\u03c0+)q(Z+)q(A+)q(Z0,\u03c00)q(A0)\n\u221d\n?\nk\u2264K+\n?q(\u03c0k)\n?\ni\nq(zik)??\nD\n?\nj=1\nq(\u02dc a:,j)?q(Z0,\u03c00)q(A0)\nUsing Jensen\u2019s inequality, we immediately obtain\nlnp(X,K+)\n?\n\u2265\n?\n\u00af Z\nq(\u00afZ,\u03c0,A+,A0)ln\np(X,[\u00afZ]\u221e\nK+,A,\u03c0)\nq(\u00afZ,\u03c0,A)\ndAd\u03c0\n= lim\nK\u2192\u221e\n?\nZ+\n?\nq(Z+,A+,\u03c0+)lnC\nK+\nK\np(X,Z+,A+,\u03c0+)\nq(Z+,A+,\u03c0+)\ndA+d\u03c0+\n?L(q;K+) (6)\nThe first equation holds since p(X,[\u00afZ]\u221e\nKp(X,Z+,A+,\u03c0+)p(Z0,\u03c00)p(A0) and we set\nq(Z0,\u03c00) = p(Z0,\u03c00) and q(A0) = p(A0). We denote the\nabove lower bound by L(q;K+) to emphasize that it\ndepends on the value of K+.\nK+,A,\u03c0) =\nlimK\u2192\u221eC\nK+\nThe details for calculating the lower bound L(q;K+)\nare shown in the appendix. Note that if we apply a\nvariational lower bound without using the equivalence\nclasses on Z, the lower bound becomes negatively in-\nfinite when K \u2192 \u221e since p(Z+,\u03c0+) \u2192 0.\nis defined to contain only non-zero columns but our\nfactorized q(zik) does not impose this constraint, the\nabove inequality holds only approximately. Since the\ndataset that we apply IMF to has at least dozens of, if\nnot hundreds or thousands (or more) of, data points,\nq(z:,k= 0) =?N\napproximation to the exact lower bound. This is con-\nfirmed by our empirical results (e.g., see in Figure 3\nwhere N=50).\nSince Z+\ni=1q(zik= 0) is a very small value. There-\nfore, the approximate lower bound is a very accurate\nMaximizing the lower bound L(q;K+), we obtain the\nfollowing iterative updates for k \u2264 K+:\nq(\u03c0k) = Beta(\u02c6 \u03b1k,\u02c6\u03b2k)\n(7)\nq(zik= 1) =\n\u03bbik\n\u03bbik+ 1\n(8)\nq(\u02dc a:,j) = N(\u02dc a:,j|mj,Vj)\n(9)"},{"page":4,"text":"Figure 1: Illustration of the synthetic image data. The\nfirst row shows four latent images. The second row\nare examples of the observed images that are random\ncombinations of the latent images with Gaussian noise\n(variance =0.5).\nwhere\n\u02c6 \u03b1k=\n?\ni\nq(zik= 1)\n\u02c6\u03b2k= N \u2212 \u02c6 \u03b1k+ 1\n?\nVj =(V0\nj)\u22121+\n1\n\u03c32\nx\n?\ni\nEq(Z+)[\u02dc zT\ni,:\u02dc zi,:]\n?\u22121\n(10)\nmj = Vj\n?(V0\nj)\u22121m0\nj+\n1\n\u03c32\nx\n?\ni\nxijEq(Z+)[\u02dc zi,:]?\n(11)\n\u03bbik= exp?\u03a8(\u02c6 \u03b1k) \u2212 \u03a8(\u02c6\u03b2k)\n\u2212\n2\u03bb2\nx\nj\n1\n?\n?2\n?\nr?=k\n(q(zir= 1)\u039bj\nrk) + \u039bj\nkk\u2212 2xijmj(k)??\nwhere \u039bj= Vj+ mjmT\nand \u03a8(\u00b7) denotes the digamma function. For all k > K+,\nq(z:,k= 0,\u03c0k= 0) = 1.\nj, mj(k) is the kthelement of mj\nNow we address the issue of how to K+.\nend, we maximize the lower bound L(q;K+) over K+.\nSince L(q;K+) approximates the marginal likelihood\np(X|K+) \u221d p(K+|X)p(X), the maximization over K+ is\njustified by Bayesian evidence maximization and does\nnot lead to overfitting. In practice, the algorithm is\ninitialized with K+ = 1. When the updates converge\nfor the current K+, we increase K+ by one and use\nthe current approximate posteriors to initialize the\nnext iterations. We stop increasing K+ when the ap-\nproximate marginal model likelihood (i.e., evidence)\nL(q;K+) stops to increase with a bigger K+.\nTo this\nTo illustrate how evidence maximization is used to\nchoose K+, we apply infinite matrix factorization on\nthe synthetic data that was used by Griffiths and\nGhahramani (2005). First, we define four latent im-\nages (each image corresponds a row of A and each\nelement of the latent image is either 1 or 0, as shown\nin the first row of figure 1) and generate a 50 by 4\nloading matrix Z by randomly sampling its elements\n123\nNumber of Bases\n4567\n\u2212500\n\u2212400\n\u2212300\n\u2212200\n\u2212100\n0\nL(q;K+)\nFigure 2: The approximate marginal model likelihood\nL(q;K+) on image data. By maximizing L(q;K+), the\nvariational infinite matrix factorization discovers the\ntrue number of latent images, which is 4.\nwith p(zik = 0.5). Then we combine the latent images\nA based on Z and add the Gaussian noise with mean\n0 and variance 0.5 to generate 50 observed image data\nX (see the second row of figure 1). We plot the vari-\national lower bound L(q;K+) for different values of K+\nin figure 2. The maximal value of L(q;K+) indicates\nK+ = 4, which matches the true number of the latent\nimages. In other words, infinite matrix factorization\nfinds the true number of the latent images.\n2.4Learning variance parameters \u03c3X and \u03c3A\nIn the previous section, we assume \u03c3X (the variance\nof the observation noise in (4)) is known and we pre-\ndefine the hyperparameter, \u03c3A, in the prior distribu-\ntion (1). Now we extend our model to estimate \u03c3X and\n\u03c3A from data. Specifically, we assign inverse-Gamma\nprior distributions over \u03c3X and \u03c3A to represent the\nuncertainty about them and approximate their poste-\nrior distributions via variational inference. Using the\ninverse-Gamma priors is equivalent to using Gamma\npriors on the reciprocals of \u03c3X and \u03c3A, \u03b7X ? 1\/\u03c32\n\u03b7A? 1\/\u03c32\nXand\nA:\np(\u03b7X) = Gamma(\u03b7X|a0\nX,b0\nX)p(\u03b7A) = Gamma(\u03b7A|a0\nA,b0\nA)\nwhere Gamma(\u00b7) is a Gamma distribution and a0\na0\nmative.\nX, a0\nX,\nX, and a0\nXare set to make the hyper-priors noninfor-\nGiven these priors, we obtain the following variational\nupdates for q(\u03b7X) and q(\u03b7A):\nq(\u03b7X) = Gamma(\u03b7X|\u02c6 aX,\u02c6bX)q(\u03b7A) = Gamma(\u03b7A|\u02c6 aA,\u02c6bA)"},{"page":5,"text":"where\n\u02c6 aX = a0\nX+ND\n2\n?\n\u02c6bX = b0\nX+1\n2\nij\n?x2\nij\u2212 2xijEq[\u02dc zi,:]Eq[\u02dc a:,j]\n+ Eq[\u02dc zi,:]Eq[\u02dc a:,j\u02dc aT\n:,j]Eq[\u02dc zT\ni,:]?\n\u02c6 aA= a0\nA+KD\n2\n?\n\u02c6bA= b0\nA+1\n2\nkj\nEq[\u02dc a2\nkj]\nThe expectations of \u03b7X and \u03b7A are Eq[\u03b7X] = \u02c6 aX\/\u02c6bX and\nEq[\u03b7A] = \u02c6 aA\/\u02c6bA. For the expanded model, we change\nthe variational updates (10) and (11) by replacing 1\/\u03c3X\nand 1\/\u03c3A with Eq[\u03b7X] and Eq[\u03b7A]. We also modify the\nvariational lower bound L(q;K+) accordingly.\n3 Infinite nonnegative matrix\nfactorization\nFor certain applications such as image decomposition,\nimposing nonnegativity constraints to the factorized\nmatrices leads to clearer model interpretation and im-\nproved predictive power (Lee and Seung, 2001). To\nincrease the utility of Bayesian matrix factorization,\nwe extend the infinite matrix factorization model by\nimposing nonnegative constraints on A. We call this\nnew model Infinite Nonnegative Matrix Factorization\n(INMF).\nFor IMFs, we assign a factorized Gaussian prior over\nA. For INMFs, we change this prior to a factorized\ntruncated Gaussian distribution:\np(A) \u221d\n?\nk,j\nN(akj|0,\u03c32\nA)I(akj \u2265 0)\n(12)\nwhere I(\u00b7) is an indicator function.\nGaussian prior not only regularizes A to prevent over-\nfitting as an L2-Regularizer but also effectively im-\nposes nonnegative constraints on elements of A.\nThis truncated\nGiven the truncated Gaussian prior over A, we can-\nnot apply variational methods directly since the lower\nbound (6) becomes negatively infinite.\nthis issue, we use the Power-Expectation Propagation\n(Power-EP) framework (Minka, 2004) to approximate\nthe exact posterior.\nTo address\n3.1Power-EP inference for INMF\nFor Power-EP, we choose the same form for the ap-\nproximate posteriors q as before.Similar to the IMF\ncase, we match q(A0)q(Z0,\u03c00) to the exact distri-\nbutions.Therefore, we only need to approximate\nq(Z+)q(\u03c0+)q(A+) based on p(Z+,A+,\u03c0+|X). Now for\nq(A+), we let\nq(A+) \u221d \u02dc p0(A+)\u02dc pX(A+)\n(13)\nwhere \u02dc pX(A+) and \u02dc p0(A+) are the Gaussian messages\nfrom the likelihood p(X|Z+,A+,\u03c0+) and the prior p(A+)\nto the variable A+, respectively. Since both messages\nhave the form of Gaussians, q(A+) is a Gaussian dis-\ntribution.\nPower-EP (Minka, 2004) generalizes expectation prop-\nagation and variational inference (in particular, varia-\ntional message passing (Winn and Bishop, 2004)) us-\ning a flexible \u03b1\u2212divergence. This divergence includes\nKL(p?q) and KL(q?p) as special cases. Using Power-\nEP, we have three steps for processing a factor in the\njoint distribution of the model: i) in the deletion step\nwe compute the partial posterior\/belief after removing\nthe message from this factor; ii) in the projection step\nwe minimize an \u03b1-divergence (e.g., KL(p?q) or KL(q?P))\nto obtain the new posterior q; and iii) in the message-\nupdate step, we update the message to be the ratio of\nthe new posterior q and the partial belief computed in\nthe deletion step. We then iteratively process all the\nfactors in the joint distribution with these three steps.\nFor INMFs, we minimize KL(p?q) when processing the\nprior factor p(A+), and minimize KL(q?p) when pro-\ncessing the the likelihood and the other priors.\nIn the deletion step, to process the likelihood and the\npriors on Z+ and \u03c0+, we compute the partial belief\nq\\X(A+) \u221d q(A+)\/\u02dc px(A+). Because of (13), we have\nq\\X(A+) \u221d \u02dc p0(A+)\n(14)\nSince q(Z+) and q(\u03c0+) are totally defined by the mes-\nsages from the likelihood and the prior on Z, the partial\nbeliefs q\\X(Z+) and q\\X(\u03c0+) are 1 after removing these\nmessages.\nThen in the projection step, we minimize the exclusive\nKL divergence over the new approximate posteriors:\nKL(q(Z+)q(\u03c0+)q(A+)?p(X|Z+,A+,\u03c0+)p(Z+,\u03c0+)q\\X(A+)).\nBy replacing p(A+) with its approximation q\\X(A+) in\nthe variational updates described in Section 2, we can\nefficiently minimize the above KL divergence.\nTo update the message update, it normally requires\nthe computation of the messages from the likeli-\nhood and the priors on Z+ and \u03c0+ to the variables\n(A,Z+,&\u03c0+. But since here we do not need the mes-\nsages explicitly in the deletion step to obtain all the\npartial beliefs, we can save the computation of these\nmessages.\nWhen processing the truncated Gaussian prior p(A) =\n?p(akj), we only update q(A+) since this prior does"},{"page":6,"text":"not involve Z+ and \u03c0+. The deletion step computes\nthe partial belief\nq\\kj(0)(\u02dc a:,j) = N(\u02dc a:,j|m\\kj(0),V\\kj(0)) \u221d q(\u02dc a:,j)\/\u02dc p0(akj)\nwhere \u02dc p0(akj \u221d N(mkj(0),Vkj(0)) (i.e., this message is a\nK+\u00d7 1 dimensional Gaussian from akj to \u02dc a:,j) and\nV\\kj(0)= (V\u22121\nm\\kj(0)= V\\kj(0)(V\u22121\nj\n\u2212 Vkj(0))\u22121\nj mj\u2212 V\u22121\n(15)\n(16)\nkj(0)mkj(0))\nThe projection step gives the new posterior\nqnew(\u02dc a:,j) \u221d N(\u02dc aj|mnew\nj\n,Vnew\nj\n),\nby minimizing the inclusive\nq\\kj(0)(\u02dc a:,j)p(akj). The following moment matching\nequations solve the minimization problem:\nKL(\u02c6 p?q) where\n\u02c6 p\n\u221d\nmnew\nj\n= mw+ \u03b1Vwek\n(17)\nVnew\nj\n= Vw\u2212\u03b1mnew\nj\n(k)\nvw(k,k)\nVwekeT\nkVT\nw\n(18)\nwhere\nVw= ([V\\kj(0)]\u22121+\n1\n\u03c32\nA\nekeT\nk)\u22121\nmw= Vw([V\\kj(0)]\u22121m\\kj(0))\n1\nvw(k,k)1\/2\n\u03b1 =\nN(vw(k,k)\u22121\/2mw(k)|0,1)\n?vw(k,k)\u22121\/2mw(k)\n\u2212\u221e\nN(z|0,1)dz\nTo update the message, we take the ratio between\nqnew(\u02dc a:,j) and q\\kj(0)(\u02dc a:,j) and obtain the new message\n\u02dc p0(akj with\nVnew\nkj(0)=?(Vnew\nmnew\nj\n)\u22121\u2212 (V\\kj(0))\u22121?\u22121\nj\n)\u22121mnew\n(19)\n?(20)\nkj(0)= Vnew\nkj(0)\n?(Vnew\nj\n\u2212 (V\\kj(0))\u22121m\\kj(0)\nNote that the deletion step and message update can\nbe further simplified by low-rank matrix operations.\nTo learn K+ for the INMF model, we maximize the\napproximate marginal likelihood L(q;K+). We also ex-\ntend the model to estimate the posteriors of \u03c3X and\n\u03c3A, just like what we have described in Section 2.4.\nThe IMF and INMF are summarized in Algorithm 1.\n4 Experiments\nTo evaluate the proposed methods, we test them on\nthree tasks: synthetic image decomposition, collabo-\nrative filtering, and role mining.\n4.1Image Decomposition\nFirst, we compare IMFs and INMFs with Gibbs sam-\npling (Wood and Griffiths, 2007), Particle Filtering\n(PF) (Wood and Griffiths, 2007), Variational Finite\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n8\n8.5\n9\n9.5\n10\n10.5\n11\n11.5\n12\nTime\/s\nError\n \n \nGibbs\nPF\nIMF\nINMF\nVFMF\nTVIMF\nFigure 3: Performance comparison on image data.\nMatrix Factorization (VFMF), and the variational infi-\nnite matrix factorization (TVIMF) (Doshi-Velez et al.,\n2009) on a synthetic dataset.\nWe use a similar data generation process as described\nin Section 2.3 and by (Griffiths and Ghahramani,\n2005). The only difference is that now we randomly\ngenerate each of the 6-by-6 latent images that contain\n8 elements of 1\u2019s (the other elements are 0\u2019s).\nFigure 3 shows the performance of all these six algo-\nrithms. For the evaluation, we interpret each row of\nthe latent matrix A as a base (i.e.,ak,:). The errors\nin the figure are measured by the mean square differ-\nence between each latent base, ak,:, and the estimated\nbase that is closest to it. As shown in Figure 3, our\nnew methods converge much faster than the alterna-\ntive methods, demonstrating their high computational\nefficiency. The results of particle filtering (for which\nwe vary the number of particles from 100 to 400, 1000,\n2500, 5000) fluctuate heavily. Sometimes the particle\nfiltering method even ends up with over 50 bases as\nthe final estimation, while the true number of bases\nAlgorithm 1 IMF and INMF\n1. Initialize K+= 1.\n2. Initialize all approximate factors.\n3. For each variational inference iteration:\na) Loop over k = 1,...,K+:\nUpdate q(\u03c0k) via (7)\nb) Loop over i = 1,...,N,k = 1,...,K+:\nUpdate q(zik) via (8)\nc) Loop over j = 1,...,D:\nFor IMF: update q(aj) via (9)\nFor INMF:\nLoop over j = 1,...,D,k = 1,...,K+:\nUpdate q(aj) via (15) to (20)\nd) Compute the evidence L(q;K+)\ne) Increase K+ if L(q;K+) increases."},{"page":7,"text":"2468101214\n0.169\n0.17\n0.171\n0.172\n0.173\n0.174\n0.175\n0.176\n0.177\nK\nNMAE\n \n \nNMF\nSVD\nIMF\nINMF\nFigure 4: Prediction errors for collaborative filter-\ning. The plot shows Normalized Mean Absolute Er-\nror (NMAE) of four models on the Jester data. The\nresults are averaged over 10 experiments.\nis only 4.\nlevel T has to be sufficiently large (we varied it from\n4 to 10) to achieve low estimate error. This makes\nthis variational approach less efficient compared to our\nmethod. The performance of TVIMF and VFMF are\nnot stable in practice. Therefore, we run them multi-\nple times and pick the best solution from these runs.\nBy contrast, the IMF and INMF work stably and the\nquality of the approximate lower bound is very good:\nIMF and INMF never over-estimate K+ in our ex-\nperiments. IMF achieves the lowest prediction error\namong all the approximate inference methods (except\nINMF) for IBP models. Furthermore, the INMF leads\nto highest prediction accuracy among all the methods,\ndemonstrating the effectiveness of the nonnegativity\nconstraints on latent matrices.\nFor TVIMF and VFMF, the truncation\n4.2 Collaborative Filtering\nWe apply the new methods to a collaborative filtering\ntask to test how accurate they are when predicting the\npreference of a specific user given his previous ratings.\nWe use a subset of the Jester dataset Goldberg et al.\n(2001). The dataset contains 100 jokes with 73421 user\nratings. The density (or fraction of the rating matrix\nthat is filled) of the Jester set is about 0.5. We select\n1000 users and for each selected user, 30 ratings are\nheld out for testing. We select 1000 users and for each\nselected user, 30 ratings are held out for testing. The\nexperiment is repeated for 10 times, each time with a\ndifferent user subset.\nIMF automatically discovers 7.8 latent bases averaged\nover the 10 experiments. INMF gives 7.6 latent bases\non average. For comparison, VFMF, TVIMF, classical\nnonnegative matrix factorization (NMF), and singular\nvalue decomposition (SVD) are applied to the same\ntraining and testing data. We did not apply Gibbs\nsamplers and particle filtes because of their limited\nscalability for large datasets. NMF and SVD require\na predefined number of bases. We vary the number of\nbases from 2 to 15 and choose this number by optimiz-\ning their performance. The truncation level of VFMF\nand TVIMF is 30. Empirically, a smaller truncation\nlevel leads to much worse prediction performance.\nThe results of NMF, SVD, IMF and INMF are shown\nin figure 4. We do not plot the results of VFMF and\nTVIMF since their results are out of the boundary of\nthis figure. VFMF achieves 0.1767 with 30 effective\nbases, and TVIMF achieves 0.1761 with 30 effective\nbases. IMF and INMF not only learn a more compact\nrepresentation of the data, but also give more accurate\npredictions than the other methods.\n4.3 Role Mining\nFinally, we apply our new methods to the role min-\ning problem. Role mining is an active research area in\ninformation security that aims at discovering a set of\nroles for role-based access control (RBAC) from an ex-\nisting user-permission assignment relation. In RBAC,\ninstead of assigning permissions directly to users, an\nadministrator assigns permissions to roles and autho-\nrizes users to roles. To simplify administration, it is\ndesirable to keep the number of roles small.\nWe use two datasets, \u201cDomino\u201d and \u201cFirewall\u201d from\nresearchers at HP Labs. The \u201cDomino\u201d dataset, from\na Lotus Domino server, is a 79x231 binary matrix\nwhere each row is a user and each column is a per-\nmission for a given level of access to files, databases,\nand custom applications. The \u201cFirewall\u201d dataset is a\n709x365 binary matrix from a Cisco firewall used to\nprovide external users access to internal resources. In\nour experiment, we first transpose the original matri-\nces, such that Z matrix becomes a meaningful binary\nrole matrix, which assigns permissions to roles.\nWe randomly hold out 20% elements of each matrix for\ntesting. Since the matrix is extremely sparse, we re-\nport the Area Under the Curve (AUC) of IMF, INMF,\nVFMF, TVIMF, NMF and SVD in Table 1. The trun-\ncation level T of VFMF and TVIMF is 20. NMF and\nSVD are chosent to have the optimal number of bases\nthat gives to their best AUC value.\nWe find that TFMF, TVIMF, IMF and SVD have\ninferior performance on this dataset.\nthe AUC comparison shown in Table 1 suggests that\nINMF outperforms all the other algorithms on both\ndatasets. For better illustration, we plot the ROC\ncurve of \u201cdomino\u201d dataset. We observe that INMF\nachieves the distinguishable True-Positive ratio when\nthe False-Positive ratio is small, which indicates the\ndesirable performance on the top of the list.\nBy contrast,"},{"page":8,"text":"0 0.20.4 0.60.81\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nFALSE POSITIVE RATIO\nTRUE POSITIVE RATIO\n \n \nVFMF\nTVIMF\nSVD\nNMF\nIMF\nINMF\nFigure 5: The ROC curve for the \u201cDomino\u201d dataset.\nAlgorithm\nDominoFirewall\nK\n20\n20\n5\n4\n8\n9\nAUC\n0.9339\n0.9382\n0.9518\n0.9755\n0.9400\n0.9869\nK\n20\n20\n10\n6\n7\n9\nAUC\n0.9748\n0.9732\n0.9745\n0.9801\n0.9823\n0.9853\nVFMF\nTVIMF\nSVD\nNMF\nIMF\nINMF\nTable 1: AUC of VFMF, TVIMF, SVD, NMF, IMF\nand INMF on \u201cDomino\u201d and \u201cFirewall\u201d datasets.\n5 Conclusions\nUsing the new equivalence classes of infinite binary\nmatrices, we have developed two novel approximate\ninference algorithms for infinite matrix factorization\nand infinite nonnegative matrix factorization, respec-\ntively. These two methods are unified in the power-\nEP framework. They learn the model dimensional-\nity automatically from the data, as well as all the\nmodel hyper-parameters.\national method (Doshi-Velez et al., 2009), the new\nmethods do not use a truncated stick-breaking rep-\nresentation (so no truncation levels to be set) and\nachieve faster convergence and lower prediction error\nrates compared to alternative inference methods.\nUnlike the previous vari-\nAcknowledgements\nThe work was supported by NSF IIS-0916443 and NSF\nECCS-0941533.\nReferences\nGriffiths TL, Ghahramani Z (2005) Infinite latent fea-\nture models and the Indian buffet process. Technical\nReport 2005-001, Gatsby Computational Neuroscience\nUnit, University College London.\nDoshi-Velez F, Miller KT, Gael JV, Teh YW (2009) Vari-\national inference for the Indian buffet process. In Pro-\nceedings of AISTATS.\nLee DD, Seung HS (2001) Algorithms for non-negative ma-\ntrix factorization. In Advances in Nerual Information\nProcessing 13. MIT Press.\nDevarajan K (2008) Nonnegative matrix factorization: An\nanalytical and interpretive tool in computational biol-\nogy. PLoS Computational Biology 4: 1\u201312.\nMinka T (2004) Power EP. Technical Report 2004-149,\nMicrosoft Research Ltd.\nTeh YW, Gorur D, Ghahramani Z (2007) Stick-breaking\nconstruction for the Indian buffet process. In Proceed-\nings of the International Conference on Artificial Intel-\nligence and Statistics. volume 11.\nWinn J, Bishop CM (2004) Variational message passing.\nJournal of Machine Learning Research 5.\nWood F, Griffiths TL (2007) Particle filtering for nonpara-\nmetric Bayesian matrix factorization. In Advances in\nNerual Information Processing 19. MIT Press.\nGoldberg K, Roeder T, Gupta D, Perkins C (2001) Eigen-\ntaste: A constant time collaborative filtering algorithm.\nInformation Retrieval 4: 133\u2013151.\nA Lower bound of marginal likelihood\nL(q;K+)\nThe lower bound L(q;K+) is given here.\nL(q;K+) = T1+ T2+ T3\u2212 H1\u2212 H2\u2212 H3\nwhere\nT1 =\nN\n?\ni=1\nD\n?\n1\n2\u03c32\nj=1\n?\u22121\nK+\n?\n2ln(2\u03c0\u03c32\nx) +xij\n2\u03c32\nx\nK+\n?\nk=1\nq(zik= 1)mj(k)\n\u2212\nx\nr=1\nK+\n?\nk=1\nEq(Z+)(zirzik)\u039bj\nrk\u2212\n1\n2\u03c32\nxx2\nij\n?\nT2 =\nD\n?\n?\n+ (\nj=1\n?\u2212K+\n?(\n?\n?\u22121\n?\n+ q(zik= 0)lnq(zik= 0)?\nq(\u03c0k)lnq(\u03c0k)d\u03c0k\n2\nln(2\u03c0\u03c32\nA) \u2212\n1\n2\u03c32\nA\nK+\n?\nk=1\n\u039bj\nkk\n?\nT3 =\nk\u2264K+\n?\nq(zik= 1) \u2212 1)?\u03a8(\u02c6 \u03b1k) \u2212 \u03a8(\u02c6 \u03b1k+\u02c6\u03b2k)?+ ln\u03b1\n2ln|Vj| \u2212K+\n2\n??\ni\nq(zik= 0))?\u03a8(\u02c6\u03b2k) \u2212 \u03a8(\u02c6 \u03b1k+\u02c6\u03b2k)?\ni\nk\n?\nH1 =\nD\n?\nj=1\n(1 + ln(2\u03c0))?\nH2 =\nk\u2264K+\ni\nq(zik= 1)lnq(zik= 1)\nH3 =\n?\n=\nk\u2264K+\n?\n?\nk\u2264K+\n+ (\u02c6\u03b2k\u2212 1){\u03c8(\u02c6\u03b2k) \u2212 \u03c8(\u02c6 \u03b1k+\u02c6\u03b2k)}\n\u2212 ln\u0393(\u02c6 \u03b1k+\u02c6\u03b2k) + ln\u0393(\u02c6 \u03b1k) + ln\u0393(\u02c6\u03b2k)?\nwhere \u039bj= Vj + mjmT\nthe covariance matrix of q(\u02dc aj) respectively, and mj(k) is\nthe k-th element of mj.\n?(\u02c6 \u03b1k\u2212 1){\u03c8(\u02c6 \u03b1k) \u2212 \u03c8(\u02c6 \u03b1k+\u02c6\u03b2k)}\nj, mj and Vj are the mean and"}],"widgetId":"rgw31_56ab9f8366cb1"},"id":"rgw31_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=220320618&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw32_56ab9f8366cb1"},"id":"rgw32_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=220320618&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":220320618,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/220320618_Nonparametric_Bayesian_Matrix_Factorization_by_Power-EP","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab9f8366cb1"},"id":"rgw2_56ab9f8366cb1","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":220320618},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=220320618&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab9f8366cb1"},"id":"rgw1_56ab9f8366cb1","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"Wimy1Ly7rO\/tLmOERQ5fhFBLZbCvWcgwCl1FXe16iYXaZLKb9BF222yzwOH31qLdsl3uKp7HeuTZETcklzaoyAxFGwFnvfo2AyUjl+4+XM7g6V9qzeLNLvI2HomRo+VXR3zOrLGSG7sNtQc1fcEmEGXxUcT+pIoihaW8hkbyNQeT5z77Tj1om1aKfQFvDvuFtVL7D1Hw+V+5ZL9Drg8qbt58Meu3pEPLIFO1L7r\/xWbmEOCBIQWaCuY\/ovx5s3vA7cCiGQMT1+JsbQNMThRIV8iD3X43vrNjZzwEmqKzh2g=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/220320618_Nonparametric_Bayesian_Matrix_Factorization_by_Power-EP\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Nonparametric Bayesian Matrix Factorization by Power-EP.\" \/>\n<meta property=\"og:description\" content=\"Many real-world applications can be modeled by matrix factorization. By approximating an observed data matrix as the product of two latent matrices, matrix factorization can reveal hidden...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/220320618_Nonparametric_Bayesian_Matrix_Factorization_by_Power-EP\/links\/02a85a480cf2fb757aff7589\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/220320618_Nonparametric_Bayesian_Matrix_Factorization_by_Power-EP\" \/>\n<meta property=\"rg:id\" content=\"PB:220320618\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Nonparametric Bayesian Matrix Factorization by Power-EP.\" \/>\n<meta name=\"citation_author\" content=\"Nan Ding\" \/>\n<meta name=\"citation_author\" content=\"Yuan (Alan) Qi\" \/>\n<meta name=\"citation_author\" content=\"Rongjing Xiang\" \/>\n<meta name=\"citation_author\" content=\"Ian Molloy\" \/>\n<meta name=\"citation_author\" content=\"Ninghui Li\" \/>\n<meta name=\"citation_publication_date\" content=\"2010\/01\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"Journal of Machine Learning Research\" \/>\n<meta name=\"citation_issn\" content=\"1532-4435\" \/>\n<meta name=\"citation_volume\" content=\"9\" \/>\n<meta name=\"citation_firstpage\" content=\"169\" \/>\n<meta name=\"citation_lastpage\" content=\"176\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/220320618_Nonparametric_Bayesian_Matrix_Factorization_by_Power-EP\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/220320618_Nonparametric_Bayesian_Matrix_Factorization_by_Power-EP\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-e2eaded6-c60f-47fb-9a97-93410b1265fd","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":545,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw33_56ab9f8366cb1"},"id":"rgw33_56ab9f8366cb1","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-e2eaded6-c60f-47fb-9a97-93410b1265fd", "6af6942953a12f531c473cfbb868c69f005b475d");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-e2eaded6-c60f-47fb-9a97-93410b1265fd", "6af6942953a12f531c473cfbb868c69f005b475d");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw34_56ab9f8366cb1"},"id":"rgw34_56ab9f8366cb1","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/220320618_Nonparametric_Bayesian_Matrix_Factorization_by_Power-EP","requestToken":"YZY8vFngkqUM1Ole5PswgXbpchdlIcr+cBulpuqaSaSAcdj9ywVOQdq49QxnopsIkx14T8zDJeIPEycu42U3al+tGiU1ZhqRlp9RI6FrOwLiSdElSDPtezGKkNddQ\/gBajAbVGFSrILAklNDKWK2iBxVVV27oTmK97Z32Xb8i30qt88skJMkW+\/\/Gn9jvTcmaufFHy0Zj2UwKO\/FRLTHPEyguYunF3x711vtG9afpG7ieuqFWp9CRAiEdQuRlOxwZApKQ0Rv8n3xz4Suc2hUQ8EnWzz\/M2W7pfqMqfeQvaw=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=tMEnM0o9wDPXnmjXLkwyhinEwBtQ_u_ccFbbWhLg-dIHvzaZK6C2x0GzNhYRgHsA","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjIwMzIwNjE4X05vbnBhcmFtZXRyaWNfQmF5ZXNpYW5fTWF0cml4X0ZhY3Rvcml6YXRpb25fYnlfUG93ZXItRVA%3D","signupCallToAction":"Join for free","widgetId":"rgw36_56ab9f8366cb1"},"id":"rgw36_56ab9f8366cb1","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw35_56ab9f8366cb1"},"id":"rgw35_56ab9f8366cb1","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw37_56ab9f8366cb1"},"id":"rgw37_56ab9f8366cb1","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
