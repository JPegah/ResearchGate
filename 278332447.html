<!DOCTYPE html> <html lang="en" class="" id="rgw45_56ab9d4c9a90b"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="H1ospcsM8r6GK8dQEHJLhA3l1w6d2GqqRyxjCGGMzHgkyZl1c3qreF/rciH/qdPjR8JC4agbJoI6NNJKSKoK42MloRZx2CflskFuJGPQg5lKk1cwe+bjqp14DSYbOvjsd8Vy7hjXFLnVfbdPgk7+/uYqG699/eWns0Hmbe4VUqXiOvafmvHBH71MsDRwUJCS6JGcERLz7BMrfBJ/017oQ0vSv46Bz0HorgxOnT5hn786sIyzuHxrz1eCHjGPteTet9SOdyPv0sxkEHQeFjkGh8MXAQX0lMKFzCWKcUVJ6f4="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-45ef0063-07b8-43eb-af6c-14a706fcab84",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="MCMC for Variationally Sparse Gaussian Processes" />
<meta property="og:description" content="Gaussian process (GP) models form a core part of probabilistic machine
learning. Considerable research effort has been made into attacking three
issues with GP models: how to compute efficiently..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes/links/558a857008aee1fc9174ea84/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes" />
<meta property="rg:id" content="PB:278332447" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="MCMC for Variationally Sparse Gaussian Processes" />
<meta name="citation_author" content="James Hensman" />
<meta name="citation_author" content="Alexander G. de G. Matthews" />
<meta name="citation_author" content="Maurizio Filippone" />
<meta name="citation_author" content="Zoubin Ghahramani" />
<meta name="citation_publication_date" content="2015/06/12" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Maurizio_Filippone/publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes/links/558a857008aee1fc9174ea84.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>MCMC for Variationally Sparse Gaussian Processes (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: MCMC for Variationally Sparse Gaussian Processes on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab9d4c9a90b" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab9d4c9a90b" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab9d4c9a90b">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=MCMC%20for%20Variationally%20Sparse%20Gaussian%20Processes&rft.date=2015&rft.au=James%20Hensman%2CAlexander%20G.%20de%20G.%20Matthews%2CMaurizio%20Filippone%2CZoubin%20Ghahramani&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">MCMC for Variationally Sparse Gaussian Processes</h1> <meta itemprop="headline" content="MCMC for Variationally Sparse Gaussian Processes">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes/links/558a857008aee1fc9174ea84/smallpreview.png">  <div id="rgw7_56ab9d4c9a90b" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab9d4c9a90b" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/James_Hensman" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A277099177889798%401443077000994_m" title="James Hensman" alt="James Hensman" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">James Hensman</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw9_56ab9d4c9a90b" data-account-key="James_Hensman">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/James_Hensman"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A277099177889798%401443077000994_l" title="James Hensman" alt="James Hensman" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/James_Hensman" class="display-name">James Hensman</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Lancaster_University" title="Lancaster University">Lancaster University</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56ab9d4c9a90b"> <a href="researcher/2048369253_Alexander_G_de_G_Matthews" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Alexander G. de G. Matthews" alt="Alexander G. de G. Matthews" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Alexander G. de G. Matthews</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw11_56ab9d4c9a90b">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2048369253_Alexander_G_de_G_Matthews"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Alexander G. de G. Matthews" alt="Alexander G. de G. Matthews" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2048369253_Alexander_G_de_G_Matthews" class="display-name">Alexander G. de G. Matthews</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56ab9d4c9a90b" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Maurizio_Filippone" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A273666076311560%401442258485613_m" title="Maurizio Filippone" alt="Maurizio Filippone" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Maurizio Filippone</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw13_56ab9d4c9a90b" data-account-key="Maurizio_Filippone">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Maurizio_Filippone"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A273666076311560%401442258485613_l" title="Maurizio Filippone" alt="Maurizio Filippone" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Maurizio_Filippone" class="display-name">Maurizio Filippone</a>    </h5> <div class="truncate-single-line meta">   </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw14_56ab9d4c9a90b"> <a href="researcher/8159937_Zoubin_Ghahramani" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Zoubin Ghahramani</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw15_56ab9d4c9a90b">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/8159937_Zoubin_Ghahramani"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/8159937_Zoubin_Ghahramani" class="display-name">Zoubin Ghahramani</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">        <meta itemprop="datePublished" content="2015-06">  06/2015;               <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1506.04000" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw16_56ab9d4c9a90b" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>Gaussian process (GP) models form a core part of probabilistic machine<br />
learning. Considerable research effort has been made into attacking three<br />
issues with GP models: how to compute efficiently when the number of data is<br />
large; how to approximate the posterior when the likelihood is not Gaussian and<br />
how to estimate covariance function parameter posteriors. This paper<br />
simultaneously addresses these, using a variational approximation to the<br />
posterior which is sparse in support of the function but otherwise free-form.<br />
The result is a Hybrid Monte-Carlo sampling scheme which allows for a<br />
non-Gaussian approximation over the function values and covariance parameters<br />
simultaneously, with efficient computations based on inducing-point sparse GPs.<br />
Code to replicate each experiment in this paper will be available shortly.</div> </p>  </div>   </div>      <div class="action-container"> <div id="rgw17_56ab9d4c9a90b" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw31_56ab9d4c9a90b">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw37_56ab9d4c9a90b">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Maurizio_Filippone/publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes/links/558a857008aee1fc9174ea84.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Maurizio_Filippone">Maurizio Filippone</a>, <span class="js-publication-date"> Jun 24, 2015 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw39_56ab9d4c9a90b" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw40_56ab9d4c9a90b" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw41_56ab9d4c9a90b" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw42_56ab9d4c9a90b" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw43_56ab9d4c9a90b" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw44_56ab9d4c9a90b" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw38_56ab9d4c9a90b" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMaurizio_Filippone%2Fpublication%2F278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes%2Flinks%2F558a857008aee1fc9174ea84.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw30_56ab9d4c9a90b"  itemprop="articleBody">  <p>Page 1</p> <p>MCMC for Variationally Sparse Gaussian Processes<br />James Hensman<br />Department of Computer Science<br />University of Sheffield<br />Sheffield, UK<br />james.hensman@sheffield.ac.uk<br />Alexander G. de G. Matthews<br />Department of Engineering<br />University of Cambridge<br />Cambridge, UK<br />am554@cam.ac.uk<br />Maurizio Filippone<br />School of Computing Science<br />University of Glasgow<br />Glasgow, UK<br />maurizio.filippone@glasgow.ac.uk<br />Zoubin Ghahramani<br />Department of Engineering<br />University of Cambridge<br />Cambridge, UK<br />zoubin@eng.cam.ac.uk<br />Abstract<br />Gaussian process (GP) models form a core part of probabilistic machine learning. Con-<br />siderable research effort has been made into attacking three issues with GP models: how<br />to compute efficiently when the number of data is large; how to approximate the posterior<br />when the likelihood is not Gaussian and how to estimate covariance function parameter<br />posteriors. This paper simultaneously addresses these, using a variational approximation<br />to the posterior which is sparse in support of the function but otherwise free-form. The<br />result is a Hybrid Monte-Carlo sampling scheme which allows for a non-Gaussian approx-<br />imation over the function values and covariance parameters simultaneously, with efficient<br />computations based on inducing-point sparse GPs. Code to replicate each experiment in<br />this paper will be available shortly.<br />1. Introduction<br />Gaussian process models are attractive for machine learning because of their flexible non-<br />parametric nature. By combining a GP prior with different likelihoods, a multitude of<br />machine learning tasks can be tackled in a probabilistic fashion [1]. There are three things<br />to consider when using a GP model: approximation of the posterior function (especially if<br />the likelihood is non-Gaussian), computation, storage and inversion of the covariance ma-<br />trix, which scales poorly in the number of data; and estimation (or marginalization) of the<br />covariance function parameters. A multitude of approximation schemes have been proposed<br />for efficient computation when the number of data is large. Early strategies were based on<br />retaining a sub-set of the data [2, 3]. Snelson and Ghahramani [4] introduced an inducing<br />point approach, where the model is augmented with additional variables, and Titsias [5]<br />arXiv:1506.04000v1  [stat.ML]  12 Jun 2015</p>  <p>Page 2</p> <p>Hensman et al.<br />Table 1: Existing variational approaches<br />Referencep(y|f)<br />probit/logit<br />Gaussian<br />softmax<br />any factorized<br />probit<br />any factorized<br />SparsePosteriorHyperparam.<br />Williams &amp; Barber[22] [also 15, 18]<br />Titsias [5]<br />Chai [19]<br />Nguyen and Bonilla [1]<br />Hensman et al. [21]<br />This work<br />?<br />?<br />?<br />?<br />?<br />?<br />Gaussian (assumed)<br />Gaussian (optimal)<br />Gaussian (assumed)<br />Mixture of Gaussians<br />Gaussian (assumed)<br />free-form<br />point estimate<br />point estimate<br />point estimate<br />point estimate<br />point estimate<br />free-form<br />used these ideas in a variational approach. Other authors have introduced approximations<br />based on the spectrum of the GP [6, 7], or which exploit specific structures within the<br />covariance matrix [8, 9], or by making unbiased stochastic estimates of key computations<br />[10]. In this work, we extend the variational inducing point framework, which we prefer for<br />general applicability (no specific requirements are made of the data or covariance function),<br />and because the variational inducing point approach can be shown to minimize the KL<br />divergence to the posterior process [11].<br />To approximate the posterior function and covariance parameters, Markov chain Monte-<br />Carlo (MCMC) approaches provide asymptotically exact approximations.<br />Adams [12] and Filippone et al. [13] examine schemes which iteratively sample the func-<br />tion values and covariance parameters. Such sampling schemes require computation and<br />inversion of the full covariance matrix at each iteration, making them unsuitable for large<br />problems. Computation may be reduced somewhat by considering variational methods,<br />approximating the posterior using some fixed family of distributions [14, 15, 16, 17, 1, 18],<br />though many covariance matrix inversions are generally required. Recent works [19, 20, 21]<br />have proposed inducing point schemes which can reduce the computation required sub-<br />stantially, though the posterior is assumed Gaussian and the covariance parameters are<br />estimated by (approximate) maximum likelihood. Table 1 places our work in the context<br />of existing variational methods for GPs.<br />This paper presents a general inference scheme, with the only concession to approx-<br />imation being the variational inducing point assumption.<br />permitted through MCMC, with the computational benefits of the inducing point frame-<br />work. The scheme jointly samples the inducing-point representation of the function with<br />the covariance function parameters; with sufficient inducing points our method approaches<br />full Bayesian inference over GP values and the covariance parameters. We show empirically<br />that the number of required inducing points is substantially smaller than the dataset size<br />for several real problems.<br />Murray and<br />Non-Gaussian posteriors are<br />2. Stochastic process posteriors<br />The model is set up as follows. We are presented with some data inputs X = {xn}N<br />and responses y = {yn}N<br />mean and covariance function k(x,x?) with (hyper-) parameters θ. Consistency of the<br />GP means that only those points with data are considered: the latent vector f represents<br />the values of the function at the observed points f = {f(xn)}N<br />n=1<br />n=1. A latent function is assumed drawn from a GP with zero<br />n=1, and has conditional<br />2</p>  <p>Page 3</p> <p>MCMC for Variationally Sparse Gaussian Processes<br />distribution p(f |X,θ) = N(f |0,Kff), where Kff is a matrix composed of evaluating<br />the covariance function at all pairs of points in X. The data likelihood depends on the<br />latent function values: p(y|f). To make a prediction for latent function value test points<br />f?= {f(x?)}x?∈X?, the posterior function values and parameters are integrated:<br />? ?<br />In order to make use of the computational savings offered by the variational inducing point<br />framework [5], we introduce additional input points to the function Z and collect the re-<br />sponses of the function at that point into the vector u = {um= f(zm)}M<br />variational posterior q(u,θ), new points are predicted similarly to the exact solution<br />? ?<br />This makes clear that the approximation is a stochastic process in the same fashion as the<br />true posterior: the length of the predictions vector f?is potentially unbounded, covering<br />the whole domain.<br />To obtain a variational objective, first consider the support of u under the true posterior,<br />and for f under the approximation. In the above, these points are subsumed into the<br />prediction vector f?: from here we shall be more explicit, letting f be the points of the<br />process at X, u be the points of the process at Z and f?be a large vector containing all<br />other points of interest1. All of the free parameters of the model are then f?,f,u,θ, and<br />using a variational framework, we aim to minimize the Kullback-Leibler divergence between<br />the approximate and true posteriors:<br />p(f?|y) =p(f?|f,θ)p(f,θ|y)dθdf .(1)<br />m=1. With some<br />q(f?) =p(f?|u,θ)q(u,θ)dθdu.(2)<br />K ? KL[q(f?,f,u,θ)||p(f?,f,u,θ|y)] =<br />−E<br />q(f?,f,u,θ)<br />?<br />logp(f?|u,f,θ)p(u|f,θ)p(f,θ|y)<br />p(f?|u,f,θ)p(f |u,θ)q(u,θ)<br />?<br />(3)<br />where the conditional distributions for f?have been expanded to make clear that they are<br />the same under the true and approximate posteriors, and X,Z and X?have been omitted<br />for clarity. Straight-forward identities simplify the expression,<br />?<br />= −Eq(f,u,θ)<br />q(u,θ)<br />K = −Eq(f,u,θ)<br />logp(u|f,θ)p(f |θ)p(θ)p(y|f)/p(y)<br />p(f |u,θ)q(u,θ)<br />logp(u|θ)p(θ)p(y|f)<br />?<br />??<br />+ logp(y),<br />(4)<br />resulting in the variational inducing-point objective investigated by Titsias [5], aside from<br />the inclusion of θ. This can be rearranged to give the following informative expression<br />K = KL<br />?<br />q(u,θ)||p(u|θ)p(θ)Ep(f |u,θ)[logp(y|f)]<br />C<br />?<br />− logC + logp(y). (5)<br />1. The vector f?here is considered finite but large enough to contain any point of interest for prediction.<br />The infinite case follows Matthews et al. [11], is omitted here for brevity, and results in the same solution.<br />3</p>  <p>Page 4</p> <p>Hensman et al.<br />Here C is an intractable constant which normalizes the distribution and is independent of<br />q. Minimizing the KL divergence on the right hand side reveals that the optimal variational<br />distribution is<br />log ˆ q(u,θ) = Ep(f |u,θ)[logp(y|f)] + logp(u|θ) + logp(θ) − logC.<br />For general likelihoods, since the optimal distribution does not take any particular form,<br />we intend to sample from it using MCMC. This is feasible using standard methods since it<br />is computable up to a constant, using O(NM2) computations. To sample effectively, the<br />following are proposed.<br />(6)<br />Whitening the prior<br />u, albeit with an interesting ‘likelihood’, we make use of an ancillary augmentation u = Rv,<br />with RR?= Kuu, v ∼ N(0,I). This results in the optimal variational distribution<br />log ˆ q(v,θ) = Ep(f |u=Rv)[logp(y|f)] + logp(v) + logp(θ) − logC<br />Previously [12, 13] this parameterization has been used with schemes which alternate be-<br />tween sampling the latent function values (represented by v or u) and the parameters θ.<br />Our scheme uses HMC across v and θ jointly, whose effectiveness is examined throughout<br />the experiment section.<br />Noting that the problem (6) appears similar to a standard GP for<br />(7)<br />Quadrature<br />tion across the data-function pairs, this results in N one-dimensional integrals. For Gaussian<br />or Poisson likelihood these integrals are tractable, otherwise they can be approximated by<br />Gauss-Hermite quadrature. Given the current sample v, the expectations are computed<br />w.r.t. p(fn|v,θ) = N(µn,γn), with:<br />µ = A?v; γ = diag(Kff− A?A); A = R−1Kuf; RR?= Kuu,<br />where the kernel matrices Kuf,Kuuare computed similarly to Kff, but over the pairs in<br />(X,Z),(Z,Z) respectively. From here, one can compute the expected likelihood and it is<br />subsequently straight-forward to compute derivatives in terms of Kuf,diag(Kff) and R.<br />The first term in (6) is the expected log-likelihood. In the case of factoriza-<br />(8)<br />Reverse mode differentiation of Cholesky<br />and Z we use reverse-mode differentiation (backpropagation) of the derivative through the<br />Cholesky matrix decomposition, transforming ∂ log ˆ q(v,θ)/∂R into ∂ log ˆ q(v,θ)/∂Kuu, and<br />then ∂ log ˆ q(v,θ)/∂θ. This is discussed by Smith [23], and results in a O(M3) operation;<br />an efficient Cython implementation is provided in the supplement.<br />To compute derivatives with respect to θ<br />3. Treatment of inducing point positions &amp; inference strategy<br />A natural question is, what strategy should be used to select the inducing points Z? In the<br />original inducing point formulation [4], the positions Z were treated as parameters to be<br />optimized. One could interpret them as parameters of the approximate prior covariance [24].<br />The variational formulation [5] treats them as parameters of the variational approximation,<br />thus protecting from over-fitting as they form part of the variational posterior. In this work,<br />since we propose a Bayesian treatment of the model, we question whether it is feasible to<br />treat Z in a Bayesian fashion.<br />4</p>  <p>Page 5</p> <p>MCMC for Variationally Sparse Gaussian Processes<br />Since u and Z are auxiliary parameters, the form of their distribution does not affect<br />the marginals of the model. The term p(u|Z) has been defined by the consistency with the<br />GP in order to preserve the posterior-process interpretation above (i.e. u should be points<br />on the GP), but we are free to choose p(Z). Omitting dependence on θ for clarity, and<br />choosing w.l.o.g. q(u,Z) = q(u|Z)q(Z), the bound on the marginal likelihood, similarly to<br />(4) is given by<br />?<br />The bound can be maximized w.r.t p(Z) by noting that the term only appears inside a<br />(negative) KL divergence: −Eq(Z)[logq(Z)/p(Z)]. Substituting the optimal p(Z) = q(Z)<br />reduces (9) to<br />?<br />which can now be optimized w.r.t. q(Z). Since no entropy term appears for q(Z), the bound<br />is maximized when the distribution becomes a Dirac’s delta. In summary, since we are free<br />to choose a prior for Z which maximizes the amount of information captured by u, the<br />optimal distribution becomes p(Z) = q(Z) = δ(Z −ˆZ). This formally motivates optimizing<br />the inducing points Z.<br />L = Ep(f |u,Z)q(u|Z)q(Z)<br />logp(y|f)p(u|Z)p(Z)<br />q(u|Z)q(Z)<br />?<br />.(9)<br />L = Eq(Z)<br />Ep(f |u,Z)q(u|Z)<br />?<br />logp(y|f)p(u|Z)<br />q(u|Z)<br />??<br />, (10)<br />Derivatives for Z<br />jective with respect to the inducing point positions. Substituting the optimal distribution<br />ˆ q(u,θ) into (4) to giveˆK and then differentiating we obtain<br />∂ˆK<br />∂Z<br />For completeness we also include the derivative of the free form ob-<br />∂Z= −∂ logC<br />= −Eˆ q(v,θ)<br />?∂<br />∂ZEp(f |u=Rv)[logp(y|f)]<br />?<br />.(11)<br />Since we aim to draw samples from ˆ q(v,θ), evaluating this free form inducing point gradient<br />using samples seems plausible but challenging. Instead we use the following strategy.<br />1. Fit a Gaussian approximation to the posterior. We follow [21] in fitting a<br />Gaussian approximation to the posterior. The positions of the inducing points are initial-<br />ized using k-means clustering of the data. The values of the latent function are represented<br />by a mean vector (initialized randomly) and a lower-triangular matrix L forms the approx-<br />imate posterior covariance as LL?. For large problems (such as the MNIST experiment),<br />stochastic optimization using AdaDelta is used. Otherwise, LBFGS is used. After a few<br />hundred iterations with the inducing points positions fixed, they are optimized in free-form<br />alongside the variational parameters and covariance function parameters.<br />2. Initialize the model using the approximation. Having found a satisfactory<br />approximation, the HMC strategy takes the optimized inducing point positions from the<br />Gaussian approximation. The initial value of v is drawn from the Gaussian approximation,<br />and the covariance parameters are initialized at the (approximate) MAP value.<br />3. Tuning HMC The HMC algorithm has two free parameters to tune, the number of<br />leapfrog steps and the step-length. We follow a strategy inspired by Wang et al. [25], where<br />5</p>  <p>Page 6</p> <p>Hensman et al.<br />the number of leapfrog steps is drawn randomly from 1 to Lmax, and Bayesian optimiza-<br />tion is used to maximize the expected square jump distance (ESJD), penalized by√Lmax.<br />Rather than allow an adaptive (but convergent) scheme as [25], we run the optimization for<br />30 iterations of 30 samples each, and use the best parameters for a long run of HMC.<br />4. Run tuned HMC to obtain predictions Having tuned the HMC, it is run for<br />several thousand iterations to obtain a good approximation to ˆ q(v,θ). The samples are used<br />to estimate the integral in equation (2). The following section investigates the effectiveness<br />of the proposed sampling scheme.<br />4. Experiments<br />4.1 Efficient sampling using Hamiltonian Monte Carlo<br />This section illustrates the effectiveness of Hamiltonian Monte Carlo in sampling from<br />ˆ q(v,θ). As already pointed out, the form assumed by the optimal variational distribution<br />ˆ q(v,θ) in equation (6) resembles the joint distribution in a GP model with a non-Gaussian<br />likelihood.<br />For a fixed θ, sampling v is relatively straightforward, and this is possible to be done<br />efficiently using HMC [13, 26, 27] or Elliptical Slice Sampling [28]. A well tuned HMC<br />has been reported to be extremely efficient in sampling the latent variables, and this mo-<br />tivates our effort into trying to extend this efficiency to the sampling of hyper-parameters<br />as well. This is also particularly appealing due to the convenience offered by the proposed<br />representation of the model.<br />The problem of drawing samples from the posterior distribution over v,θ has been in-<br />vestigated in detail in [12, 13]. In these works, it has been advocated to alternate between<br />the sampling of v and θ in a Gibbs sampling fashion and condition the sampling of θ on a<br />suitably chosen transformation of the latent variables. For each likelihood model, we com-<br />pare efficiency and convergence speed of the proposed HMC sampler with a Gibbs sampler<br />where v is sampled using HMC θ is sampled using the Metropolis-Hastings algorithm. To<br />make the comparison fair, we imposed the mass matrix in HMC and the covariance in MH<br />to be isotropic, and any parameters of the proposal were tuned using Bayesian optimiza-<br />tion. Unlike in the proposed HMC sampler, for the Gibbs sampler we did not penalize the<br />objective function of the Bayesian optimization for large numbers of leapfrog steps, as in<br />this case HMC proposals on the latent variables are computationally cheaper than those on<br />the hyper-parameters. We report efficiency in sampling from ˆ q(v,θ) using Effective Sam-<br />ple Size (ESS) and Time Normalized (TN)-ESS. In the supplement we include convergence<br />plots based on the Potential Scale Reduction Factor (PSRF) computed based on ten parallel<br />chains; in these each chain is initialized from the VB solution and individually tuned using<br />Bayesian optimization.<br />4.2 Binary Classification<br />We first use the image dataset [29] to investigate the benefits of the approach over a Gaussian<br />approximation, and to investigate the effect of changing the number of inducing points, as<br />well as optimizing the inducing points under the Gaussian approximation. The data are<br />6</p>  <p>Page 7</p> <p>MCMC for Variationally Sparse Gaussian Processes<br />Zoptimized<br />Zk-means<br />5 10 20 50 100 5 10 20 50 100<br />number of inducing points<br />Performance of the method on the image dataset, with one lengthscale per di-<br />mension. Left, box-plots show performance for varying numbers of inducing points and Z<br />strategies. Optimizing Z using the Gaussian approximation offers significant improvement<br />over the k-means strategy. Right: improvement of the performance of the Gaussian approx-<br />imation method, with the same inducing points. The method offers consistent performance<br />gains when the number of inducing points is larger. The supplement contains a similar<br />figure with only a single lengthscale.<br />−0.4<br />−0.2<br />logp(y?)[MCMC]<br />Zoptimized<br />Zk-means<br />5 10 20 50 100 5 10 20 50 100<br />number of inducing points<br />0<br />2<br />4<br />·10−2<br />logp(y?)[MCMC]− logp(y?)[Gauss.]<br />Figure 1:<br />18 dimensional: we investigated the effect of our approximation using both ARD (one<br />lengthscale per dimension) and an isotropic RBF kernel. The data were split randomly<br />into 1000/1019 train/test sets; the log predictive density over ten random splits is shown in<br />Figure 1.<br />Following the strategy outlined above, we fitted a Gaussian approximation to the pos-<br />terior, with Z initialized with k-means. Figure 1 investigates the difference in performance<br />when Z is optimized using the Gaussian approximation, compared to just using k-means<br />for Z. Whilst our strategy is not guaranteed to find the global optimum, it is clear that it<br />improves the performance.<br />The second part of Figure 1 shows the performance improvement of our sampling ap-<br />proach over the Gaussian approximation. We drew 10,000 samples, discarding the first 1000:<br />we see a consistent improvement in performance once M is large enough. For small M, The<br />Gaussian approximation appears to work very well. The supplement contains a similar Fig-<br />ure for the case where a single lengthscale is shared: there, the improvement of the MCMC<br />method over the Gaussian approximation is smaller but consistent. We speculate that the<br />larger gains for ARD are due to posterior uncertainty in the lengthscale parameters, which<br />is poorly represented by a point in the Gaussian/MAP approximation.<br />The ESS and TN-ESS are comparable between HMC and the Gibbs sampler. In par-<br />ticular, for 100 inducing points and the RBF covariance, ESS and TN-ESS for HMC are 11<br />and 1.0·10−3and for the Gibbs sampler are 53 and 5.1·10−3. For the ARD covariance, ESS<br />and TN-ESS for HMC are 14 and 5.1·10−3and for the Gibbs sampler are 1.6 and 1.5·10−4.<br />Convergence, however, seems to be faster for HMC, especially for the ARD covariance (see<br />the supplement).<br />7</p>  <p>Page 8</p> <p>Hensman et al.<br />1860 1880 1900192019401960<br />0<br />1<br />2<br />time (years)<br />rate<br />VB+Gaussian<br />VB+MCMC<br />MCMC<br />VB+MCMC<br />MCMC<br />0 20 40 60<br />lengthscale<br />024<br />variance<br />Figure 2:<br />rates using our variational MCMC method and a Gaussian approximation. Data are shown<br />as vertical bars. Right: posterior samples for the covariance function parameters using<br />MCMC. The Gaussian approximation estimated the parameters as (12.06, 0.55).<br />The posterior of the rates for the coal mining disaster data. Left: posterior<br />4.3 Log Gaussian Cox Processes<br />We apply our methods to Log Gaussian Cox processes [30]: doubly stochastic models where<br />the rate of an inhomogeneous Poisson process is given by a Gaussian process. The main<br />difficulty for inference lies in that the likelihood of the GP requires an integral over the<br />domain, which is typically intractable. For low dimensional problems, this integral can<br />be approximated on a grid; assuming that the GP is constant over the width of the grid<br />leads to a factorizing Poisson likelihood for each of the grid points. Whilst some recent<br />approaches allow for a grid-free approach [20, 31], these usually require concessions in the<br />model, such as an alternative link function, and do not approach full Bayesian inference<br />over the covariance function parameters.<br />Coal mining disasters<br />50% of the data at random, and using a grid of 100 points with 30 evenly spaced inducing<br />points Z, fitted both a Gaussian approximation to the posterior process with an (approx-<br />imate) MAP estimate for the covariance function parameters (variance and lengthscale of<br />an RBF kernel). With Gamma priors on the covariance parameters we ran our sampling<br />scheme using HMC, drawing 3000 samples. The resulting posterior approximations are<br />shown in Figure 2, alongside the true posterior using a sampling scheme similar to ours<br />(but without the inducing point approximation). The free-form variational approximation<br />matches the true posterior closely, whilst the Gaussian approximation misses important<br />detail. The approximate and true posteriors over covariance function parameters are shown<br />in the right hand part of Figure 2, there is minimal discrepancy in the distributions.<br />Over 10 random splits of the data, the average held-out log-likelihood was −1.229 for the<br />Gaussian approximation and −1.225 for the free-form MCMC variant; the average difference<br />was 0.003, and the MCMC variant was always better than the Gaussian approximation. We<br />attribute this improved performance to marginalization of the covariance function parame-<br />ters.<br />Efficiency of HMC is greater than for the Gibbs sampler; ESS and TN-ESS for HMC<br />are 6.7 and 3.1·10−2and for the Gibbs sampler are 9.7 and 1.9·10−2. Also, chains converge<br />On the one-dimensional coal-mining disaster data. We held out<br />8</p>  <p>Page 9</p> <p>MCMC for Variationally Sparse Gaussian Processes<br />0.0<br />1.5<br />3.0<br />4.5<br />6.0<br />7.5<br />9.0<br />10.5<br />12.0<br />Figure 3: Pine sapling data. From left to right: reported locations of pine saplings; posterior<br />mean intensity on a 32x32 grid using full MCMC; posterior mean intensity on a 32x32 grid<br />(with sparsity using 225 inducing points), posterior mean intensity on a 64x64 grid (using<br />225 inducing points). The supplement contains a larger version of this figure.<br />within few thousand iterations for both methods, although convergence for HMC is faster<br />(see the supplement).<br />Pine saplings<br />ber of grid points become higher, an effect emphasized with increasing dimension of the<br />domain. We fitted a similar model to the above to the pine sapling data [30].<br />We compared the sampling solution obtained using 225 inducing points on a 32 x 32 grid<br />to the gold standard full MCMC run with the same prior and grid size. Figure 3 shows that<br />the agreement between the variational sampling and full sampling is very close. However<br />the variational method was considerably faster. Using a single core on a desktop computer<br />required 3.4 seconds to obtain 1 effective sample for a well tuned variational method whereas<br />it took 554 seconds for well tuned full MCMC. This effect becomes even larger as we increase<br />the resolution of the grid to 64 x 64, which gives a better approximation to the underlying<br />smooth function as can be seen in figure 3. It took 4.7 seconds to obtain one effective sample<br />for the variational method, but now gold standard MCMC comparison was computationally<br />extremely challenging to run for even a single HMC step. This is because it requires linear<br />algebra operations using O(N3) flops with N = 4096.<br />The advantages of the proposed approximation are prominent as the num-<br />4.4 Multi-class Classification<br />To do multi-class classification with Gaussian processes, one latent function is defined for<br />each of the classes. The functions are defined a-priori independent, but covary a poste-<br />riori because of the likelihood. Chai [19] studies a sparse variational approximation to<br />the softmax multi-class likelihood restricted to a Gaussian approximation. Here, following<br />[32, 33, 34], we use a robust-max likelihood. Given a vector fncontaining K latent functions<br />evaluated at the point xn, the probability that the label takes the integer value ynis<br />?<br />?/(K − 1),<br />As Girolami and Rogers [32] discuss, the ‘soft’ probit-like behaviour is recovered by adding<br />a diagonal ‘nugget’ to the covariance function. In this work, ? was fixed to 0.001, though it<br />would also be possible to treat this as a parameter for inference. The expected log-likelihood<br />is Ep(fn|v,θ)[logp(yn|fn)] = plog(?)+(1−p)log(?/(K −1)), where p is the probability that<br />p(yn|fn) =<br />1 − ?,if yn= argmaxfn<br />otherwise.<br />(12)<br />9</p>  <p>Page 10</p> <p>Hensman et al.<br />Figure 4: A toy multiclass problem. Left: the Gaussian approximation, colored points show<br />the simulated data, lines show posterior probability contours at 0.3, 0.95, 0.99. Inducing<br />points positions shows as black points. Middle: the free form solution with 10,000 posterior<br />samples. The free-form solution is more conservative (the contours are smaller). Right:<br />posterior samples for v at the same position but across different latent functions. The<br />posterior exhibits strong correlations and edges.<br />the labelled function is largest, which is computable using one-dimensional quadrature. An<br />efficient Cython implementation is contained in the supplement.<br />Toy example<br />classification case, we turn to the toy data shown in Figure 4. We drew 750 data points from<br />three Gaussian distributions. The synthetic data was chosen to include non-linear decision<br />boundaries and ambiguous decision areas. Figure 4 shows that there are differences between<br />the variational and sampling solutions, with the sampling solution being more conservative<br />in general (the contours of 95% confidence are smaller). As one would expect at the decision<br />boundary there are strong correlations between the functions which could not be captured<br />by the Gaussian approximation we are using. Note the movement of inducing points away<br />from k-means and towards the decision boundaries.<br />Efficiency of HMC and the Gibbs sampler is comparable. In the RBF case, ESS and<br />TN-ESS for HMC are 1.9 and 3.8·10−4and for the Gibbs sampler are 2.5 and 3.6·10−4. In<br />the ARD case, ESS and TN-ESS for HMC are 1.2 and 2.8·10−3and for the Gibbs sampler<br />are 5.1 and 6.8 · 10−4. For both cases, the Gibbs sampler struggles to reach convergence<br />even though the average acceptance rates are similar to those recommended for the two<br />samplers individually.<br />To investigate the proposed posterior approximation for the multivariate<br />MNIST<br />split. We used 500 inducing points, initialized from the training data using k-means. A<br />Gaussian approximation was optimized using minibatch-based optimization over the means<br />and variances of q(u), as well as the inducing points and covariance function parameters.<br />The accuracy on the held-out data was 98.04%, significantly improving on previous ap-<br />proaches to to classify these digits using GP models.<br />For binary classification, Hensman et al. [21] reported that their Gaussian approximation<br />resulted in movement of the inducing point positions toward the decision boundary. The<br />same effect appears in the multivariate case, as shown in Figure 5, which shows three of<br />the 500 inducing points used in the MNIST problem. The three examples were initialized<br />close to the many six digits, and after optimization have moved close to other digits (five<br />The MNIST dataset is a well studied benchmark with a defined training/test<br />10</p>  <p>Page 11</p> <p>MCMC for Variationally Sparse Gaussian Processes<br />Figure 5: Left: three k-means centers used to initialize the inducing point positions. Center:<br />the positions of the same inducing points after optimization. Right: difference.<br />and four). The last example still appears to be a six, but has moved to a more ‘unusual’<br />six shape, supporting the function at another extremity. Similar effects are observed for all<br />inducing-point digits. Having optimized the inducing point positions with the approximate<br />q(v), and estimate for θ, we used these optimal inducing points to draw samples from v and<br />θ. This did not result in an increase in accuracy, but did improve the log-density on the test<br />set from -0.068 to -0.064. Evaluating the gradients for the sampler took approximately 0.4<br />seconds on a desktop machine, and we were easily able to draw 1000 samples. This dataset<br />size has generally be viewed as challenging in the GP community and consequently there<br />are not many published results to compare with. One recent work [35] reports a 94.05%<br />accuracy using variational inference and a GP latent variable model.<br />5. Discussion<br />We have presented an inference scheme for general GP models. The scheme significantly re-<br />duces the computational cost whilst approaching exact Bayesian inference, making minimal<br />assumptions about the form of the posterior. The improvements in accuracy in compar-<br />ison with the Gaussian approximation of previous works has been demonstrated, as has<br />the quality of the approximation to the hyper-parameter distribution. Our MCMC scheme<br />was shown to be effective for several likelihoods, and we note that the automatic tuning<br />of the sampling parameters worked well over hundreds of experiments. This paper shows<br />that MCMC methods are feasible for inference in large GP problems, addressing the unfair<br />sterotype of ‘slow’ MCMC.<br />Acknowledgments<br />JH was supported by a fellowship from the Medical Research Council. AM and ZG ac-<br />knowledge EPSRC grant EP/I036575/1, and a Google Focussed Research award. MF ac-<br />knowledges EPSRC grant EP/L020319/1.<br />References<br />[1] T. V. Nguyen and E. V. Bonilla. Automated variational inference for Gaussian process<br />models. In NIPS, pages 1404–1412, 2014.<br />[2] L. Csat´ o and M. Opper. Sparse on-line Gaussian processes. Neural comp., 14(3):<br />641–668, 2002.<br />11</p>  <p>Page 12</p> <p>Hensman et al.<br />[3] N. Lawrence, M. Seeger, and R. Herbrich. Fast sparse Gaussian process methods: The<br />informative vector machine. In NIPS, pages 609–616, 2003.<br />[4] E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs. In<br />NIPS, pages 1257–1264, 2005.<br />[5] M. K. Titsias. Variational learning of inducing variables in sparse Gaussian processes.<br />In AISTATS, pages 567–574, 2009.<br />[6] M. L´ azaro-Gredilla, J. Qui˜ nonero-Candela, C. E. Rasmussen, and A. Figueiras-Vidal.<br />Sparse spectrum Gaussian process regression. JMLR, 11:1865–1881, 2010.<br />[7] A. Solin and S. S¨ arkk¨ a. Hilbert space methods for reduced-rank Gaussian process<br />regression. arXiv preprint 1401.5508, 2014.<br />[8] A. G. Wilson, E. Gilboa, A. Nehorai, and J. P. Cunningham. Fast kernel learning for<br />multidimensional pattern extrapolation. In NIPS, pages 3626–3634. 2014.<br />[9] S. S¨ arkk¨ a. Bayesian filtering and smoothing, volume 3. Cambridge University Press,<br />2013.<br />[10] M. Filippone and R. Engler. Enabling scalable stochastic gradient-based inference for<br />Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE). ICML<br />2015, 2015.<br />[11] A. G. D. G. Matthews, J. Hensman, R. E. Turner, and Z. Ghahramani. On sparse vari-<br />ational methods and the KL divergence between stochastic processes. arXiv preprint<br />1504.07027, 2015.<br />[12] I. Murray and R. P. Adams.<br />Gaussian models. In NIPS, pages 1732–1740, 2010.<br />Slice sampling covariance hyperparameters of latent<br />[13] M. Filippone, M. Zhong, and M. Girolami. A comparative evaluation of stochastic-<br />based inference methods for Gaussian process models. Mach. Learn., 93(1):93–114,<br />2013.<br />[14] M. N. Gibbs and D. J. C. MacKay. Variational Gaussian process classifiers. IEEE<br />Trans. Neural Netw., 11(6):1458–1464, 2000.<br />[15] M. Opper and C. Archambeau. The variational Gaussian approximation revisited.<br />Neural comp., 21(3):786–792, 2009.<br />[16] M. Kuss and C. E. Rasmussen. Assessing approximate inference for binary Gaussian<br />process classification. JMLR, 6:1679–1704, 2005.<br />[17] H. Nickisch and C. E. Rasmussen. Approximations for binary Gaussian process classi-<br />fication. JMLR, 9:2035–2078, 2008.<br />[18] E. Khan, S. Mohamed, and K. P. Murphy. Fast Bayesian inference for non-conjugate<br />Gaussian process regression. In NIPS, pages 3140–3148, 2012.<br />12</p>  <p>Page 13</p> <p>MCMC for Variationally Sparse Gaussian Processes<br />[19] K. M. A. Chai. Variational multinomial logit Gaussian process. JMLR, 13(1):1745–<br />1808, June 2012. ISSN 1532-4435.<br />[20] C. Lloyd, T. Gunter, M. A. Osborne, and S. J. Roberts. Variational inference for<br />Gaussian process modulated poisson processes. ICML 2015, 2015.<br />[21] J. Hensman, A. Matthews, and Z. Ghahramani. Scalable variational Gaussian process<br />classification. In AISTATS, pages 351–360, 2014.<br />[22] C. K. I. Williams and D. Barber. Bayesian classification with Gaussian processes. IEEE<br />Trans. Pattern Anal. Mach. Intell., 20(12):1342–1351, 1998.<br />[23] S. P. Smith. Differentiation of the cholesky algorithm. J. Comp. Graph. Stat., 4(2):<br />134–147, 1995.<br />[24] J. Qui˜ nonero-Candela and C. E. Rasmussen. A unifying view of sparse approximate<br />Gaussian process regression. JMLR, 6:1939–1959, 2005.<br />[25] Z. Wang, S. Mohamed, and N. De Freitas. Adaptive Hamiltonian and Riemann mani-<br />fold Monte Carlo. In ICML, volume 28, pages 1462–1470, 2013.<br />[26] J. Vanhatalo and A. Vehtari. Sparse Log Gaussian Processes via MCMC for Spatial<br />Epidemiology. In Gaussian processes in practice, volume 1, pages 73–89, 2007.<br />[27] O. F. Christensen, G. O. Roberts, and J. S. Rosenthal. Scaling limits for the transient<br />phase of local MetropolisHastings algorithms. JRSS:B, 67(2):253–268, 2005.<br />[28] I. Murray, R. P. Adams, and D. J. C. MacKay. Elliptical slice sampling. In AISTATS,<br />volume 9, pages 541–548, 2010.<br />[29] G. R¨ atsch, T. Onoda, and K-R M¨ uller. Soft margins for adaboost. Mach. Learn., 42<br />(3):287–320, 2001.<br />[30] J. Møller, A. R. Syversveen, and R. P. Waagepetersen. Log Gaussian cox processes.<br />Scand. stat., 25(3):451–482, 1998.<br />[31] Y. K. Samo and S. Roberts. Scalable nonparametric Bayesian inference on point pro-<br />cesses with Gaussian processes. arXiv preprint arXiv:1410.6834, 2014.<br />[32] M. Girolami and S. Rogers Variational Bayesian multinomial probit regression with<br />Gaussian process priors. Neural Comp., 18:2006, 2005.<br />[33] H. Kim and Z. Ghahramani. Bayesian Gaussian Process Classification with the EM-EP<br />Algorithm. IEEE TPAMI, 28(12):1948–1959, 2006.<br />[34] D. Hern´ andez-Lobato, J. M. Hern´ andez-Lobato, and P. Dupont. Robust multi-class<br />Gaussian process classification. In NIPS, pages 280–288, 2011.<br />[35] Y. Gal, M. Van der Wilk, and Rasmussen C. E. Distributed variational inference in<br />sparse Gaussian process regression and latent variable models. In NIPS. 2014.<br />13</p>  <p>Page 14</p> <p>Hensman et al.<br />Supplementary material for:<br />MCMC for Variationally Sparse GPs<br />5.1 Coal data<br />Figure 6 replicates Figure 1, but with a single lengthscale shared across each input.<br />Zopt<br />10<br />Zopt<br />20<br />Zopt<br />50<br />Zopt<br />100<br />Zfix<br />10<br />Zfix<br />20<br />Zfix<br />50<br />Zfix<br />100<br />−0.5<br />−0.4<br />−0.3<br />−0.2<br />−0.1<br />Z fixed, number inducing points]<br />logp(y?)<br />Zopt<br />10<br />Zopt<br />20<br />Zopt<br />50<br />Zopt<br />100<br />Zfix<br />10<br />Zfix<br />20<br />Zfix<br />50<br />Zfix<br />100<br />−1<br />0<br />1<br />2<br />·10−3<br />Z fixed, number inducing points]<br />MCMC improvement over Gaussian VB<br />Figure 6: Performance of the method on the image dataset, with a single lengthscale.<br />5.2 Convergence plots<br />Convergence of the samplers on the Image dataset is reported in fig. 7 and shows the<br />evolution of the PSRF for the twenty slowest parameters for HMC and the Gibbs sampler in<br />the case of RBF and ARD covariances. The figure shows that HMC consistently converges<br />faster than the Gibbs sampler for both covariances, even when the ESS of the slowest<br />variable is comparable.<br />Fig. 7 shows the convergence analysis on the coal dataset. In this case, HMC converges<br />faster than the Gibbs sampler and efficiency is comparable.<br />Convergence of the samplers on the toy multi-class dataset is reported in fig. 9. HMC<br />converges much faster than the Gibbs sampler even though efficiency measured through<br />ESS is comparable.<br />14</p>  <p>Page 15</p> <p>MCMC for Variationally Sparse Gaussian Processes<br />2,0004,000<br />2<br />4<br />6<br />8<br />10<br />iteration<br />PSRF<br />2,0004,000<br />2<br />4<br />6<br />8<br />10<br />iteration<br />HMC<br />Gibbs<br />Figure 7:<br />traces for HMC (blue) and the Gibbs sampler (red). Left panel: RBF case - minimum ESS<br />and TN-ESS for HMC are 11 and 1.0·10−3and for the Gibbs sampler are 53 and 5.1·10−3.<br />Right panel: ARD case - minimum ESS and TN-ESS for HMC are 14 and 5.1 · 10−3and<br />for the Gibbs sampler are 1.6 and 1.5 · 10−4.<br />Image dataset - Evolution of the PSRF of the twenty least efficient parameter<br />2,0004,000<br />2<br />4<br />6<br />8<br />10<br />iteration<br />PSRF<br />Figure 8:<br />traces for HMC (blue) and the Gibbs sampler (red). Minimum ESS and TN-ESS for HMC<br />are 6.7 and 3.1 · 10−2and for the Gibbs sampler are 9.7 and 1.9 · 10−2.<br />Coal dataset - Evolution of the PSRF of the twenty least efficient parameter<br />2,0004,000<br />2<br />4<br />6<br />8<br />10<br />iteration<br />PSRF<br />2,0004,000<br />2<br />4<br />6<br />8<br />10<br />iteration<br />HMC<br />Gibbs<br />Figure 9: Multiclass dataset - Evolution of the PSRF of the twenty least efficient parameter<br />traces for HMC (blue) and the Gibbs sampler (red). Left panel: RBF case - minimum ESS<br />and TN-ESS for HMC are 1.9 and 3.8·10−4and for the Gibbs sampler are 2.5 and 3.6·10−4.<br />Right panel: ARD case - minimum ESS and TN-ESS for HMC are 1.2 and 2.8 · 10−3and<br />for the Gibbs sampler are 5.1 and 6.8 · 10−4.<br />15</p>  <p>Page 16</p> <p>Hensman et al.<br />5.3 Pine saplings<br />0.0<br />1.5<br />3.0<br />4.5<br />6.0<br />7.5<br />9.0<br />10.5<br />12.0<br />Figure 10: A larger version of Figure 3. Top right: gold standard MCMC 32x32 grid.<br />Bottom left: Variational MCMC 32x32 grid. Bottom right: Variational MCMC 64x64 grid,<br />with 225 inducing points in the non-exact case.<br />16</p>  <a href="https://www.researchgate.net/profile/Maurizio_Filippone/publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes/links/558a857008aee1fc9174ea84.pdf">Download full-text</a> </div> <div id="rgw22_56ab9d4c9a90b" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw23_56ab9d4c9a90b">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw24_56ab9d4c9a90b"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Maurizio_Filippone/publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes/links/558a857008aee1fc9174ea84.pdf" class="publication-viewer" title="558a857008aee1fc9174ea84.pdf">558a857008aee1fc9174ea84.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Maurizio_Filippone">Maurizio Filippone</a> &middot; Jun 24, 2015 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw25_56ab9d4c9a90b"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://de.arxiv.org/pdf/1506.04000" target="_blank" rel="nofollow" class="publication-viewer" title="MCMC for Variationally Sparse Gaussian Processes">MCMC for Variationally Sparse Gaussian Processes</a> </div>  <div class="details">   Available from <a href="http://de.arxiv.org/pdf/1506.04000" target="_blank" rel="nofollow">de.arxiv.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw32_56ab9d4c9a90b" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  <small> (32)  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (1) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw33_56ab9d4c9a90b" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw34_56ab9d4c9a90b" >  <div class="indent-left">  <div id="rgw35_56ab9d4c9a90b" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Vasek_Smidl" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Vasek Smidl </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw36_56ab9d4c9a90b">  <li class="citation-context-item"> "Inference of GP covariance parameters is analytically intractable, and standard inference methods require repeatedly calculating the so called marginal likelihood . When the likelihood function is not Gaussian, e.g., in classification, in ordinal regression, in modeling of stochastic volatility, in Cox-processes, the marginal likelihood cannot be computed analytically, and this has motivated a large body of the literature to develop approximate inference methods [56] [43] [31] [47] [41] [24], reparameterization techniques [36] [34] [54] [14], and exact inference with unbiased computations of the marginal likelihood [12] [10]. Even in the case of a Gaussian likelihood, which makes the marginal likelihood computable, inference is generally costly because the computation of the marginal likelihood has time complexity scaling with the cube of the number of input vectors [11]. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link ga-publication-item" href="publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes"> <span class="publication-title js-publication-title">Adaptive Multiple Importance Sampling for Gaussian Processes</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2079162962_Xiaoyu_Xiong" class="authors js-author-name ga-publications-authors">Xiaoyu Xiong</a> &middot;     <a href="researcher/82161005_Vaclav_Smidl" class="authors js-author-name ga-publications-authors">Václav Šmídl</a> &middot;     <a href="researcher/70871340_Maurizio_Filippone" class="authors js-author-name ga-publications-authors">Maurizio Filippone</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> In applications of Gaussian processes where quantification of uncertainty is
a strict requirement, it is necessary to accurately characterize the posterior
distribution over Gaussian process covariance parameters. Normally, this is
done by means of Markov chain Monte Carlo (MCMC) algorithms. Focusing on
Gaussian process regression where the marginal likelihood is computable but
expensive to evaluate, this paper studies algorithms based on importance
sampling to carry out expectations under the posterior distribution over
covariance parameters. The results indicate that expectations computed using
Adaptive Multiple Importance Sampling converge faster per unit of computation
than those computed with MCMC algorithms for models with few covariance
parameters, and converge as fast as MCMC for models with up to around twenty
covariance parameters. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Aug 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Vasek_Smidl/publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes/links/55e94dff08ae65b6389aee89.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw27_56ab9d4c9a90b" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw28_56ab9d4c9a90b">  </ul> </div> </div>   <div id="rgw18_56ab9d4c9a90b" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw19_56ab9d4c9a90b"> <div> <h5> <a href="publication/289587204_State_Space_representation_of_non-stationary_Gaussian_Processes" class="color-inherit ga-similar-publication-title"><span class="publication-title">State Space representation of non-stationary Gaussian Processes</span></a>  </h5>  <div class="authors"> <a href="researcher/33382619_Alessio_Benavoli" class="authors ga-similar-publication-author">Alessio Benavoli</a>, <a href="researcher/10654903_Marco_Zaffalon" class="authors ga-similar-publication-author">Marco Zaffalon</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw20_56ab9d4c9a90b"> <div> <h5> <a href="publication/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization" class="color-inherit ga-similar-publication-title"><span class="publication-title">Probabilistic Programming with Gaussian Process Memoization</span></a>  </h5>  <div class="authors"> <a href="researcher/2034167984_Ulrich_Schaechtle" class="authors ga-similar-publication-author">Ulrich Schaechtle</a>, <a href="researcher/2089456342_Ben_Zinberg" class="authors ga-similar-publication-author">Ben Zinberg</a>, <a href="researcher/2089399536_Alexey_Radul" class="authors ga-similar-publication-author">Alexey Radul</a>, <a href="researcher/8074903_Kostas_Stathis" class="authors ga-similar-publication-author">Kostas Stathis</a>, <a href="researcher/2089392273_Vikash_K_Mansinghka" class="authors ga-similar-publication-author">Vikash K. Mansinghka</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw21_56ab9d4c9a90b"> <div> <h5> <a href="publication/287250902_Quantum_assisted_Gaussian_process_regression" class="color-inherit ga-similar-publication-title"><span class="publication-title">Quantum assisted Gaussian process regression</span></a>  </h5>  <div class="authors"> <a href="researcher/2089556720_Zhikuan_Zhao" class="authors ga-similar-publication-author">Zhikuan Zhao</a>, <a href="researcher/2089802894_Jack_K_Fitzsimons" class="authors ga-similar-publication-author">Jack K. Fitzsimons</a>, <a href="researcher/2089493348_Joseph_F_Fitzsimons" class="authors ga-similar-publication-author">Joseph F. Fitzsimons</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw46_56ab9d4c9a90b" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw47_56ab9d4c9a90b">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw48_56ab9d4c9a90b" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=xFrAHJJA5RdEvoEgpIrU9vj_72BAAu0U-H64Vm8EjgCm2nSa4daSFCezYtGKKIY0" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="PtF1V9dW+6qvL568bxiRp8aANXv7J5QjGfVKrUsCgOYV8RJmymVUfehQnWz7orjFkH21JvERN9HCdiw5qTVbzSuP9df01/V4zYhz0bGGSBCqCkpREnq0kertLA3uAGN34A0WOqfAEQAdIV/2y3t7MqDs1EYo8Gq/H29CM5v6eW7FIQyMg2/8gKm+FjhfMIRuUb+RzXQ0AimZSzH1JGMczcL2M/2GHByGh8MUQHBrqV7GURy6gLVWYDXYQh4T84V0UvWOzaimXYAlJdKq3L7GtqeHws+fFoLnhUEy/3tkPZg="/> <input type="hidden" name="urlAfterLogin" value="publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjc4MzMyNDQ3X01DTUNfZm9yX1ZhcmlhdGlvbmFsbHlfU3BhcnNlX0dhdXNzaWFuX1Byb2Nlc3Nlcw%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjc4MzMyNDQ3X01DTUNfZm9yX1ZhcmlhdGlvbmFsbHlfU3BhcnNlX0dhdXNzaWFuX1Byb2Nlc3Nlcw%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjc4MzMyNDQ3X01DTUNfZm9yX1ZhcmlhdGlvbmFsbHlfU3BhcnNlX0dhdXNzaWFuX1Byb2Nlc3Nlcw%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw49_56ab9d4c9a90b"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 529;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"James Hensman","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277099177889798%401443077000994_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/James_Hensman","institution":"Lancaster University","institutionUrl":false,"widgetId":"rgw4_56ab9d4c9a90b"},"id":"rgw4_56ab9d4c9a90b","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=3686694","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab9d4c9a90b"},"id":"rgw3_56ab9d4c9a90b","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=278332447","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":278332447,"title":"MCMC for Variationally Sparse Gaussian Processes","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"06\/2015;","publicationDateRobot":"2015-06","article":""}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1506.04000","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"MCMC for Variationally Sparse Gaussian Processes"},{"key":"rft.date","value":"2015"},{"key":"rft.au","value":"James Hensman,Alexander G. de G. Matthews,Maurizio Filippone,Zoubin Ghahramani"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56ab9d4c9a90b"},"id":"rgw6_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=278332447","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":278332447,"peopleItems":[{"data":{"authorNameOnPublication":"James Hensman","accountUrl":"profile\/James_Hensman","accountKey":"James_Hensman","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277099177889798%401443077000994_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"James Hensman","profile":{"professionalInstitution":{"professionalInstitutionName":"Lancaster University","professionalInstitutionUrl":"institution\/Lancaster_University"}},"professionalInstitutionName":"Lancaster University","professionalInstitutionUrl":"institution\/Lancaster_University","url":"profile\/James_Hensman","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277099177889798%401443077000994_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"James_Hensman","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw9_56ab9d4c9a90b"},"id":"rgw9_56ab9d4c9a90b","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=3686694&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Lancaster University","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":4,"accountCount":2,"publicationUid":278332447,"widgetId":"rgw8_56ab9d4c9a90b"},"id":"rgw8_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=3686694&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=4&accountCount=2&publicationUid=278332447","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/2048369253_Alexander_G_de_G_Matthews","authorNameOnPublication":"Alexander G. de G. Matthews","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Alexander G. de G. Matthews","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2048369253_Alexander_G_de_G_Matthews","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw11_56ab9d4c9a90b"},"id":"rgw11_56ab9d4c9a90b","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2048369253&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw10_56ab9d4c9a90b"},"id":"rgw10_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2048369253&authorNameOnPublication=Alexander%20G.%20de%20G.%20Matthews","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Maurizio Filippone","accountUrl":"profile\/Maurizio_Filippone","accountKey":"Maurizio_Filippone","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273666076311560%401442258485613_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Maurizio Filippone","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":false}},"professionalInstitutionName":false,"professionalInstitutionUrl":false,"url":"profile\/Maurizio_Filippone","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273666076311560%401442258485613_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Maurizio_Filippone","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw13_56ab9d4c9a90b"},"id":"rgw13_56ab9d4c9a90b","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=4709876&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":false,"score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":4,"accountCount":2,"publicationUid":278332447,"widgetId":"rgw12_56ab9d4c9a90b"},"id":"rgw12_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=4709876&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=4&accountCount=2&publicationUid=278332447","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/8159937_Zoubin_Ghahramani","authorNameOnPublication":"Zoubin Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Zoubin Ghahramani","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/8159937_Zoubin_Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw15_56ab9d4c9a90b"},"id":"rgw15_56ab9d4c9a90b","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=8159937&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw14_56ab9d4c9a90b"},"id":"rgw14_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=8159937&authorNameOnPublication=Zoubin%20Ghahramani","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab9d4c9a90b"},"id":"rgw7_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=278332447&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":278332447,"abstract":"<noscript><\/noscript><div>Gaussian process (GP) models form a core part of probabilistic machine<br \/>\nlearning. Considerable research effort has been made into attacking three<br \/>\nissues with GP models: how to compute efficiently when the number of data is<br \/>\nlarge; how to approximate the posterior when the likelihood is not Gaussian and<br \/>\nhow to estimate covariance function parameter posteriors. This paper<br \/>\nsimultaneously addresses these, using a variational approximation to the<br \/>\nposterior which is sparse in support of the function but otherwise free-form.<br \/>\nThe result is a Hybrid Monte-Carlo sampling scheme which allows for a<br \/>\nnon-Gaussian approximation over the function values and covariance parameters<br \/>\nsimultaneously, with efficient computations based on inducing-point sparse GPs.<br \/>\nCode to replicate each experiment in this paper will be available shortly.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw16_56ab9d4c9a90b"},"id":"rgw16_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=278332447","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes\/links\/558a857008aee1fc9174ea84\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw17_56ab9d4c9a90b"},"id":"rgw17_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab9d4c9a90b"},"id":"rgw5_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=278332447&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":33382619,"url":"researcher\/33382619_Alessio_Benavoli","fullname":"Alessio Benavoli","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10654903,"url":"researcher\/10654903_Marco_Zaffalon","fullname":"Marco Zaffalon","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/289587204_State_Space_representation_of_non-stationary_Gaussian_Processes","usePlainButton":true,"publicationUid":289587204,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/289587204_State_Space_representation_of_non-stationary_Gaussian_Processes","title":"State Space representation of non-stationary Gaussian Processes","displayTitleAsLink":true,"authors":[{"id":33382619,"url":"researcher\/33382619_Alessio_Benavoli","fullname":"Alessio Benavoli","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10654903,"url":"researcher\/10654903_Marco_Zaffalon","fullname":"Marco Zaffalon","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/289587204_State_Space_representation_of_non-stationary_Gaussian_Processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/289587204_State_Space_representation_of_non-stationary_Gaussian_Processes\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab9d4c9a90b"},"id":"rgw19_56ab9d4c9a90b","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=289587204","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2034167984,"url":"researcher\/2034167984_Ulrich_Schaechtle","fullname":"Ulrich Schaechtle","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089456342,"url":"researcher\/2089456342_Ben_Zinberg","fullname":"Ben Zinberg","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089399536,"url":"researcher\/2089399536_Alexey_Radul","fullname":"Alexey Radul","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":8074903,"url":"researcher\/8074903_Kostas_Stathis","fullname":"Kostas Stathis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Dec 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization","usePlainButton":true,"publicationUid":287249271,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization","title":"Probabilistic Programming with Gaussian Process Memoization","displayTitleAsLink":true,"authors":[{"id":2034167984,"url":"researcher\/2034167984_Ulrich_Schaechtle","fullname":"Ulrich Schaechtle","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089456342,"url":"researcher\/2089456342_Ben_Zinberg","fullname":"Ben Zinberg","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089399536,"url":"researcher\/2089399536_Alexey_Radul","fullname":"Alexey Radul","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8074903,"url":"researcher\/8074903_Kostas_Stathis","fullname":"Kostas Stathis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089392273,"url":"researcher\/2089392273_Vikash_K_Mansinghka","fullname":"Vikash K. Mansinghka","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56ab9d4c9a90b"},"id":"rgw20_56ab9d4c9a90b","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=287249271","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2089556720,"url":"researcher\/2089556720_Zhikuan_Zhao","fullname":"Zhikuan Zhao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089802894,"url":"researcher\/2089802894_Jack_K_Fitzsimons","fullname":"Jack K. Fitzsimons","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089493348,"url":"researcher\/2089493348_Joseph_F_Fitzsimons","fullname":"Joseph F. Fitzsimons","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Dec 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/287250902_Quantum_assisted_Gaussian_process_regression","usePlainButton":true,"publicationUid":287250902,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/287250902_Quantum_assisted_Gaussian_process_regression","title":"Quantum assisted Gaussian process regression","displayTitleAsLink":true,"authors":[{"id":2089556720,"url":"researcher\/2089556720_Zhikuan_Zhao","fullname":"Zhikuan Zhao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089802894,"url":"researcher\/2089802894_Jack_K_Fitzsimons","fullname":"Jack K. Fitzsimons","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089493348,"url":"researcher\/2089493348_Joseph_F_Fitzsimons","fullname":"Joseph F. Fitzsimons","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/287250902_Quantum_assisted_Gaussian_process_regression","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/287250902_Quantum_assisted_Gaussian_process_regression\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw21_56ab9d4c9a90b"},"id":"rgw21_56ab9d4c9a90b","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=287250902","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw18_56ab9d4c9a90b"},"id":"rgw18_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=278332447&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":278332447,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":278332447,"publicationType":"article","linkId":"558a857008aee1fc9174ea84","fileName":"558a857008aee1fc9174ea84.pdf","fileUrl":"profile\/Maurizio_Filippone\/publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes\/links\/558a857008aee1fc9174ea84.pdf","name":"Maurizio Filippone","nameUrl":"profile\/Maurizio_Filippone","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Jun 24, 2015","fileSize":"1.02 MB","widgetId":"rgw24_56ab9d4c9a90b"},"id":"rgw24_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=278332447&linkId=558a857008aee1fc9174ea84&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":278332447,"publicationType":"article","linkId":"5587641308aeb0cdade0ba5e","fileName":"MCMC for Variationally Sparse Gaussian Processes","fileUrl":"http:\/\/de.arxiv.org\/pdf\/1506.04000","name":"de.arxiv.org","nameUrl":"http:\/\/de.arxiv.org\/pdf\/1506.04000","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw25_56ab9d4c9a90b"},"id":"rgw25_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=278332447&linkId=5587641308aeb0cdade0ba5e&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw23_56ab9d4c9a90b"},"id":"rgw23_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=278332447&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":37,"valueFormatted":"37","widgetId":"rgw26_56ab9d4c9a90b"},"id":"rgw26_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=278332447","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw22_56ab9d4c9a90b"},"id":"rgw22_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=278332447&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":278332447,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw28_56ab9d4c9a90b"},"id":"rgw28_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=278332447&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":37,"valueFormatted":"37","widgetId":"rgw29_56ab9d4c9a90b"},"id":"rgw29_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=278332447","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw27_56ab9d4c9a90b"},"id":"rgw27_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=278332447&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"MCMC for Variationally Sparse Gaussian Processes\nJames Hensman\nDepartment of Computer Science\nUniversity of Sheffield\nSheffield, UK\njames.hensman@sheffield.ac.uk\nAlexander G. de G. Matthews\nDepartment of Engineering\nUniversity of Cambridge\nCambridge, UK\nam554@cam.ac.uk\nMaurizio Filippone\nSchool of Computing Science\nUniversity of Glasgow\nGlasgow, UK\nmaurizio.filippone@glasgow.ac.uk\nZoubin Ghahramani\nDepartment of Engineering\nUniversity of Cambridge\nCambridge, UK\nzoubin@eng.cam.ac.uk\nAbstract\nGaussian process (GP) models form a core part of probabilistic machine learning. Con-\nsiderable research effort has been made into attacking three issues with GP models: how\nto compute efficiently when the number of data is large; how to approximate the posterior\nwhen the likelihood is not Gaussian and how to estimate covariance function parameter\nposteriors. This paper simultaneously addresses these, using a variational approximation\nto the posterior which is sparse in support of the function but otherwise free-form. The\nresult is a Hybrid Monte-Carlo sampling scheme which allows for a non-Gaussian approx-\nimation over the function values and covariance parameters simultaneously, with efficient\ncomputations based on inducing-point sparse GPs. Code to replicate each experiment in\nthis paper will be available shortly.\n1. Introduction\nGaussian process models are attractive for machine learning because of their flexible non-\nparametric nature. By combining a GP prior with different likelihoods, a multitude of\nmachine learning tasks can be tackled in a probabilistic fashion [1]. There are three things\nto consider when using a GP model: approximation of the posterior function (especially if\nthe likelihood is non-Gaussian), computation, storage and inversion of the covariance ma-\ntrix, which scales poorly in the number of data; and estimation (or marginalization) of the\ncovariance function parameters. A multitude of approximation schemes have been proposed\nfor efficient computation when the number of data is large. Early strategies were based on\nretaining a sub-set of the data [2, 3]. Snelson and Ghahramani [4] introduced an inducing\npoint approach, where the model is augmented with additional variables, and Titsias [5]\narXiv:1506.04000v1  [stat.ML]  12 Jun 2015"},{"page":2,"text":"Hensman et al.\nTable 1: Existing variational approaches\nReferencep(y|f)\nprobit\/logit\nGaussian\nsoftmax\nany factorized\nprobit\nany factorized\nSparsePosteriorHyperparam.\nWilliams & Barber[22] [also 15, 18]\nTitsias [5]\nChai [19]\nNguyen and Bonilla [1]\nHensman et al. [21]\nThis work\n?\n?\n?\n?\n?\n?\nGaussian (assumed)\nGaussian (optimal)\nGaussian (assumed)\nMixture of Gaussians\nGaussian (assumed)\nfree-form\npoint estimate\npoint estimate\npoint estimate\npoint estimate\npoint estimate\nfree-form\nused these ideas in a variational approach. Other authors have introduced approximations\nbased on the spectrum of the GP [6, 7], or which exploit specific structures within the\ncovariance matrix [8, 9], or by making unbiased stochastic estimates of key computations\n[10]. In this work, we extend the variational inducing point framework, which we prefer for\ngeneral applicability (no specific requirements are made of the data or covariance function),\nand because the variational inducing point approach can be shown to minimize the KL\ndivergence to the posterior process [11].\nTo approximate the posterior function and covariance parameters, Markov chain Monte-\nCarlo (MCMC) approaches provide asymptotically exact approximations.\nAdams [12] and Filippone et al. [13] examine schemes which iteratively sample the func-\ntion values and covariance parameters. Such sampling schemes require computation and\ninversion of the full covariance matrix at each iteration, making them unsuitable for large\nproblems. Computation may be reduced somewhat by considering variational methods,\napproximating the posterior using some fixed family of distributions [14, 15, 16, 17, 1, 18],\nthough many covariance matrix inversions are generally required. Recent works [19, 20, 21]\nhave proposed inducing point schemes which can reduce the computation required sub-\nstantially, though the posterior is assumed Gaussian and the covariance parameters are\nestimated by (approximate) maximum likelihood. Table 1 places our work in the context\nof existing variational methods for GPs.\nThis paper presents a general inference scheme, with the only concession to approx-\nimation being the variational inducing point assumption.\npermitted through MCMC, with the computational benefits of the inducing point frame-\nwork. The scheme jointly samples the inducing-point representation of the function with\nthe covariance function parameters; with sufficient inducing points our method approaches\nfull Bayesian inference over GP values and the covariance parameters. We show empirically\nthat the number of required inducing points is substantially smaller than the dataset size\nfor several real problems.\nMurray and\nNon-Gaussian posteriors are\n2. Stochastic process posteriors\nThe model is set up as follows. We are presented with some data inputs X = {xn}N\nand responses y = {yn}N\nmean and covariance function k(x,x?) with (hyper-) parameters \u03b8. Consistency of the\nGP means that only those points with data are considered: the latent vector f represents\nthe values of the function at the observed points f = {f(xn)}N\nn=1\nn=1. A latent function is assumed drawn from a GP with zero\nn=1, and has conditional\n2"},{"page":3,"text":"MCMC for Variationally Sparse Gaussian Processes\ndistribution p(f |X,\u03b8) = N(f |0,Kff), where Kff is a matrix composed of evaluating\nthe covariance function at all pairs of points in X. The data likelihood depends on the\nlatent function values: p(y|f). To make a prediction for latent function value test points\nf?= {f(x?)}x?\u2208X?, the posterior function values and parameters are integrated:\n? ?\nIn order to make use of the computational savings offered by the variational inducing point\nframework [5], we introduce additional input points to the function Z and collect the re-\nsponses of the function at that point into the vector u = {um= f(zm)}M\nvariational posterior q(u,\u03b8), new points are predicted similarly to the exact solution\n? ?\nThis makes clear that the approximation is a stochastic process in the same fashion as the\ntrue posterior: the length of the predictions vector f?is potentially unbounded, covering\nthe whole domain.\nTo obtain a variational objective, first consider the support of u under the true posterior,\nand for f under the approximation. In the above, these points are subsumed into the\nprediction vector f?: from here we shall be more explicit, letting f be the points of the\nprocess at X, u be the points of the process at Z and f?be a large vector containing all\nother points of interest1. All of the free parameters of the model are then f?,f,u,\u03b8, and\nusing a variational framework, we aim to minimize the Kullback-Leibler divergence between\nthe approximate and true posteriors:\np(f?|y) =p(f?|f,\u03b8)p(f,\u03b8|y)d\u03b8df .(1)\nm=1. With some\nq(f?) =p(f?|u,\u03b8)q(u,\u03b8)d\u03b8du.(2)\nK ? KL[q(f?,f,u,\u03b8)||p(f?,f,u,\u03b8|y)] =\n\u2212E\nq(f?,f,u,\u03b8)\n?\nlogp(f?|u,f,\u03b8)p(u|f,\u03b8)p(f,\u03b8|y)\np(f?|u,f,\u03b8)p(f |u,\u03b8)q(u,\u03b8)\n?\n(3)\nwhere the conditional distributions for f?have been expanded to make clear that they are\nthe same under the true and approximate posteriors, and X,Z and X?have been omitted\nfor clarity. Straight-forward identities simplify the expression,\n?\n= \u2212Eq(f,u,\u03b8)\nq(u,\u03b8)\nK = \u2212Eq(f,u,\u03b8)\nlogp(u|f,\u03b8)p(f |\u03b8)p(\u03b8)p(y|f)\/p(y)\np(f |u,\u03b8)q(u,\u03b8)\nlogp(u|\u03b8)p(\u03b8)p(y|f)\n?\n??\n+ logp(y),\n(4)\nresulting in the variational inducing-point objective investigated by Titsias [5], aside from\nthe inclusion of \u03b8. This can be rearranged to give the following informative expression\nK = KL\n?\nq(u,\u03b8)||p(u|\u03b8)p(\u03b8)Ep(f |u,\u03b8)[logp(y|f)]\nC\n?\n\u2212 logC + logp(y). (5)\n1. The vector f?here is considered finite but large enough to contain any point of interest for prediction.\nThe infinite case follows Matthews et al. [11], is omitted here for brevity, and results in the same solution.\n3"},{"page":4,"text":"Hensman et al.\nHere C is an intractable constant which normalizes the distribution and is independent of\nq. Minimizing the KL divergence on the right hand side reveals that the optimal variational\ndistribution is\nlog \u02c6 q(u,\u03b8) = Ep(f |u,\u03b8)[logp(y|f)] + logp(u|\u03b8) + logp(\u03b8) \u2212 logC.\nFor general likelihoods, since the optimal distribution does not take any particular form,\nwe intend to sample from it using MCMC. This is feasible using standard methods since it\nis computable up to a constant, using O(NM2) computations. To sample effectively, the\nfollowing are proposed.\n(6)\nWhitening the prior\nu, albeit with an interesting \u2018likelihood\u2019, we make use of an ancillary augmentation u = Rv,\nwith RR?= Kuu, v \u223c N(0,I). This results in the optimal variational distribution\nlog \u02c6 q(v,\u03b8) = Ep(f |u=Rv)[logp(y|f)] + logp(v) + logp(\u03b8) \u2212 logC\nPreviously [12, 13] this parameterization has been used with schemes which alternate be-\ntween sampling the latent function values (represented by v or u) and the parameters \u03b8.\nOur scheme uses HMC across v and \u03b8 jointly, whose effectiveness is examined throughout\nthe experiment section.\nNoting that the problem (6) appears similar to a standard GP for\n(7)\nQuadrature\ntion across the data-function pairs, this results in N one-dimensional integrals. For Gaussian\nor Poisson likelihood these integrals are tractable, otherwise they can be approximated by\nGauss-Hermite quadrature. Given the current sample v, the expectations are computed\nw.r.t. p(fn|v,\u03b8) = N(\u00b5n,\u03b3n), with:\n\u00b5 = A?v; \u03b3 = diag(Kff\u2212 A?A); A = R\u22121Kuf; RR?= Kuu,\nwhere the kernel matrices Kuf,Kuuare computed similarly to Kff, but over the pairs in\n(X,Z),(Z,Z) respectively. From here, one can compute the expected likelihood and it is\nsubsequently straight-forward to compute derivatives in terms of Kuf,diag(Kff) and R.\nThe first term in (6) is the expected log-likelihood. In the case of factoriza-\n(8)\nReverse mode differentiation of Cholesky\nand Z we use reverse-mode differentiation (backpropagation) of the derivative through the\nCholesky matrix decomposition, transforming \u2202 log \u02c6 q(v,\u03b8)\/\u2202R into \u2202 log \u02c6 q(v,\u03b8)\/\u2202Kuu, and\nthen \u2202 log \u02c6 q(v,\u03b8)\/\u2202\u03b8. This is discussed by Smith [23], and results in a O(M3) operation;\nan efficient Cython implementation is provided in the supplement.\nTo compute derivatives with respect to \u03b8\n3. Treatment of inducing point positions & inference strategy\nA natural question is, what strategy should be used to select the inducing points Z? In the\noriginal inducing point formulation [4], the positions Z were treated as parameters to be\noptimized. One could interpret them as parameters of the approximate prior covariance [24].\nThe variational formulation [5] treats them as parameters of the variational approximation,\nthus protecting from over-fitting as they form part of the variational posterior. In this work,\nsince we propose a Bayesian treatment of the model, we question whether it is feasible to\ntreat Z in a Bayesian fashion.\n4"},{"page":5,"text":"MCMC for Variationally Sparse Gaussian Processes\nSince u and Z are auxiliary parameters, the form of their distribution does not affect\nthe marginals of the model. The term p(u|Z) has been defined by the consistency with the\nGP in order to preserve the posterior-process interpretation above (i.e. u should be points\non the GP), but we are free to choose p(Z). Omitting dependence on \u03b8 for clarity, and\nchoosing w.l.o.g. q(u,Z) = q(u|Z)q(Z), the bound on the marginal likelihood, similarly to\n(4) is given by\n?\nThe bound can be maximized w.r.t p(Z) by noting that the term only appears inside a\n(negative) KL divergence: \u2212Eq(Z)[logq(Z)\/p(Z)]. Substituting the optimal p(Z) = q(Z)\nreduces (9) to\n?\nwhich can now be optimized w.r.t. q(Z). Since no entropy term appears for q(Z), the bound\nis maximized when the distribution becomes a Dirac\u2019s delta. In summary, since we are free\nto choose a prior for Z which maximizes the amount of information captured by u, the\noptimal distribution becomes p(Z) = q(Z) = \u03b4(Z \u2212\u02c6Z). This formally motivates optimizing\nthe inducing points Z.\nL = Ep(f |u,Z)q(u|Z)q(Z)\nlogp(y|f)p(u|Z)p(Z)\nq(u|Z)q(Z)\n?\n.(9)\nL = Eq(Z)\nEp(f |u,Z)q(u|Z)\n?\nlogp(y|f)p(u|Z)\nq(u|Z)\n??\n, (10)\nDerivatives for Z\njective with respect to the inducing point positions. Substituting the optimal distribution\n\u02c6 q(u,\u03b8) into (4) to give\u02c6K and then differentiating we obtain\n\u2202\u02c6K\n\u2202Z\nFor completeness we also include the derivative of the free form ob-\n\u2202Z= \u2212\u2202 logC\n= \u2212E\u02c6 q(v,\u03b8)\n?\u2202\n\u2202ZEp(f |u=Rv)[logp(y|f)]\n?\n.(11)\nSince we aim to draw samples from \u02c6 q(v,\u03b8), evaluating this free form inducing point gradient\nusing samples seems plausible but challenging. Instead we use the following strategy.\n1. Fit a Gaussian approximation to the posterior. We follow [21] in fitting a\nGaussian approximation to the posterior. The positions of the inducing points are initial-\nized using k-means clustering of the data. The values of the latent function are represented\nby a mean vector (initialized randomly) and a lower-triangular matrix L forms the approx-\nimate posterior covariance as LL?. For large problems (such as the MNIST experiment),\nstochastic optimization using AdaDelta is used. Otherwise, LBFGS is used. After a few\nhundred iterations with the inducing points positions fixed, they are optimized in free-form\nalongside the variational parameters and covariance function parameters.\n2. Initialize the model using the approximation. Having found a satisfactory\napproximation, the HMC strategy takes the optimized inducing point positions from the\nGaussian approximation. The initial value of v is drawn from the Gaussian approximation,\nand the covariance parameters are initialized at the (approximate) MAP value.\n3. Tuning HMC The HMC algorithm has two free parameters to tune, the number of\nleapfrog steps and the step-length. We follow a strategy inspired by Wang et al. [25], where\n5"},{"page":6,"text":"Hensman et al.\nthe number of leapfrog steps is drawn randomly from 1 to Lmax, and Bayesian optimiza-\ntion is used to maximize the expected square jump distance (ESJD), penalized by\u221aLmax.\nRather than allow an adaptive (but convergent) scheme as [25], we run the optimization for\n30 iterations of 30 samples each, and use the best parameters for a long run of HMC.\n4. Run tuned HMC to obtain predictions Having tuned the HMC, it is run for\nseveral thousand iterations to obtain a good approximation to \u02c6 q(v,\u03b8). The samples are used\nto estimate the integral in equation (2). The following section investigates the effectiveness\nof the proposed sampling scheme.\n4. Experiments\n4.1 Efficient sampling using Hamiltonian Monte Carlo\nThis section illustrates the effectiveness of Hamiltonian Monte Carlo in sampling from\n\u02c6 q(v,\u03b8). As already pointed out, the form assumed by the optimal variational distribution\n\u02c6 q(v,\u03b8) in equation (6) resembles the joint distribution in a GP model with a non-Gaussian\nlikelihood.\nFor a fixed \u03b8, sampling v is relatively straightforward, and this is possible to be done\nefficiently using HMC [13, 26, 27] or Elliptical Slice Sampling [28]. A well tuned HMC\nhas been reported to be extremely efficient in sampling the latent variables, and this mo-\ntivates our effort into trying to extend this efficiency to the sampling of hyper-parameters\nas well. This is also particularly appealing due to the convenience offered by the proposed\nrepresentation of the model.\nThe problem of drawing samples from the posterior distribution over v,\u03b8 has been in-\nvestigated in detail in [12, 13]. In these works, it has been advocated to alternate between\nthe sampling of v and \u03b8 in a Gibbs sampling fashion and condition the sampling of \u03b8 on a\nsuitably chosen transformation of the latent variables. For each likelihood model, we com-\npare efficiency and convergence speed of the proposed HMC sampler with a Gibbs sampler\nwhere v is sampled using HMC \u03b8 is sampled using the Metropolis-Hastings algorithm. To\nmake the comparison fair, we imposed the mass matrix in HMC and the covariance in MH\nto be isotropic, and any parameters of the proposal were tuned using Bayesian optimiza-\ntion. Unlike in the proposed HMC sampler, for the Gibbs sampler we did not penalize the\nobjective function of the Bayesian optimization for large numbers of leapfrog steps, as in\nthis case HMC proposals on the latent variables are computationally cheaper than those on\nthe hyper-parameters. We report efficiency in sampling from \u02c6 q(v,\u03b8) using Effective Sam-\nple Size (ESS) and Time Normalized (TN)-ESS. In the supplement we include convergence\nplots based on the Potential Scale Reduction Factor (PSRF) computed based on ten parallel\nchains; in these each chain is initialized from the VB solution and individually tuned using\nBayesian optimization.\n4.2 Binary Classification\nWe first use the image dataset [29] to investigate the benefits of the approach over a Gaussian\napproximation, and to investigate the effect of changing the number of inducing points, as\nwell as optimizing the inducing points under the Gaussian approximation. The data are\n6"},{"page":7,"text":"MCMC for Variationally Sparse Gaussian Processes\nZoptimized\nZk-means\n5 10 20 50 100 5 10 20 50 100\nnumber of inducing points\nPerformance of the method on the image dataset, with one lengthscale per di-\nmension. Left, box-plots show performance for varying numbers of inducing points and Z\nstrategies. Optimizing Z using the Gaussian approximation offers significant improvement\nover the k-means strategy. Right: improvement of the performance of the Gaussian approx-\nimation method, with the same inducing points. The method offers consistent performance\ngains when the number of inducing points is larger. The supplement contains a similar\nfigure with only a single lengthscale.\n\u22120.4\n\u22120.2\nlogp(y?)[MCMC]\nZoptimized\nZk-means\n5 10 20 50 100 5 10 20 50 100\nnumber of inducing points\n0\n2\n4\n\u00b710\u22122\nlogp(y?)[MCMC]\u2212 logp(y?)[Gauss.]\nFigure 1:\n18 dimensional: we investigated the effect of our approximation using both ARD (one\nlengthscale per dimension) and an isotropic RBF kernel. The data were split randomly\ninto 1000\/1019 train\/test sets; the log predictive density over ten random splits is shown in\nFigure 1.\nFollowing the strategy outlined above, we fitted a Gaussian approximation to the pos-\nterior, with Z initialized with k-means. Figure 1 investigates the difference in performance\nwhen Z is optimized using the Gaussian approximation, compared to just using k-means\nfor Z. Whilst our strategy is not guaranteed to find the global optimum, it is clear that it\nimproves the performance.\nThe second part of Figure 1 shows the performance improvement of our sampling ap-\nproach over the Gaussian approximation. We drew 10,000 samples, discarding the first 1000:\nwe see a consistent improvement in performance once M is large enough. For small M, The\nGaussian approximation appears to work very well. The supplement contains a similar Fig-\nure for the case where a single lengthscale is shared: there, the improvement of the MCMC\nmethod over the Gaussian approximation is smaller but consistent. We speculate that the\nlarger gains for ARD are due to posterior uncertainty in the lengthscale parameters, which\nis poorly represented by a point in the Gaussian\/MAP approximation.\nThe ESS and TN-ESS are comparable between HMC and the Gibbs sampler. In par-\nticular, for 100 inducing points and the RBF covariance, ESS and TN-ESS for HMC are 11\nand 1.0\u00b710\u22123and for the Gibbs sampler are 53 and 5.1\u00b710\u22123. For the ARD covariance, ESS\nand TN-ESS for HMC are 14 and 5.1\u00b710\u22123and for the Gibbs sampler are 1.6 and 1.5\u00b710\u22124.\nConvergence, however, seems to be faster for HMC, especially for the ARD covariance (see\nthe supplement).\n7"},{"page":8,"text":"Hensman et al.\n1860 1880 1900192019401960\n0\n1\n2\ntime (years)\nrate\nVB+Gaussian\nVB+MCMC\nMCMC\nVB+MCMC\nMCMC\n0 20 40 60\nlengthscale\n024\nvariance\nFigure 2:\nrates using our variational MCMC method and a Gaussian approximation. Data are shown\nas vertical bars. Right: posterior samples for the covariance function parameters using\nMCMC. The Gaussian approximation estimated the parameters as (12.06, 0.55).\nThe posterior of the rates for the coal mining disaster data. Left: posterior\n4.3 Log Gaussian Cox Processes\nWe apply our methods to Log Gaussian Cox processes [30]: doubly stochastic models where\nthe rate of an inhomogeneous Poisson process is given by a Gaussian process. The main\ndifficulty for inference lies in that the likelihood of the GP requires an integral over the\ndomain, which is typically intractable. For low dimensional problems, this integral can\nbe approximated on a grid; assuming that the GP is constant over the width of the grid\nleads to a factorizing Poisson likelihood for each of the grid points. Whilst some recent\napproaches allow for a grid-free approach [20, 31], these usually require concessions in the\nmodel, such as an alternative link function, and do not approach full Bayesian inference\nover the covariance function parameters.\nCoal mining disasters\n50% of the data at random, and using a grid of 100 points with 30 evenly spaced inducing\npoints Z, fitted both a Gaussian approximation to the posterior process with an (approx-\nimate) MAP estimate for the covariance function parameters (variance and lengthscale of\nan RBF kernel). With Gamma priors on the covariance parameters we ran our sampling\nscheme using HMC, drawing 3000 samples. The resulting posterior approximations are\nshown in Figure 2, alongside the true posterior using a sampling scheme similar to ours\n(but without the inducing point approximation). The free-form variational approximation\nmatches the true posterior closely, whilst the Gaussian approximation misses important\ndetail. The approximate and true posteriors over covariance function parameters are shown\nin the right hand part of Figure 2, there is minimal discrepancy in the distributions.\nOver 10 random splits of the data, the average held-out log-likelihood was \u22121.229 for the\nGaussian approximation and \u22121.225 for the free-form MCMC variant; the average difference\nwas 0.003, and the MCMC variant was always better than the Gaussian approximation. We\nattribute this improved performance to marginalization of the covariance function parame-\nters.\nEfficiency of HMC is greater than for the Gibbs sampler; ESS and TN-ESS for HMC\nare 6.7 and 3.1\u00b710\u22122and for the Gibbs sampler are 9.7 and 1.9\u00b710\u22122. Also, chains converge\nOn the one-dimensional coal-mining disaster data. We held out\n8"},{"page":9,"text":"MCMC for Variationally Sparse Gaussian Processes\n0.0\n1.5\n3.0\n4.5\n6.0\n7.5\n9.0\n10.5\n12.0\nFigure 3: Pine sapling data. From left to right: reported locations of pine saplings; posterior\nmean intensity on a 32x32 grid using full MCMC; posterior mean intensity on a 32x32 grid\n(with sparsity using 225 inducing points), posterior mean intensity on a 64x64 grid (using\n225 inducing points). The supplement contains a larger version of this figure.\nwithin few thousand iterations for both methods, although convergence for HMC is faster\n(see the supplement).\nPine saplings\nber of grid points become higher, an effect emphasized with increasing dimension of the\ndomain. We fitted a similar model to the above to the pine sapling data [30].\nWe compared the sampling solution obtained using 225 inducing points on a 32 x 32 grid\nto the gold standard full MCMC run with the same prior and grid size. Figure 3 shows that\nthe agreement between the variational sampling and full sampling is very close. However\nthe variational method was considerably faster. Using a single core on a desktop computer\nrequired 3.4 seconds to obtain 1 effective sample for a well tuned variational method whereas\nit took 554 seconds for well tuned full MCMC. This effect becomes even larger as we increase\nthe resolution of the grid to 64 x 64, which gives a better approximation to the underlying\nsmooth function as can be seen in figure 3. It took 4.7 seconds to obtain one effective sample\nfor the variational method, but now gold standard MCMC comparison was computationally\nextremely challenging to run for even a single HMC step. This is because it requires linear\nalgebra operations using O(N3) flops with N = 4096.\nThe advantages of the proposed approximation are prominent as the num-\n4.4 Multi-class Classification\nTo do multi-class classification with Gaussian processes, one latent function is defined for\neach of the classes. The functions are defined a-priori independent, but covary a poste-\nriori because of the likelihood. Chai [19] studies a sparse variational approximation to\nthe softmax multi-class likelihood restricted to a Gaussian approximation. Here, following\n[32, 33, 34], we use a robust-max likelihood. Given a vector fncontaining K latent functions\nevaluated at the point xn, the probability that the label takes the integer value ynis\n?\n?\/(K \u2212 1),\nAs Girolami and Rogers [32] discuss, the \u2018soft\u2019 probit-like behaviour is recovered by adding\na diagonal \u2018nugget\u2019 to the covariance function. In this work, ? was fixed to 0.001, though it\nwould also be possible to treat this as a parameter for inference. The expected log-likelihood\nis Ep(fn|v,\u03b8)[logp(yn|fn)] = plog(?)+(1\u2212p)log(?\/(K \u22121)), where p is the probability that\np(yn|fn) =\n1 \u2212 ?,if yn= argmaxfn\notherwise.\n(12)\n9"},{"page":10,"text":"Hensman et al.\nFigure 4: A toy multiclass problem. Left: the Gaussian approximation, colored points show\nthe simulated data, lines show posterior probability contours at 0.3, 0.95, 0.99. Inducing\npoints positions shows as black points. Middle: the free form solution with 10,000 posterior\nsamples. The free-form solution is more conservative (the contours are smaller). Right:\nposterior samples for v at the same position but across different latent functions. The\nposterior exhibits strong correlations and edges.\nthe labelled function is largest, which is computable using one-dimensional quadrature. An\nefficient Cython implementation is contained in the supplement.\nToy example\nclassification case, we turn to the toy data shown in Figure 4. We drew 750 data points from\nthree Gaussian distributions. The synthetic data was chosen to include non-linear decision\nboundaries and ambiguous decision areas. Figure 4 shows that there are differences between\nthe variational and sampling solutions, with the sampling solution being more conservative\nin general (the contours of 95% confidence are smaller). As one would expect at the decision\nboundary there are strong correlations between the functions which could not be captured\nby the Gaussian approximation we are using. Note the movement of inducing points away\nfrom k-means and towards the decision boundaries.\nEfficiency of HMC and the Gibbs sampler is comparable. In the RBF case, ESS and\nTN-ESS for HMC are 1.9 and 3.8\u00b710\u22124and for the Gibbs sampler are 2.5 and 3.6\u00b710\u22124. In\nthe ARD case, ESS and TN-ESS for HMC are 1.2 and 2.8\u00b710\u22123and for the Gibbs sampler\nare 5.1 and 6.8 \u00b7 10\u22124. For both cases, the Gibbs sampler struggles to reach convergence\neven though the average acceptance rates are similar to those recommended for the two\nsamplers individually.\nTo investigate the proposed posterior approximation for the multivariate\nMNIST\nsplit. We used 500 inducing points, initialized from the training data using k-means. A\nGaussian approximation was optimized using minibatch-based optimization over the means\nand variances of q(u), as well as the inducing points and covariance function parameters.\nThe accuracy on the held-out data was 98.04%, significantly improving on previous ap-\nproaches to to classify these digits using GP models.\nFor binary classification, Hensman et al. [21] reported that their Gaussian approximation\nresulted in movement of the inducing point positions toward the decision boundary. The\nsame effect appears in the multivariate case, as shown in Figure 5, which shows three of\nthe 500 inducing points used in the MNIST problem. The three examples were initialized\nclose to the many six digits, and after optimization have moved close to other digits (five\nThe MNIST dataset is a well studied benchmark with a defined training\/test\n10"},{"page":11,"text":"MCMC for Variationally Sparse Gaussian Processes\nFigure 5: Left: three k-means centers used to initialize the inducing point positions. Center:\nthe positions of the same inducing points after optimization. Right: difference.\nand four). The last example still appears to be a six, but has moved to a more \u2018unusual\u2019\nsix shape, supporting the function at another extremity. Similar effects are observed for all\ninducing-point digits. Having optimized the inducing point positions with the approximate\nq(v), and estimate for \u03b8, we used these optimal inducing points to draw samples from v and\n\u03b8. This did not result in an increase in accuracy, but did improve the log-density on the test\nset from -0.068 to -0.064. Evaluating the gradients for the sampler took approximately 0.4\nseconds on a desktop machine, and we were easily able to draw 1000 samples. This dataset\nsize has generally be viewed as challenging in the GP community and consequently there\nare not many published results to compare with. One recent work [35] reports a 94.05%\naccuracy using variational inference and a GP latent variable model.\n5. Discussion\nWe have presented an inference scheme for general GP models. The scheme significantly re-\nduces the computational cost whilst approaching exact Bayesian inference, making minimal\nassumptions about the form of the posterior. The improvements in accuracy in compar-\nison with the Gaussian approximation of previous works has been demonstrated, as has\nthe quality of the approximation to the hyper-parameter distribution. Our MCMC scheme\nwas shown to be effective for several likelihoods, and we note that the automatic tuning\nof the sampling parameters worked well over hundreds of experiments. This paper shows\nthat MCMC methods are feasible for inference in large GP problems, addressing the unfair\nsterotype of \u2018slow\u2019 MCMC.\nAcknowledgments\nJH was supported by a fellowship from the Medical Research Council. AM and ZG ac-\nknowledge EPSRC grant EP\/I036575\/1, and a Google Focussed Research award. MF ac-\nknowledges EPSRC grant EP\/L020319\/1.\nReferences\n[1] T. V. Nguyen and E. V. Bonilla. Automated variational inference for Gaussian process\nmodels. In NIPS, pages 1404\u20131412, 2014.\n[2] L. Csat\u00b4 o and M. Opper. Sparse on-line Gaussian processes. Neural comp., 14(3):\n641\u2013668, 2002.\n11"},{"page":12,"text":"Hensman et al.\n[3] N. Lawrence, M. Seeger, and R. Herbrich. Fast sparse Gaussian process methods: The\ninformative vector machine. In NIPS, pages 609\u2013616, 2003.\n[4] E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs. In\nNIPS, pages 1257\u20131264, 2005.\n[5] M. K. Titsias. Variational learning of inducing variables in sparse Gaussian processes.\nIn AISTATS, pages 567\u2013574, 2009.\n[6] M. L\u00b4 azaro-Gredilla, J. Qui\u02dc nonero-Candela, C. E. Rasmussen, and A. Figueiras-Vidal.\nSparse spectrum Gaussian process regression. JMLR, 11:1865\u20131881, 2010.\n[7] A. Solin and S. S\u00a8 arkk\u00a8 a. Hilbert space methods for reduced-rank Gaussian process\nregression. arXiv preprint 1401.5508, 2014.\n[8] A. G. Wilson, E. Gilboa, A. Nehorai, and J. P. Cunningham. Fast kernel learning for\nmultidimensional pattern extrapolation. In NIPS, pages 3626\u20133634. 2014.\n[9] S. S\u00a8 arkk\u00a8 a. Bayesian filtering and smoothing, volume 3. Cambridge University Press,\n2013.\n[10] M. Filippone and R. Engler. Enabling scalable stochastic gradient-based inference for\nGaussian processes by employing the Unbiased LInear System SolvEr (ULISSE). ICML\n2015, 2015.\n[11] A. G. D. G. Matthews, J. Hensman, R. E. Turner, and Z. Ghahramani. On sparse vari-\national methods and the KL divergence between stochastic processes. arXiv preprint\n1504.07027, 2015.\n[12] I. Murray and R. P. Adams.\nGaussian models. In NIPS, pages 1732\u20131740, 2010.\nSlice sampling covariance hyperparameters of latent\n[13] M. Filippone, M. Zhong, and M. Girolami. A comparative evaluation of stochastic-\nbased inference methods for Gaussian process models. Mach. Learn., 93(1):93\u2013114,\n2013.\n[14] M. N. Gibbs and D. J. C. MacKay. Variational Gaussian process classifiers. IEEE\nTrans. Neural Netw., 11(6):1458\u20131464, 2000.\n[15] M. Opper and C. Archambeau. The variational Gaussian approximation revisited.\nNeural comp., 21(3):786\u2013792, 2009.\n[16] M. Kuss and C. E. Rasmussen. Assessing approximate inference for binary Gaussian\nprocess classification. JMLR, 6:1679\u20131704, 2005.\n[17] H. Nickisch and C. E. Rasmussen. Approximations for binary Gaussian process classi-\nfication. JMLR, 9:2035\u20132078, 2008.\n[18] E. Khan, S. Mohamed, and K. P. Murphy. Fast Bayesian inference for non-conjugate\nGaussian process regression. In NIPS, pages 3140\u20133148, 2012.\n12"},{"page":13,"text":"MCMC for Variationally Sparse Gaussian Processes\n[19] K. M. A. Chai. Variational multinomial logit Gaussian process. JMLR, 13(1):1745\u2013\n1808, June 2012. ISSN 1532-4435.\n[20] C. Lloyd, T. Gunter, M. A. Osborne, and S. J. Roberts. Variational inference for\nGaussian process modulated poisson processes. ICML 2015, 2015.\n[21] J. Hensman, A. Matthews, and Z. Ghahramani. Scalable variational Gaussian process\nclassification. In AISTATS, pages 351\u2013360, 2014.\n[22] C. K. I. Williams and D. Barber. Bayesian classification with Gaussian processes. IEEE\nTrans. Pattern Anal. Mach. Intell., 20(12):1342\u20131351, 1998.\n[23] S. P. Smith. Differentiation of the cholesky algorithm. J. Comp. Graph. Stat., 4(2):\n134\u2013147, 1995.\n[24] J. Qui\u02dc nonero-Candela and C. E. Rasmussen. A unifying view of sparse approximate\nGaussian process regression. JMLR, 6:1939\u20131959, 2005.\n[25] Z. Wang, S. Mohamed, and N. De Freitas. Adaptive Hamiltonian and Riemann mani-\nfold Monte Carlo. In ICML, volume 28, pages 1462\u20131470, 2013.\n[26] J. Vanhatalo and A. Vehtari. Sparse Log Gaussian Processes via MCMC for Spatial\nEpidemiology. In Gaussian processes in practice, volume 1, pages 73\u201389, 2007.\n[27] O. F. Christensen, G. O. Roberts, and J. S. Rosenthal. Scaling limits for the transient\nphase of local MetropolisHastings algorithms. JRSS:B, 67(2):253\u2013268, 2005.\n[28] I. Murray, R. P. Adams, and D. J. C. MacKay. Elliptical slice sampling. In AISTATS,\nvolume 9, pages 541\u2013548, 2010.\n[29] G. R\u00a8 atsch, T. Onoda, and K-R M\u00a8 uller. Soft margins for adaboost. Mach. Learn., 42\n(3):287\u2013320, 2001.\n[30] J. M\u00f8ller, A. R. Syversveen, and R. P. Waagepetersen. Log Gaussian cox processes.\nScand. stat., 25(3):451\u2013482, 1998.\n[31] Y. K. Samo and S. Roberts. Scalable nonparametric Bayesian inference on point pro-\ncesses with Gaussian processes. arXiv preprint arXiv:1410.6834, 2014.\n[32] M. Girolami and S. Rogers Variational Bayesian multinomial probit regression with\nGaussian process priors. Neural Comp., 18:2006, 2005.\n[33] H. Kim and Z. Ghahramani. Bayesian Gaussian Process Classification with the EM-EP\nAlgorithm. IEEE TPAMI, 28(12):1948\u20131959, 2006.\n[34] D. Hern\u00b4 andez-Lobato, J. M. Hern\u00b4 andez-Lobato, and P. Dupont. Robust multi-class\nGaussian process classification. In NIPS, pages 280\u2013288, 2011.\n[35] Y. Gal, M. Van der Wilk, and Rasmussen C. E. Distributed variational inference in\nsparse Gaussian process regression and latent variable models. In NIPS. 2014.\n13"},{"page":14,"text":"Hensman et al.\nSupplementary material for:\nMCMC for Variationally Sparse GPs\n5.1 Coal data\nFigure 6 replicates Figure 1, but with a single lengthscale shared across each input.\nZopt\n10\nZopt\n20\nZopt\n50\nZopt\n100\nZfix\n10\nZfix\n20\nZfix\n50\nZfix\n100\n\u22120.5\n\u22120.4\n\u22120.3\n\u22120.2\n\u22120.1\nZ fixed, number inducing points]\nlogp(y?)\nZopt\n10\nZopt\n20\nZopt\n50\nZopt\n100\nZfix\n10\nZfix\n20\nZfix\n50\nZfix\n100\n\u22121\n0\n1\n2\n\u00b710\u22123\nZ fixed, number inducing points]\nMCMC improvement over Gaussian VB\nFigure 6: Performance of the method on the image dataset, with a single lengthscale.\n5.2 Convergence plots\nConvergence of the samplers on the Image dataset is reported in fig. 7 and shows the\nevolution of the PSRF for the twenty slowest parameters for HMC and the Gibbs sampler in\nthe case of RBF and ARD covariances. The figure shows that HMC consistently converges\nfaster than the Gibbs sampler for both covariances, even when the ESS of the slowest\nvariable is comparable.\nFig. 7 shows the convergence analysis on the coal dataset. In this case, HMC converges\nfaster than the Gibbs sampler and efficiency is comparable.\nConvergence of the samplers on the toy multi-class dataset is reported in fig. 9. HMC\nconverges much faster than the Gibbs sampler even though efficiency measured through\nESS is comparable.\n14"},{"page":15,"text":"MCMC for Variationally Sparse Gaussian Processes\n2,0004,000\n2\n4\n6\n8\n10\niteration\nPSRF\n2,0004,000\n2\n4\n6\n8\n10\niteration\nHMC\nGibbs\nFigure 7:\ntraces for HMC (blue) and the Gibbs sampler (red). Left panel: RBF case - minimum ESS\nand TN-ESS for HMC are 11 and 1.0\u00b710\u22123and for the Gibbs sampler are 53 and 5.1\u00b710\u22123.\nRight panel: ARD case - minimum ESS and TN-ESS for HMC are 14 and 5.1 \u00b7 10\u22123and\nfor the Gibbs sampler are 1.6 and 1.5 \u00b7 10\u22124.\nImage dataset - Evolution of the PSRF of the twenty least efficient parameter\n2,0004,000\n2\n4\n6\n8\n10\niteration\nPSRF\nFigure 8:\ntraces for HMC (blue) and the Gibbs sampler (red). Minimum ESS and TN-ESS for HMC\nare 6.7 and 3.1 \u00b7 10\u22122and for the Gibbs sampler are 9.7 and 1.9 \u00b7 10\u22122.\nCoal dataset - Evolution of the PSRF of the twenty least efficient parameter\n2,0004,000\n2\n4\n6\n8\n10\niteration\nPSRF\n2,0004,000\n2\n4\n6\n8\n10\niteration\nHMC\nGibbs\nFigure 9: Multiclass dataset - Evolution of the PSRF of the twenty least efficient parameter\ntraces for HMC (blue) and the Gibbs sampler (red). Left panel: RBF case - minimum ESS\nand TN-ESS for HMC are 1.9 and 3.8\u00b710\u22124and for the Gibbs sampler are 2.5 and 3.6\u00b710\u22124.\nRight panel: ARD case - minimum ESS and TN-ESS for HMC are 1.2 and 2.8 \u00b7 10\u22123and\nfor the Gibbs sampler are 5.1 and 6.8 \u00b7 10\u22124.\n15"},{"page":16,"text":"Hensman et al.\n5.3 Pine saplings\n0.0\n1.5\n3.0\n4.5\n6.0\n7.5\n9.0\n10.5\n12.0\nFigure 10: A larger version of Figure 3. Top right: gold standard MCMC 32x32 grid.\nBottom left: Variational MCMC 32x32 grid. Bottom right: Variational MCMC 64x64 grid,\nwith 225 inducing points in the non-exact case.\n16"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Maurizio_Filippone\/publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes\/links\/558a857008aee1fc9174ea84.pdf","widgetId":"rgw30_56ab9d4c9a90b"},"id":"rgw30_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=278332447&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw31_56ab9d4c9a90b"},"id":"rgw31_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=278332447&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":278332447,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":278332447,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2079162962,"url":"researcher\/2079162962_Xiaoyu_Xiong","fullname":"Xiaoyu Xiong","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A309409379487746%401450780352548_m\/Xiaoyu_Xiong.png"},{"id":82161005,"url":"researcher\/82161005_Vaclav_Smidl","fullname":"V\u00e1clav \u0160m\u00eddl","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70871340,"url":"researcher\/70871340_Maurizio_Filippone","fullname":"Maurizio Filippone","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Aug 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes","usePlainButton":true,"publicationUid":280773011,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes","title":"Adaptive Multiple Importance Sampling for Gaussian Processes","displayTitleAsLink":true,"authors":[{"id":2079162962,"url":"researcher\/2079162962_Xiaoyu_Xiong","fullname":"Xiaoyu Xiong","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A309409379487746%401450780352548_m\/Xiaoyu_Xiong.png"},{"id":82161005,"url":"researcher\/82161005_Vaclav_Smidl","fullname":"V\u00e1clav \u0160m\u00eddl","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70871340,"url":"researcher\/70871340_Maurizio_Filippone","fullname":"Maurizio Filippone","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"In applications of Gaussian processes where quantification of uncertainty is\na strict requirement, it is necessary to accurately characterize the posterior\ndistribution over Gaussian process covariance parameters. Normally, this is\ndone by means of Markov chain Monte Carlo (MCMC) algorithms. Focusing on\nGaussian process regression where the marginal likelihood is computable but\nexpensive to evaluate, this paper studies algorithms based on importance\nsampling to carry out expectations under the posterior distribution over\ncovariance parameters. The results indicate that expectations computed using\nAdaptive Multiple Importance Sampling converge faster per unit of computation\nthan those computed with MCMC algorithms for models with few covariance\nparameters, and converge as fast as MCMC for models with up to around twenty\ncovariance parameters.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Vasek_Smidl\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/links\/55e94dff08ae65b6389aee89.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Vasek_Smidl","sourceName":"Vasek Smidl","hasSourceUrl":true},"publicationUid":280773011,"publicationUrl":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/links\/55e94dff08ae65b6389aee89\/smallpreview.png","linkId":"55e94dff08ae65b6389aee89","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=280773011&reference=55e94dff08ae65b6389aee89&eventCode=&origin=publication_list","widgetId":"rgw35_56ab9d4c9a90b"},"id":"rgw35_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=280773011&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"55e94dff08ae65b6389aee89","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":278332447,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Inference of GP covariance parameters is analytically intractable, and standard inference methods require repeatedly calculating the so called marginal likelihood . When the likelihood function is not Gaussian, e.g., in classification, in ordinal regression, in modeling of stochastic volatility, in Cox-processes, the marginal likelihood cannot be computed analytically, and this has motivated a large body of the literature to develop approximate inference methods [56] [43] [31] [47] [41] [24], reparameterization techniques [36] [34] [54] [14], and exact inference with unbiased computations of the marginal likelihood [12] [10]. Even in the case of a Gaussian likelihood, which makes the marginal likelihood computable, inference is generally costly because the computation of the marginal likelihood has time complexity scaling with the cube of the number of input vectors [11]. "],"widgetId":"rgw36_56ab9d4c9a90b"},"id":"rgw36_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw34_56ab9d4c9a90b"},"id":"rgw34_56ab9d4c9a90b","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=280773011&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":278332447,"publicationLink":"publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes","hasShowMore":false,"newOffset":3,"pageSize":10,"widgetId":"rgw33_56ab9d4c9a90b"},"id":"rgw33_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=278332447&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=1","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":true,"citationsCount":32,"hasIncomingCitations":true,"incomingCitationsCount":1,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw32_56ab9d4c9a90b"},"id":"rgw32_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=278332447&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"558a857008aee1fc9174ea84","name":"Maurizio Filippone","date":"Jun 24, 2015 ","nameLink":"profile\/Maurizio_Filippone","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Maurizio_Filippone\/publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes\/links\/558a857008aee1fc9174ea84.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Maurizio_Filippone\/publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes\/links\/558a857008aee1fc9174ea84.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"0b248a375cfea43d8048c830e89158b3","showFileSizeNote":false,"fileSize":"1.02 MB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"558a857008aee1fc9174ea84","name":"Maurizio Filippone","date":"Jun 24, 2015 ","nameLink":"profile\/Maurizio_Filippone","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Maurizio_Filippone\/publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes\/links\/558a857008aee1fc9174ea84.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Maurizio_Filippone\/publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes\/links\/558a857008aee1fc9174ea84.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"0b248a375cfea43d8048c830e89158b3","showFileSizeNote":false,"fileSize":"1.02 MB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=8D1MkZR3vF15SQ_HmvkrIcOH_VUS_WPT_F48flmPM0s4tqzyB5OjQ7BlX5e1Vpori7hfZenI6_2dkcpU9smrHw.PnEeHbKRsYiALwrJQPWmGwv_i_CT321eVyUP-Q-khMKqMIPgwOG3i27VUEiZaPJTYDAjfaDB6edOcnlWf7tlVw","clickOnPill":"publication.PublicationFigures.html?_sg=qNTTN_xyTrCuFJt5O561lBna31jhu4L6V71CSDMZXq57psYPVffg8n69DYWmNkjI5QfN_EwZz6JoltcWNdbTBg.voxrKRCBfrbtFCMlBGjNX7jyMksTcPIaS7itezUT773FxlDM0Gc8S-zMH61ywjFgyw4nM8BwddcUSg-_fgDYEw"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMaurizio_Filippone%2Fpublication%2F278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes%2Flinks%2F558a857008aee1fc9174ea84.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=bWSsUXJuBODYjCN7E9hsSqTJ2i_sqUzRzn4NiaCAKjIb_gUcjLsifKCgrVY4uQdylTptfKVGVk5eXZppeUAKwg","urlHash":"c87d63a10d310e2b520ecccd0fed537d","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=meoUT6yiMI_unt1lHq1bU7_iuOJ-xLlLTnFWWQImji9Yl73NB8LUKLN4hAjbT_OdQXHCecUKNnUGfAWUthDOujutnSbf8KthX8_DTQXYgcI.R_2i8pA-Rj_K1jro_KzEgS32vWueeoLIO6lf1gkgXKevI3cRbopNsCNPHkUfSpVi-tFg3B3q_sW2vbbdbP8wMQ.jh-yDIXH9jUaVASKZGWs_roJrnE49De1jK9VoiWrOo2Qk2dsqs0q1fwr6V8PzTtzNUSGukkSA0dI1RSKCUunZg","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"558a857008aee1fc9174ea84","trackedDownloads":{"558a857008aee1fc9174ea84":{"v":false,"d":false}},"assetId":"AS:243815227326464@1435141488184","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":278332447,"commentCursorPromo":null,"widgetId":"rgw38_56ab9d4c9a90b"},"id":"rgw38_56ab9d4c9a90b","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMaurizio_Filippone%2Fpublication%2F278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes%2Flinks%2F558a857008aee1fc9174ea84.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A243815227326464%401435141488184&publicationUid=278332447&linkId=558a857008aee1fc9174ea84&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"MCMC for Variationally Sparse Gaussian Processes","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=VEDFyVrYcmr5XBhOISuHi3oNVnCaoaYZxlIrHb2BXF9Nggf5khzAFWHb2g0FhoeDmD9Ylsg6-vGVVh_M3AgVoLg2LQWBuCQreA4Vp7JKCKk.12DoS8M6FDTX_MOmIcvpInd44eRQYq5UHgRA2r7opmOTKkwEB1PwTYBKLum0l1rJOCTzx8w4gryZpL6BsvgcNQ.fOiqWZNqk8gPuVajv_iLcAR03VaZOplsIquYYsoT26axbvwbh5x9RSOCPv6IvZcVP3wx_GU4Sfau5MsAjZ5YVw","publicationUid":278332447,"trackedDownloads":{"558a857008aee1fc9174ea84":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw40_56ab9d4c9a90b"},"id":"rgw40_56ab9d4c9a90b","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw41_56ab9d4c9a90b"},"id":"rgw41_56ab9d4c9a90b","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw42_56ab9d4c9a90b"},"id":"rgw42_56ab9d4c9a90b","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw43_56ab9d4c9a90b"},"id":"rgw43_56ab9d4c9a90b","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw44_56ab9d4c9a90b"},"id":"rgw44_56ab9d4c9a90b","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw39_56ab9d4c9a90b"},"id":"rgw39_56ab9d4c9a90b","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw37_56ab9d4c9a90b"},"id":"rgw37_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab9d4c9a90b"},"id":"rgw2_56ab9d4c9a90b","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":278332447},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=278332447&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab9d4c9a90b"},"id":"rgw1_56ab9d4c9a90b","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"H1ospcsM8r6GK8dQEHJLhA3l1w6d2GqqRyxjCGGMzHgkyZl1c3qreF\/rciH\/qdPjR8JC4agbJoI6NNJKSKoK42MloRZx2CflskFuJGPQg5lKk1cwe+bjqp14DSYbOvjsd8Vy7hjXFLnVfbdPgk7+\/uYqG699\/eWns0Hmbe4VUqXiOvafmvHBH71MsDRwUJCS6JGcERLz7BMrfBJ\/017oQ0vSv46Bz0HorgxOnT5hn786sIyzuHxrz1eCHjGPteTet9SOdyPv0sxkEHQeFjkGh8MXAQX0lMKFzCWKcUVJ6f4=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"MCMC for Variationally Sparse Gaussian Processes\" \/>\n<meta property=\"og:description\" content=\"Gaussian process (GP) models form a core part of probabilistic machine\nlearning. Considerable research effort has been made into attacking three\nissues with GP models: how to compute efficiently...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes\/links\/558a857008aee1fc9174ea84\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes\" \/>\n<meta property=\"rg:id\" content=\"PB:278332447\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"MCMC for Variationally Sparse Gaussian Processes\" \/>\n<meta name=\"citation_author\" content=\"James Hensman\" \/>\n<meta name=\"citation_author\" content=\"Alexander G. de G. Matthews\" \/>\n<meta name=\"citation_author\" content=\"Maurizio Filippone\" \/>\n<meta name=\"citation_author\" content=\"Zoubin Ghahramani\" \/>\n<meta name=\"citation_publication_date\" content=\"2015\/06\/12\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Maurizio_Filippone\/publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes\/links\/558a857008aee1fc9174ea84.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-45ef0063-07b8-43eb-af6c-14a706fcab84","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":509,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw45_56ab9d4c9a90b"},"id":"rgw45_56ab9d4c9a90b","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-45ef0063-07b8-43eb-af6c-14a706fcab84", "d1c9f67985048b935fc512f5a9bbbd5129ef9219");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-45ef0063-07b8-43eb-af6c-14a706fcab84", "d1c9f67985048b935fc512f5a9bbbd5129ef9219");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw46_56ab9d4c9a90b"},"id":"rgw46_56ab9d4c9a90b","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes","requestToken":"PtF1V9dW+6qvL568bxiRp8aANXv7J5QjGfVKrUsCgOYV8RJmymVUfehQnWz7orjFkH21JvERN9HCdiw5qTVbzSuP9df01\/V4zYhz0bGGSBCqCkpREnq0kertLA3uAGN34A0WOqfAEQAdIV\/2y3t7MqDs1EYo8Gq\/H29CM5v6eW7FIQyMg2\/8gKm+FjhfMIRuUb+RzXQ0AimZSzH1JGMczcL2M\/2GHByGh8MUQHBrqV7GURy6gLVWYDXYQh4T84V0UvWOzaimXYAlJdKq3L7GtqeHws+fFoLnhUEy\/3tkPZg=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=xFrAHJJA5RdEvoEgpIrU9vj_72BAAu0U-H64Vm8EjgCm2nSa4daSFCezYtGKKIY0","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjc4MzMyNDQ3X01DTUNfZm9yX1ZhcmlhdGlvbmFsbHlfU3BhcnNlX0dhdXNzaWFuX1Byb2Nlc3Nlcw%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw48_56ab9d4c9a90b"},"id":"rgw48_56ab9d4c9a90b","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw47_56ab9d4c9a90b"},"id":"rgw47_56ab9d4c9a90b","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw49_56ab9d4c9a90b"},"id":"rgw49_56ab9d4c9a90b","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
