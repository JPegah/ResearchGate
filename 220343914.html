<!DOCTYPE html> <html lang="en" class="" id="rgw51_56ab1df743862"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="eaNxojqssxNSXNvDaHSOUXXLwwzP4/EIfZ+mwcNPQtbFxFnO1WaLFVW4wsRkgVadd2nF3ITzQLCWz2k7yhpska9gwNPTBayKdyBBbgAvpS04hS01oSDi9EhVDeii1NXJ9xR/G6WCYX3PUnTeOoipI/zv1/iv3OMhRpIohpgiIcn8Xf2ApnHo1wxxz//mXk2IgygR//s1N7SxPx1fjWyVVyG3hWXaP1fT7pORgwc5Tzb32RbXrWbAd3IimZBcIiUsJ+frwyLbDxE9xcBl+EfBPMuk58tWmEU5dvyF4Pjk0tU="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-06212ce2-6dfe-47e6-b61f-0dfa36376db2",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/220343914_Soft_Margins_for_AdaBoost" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Soft Margins for AdaBoost" />
<meta property="og:description" content="Recently ensemble methods like ADABOOST have been applied successfully in many problems, while seemingly defying the problems of overfitting.
ADABOOST rarely overfits in the low noise regime,..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/220343914_Soft_Margins_for_AdaBoost/links/0046352aaf02269b25000000/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/220343914_Soft_Margins_for_AdaBoost" />
<meta property="rg:id" content="PB:220343914" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1023/A:1007618119488" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Soft Margins for AdaBoost" />
<meta name="citation_author" content="Gunnar Rätsch" />
<meta name="citation_author" content="Takashi Onoda" />
<meta name="citation_author" content="Klaus-Robert Müller" />
<meta name="citation_publication_date" content="2001/03/01" />
<meta name="citation_journal_title" content="Machine Learning" />
<meta name="citation_issn" content="0885-6125" />
<meta name="citation_volume" content="42" />
<meta name="citation_issue" content="3" />
<meta name="citation_firstpage" content="287" />
<meta name="citation_lastpage" content="320" />
<meta name="citation_doi" content="10.1023/A:1007618119488" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Klaus-Robert_Mueller/publication/220343914_Soft_Margins_for_AdaBoost/links/0046352aaf02269b25000000.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/220343914_Soft_Margins_for_AdaBoost" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/220343914_Soft_Margins_for_AdaBoost" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/215868066921738/styles/pow/publicliterature/FigureList.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Soft Margins for AdaBoost (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Soft Margins for AdaBoost on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1df743862" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1df743862" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab1df743862">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1023%2FA%3A1007618119488&rft.atitle=Soft%20Margins%20for%20AdaBoost&rft.title=Machine%20Learning&rft.jtitle=Machine%20Learning&rft.volume=42&rft.issue=3&rft.date=2001&rft.pages=287-320&rft.issn=0885-6125&rft.au=Gunnar%20R%C3%A4tsch%2CTakashi%20Onoda%2CKlaus-Robert%20M%C3%BCller&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Soft Margins for AdaBoost</h1> <meta itemprop="headline" content="Soft Margins for AdaBoost">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/220343914_Soft_Margins_for_AdaBoost/links/0046352aaf02269b25000000/smallpreview.png">  <div id="rgw8_56ab1df743862" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56ab1df743862" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Gunnar_Raetsch" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Gunnar Rätsch" alt="Gunnar Rätsch" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Gunnar Rätsch</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw10_56ab1df743862">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">   <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Private Profile" height="90px" width="90px"/>  </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">   Private Profile   </h5> <div class="truncate-single-line meta">   </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56ab1df743862"> <a href="researcher/69960989_Takashi_Onoda" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Takashi Onoda" alt="Takashi Onoda" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Takashi Onoda</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56ab1df743862">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/69960989_Takashi_Onoda"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Takashi Onoda" alt="Takashi Onoda" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/69960989_Takashi_Onoda" class="display-name">Takashi Onoda</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw13_56ab1df743862" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Klaus-Robert_Mueller" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Klaus-Robert Müller" alt="Klaus-Robert Müller" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Klaus-Robert Müller</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw14_56ab1df743862" data-account-key="Klaus-Robert_Mueller">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Klaus-Robert_Mueller"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Klaus-Robert Müller" alt="Klaus-Robert Müller" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Klaus-Robert_Mueller" class="display-name">Klaus-Robert Müller</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Technische_Universitaet_Berlin" title="Technische Universität Berlin">Technische Universität Berlin</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">  <div> University of Potsdam </div>      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/0885-6125_Machine_Learning"><span itemprop="name">Machine Learning</span></a> </span>    (Impact Factor: 1.89).     <meta itemprop="datePublished" content="2001-03">  03/2001;  42(3):287-320.    DOI:&nbsp;10.1023/A:1007618119488           <div class="pub-source"> Source: <a href="http://dblp.uni-trier.de/db/journals/ml/ml42.html#RatschOM01" rel="nofollow">DBLP</a> </div>  </div> <div id="rgw15_56ab1df743862" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>Recently ensemble methods like ADABOOST have been applied successfully in many problems, while seemingly defying the problems of overfitting.<br />
ADABOOST rarely overfits in the low noise regime, however, we show that it clearly does so for higher noise levels. Central to the understanding of this fact is the margin distribution. ADABOOST can be viewed as a constraint gradient descent in an error function with respect to the margin. We find that ADABOOST asymptotically achieves a hard margin distribution, i.e. the algorithm concentrates its resources on a few hard-to-learn patterns that are interestingly very similar to Support Vectors. A hard margin is clearly a sub-optimal strategy in the noisy case, and regularization, in our case a &ldquo;mistrust&rdquo; in the data, must be introduced in the algorithm to alleviate the distortions that single difficult patterns (e.g. outliers) can cause to the margin distribution. We propose several regularization methods and generalizations of the original ADABOOST algorithm to achieve a soft margin. In particular we suggest (1) regularized ADABOOSTREG where the gradient decent is done directly with respect to the soft margin and (2) regularized linear and quadratic programming (LP/QP-) ADABOOST, where the soft margin is attained by introducing slack variables.<br />
Extensive simulations demonstrate that the proposed regularized ADABOOST-type algorithms are useful and yield competitive results for noisy data.</div> </p>  </div>  </div>   </div>     <div id="rgw16_56ab1df743862" class="figure-carousel"> <div class="carousel-hd"> Figures in this publication </div> <div class="carousel-bd"> <ul class="clearfix">  <li> <a href="/figure/220343914_fig1_Figure-1" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 1 . " data-key="220343914_fig1_Figure-1"> <img class="fig" src="https://www.researchgate.net/profile/Klaus-Robert_Mueller/publication/220343914/figure/fig1/Figure-1_small.png" alt="Figure 1 . " title="Figure 1 . "/> </a> </li>  <li> <a href="/figure/220343914_fig2_Figure-2" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 2 . " data-key="220343914_fig2_Figure-2"> <img class="fig" src="https://www.researchgate.net/profile/Klaus-Robert_Mueller/publication/220343914/figure/fig2/Figure-2_small.png" alt="Figure 2 . " title="Figure 2 . "/> </a> </li>  <li> <a href="/figure/220343914_fig3_Figure-3" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 3 . " data-key="220343914_fig3_Figure-3"> <img class="fig" src="https://www.researchgate.net/profile/Klaus-Robert_Mueller/publication/220343914/figure/fig3/Figure-3_small.png" alt="Figure 3 . " title="Figure 3 . "/> </a> </li>  <li> <a href="/figure/220343914_fig4_Figure-4" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 4 . " data-key="220343914_fig4_Figure-4"> <img class="fig" src="https://www.researchgate.net/profile/Klaus-Robert_Mueller/publication/220343914/figure/fig4/Figure-4_small.png" alt="Figure 4 . " title="Figure 4 . "/> </a> </li>  <li> <a href="/figure/220343914_fig5_Figure-5" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 5 . " data-key="220343914_fig5_Figure-5"> <img class="fig" src="https://www.researchgate.net/profile/Klaus-Robert_Mueller/publication/220343914/figure/fig5/Figure-5_small.png" alt="Figure 5 . " title="Figure 5 . "/> </a> </li>  <li> <a href="/figure/220343914_fig6_Figure-6" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 6 . " data-key="220343914_fig6_Figure-6"> <img class="fig" src="https://www.researchgate.net/profile/Klaus-Robert_Mueller/publication/220343914/figure/fig6/Figure-6_small.png" alt="Figure 6 . " title="Figure 6 . "/> </a> </li>  <li> <a href="/figure/220343914_fig7_Figure-7" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 7 . " data-key="220343914_fig7_Figure-7"> <img class="fig" src="https://www.researchgate.net/profile/Klaus-Robert_Mueller/publication/220343914/figure/fig7/Figure-7_small.png" alt="Figure 7 . " title="Figure 7 . "/> </a> </li>  <li> <a href="/figure/220343914_fig8_Figure-8" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 8 . " data-key="220343914_fig8_Figure-8"> <img class="fig" src="https://www.researchgate.net/profile/Klaus-Robert_Mueller/publication/220343914/figure/fig8/Figure-8_small.png" alt="Figure 8 . " title="Figure 8 . "/> </a> </li>  <li> <a href="/figure/220343914_fig9_Figure-9" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 9 . " data-key="220343914_fig9_Figure-9"> <img class="fig" src="https://www.researchgate.net/profile/Klaus-Robert_Mueller/publication/220343914/figure/fig9/Figure-9_small.png" alt="Figure 9 . " title="Figure 9 . "/> </a> </li>  <li> <a href="/figure/220343914_fig10_Figure-10" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 10 . " data-key="220343914_fig10_Figure-10"> <img class="fig" src="https://www.researchgate.net/profile/Klaus-Robert_Mueller/publication/220343914/figure/fig10/Figure-10_small.png" alt="Figure 10 . " title="Figure 10 . "/> </a> </li>  <li> <a href="/figure/220343914_fig11_Figure-11-Margin-distribution-graphs-of-the-RBF-base-hypothesis-scaled-trained-with" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 11 . Margin distribution graphs of the RBF base hypothesis..." data-key="220343914_fig11_Figure-11-Margin-distribution-graphs-of-the-RBF-base-hypothesis-scaled-trained-with"> <img class="fig" src="https://www.researchgate.net/profile/Klaus-Robert_Mueller/publication/220343914/figure/fig11/Figure-11-Margin-distribution-graphs-of-the-RBF-base-hypothesis-scaled-trained-with_small.png" alt="Figure 11 . Margin distribution graphs of the RBF base hypothesis..." title="Figure 11 . Margin distribution graphs of the RBF base hypothesis..."/> </a> </li>  <li> <a href="/figure/220343914_fig12_Figure-12" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 12 . " data-key="220343914_fig12_Figure-12"> <img class="fig" src="https://www.researchgate.net/profile/Klaus-Robert_Mueller/publication/220343914/figure/fig12/Figure-12_small.png" alt="Figure 12 . " title="Figure 12 . "/> </a> </li>  <li> <a href="/figure/220343914_fig13_Figure-13" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 13 . " data-key="220343914_fig13_Figure-13"> <img class="fig" src="https://www.researchgate.net/profile/Klaus-Robert_Mueller/publication/220343914/figure/fig13/Figure-13_small.png" alt="Figure 13 . " title="Figure 13 . "/> </a> </li>  <li> <a href="/figure/220343914_fig14_Figure-14" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 14 . " data-key="220343914_fig14_Figure-14"> <img class="fig" src="https://www.researchgate.net/profile/Klaus-Robert_Mueller/publication/220343914/figure/fig14/Figure-14_small.png" alt="Figure 14 . " title="Figure 14 . "/> </a> </li>  <li> <a href="/figure/220343914_fig15_Figure-15-Pseudo-code-description-of-the-RBF-net-algorithm-which-is-used-as-base" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 15 . Pseudo-code description of the RBF net algorithm, which is..." data-key="220343914_fig15_Figure-15-Pseudo-code-description-of-the-RBF-net-algorithm-which-is-used-as-base"> <img class="fig" src="https://www.researchgate.net/profile/Klaus-Robert_Mueller/publication/220343914/figure/fig15/Figure-15-Pseudo-code-description-of-the-RBF-net-algorithm-which-is-used-as-base_small.png" alt="Figure 15 . Pseudo-code description of the RBF net algorithm, which is..." title="Figure 15 . Pseudo-code description of the RBF net algorithm, which is..."/> </a> </li>  </ul> </div> </div> <div class="action-container"> <div id="rgw17_56ab1df743862" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw31_56ab1df743862">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw43_56ab1df743862">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Klaus-Robert_Mueller/publication/220343914_Soft_Margins_for_AdaBoost/links/0046352aaf02269b25000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Klaus-Robert_Mueller">Klaus-Robert Müller</a>, <span class="js-publication-date"> Dec 13, 2013 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw45_56ab1df743862" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw46_56ab1df743862" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw47_56ab1df743862" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw48_56ab1df743862" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw49_56ab1df743862" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw50_56ab1df743862" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw44_56ab1df743862" src="https://www.researchgate.net/c/o1o9o3/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FKlaus-Robert_Mueller%2Fpublication%2F220343914_Soft_Margins_for_AdaBoost%2Flinks%2F0046352aaf02269b25000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw30_56ab1df743862"  itemprop="articleBody">  <p>Page 1</p> <p>Machine Learning, 42, 287–320, 2001<br />c ? 2001 Kluwer Academic Publishers. Manufactured in The Netherlands.<br />Soft Margins for AdaBoost<br />G. R¨ATSCH<br />GMD FIRST,∗Kekul´ estr. 7, 12489 Berlin, Germany<br />raetsch@first.gmd.de<br />T. ONODA<br />CRIEPI,†Komae-shi, 2-11-1 Iwado Kita, Tokyo, Japan<br />onoda@criepi.denken.or.jp<br />K.-R. M¨ULLER<br />GMD FIRST, Kekul´ estr. 7, 12489 Berlin, Germany; University of Potsdam,∗Neues Palais 10,<br />14469 Potsdam, Germany<br />klaus@first.gmd.de<br />Editor: Robert Schapire<br />Abstract.<br />while seemingly defying the problems of overfitting.<br />ADABOOST rarely overfits in the low noise regime, however, we show that it clearly does so for higher noise<br />levels.Centraltotheunderstandingofthisfactisthemargindistribution.ADABOOSTcanbeviewedasaconstraint<br />gradientdescentinanerrorfunctionwithrespecttothemargin.WefindthatADABOOSTasymptoticallyachieves<br />a hard margin distribution, i.e. the algorithm concentrates its resources on a few hard-to-learn patterns that are<br />interestingly very similar to Support Vectors. A hard margin is clearly a sub-optimal strategy in the noisy case, and<br />regularization, in our case a “mistrust” in the data, must be introduced in the algorithm to alleviate the distortions<br />that single difficult patterns (e.g. outliers) can cause to the margin distribution. We propose several regularization<br />methods and generalizations of the original ADABOOST algorithm to achieve a soft margin. In particular we<br />suggest (1) regularized ADABOOSTREGwhere the gradient decent is done directly with respect to the soft margin<br />and (2) regularized linear and quadratic programming (LP/QP-) ADABOOST, where the soft margin is attained<br />by introducing slack variables.<br />Extensive simulations demonstrate that the proposed regularized ADABOOST-type algorithms are useful and<br />yield competitive results for noisy data.<br />Recently ensemble methods like ADABOOST have been applied successfully in many problems,<br />Keywords:<br />ADABOOST, arcing, large margin, soft margin, classification, support vectors<br />1. Introduction<br />Boosting and other ensemble1learning methods have been recently used with great success<br />in applications like OCR (Schwenk &amp; Bengio, 1997; LeCun et al., 1995). But so far the<br />reduction of the generalization error by Boosting algorithms has not been fully understood.<br />For low noise cases Boosting algorithms are performing well for good reasons (Schapire<br />et al., 1997; Breiman, 1998). However, recent studies with highly noisy patterns (Quinlan,<br />1996; Grove &amp; Schuurmans, 1998; R¨ atsch et al., 1998) showed that it is clearly a myth that<br />Boosting methods do not overfit.<br />∗www.first.gmd.de<br />†criepi.denken.or.jp<br />∗www.uni-potsdam.de</p>  <p>Page 2</p> <p>288<br />G. R¨ATSCH, T. ONODA AND K.-R. M¨ULLER<br />In this work, we try to gain insight into these seemingly contradictory results for the low<br />and high noise regime and we propose improvements of ADABOOST that help to achieve<br />noise robustness.<br />Duetotheirsimilarity,wewillreferinthefollowingtoADABOOST(Freund&amp;Schapire,<br />1994) and unnormalized Arcing (Breiman, 1997b) (with exponential function) as<br />ADABOOST-typealgorithms(ATA).InSection2wegiveanasymptoticalanalysisofATAs.<br />We find that the error function of ATAs can be expressed in terms of the margin and that in<br />every iteration ADABOOST tries to minimize this error by a stepwise maximization of the<br />margin (see also Breiman, 1997a; Frean &amp; Downs, 1998; Friedman, Hastie, &amp; Tibshirani,<br />1998; Onoda, R¨ atsch, &amp; M¨ uller, 1998; R¨ atsch, 1998). As a result of the asymptotical anal-<br />ysis of this error function, we introduce the hard margin concept and show connections to<br />Support Vector (SV) learning (Boser, Guyon, &amp; Vapnik, 1992) and to linear programming<br />(LP). Bounds on the size of the margin are also given.<br />In Section 3 we explain why an ATA that enforces a hard margin in training will overfit<br />for noisy data or overlapping class distributions. So far, we only know what a margin distri-<br />bution to achieve optimal classification in the no-noise case should look like: a large hard<br />margin is clearly a good choice (Vapnik, 1995). However, for noisy data there is always the<br />tradeoff between “believing” in the data or “mistrusting” it, as the very data point could<br />be mislabeled or an outlier. So we propose to relax the hard margin and to regularize by<br />allowing for misclassifications (soft margin). In Section 4 we introduce such a regulariza-<br />tion strategy to ADABOOST and subsequently extend the LP-ADABOOST algorithm of<br />Grove and Schuurmans (1998) by slack variables to achieve soft margins. Furthermore, we<br />proposeaquadraticprogrammingADABOOSTalgorithm(QP-ADABOOST)andshowits<br />connections to SUPPORT VECTOR MACHINES (SVMs).<br />Finally, in Section 5 numerical experiments on several artificial and real-world data sets<br />show the validity and competitiveness of our regularized Boosting algorithms. The paper<br />concludes with a brief discussion.<br />2.Analysis of ADABOOST’s learning process<br />2.1.Algorithm<br />Let {ht(x) : t = 1,...,T} be an ensemble of T hypotheses defined on an input vector<br />x ∈ X and let c = [c1· · · cT] be their weights satisfying ct ≥ 0 and?T<br />however,canbetransferedeasilytoclassificationwithmorethantwoclasses(e.g.Schapire,<br />1999; Schapire &amp; Singer, 1998; Breiman, 1997b).<br />The ensemble generates the label ˜f (x) ≡ ˜fT(x) which is the weighted majority of the<br />votes, where<br />t=1ct = 1. We<br />will consider only the binary classification case in this work, i.e. ht(x) = ±1; most results,<br />fT(x) :=<br />T ?<br />t=1<br />ctht(x)<br />and<br />˜fT(x) := sign( fT(x)).<br />In order to train the ensemble, i.e. to find T appropriate hypotheses {ht(x)} and the</p>  <p>Page 3</p> <p>SOFT MARGINS FOR ADABOOST<br />289<br />weights c for the convex combination, several algorithms have been proposed: popular<br />ones are WINDOWING (Quinlan, 1992), BAGGING (Breiman, 1996), ADABOOST (Freund<br />&amp; Schapire, 1994), ARC-X4 (Breiman, 1998) and ARC-GV (Breiman, 1997b). In the<br />sequel analysis, we will focus on ADABOOST-type algorithms and give their pseudo-<br />code in figure 1 (further details can be found in e.g. Freund &amp; Schapire, 1994; Breiman,<br />1997b).<br />Inthebinaryclassificationcase,wedefinethemarginforaninput-outputpairzi= (xi, yi)<br />as<br />ρ(zi,c) = yif (xi) = yi<br />T ?<br />t=1<br />cthr(xi),<br />(1)<br />Figure 1.<br />rithm (Freund &amp; Schapire, 1994). The ATA is a specialization of unnormalized Arcing (Breiman, 1997a) (with<br />exponential function).<br />The ADABOOST-type algorithm (ATA). For φ =<br />1<br />2, we retrieve the original ADABOOST algo-</p>  <p>Page 4</p> <p>290<br />G. R¨ATSCH, T. ONODA AND K.-R. M¨ULLER<br />wherei = 1,...,l andl denotesthenumberoftrainingpatterns.Themarginatzispositive<br />if the correct class label of the pattern is predicted. As the positivity of the margin value<br />increases, the decision stability becomes larger. Moreover, if |c| :=?T<br />b (instead of c) which denotes simply an unnormalized version of c, i.e. usually |b| ?= 1<br />(cf. (F1.2) and (F1.4) in figure 1). Note that the edge (cf. Breiman, 1997b) is just an affine<br />transformation of the margin.<br />The margin ?(c) of a classifier (instance) is defined as the smallest margin of a pattern<br />over the training set, i.e.<br />t=1ct = 1, then<br />ρ(zi,c) ∈ [−1,1]. We will sometimes for convenience also use a margin definition with<br />?(c) = min<br />Figure 2 illustrates the functioning of ADABOOST. Patterns that are misclassified get<br />higher weights in the next iteration. The patterns near the decision boundary are usually<br />harder to classify and therefore get high weights after a few iterations.<br />i=1,...,lρ(zi,c).<br />2.2. Error function of ADABOOST<br />An important question in the analysis of ATAs is: what kind of error function is optimized?<br />From the algorithmic formulation (cf. figure 1), it is not straight forward to see what the<br />Figure 2.<br />the class label) is proportional to the weight that the pattern gets in the first, second, third, 5th, 10th and 100th<br />iteration. The dash-dotted lines show the decision boundaries of the single classifiers (up to the 5th iteration). The<br />solid line shows the decision line of the combined classifier. In the last two plots the decision line of BAGGING is<br />plotted for a comparison (dotted).<br />Illustration of ADABOOST on a 2D toy data set: The diameter of the points (the brightness gives</p>  <p>Page 5</p> <p>SOFT MARGINS FOR ADABOOST<br />291<br />aim of this algorithm is. So to gain a better understanding why one should use the weights<br />of the hypotheses ctand of the patterns wt(zi) in the manner of Eqs. (F1.2) and (F1.3), let<br />us study the following three statements<br />1. The weights wt(zi) in the t-th iteration are chosen such that the previous hypothesis has<br />exactly a weighted training error ? of 1/2 (Schapire et al., 1997).<br />2. The weight bt(and ct) of a hypothesis is chosen such that it minimizes a functional G<br />first introduced by Breiman (1997b) (see also R¨ atsch, 1998; Mason, Bartlett, &amp; Baxter,<br />2000a;Onodaetal.,1998;Friedmanetal.,1998;Frean&amp;Downs,1998).Thisfunctional<br />depends on the margins of all patterns and is defined by<br />G(bt,bt−1) =<br />l ?<br />i=1<br />exp<br />?<br />−ρ(zi,bt) + |bt|<br />?1<br />2− φ<br />??<br />,<br />(2)<br />where φ is a constant (cf. figure 1). This functional can be minimized analytically<br />(Breiman, 1997b) and one gets the explicit form of Eq. (F1.2) as a solution of<br />∂G(bt,bt−1)<br />∂bt<br />=0.<br />3. To train the t-th hypothesis (step 1 in figure 1) we can either use bootstrap replicates<br />of the training set (sampled according to wt) or minimize a weighted error function for<br />the base learning algorithm. We observed that the convergence of the ATA is faster if a<br />weighted error function is used.<br />Taking a closer look at the definition of G, one finds that the computation of the sample<br />distribution wt(cf. Eq. (F1.3)) can be derived directly from G. Assuming that G is the error<br />function which is minimized by the ATA, then G essentially defines a loss function over<br />margin distributions, which depends on the value of |b|. The larger the margins ρ(zi), the<br />smaller will be the value of G.<br />So, the gradient<br />marginmoststronglyinordertodecreaseG maximally(gradientdescent).Thisinformation<br />canthenbeusedtocomputeare-weightingofthesampledistributionwtfortrainingthenext<br />hypothesis ht. If it is important to increase the margin of a pattern zi, then its weight wt(zi)<br />shouldbehigh—otherwiselow(becausethedistributionwtsumstoone).Interestingly,this<br />is exactly what ATAs are doing and we arrive at the following lemma (Breiman, 1997b;<br />R¨ atsch, 1998):<br />∂G<br />∂ρ(zi)gives an answer to the question, which pattern should increase its<br />Lemma 1.<br />alent to normalizing the gradient of G(bt+1,bt) with respect to ρ(zi,bt), i.e.<br />?<br />j=1<br />The computation of the pattern distribution wt+1in the t-th iteration is equiv-<br />wt+1(zi) =∂G(bt+1,bt)<br />∂ρ(zi,bt)<br />l ?<br />∂G(bt+1,bt)<br />∂ρ(zj,bt)<br />.<br />(3)<br />The proof can found in Appendix A.<br />From Lemma 1, the analogy to a gradient descent method is (almost) complete. In a<br />gradient descent method, the first step is to compute the gradient of the error function with</p>  <p>Page 6</p> <p>292<br />G. R¨ATSCH, T. ONODA AND K.-R. M¨ULLER<br />respect to the parameters which are to be optimized: this corresponds to computing the<br />gradient of G with respect to the margins. The second step is to determine the step size in<br />gradient direction (usually done by a line-search): this is analogous to the minimization of<br />G with respect to bt(cf. point 2).<br />Therefore,ATAscanberelatedtoagradientdescentmethodinahypothesis(orfunction)<br />space H which is determined by the structure of the base learning algorithm, i.e. ATAs aim<br />to minimize the functional G by constructing an ensemble of classifiers (Onoda et al.,<br />1998; R¨ atsch, 1998; Mason et al., 2000b; Friedman et al., 1998; Friedman, 1999). This also<br />explains point 1 in the list above, as in a standard gradient descent method, a new search<br />direction is usually chosen perpendicular to the previous one.<br />In the ADABOOST-type algorithm, the gradients are found by changing the weights of<br />the training patterns, and there are essentially two ways of incorporating the re-weighting<br />into the boosting procedure. The first is to create bootstrap replicates sampled according to<br />the pattern weightings, which usually induces strong random effects that hide the “true” in-<br />formation contained in the pattern weightings. The second and more direct way is to weight<br />the error function and use weighted minimization (Breiman, 1997b). Clearly, weighted<br />minimization is more efficient in terms of the number of boosting iterations than the boot-<br />strap approach.2In fact, it can be shown that employing weighted minimization (Breiman,<br />1997b) for finding the next hypothesis in each iteration leads to the best (single) hypothesis<br />for minimizing G (Mason et al., 2000a), i.e. adding the hypothesis with smallest weighted<br />training error ?twill lead to the smallest value of G and therefore to a fast convergence.<br />This reasoning explains the third statement.<br />2.3.ADABOOST as an annealing process<br />From the definition of G and ρ(zi,bt), Eq. (3) can be rewritten as<br />wt+1(zi) =<br />exp?−1<br />2ρ(zi,ct)?|bt|<br />?l<br />j=1exp?−1<br />2ρ(zj,ct)?|bt|,<br />(4)<br />where we emphasize that |bt| can be written in the exponent. Inspecting this equation more<br />closely, we see that ATA uses a soft-max function (e.g. Bishop, 1995) with parameter |bt|<br />that we would like to interpret as an annealing parameter (Onoda et al., 1998; Onoda,<br />R¨ atsch, &amp; M¨ uller, 2000). In the beginning |bt| is small and all patterns have almost the<br />same weights (if |bt| = 0 then all weights are the same). As |bt| increases, the patterns<br />with smallest margin will get higher and higher weights. In the limit of large |bt|, we arrive<br />at the maximum function: Only the pattern(s) with the smallest margin will be taken into<br />account for learning and get a non-zero weight.<br />Note that in the limit for |bt| → ∞, a simple rescaling of the error function G(bt) gives<br />the minimum margin ?(zi,ct), i.e. ?(zi,ct) = −lim|bt|→∞<br />The following lemma shows that under usual circumstances, the length of the hypothesis<br />weight vector |bt| increases at least linearly with the number of iterations.<br />1<br />|bt|logG(bt).</p>  <p>Page 7</p> <p>SOFT MARGINS FOR ADABOOST<br />293<br />Lemma 2.<br />errors ?tare bounded by ?t≤ φ −? (0 &lt; ? &lt; φ), then |b| increases at least linearly with<br />the number of iterations t.<br />If, in the learning process of an ATA with 0 &lt; φ &lt; 1, all weighted training<br />Proof:<br />have bt = log<br />φ(1 − φ) &gt; ?(1 − φ) we get φ(1 − φ + ?) &gt; ? and hence also q − ? &gt; 0. Therefore,<br />the smallest value of btis log<br />on φ and ?. Thus, we have |bt| &gt; tγ.<br />With (F1.2), the smallest value for bt is achieved, if ?t = φ − ?. Then we<br />q<br />q−?, where q := φ(1 − φ + ?). We find q &gt; ? &gt; 0 and because of<br />q<br />q−?is always larger than a constant γ, which only depends<br />2<br />If the annealing speed is low, the achieved solution should have larger margins than for<br />a high speed annealing strategy. This holds for similar reasons as for a standard annealing<br />process (Kirkpatrick, 1984): in the error surface, a better local minimum (if exist) can be<br />obtained locally, if the annealing speed is slow enough. From Eq. (F1.2), we observe that<br />if the training error ?ttakes a small value, btbecomes large. So, strong learners can reduce<br />their training errors strongly and will make |b| large after only a few ATA iterations, i.e.<br />the asymptotics, where the addition of a new ensemble member does not change the result,<br />is reached faster. To reduce the annealing speed either φ or the complexity of the base<br />hypotheses has to be decreased (with the constraint ?t&lt; φ − ?; cf. Onoda et al., 1998).<br />In figure 3 (left), the ADABOOST Error (φ =1<br />Kullback-Leibler Error ln ρ(z)/ln 2 are plotted. Interestingly, the Squared and Kullback-<br />Leibler Error are very similar to the error function of ADABOOST for |b| = 3. As |b|<br />increases,3the ATA error function approximates a 0/∞ loss (patterns with margin smaller<br />than1−2φ getloss∞;allothershaveloss0)andboundsthe0/1lossat1−2φ fromabove.<br />2), the Squared Error (y − f (x))2and the<br />Figure 3.<br />and the y-coordinate shows the monotone loss for that pattern: 0/1-Loss, Squared Error, Kullback-Leibler Error<br />and ADABOOST Error (cf. Eq. (2)), where |b| is either 3, 5, 10 or 100 (in reading order). On the left panel is<br />φ = 1/2 and on the right plot φ is either 1/3 or 2/3. The step position of the 0/∞ loss, which is approximated<br />for |b| → ∞, is determined by φ.<br />Different loss functions for classification (see text). The abscissa shows the margin yf (x) of a pattern</p>  <p>Page 8</p> <p>294<br />G. R¨ATSCH, T. ONODA AND K.-R. M¨ULLER<br />Figure 3 (right) shows the different offsets of the step seen in the ATA Error (|b| → ∞).<br />They result from different values of φ (here φ is either 1/3 or 2/3).<br />2.4. Asymptotic analysis<br />2.4.1.Howlargeisthemargin? ATA’sgoodgeneralizationperformancecanbeexplained<br />intermsofthesizeofthe(hard)marginthatcanbeachieved(Schapireetal.,1997;Breiman,<br />1997b):forlownoise,thehypothesiswiththelargestmarginwillhavethebestgeneralization<br />performance(Vapnik,1995;Schapireetal.,1997).Thus,itisinterestingtounderstandwhat<br />the margin size depends on.<br />Generalizing Theorem 5 of Freund and Schapire (1994) to the case φ ?=1<br />2we get<br />Theorem 3.<br />are generated by running an ATA and 1 &gt; φ &gt; maxt=1,...,T?t. Then the following<br />inequality holds for all θ ∈ [−1,1]:<br />Assume, ?1,...,?Tare the weighted classification errors of h1,...,hTthat<br />1<br />l<br />l ?<br />i=1<br />I(yif (xi) ≤ θ) ≤?ϕ<br />1+θ<br />2 + ϕ−1−θ<br />2?T<br />T ?<br />t=1<br />?<br />?1−θ<br />t<br />(1 − ?t)1+θ,<br />(5)<br />where f is the final hypothesis and ϕ =<br />φ<br />1−φ, where I is the indicator function.<br />The proof can be found in Appendix B.<br />Corollary4.<br />gin distributions with a margin ?, which is bounded from below by<br />AnADABOOST-typealgorithmwillasymptotically(t → ∞)generatemar-<br />? ≥ln(φ?−1) + ln((1 − φ)(1 − ?)−1)<br />ln(φ?−1) − ln((1 − φ)(1 − ?)−1),<br />where ? = maxt?t, if ? ≤ (1 − ?)/2 is satisfied.<br />The maximum of ?1−θ<br />for 0 ≤ ?t≤1<br />? in Eq. (5) for θ ≤ ?:<br />P(x,y)∼Z[yf (x) ≤ θ] ≤??ϕ<br />If the basis on the right hand side is smaller than 1, then asymptotically we have P(x,y)∼Z<br />[yf (x) ≤ θ] = 0; this means that asymptotically, there is no example that has a smaller<br />margin than θ, for any θ &lt; 1. The supremum over all θ such that the basis is less than<br />1,θmaxis described by<br />(6)<br />Proof:<br />t<br />(1 − ?t)1+θwith respect to ?tis obtained for1<br />2(1 − θ) it is increasing monotonically in ?t. Therefore, we can replace ?tby<br />2(1 − θ) and<br />1+θ<br />2 + ϕ−1−θ<br />2??<br />1−θ<br />2(1 − ?)<br />1+θ<br />2?T.<br />?ϕ<br />1+θmax<br />2<br />+ ϕ−1−θmax<br />2<br />??<br />1−θmax<br />2<br />(1 − ?)<br />1+θmax<br />2<br />= 1.</p>  <p>Page 9</p> <p>SOFT MARGINS FOR ADABOOST<br />295<br />We can solve this equation to obtain θmax<br />θmax=ln(φ?−1) + ln((1 − φ)(1 − ?)−1)<br />ln(φ?−1) − ln((1 − φ)(1 − ?)−1).<br />We get the assertion because ? is always larger or equal θmax.<br />2<br />From Eq. (6) we can see the interaction between φ and ?: if the difference between ? and<br />φ is small, then the right hand side of (6) is small. The smaller φ is, the more important<br />this difference is. From Theorem 7.2 of Breiman (1997b) we also have the weaker bound<br />? ≥ 1 − 2φ and so, if φ is small then ? must be large, i.e. choosing a small φ results in a<br />larger margin on the training patterns. On the other hand, an increase of the complexity of<br />the basis algorithm leads to an increased ?, because the error ?twill decrease.<br />2.4.2. Support patterns.<br />is predominantly achieved by improvements of the margin ρ(zi,c). If the margin ρ(zi,c) is<br />negative,thentheerrorG(c,|b|)clearlytakesalargevalue,amplifiedby|b|intheexponent.<br />So, ATA tries to decrease the negative margins most efficiently in order to improve the error<br />G(c,|b|).<br />Now let us consider the asymptotic case, where the number of iterations and therefore<br />|b| take large values (cf. Lemma 2). Here, the margins ρ(zi,c) of all patterns z1,...,zl,<br />are almost the same and small differences are amplified strongly in G(c,|b|). For example,<br />given two margins ρ(zi,c) = 0.2 and ρ(zj,c) = 0.3 and |b| = 100, then this difference is<br />amplified exponentially exp{−100×0.2<br />by a factor of e5≈ 150. From Eq. (4) we see that as soon as the annealing parameter |b| is<br />large, the ATA learning becomes a hard competition case: only the patterns with smallest<br />marginwillgethighweights,otherpatternsareeffectivelyneglectedinthelearningprocess.<br />We get the following interesting lemma.<br />A decrease in the functional G(c,|b|) := G(b) (with c = b/|b|)<br />2<br />} = e−10and exp{−100×0.3<br />2<br />} = e−15in G(c,|b|), i.e.<br />Lemma5.<br />of each class will asymptotically converge to the same value, i.e.<br />DuringtheATAlearningprocess,thesmallestmarginofthe(training)patterns<br />lim<br />t→∞min<br />i:yi=1ρ(zi,ct) = lim<br />t→∞min<br />i:yi=−1ρ(zi,ct),<br />(7)<br />if the following assumptions are fulfilled:<br />1. the weight of each hypothesis is bounded from below and above by<br />0 &lt; γ &lt; bt&lt; ? &lt; ∞,and<br />(8)<br />2. the learning algorithm must (in principle) be able to classify all patterns to one class<br />c ∈ {±1}, if the sum over the weights of patterns of class c is larger than a constant δ,<br />i.e.<br />?<br />The proof can be found in Appendix C.<br />i:yi=c<br />w(zi) &gt; δ ⇒ h(xi) = c<br />(i = 1,...,l).<br />(9)</p>  <p>Page 10</p> <p>296<br />G. R¨ATSCH, T. ONODA AND K.-R. M¨ULLER<br />Figure 4.<br />(solid) with RBF nets (13 centers) as base hypotheses (left); and with 7 (dotted), 13 (dashed), 30 (solid) centers<br />in the base hypotheses for data with σ2= 16% (right) after 104ADABOOST iterations (φ = 1/2). These graphs<br />are an experimental confirmation of the trends expected from Eq. (6).<br />Margin distributions of ADABOOST for different noise levels: σ2= 0% (dotted), 9% (dashed), 16%<br />Note 6.<br />misclassify every single pattern. Effectively, this assumption introduces something like a<br />biastermbthatisautomaticallyadjustedifthesmallestmarginsofoneclassaresignificantly<br />differentfromtheotherclass.InSVMsfortheseparablecase(Boseretal.,1992),bisdirectly<br />computed such that the smallest margins of both classes are the same.<br />Assumption 2 of Lemma 5 ensures, that the classifier is in principle able to<br />Therefore, the ATA learning process converges, under rather mild assumptions, to a<br />solutionwhereasubsetofthetrainingpatternshasasymptoticallythesamesmallestmargin.<br />We call these patterns Support Patterns (SPs) (cf. figure 4).<br />To validate our theoretical analysis we performed numerical simulations on toy data with<br />anasymptoticnumber(104)ofboostingsteps.Thetrainingdatawasgeneratedfromseveral<br />(non-linearly transformed) Gaussian and uniform blobs,4which were additionally distorted<br />by uniformly distributed noiseU(0.0,σ2). In our simulations, we used 300 patterns and σ2<br />is either 0%, 9%, or 16%.<br />In all simulations, radial basis function (RBF) networks with adaptive centers are used<br />as base learners (cf. Appendix D or M¨ uller et al., 1998 for a detailed description). Figure 4<br />showsthemargindistributionsafter104boostingiterationsatdifferentnoiselevelsσ2(left)<br />and for different strengths of the base hypotheses (right). From these figures, it becomes<br />apparent that the margin distribution asymptotically makes a step at a specific margin size<br />and that some subset of the training patterns all have similar margins that correspond to<br />the minimal margin discussed above. If the noise level is high or the complexity of the<br />base hypothesis is low, one gets higher training errors ?tand therefore a smaller value of<br />?. These numerical results support our theoretical asymptotic analysis. Interestingly, the<br />margin distributions of ATAs resembles the ones of SUPPORT VECTOR MACHINES for the<br />separable case (Boser et al., 1992; Cortes &amp; Vapnik, 1995; Vapnik, 1995, cf. figure 5). In<br />our toy example (cf. figure 6) we show the decision lines of SVMs and ATAs. We note<br />a very high overlap between the patterns that become support vectors (SVs) (cf. figure 6</p>  <p>Page 11</p> <p>SOFT MARGINS FOR ADABOOST<br />297<br />Figure 5.<br />with C = 10−3(dashed) and C = 10−1(dash-dotted). Here for the same toy example a RBF kernel (width = 0.3)<br />is used. The generalization error of the SVM with hard margin is more than two times larger as with C = 10−1.<br />Typical margin distribution graphs (normalized) of a SVM with hard margin (solid) and soft margin<br />Figure 6.<br />(right) for a low noise case with similar generalization errors. The positive and negative training patterns are<br />shown as ‘+’ and ‘∗’ respectively, the support patterns and support vectors are marked with ‘o’.<br />Training patterns with decision lines for ADABOOST (left) with RBF nets (13 centers) and SVM<br />right) and the patterns that lie within the step part of the margin distribution for ATA (cf.<br />figure 4 left).<br />So,theADABOOST-typealgorithmachievesasymptoticallyadecisionwithhardmargin,<br />very similar to the one of SVMs for the separable case. Intuitively this is clear: the most<br />difficult patterns are emphasized strongly and become support patterns or support vectors<br />asymptotically. The degree of overlap between the support vectors and support patterns<br />dependsonthekernel(SVM)andonthebasehypothesis(ATA)beingused.FortheSUPPORT<br />VECTOR MACHINE with RBF kernel the highest overlap was achieved, when the average<br />widths of the RBF networks was used as kernel width for the SUPPORT VECTOR MACHINE</p>  <p>Page 12</p> <p>298<br />G. R¨ATSCH, T. ONODA AND K.-R. M¨ULLER<br />Figure 7.<br />(dashed) and 104(solid) iterations. Here, the toy example (300 patterns, σ = 16%) and RBF networks with 30<br />centers are used. After already 200 iterations the asymptotical convergence is almost reached.<br />Typical margin distribution graphs of (original) ADABOOST after 20 (dotted), 70 (dash-dotted), 200<br />(R¨ atsch, 1998). We have also observed this striking similarity of SPs in ADABOOST and<br />SVs of the SVM in several other non-toy applications.<br />In the sequel, we can often assume the asymptotic case, where a hard margin is achieved<br />(the more hypotheses we combine, the better is this approximation). Experimentally we<br />find that the hard margin approximation is valid (cf. Eq. (4)) already for e.g. |b| &gt; 100.<br />This is illustrated by figure 7, which shows some typical ATA margin distributions after 20,<br />70, 200 and 104iterations.<br />To recapitulate our findings of this section:<br />1. ADABOOST-typealgorithmsaimtominimizeafunctional,whichdependsonthemargin<br />distribution. The minimization is done by means of a constraint gradient descent with<br />respect to the margin.<br />2. Some training patterns, which are in the area of the decision boundary, have asymptot-<br />ically (for a large number of boosting steps) the same margin. We call these patterns<br />Support Patterns. They have a large overlap to the SVs found by a SVM.<br />3. Asymptotically,ATAsreachahardmargincomparabletotheoneobtainedbytheoriginal<br />SVM approach (Boser et al., 1992).<br />4. Larger hard margins can be achieved, if ?t(more complex base hypotheses) and/or φ<br />are small (cf. Corollary 4). For the low noise case, a choice of θ ?=<br />better generalization performance, as shown for e.g. OCR benchmark data in Onoda<br />et al. (1998).<br />1<br />2can lead to a<br />3.Hard margin and overfitting<br />In this section, we give reasons why the ATA is not noise robust and exhibits suboptimal<br />generalization ability in the presence of noise. According to our understanding, noisy data<br />has at least one of the following properties: (a) overlapping class probability distributions,</p>  <p>Page 13</p> <p>SOFT MARGINS FOR ADABOOST<br />299<br />(b) outliers and (c) mislabeled patterns. All three types of noise appear very often in data<br />analysis. Therefore the development of a noise robust version of ADABOOST is very<br />important.<br />The first theoretical analysis of ADABOOST in connection with margin distributions<br />was done by Schapire et al. (1997). Their main result is a bound on the generalization error<br />Pz∼D[ρ(z) ≤ 0] depending on the VC-dimension d of the base hypotheses class and on the<br />margin distribution on the training set. With probability at least 1 − δ<br />Pz∼D[ρ(z) ≤ 0] ≤ Pz∼Z[ρ(z) ≤ θ] + O<br />?1<br />√l<br />?d log2(l/d)<br />θ2<br />+ log(1/δ)<br />??<br />(10)<br />issatisfied,whereθ &gt; 0andl denotesthenumberofpatterns.Itwasstatedthatthereasonfor<br />thesuccessofADABOOST,comparedtootherensemblelearningmethods(e.g.BAGGING),<br />is the maximization of the margin. The authors observed experimentally that ADABOOST<br />maximizes the margin of patterns which are most difficult, i.e. have the smallest margin and<br />that on the other hand by increasing the minimum margin of a few patterns, the margin of<br />the rest of the other patterns is also reduced.<br />InBreiman(1997b),theconnectionbetweenmaximizingthesmallestmarginandagood<br />generalizationerrorwasanalyzedexperimentallyandcouldnotbeconfirmedfornoisydata.<br />In Grove and Schuurmans (1998) the Linear Programming (LP) approach of Freund and<br />Schapire (1996) and Breiman (1997b) was extended and used to maximize the smallest<br />margin of an existing ensemble of classifiers. Several experiments with LP-ADABOOST<br />onUCIbenchmarks(oftennoisydata)weremadeanditwasunexpectedlyobservedthatLP-<br />ADABOOST performs in almost all cases worse than the original ADABOOST algorithm,<br />even though the smallest observed margins were larger.<br />Ourexperimentshaveshownthatasthemarginincreases,thegeneralizationperformance<br />becomes better on data sets with almost no noise (e.g. OCR, cf. Onoda et al., 1998),<br />however, we also observe that ADABOOST overfits on noisy data (for a moderate number<br />of combined hypotheses).<br />Asanexampleforoverlappingclasses,figure8(left)showsatypicaloverfittingbehavior<br />in the generalization error for ADABOOST on the same data as in Section 2. Here, already<br />after only 80 boosting iterations the best generalization performance is achieved. From<br />Eq. (6) we see that ADABOOST will asymptotically achieve a positive margin (for φ &lt;1<br />and all training patterns are classified according to their possibly wrong labels (cf. figure 8<br />(right)). However, this is at the expense that the complexity of the combined hypotheses<br />increases and the decision surface becomes clearly less smooth. The achieved decision line<br />is far away from the Bayes optimal line (cf. dashed line in figure 8 (right)).<br />To discuss the generally bad performance of hard margin classifiers in the presence<br />of outliers and mislabeled patterns, we analyze the toy example in figure 9. Let us first<br />consider the case without noise (left). Here, we can estimate the optimal separating hyper-<br />planecorrectly.Infigure9(middle)wehaveoneoutlier,whichcorruptstheestimation.The<br />ADABOOST-type algorithm will certainly concentrate its weights on this outlier and spoil<br />the good estimate that we would get without outlier. Next, let us consider more complex<br />decision lines. Here the overfitting problem gets even more distinct, if we can generate<br />2)</p>  <p>Page 14</p> <p>300<br />G. R¨ATSCH, T. ONODA AND K.-R. M¨ULLER<br />Figure 8.<br />iterations (left; log scale) and a typical decision line (right) generated by ADABOOST (104iterations) using RBF<br />networks (30 centers) in the case of noisy data (300 patterns, σ2= 16%). The positive and negative training<br />patterns are shown as ‘+’ and ‘∗’ respectively, the support patterns are marked with ‘o’. An approximation to the<br />Bayes decision line is plotted dashed.<br />Typical overfitting behavior in the generalization error (smoothed) as a function of the number of<br />Figure 9.<br />(middle) and with a mislabeled pattern (right). The solid line shows the resulting decision line, whereas the dashed<br />line marks the margin area. In the middle and on the left the original decision line is plotted with dots. The hard<br />margin implies noise sensitivity, because only one pattern can spoil the whole estimation of the decision line.<br />The problem of finding a maximum margin “hyper-plane” on reliable data (left), data with outlier<br />more and more complexity by combining a lot of hypotheses. Then all training patterns<br />(even mislabeled ones or outliers) can be classified correctly. In figure 8 (right) and figure 9<br />(right) we see that the decision surface is rather rough and gives bad generalization.<br />From these cartoons, it becomes apparent that ATA is noise sensitive and maximizing<br />the smallest margin in the case of noisy data can (and will) lead to bad results. Therefore,<br />we need to relax the hard margin and allow for a possibility of mistrusting the data.</p>  <p>Page 15</p> <p>SOFT MARGINS FOR ADABOOST<br />301<br />From the bound (10) it is indeed not obvious that we should maximize the smallest<br />margin: the first term on the right hand side of Eq. (10) takes the whole margin distribution<br />into account. If we would allow a non-zero training error in the settings of figure 9, then<br />the first term of the right hand side of (10) becomes non-zero (θ &gt; 0). But then θ can be<br />larger, such that the second term is much smaller. In Mason et al. (2000a) and Mason et al.<br />(2000b) similar bounds were used to optimize the margin distribution (a piecewise linear<br />approximation) directly. This approach, similar in spirit than ours, is more successful on<br />noisy data than the simple maximization of the smallest margin.<br />In the following we introduce several possibilities to mistrust parts of the data, which<br />leads to the soft margin concept.<br />4. Improvements using a soft margin<br />SincetheoriginalSVMalgorithm(Boseretal.,1992)assumedseparableclassesandpursued<br />a hard margin strategy, it had similarly poor generalization performance on noisy data as<br />the ATAs. Only the introduction of soft margins for SUPPORT VECTOR MACHINES (Cortes<br />&amp; Vapnik, 1995) allowed them to achieve much better generalization results (cf. figure 5).<br />We will now show how to use the soft margin idea for ATAs. In Section 4.1 we modify<br />the error function from Eq. (2) by introducing a new term, which controls the importance<br />of a pattern in the reweighting scheme. In Section 4.2 we demonstrate that the soft margin<br />idea can be directly build into the LP-ADABOOST algorithm and in Section 4.3 we show<br />an extension to quadratic programming—QP-ADABOOST—with its connections to the<br />support vector approach.<br />In the following subsections and also in the experimental section we will only consider<br />the case φ =1<br />2. Generalizing to other values of φ is straightforward.<br />4.1. Margin vs. influence of a pattern<br />First, we propose an improvement of the original ADABOOST by using a regularization<br />term in (2) in analogy to weight decay in neural networks and to the soft margin approach<br />of SVMs.<br />From Corollary 4 and Theorem 2 of Breiman (1997b), all training patterns will get a<br />marginρ(zi)largerthanorequalto1−2φ afterasymptoticallymanyiterations(cf.figure3<br />and discussion in Section 2). From Eq. (2) we can see that G(b) is minimized as ? is<br />maximized, where<br />ρ(zi,c) ≥ ?<br />After many iterations, these inequalities are satisfied for a ? that is larger or equal than<br />the margin given in Corollary 4. If ? &gt; 0 (cf. Corollary 4), then all patterns are classified<br />according to their possibly wrong labels, which leads to overfitting in the presence of noise.<br />Therefore, any modification that improves ADABOOST on noisy data, must not force all<br />margins beyond 0. Especially those patterns that are mislabeled and usually more difficult<br />to classify, should be able to attain margins smaller than 0.<br />for all i = 1,...,l.<br />(11)</p>  <p>Page 16</p> <p>302<br />G. R¨ATSCH, T. ONODA AND K.-R. M¨ULLER<br />If we knew beforehand which patterns were unreliable we could just remove them from<br />the training set or, alternatively, we could not require that they have a large margin (cf.<br />also the interesting approach for regression in Breiman (1999)). Suppose we have defined<br />a non-negative quantity ζ(zi), which expresses our “mistrust” in a pattern zi. For instance,<br />this could be a probability that the label of a pattern is incorrect. Then we can relax (11)<br />and get<br />ρ(zi,c) ≥ ? − Cζ(zi),<br />where C is an a priori chosen constant. Furthermore, we can define the soft margin ˜ ρ(zi)<br />of a pattern zias a tradeoff between the margin and ζ(zi)<br />(12)<br />˜ ρ(zi,c) := ρ(zi,c) + Cζ(zi)<br />and from Eq. (12) we obtain<br />(13)<br />˜ ρ(zi,c) ≥ ?.<br />Now we can again simply maximize the smallest soft margin (i.e. maximize ?) and we<br />expect to observe less overfitting. The problem now is, how to define ζ(zi). We restrict<br />ourselves here by presenting only one definition of ζ based on the influence of a pattern on<br />the combined hypotheses hr<br />(14)<br />µt(zi) =<br />t ?<br />r=1<br />crwr(zi),<br />which is the average weight of a pattern computed during the ATA learning process (cf.<br />pseudo-code in figure 1). The rationale is: a pattern which is very often misclassified (i.e.<br />difficult to classify) will have a high average weight, i.e. a high influence.5Interestingly, in<br />the noisy case there is (usually) a high overlap between patterns with high influence and<br />mislabeled patterns (or other patterns very near to or just beyond the decision line).<br />In the SVM approach it turns out that introducing slack variables to the quadratic opti-<br />mization problem (Cortes &amp; Vapnik, 1995) is the same as introducing a upper bound on<br />the Lagrange multipliers of the patterns (Cortes &amp; Vapnik, 1995; Vapnik, 1995; Sch¨ olkopf,<br />1997). Empirical evidence shows that the influence of a pattern µt(zi) is very similar to a<br />Lagrange multiplier in LP-ADABOOST (Grove &amp; Schuurmans, 1998), since it indicates<br />how much the pattern contributes to the decision. Lagrange multipliers of patterns that are<br />notsupportpatternsinthelinearprogramwillbe0andtheinfluenceofanonsupportpattern<br />will also converge asymptotically to 0 (for t → ∞). Furthermore, we found experimentally<br />that both numerical values coincide within a small range (details on this connection can be<br />found in R¨ atsch et al., 2000).<br />From this discussion it becomes apparent that it makes sense to mistrust patterns with<br />high influences in the noisy case. From this we define ζ by<br />ζ(zi) ≡ µt(zi)p,<br />such that the influence of a pattern is penalized, where p is an exponent chosen a priori (for<br />example choose p = 1 or 2).6If a training pattern has high weights ζ(zi), then also the soft<br />(15)</p>  <p>Page 17</p> <p>SOFT MARGINS FOR ADABOOST<br />303<br />margin is increasing. If we now maximize the smallest soft margin, we do not force outliers<br />to be classified according to their possibly wrong labels, but we allow for some errors. Our<br />prior for the choice (15) is to weight all patterns equally. This counterbalances the tendency<br />of ATAs to overweight certain patterns. So we tradeoff between margin and influence.<br />Note 7.<br />IfC ischosenhigh,theneachsingledatapointis“nottakenveryseriously”andweobserve<br />empirically that the number of support patterns increases. For C → ∞ we (almost) retrieve<br />the BAGGING algorithm (Breiman, 1996) (in this case, the pattern weighting will be always<br />uniform).<br />If we choose C = 0 in Eq. (12), the original ADABOOST algorithm is retrieved.<br />Of course, other functional forms of ζ are also possible (see also R¨ atsch et al., 2000),<br />for instance ζt(zi) = P ft(xi), where P is an arbitrary regularization operator. With P it is<br />possible to incorporate (other) prior knowledge about the problem into ATAs like smooth-<br />ness of the decision surface much in the spirit of Tikhonov regularizers (e.g. Tikhonov &amp;<br />Arsenin, 1977; Smola, Sch¨ olkopf, &amp; M¨ uller, 1998; Rokui &amp; Shimodaira, 1998).<br />Now we can reformulate the ATA optimization process in terms of soft margins. From<br />Eq. (14) and the definition in (15) we can easily derive the new error function (cf. Eq. (2)),<br />which aims to maximize the soft margin (we assume φ =1<br />?<br />l ?<br />The weight wt+1(zi) of a pattern is computed as the derivative of Eq. (16) with respect to<br />˜ ρ(zi,bt) (cf. Lemma 1)<br />1<br />Zt<br />2):<br />GReg(bt) =<br />l ?<br />i=1<br />exp<br />−1<br />2˜ ρ(zi,bt)<br />?<br />=<br />i=1<br />exp<br />?<br />−1<br />2|bt|[ρ(zi,ct) + Cµt(zi)p]<br />?<br />.<br />(16)<br />wt+1(zi) =<br />∂GReg(bt)<br />∂ ˜ ρ(zi,bt)=<br />exp{− ˜ ρ(zi,bt)/2}<br />j=1exp{− ˜ ρ(zj,bt)/2},<br />?l<br />(17)<br />where Ztis a normalization constant such that?l<br />1998) as<br />i=1wt+1(zi) = 1. For p = 1 we get the<br />update rule for the weight of a training pattern in the t-th iteration (for details cf. R¨ atsch,<br />wt+1(zi) =wt(zi)<br />Zt<br />exp{btI(yi?= ht(xi)) − Cζt(zi)|bt|},<br />(18)<br />and for p = 2 we obtain<br />wt+1(zi) =wt(zi)<br />Zt<br />exp{btI(yi?= ht(x)) − Cζt(zi)|bt| + Cζt−1(zi)|bt−1|},<br />(19)<br />where Ztis again a normalization constant. It is more difficult to compute the weight bt<br />of the t-th hypothesis analytically. However, we can get bt efficiently by a line search</p>  <p>Page 18</p> <p>304<br />G. R¨ATSCH, T. ONODA AND K.-R. M¨ULLER<br />Figure 10.<br />a regularization constant and p is a parameter that changes the regularization characteristics. In all simulations in<br />Section5weused p = 2.Animplementationcanbedownloadedfromhttp://ida.first.gmd.de/˜raetsch.<br />The ADABOOSTREG(ABR) algorithm (R¨ atsch, 1998; R¨ atsch, Onoda, &amp; M¨ uller, 1999), where C is<br />procedure (e.g. Press et al., 1992) minimizing (16), which has a unique solution because<br />∂<br />∂btGReg(bt) &gt; 0 is satisfied for bt &gt; 0. An algorithmic formulation can be found in<br />figure 10.<br />We can interpret this approach as regularization analogous to weight decay. Our prior is<br />that some patterns are likely not to be reliable, so in the noisy case we prefer hypotheses<br />which do not rely on only a few patterns with high weights.7Instead, we are looking for<br />hypotheses with smaller values of ζ(zi). So by this regularization, ADABOOST is not<br />changed for easily classifiable patterns, but only for the most difficult ones.<br />The variables ζ(zi) in Eq. (12) can also be interpreted as slack-variables (cf. SVM<br />approach and next section), which are non-linearly involved in the error function. Large<br />values of ζ(zi) for some patterns allow for a larger (soft-) margin ?. For a comparison of<br />the soft margin distributions of a single RBF classifier and ADABOOSTREGsee figure 11.<br />Summarizing, our modification of ADABOOST constructs a soft margin to avoid over-<br />fitting.</p>  <p>Page 19</p> <p>SOFT MARGINS FOR ADABOOST<br />305<br />Figure 11.<br />ADABOOSTREG(right) with different values of C for the toy data set after 1000 iterations. Note that for some<br />values for C the graphs of ADABOOSTREGare quite similar to the graphs of the single RBF net.<br />Margin distribution graphs of the RBF base hypothesis (scaled) trained with Squared Error (left) and<br />4.2.Linear programming with slack variables<br />Grove and Schuurmans (1998) showed how to use linear programming to maximize the<br />smallest margin for a given ensemble and proposed LP-ADABOOST (cf. Eq. (21)). In<br />their approach, they first compute a margin (or gain) matrix M ∈ {±1}l×Tfor the given<br />hypotheses set, which is defined by<br />Mi,t= yiht(xi).<br />M defines which hypothesis contributes a positive (or negative) part to the margin of a<br />patternandisusedtoformulatethefollowingmax-minproblem:findaweightvectorc ∈ RT<br />for hypotheses {ht}T<br />problem can be solved by linear programming (e.g. Mangasarian, 1965):<br />(20)<br />t=1, which maximizes the smallest margin ? := mini=1,...,lρ(zi). This<br />Maximize ? subject to<br />T ?<br />t=1<br />Mi,tct≥ ?<br />ct≥ 0<br />T ?<br />i = 1,...,l<br />t = 1,...,T<br />(21)<br />t=1<br />ct= 1.<br />This LP-ADABOOST algorithm achieves a larger hard margin than the original<br />ADABOOST algorithm, however in this form it cannot hope to generalize well on noisy<br />data (see our discussion in Section 3). Therefore we also define a soft-margin for a pattern<br />˜ ρ?(zi) = ρ(zi) + ξi, which is technically equivalent to the introduction of slack variables<br />ξiand we arrive at the algorithm LPREG-ADABOOST (R¨ atsch, 1998; R¨ atsch et al., 1999;<br />R¨ atsch, Onoda, &amp; M¨ uller, 1998). To avoid large values of the slack variables, while solving</p>  <p>Page 20</p> <p>306<br />G. R¨ATSCH, T. ONODA AND K.-R. M¨ULLER<br />Figure 12. The LPREG-ADABOOST algorithm, where C is a regularization constant.<br />the linear program with slack variables, the sum of all ξiis penalized in the objective func-<br />tion(cf.pseudo-codeinfigure12).Thismodificationallowsthatsomepatternshavesmaller<br />marginsthan?.ThereisatradeoffcontrolledbytheconstantC:(a)makeallmarginslarger<br />than ? and (b) maximize ? −C<br />Our algorithm is related to the LP-SVM approach (Sch¨ olkopf, Smola, &amp; Williamson,<br />2000). As in the original SVM approach, the Lagrange multipliers will be sparse and again<br />wegetsupportvector/patterns.Interestingly,itturnsoutinbothapproaches(asymptotically,<br />i.e. with the number of patterns) that ν :=<br />of misclassified samples and a lower bound on the fraction of support vectors (R¨ atsch et al.,<br />2000).<br />l<br />?<br />iξi.<br />1<br />C∈ [0...1] is an upper bound on the fraction<br />4.3. Quadratic programming and the connection to support vector machines<br />In the following section, we extend the LPREG-ADABOOST (LPR) algorithm to quadratic<br />programming by using similar techniques as in SUPPORT VECTOR MACHINES (Boser et al.,<br />1992; Cortes &amp; Vapnik, 1995; Mangasarian, 1965). This gives interesting insights to the<br />connection between SUPPORT VECTOR MACHINES and ADABOOST.<br />We start by transforming the LPREG-ADABOOST algorithm, which maximizes ?, while<br />|c| is kept fixed, to a linear program in which ? is fixed (to e.g. 1) and |b| is minimized.<br />Unfortunately, there is no equivalent linear program because of the slack variables. But<br />we can use a Taylor expansion8to get the following linear program (compare with linear<br />programmingapproachesrelatedtoSVlearninge.g.Bennett&amp;Mangasarian,1992;Weston</p>  <p>Page 21</p> <p>SOFT MARGINS FOR ADABOOST<br />307<br />et al., 1997; Frieß &amp; Harrison, 1998; Bennett, 1998):<br />?<br />T ?<br />Minimize ?b?1+ C<br />i<br />ξisubject to<br />t=1<br />btMi,t≥ 1 − ξi,<br />t = 1,...,T,<br />t = 1,...,T,<br />i = 1,...,l.<br />(22)<br />bt≥ 0,<br />ξi≥ 0,<br />Essentially, this is the same algorithm as in figure 12: for a different value of C problem,<br />(22) is equivalent to the one in figure 12 (cf. Smola, 1998 and Lemma 3 in R¨ atsch et al.,<br />2000). Instead of using the ?1-norm in the optimization objective of (22), we can also use<br />the ?p-norm. Clearly, each p will imply its own soft margin characteristics. Using p = 2<br />leads to an algorithm similar to the SVM (cf. figure 14).<br />TheoptimizationobjectiveofaSVMistofindafunctionhwwhichminimizesafunctional<br />of the form (Vapnik, 1995)<br />E = ?w?2+ C<br />l ?<br />i=1<br />ξi,<br />(23)<br />subject to the constraints<br />yih(xi) ≥ 1 − ξi<br />and<br />ξi≥ 0,for i = 1,...,l.<br />Here, the variables ξiare the slack-variables responsible for obtaining a soft margin. The<br />norm of the parameter vector w defines a system of nested subsets in the combined hypoth-<br />esisspaceandcanberegardedasameasureofthecomplexity(asalsothesizeofthemargin<br />Figure13.<br />values of C for the toy data set after 1000 iterations. LPREG-ADABOOST sometimes generates margins on the<br />training set, which are either 1 or −1 (step in the distribution).<br />MargindistributiongraphsofLPREG-ADABOOST(left)andQPREG-ADABOOST(right)fordifferent</p>  <p>Page 22</p> <p>308<br />G. R¨ATSCH, T. ONODA AND K.-R. M¨ULLER<br />of hypothesis hw) (Vapnik, 1995). With functional (23), we get a tradeoff (controlled by C)<br />betweenthecomplexityofthehypothesis(?w?2)andthedegreehowmuchtheclassification<br />may differ from the labels of the training patterns (?<br />we observed that the more different the weights are for the hypotheses, the higher the<br />complexity of the ensemble. With this in mind, we can use the ?pnorm (p &gt; 1) of the<br />hypothesesweightvector?b?pasacomplexitymeasure.Letusassume,forexamplethatwe<br />have |b| = 1, then ?b?pthis is a small value, as the elements of b are approximately equal<br />(analogous to BAGGING). If ?b?phas high values, then there are some strongly emphasized<br />hypotheses (far away from BAGGING).9Hence, we can apply the optimization principles of<br />SVMs to ADABOOST and get a quadratic optimization problem in b:<br />iξi).<br />For ensemble learning, so far we do not have such a measure of complexity. Empirically,<br />Minimize ?b?2+ C<br />?<br />i<br />ξi,<br />with the constraints given in Eq. (22). We call this algorithm QPREG-ADABOOST (QPR)<br />since it was motivated by the connection to LPREG-ADABOOST (cf. algorithm (22)) and by<br />theanalogytothesupportvectoralgorithm(forpseudocodeseefigure14).Weexpectasim-<br />ilar performance of QPREGand LPREG-ADABOOST with subtle differences on specific data<br />sets due to the different “types” of soft margins. Furthermore, they should exhibit superior<br />performance compared to the original ADABOOST algorithm on noisy data. For an overall<br />comparison of the margin distributions of original ADABOOST, SVM, ADABOOSTREG<br />and LP/QP-ADABOOST see figures 5, 7, 11 and 13.<br />Summarizing, we introduced in this section the soft margin concept to ADABOOST<br />by (a) regularizing the objective function (2), (b) LPREG-ADABOOST, which uses slack<br />variables and (c) QPREG-ADABOOST, which exhibits an interesting connection to SVMs.<br />Figure 14. The QPREG-ADABOOST algorithm, where C is a regularization constant.</p>  <p>Page 23</p> <p>SOFT MARGINS FOR ADABOOST<br />309<br />5. Experiments<br />In order to evaluate the performance of our new algorithms, we perform large scale simula-<br />tions and compare the single RBF classifier, the original ADABOOST algorithm,<br />ADABOOSTREG, L/QPREG-ADABOOST and a SUPPORT VECTOR MACHINE (with RBF<br />kernel).<br />5.1. Experimental setup<br />For this, we use 13 artificial and real world data sets from the UCI, DELVE and STAT-<br />LOG benchmark repositories10: banana (toy data set used in the previous sections), breast<br />cancer,11diabetes, german, heart, image segment, ringnorm, flare solar, splice, new-<br />thyroid, titanic, twonorm, waveform. Some of the problems are originally not binary classi-<br />fication problems, hence a random partition into two classes is used.12At first we generate<br />100 partitions into training and test set (mostly ≈ 60% : 40%). On each partition we train<br />a classifier and then compute its test set error.<br />In all experiments, we combine 200 hypotheses. Clearly, this number of hypotheses is<br />somewhatarbitraryandmaynotbeoptimal.HoweverwecheckedthatoriginalADABOOST<br />withearlystoppingismostofthetimeworsethananyoftheproposedsoftmarginalgorithms<br />(cf. an earlier study R¨ atsch, 1998). However, we use a fixed number of iterations for all<br />algorithms, therefore this comparison should be fair.<br />As base hypotheses we use RBF nets with adaptive centers as described in Appendix D.<br />On each of the 13 data sets we employ cross validation to find the best base hypothesis<br />model, which is then used in the ensemble learning algorithms. For selecting the best RBF<br />model we optimize the number of centers (parameter K, cf. figure 15) and the number of<br />iteration steps for adapting the RBF centers and widths (parameter O). The parameter λ<br />was fixed to 10−6.<br />The parameter C of the regularized versions of ADABOOST and the parameters (C,σ)<br />of the SVM (C is the regularization constant and σ is the width of the RBF-kernel be-<br />ing used) are optimized on the first five realizations of each data set. On each of these<br />realizations, a 5-fold-cross validation procedure gives a good model.13Finally, the model<br />parameters are computed as the median of the five estimations and used throughout the<br />training on all 100 realization of that data set. This way of estimating the parameters is<br />computationally highly expensive, but it will make our comparison more robust and the<br />results more reliable.<br />Note, to perform the simulations in this setup we had to train more than 3×106adaptive<br />RBFnetsandtosolvemorethan105linearorquadraticprogrammingproblems—ataskthat<br />would have taken altogether 2 years of computing time on a single Ultra-SPARC machine,<br />if we had not distributed it over 30 computers.<br />5.2.Experimental results<br />In Table 1 the average generalization performance (with standard deviation) over the 100<br />partitions of the data sets is given. The second last line in Table 1 showing ‘Mean%’, is</p>  <p>Page 24</p> <p>310<br />G. R¨ATSCH, T. ONODA AND K.-R. M¨ULLER<br />Figure 15.<br />simulations with ADABOOST.<br />Pseudo-code description of the RBF net algorithm, which is used as base learning algorithm in the<br />computed as follows: For each data set the average error rates of all classifier types are<br />divided by the minimum error rate for this data set and 1 is subtracted. These resulting<br />numbers are averaged over the 13 data sets and the variance is computed. The last line gives<br />the Laplacian probability (and variance) over 13 data sets whether a particular method<br />wins on a particular realization of a data set, i.e. has the lowest generalization error. Our<br />experiments on noisy data (cf. Table 1) show that:<br />– The results of ADABOOST are in almost all cases worse than the single classifier. This<br />is clearly due to the overfitting of ADABOOST. If early stopping is used then the effect<br />is less drastic but still clearly observable (R¨ atsch, 1998).<br />– The averaged results for ADABOOSTREGare a bit better (Mean% and Winner%) than<br />the results of the SVM, which is known to be an excellent classifier. In five (out of<br />seven) cases ADABOOSTREGis significant better than the SVM. Moreover, the single</p>  <p>Page 25</p> <p>SOFT MARGINS FOR ADABOOST<br />311<br />Table1.<br />p = 2), L/QPREG-ADABOOST (L/QPR-AB) and a SUPPORT VECTOR MACHINE: Estimation of generalization<br />error in % on 13 data sets (best method in bold face, second emphasized). The columns S1and S2show the results<br />ofasignificancetest(95%-t-test)betweenAB/ABRandABR/SVM,respectively.ADABOOSTREGgivesthebest<br />overall performance.<br />Comparisonamongthesixmethods:SingleRBFclassifier,ADABOOST(AB),ADABOOSTREG(ABR;<br />RBF ABS1<br />ABR<br />LPR-AB QPR-ABS2<br />SVM<br />Banana<br />B. Cancer<br />Diabetes<br />German<br />Heart<br />Image<br />Ringnorm<br />F. Solar<br />Splice<br />Thyroid<br />Titanic<br />Twonorm<br />Waveform<br />10.8±0.6<br />27.6±4.7<br />24.3±1.9<br />24.7±2.4<br />17.6±3.3<br />3.3±0.6<br />1.7±0.2<br />34.4±2.0<br />10.0±1.0<br />4.5±2.1<br />23.3±1.3<br />2.9±0.3<br />10.7±1.1<br />6.6±5.8<br />14.8±8.5<br />12.3±0.7<br />30.4±4.7<br />26.5±2.3<br />27.5±2.5<br />20.3±3.4<br />2.7±0.7<br />1.9±0.3<br />35.7±1.8<br />10.1±0.5<br />4.4±2.2<br />22.6±1.2<br />3.0±0.3<br />10.8±0.6<br />11.9±7.9<br />7.2±7.8<br />+<br />+<br />+<br />+<br />+<br />10.9±0.4<br />26.5±4.5<br />23.8±1.8<br />24.3±2.1<br />16.5±3.5<br />2.7±0.6<br />1.6±0.1<br />34.2±2.2<br />9.5±0.7<br />4.6±2.2<br />22.6±1.2<br />2.7±0.2<br />9.8±0.8<br />1.7±1.9<br />26.0±12.4<br />10.7±0.4<br />26.8±6.1<br />24.1±1.9<br />24.8±2.2<br />17.5±3.5<br />2.8±0.6<br />2.2±0.5<br />34.7±2.0<br />10.2±1.6<br />4.6±2.2<br />24.0±4.4<br />3.2±0.4<br />10.5±1.0<br />8.9±10.8<br />14.4±8.6<br />10.9±0.5<br />25.9±4.6<br />25.4±2.2<br />25.3±2.1<br />17.2±3.4<br />2.7±0.6<br />1.9 ±0.2<br />36.2±1.8<br />10.1±0.5<br />4.4±2.2<br />22.7±1.1<br />3.0±0.3<br />10.1±0.5<br />5.8±5.5<br />13.2±7.6<br />+<br />11.5±0.7<br />26.0±4.7<br />23.5±1.7<br />23.6±2.1<br />16.0±3.3<br />3.0±0.6<br />1.7±0.1<br />32.4±1.8<br />10.9±0.7<br />4.8±2.2<br />22.4±1.0<br />3.0±0.2<br />9.9±0.4<br />4.6±5.4<br />23.5±18.0<br />+<br />−<br />+<br />+<br />+<br />−<br />+<br />−<br />+<br />+<br />+<br />+<br />Mean%<br />Winner%<br />RBF classifier wins less often than the SVM (for a comparison in the regression case cf.<br />M¨ uller et al., 1998).<br />– L/QPREG-ADABOOST improves the results of ADABOOST. This is due to the use of<br />a soft margin. But the results are not as good as the results of ADABOOSTREGand the<br />SVM. One reason is that the hypotheses generated by ADABOOST (aimed to construct<br />a hard margin) may not provide the appropriate basis to subsequently generate a good<br />soft margin with linear and quadratic programming approaches.<br />– We can observe that quadratic programming gives slightly better results than linear<br />programming. This may be due to the fact that the hypotheses coefficients generated by<br />LPREG-ADABOOST are more sparse (smaller ensemble) and larger ensembles may have<br />a better generalization ability (Breiman, 1998). Furthermore, with QP-ADABOOST we<br />prefer ensembles which have approximately equally weighted hypotheses. As stated in<br />Section 4.3, this implies a lower complexity of the combined hypothesis, which can lead<br />to a better generalization performance.<br />– The results of ADABOOSTREGare in ten (out of 13) cases significantly better than the<br />results of ADABOOST. Also, in ten cases ADABOOSTREGperforms better than the<br />single RBF classifier.<br />Summarizing, ADABOOSTREGwins most often and shows the best average performance.<br />In most of the cases it performs significantly better than ADABOOST and it performs</p>  <p>Page 26</p> <p>312<br />G. R¨ATSCH, T. ONODA AND K.-R. M¨ULLER<br />slightly better than SUPPORT VECTOR MACHINES. This demonstrates the noise robustness<br />of the proposed algorithm.<br />The slightly inferior performance of SVM compared to ADABOOSTREGmay be ex-<br />plained with the fixed σ of the RBF-kernel for SVM. By fixing σ we look at the data only<br />at one scale, i.e. we are losing possible multiscale information that could be inherent of the<br />data. Further causes could be the coarse model selection, and the error function of the SV<br />algorithm, which is not adapted to the noise model in the data (see Smola et al., 1998).<br />So, the original ADABOOST algorithm is useful for low noise cases, where the classes<br />are easily separable (as shown for OCR cf. Schwenk &amp; Bengio, 1997; LeCun et al., 1995).<br />L/QPREG-ADABOOSTcanimprovetheensemblestructurethroughintroducingasoftmar-<br />gin and the same hypotheses (just with another weighting) can result in a much better gen-<br />eralization performance. The hypotheses, which are used by L/QPREG-ADABOOST may<br />be sub-optimal, because they are not part of the L/QP optimization process that aims for<br />a soft margin. ADABOOSTREGdoes not have this problem: the hypotheses are generated<br />such that they are appropriate to form the desired soft-margin. ADABOOSTREGextends the<br />applicability of Boosting/Arcing methods to non-separable cases and should be preferably<br />applied if the data is noisy.<br />6. Conclusion<br />We have shown that ADABOOST performs a constrained gradient descent in an error func-<br />tion that optimizes the margin (cf. Eq. (2)). Asymptotically, all emphasis is concentrated on<br />the difficult patterns with small margins, easy patterns effectively do not contribute to the<br />error measure and are neglected in the training process (very much similar to support vec-<br />tors). It was shown theoretically and experimentally that the cumulative margin distribution<br />ofthetrainingpatternsconvergesasymptoticallytoastep.Therefore,originalADABOOST<br />achieves a hard margin classification asymptotically. The asymptotic margin distribution<br />of ADABOOST and SVM (for the separable case) are very similar. Hence, the patterns<br />lying in the step part (support patterns) of the margin distribution show a large overlap to<br />the support vectors found by a SVM.<br />WediscussedindetailthatATAsandhardmarginclassifiersareingeneralnoisesensitive<br />and prone to overfit. We introduced three regularization-based ADABOOST algorithms to<br />alleviate this overfitting problem: (1) direct incorporation of the regularization term into<br />the error function (ADABOOSTREG), use of (2) linear and (3) quadratic programming with<br />slack variables to improve existing ensembles. The essence of our proposed algorithms is<br />to achieve a soft margin (through regularization term and slack variables) in contrast to the<br />hard margin classification used before. The soft-margin approach allows to control how<br />much we trust the data, so we are permitted to ignore noisy patterns (e.g. outliers) which<br />would otherwise spoile our classification. This generalization is very much in the spirit of<br />SUPPORT VECTOR MACHINES that also tradeoff the maximization of the margin and the<br />minimization of the classification errors by introducing slack variables. Note that we just<br />gave one definition for the soft margin in ADABOOSTREGother extensions that e.g. use<br />regularization operators (e.g. Smola et al., 1998; Rokui &amp; Shimodaira, 1998; Bishop, 1995)<br />or that have other functional forms (cf. R¨ atsch et al., 2000) are also possible.</p>  <p>Page 27</p> <p>SOFT MARGINS FOR ADABOOST<br />313<br />In our experiments on noisy data the proposed regularized versions of ADABOOST:<br />ADABOOSTREGandL/QPREG-ADABOOSTshowamorerobustbehaviorthantheoriginal<br />ADABOOST algorithm. Furthermore, ADABOOSTREGexhibits a better overall general-<br />ization performance than all other analyzed algorithms including the SUPPORT VECTOR<br />MACHINES. We conjecture that this result is mostly due to the fact that SUPPORT VECTOR<br />MACHINEScanonlyuseoneσ,i.e.onlyone–fixed–kernel,andthereforelosesmulti-scaling<br />information. ADABOOST does not have this limitation, since we use RBF nets with adap-<br />tive kernel widths as base hypotheses.<br />Our future work will concentrate on a continuing improvement of ADABOOST-type<br />algorithmsfornoisyrealworldapplications.Also,afurtheranalysisoftherelationbetween<br />ADABOOST (in particular QPREG-ADABOOST) and SUPPORT VECTOR MACHINES from<br />themarginpointofviewseemspromising,withparticularfocusonthequestionofwhatgood<br />margin distributions should look like. Moreover, it is interesting to see how the techniques<br />establishedinthisworkcanbeappliedtoADABOOSTinaregressionscenario(cf.Bertoni,<br />Campadelli, &amp; Parodi, 1997; Friedman, 1999; R¨ atsch et al., 2000).<br />Appendix<br />A. Proof of Lemma 1<br />Proof:<br />d we get<br />We define πt(zi) :=?t<br />r=1exp(−brI(hr(zi) = yi)) and from definition of G and<br />∂G<br />∂ρ(zi,bt)<br />?l<br />j=1<br />∂G<br />∂ρ(zj,bt)<br />=<br />exp?−1<br />πt(zi)<br />j=1πt(xj)<br />2ρ(zi,bt)?<br />?l<br />?l<br />j=1exp?−1<br />2ρ(zi,bt)?<br />=<br />=πt(zi)<br />˜Zt<br />,<br />where˜Zt :=?l<br />wt+1(zi) =πt(zi)<br />i=1πt(zi). By definition, πt(zi) = πt−1(zi)exp(−btI(ht(zi) = yi)) and<br />π1(zi) = 1/l. Thus, we get<br />=πt−1(zi)exp(−btI(ht(zi) = yi))<br />˜Zt<br />=wt−1(zi)˜Zt−1exp(−btI(ht(zi) = yi))<br />=wt−1(zi)exp(−btI(ht(zi) = yi))<br />Zt<br />˜Zt<br />˜Zt<br />,<br />where Zt=˜Zt˜Zt−1(cf. step 4 in figure 1).<br />2</p>  <p>Page 28</p> <p>314<br />G. R¨ATSCH, T. ONODA AND K.-R. M¨ULLER<br />B. Proof of Theorem 3<br />The proof follows the one of Theorem 5 in (Schapire et al., 1997). Theorem 3 is a general-<br />ization for φ ?=1<br />2.<br />Proof:<br />If yf (x) ≤ θ, then we have<br />y<br />T ?<br />t=1<br />btht(x) ≤ θ<br />T ?<br />t=1<br />bt,<br />and also<br />exp<br />?<br />−y<br />2<br />T ?<br />t=1<br />btht(x) +θ<br />2<br />T ?<br />t=1<br />bt<br />?<br />≥ 1.<br />Thus,<br />P(x,y)∼Z[yf (x) ≤ θ] ≤1<br />l<br />l ?<br />i=1<br />exp<br />?<br />−yi<br />2<br />T ?<br />l ?<br />t=1<br />btht(xi) +θ<br />?<br />2<br />T ?<br />btht(xi)<br />t=1<br />bt<br />?<br />=exp?θ<br />2<br />?T<br />t=1bt<br />?<br />l<br />i=1<br />exp<br />−yi<br />2<br />T ?<br />t=1<br />?<br />,<br />where<br />l ?<br />i=1<br />exp<br />?<br />−yi<br />2<br />T ?<br />?<br />t=1<br />−yi<br />btht(xi)<br />?<br />=<br />l ?<br />i=1<br />exp<br />2<br />?<br />T−1<br />?<br />−yi<br />?<br />t=1<br />btht(xi)<br />?<br />exp<br />?<br />?<br />−yi<br />2bThT(xi)<br />?<br />=<br />?<br />i:hT(xi)=yi<br />exp<br />2<br />T−1<br />?<br />t=1<br />btht(xi)<br />e−bT/2<br />+<br />?<br />?<br />i:hT(xi)?=yi<br />l ?<br />exp<br />−yi<br />2<br />T−1<br />?<br />t=1<br />btht(xi)<br />?<br />ebT/2<br />=<br />i=1<br />exp<br />?<br />−yi<br />2<br />T−1<br />?<br />t=1<br />btht(xi)<br />??<br />((1 − ?T)e−bT/2+ ?TebT/2),<br />because<br />?T=<br />1<br />?l<br />j=1wT<br />j<br />?<br />i:hT(xi)?=yi<br />wT<br />i.</p>  <p>Page 29</p> <p>SOFT MARGINS FOR ADABOOST<br />315<br />With?l<br />i=1exp(0) = l (for t = 1), we get recursively<br />P(x,y)∼Z[yf (x) ≤ θ] ≤ exp<br />?<br />θ<br />2<br />T ?<br />t=1<br />bt<br />?<br />T ?<br />t=1<br />((1 − ?t)e−bt/2+ ?tebt/2).<br />Plugging in the definition for btwe get<br />P(x,y)∼Z[yf (x) ≤ θ] ≤<br />?<br />T ?<br />t=1<br />1 − ?t<br />?t<br />??<br />T ?<br />φ<br />1 − φ+<br />?1+φ<br />t=1<br />φ<br />1 − φ<br />?<br />?1 − φ<br />T ?<br />?θ/2<br />×<br />1 − φ<br />φ<br />?T<br />?1−φ<br />T ?<br />?T<br />t=1<br />?<br />T ?<br />(1 − ?t)?t<br />=<br />??<br />φ<br />1 − φ<br />2<br />+<br />φ<br />2<br />t=1<br />?<br />(1 − ?t)1+θ?1−θ<br />t<br />=?ϕ<br />1+θ<br />2 + ϕ−1−θ<br />2?T<br />t=1<br />?<br />?1−θ<br />t<br />(1 − ?t)1+θ.<br />2<br />C. Proof of Lemma 5<br />Proof:<br />We have to show that limt→∞εt= 0, where<br />???min<br />The set St<br />εt:=<br />i:yi=1ρ(zi,ct) − min<br />j:yj=−1ρ(zj,ct)<br />???.<br />cis the set of support patterns at iteration t:<br />St<br />c=<br />?<br />j ∈ {1,...,l} : ρ(zj,ct) = min<br />i:yi=cρ(zi,ct)<br />?<br />,<br />whichclearlycontainsatleastoneelement.Notethat S∞<br />which we will get asymptotically.<br />Suppose we have εt &gt; ?/2 &gt; 0 and s ∈ {±1} is the class with the smallest margin.<br />With (4), q ∈ St<br />1∪S∞<br />−1isthesetofsupportpatterns,<br />sand Q := exp(−1<br />2ρ(zq,ct)) we get<br />?<br />i:yi=s<br />wt(zi) ≥<br />Q|bt|<br />Q|bt|+?<br />Q|bt|+ (l − 1)Q|bt|e−|bt|?/4<br />i:yi?=sexp?−1<br />Q|bt|<br />2ρ(zi,ct)?|bt|<br />&gt;</p>  <p>Page 30</p> <p>316<br />G. R¨ATSCH, T. ONODA AND K.-R. M¨ULLER<br />With|bt|&gt;tγ fromassumption(8)weget?<br />sified correctly and patterns of class r = −s will be misclassified. Therefore, we obtain<br />ρ(zi,ct+1) =<br />classr will be misclassified and we have ρ(zj,ct+1) =<br />It follows<br />????min<br />=<br />|bt+1|<br />As long as εt &gt; ?/2, all patterns of class r will be misclassified and patterns of class s<br />will be classified correctly. If it becomes (εt|bt| − 2bt+1)/|bt+1| &lt; −?/2, then the same<br />reasoning can be done interchanging the role of s and r. If furthermore t &gt; t2:=<br />εt|bt| − 2bt+1&gt; 0 and we have εt+1&lt; εt− ωt, where ωtis decreasing not too fast (i.e. it<br />can be bounded by O(1/t)). Hence, we will reach the case εt&lt; ?/2 and get: If t is large<br />enough (t &gt; t3:=2?(2−?)<br />i:yi=swt(zi)≥δ,fort &gt;t1:=log(l−1)−log(1/δ−1)<br />γ?/4<br />.<br />Hence, with assumption (9) we get: If t ≥ t1, then all patterns ziof class s will be clas-<br />|bt|ρ(zi,ct)+bt+1<br />|bt+1|<br />, for i ∈ {k : yk= s} (especially for i ∈ St<br />s). The patterns of<br />, for j ∈ {k : yk= r}.<br />|bt|ρ(zj,ct)−bt+1<br />|bt+1|<br />εt+1=<br />i:yi=s<br />????<br />|bt|ρ(zi,ct) − bt+1<br />|bt+1|<br />εt|bt| − 2bt+1<br />− min<br />j:yj=r<br />|bt|ρ(zj,ct) + bt+1<br />|bt+1|<br />????<br />????<br />4?<br />γ?, then<br />?γ<br />), then<br />−? &lt;εt|bt| − 2bt+1<br />|bt+1|<br />&lt; ?,<br />i.e. adding a new hypothesis will not lead to εt+1≥ ?.<br />Therefore, after a finite number ˜ t of subsequent steps, we have we will reach εt &lt; ?.<br />Furthermore, the discussion above shows that, if t is large enough, it is not possible to leave<br />the?areaaround0.Hence,foreach? &gt; 0,wecanprovideanindexT = max(t1,t2,t3) + ˜ t<br />(where t1,...,t3are the lower bounds on t used above), such that εt &lt; ? for all t &gt; T.<br />This implies the desired result.<br />2<br />D. RBF nets with adaptive centers<br />TheRBFnetsusedintheexperimentsareanextensionofthemethodofMoodyandDarken<br />(1989), since centers and variances are also adapted (see also Bishop, 1995; M¨ uller et al.,<br />1998). The output of the network is computed as a linear superposition of K basis functions<br />f (x) =<br />K<br />?<br />k=1<br />wkgk(x),<br />(24)<br />where wk,k = 1,..., K, denotes the weights of the output layer. The Gaussian basis<br />functions gkare defined as<br />gk(x) = exp<br />?<br />−?x − µk?2<br />2σ2<br />k<br />?<br />,<br />(25)</p>  <p>Page 31</p> <p>SOFT MARGINS FOR ADABOOST<br />317<br />where µkand σ2<br />are initialized with K-means clustering and the variances σkare determined as the distance<br />between µkand the closest µi(i ?= k,i ∈ {1,..., K}). Then in the following steps we<br />perform a gradient descent in the regularized error function (weight decay)<br />kdenote means and variances, respectively. In a first step, the means µk<br />E =1<br />2<br />l ?<br />i=1<br />(yi− f (xi))2+λ<br />2l<br />K<br />?<br />k=1<br />w2<br />k.<br />(26)<br />Taking the derivative of Eq. (26) with respect to RBF means µkand variances σkwe obtain<br />∂E<br />∂µk<br />=<br />l ?<br />i=1<br />( f (xi) − yi)<br />∂<br />∂µk<br />f (xi),<br />(27)<br />with<br />∂<br />∂µkf (xi) = wk<br />xi−µk<br />σ2<br />k<br />gk(xi) and<br />∂E<br />∂σk<br />=<br />l ?<br />i=1<br />( f (xi) − yi)<br />∂<br />∂σk<br />f (xi),<br />(28)<br />with<br />of Eq. (26) by a conjugate gradient descent with line search, where we always compute the<br />optimal output weights in every evaluation of the error function during the line search. The<br />optimal output weights w = [w1,...,wK]?in matrix notation can be computed in closed<br />form by<br />∂<br />∂σkf (xi) = wk<br />?µk−xi?2<br />σ3<br />k<br />gk(xi).Thesetwoderivativesareemployedintheminimization<br />w =<br />?<br />GTG + 2λ<br />lI<br />?−1<br />GTy, where Gik= gk(xi)<br />(29)<br />and y = [y1,..., yl]?denotes the output vector, and I an identity matrix. For λ = 0, this<br />corresponds to the calculation of a pseudo-inverse of G.<br />So, we simultaneously adjust the output weights and the RBF centers and variances (see<br />figure 15 for pseudo-code of this algorithm). In this way, the network fine-tunes itself to<br />the data after the initial clustering step, yet, of course, overfitting has to be avoided by<br />careful tuning of the regularization parameter, the number of centers K and the number of<br />iterations (cf. Bishop, 1995). In our experiments we always used λ = 10−6and up to ten<br />CG iterations.<br />Acknowledgments<br />We thank for valuable discussions with B. Sch¨ olkopf, A. Smola, T. Frieß, D. Schuurmans<br />and B. Williamson. Partial funding from EC STORM project number 25387 is gratefully<br />acknowledged. Furthermore, we acknowledge the referees for valuable comments.</p>  <p>Page 32</p> <p>318<br />G. R¨ATSCH, T. ONODA AND K.-R. M¨ULLER<br />Notes<br />1. An ensemble is a collection of neural networks or other types of classifiers (hypotheses) that are trained for<br />the same task.<br />2. InFriedmanetal.(1998)itwasmentionedthatsometimestherandomizedversionshowsabetterperformance,<br />than the version with weighted minimization. In connection with the discussion in Section 3 this becomes<br />clearer, because the randomized version will show an overfitting effect (possibly much) later and overfitting<br />may be not observed, whereas it was observed using the more efficient weighted minimization.<br />3. In our experiments we have often observed values for |b| that are larger than 100 after only 200 iterations.<br />4. A detailed description of the generation of the toy data used in the asymptotical simulations can be found in<br />the Internet http://ida.first.gmd.de/˜raetsch/data/banana.html.<br />5. The definition of the influence clearly depends on the base hypothesis space H. If the hypothesis space<br />changes, other patterns may be more difficult to classify.<br />6. Note that for p = 1 there is a connection to the leave-one-out-SVM approach of Weston (1999).<br />7. Interestingly,the(soft)SVMgeneratesmanymoresupportvectorsinthehighnoisecasethaninthelownoise<br />case (Vapnik, 1995).<br />8. From the algorithm in figure 12, it is straight forward to get the following linear program, which is equivalent<br />for any fixed S &gt; 0:<br />Minimize?+<br />S<br />+C<br />S<br />?<br />i<br />ξ+<br />isubject to S<br />T ?<br />t=1<br />ctMi,t≥ ?+− ξ+<br />i,<br />where ?+:= S?,ξ+<br />i:= Sξi,bt≥ 0,ξ+<br />i≥ 0,<br />?<br />t<br />bt= S.<br />In this problem we can set ?+to 1 and try to optimize S. To retrieve a linear program, we use the Taylor<br />expansion around 1:1<br />(22).<br />9. For p = 2,?b?2corresponds to the Renyi entropy of the hypothesis vector and we are effectively trying to<br />minimize this entropy while separating the data.<br />10. These data sets including a short description, the splits into the 100 realizations and the simulation results are<br />available at http://ida.first.gmd.de/˜raetsch/data/benchmarks.htm.<br />11. The breast cancer domain was obtained from the University Medical Center, Inst. of Oncology, Ljubljana,<br />Yugoslavia. Thanks go to M. Zwitter and M. Soklic for providing the data.<br />12. A random partition generates a mapping m of n to two classes. For this a random ±1 vector m of length n is<br />generated. The positive classes (and the negative respectively) are then concatenated.<br />13. The parameters selected by the cross validation are only near-optimal. Only 15–25 values for each parameter<br />are tested in two stages: first a global search (i.e. over a wide range of the parameter space) was done to find<br />a good guess of the parameter, which becomes more precise in the second stage.<br />S= S+O((S−1)2) and?<br />iξ+<br />i/S =?<br />iξ+<br />i+O(S−1). For S = |b| we get algorithm<br />References<br />Bennett, K. (1998). Combining support vector and mathematical programming methods for induction. In B.<br />Sch¨ olkopf, C. Burges, &amp; A. Smola (Eds.), Advances in kernel methods—SV learning. Cambridge, MA: MIT<br />Press.<br />Bennett, K. &amp; Mangasarian, O. (1992). Robust linear programming discrimination of two linearly inseparable<br />sets. Optimization Methods and Software, 1, 23–34.<br />Bertoni,A.,Campadelli,P.,&amp;Parodi,M.(1997).Aboostingalgorithmforregression.InW.Gerstner,A.Germond,<br />M.Hasler,&amp;J.-D.Nicoud(Eds.),LNCS,Vol.V:ProceedingsICANN’97:Int.Conf.onArtificialNeuralNetworks<br />(pp. 343–348). Berlin: Springer.<br />Bishop, C. (1995). Neural Networks for Pattern Recognition. Oxford: Clarendon Press.</p>  <p>Page 33</p> <p>SOFT MARGINS FOR ADABOOST<br />319<br />Boser, B., Guyon, I., &amp; Vapnik, V. (1992). A training algorithm for optimal margin classifiers. In D. Haussler<br />(Ed.), Proceedings COLT’92: Conference on Computational Learning Theory (pp. 144–152). New York, NY:<br />ACM Press.<br />Breiman, L. (1996). Bagging predictors. Mechine Learning, 26(2), 123–140.<br />Breiman, L. (1997a). Arcing the edge. Technical Report 486, Statistics Department, University of California.<br />Breiman, L. (1997b). Prediction games and arcing algorithms. Technical Report 504, Statistics Department,<br />University of California.<br />Breiman, L. (1998). Arcing classifiers. The Annals of Statistics, 26(3), 801–849.<br />Breiman, L. (1999). Using adaptive bagging to debias regressions. Technical Report 547, Statistics Department,<br />University of California.<br />Cortes, C. &amp; Vapnik, V. (1995). Support vector networks. Machine Learning, 20, 273–297.<br />Frean, M. &amp; Downs, T. (1998). A simple cost function for boosting. Technical Report, Department of Computer<br />Science and Electrical Engineering, University of Queensland.<br />Freund, Y. &amp; Schapire, R. (1994). A decision-theoretic generalization of on-line learning and an applica-<br />tion to boosting. In Proceedings EuroCOLT’94: European Conference on Computational Learning Theory.<br />LNCS.<br />Freund, Y. &amp; Schapire, R. (1996). Game theory, on-line prediction and boosting. In Proceedings COLT’86: Conf.<br />on Comput. Learning Theory (pp. 325–332). New York, NY: ACM Press.<br />Friedman, J. (1999). Greedy function approximation. Technical Report, Department of Statistics, Stanford<br />University.<br />Friedman, J., Hastie, T., &amp; Tibshirani, R. (1998). Additive logistic regression: A statistical view of boosting.<br />Technical Report, Department of Statistics, Sequoia Hall, Stanford University.<br />Frieß, T. &amp; Harrison, R. (1998). Perceptrons in kernel feature space. Research Report RR-720, Department of<br />Automatic Control and Systems Engineering, University of Sheffield, Sheffield, UK.<br />Grove, A. &amp; Schuurmans, D. (1998). Boosting in the limit: Maximizing the margin of learned ensembles. In<br />Proceedings of the Fifteenth National Conference on Artifical Intelligence.<br />Kirkpatrick, S. (1984). Optimization by simulated annealing: Quantitative studies. J. Statistical Physics, 34, 975–<br />986.<br />LeCun, Y., Jackel, L., Bottou, L., Cortes, C., Denker, J., Drucker, H., Guyon, I., M¨ uller, U., S¨ ackinger, E., Simard,<br />P., &amp; Vapnik, V. (1995). Learning algorithms for classification: A comparism on handwritten digit recognition.<br />Neural Networks, 261–276.<br />Mangasarian,O.(1965).Linearandnonlinearseparationofpatternsbylinearprogramming.OperationsResearch,<br />13, 444–452.<br />Mason, L., Bartlett, P. L., &amp; Baxter, J. (2000a). Improved generalization through explicit optimization of margins.<br />Machine Learning 38(3), 243–255.<br />Mason,L.,Baxter,J.,Bartlett,P.L.,&amp;Frean,M.(2000b).Functionalgradienttechniquesforcombininghypotheses.<br />In A. J. Smola, P. Bartlett, B. Sch¨ olkopf, &amp; C. Schuurmans (Eds.), Advances in Large Margin Classifiers.<br />Cambridge, MA: MIT Press.<br />Moody, J. &amp; Darken, C. (1989). Fast learning in networks of locally-tuned processing units. Neural Computation,<br />1(2), 281–294.<br />M¨ uller, K.-R., Smola, A., R¨ atsch, G., Sch¨ olkopf, B., Kohlmorgen, J., &amp; Vapnik, V. (1998). Using support vector<br />machines for time series prediction. In B. Sch¨ olkopf, C. Burges, &amp; A. Smola (Eds.), Advances in Kernel<br />Methods—Support Vector Learning. Cambridge, MA: MIT Press.<br />Onoda,T.,R¨ atsch,G.,&amp;M¨ uller,K.-R.(1998).AnasymptoticanalysisofADABOOSTinthebinaryclassification<br />case. In L. Niklasson, M. Bod´ en, &amp; T. Ziemke (Eds.), Proceedings ICANN’98: Int. Conf. on Artificial Neural<br />Networks (pp. 195–200).<br />Onoda, T., R¨ atsch, G., &amp; M¨ uller, K.-R. (2000). An asymptotical analysis and improvement of ADABOOST in the<br />binary classification case. Journal of Japanese Society for AI, 15(2), 287–296 (in Japanese).<br />Press, W., Flannery, B., Teukolsky, S., &amp; Vetterling, W. (1992). Numerical Recipes in C (2nd ed.). Cambridge:<br />Cambridge University Press.<br />Quinlan, J. (1992). C4.5: Programs for Machine Learning. Los Altos, CA: Morgan Kaufmann.<br />Quinlan,J.(1996).Boostingfirst-orderlearning.InS.Arikawa&amp;A.Sharma(Eds.),LNAI,Vol.1160:Proceedings<br />of the 7th International Workshop on Algorithmic Learning Theory (pp. 143–155). Berlin: Springer.</p>  <p>Page 34</p> <p>320<br />G. R¨ATSCH, T. ONODA AND K.-R. M¨ULLER<br />R¨ atsch, G. (1998). Ensemble learning methods for classification. Master’s Thesis, Department of Computer Sci-<br />ence, University of Potsdam, Germany (in German).<br />R¨ atsch,G.,Onoda,T.,&amp;M¨ uller,K.-R.(1998).SoftmarginsforADABOOST.TechnicalReportNC-TR-1998-021,<br />Department of Computer Science, Royal Holloway, University of London, Egham, UK.<br />R¨ atsch, G., Onoda, T., &amp; M¨ uller, K.-R. (1999). Regularizing ADABOOST. In M. Kearns, S. Solla, &amp; D. Cohn<br />(Eds.), Advances in Neural Information Processing Systems 11 (pp. 564–570). Cambridge, MA: MIT Press.<br />R¨ atsch,G.,Sch¨ olkopf,B.,Smola,A.,Mika,S.,Onoda,T.,&amp;M¨ uller,K.-R.(2000).Robustensemblelearning.InA.<br />Smola,P.Bartlett,B.Sch¨ olkopf,&amp;D.Schuurmans(Eds.),AdvancesinLargeMarginClassifiers(pp.207–219).<br />Cambridge, MA: MIT Press.<br />R¨ atsch,G.,Warmuth,M.,Mika,S.,Onoda,T.,Lemm,S.,&amp;M¨ uller,K.-R.(2000).Barrierboosting.InProceedings<br />COLT’00: Conference on Computational Learning Theory (pp. 170–179). Los Altos, CA: Morgan Kaufmann.<br />Rokui, J. &amp; Shimodaira, H. (1998). Improving the generalization performance of the minimum classification error<br />learning and its application to neural networks. In Proc. of the Int. Conf. on Neural Information Processing<br />(ICONIP) (pp. 63–66). Japan, Kitakyushu.<br />Schapire, R. (1999). Theoretical views of boosting. In Proceedings EuroCOLT’99: European Conference on<br />Computational Learning Theory.<br />Schapire,R.,Freund,Y.,Bartlett,P.,&amp;Lee,W.(1997).Boostingthemargin:Anewexplanationfortheeffectiveness<br />of voting methods. In Proceedings ICML’97: International Conference on Machine Learning (pp. 322–330).<br />Los Altos, CA: Morgan Kaufmann.<br />Schapire,R.&amp;Singer,Y.(1998).Improvedboostingalgorithmsusingconfidence-ratedpredictions.InProceedings<br />COLT’98: Conference on Computational Learning Theory (pp. 80–91).<br />Sch¨ olkopf, B. (1997). Support Vector Learning. R. Oldenbourg Verlag, Berlin.<br />Sch¨ olkopf, B., Smola, A., &amp; Williamson, R. (2000). New support vector algorithms. Neural Computation. also<br />NeuroCOLT TR–31–89, 12:1083–1121.<br />Schwenk, H. &amp; Bengio, Y. (1997). AdaBoosting neural networks. In W. Gerstner, A. Germond, M. Hasler, &amp;<br />J.-D. Nicoud (Eds.), Proceedings ICANN’97: Int. Conf. on Artificial Neural Networks, Vol. 1327 of LNCS<br />(pp. 967–972). Berlin: Springer.<br />Smola, A. J. (1998). Learning with kernels. Ph.D. Thesis, Technische Universit¨ at Berlin.<br />Smola, A., Sch¨ olkopf, B., &amp; M¨ uller, K.-R. (1998). The connection between regularization operators and support<br />vector kernels. Neural Networks, 11, 637–649.<br />Tikhonov, A. &amp; Arsenin, V. (1977). Solutions of Ill-Posed Problems. Washington, D.C.: W.H. Winston.<br />Vapnik, V. (1995). The Nature of Statistical Learning Theory. Berlin: Springer.<br />Weston, J. (1999). LOO-support vector machines. In Proceedings of IJCNN’99.<br />Weston, J., Gammerman, A., Stitson, M. O., Vapnik, V., Vovk, V., &amp; Watkins, C. (1997). Density estimation using<br />SV machines. Technical Report CSD-TR-97-23, Royal Holloway, University of London, Egham, UK.<br />Received August 14, 1998<br />Revised June 14, 1999<br />Accepted October 27, 1999<br />Final manuscript September 18, 2000</p>  <a href="https://www.researchgate.net/profile/Klaus-Robert_Mueller/publication/220343914_Soft_Margins_for_AdaBoost/links/0046352aaf02269b25000000.pdf">Download full-text</a> </div> <div id="rgw22_56ab1df743862" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw23_56ab1df743862">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw24_56ab1df743862"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Klaus-Robert_Mueller/publication/220343914_Soft_Margins_for_AdaBoost/links/0046352aaf02269b25000000.pdf" class="publication-viewer" title="0046352aaf02269b25000000.pdf">0046352aaf02269b25000000.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Klaus-Robert_Mueller">Klaus-Robert Müller</a> &middot; May 29, 2014 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw25_56ab1df743862"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://link.springer.com/content/pdf/10.1023%2FA%3A1007618119488.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Soft Margins for AdaBoost">Soft Margins for AdaBoost</a> </div>  <div class="details">   Available from <a href="http://link.springer.com/content/pdf/10.1023%2FA%3A1007618119488.pdf" target="_blank" rel="nofollow">link.springer.com</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw32_56ab1df743862" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (720) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw33_56ab1df743862" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw34_56ab1df743862" >  <div class="indent-left">  <div id="rgw35_56ab1df743862" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview preview-not-available ga-publication-viewer-not-available js-publication-item-fulltext-content" href="publication/282590246_Active_Cleaning_of_Label_Noise">       </a>    </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw36_56ab1df743862">  <li class="citation-context-item"> "This will create a group of base classifiers which 135 correctly predict the examples that have large weights. The work of Ratsch et al. [20] and Dietterich [21] show that AdaBoost tends to overfit in the presence of mislabeled examples. In order to avoid building base classifiers for noisy examples, a method was proposed by Cao et al. [22] to reduce the weights of the noisy examples using kNN and Expectation Maximization methods. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/282590246_Active_Cleaning_of_Label_Noise"> <span class="publication-title js-publication-title">Active Cleaning of Label Noise</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2076161895_Rajmadhan_Ekambaram" class="authors js-author-name ga-publications-authors">Rajmadhan Ekambaram</a> &middot;     <a href="researcher/70970717_Sergiy_Fefilatyev" class="authors js-author-name ga-publications-authors">Sergiy Fefilatyev</a> &middot;     <a href="researcher/75003125_Matthew_Shreve" class="authors js-author-name ga-publications-authors">Matthew Shreve</a> &middot;     <a href="researcher/10901919_Kurt_Kramer" class="authors js-author-name ga-publications-authors">Kurt Kramer</a> &middot;     <a href="researcher/6122168_Lawrence_O_Hall" class="authors js-author-name ga-publications-authors">Lawrence O. Hall</a> &middot;     <a href="researcher/6403646_Dmitry_B_Goldgof" class="authors js-author-name ga-publications-authors">Dmitry B. Goldgof</a> &middot;     <a href="researcher/7239597_Rangachar_Kasturi" class="authors js-author-name ga-publications-authors">Rangachar Kasturi</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Mislabeled examples in the training data can severely affect the performance of supervised classifiers. In this paper, we present an approach to remove any mislabeled examples in the dataset by selecting suspicious examples as targets for inspection. We show that the large margin and soft margin principles used in support vector machines (SVM) have the characteristic of capturing the mislabeled examples as support vectors. Experimental results on two character recognition datasets show that one-class and two-class SVMs are able to capture around 85% and 99% of label noise examples, respectively, as their support vectors. We propose another new method that iteratively builds two-class SVM classifiers on the non-support vector examples from the training data followed by an expert manually verifying the support vectors based on their classification score to identify any mislabeled examples. We show that this method reduces the number of examples to be reviewed, as well as the parameter independence of this method, through experimental results on four data sets. So, by (re-)examining the labels of the selective support vectors, most noise can be removed. This can be quite advantageous when rapidly building a labeled data set. </span> </div>    <div class="publication-meta publication-meta">    No preview  &middot; Article &middot; Sep 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw37_56ab1df743862" >  <div class="indent-left">  <div id="rgw38_56ab1df743862" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/275280232_F-SVM_Combination_of_Feature_Transformation_and_SVM_Learning_via_Convex_Relaxation">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Wangmeng_Zuo" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Wangmeng Zuo </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw39_56ab1df743862">  <li class="citation-context-item"> "reason to choose them is that they had been widely adopted for evaluating SVM and kernel methods [43], [44], [45]. Table I provides a brief summary of these UCI datasets, which includes 6 2-class problems and 5 multi-class problems. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/275280232_F-SVM_Combination_of_Feature_Transformation_and_SVM_Learning_via_Convex_Relaxation"> <span class="publication-title js-publication-title">F-SVM: Combination of Feature Transformation and SVM Learning via Convex Relaxation</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2071997355_Xiaohe_Wu" class="authors js-author-name ga-publications-authors">Xiaohe Wu</a> &middot;     <a href="researcher/11736793_Wangmeng_Zuo" class="authors js-author-name ga-publications-authors">Wangmeng Zuo</a> &middot;     <a href="researcher/2071950427_Yuanyuan_Zhu" class="authors js-author-name ga-publications-authors">Yuanyuan Zhu</a> &middot;     <a href="researcher/70465483_Liang_Lin" class="authors js-author-name ga-publications-authors">Liang Lin</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> The generalization error bound of support vector machine (SVM) depends on the
ratio of radius and margin, while standard SVM only considers the maximization
of the margin but ignores the minimization of the radius. Several approaches
have been proposed to integrate radius and margin for joint learning of feature
transformation and SVM classifier. However, most of them either require the
form of the transformation matrix to be diagonal, or are non-convex and
computationally expensive. In this paper, we suggest a novel approximation for
the radius of minimum enclosing ball (MEB) in feature space, and then propose a
convex radius-margin based SVM model for joint learning of feature
transformation and SVM classifier, i.e., F-SVM. An alternating minimization
method is adopted to solve the F-SVM model, where the feature transformation is
updatedvia gradient descent and the classifier is updated by employing the
existing SVM solver. By incorporating with kernel principal component analysis,
F-SVM is further extended for joint learning of nonlinear transformation and
classifier. Experimental results on the UCI machine learning datasets and the
LFW face datasets show that F-SVM outperforms the standard SVM and the existing
radius-margin based SVMs, e.g., RMM, R-SVM+ and R-SVM+{\mu}. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Apr 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Wangmeng_Zuo/publication/275280232_F-SVM_Combination_of_Feature_Transformation_and_SVM_Learning_via_Convex_Relaxation/links/55481aac0cf2e2031b3863ab.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw40_56ab1df743862" >  <div class="indent-left">  <div id="rgw41_56ab1df743862" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/265788039_Ensembles_of_Random_Sphere_Cover_Classifiers">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Anthony_Bagnall" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Anthony Bagnall </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw42_56ab1df743862">  <li class="citation-context-item"> "For alternative fusion schemes see [25]. Beyond simple accuracy comparison, there are three common approaches to analyse ensemble performance: diversity measures [28] [42]; margin theory [37] [33]; and BV decomposition [22] [43] [14] [5] [44] [2]. These have all been linked [42] [10]. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/265788039_Ensembles_of_Random_Sphere_Cover_Classifiers"> <span class="publication-title js-publication-title">Ensembles of Random Sphere Cover Classifiers</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/70229010_Anthony_Bagnall" class="authors js-author-name ga-publications-authors">Anthony Bagnall</a> &middot;     <a href="researcher/2050878419_Reda_Younsi" class="authors js-author-name ga-publications-authors">Reda Younsi</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> We propose and evaluate alternative ensemble schemes for a new instance based
learning classifier, the Randomised Sphere Cover (RSC) classifier. RSC fuses
instances into spheres, then bases classification on distance to spheres rather
than distance to instances. The randomised nature of RSC makes it ideal for use
in ensembles. We propose two ensemble methods tailored to the RSC classifier;
$\alpha \beta$RSE, an ensemble based on instance resampling and $\alpha$RSSE, a
subspace ensemble. We compare $\alpha \beta$RSE and $\alpha$RSSE to tree based
ensembles on a set of UCI datasets and demonstrates that RSC ensembles perform
significantly better than some of these ensembles, and not significantly worse
than the others. We demonstrate via a case study on six gene expression data
sets that $\alpha$RSSE can outperform other subspace ensemble methods on high
dimensional data when used in conjunction with an attribute filter. Finally, we
perform a set of Bias/Variance decomposition experiments to analyse the source
of improvement in comparison to a base classifier. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Sep 2014  &middot; Pattern Recognition  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Anthony_Bagnall/publication/265788039_Ensembles_of_Random_Sphere_Cover_Classifiers/links/551bc6e40cf2fe6cbf75e774.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw27_56ab1df743862" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw28_56ab1df743862">  </ul> </div> </div>   <div id="rgw18_56ab1df743862" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw19_56ab1df743862"> <div> <h5> <a href="publication/2580330_Soft_Margins_for_AdaBoost" class="color-inherit ga-similar-publication-title"><span class="publication-title">Soft Margins for AdaBoost</span></a>  </h5>  <div class="authors"> <a href="researcher/29697398_G_R_Atsch" class="authors ga-similar-publication-author">G. R Atsch</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw20_56ab1df743862"> <div> <h5> <a href="publication/2945124_Soft_Margins_for_AdaBoost" class="color-inherit ga-similar-publication-title"><span class="publication-title">Soft Margins for AdaBoost</span></a>  </h5>  <div class="authors"> <a href="researcher/11033217_Takashi_Onoda_Gmd" class="authors ga-similar-publication-author">Takashi Onoda Gmd</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw21_56ab1df743862"> <div> <h5> <a href="publication/262237044_Online_Metric_Learning_Methods_Using_Soft_Margins_and_Least_Squares_Formulations" class="color-inherit ga-similar-publication-title"><span class="publication-title">Online Metric Learning Methods Using Soft Margins and Least Squares Formulations</span></a>  </h5>  <div class="authors"> <a href="researcher/70002525_Adrian_Perez-Suay" class="authors ga-similar-publication-author">Adrian Perez-Suay</a>, <a href="researcher/7686205_Francesc_J_Ferri" class="authors ga-similar-publication-author">Francesc J. Ferri</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw52_56ab1df743862" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw53_56ab1df743862">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw54_56ab1df743862" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=c1OOqIOfU0ICdM0noikywxWM8dRyyW4P71aXc63jILRg_08XVockhDrxqTnJtGa2" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="jbqFmAYL85Nouea0pZNYyc7uVECd3XTZ0aT03pt2PvPI1wGDfTipuorFs8SzoveUwFSCm2M/LTLR/H0FXTzxl/NdkG1+TszpirM9CO6bLM7WSWhkVHigVh6ECReOw+RgRskWgNd9GkfK6UAO/9n7i3kYFZ43Jrg5jXc+xePjSQ2gpwCTmx7pU+XDLhR6WncVPmk/am1Y6jYruG/T1EhVSQIXserNOVbBliQuqZ3ddobOl4W9FT56jm4anNvLhzBHATLzlAh/ls34JYYW6iKoBecKREh/IX7l/Xihd556i/c="/> <input type="hidden" name="urlAfterLogin" value="publication/220343914_Soft_Margins_for_AdaBoost"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMzQzOTE0X1NvZnRfTWFyZ2luc19mb3JfQWRhQm9vc3Q%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMzQzOTE0X1NvZnRfTWFyZ2luc19mb3JfQWRhQm9vc3Q%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMzQzOTE0X1NvZnRfTWFyZ2luc19mb3JfQWRhQm9vc3Q%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw55_56ab1df743862"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 1012;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FigureList","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Gunnar R\u00e4tsch","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"widgetId":"rgw4_56ab1df743862"},"id":"rgw4_56ab1df743862","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=27859","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab1df743862"},"id":"rgw3_56ab1df743862","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=220343914","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":220343914,"title":"Soft Margins for AdaBoost","journalTitle":"Machine Learning","journalDetailsTooltip":{"data":{"journalTitle":"Machine Learning","journalAbbrev":"MACH LEARN","publisher":"Springer Verlag","issn":"0885-6125","impactFactor":"1.89","fiveYearImpactFactor":"2.64","citedHalfLife":">10.0","immediacyIndex":"0.21","eigenFactor":"0.01","articleInfluence":"1.71","widgetId":"rgw6_56ab1df743862"},"id":"rgw6_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=0885-6125","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":"University of Potsdam","type":"Article","details":{"doi":"10.1023\/A:1007618119488","journalInfos":{"journal":"","publicationDate":"03\/2001;","publicationDateRobot":"2001-03","article":"42(3):287-320.","journalTitle":"Machine Learning","journalUrl":"journal\/0885-6125_Machine_Learning","impactFactor":1.89}},"source":{"sourceUrl":"http:\/\/dblp.uni-trier.de\/db\/journals\/ml\/ml42.html#RatschOM01","sourceName":"DBLP"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1023\/A:1007618119488"},{"key":"rft.atitle","value":"Soft Margins for AdaBoost"},{"key":"rft.title","value":"Machine Learning"},{"key":"rft.jtitle","value":"Machine Learning"},{"key":"rft.volume","value":"42"},{"key":"rft.issue","value":"3"},{"key":"rft.date","value":"2001"},{"key":"rft.pages","value":"287-320"},{"key":"rft.issn","value":"0885-6125"},{"key":"rft.au","value":"Gunnar R\u00e4tsch,Takashi Onoda,Klaus-Robert M\u00fcller"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56ab1df743862"},"id":"rgw7_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=220343914","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":220343914,"peopleItems":[{"data":{"authorNameOnPublication":"Gunnar R\u00e4tsch","accountUrl":"profile\/Gunnar_Raetsch","accountKey":"Gunnar_Raetsch","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Private Profile","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":false}},"professionalInstitutionName":false,"professionalInstitutionUrl":false,"url":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":null,"hasInfoPopup":false,"hasTeaserPopup":false,"showContactAuthorButton":true,"widgetId":"rgw10_56ab1df743862"},"id":"rgw10_56ab1df743862","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=27859&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":false,"score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":2,"publicationUid":220343914,"widgetId":"rgw9_56ab1df743862"},"id":"rgw9_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=27859&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=2&publicationUid=220343914","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/69960989_Takashi_Onoda","authorNameOnPublication":"Takashi Onoda","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Takashi Onoda","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/69960989_Takashi_Onoda","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56ab1df743862"},"id":"rgw12_56ab1df743862","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=69960989&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56ab1df743862"},"id":"rgw11_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=69960989&authorNameOnPublication=Takashi%20Onoda","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Klaus-Robert M\u00fcller","accountUrl":"profile\/Klaus-Robert_Mueller","accountKey":"Klaus-Robert_Mueller","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Klaus-Robert M\u00fcller","profile":{"professionalInstitution":{"professionalInstitutionName":"Technische Universit\u00e4t Berlin","professionalInstitutionUrl":"institution\/Technische_Universitaet_Berlin"}},"professionalInstitutionName":"Technische Universit\u00e4t Berlin","professionalInstitutionUrl":"institution\/Technische_Universitaet_Berlin","url":"profile\/Klaus-Robert_Mueller","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Klaus-Robert_Mueller","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw14_56ab1df743862"},"id":"rgw14_56ab1df743862","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=4080101&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Technische Universit\u00e4t Berlin","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":2,"publicationUid":220343914,"widgetId":"rgw13_56ab1df743862"},"id":"rgw13_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=4080101&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=2&publicationUid=220343914","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab1df743862"},"id":"rgw8_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=220343914&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":220343914,"abstract":"<noscript><\/noscript><div>Recently ensemble methods like ADABOOST have been applied successfully in many problems, while seemingly defying the problems of overfitting.<br \/>\nADABOOST rarely overfits in the low noise regime, however, we show that it clearly does so for higher noise levels. Central to the understanding of this fact is the margin distribution. ADABOOST can be viewed as a constraint gradient descent in an error function with respect to the margin. We find that ADABOOST asymptotically achieves a hard margin distribution, i.e. the algorithm concentrates its resources on a few hard-to-learn patterns that are interestingly very similar to Support Vectors. A hard margin is clearly a sub-optimal strategy in the noisy case, and regularization, in our case a &ldquo;mistrust&rdquo; in the data, must be introduced in the algorithm to alleviate the distortions that single difficult patterns (e.g. outliers) can cause to the margin distribution. We propose several regularization methods and generalizations of the original ADABOOST algorithm to achieve a soft margin. In particular we suggest (1) regularized ADABOOSTREG where the gradient decent is done directly with respect to the soft margin and (2) regularized linear and quadratic programming (LP\/QP-) ADABOOST, where the soft margin is attained by introducing slack variables.<br \/>\nExtensive simulations demonstrate that the proposed regularized ADABOOST-type algorithms are useful and yield competitive results for noisy data.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw15_56ab1df743862"},"id":"rgw15_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=220343914","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":{"data":{"figures":[{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig1\/Figure-1.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig1\/Figure-1_small.png","figureUrl":"\/figure\/220343914_fig1_Figure-1","selected":false,"title":"Figure 1 .\u00a0","key":"220343914_fig1_Figure-1"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig2\/Figure-2.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig2\/Figure-2_small.png","figureUrl":"\/figure\/220343914_fig2_Figure-2","selected":false,"title":"Figure 2 .\u00a0","key":"220343914_fig2_Figure-2"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig3\/Figure-3.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig3\/Figure-3_small.png","figureUrl":"\/figure\/220343914_fig3_Figure-3","selected":false,"title":"Figure 3 .\u00a0","key":"220343914_fig3_Figure-3"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig4\/Figure-4.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig4\/Figure-4_small.png","figureUrl":"\/figure\/220343914_fig4_Figure-4","selected":false,"title":"Figure 4 .\u00a0","key":"220343914_fig4_Figure-4"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig5\/Figure-5.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig5\/Figure-5_small.png","figureUrl":"\/figure\/220343914_fig5_Figure-5","selected":false,"title":"Figure 5 .\u00a0","key":"220343914_fig5_Figure-5"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig6\/Figure-6.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig6\/Figure-6_small.png","figureUrl":"\/figure\/220343914_fig6_Figure-6","selected":false,"title":"Figure 6 .\u00a0","key":"220343914_fig6_Figure-6"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig7\/Figure-7.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig7\/Figure-7_small.png","figureUrl":"\/figure\/220343914_fig7_Figure-7","selected":false,"title":"Figure 7 .\u00a0","key":"220343914_fig7_Figure-7"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig8\/Figure-8.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig8\/Figure-8_small.png","figureUrl":"\/figure\/220343914_fig8_Figure-8","selected":false,"title":"Figure 8 .\u00a0","key":"220343914_fig8_Figure-8"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig9\/Figure-9.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig9\/Figure-9_small.png","figureUrl":"\/figure\/220343914_fig9_Figure-9","selected":false,"title":"Figure 9 .\u00a0","key":"220343914_fig9_Figure-9"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig10\/Figure-10.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig10\/Figure-10_small.png","figureUrl":"\/figure\/220343914_fig10_Figure-10","selected":false,"title":"Figure 10 .\u00a0","key":"220343914_fig10_Figure-10"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig11\/Figure-11-Margin-distribution-graphs-of-the-RBF-base-hypothesis-scaled-trained-with.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig11\/Figure-11-Margin-distribution-graphs-of-the-RBF-base-hypothesis-scaled-trained-with_small.png","figureUrl":"\/figure\/220343914_fig11_Figure-11-Margin-distribution-graphs-of-the-RBF-base-hypothesis-scaled-trained-with","selected":false,"title":"Figure 11 . Margin distribution graphs of the RBF base hypothesis...","key":"220343914_fig11_Figure-11-Margin-distribution-graphs-of-the-RBF-base-hypothesis-scaled-trained-with"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig12\/Figure-12.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig12\/Figure-12_small.png","figureUrl":"\/figure\/220343914_fig12_Figure-12","selected":false,"title":"Figure 12 .\u00a0","key":"220343914_fig12_Figure-12"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig13\/Figure-13.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig13\/Figure-13_small.png","figureUrl":"\/figure\/220343914_fig13_Figure-13","selected":false,"title":"Figure 13 .\u00a0","key":"220343914_fig13_Figure-13"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig14\/Figure-14.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig14\/Figure-14_small.png","figureUrl":"\/figure\/220343914_fig14_Figure-14","selected":false,"title":"Figure 14 .\u00a0","key":"220343914_fig14_Figure-14"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig15\/Figure-15-Pseudo-code-description-of-the-RBF-net-algorithm-which-is-used-as-base.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914\/figure\/fig15\/Figure-15-Pseudo-code-description-of-the-RBF-net-algorithm-which-is-used-as-base_small.png","figureUrl":"\/figure\/220343914_fig15_Figure-15-Pseudo-code-description-of-the-RBF-net-algorithm-which-is-used-as-base","selected":false,"title":"Figure 15 . Pseudo-code description of the RBF net algorithm, which is...","key":"220343914_fig15_Figure-15-Pseudo-code-description-of-the-RBF-net-algorithm-which-is-used-as-base"}],"readerDocId":"2849244","linkBehaviour":"dialog","isDialog":true,"headerText":"Figures in this publication","isNewPublicationDesign":false,"widgetId":"rgw16_56ab1df743862"},"id":"rgw16_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/FigureList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FigureList.html?readerDocId=2849244&isDialog=1&linkBehaviour=dialog","viewClass":"views.publicliterature.FigureListView","yuiModules":["rg.views.publicliterature.FigureListView","css-pow-publicliterature-FigureList"],"stylesheets":["pow\/publicliterature\/FigureList.css"],"_isYUI":true},"previewImage":"https:\/\/i1.rgstatic.net\/publication\/220343914_Soft_Margins_for_AdaBoost\/links\/0046352aaf02269b25000000\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw17_56ab1df743862"},"id":"rgw17_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab1df743862"},"id":"rgw5_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=220343914&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":29697398,"url":"researcher\/29697398_G_R_Atsch","fullname":"G. R Atsch","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2000","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/2580330_Soft_Margins_for_AdaBoost","usePlainButton":true,"publicationUid":2580330,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/2580330_Soft_Margins_for_AdaBoost","title":"Soft Margins for AdaBoost","displayTitleAsLink":true,"authors":[{"id":29697398,"url":"researcher\/29697398_G_R_Atsch","fullname":"G. R Atsch","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/2580330_Soft_Margins_for_AdaBoost","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/2580330_Soft_Margins_for_AdaBoost\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab1df743862"},"id":"rgw19_56ab1df743862","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=2580330","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":11033217,"url":"researcher\/11033217_Takashi_Onoda_Gmd","fullname":"Takashi Onoda Gmd","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2004","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/2945124_Soft_Margins_for_AdaBoost","usePlainButton":true,"publicationUid":2945124,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/2945124_Soft_Margins_for_AdaBoost","title":"Soft Margins for AdaBoost","displayTitleAsLink":true,"authors":[{"id":11033217,"url":"researcher\/11033217_Takashi_Onoda_Gmd","fullname":"Takashi Onoda Gmd","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/2945124_Soft_Margins_for_AdaBoost","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/2945124_Soft_Margins_for_AdaBoost\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56ab1df743862"},"id":"rgw20_56ab1df743862","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=2945124","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":70002525,"url":"researcher\/70002525_Adrian_Perez-Suay","fullname":"Adrian Perez-Suay","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7686205,"url":"researcher\/7686205_Francesc_J_Ferri","fullname":"Francesc J. Ferri","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Conference Paper","publicationDate":"Nov 2012","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/262237044_Online_Metric_Learning_Methods_Using_Soft_Margins_and_Least_Squares_Formulations","usePlainButton":true,"publicationUid":262237044,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/262237044_Online_Metric_Learning_Methods_Using_Soft_Margins_and_Least_Squares_Formulations","title":"Online Metric Learning Methods Using Soft Margins and Least Squares Formulations","displayTitleAsLink":true,"authors":[{"id":70002525,"url":"researcher\/70002525_Adrian_Perez-Suay","fullname":"Adrian Perez-Suay","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7686205,"url":"researcher\/7686205_Francesc_J_Ferri","fullname":"Francesc J. Ferri","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Proceedings of the 2012 Joint IAPR international conference on Structural, Syntactic, and Statistical Pattern Recognition; 11\/2012"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/262237044_Online_Metric_Learning_Methods_Using_Soft_Margins_and_Least_Squares_Formulations","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/262237044_Online_Metric_Learning_Methods_Using_Soft_Margins_and_Least_Squares_Formulations\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw21_56ab1df743862"},"id":"rgw21_56ab1df743862","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=262237044","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw18_56ab1df743862"},"id":"rgw18_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=220343914&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":220343914,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":220343914,"publicationType":"article","linkId":"0046352aaf02269b25000000","fileName":"0046352aaf02269b25000000.pdf","fileUrl":"profile\/Klaus-Robert_Mueller\/publication\/220343914_Soft_Margins_for_AdaBoost\/links\/0046352aaf02269b25000000.pdf","name":"Klaus-Robert M\u00fcller","nameUrl":"profile\/Klaus-Robert_Mueller","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"May 29, 2014","fileSize":"263.01 KB","widgetId":"rgw24_56ab1df743862"},"id":"rgw24_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=220343914&linkId=0046352aaf02269b25000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":220343914,"publicationType":"article","linkId":"0278d46b0cf2c6a3a06f673f","fileName":"Soft Margins for AdaBoost","fileUrl":"http:\/\/link.springer.com\/content\/pdf\/10.1023%2FA%3A1007618119488.pdf","name":"link.springer.com","nameUrl":"http:\/\/link.springer.com\/content\/pdf\/10.1023%2FA%3A1007618119488.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw25_56ab1df743862"},"id":"rgw25_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=220343914&linkId=0278d46b0cf2c6a3a06f673f&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw23_56ab1df743862"},"id":"rgw23_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=220343914&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":876,"valueFormatted":"876","widgetId":"rgw26_56ab1df743862"},"id":"rgw26_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=220343914","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw22_56ab1df743862"},"id":"rgw22_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=220343914&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":220343914,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw28_56ab1df743862"},"id":"rgw28_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=220343914&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":876,"valueFormatted":"876","widgetId":"rgw29_56ab1df743862"},"id":"rgw29_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=220343914","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw27_56ab1df743862"},"id":"rgw27_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=220343914&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Machine Learning, 42, 287\u2013320, 2001\nc ? 2001 Kluwer Academic Publishers. Manufactured in The Netherlands.\nSoft Margins for AdaBoost\nG. R\u00a8ATSCH\nGMD FIRST,\u2217Kekul\u00b4 estr. 7, 12489 Berlin, Germany\nraetsch@first.gmd.de\nT. ONODA\nCRIEPI,\u2020Komae-shi, 2-11-1 Iwado Kita, Tokyo, Japan\nonoda@criepi.denken.or.jp\nK.-R. M\u00a8ULLER\nGMD FIRST, Kekul\u00b4 estr. 7, 12489 Berlin, Germany; University of Potsdam,\u2217Neues Palais 10,\n14469 Potsdam, Germany\nklaus@first.gmd.de\nEditor: Robert Schapire\nAbstract.\nwhile seemingly defying the problems of overfitting.\nADABOOST rarely overfits in the low noise regime, however, we show that it clearly does so for higher noise\nlevels.Centraltotheunderstandingofthisfactisthemargindistribution.ADABOOSTcanbeviewedasaconstraint\ngradientdescentinanerrorfunctionwithrespecttothemargin.WefindthatADABOOSTasymptoticallyachieves\na hard margin distribution, i.e. the algorithm concentrates its resources on a few hard-to-learn patterns that are\ninterestingly very similar to Support Vectors. A hard margin is clearly a sub-optimal strategy in the noisy case, and\nregularization, in our case a \u201cmistrust\u201d in the data, must be introduced in the algorithm to alleviate the distortions\nthat single difficult patterns (e.g. outliers) can cause to the margin distribution. We propose several regularization\nmethods and generalizations of the original ADABOOST algorithm to achieve a soft margin. In particular we\nsuggest (1) regularized ADABOOSTREGwhere the gradient decent is done directly with respect to the soft margin\nand (2) regularized linear and quadratic programming (LP\/QP-) ADABOOST, where the soft margin is attained\nby introducing slack variables.\nExtensive simulations demonstrate that the proposed regularized ADABOOST-type algorithms are useful and\nyield competitive results for noisy data.\nRecently ensemble methods like ADABOOST have been applied successfully in many problems,\nKeywords:\nADABOOST, arcing, large margin, soft margin, classification, support vectors\n1. Introduction\nBoosting and other ensemble1learning methods have been recently used with great success\nin applications like OCR (Schwenk & Bengio, 1997; LeCun et al., 1995). But so far the\nreduction of the generalization error by Boosting algorithms has not been fully understood.\nFor low noise cases Boosting algorithms are performing well for good reasons (Schapire\net al., 1997; Breiman, 1998). However, recent studies with highly noisy patterns (Quinlan,\n1996; Grove & Schuurmans, 1998; R\u00a8 atsch et al., 1998) showed that it is clearly a myth that\nBoosting methods do not overfit.\n\u2217www.first.gmd.de\n\u2020criepi.denken.or.jp\n\u2217www.uni-potsdam.de"},{"page":2,"text":"288\nG. R\u00a8ATSCH, T. ONODA AND K.-R. M\u00a8ULLER\nIn this work, we try to gain insight into these seemingly contradictory results for the low\nand high noise regime and we propose improvements of ADABOOST that help to achieve\nnoise robustness.\nDuetotheirsimilarity,wewillreferinthefollowingtoADABOOST(Freund&Schapire,\n1994) and unnormalized Arcing (Breiman, 1997b) (with exponential function) as\nADABOOST-typealgorithms(ATA).InSection2wegiveanasymptoticalanalysisofATAs.\nWe find that the error function of ATAs can be expressed in terms of the margin and that in\nevery iteration ADABOOST tries to minimize this error by a stepwise maximization of the\nmargin (see also Breiman, 1997a; Frean & Downs, 1998; Friedman, Hastie, & Tibshirani,\n1998; Onoda, R\u00a8 atsch, & M\u00a8 uller, 1998; R\u00a8 atsch, 1998). As a result of the asymptotical anal-\nysis of this error function, we introduce the hard margin concept and show connections to\nSupport Vector (SV) learning (Boser, Guyon, & Vapnik, 1992) and to linear programming\n(LP). Bounds on the size of the margin are also given.\nIn Section 3 we explain why an ATA that enforces a hard margin in training will overfit\nfor noisy data or overlapping class distributions. So far, we only know what a margin distri-\nbution to achieve optimal classification in the no-noise case should look like: a large hard\nmargin is clearly a good choice (Vapnik, 1995). However, for noisy data there is always the\ntradeoff between \u201cbelieving\u201d in the data or \u201cmistrusting\u201d it, as the very data point could\nbe mislabeled or an outlier. So we propose to relax the hard margin and to regularize by\nallowing for misclassifications (soft margin). In Section 4 we introduce such a regulariza-\ntion strategy to ADABOOST and subsequently extend the LP-ADABOOST algorithm of\nGrove and Schuurmans (1998) by slack variables to achieve soft margins. Furthermore, we\nproposeaquadraticprogrammingADABOOSTalgorithm(QP-ADABOOST)andshowits\nconnections to SUPPORT VECTOR MACHINES (SVMs).\nFinally, in Section 5 numerical experiments on several artificial and real-world data sets\nshow the validity and competitiveness of our regularized Boosting algorithms. The paper\nconcludes with a brief discussion.\n2.Analysis of ADABOOST\u2019s learning process\n2.1.Algorithm\nLet {ht(x) : t = 1,...,T} be an ensemble of T hypotheses defined on an input vector\nx \u2208 X and let c = [c1\u00b7 \u00b7 \u00b7 cT] be their weights satisfying ct \u2265 0 and?T\nhowever,canbetransferedeasilytoclassificationwithmorethantwoclasses(e.g.Schapire,\n1999; Schapire & Singer, 1998; Breiman, 1997b).\nThe ensemble generates the label \u02dcf (x) \u2261 \u02dcfT(x) which is the weighted majority of the\nvotes, where\nt=1ct = 1. We\nwill consider only the binary classification case in this work, i.e. ht(x) = \u00b11; most results,\nfT(x) :=\nT ?\nt=1\nctht(x)\nand\n\u02dcfT(x) := sign( fT(x)).\nIn order to train the ensemble, i.e. to find T appropriate hypotheses {ht(x)} and the"},{"page":3,"text":"SOFT MARGINS FOR ADABOOST\n289\nweights c for the convex combination, several algorithms have been proposed: popular\nones are WINDOWING (Quinlan, 1992), BAGGING (Breiman, 1996), ADABOOST (Freund\n& Schapire, 1994), ARC-X4 (Breiman, 1998) and ARC-GV (Breiman, 1997b). In the\nsequel analysis, we will focus on ADABOOST-type algorithms and give their pseudo-\ncode in figure 1 (further details can be found in e.g. Freund & Schapire, 1994; Breiman,\n1997b).\nInthebinaryclassificationcase,wedefinethemarginforaninput-outputpairzi= (xi, yi)\nas\n\u03c1(zi,c) = yif (xi) = yi\nT ?\nt=1\ncthr(xi),\n(1)\nFigure 1.\nrithm (Freund & Schapire, 1994). The ATA is a specialization of unnormalized Arcing (Breiman, 1997a) (with\nexponential function).\nThe ADABOOST-type algorithm (ATA). For \u03c6 =\n1\n2, we retrieve the original ADABOOST algo-"},{"page":4,"text":"290\nG. R\u00a8ATSCH, T. ONODA AND K.-R. M\u00a8ULLER\nwherei = 1,...,l andl denotesthenumberoftrainingpatterns.Themarginatzispositive\nif the correct class label of the pattern is predicted. As the positivity of the margin value\nincreases, the decision stability becomes larger. Moreover, if |c| :=?T\nb (instead of c) which denotes simply an unnormalized version of c, i.e. usually |b| ?= 1\n(cf. (F1.2) and (F1.4) in figure 1). Note that the edge (cf. Breiman, 1997b) is just an affine\ntransformation of the margin.\nThe margin ?(c) of a classifier (instance) is defined as the smallest margin of a pattern\nover the training set, i.e.\nt=1ct = 1, then\n\u03c1(zi,c) \u2208 [\u22121,1]. We will sometimes for convenience also use a margin definition with\n?(c) = min\nFigure 2 illustrates the functioning of ADABOOST. Patterns that are misclassified get\nhigher weights in the next iteration. The patterns near the decision boundary are usually\nharder to classify and therefore get high weights after a few iterations.\ni=1,...,l\u03c1(zi,c).\n2.2. Error function of ADABOOST\nAn important question in the analysis of ATAs is: what kind of error function is optimized?\nFrom the algorithmic formulation (cf. figure 1), it is not straight forward to see what the\nFigure 2.\nthe class label) is proportional to the weight that the pattern gets in the first, second, third, 5th, 10th and 100th\niteration. The dash-dotted lines show the decision boundaries of the single classifiers (up to the 5th iteration). The\nsolid line shows the decision line of the combined classifier. In the last two plots the decision line of BAGGING is\nplotted for a comparison (dotted).\nIllustration of ADABOOST on a 2D toy data set: The diameter of the points (the brightness gives"},{"page":5,"text":"SOFT MARGINS FOR ADABOOST\n291\naim of this algorithm is. So to gain a better understanding why one should use the weights\nof the hypotheses ctand of the patterns wt(zi) in the manner of Eqs. (F1.2) and (F1.3), let\nus study the following three statements\n1. The weights wt(zi) in the t-th iteration are chosen such that the previous hypothesis has\nexactly a weighted training error ? of 1\/2 (Schapire et al., 1997).\n2. The weight bt(and ct) of a hypothesis is chosen such that it minimizes a functional G\nfirst introduced by Breiman (1997b) (see also R\u00a8 atsch, 1998; Mason, Bartlett, & Baxter,\n2000a;Onodaetal.,1998;Friedmanetal.,1998;Frean&Downs,1998).Thisfunctional\ndepends on the margins of all patterns and is defined by\nG(bt,bt\u22121) =\nl ?\ni=1\nexp\n?\n\u2212\u03c1(zi,bt) + |bt|\n?1\n2\u2212 \u03c6\n??\n,\n(2)\nwhere \u03c6 is a constant (cf. figure 1). This functional can be minimized analytically\n(Breiman, 1997b) and one gets the explicit form of Eq. (F1.2) as a solution of\n\u2202G(bt,bt\u22121)\n\u2202bt\n=0.\n3. To train the t-th hypothesis (step 1 in figure 1) we can either use bootstrap replicates\nof the training set (sampled according to wt) or minimize a weighted error function for\nthe base learning algorithm. We observed that the convergence of the ATA is faster if a\nweighted error function is used.\nTaking a closer look at the definition of G, one finds that the computation of the sample\ndistribution wt(cf. Eq. (F1.3)) can be derived directly from G. Assuming that G is the error\nfunction which is minimized by the ATA, then G essentially defines a loss function over\nmargin distributions, which depends on the value of |b|. The larger the margins \u03c1(zi), the\nsmaller will be the value of G.\nSo, the gradient\nmarginmoststronglyinordertodecreaseG maximally(gradientdescent).Thisinformation\ncanthenbeusedtocomputeare-weightingofthesampledistributionwtfortrainingthenext\nhypothesis ht. If it is important to increase the margin of a pattern zi, then its weight wt(zi)\nshouldbehigh\u2014otherwiselow(becausethedistributionwtsumstoone).Interestingly,this\nis exactly what ATAs are doing and we arrive at the following lemma (Breiman, 1997b;\nR\u00a8 atsch, 1998):\n\u2202G\n\u2202\u03c1(zi)gives an answer to the question, which pattern should increase its\nLemma 1.\nalent to normalizing the gradient of G(bt+1,bt) with respect to \u03c1(zi,bt), i.e.\n?\nj=1\nThe computation of the pattern distribution wt+1in the t-th iteration is equiv-\nwt+1(zi) =\u2202G(bt+1,bt)\n\u2202\u03c1(zi,bt)\nl ?\n\u2202G(bt+1,bt)\n\u2202\u03c1(zj,bt)\n.\n(3)\nThe proof can found in Appendix A.\nFrom Lemma 1, the analogy to a gradient descent method is (almost) complete. In a\ngradient descent method, the first step is to compute the gradient of the error function with"},{"page":6,"text":"292\nG. R\u00a8ATSCH, T. ONODA AND K.-R. M\u00a8ULLER\nrespect to the parameters which are to be optimized: this corresponds to computing the\ngradient of G with respect to the margins. The second step is to determine the step size in\ngradient direction (usually done by a line-search): this is analogous to the minimization of\nG with respect to bt(cf. point 2).\nTherefore,ATAscanberelatedtoagradientdescentmethodinahypothesis(orfunction)\nspace H which is determined by the structure of the base learning algorithm, i.e. ATAs aim\nto minimize the functional G by constructing an ensemble of classifiers (Onoda et al.,\n1998; R\u00a8 atsch, 1998; Mason et al., 2000b; Friedman et al., 1998; Friedman, 1999). This also\nexplains point 1 in the list above, as in a standard gradient descent method, a new search\ndirection is usually chosen perpendicular to the previous one.\nIn the ADABOOST-type algorithm, the gradients are found by changing the weights of\nthe training patterns, and there are essentially two ways of incorporating the re-weighting\ninto the boosting procedure. The first is to create bootstrap replicates sampled according to\nthe pattern weightings, which usually induces strong random effects that hide the \u201ctrue\u201d in-\nformation contained in the pattern weightings. The second and more direct way is to weight\nthe error function and use weighted minimization (Breiman, 1997b). Clearly, weighted\nminimization is more efficient in terms of the number of boosting iterations than the boot-\nstrap approach.2In fact, it can be shown that employing weighted minimization (Breiman,\n1997b) for finding the next hypothesis in each iteration leads to the best (single) hypothesis\nfor minimizing G (Mason et al., 2000a), i.e. adding the hypothesis with smallest weighted\ntraining error ?twill lead to the smallest value of G and therefore to a fast convergence.\nThis reasoning explains the third statement.\n2.3.ADABOOST as an annealing process\nFrom the definition of G and \u03c1(zi,bt), Eq. (3) can be rewritten as\nwt+1(zi) =\nexp?\u22121\n2\u03c1(zi,ct)?|bt|\n?l\nj=1exp?\u22121\n2\u03c1(zj,ct)?|bt|,\n(4)\nwhere we emphasize that |bt| can be written in the exponent. Inspecting this equation more\nclosely, we see that ATA uses a soft-max function (e.g. Bishop, 1995) with parameter |bt|\nthat we would like to interpret as an annealing parameter (Onoda et al., 1998; Onoda,\nR\u00a8 atsch, & M\u00a8 uller, 2000). In the beginning |bt| is small and all patterns have almost the\nsame weights (if |bt| = 0 then all weights are the same). As |bt| increases, the patterns\nwith smallest margin will get higher and higher weights. In the limit of large |bt|, we arrive\nat the maximum function: Only the pattern(s) with the smallest margin will be taken into\naccount for learning and get a non-zero weight.\nNote that in the limit for |bt| \u2192 \u221e, a simple rescaling of the error function G(bt) gives\nthe minimum margin ?(zi,ct), i.e. ?(zi,ct) = \u2212lim|bt|\u2192\u221e\nThe following lemma shows that under usual circumstances, the length of the hypothesis\nweight vector |bt| increases at least linearly with the number of iterations.\n1\n|bt|logG(bt)."},{"page":7,"text":"SOFT MARGINS FOR ADABOOST\n293\nLemma 2.\nerrors ?tare bounded by ?t\u2264 \u03c6 \u2212? (0 < ? < \u03c6), then |b| increases at least linearly with\nthe number of iterations t.\nIf, in the learning process of an ATA with 0 < \u03c6 < 1, all weighted training\nProof:\nhave bt = log\n\u03c6(1 \u2212 \u03c6) > ?(1 \u2212 \u03c6) we get \u03c6(1 \u2212 \u03c6 + ?) > ? and hence also q \u2212 ? > 0. Therefore,\nthe smallest value of btis log\non \u03c6 and ?. Thus, we have |bt| > t\u03b3.\nWith (F1.2), the smallest value for bt is achieved, if ?t = \u03c6 \u2212 ?. Then we\nq\nq\u2212?, where q := \u03c6(1 \u2212 \u03c6 + ?). We find q > ? > 0 and because of\nq\nq\u2212?is always larger than a constant \u03b3, which only depends\n2\nIf the annealing speed is low, the achieved solution should have larger margins than for\na high speed annealing strategy. This holds for similar reasons as for a standard annealing\nprocess (Kirkpatrick, 1984): in the error surface, a better local minimum (if exist) can be\nobtained locally, if the annealing speed is slow enough. From Eq. (F1.2), we observe that\nif the training error ?ttakes a small value, btbecomes large. So, strong learners can reduce\ntheir training errors strongly and will make |b| large after only a few ATA iterations, i.e.\nthe asymptotics, where the addition of a new ensemble member does not change the result,\nis reached faster. To reduce the annealing speed either \u03c6 or the complexity of the base\nhypotheses has to be decreased (with the constraint ?t< \u03c6 \u2212 ?; cf. Onoda et al., 1998).\nIn figure 3 (left), the ADABOOST Error (\u03c6 =1\nKullback-Leibler Error ln \u03c1(z)\/ln 2 are plotted. Interestingly, the Squared and Kullback-\nLeibler Error are very similar to the error function of ADABOOST for |b| = 3. As |b|\nincreases,3the ATA error function approximates a 0\/\u221e loss (patterns with margin smaller\nthan1\u22122\u03c6 getloss\u221e;allothershaveloss0)andboundsthe0\/1lossat1\u22122\u03c6 fromabove.\n2), the Squared Error (y \u2212 f (x))2and the\nFigure 3.\nand the y-coordinate shows the monotone loss for that pattern: 0\/1-Loss, Squared Error, Kullback-Leibler Error\nand ADABOOST Error (cf. Eq. (2)), where |b| is either 3, 5, 10 or 100 (in reading order). On the left panel is\n\u03c6 = 1\/2 and on the right plot \u03c6 is either 1\/3 or 2\/3. The step position of the 0\/\u221e loss, which is approximated\nfor |b| \u2192 \u221e, is determined by \u03c6.\nDifferent loss functions for classification (see text). The abscissa shows the margin yf (x) of a pattern"},{"page":8,"text":"294\nG. R\u00a8ATSCH, T. ONODA AND K.-R. M\u00a8ULLER\nFigure 3 (right) shows the different offsets of the step seen in the ATA Error (|b| \u2192 \u221e).\nThey result from different values of \u03c6 (here \u03c6 is either 1\/3 or 2\/3).\n2.4. Asymptotic analysis\n2.4.1.Howlargeisthemargin? ATA\u2019sgoodgeneralizationperformancecanbeexplained\nintermsofthesizeofthe(hard)marginthatcanbeachieved(Schapireetal.,1997;Breiman,\n1997b):forlownoise,thehypothesiswiththelargestmarginwillhavethebestgeneralization\nperformance(Vapnik,1995;Schapireetal.,1997).Thus,itisinterestingtounderstandwhat\nthe margin size depends on.\nGeneralizing Theorem 5 of Freund and Schapire (1994) to the case \u03c6 ?=1\n2we get\nTheorem 3.\nare generated by running an ATA and 1 > \u03c6 > maxt=1,...,T?t. Then the following\ninequality holds for all \u03b8 \u2208 [\u22121,1]:\nAssume, ?1,...,?Tare the weighted classification errors of h1,...,hTthat\n1\nl\nl ?\ni=1\nI(yif (xi) \u2264 \u03b8) \u2264?\u03d5\n1+\u03b8\n2 + \u03d5\u22121\u2212\u03b8\n2?T\nT ?\nt=1\n?\n?1\u2212\u03b8\nt\n(1 \u2212 ?t)1+\u03b8,\n(5)\nwhere f is the final hypothesis and \u03d5 =\n\u03c6\n1\u2212\u03c6, where I is the indicator function.\nThe proof can be found in Appendix B.\nCorollary4.\ngin distributions with a margin ?, which is bounded from below by\nAnADABOOST-typealgorithmwillasymptotically(t \u2192 \u221e)generatemar-\n? \u2265ln(\u03c6?\u22121) + ln((1 \u2212 \u03c6)(1 \u2212 ?)\u22121)\nln(\u03c6?\u22121) \u2212 ln((1 \u2212 \u03c6)(1 \u2212 ?)\u22121),\nwhere ? = maxt?t, if ? \u2264 (1 \u2212 ?)\/2 is satisfied.\nThe maximum of ?1\u2212\u03b8\nfor 0 \u2264 ?t\u22641\n? in Eq. (5) for \u03b8 \u2264 ?:\nP(x,y)\u223cZ[yf (x) \u2264 \u03b8] \u2264??\u03d5\nIf the basis on the right hand side is smaller than 1, then asymptotically we have P(x,y)\u223cZ\n[yf (x) \u2264 \u03b8] = 0; this means that asymptotically, there is no example that has a smaller\nmargin than \u03b8, for any \u03b8 < 1. The supremum over all \u03b8 such that the basis is less than\n1,\u03b8maxis described by\n(6)\nProof:\nt\n(1 \u2212 ?t)1+\u03b8with respect to ?tis obtained for1\n2(1 \u2212 \u03b8) it is increasing monotonically in ?t. Therefore, we can replace ?tby\n2(1 \u2212 \u03b8) and\n1+\u03b8\n2 + \u03d5\u22121\u2212\u03b8\n2??\n1\u2212\u03b8\n2(1 \u2212 ?)\n1+\u03b8\n2?T.\n?\u03d5\n1+\u03b8max\n2\n+ \u03d5\u22121\u2212\u03b8max\n2\n??\n1\u2212\u03b8max\n2\n(1 \u2212 ?)\n1+\u03b8max\n2\n= 1."},{"page":9,"text":"SOFT MARGINS FOR ADABOOST\n295\nWe can solve this equation to obtain \u03b8max\n\u03b8max=ln(\u03c6?\u22121) + ln((1 \u2212 \u03c6)(1 \u2212 ?)\u22121)\nln(\u03c6?\u22121) \u2212 ln((1 \u2212 \u03c6)(1 \u2212 ?)\u22121).\nWe get the assertion because ? is always larger or equal \u03b8max.\n2\nFrom Eq. (6) we can see the interaction between \u03c6 and ?: if the difference between ? and\n\u03c6 is small, then the right hand side of (6) is small. The smaller \u03c6 is, the more important\nthis difference is. From Theorem 7.2 of Breiman (1997b) we also have the weaker bound\n? \u2265 1 \u2212 2\u03c6 and so, if \u03c6 is small then ? must be large, i.e. choosing a small \u03c6 results in a\nlarger margin on the training patterns. On the other hand, an increase of the complexity of\nthe basis algorithm leads to an increased ?, because the error ?twill decrease.\n2.4.2. Support patterns.\nis predominantly achieved by improvements of the margin \u03c1(zi,c). If the margin \u03c1(zi,c) is\nnegative,thentheerrorG(c,|b|)clearlytakesalargevalue,amplifiedby|b|intheexponent.\nSo, ATA tries to decrease the negative margins most efficiently in order to improve the error\nG(c,|b|).\nNow let us consider the asymptotic case, where the number of iterations and therefore\n|b| take large values (cf. Lemma 2). Here, the margins \u03c1(zi,c) of all patterns z1,...,zl,\nare almost the same and small differences are amplified strongly in G(c,|b|). For example,\ngiven two margins \u03c1(zi,c) = 0.2 and \u03c1(zj,c) = 0.3 and |b| = 100, then this difference is\namplified exponentially exp{\u2212100\u00d70.2\nby a factor of e5\u2248 150. From Eq. (4) we see that as soon as the annealing parameter |b| is\nlarge, the ATA learning becomes a hard competition case: only the patterns with smallest\nmarginwillgethighweights,otherpatternsareeffectivelyneglectedinthelearningprocess.\nWe get the following interesting lemma.\nA decrease in the functional G(c,|b|) := G(b) (with c = b\/|b|)\n2\n} = e\u221210and exp{\u2212100\u00d70.3\n2\n} = e\u221215in G(c,|b|), i.e.\nLemma5.\nof each class will asymptotically converge to the same value, i.e.\nDuringtheATAlearningprocess,thesmallestmarginofthe(training)patterns\nlim\nt\u2192\u221emin\ni:yi=1\u03c1(zi,ct) = lim\nt\u2192\u221emin\ni:yi=\u22121\u03c1(zi,ct),\n(7)\nif the following assumptions are fulfilled:\n1. the weight of each hypothesis is bounded from below and above by\n0 < \u03b3 < bt< ? < \u221e,and\n(8)\n2. the learning algorithm must (in principle) be able to classify all patterns to one class\nc \u2208 {\u00b11}, if the sum over the weights of patterns of class c is larger than a constant \u03b4,\ni.e.\n?\nThe proof can be found in Appendix C.\ni:yi=c\nw(zi) > \u03b4 \u21d2 h(xi) = c\n(i = 1,...,l).\n(9)"},{"page":10,"text":"296\nG. R\u00a8ATSCH, T. ONODA AND K.-R. M\u00a8ULLER\nFigure 4.\n(solid) with RBF nets (13 centers) as base hypotheses (left); and with 7 (dotted), 13 (dashed), 30 (solid) centers\nin the base hypotheses for data with \u03c32= 16% (right) after 104ADABOOST iterations (\u03c6 = 1\/2). These graphs\nare an experimental confirmation of the trends expected from Eq. (6).\nMargin distributions of ADABOOST for different noise levels: \u03c32= 0% (dotted), 9% (dashed), 16%\nNote 6.\nmisclassify every single pattern. Effectively, this assumption introduces something like a\nbiastermbthatisautomaticallyadjustedifthesmallestmarginsofoneclassaresignificantly\ndifferentfromtheotherclass.InSVMsfortheseparablecase(Boseretal.,1992),bisdirectly\ncomputed such that the smallest margins of both classes are the same.\nAssumption 2 of Lemma 5 ensures, that the classifier is in principle able to\nTherefore, the ATA learning process converges, under rather mild assumptions, to a\nsolutionwhereasubsetofthetrainingpatternshasasymptoticallythesamesmallestmargin.\nWe call these patterns Support Patterns (SPs) (cf. figure 4).\nTo validate our theoretical analysis we performed numerical simulations on toy data with\nanasymptoticnumber(104)ofboostingsteps.Thetrainingdatawasgeneratedfromseveral\n(non-linearly transformed) Gaussian and uniform blobs,4which were additionally distorted\nby uniformly distributed noiseU(0.0,\u03c32). In our simulations, we used 300 patterns and \u03c32\nis either 0%, 9%, or 16%.\nIn all simulations, radial basis function (RBF) networks with adaptive centers are used\nas base learners (cf. Appendix D or M\u00a8 uller et al., 1998 for a detailed description). Figure 4\nshowsthemargindistributionsafter104boostingiterationsatdifferentnoiselevels\u03c32(left)\nand for different strengths of the base hypotheses (right). From these figures, it becomes\napparent that the margin distribution asymptotically makes a step at a specific margin size\nand that some subset of the training patterns all have similar margins that correspond to\nthe minimal margin discussed above. If the noise level is high or the complexity of the\nbase hypothesis is low, one gets higher training errors ?tand therefore a smaller value of\n?. These numerical results support our theoretical asymptotic analysis. Interestingly, the\nmargin distributions of ATAs resembles the ones of SUPPORT VECTOR MACHINES for the\nseparable case (Boser et al., 1992; Cortes & Vapnik, 1995; Vapnik, 1995, cf. figure 5). In\nour toy example (cf. figure 6) we show the decision lines of SVMs and ATAs. We note\na very high overlap between the patterns that become support vectors (SVs) (cf. figure 6"},{"page":11,"text":"SOFT MARGINS FOR ADABOOST\n297\nFigure 5.\nwith C = 10\u22123(dashed) and C = 10\u22121(dash-dotted). Here for the same toy example a RBF kernel (width = 0.3)\nis used. The generalization error of the SVM with hard margin is more than two times larger as with C = 10\u22121.\nTypical margin distribution graphs (normalized) of a SVM with hard margin (solid) and soft margin\nFigure 6.\n(right) for a low noise case with similar generalization errors. The positive and negative training patterns are\nshown as \u2018+\u2019 and \u2018\u2217\u2019 respectively, the support patterns and support vectors are marked with \u2018o\u2019.\nTraining patterns with decision lines for ADABOOST (left) with RBF nets (13 centers) and SVM\nright) and the patterns that lie within the step part of the margin distribution for ATA (cf.\nfigure 4 left).\nSo,theADABOOST-typealgorithmachievesasymptoticallyadecisionwithhardmargin,\nvery similar to the one of SVMs for the separable case. Intuitively this is clear: the most\ndifficult patterns are emphasized strongly and become support patterns or support vectors\nasymptotically. The degree of overlap between the support vectors and support patterns\ndependsonthekernel(SVM)andonthebasehypothesis(ATA)beingused.FortheSUPPORT\nVECTOR MACHINE with RBF kernel the highest overlap was achieved, when the average\nwidths of the RBF networks was used as kernel width for the SUPPORT VECTOR MACHINE"},{"page":12,"text":"298\nG. R\u00a8ATSCH, T. ONODA AND K.-R. M\u00a8ULLER\nFigure 7.\n(dashed) and 104(solid) iterations. Here, the toy example (300 patterns, \u03c3 = 16%) and RBF networks with 30\ncenters are used. After already 200 iterations the asymptotical convergence is almost reached.\nTypical margin distribution graphs of (original) ADABOOST after 20 (dotted), 70 (dash-dotted), 200\n(R\u00a8 atsch, 1998). We have also observed this striking similarity of SPs in ADABOOST and\nSVs of the SVM in several other non-toy applications.\nIn the sequel, we can often assume the asymptotic case, where a hard margin is achieved\n(the more hypotheses we combine, the better is this approximation). Experimentally we\nfind that the hard margin approximation is valid (cf. Eq. (4)) already for e.g. |b| > 100.\nThis is illustrated by figure 7, which shows some typical ATA margin distributions after 20,\n70, 200 and 104iterations.\nTo recapitulate our findings of this section:\n1. ADABOOST-typealgorithmsaimtominimizeafunctional,whichdependsonthemargin\ndistribution. The minimization is done by means of a constraint gradient descent with\nrespect to the margin.\n2. Some training patterns, which are in the area of the decision boundary, have asymptot-\nically (for a large number of boosting steps) the same margin. We call these patterns\nSupport Patterns. They have a large overlap to the SVs found by a SVM.\n3. Asymptotically,ATAsreachahardmargincomparabletotheoneobtainedbytheoriginal\nSVM approach (Boser et al., 1992).\n4. Larger hard margins can be achieved, if ?t(more complex base hypotheses) and\/or \u03c6\nare small (cf. Corollary 4). For the low noise case, a choice of \u03b8 ?=\nbetter generalization performance, as shown for e.g. OCR benchmark data in Onoda\net al. (1998).\n1\n2can lead to a\n3.Hard margin and overfitting\nIn this section, we give reasons why the ATA is not noise robust and exhibits suboptimal\ngeneralization ability in the presence of noise. According to our understanding, noisy data\nhas at least one of the following properties: (a) overlapping class probability distributions,"},{"page":13,"text":"SOFT MARGINS FOR ADABOOST\n299\n(b) outliers and (c) mislabeled patterns. All three types of noise appear very often in data\nanalysis. Therefore the development of a noise robust version of ADABOOST is very\nimportant.\nThe first theoretical analysis of ADABOOST in connection with margin distributions\nwas done by Schapire et al. (1997). Their main result is a bound on the generalization error\nPz\u223cD[\u03c1(z) \u2264 0] depending on the VC-dimension d of the base hypotheses class and on the\nmargin distribution on the training set. With probability at least 1 \u2212 \u03b4\nPz\u223cD[\u03c1(z) \u2264 0] \u2264 Pz\u223cZ[\u03c1(z) \u2264 \u03b8] + O\n?1\n\u221al\n?d log2(l\/d)\n\u03b82\n+ log(1\/\u03b4)\n??\n(10)\nissatisfied,where\u03b8 > 0andl denotesthenumberofpatterns.Itwasstatedthatthereasonfor\nthesuccessofADABOOST,comparedtootherensemblelearningmethods(e.g.BAGGING),\nis the maximization of the margin. The authors observed experimentally that ADABOOST\nmaximizes the margin of patterns which are most difficult, i.e. have the smallest margin and\nthat on the other hand by increasing the minimum margin of a few patterns, the margin of\nthe rest of the other patterns is also reduced.\nInBreiman(1997b),theconnectionbetweenmaximizingthesmallestmarginandagood\ngeneralizationerrorwasanalyzedexperimentallyandcouldnotbeconfirmedfornoisydata.\nIn Grove and Schuurmans (1998) the Linear Programming (LP) approach of Freund and\nSchapire (1996) and Breiman (1997b) was extended and used to maximize the smallest\nmargin of an existing ensemble of classifiers. Several experiments with LP-ADABOOST\nonUCIbenchmarks(oftennoisydata)weremadeanditwasunexpectedlyobservedthatLP-\nADABOOST performs in almost all cases worse than the original ADABOOST algorithm,\neven though the smallest observed margins were larger.\nOurexperimentshaveshownthatasthemarginincreases,thegeneralizationperformance\nbecomes better on data sets with almost no noise (e.g. OCR, cf. Onoda et al., 1998),\nhowever, we also observe that ADABOOST overfits on noisy data (for a moderate number\nof combined hypotheses).\nAsanexampleforoverlappingclasses,figure8(left)showsatypicaloverfittingbehavior\nin the generalization error for ADABOOST on the same data as in Section 2. Here, already\nafter only 80 boosting iterations the best generalization performance is achieved. From\nEq. (6) we see that ADABOOST will asymptotically achieve a positive margin (for \u03c6 <1\nand all training patterns are classified according to their possibly wrong labels (cf. figure 8\n(right)). However, this is at the expense that the complexity of the combined hypotheses\nincreases and the decision surface becomes clearly less smooth. The achieved decision line\nis far away from the Bayes optimal line (cf. dashed line in figure 8 (right)).\nTo discuss the generally bad performance of hard margin classifiers in the presence\nof outliers and mislabeled patterns, we analyze the toy example in figure 9. Let us first\nconsider the case without noise (left). Here, we can estimate the optimal separating hyper-\nplanecorrectly.Infigure9(middle)wehaveoneoutlier,whichcorruptstheestimation.The\nADABOOST-type algorithm will certainly concentrate its weights on this outlier and spoil\nthe good estimate that we would get without outlier. Next, let us consider more complex\ndecision lines. Here the overfitting problem gets even more distinct, if we can generate\n2)"},{"page":14,"text":"300\nG. R\u00a8ATSCH, T. ONODA AND K.-R. M\u00a8ULLER\nFigure 8.\niterations (left; log scale) and a typical decision line (right) generated by ADABOOST (104iterations) using RBF\nnetworks (30 centers) in the case of noisy data (300 patterns, \u03c32= 16%). The positive and negative training\npatterns are shown as \u2018+\u2019 and \u2018\u2217\u2019 respectively, the support patterns are marked with \u2018o\u2019. An approximation to the\nBayes decision line is plotted dashed.\nTypical overfitting behavior in the generalization error (smoothed) as a function of the number of\nFigure 9.\n(middle) and with a mislabeled pattern (right). The solid line shows the resulting decision line, whereas the dashed\nline marks the margin area. In the middle and on the left the original decision line is plotted with dots. The hard\nmargin implies noise sensitivity, because only one pattern can spoil the whole estimation of the decision line.\nThe problem of finding a maximum margin \u201chyper-plane\u201d on reliable data (left), data with outlier\nmore and more complexity by combining a lot of hypotheses. Then all training patterns\n(even mislabeled ones or outliers) can be classified correctly. In figure 8 (right) and figure 9\n(right) we see that the decision surface is rather rough and gives bad generalization.\nFrom these cartoons, it becomes apparent that ATA is noise sensitive and maximizing\nthe smallest margin in the case of noisy data can (and will) lead to bad results. Therefore,\nwe need to relax the hard margin and allow for a possibility of mistrusting the data."},{"page":15,"text":"SOFT MARGINS FOR ADABOOST\n301\nFrom the bound (10) it is indeed not obvious that we should maximize the smallest\nmargin: the first term on the right hand side of Eq. (10) takes the whole margin distribution\ninto account. If we would allow a non-zero training error in the settings of figure 9, then\nthe first term of the right hand side of (10) becomes non-zero (\u03b8 > 0). But then \u03b8 can be\nlarger, such that the second term is much smaller. In Mason et al. (2000a) and Mason et al.\n(2000b) similar bounds were used to optimize the margin distribution (a piecewise linear\napproximation) directly. This approach, similar in spirit than ours, is more successful on\nnoisy data than the simple maximization of the smallest margin.\nIn the following we introduce several possibilities to mistrust parts of the data, which\nleads to the soft margin concept.\n4. Improvements using a soft margin\nSincetheoriginalSVMalgorithm(Boseretal.,1992)assumedseparableclassesandpursued\na hard margin strategy, it had similarly poor generalization performance on noisy data as\nthe ATAs. Only the introduction of soft margins for SUPPORT VECTOR MACHINES (Cortes\n& Vapnik, 1995) allowed them to achieve much better generalization results (cf. figure 5).\nWe will now show how to use the soft margin idea for ATAs. In Section 4.1 we modify\nthe error function from Eq. (2) by introducing a new term, which controls the importance\nof a pattern in the reweighting scheme. In Section 4.2 we demonstrate that the soft margin\nidea can be directly build into the LP-ADABOOST algorithm and in Section 4.3 we show\nan extension to quadratic programming\u2014QP-ADABOOST\u2014with its connections to the\nsupport vector approach.\nIn the following subsections and also in the experimental section we will only consider\nthe case \u03c6 =1\n2. Generalizing to other values of \u03c6 is straightforward.\n4.1. Margin vs. influence of a pattern\nFirst, we propose an improvement of the original ADABOOST by using a regularization\nterm in (2) in analogy to weight decay in neural networks and to the soft margin approach\nof SVMs.\nFrom Corollary 4 and Theorem 2 of Breiman (1997b), all training patterns will get a\nmargin\u03c1(zi)largerthanorequalto1\u22122\u03c6 afterasymptoticallymanyiterations(cf.figure3\nand discussion in Section 2). From Eq. (2) we can see that G(b) is minimized as ? is\nmaximized, where\n\u03c1(zi,c) \u2265 ?\nAfter many iterations, these inequalities are satisfied for a ? that is larger or equal than\nthe margin given in Corollary 4. If ? > 0 (cf. Corollary 4), then all patterns are classified\naccording to their possibly wrong labels, which leads to overfitting in the presence of noise.\nTherefore, any modification that improves ADABOOST on noisy data, must not force all\nmargins beyond 0. Especially those patterns that are mislabeled and usually more difficult\nto classify, should be able to attain margins smaller than 0.\nfor all i = 1,...,l.\n(11)"},{"page":16,"text":"302\nG. R\u00a8ATSCH, T. ONODA AND K.-R. M\u00a8ULLER\nIf we knew beforehand which patterns were unreliable we could just remove them from\nthe training set or, alternatively, we could not require that they have a large margin (cf.\nalso the interesting approach for regression in Breiman (1999)). Suppose we have defined\na non-negative quantity \u03b6(zi), which expresses our \u201cmistrust\u201d in a pattern zi. For instance,\nthis could be a probability that the label of a pattern is incorrect. Then we can relax (11)\nand get\n\u03c1(zi,c) \u2265 ? \u2212 C\u03b6(zi),\nwhere C is an a priori chosen constant. Furthermore, we can define the soft margin \u02dc \u03c1(zi)\nof a pattern zias a tradeoff between the margin and \u03b6(zi)\n(12)\n\u02dc \u03c1(zi,c) := \u03c1(zi,c) + C\u03b6(zi)\nand from Eq. (12) we obtain\n(13)\n\u02dc \u03c1(zi,c) \u2265 ?.\nNow we can again simply maximize the smallest soft margin (i.e. maximize ?) and we\nexpect to observe less overfitting. The problem now is, how to define \u03b6(zi). We restrict\nourselves here by presenting only one definition of \u03b6 based on the influence of a pattern on\nthe combined hypotheses hr\n(14)\n\u00b5t(zi) =\nt ?\nr=1\ncrwr(zi),\nwhich is the average weight of a pattern computed during the ATA learning process (cf.\npseudo-code in figure 1). The rationale is: a pattern which is very often misclassified (i.e.\ndifficult to classify) will have a high average weight, i.e. a high influence.5Interestingly, in\nthe noisy case there is (usually) a high overlap between patterns with high influence and\nmislabeled patterns (or other patterns very near to or just beyond the decision line).\nIn the SVM approach it turns out that introducing slack variables to the quadratic opti-\nmization problem (Cortes & Vapnik, 1995) is the same as introducing a upper bound on\nthe Lagrange multipliers of the patterns (Cortes & Vapnik, 1995; Vapnik, 1995; Sch\u00a8 olkopf,\n1997). Empirical evidence shows that the influence of a pattern \u00b5t(zi) is very similar to a\nLagrange multiplier in LP-ADABOOST (Grove & Schuurmans, 1998), since it indicates\nhow much the pattern contributes to the decision. Lagrange multipliers of patterns that are\nnotsupportpatternsinthelinearprogramwillbe0andtheinfluenceofanonsupportpattern\nwill also converge asymptotically to 0 (for t \u2192 \u221e). Furthermore, we found experimentally\nthat both numerical values coincide within a small range (details on this connection can be\nfound in R\u00a8 atsch et al., 2000).\nFrom this discussion it becomes apparent that it makes sense to mistrust patterns with\nhigh influences in the noisy case. From this we define \u03b6 by\n\u03b6(zi) \u2261 \u00b5t(zi)p,\nsuch that the influence of a pattern is penalized, where p is an exponent chosen a priori (for\nexample choose p = 1 or 2).6If a training pattern has high weights \u03b6(zi), then also the soft\n(15)"},{"page":17,"text":"SOFT MARGINS FOR ADABOOST\n303\nmargin is increasing. If we now maximize the smallest soft margin, we do not force outliers\nto be classified according to their possibly wrong labels, but we allow for some errors. Our\nprior for the choice (15) is to weight all patterns equally. This counterbalances the tendency\nof ATAs to overweight certain patterns. So we tradeoff between margin and influence.\nNote 7.\nIfC ischosenhigh,theneachsingledatapointis\u201cnottakenveryseriously\u201dandweobserve\nempirically that the number of support patterns increases. For C \u2192 \u221e we (almost) retrieve\nthe BAGGING algorithm (Breiman, 1996) (in this case, the pattern weighting will be always\nuniform).\nIf we choose C = 0 in Eq. (12), the original ADABOOST algorithm is retrieved.\nOf course, other functional forms of \u03b6 are also possible (see also R\u00a8 atsch et al., 2000),\nfor instance \u03b6t(zi) = P ft(xi), where P is an arbitrary regularization operator. With P it is\npossible to incorporate (other) prior knowledge about the problem into ATAs like smooth-\nness of the decision surface much in the spirit of Tikhonov regularizers (e.g. Tikhonov &\nArsenin, 1977; Smola, Sch\u00a8 olkopf, & M\u00a8 uller, 1998; Rokui & Shimodaira, 1998).\nNow we can reformulate the ATA optimization process in terms of soft margins. From\nEq. (14) and the definition in (15) we can easily derive the new error function (cf. Eq. (2)),\nwhich aims to maximize the soft margin (we assume \u03c6 =1\n?\nl ?\nThe weight wt+1(zi) of a pattern is computed as the derivative of Eq. (16) with respect to\n\u02dc \u03c1(zi,bt) (cf. Lemma 1)\n1\nZt\n2):\nGReg(bt) =\nl ?\ni=1\nexp\n\u22121\n2\u02dc \u03c1(zi,bt)\n?\n=\ni=1\nexp\n?\n\u22121\n2|bt|[\u03c1(zi,ct) + C\u00b5t(zi)p]\n?\n.\n(16)\nwt+1(zi) =\n\u2202GReg(bt)\n\u2202 \u02dc \u03c1(zi,bt)=\nexp{\u2212 \u02dc \u03c1(zi,bt)\/2}\nj=1exp{\u2212 \u02dc \u03c1(zj,bt)\/2},\n?l\n(17)\nwhere Ztis a normalization constant such that?l\n1998) as\ni=1wt+1(zi) = 1. For p = 1 we get the\nupdate rule for the weight of a training pattern in the t-th iteration (for details cf. R\u00a8 atsch,\nwt+1(zi) =wt(zi)\nZt\nexp{btI(yi?= ht(xi)) \u2212 C\u03b6t(zi)|bt|},\n(18)\nand for p = 2 we obtain\nwt+1(zi) =wt(zi)\nZt\nexp{btI(yi?= ht(x)) \u2212 C\u03b6t(zi)|bt| + C\u03b6t\u22121(zi)|bt\u22121|},\n(19)\nwhere Ztis again a normalization constant. It is more difficult to compute the weight bt\nof the t-th hypothesis analytically. However, we can get bt efficiently by a line search"},{"page":18,"text":"304\nG. R\u00a8ATSCH, T. ONODA AND K.-R. M\u00a8ULLER\nFigure 10.\na regularization constant and p is a parameter that changes the regularization characteristics. In all simulations in\nSection5weused p = 2.Animplementationcanbedownloadedfromhttp:\/\/ida.first.gmd.de\/\u02dcraetsch.\nThe ADABOOSTREG(ABR) algorithm (R\u00a8 atsch, 1998; R\u00a8 atsch, Onoda, & M\u00a8 uller, 1999), where C is\nprocedure (e.g. Press et al., 1992) minimizing (16), which has a unique solution because\n\u2202\n\u2202btGReg(bt) > 0 is satisfied for bt > 0. An algorithmic formulation can be found in\nfigure 10.\nWe can interpret this approach as regularization analogous to weight decay. Our prior is\nthat some patterns are likely not to be reliable, so in the noisy case we prefer hypotheses\nwhich do not rely on only a few patterns with high weights.7Instead, we are looking for\nhypotheses with smaller values of \u03b6(zi). So by this regularization, ADABOOST is not\nchanged for easily classifiable patterns, but only for the most difficult ones.\nThe variables \u03b6(zi) in Eq. (12) can also be interpreted as slack-variables (cf. SVM\napproach and next section), which are non-linearly involved in the error function. Large\nvalues of \u03b6(zi) for some patterns allow for a larger (soft-) margin ?. For a comparison of\nthe soft margin distributions of a single RBF classifier and ADABOOSTREGsee figure 11.\nSummarizing, our modification of ADABOOST constructs a soft margin to avoid over-\nfitting."},{"page":19,"text":"SOFT MARGINS FOR ADABOOST\n305\nFigure 11.\nADABOOSTREG(right) with different values of C for the toy data set after 1000 iterations. Note that for some\nvalues for C the graphs of ADABOOSTREGare quite similar to the graphs of the single RBF net.\nMargin distribution graphs of the RBF base hypothesis (scaled) trained with Squared Error (left) and\n4.2.Linear programming with slack variables\nGrove and Schuurmans (1998) showed how to use linear programming to maximize the\nsmallest margin for a given ensemble and proposed LP-ADABOOST (cf. Eq. (21)). In\ntheir approach, they first compute a margin (or gain) matrix M \u2208 {\u00b11}l\u00d7Tfor the given\nhypotheses set, which is defined by\nMi,t= yiht(xi).\nM defines which hypothesis contributes a positive (or negative) part to the margin of a\npatternandisusedtoformulatethefollowingmax-minproblem:findaweightvectorc \u2208 RT\nfor hypotheses {ht}T\nproblem can be solved by linear programming (e.g. Mangasarian, 1965):\n(20)\nt=1, which maximizes the smallest margin ? := mini=1,...,l\u03c1(zi). This\nMaximize ? subject to\nT ?\nt=1\nMi,tct\u2265 ?\nct\u2265 0\nT ?\ni = 1,...,l\nt = 1,...,T\n(21)\nt=1\nct= 1.\nThis LP-ADABOOST algorithm achieves a larger hard margin than the original\nADABOOST algorithm, however in this form it cannot hope to generalize well on noisy\ndata (see our discussion in Section 3). Therefore we also define a soft-margin for a pattern\n\u02dc \u03c1?(zi) = \u03c1(zi) + \u03bei, which is technically equivalent to the introduction of slack variables\n\u03beiand we arrive at the algorithm LPREG-ADABOOST (R\u00a8 atsch, 1998; R\u00a8 atsch et al., 1999;\nR\u00a8 atsch, Onoda, & M\u00a8 uller, 1998). To avoid large values of the slack variables, while solving"},{"page":20,"text":"306\nG. R\u00a8ATSCH, T. ONODA AND K.-R. M\u00a8ULLER\nFigure 12. The LPREG-ADABOOST algorithm, where C is a regularization constant.\nthe linear program with slack variables, the sum of all \u03beiis penalized in the objective func-\ntion(cf.pseudo-codeinfigure12).Thismodificationallowsthatsomepatternshavesmaller\nmarginsthan?.ThereisatradeoffcontrolledbytheconstantC:(a)makeallmarginslarger\nthan ? and (b) maximize ? \u2212C\nOur algorithm is related to the LP-SVM approach (Sch\u00a8 olkopf, Smola, & Williamson,\n2000). As in the original SVM approach, the Lagrange multipliers will be sparse and again\nwegetsupportvector\/patterns.Interestingly,itturnsoutinbothapproaches(asymptotically,\ni.e. with the number of patterns) that \u03bd :=\nof misclassified samples and a lower bound on the fraction of support vectors (R\u00a8 atsch et al.,\n2000).\nl\n?\ni\u03bei.\n1\nC\u2208 [0...1] is an upper bound on the fraction\n4.3. Quadratic programming and the connection to support vector machines\nIn the following section, we extend the LPREG-ADABOOST (LPR) algorithm to quadratic\nprogramming by using similar techniques as in SUPPORT VECTOR MACHINES (Boser et al.,\n1992; Cortes & Vapnik, 1995; Mangasarian, 1965). This gives interesting insights to the\nconnection between SUPPORT VECTOR MACHINES and ADABOOST.\nWe start by transforming the LPREG-ADABOOST algorithm, which maximizes ?, while\n|c| is kept fixed, to a linear program in which ? is fixed (to e.g. 1) and |b| is minimized.\nUnfortunately, there is no equivalent linear program because of the slack variables. But\nwe can use a Taylor expansion8to get the following linear program (compare with linear\nprogrammingapproachesrelatedtoSVlearninge.g.Bennett&Mangasarian,1992;Weston"},{"page":21,"text":"SOFT MARGINS FOR ADABOOST\n307\net al., 1997; Frie\u00df & Harrison, 1998; Bennett, 1998):\n?\nT ?\nMinimize ?b?1+ C\ni\n\u03beisubject to\nt=1\nbtMi,t\u2265 1 \u2212 \u03bei,\nt = 1,...,T,\nt = 1,...,T,\ni = 1,...,l.\n(22)\nbt\u2265 0,\n\u03bei\u2265 0,\nEssentially, this is the same algorithm as in figure 12: for a different value of C problem,\n(22) is equivalent to the one in figure 12 (cf. Smola, 1998 and Lemma 3 in R\u00a8 atsch et al.,\n2000). Instead of using the ?1-norm in the optimization objective of (22), we can also use\nthe ?p-norm. Clearly, each p will imply its own soft margin characteristics. Using p = 2\nleads to an algorithm similar to the SVM (cf. figure 14).\nTheoptimizationobjectiveofaSVMistofindafunctionhwwhichminimizesafunctional\nof the form (Vapnik, 1995)\nE = ?w?2+ C\nl ?\ni=1\n\u03bei,\n(23)\nsubject to the constraints\nyih(xi) \u2265 1 \u2212 \u03bei\nand\n\u03bei\u2265 0,for i = 1,...,l.\nHere, the variables \u03beiare the slack-variables responsible for obtaining a soft margin. The\nnorm of the parameter vector w defines a system of nested subsets in the combined hypoth-\nesisspaceandcanberegardedasameasureofthecomplexity(asalsothesizeofthemargin\nFigure13.\nvalues of C for the toy data set after 1000 iterations. LPREG-ADABOOST sometimes generates margins on the\ntraining set, which are either 1 or \u22121 (step in the distribution).\nMargindistributiongraphsofLPREG-ADABOOST(left)andQPREG-ADABOOST(right)fordifferent"},{"page":22,"text":"308\nG. R\u00a8ATSCH, T. ONODA AND K.-R. M\u00a8ULLER\nof hypothesis hw) (Vapnik, 1995). With functional (23), we get a tradeoff (controlled by C)\nbetweenthecomplexityofthehypothesis(?w?2)andthedegreehowmuchtheclassification\nmay differ from the labels of the training patterns (?\nwe observed that the more different the weights are for the hypotheses, the higher the\ncomplexity of the ensemble. With this in mind, we can use the ?pnorm (p > 1) of the\nhypothesesweightvector?b?pasacomplexitymeasure.Letusassume,forexamplethatwe\nhave |b| = 1, then ?b?pthis is a small value, as the elements of b are approximately equal\n(analogous to BAGGING). If ?b?phas high values, then there are some strongly emphasized\nhypotheses (far away from BAGGING).9Hence, we can apply the optimization principles of\nSVMs to ADABOOST and get a quadratic optimization problem in b:\ni\u03bei).\nFor ensemble learning, so far we do not have such a measure of complexity. Empirically,\nMinimize ?b?2+ C\n?\ni\n\u03bei,\nwith the constraints given in Eq. (22). We call this algorithm QPREG-ADABOOST (QPR)\nsince it was motivated by the connection to LPREG-ADABOOST (cf. algorithm (22)) and by\ntheanalogytothesupportvectoralgorithm(forpseudocodeseefigure14).Weexpectasim-\nilar performance of QPREGand LPREG-ADABOOST with subtle differences on specific data\nsets due to the different \u201ctypes\u201d of soft margins. Furthermore, they should exhibit superior\nperformance compared to the original ADABOOST algorithm on noisy data. For an overall\ncomparison of the margin distributions of original ADABOOST, SVM, ADABOOSTREG\nand LP\/QP-ADABOOST see figures 5, 7, 11 and 13.\nSummarizing, we introduced in this section the soft margin concept to ADABOOST\nby (a) regularizing the objective function (2), (b) LPREG-ADABOOST, which uses slack\nvariables and (c) QPREG-ADABOOST, which exhibits an interesting connection to SVMs.\nFigure 14. The QPREG-ADABOOST algorithm, where C is a regularization constant."},{"page":23,"text":"SOFT MARGINS FOR ADABOOST\n309\n5. Experiments\nIn order to evaluate the performance of our new algorithms, we perform large scale simula-\ntions and compare the single RBF classifier, the original ADABOOST algorithm,\nADABOOSTREG, L\/QPREG-ADABOOST and a SUPPORT VECTOR MACHINE (with RBF\nkernel).\n5.1. Experimental setup\nFor this, we use 13 artificial and real world data sets from the UCI, DELVE and STAT-\nLOG benchmark repositories10: banana (toy data set used in the previous sections), breast\ncancer,11diabetes, german, heart, image segment, ringnorm, flare solar, splice, new-\nthyroid, titanic, twonorm, waveform. Some of the problems are originally not binary classi-\nfication problems, hence a random partition into two classes is used.12At first we generate\n100 partitions into training and test set (mostly \u2248 60% : 40%). On each partition we train\na classifier and then compute its test set error.\nIn all experiments, we combine 200 hypotheses. Clearly, this number of hypotheses is\nsomewhatarbitraryandmaynotbeoptimal.HoweverwecheckedthatoriginalADABOOST\nwithearlystoppingismostofthetimeworsethananyoftheproposedsoftmarginalgorithms\n(cf. an earlier study R\u00a8 atsch, 1998). However, we use a fixed number of iterations for all\nalgorithms, therefore this comparison should be fair.\nAs base hypotheses we use RBF nets with adaptive centers as described in Appendix D.\nOn each of the 13 data sets we employ cross validation to find the best base hypothesis\nmodel, which is then used in the ensemble learning algorithms. For selecting the best RBF\nmodel we optimize the number of centers (parameter K, cf. figure 15) and the number of\niteration steps for adapting the RBF centers and widths (parameter O). The parameter \u03bb\nwas fixed to 10\u22126.\nThe parameter C of the regularized versions of ADABOOST and the parameters (C,\u03c3)\nof the SVM (C is the regularization constant and \u03c3 is the width of the RBF-kernel be-\ning used) are optimized on the first five realizations of each data set. On each of these\nrealizations, a 5-fold-cross validation procedure gives a good model.13Finally, the model\nparameters are computed as the median of the five estimations and used throughout the\ntraining on all 100 realization of that data set. This way of estimating the parameters is\ncomputationally highly expensive, but it will make our comparison more robust and the\nresults more reliable.\nNote, to perform the simulations in this setup we had to train more than 3\u00d7106adaptive\nRBFnetsandtosolvemorethan105linearorquadraticprogrammingproblems\u2014ataskthat\nwould have taken altogether 2 years of computing time on a single Ultra-SPARC machine,\nif we had not distributed it over 30 computers.\n5.2.Experimental results\nIn Table 1 the average generalization performance (with standard deviation) over the 100\npartitions of the data sets is given. The second last line in Table 1 showing \u2018Mean%\u2019, is"},{"page":24,"text":"310\nG. R\u00a8ATSCH, T. ONODA AND K.-R. M\u00a8ULLER\nFigure 15.\nsimulations with ADABOOST.\nPseudo-code description of the RBF net algorithm, which is used as base learning algorithm in the\ncomputed as follows: For each data set the average error rates of all classifier types are\ndivided by the minimum error rate for this data set and 1 is subtracted. These resulting\nnumbers are averaged over the 13 data sets and the variance is computed. The last line gives\nthe Laplacian probability (and variance) over 13 data sets whether a particular method\nwins on a particular realization of a data set, i.e. has the lowest generalization error. Our\nexperiments on noisy data (cf. Table 1) show that:\n\u2013 The results of ADABOOST are in almost all cases worse than the single classifier. This\nis clearly due to the overfitting of ADABOOST. If early stopping is used then the effect\nis less drastic but still clearly observable (R\u00a8 atsch, 1998).\n\u2013 The averaged results for ADABOOSTREGare a bit better (Mean% and Winner%) than\nthe results of the SVM, which is known to be an excellent classifier. In five (out of\nseven) cases ADABOOSTREGis significant better than the SVM. Moreover, the single"},{"page":25,"text":"SOFT MARGINS FOR ADABOOST\n311\nTable1.\np = 2), L\/QPREG-ADABOOST (L\/QPR-AB) and a SUPPORT VECTOR MACHINE: Estimation of generalization\nerror in % on 13 data sets (best method in bold face, second emphasized). The columns S1and S2show the results\nofasignificancetest(95%-t-test)betweenAB\/ABRandABR\/SVM,respectively.ADABOOSTREGgivesthebest\noverall performance.\nComparisonamongthesixmethods:SingleRBFclassifier,ADABOOST(AB),ADABOOSTREG(ABR;\nRBF ABS1\nABR\nLPR-AB QPR-ABS2\nSVM\nBanana\nB. Cancer\nDiabetes\nGerman\nHeart\nImage\nRingnorm\nF. Solar\nSplice\nThyroid\nTitanic\nTwonorm\nWaveform\n10.8\u00b10.6\n27.6\u00b14.7\n24.3\u00b11.9\n24.7\u00b12.4\n17.6\u00b13.3\n3.3\u00b10.6\n1.7\u00b10.2\n34.4\u00b12.0\n10.0\u00b11.0\n4.5\u00b12.1\n23.3\u00b11.3\n2.9\u00b10.3\n10.7\u00b11.1\n6.6\u00b15.8\n14.8\u00b18.5\n12.3\u00b10.7\n30.4\u00b14.7\n26.5\u00b12.3\n27.5\u00b12.5\n20.3\u00b13.4\n2.7\u00b10.7\n1.9\u00b10.3\n35.7\u00b11.8\n10.1\u00b10.5\n4.4\u00b12.2\n22.6\u00b11.2\n3.0\u00b10.3\n10.8\u00b10.6\n11.9\u00b17.9\n7.2\u00b17.8\n+\n+\n+\n+\n+\n10.9\u00b10.4\n26.5\u00b14.5\n23.8\u00b11.8\n24.3\u00b12.1\n16.5\u00b13.5\n2.7\u00b10.6\n1.6\u00b10.1\n34.2\u00b12.2\n9.5\u00b10.7\n4.6\u00b12.2\n22.6\u00b11.2\n2.7\u00b10.2\n9.8\u00b10.8\n1.7\u00b11.9\n26.0\u00b112.4\n10.7\u00b10.4\n26.8\u00b16.1\n24.1\u00b11.9\n24.8\u00b12.2\n17.5\u00b13.5\n2.8\u00b10.6\n2.2\u00b10.5\n34.7\u00b12.0\n10.2\u00b11.6\n4.6\u00b12.2\n24.0\u00b14.4\n3.2\u00b10.4\n10.5\u00b11.0\n8.9\u00b110.8\n14.4\u00b18.6\n10.9\u00b10.5\n25.9\u00b14.6\n25.4\u00b12.2\n25.3\u00b12.1\n17.2\u00b13.4\n2.7\u00b10.6\n1.9 \u00b10.2\n36.2\u00b11.8\n10.1\u00b10.5\n4.4\u00b12.2\n22.7\u00b11.1\n3.0\u00b10.3\n10.1\u00b10.5\n5.8\u00b15.5\n13.2\u00b17.6\n+\n11.5\u00b10.7\n26.0\u00b14.7\n23.5\u00b11.7\n23.6\u00b12.1\n16.0\u00b13.3\n3.0\u00b10.6\n1.7\u00b10.1\n32.4\u00b11.8\n10.9\u00b10.7\n4.8\u00b12.2\n22.4\u00b11.0\n3.0\u00b10.2\n9.9\u00b10.4\n4.6\u00b15.4\n23.5\u00b118.0\n+\n\u2212\n+\n+\n+\n\u2212\n+\n\u2212\n+\n+\n+\n+\nMean%\nWinner%\nRBF classifier wins less often than the SVM (for a comparison in the regression case cf.\nM\u00a8 uller et al., 1998).\n\u2013 L\/QPREG-ADABOOST improves the results of ADABOOST. This is due to the use of\na soft margin. But the results are not as good as the results of ADABOOSTREGand the\nSVM. One reason is that the hypotheses generated by ADABOOST (aimed to construct\na hard margin) may not provide the appropriate basis to subsequently generate a good\nsoft margin with linear and quadratic programming approaches.\n\u2013 We can observe that quadratic programming gives slightly better results than linear\nprogramming. This may be due to the fact that the hypotheses coefficients generated by\nLPREG-ADABOOST are more sparse (smaller ensemble) and larger ensembles may have\na better generalization ability (Breiman, 1998). Furthermore, with QP-ADABOOST we\nprefer ensembles which have approximately equally weighted hypotheses. As stated in\nSection 4.3, this implies a lower complexity of the combined hypothesis, which can lead\nto a better generalization performance.\n\u2013 The results of ADABOOSTREGare in ten (out of 13) cases significantly better than the\nresults of ADABOOST. Also, in ten cases ADABOOSTREGperforms better than the\nsingle RBF classifier.\nSummarizing, ADABOOSTREGwins most often and shows the best average performance.\nIn most of the cases it performs significantly better than ADABOOST and it performs"},{"page":26,"text":"312\nG. R\u00a8ATSCH, T. ONODA AND K.-R. M\u00a8ULLER\nslightly better than SUPPORT VECTOR MACHINES. This demonstrates the noise robustness\nof the proposed algorithm.\nThe slightly inferior performance of SVM compared to ADABOOSTREGmay be ex-\nplained with the fixed \u03c3 of the RBF-kernel for SVM. By fixing \u03c3 we look at the data only\nat one scale, i.e. we are losing possible multiscale information that could be inherent of the\ndata. Further causes could be the coarse model selection, and the error function of the SV\nalgorithm, which is not adapted to the noise model in the data (see Smola et al., 1998).\nSo, the original ADABOOST algorithm is useful for low noise cases, where the classes\nare easily separable (as shown for OCR cf. Schwenk & Bengio, 1997; LeCun et al., 1995).\nL\/QPREG-ADABOOSTcanimprovetheensemblestructurethroughintroducingasoftmar-\ngin and the same hypotheses (just with another weighting) can result in a much better gen-\neralization performance. The hypotheses, which are used by L\/QPREG-ADABOOST may\nbe sub-optimal, because they are not part of the L\/QP optimization process that aims for\na soft margin. ADABOOSTREGdoes not have this problem: the hypotheses are generated\nsuch that they are appropriate to form the desired soft-margin. ADABOOSTREGextends the\napplicability of Boosting\/Arcing methods to non-separable cases and should be preferably\napplied if the data is noisy.\n6. Conclusion\nWe have shown that ADABOOST performs a constrained gradient descent in an error func-\ntion that optimizes the margin (cf. Eq. (2)). Asymptotically, all emphasis is concentrated on\nthe difficult patterns with small margins, easy patterns effectively do not contribute to the\nerror measure and are neglected in the training process (very much similar to support vec-\ntors). It was shown theoretically and experimentally that the cumulative margin distribution\nofthetrainingpatternsconvergesasymptoticallytoastep.Therefore,originalADABOOST\nachieves a hard margin classification asymptotically. The asymptotic margin distribution\nof ADABOOST and SVM (for the separable case) are very similar. Hence, the patterns\nlying in the step part (support patterns) of the margin distribution show a large overlap to\nthe support vectors found by a SVM.\nWediscussedindetailthatATAsandhardmarginclassifiersareingeneralnoisesensitive\nand prone to overfit. We introduced three regularization-based ADABOOST algorithms to\nalleviate this overfitting problem: (1) direct incorporation of the regularization term into\nthe error function (ADABOOSTREG), use of (2) linear and (3) quadratic programming with\nslack variables to improve existing ensembles. The essence of our proposed algorithms is\nto achieve a soft margin (through regularization term and slack variables) in contrast to the\nhard margin classification used before. The soft-margin approach allows to control how\nmuch we trust the data, so we are permitted to ignore noisy patterns (e.g. outliers) which\nwould otherwise spoile our classification. This generalization is very much in the spirit of\nSUPPORT VECTOR MACHINES that also tradeoff the maximization of the margin and the\nminimization of the classification errors by introducing slack variables. Note that we just\ngave one definition for the soft margin in ADABOOSTREGother extensions that e.g. use\nregularization operators (e.g. Smola et al., 1998; Rokui & Shimodaira, 1998; Bishop, 1995)\nor that have other functional forms (cf. R\u00a8 atsch et al., 2000) are also possible."},{"page":27,"text":"SOFT MARGINS FOR ADABOOST\n313\nIn our experiments on noisy data the proposed regularized versions of ADABOOST:\nADABOOSTREGandL\/QPREG-ADABOOSTshowamorerobustbehaviorthantheoriginal\nADABOOST algorithm. Furthermore, ADABOOSTREGexhibits a better overall general-\nization performance than all other analyzed algorithms including the SUPPORT VECTOR\nMACHINES. We conjecture that this result is mostly due to the fact that SUPPORT VECTOR\nMACHINEScanonlyuseone\u03c3,i.e.onlyone\u2013fixed\u2013kernel,andthereforelosesmulti-scaling\ninformation. ADABOOST does not have this limitation, since we use RBF nets with adap-\ntive kernel widths as base hypotheses.\nOur future work will concentrate on a continuing improvement of ADABOOST-type\nalgorithmsfornoisyrealworldapplications.Also,afurtheranalysisoftherelationbetween\nADABOOST (in particular QPREG-ADABOOST) and SUPPORT VECTOR MACHINES from\nthemarginpointofviewseemspromising,withparticularfocusonthequestionofwhatgood\nmargin distributions should look like. Moreover, it is interesting to see how the techniques\nestablishedinthisworkcanbeappliedtoADABOOSTinaregressionscenario(cf.Bertoni,\nCampadelli, & Parodi, 1997; Friedman, 1999; R\u00a8 atsch et al., 2000).\nAppendix\nA. Proof of Lemma 1\nProof:\nd we get\nWe define \u03c0t(zi) :=?t\nr=1exp(\u2212brI(hr(zi) = yi)) and from definition of G and\n\u2202G\n\u2202\u03c1(zi,bt)\n?l\nj=1\n\u2202G\n\u2202\u03c1(zj,bt)\n=\nexp?\u22121\n\u03c0t(zi)\nj=1\u03c0t(xj)\n2\u03c1(zi,bt)?\n?l\n?l\nj=1exp?\u22121\n2\u03c1(zi,bt)?\n=\n=\u03c0t(zi)\n\u02dcZt\n,\nwhere\u02dcZt :=?l\nwt+1(zi) =\u03c0t(zi)\ni=1\u03c0t(zi). By definition, \u03c0t(zi) = \u03c0t\u22121(zi)exp(\u2212btI(ht(zi) = yi)) and\n\u03c01(zi) = 1\/l. Thus, we get\n=\u03c0t\u22121(zi)exp(\u2212btI(ht(zi) = yi))\n\u02dcZt\n=wt\u22121(zi)\u02dcZt\u22121exp(\u2212btI(ht(zi) = yi))\n=wt\u22121(zi)exp(\u2212btI(ht(zi) = yi))\nZt\n\u02dcZt\n\u02dcZt\n,\nwhere Zt=\u02dcZt\u02dcZt\u22121(cf. step 4 in figure 1).\n2"},{"page":28,"text":"314\nG. R\u00a8ATSCH, T. ONODA AND K.-R. M\u00a8ULLER\nB. Proof of Theorem 3\nThe proof follows the one of Theorem 5 in (Schapire et al., 1997). Theorem 3 is a general-\nization for \u03c6 ?=1\n2.\nProof:\nIf yf (x) \u2264 \u03b8, then we have\ny\nT ?\nt=1\nbtht(x) \u2264 \u03b8\nT ?\nt=1\nbt,\nand also\nexp\n?\n\u2212y\n2\nT ?\nt=1\nbtht(x) +\u03b8\n2\nT ?\nt=1\nbt\n?\n\u2265 1.\nThus,\nP(x,y)\u223cZ[yf (x) \u2264 \u03b8] \u22641\nl\nl ?\ni=1\nexp\n?\n\u2212yi\n2\nT ?\nl ?\nt=1\nbtht(xi) +\u03b8\n?\n2\nT ?\nbtht(xi)\nt=1\nbt\n?\n=exp?\u03b8\n2\n?T\nt=1bt\n?\nl\ni=1\nexp\n\u2212yi\n2\nT ?\nt=1\n?\n,\nwhere\nl ?\ni=1\nexp\n?\n\u2212yi\n2\nT ?\n?\nt=1\n\u2212yi\nbtht(xi)\n?\n=\nl ?\ni=1\nexp\n2\n?\nT\u22121\n?\n\u2212yi\n?\nt=1\nbtht(xi)\n?\nexp\n?\n?\n\u2212yi\n2bThT(xi)\n?\n=\n?\ni:hT(xi)=yi\nexp\n2\nT\u22121\n?\nt=1\nbtht(xi)\ne\u2212bT\/2\n+\n?\n?\ni:hT(xi)?=yi\nl ?\nexp\n\u2212yi\n2\nT\u22121\n?\nt=1\nbtht(xi)\n?\nebT\/2\n=\ni=1\nexp\n?\n\u2212yi\n2\nT\u22121\n?\nt=1\nbtht(xi)\n??\n((1 \u2212 ?T)e\u2212bT\/2+ ?TebT\/2),\nbecause\n?T=\n1\n?l\nj=1wT\nj\n?\ni:hT(xi)?=yi\nwT\ni."},{"page":29,"text":"SOFT MARGINS FOR ADABOOST\n315\nWith?l\ni=1exp(0) = l (for t = 1), we get recursively\nP(x,y)\u223cZ[yf (x) \u2264 \u03b8] \u2264 exp\n?\n\u03b8\n2\nT ?\nt=1\nbt\n?\nT ?\nt=1\n((1 \u2212 ?t)e\u2212bt\/2+ ?tebt\/2).\nPlugging in the definition for btwe get\nP(x,y)\u223cZ[yf (x) \u2264 \u03b8] \u2264\n?\nT ?\nt=1\n1 \u2212 ?t\n?t\n??\nT ?\n\u03c6\n1 \u2212 \u03c6+\n?1+\u03c6\nt=1\n\u03c6\n1 \u2212 \u03c6\n?\n?1 \u2212 \u03c6\nT ?\n?\u03b8\/2\n\u00d7\n1 \u2212 \u03c6\n\u03c6\n?T\n?1\u2212\u03c6\nT ?\n?T\nt=1\n?\nT ?\n(1 \u2212 ?t)?t\n=\n??\n\u03c6\n1 \u2212 \u03c6\n2\n+\n\u03c6\n2\nt=1\n?\n(1 \u2212 ?t)1+\u03b8?1\u2212\u03b8\nt\n=?\u03d5\n1+\u03b8\n2 + \u03d5\u22121\u2212\u03b8\n2?T\nt=1\n?\n?1\u2212\u03b8\nt\n(1 \u2212 ?t)1+\u03b8.\n2\nC. Proof of Lemma 5\nProof:\nWe have to show that limt\u2192\u221e\u03b5t= 0, where\n???min\nThe set St\n\u03b5t:=\ni:yi=1\u03c1(zi,ct) \u2212 min\nj:yj=\u22121\u03c1(zj,ct)\n???.\ncis the set of support patterns at iteration t:\nSt\nc=\n?\nj \u2208 {1,...,l} : \u03c1(zj,ct) = min\ni:yi=c\u03c1(zi,ct)\n?\n,\nwhichclearlycontainsatleastoneelement.Notethat S\u221e\nwhich we will get asymptotically.\nSuppose we have \u03b5t > ?\/2 > 0 and s \u2208 {\u00b11} is the class with the smallest margin.\nWith (4), q \u2208 St\n1\u222aS\u221e\n\u22121isthesetofsupportpatterns,\nsand Q := exp(\u22121\n2\u03c1(zq,ct)) we get\n?\ni:yi=s\nwt(zi) \u2265\nQ|bt|\nQ|bt|+?\nQ|bt|+ (l \u2212 1)Q|bt|e\u2212|bt|?\/4\ni:yi?=sexp?\u22121\nQ|bt|\n2\u03c1(zi,ct)?|bt|\n>"},{"page":30,"text":"316\nG. R\u00a8ATSCH, T. ONODA AND K.-R. M\u00a8ULLER\nWith|bt|>t\u03b3 fromassumption(8)weget?\nsified correctly and patterns of class r = \u2212s will be misclassified. Therefore, we obtain\n\u03c1(zi,ct+1) =\nclassr will be misclassified and we have \u03c1(zj,ct+1) =\nIt follows\n????min\n=\n|bt+1|\nAs long as \u03b5t > ?\/2, all patterns of class r will be misclassified and patterns of class s\nwill be classified correctly. If it becomes (\u03b5t|bt| \u2212 2bt+1)\/|bt+1| < \u2212?\/2, then the same\nreasoning can be done interchanging the role of s and r. If furthermore t > t2:=\n\u03b5t|bt| \u2212 2bt+1> 0 and we have \u03b5t+1< \u03b5t\u2212 \u03c9t, where \u03c9tis decreasing not too fast (i.e. it\ncan be bounded by O(1\/t)). Hence, we will reach the case \u03b5t< ?\/2 and get: If t is large\nenough (t > t3:=2?(2\u2212?)\ni:yi=swt(zi)\u2265\u03b4,fort >t1:=log(l\u22121)\u2212log(1\/\u03b4\u22121)\n\u03b3?\/4\n.\nHence, with assumption (9) we get: If t \u2265 t1, then all patterns ziof class s will be clas-\n|bt|\u03c1(zi,ct)+bt+1\n|bt+1|\n, for i \u2208 {k : yk= s} (especially for i \u2208 St\ns). The patterns of\n, for j \u2208 {k : yk= r}.\n|bt|\u03c1(zj,ct)\u2212bt+1\n|bt+1|\n\u03b5t+1=\ni:yi=s\n????\n|bt|\u03c1(zi,ct) \u2212 bt+1\n|bt+1|\n\u03b5t|bt| \u2212 2bt+1\n\u2212 min\nj:yj=r\n|bt|\u03c1(zj,ct) + bt+1\n|bt+1|\n????\n????\n4?\n\u03b3?, then\n?\u03b3\n), then\n\u2212? <\u03b5t|bt| \u2212 2bt+1\n|bt+1|\n< ?,\ni.e. adding a new hypothesis will not lead to \u03b5t+1\u2265 ?.\nTherefore, after a finite number \u02dc t of subsequent steps, we have we will reach \u03b5t < ?.\nFurthermore, the discussion above shows that, if t is large enough, it is not possible to leave\nthe?areaaround0.Hence,foreach? > 0,wecanprovideanindexT = max(t1,t2,t3) + \u02dc t\n(where t1,...,t3are the lower bounds on t used above), such that \u03b5t < ? for all t > T.\nThis implies the desired result.\n2\nD. RBF nets with adaptive centers\nTheRBFnetsusedintheexperimentsareanextensionofthemethodofMoodyandDarken\n(1989), since centers and variances are also adapted (see also Bishop, 1995; M\u00a8 uller et al.,\n1998). The output of the network is computed as a linear superposition of K basis functions\nf (x) =\nK\n?\nk=1\nwkgk(x),\n(24)\nwhere wk,k = 1,..., K, denotes the weights of the output layer. The Gaussian basis\nfunctions gkare defined as\ngk(x) = exp\n?\n\u2212?x \u2212 \u00b5k?2\n2\u03c32\nk\n?\n,\n(25)"},{"page":31,"text":"SOFT MARGINS FOR ADABOOST\n317\nwhere \u00b5kand \u03c32\nare initialized with K-means clustering and the variances \u03c3kare determined as the distance\nbetween \u00b5kand the closest \u00b5i(i ?= k,i \u2208 {1,..., K}). Then in the following steps we\nperform a gradient descent in the regularized error function (weight decay)\nkdenote means and variances, respectively. In a first step, the means \u00b5k\nE =1\n2\nl ?\ni=1\n(yi\u2212 f (xi))2+\u03bb\n2l\nK\n?\nk=1\nw2\nk.\n(26)\nTaking the derivative of Eq. (26) with respect to RBF means \u00b5kand variances \u03c3kwe obtain\n\u2202E\n\u2202\u00b5k\n=\nl ?\ni=1\n( f (xi) \u2212 yi)\n\u2202\n\u2202\u00b5k\nf (xi),\n(27)\nwith\n\u2202\n\u2202\u00b5kf (xi) = wk\nxi\u2212\u00b5k\n\u03c32\nk\ngk(xi) and\n\u2202E\n\u2202\u03c3k\n=\nl ?\ni=1\n( f (xi) \u2212 yi)\n\u2202\n\u2202\u03c3k\nf (xi),\n(28)\nwith\nof Eq. (26) by a conjugate gradient descent with line search, where we always compute the\noptimal output weights in every evaluation of the error function during the line search. The\noptimal output weights w = [w1,...,wK]?in matrix notation can be computed in closed\nform by\n\u2202\n\u2202\u03c3kf (xi) = wk\n?\u00b5k\u2212xi?2\n\u03c33\nk\ngk(xi).Thesetwoderivativesareemployedintheminimization\nw =\n?\nGTG + 2\u03bb\nlI\n?\u22121\nGTy, where Gik= gk(xi)\n(29)\nand y = [y1,..., yl]?denotes the output vector, and I an identity matrix. For \u03bb = 0, this\ncorresponds to the calculation of a pseudo-inverse of G.\nSo, we simultaneously adjust the output weights and the RBF centers and variances (see\nfigure 15 for pseudo-code of this algorithm). In this way, the network fine-tunes itself to\nthe data after the initial clustering step, yet, of course, overfitting has to be avoided by\ncareful tuning of the regularization parameter, the number of centers K and the number of\niterations (cf. Bishop, 1995). In our experiments we always used \u03bb = 10\u22126and up to ten\nCG iterations.\nAcknowledgments\nWe thank for valuable discussions with B. Sch\u00a8 olkopf, A. Smola, T. Frie\u00df, D. Schuurmans\nand B. Williamson. Partial funding from EC STORM project number 25387 is gratefully\nacknowledged. Furthermore, we acknowledge the referees for valuable comments."},{"page":32,"text":"318\nG. R\u00a8ATSCH, T. ONODA AND K.-R. M\u00a8ULLER\nNotes\n1. An ensemble is a collection of neural networks or other types of classifiers (hypotheses) that are trained for\nthe same task.\n2. InFriedmanetal.(1998)itwasmentionedthatsometimestherandomizedversionshowsabetterperformance,\nthan the version with weighted minimization. In connection with the discussion in Section 3 this becomes\nclearer, because the randomized version will show an overfitting effect (possibly much) later and overfitting\nmay be not observed, whereas it was observed using the more efficient weighted minimization.\n3. In our experiments we have often observed values for |b| that are larger than 100 after only 200 iterations.\n4. A detailed description of the generation of the toy data used in the asymptotical simulations can be found in\nthe Internet http:\/\/ida.first.gmd.de\/\u02dcraetsch\/data\/banana.html.\n5. The definition of the influence clearly depends on the base hypothesis space H. If the hypothesis space\nchanges, other patterns may be more difficult to classify.\n6. Note that for p = 1 there is a connection to the leave-one-out-SVM approach of Weston (1999).\n7. Interestingly,the(soft)SVMgeneratesmanymoresupportvectorsinthehighnoisecasethaninthelownoise\ncase (Vapnik, 1995).\n8. From the algorithm in figure 12, it is straight forward to get the following linear program, which is equivalent\nfor any fixed S > 0:\nMinimize?+\nS\n+C\nS\n?\ni\n\u03be+\nisubject to S\nT ?\nt=1\nctMi,t\u2265 ?+\u2212 \u03be+\ni,\nwhere ?+:= S?,\u03be+\ni:= S\u03bei,bt\u2265 0,\u03be+\ni\u2265 0,\n?\nt\nbt= S.\nIn this problem we can set ?+to 1 and try to optimize S. To retrieve a linear program, we use the Taylor\nexpansion around 1:1\n(22).\n9. For p = 2,?b?2corresponds to the Renyi entropy of the hypothesis vector and we are effectively trying to\nminimize this entropy while separating the data.\n10. These data sets including a short description, the splits into the 100 realizations and the simulation results are\navailable at http:\/\/ida.first.gmd.de\/\u02dcraetsch\/data\/benchmarks.htm.\n11. The breast cancer domain was obtained from the University Medical Center, Inst. of Oncology, Ljubljana,\nYugoslavia. Thanks go to M. Zwitter and M. Soklic for providing the data.\n12. A random partition generates a mapping m of n to two classes. For this a random \u00b11 vector m of length n is\ngenerated. The positive classes (and the negative respectively) are then concatenated.\n13. The parameters selected by the cross validation are only near-optimal. Only 15\u201325 values for each parameter\nare tested in two stages: first a global search (i.e. over a wide range of the parameter space) was done to find\na good guess of the parameter, which becomes more precise in the second stage.\nS= S+O((S\u22121)2) and?\ni\u03be+\ni\/S =?\ni\u03be+\ni+O(S\u22121). For S = |b| we get algorithm\nReferences\nBennett, K. (1998). Combining support vector and mathematical programming methods for induction. In B.\nSch\u00a8 olkopf, C. Burges, & A. Smola (Eds.), Advances in kernel methods\u2014SV learning. Cambridge, MA: MIT\nPress.\nBennett, K. & Mangasarian, O. (1992). Robust linear programming discrimination of two linearly inseparable\nsets. Optimization Methods and Software, 1, 23\u201334.\nBertoni,A.,Campadelli,P.,&Parodi,M.(1997).Aboostingalgorithmforregression.InW.Gerstner,A.Germond,\nM.Hasler,&J.-D.Nicoud(Eds.),LNCS,Vol.V:ProceedingsICANN\u201997:Int.Conf.onArtificialNeuralNetworks\n(pp. 343\u2013348). Berlin: Springer.\nBishop, C. (1995). Neural Networks for Pattern Recognition. Oxford: Clarendon Press."},{"page":33,"text":"SOFT MARGINS FOR ADABOOST\n319\nBoser, B., Guyon, I., & Vapnik, V. (1992). A training algorithm for optimal margin classifiers. In D. Haussler\n(Ed.), Proceedings COLT\u201992: Conference on Computational Learning Theory (pp. 144\u2013152). New York, NY:\nACM Press.\nBreiman, L. (1996). Bagging predictors. Mechine Learning, 26(2), 123\u2013140.\nBreiman, L. (1997a). Arcing the edge. Technical Report 486, Statistics Department, University of California.\nBreiman, L. (1997b). Prediction games and arcing algorithms. Technical Report 504, Statistics Department,\nUniversity of California.\nBreiman, L. (1998). Arcing classifiers. The Annals of Statistics, 26(3), 801\u2013849.\nBreiman, L. (1999). Using adaptive bagging to debias regressions. Technical Report 547, Statistics Department,\nUniversity of California.\nCortes, C. & Vapnik, V. (1995). Support vector networks. Machine Learning, 20, 273\u2013297.\nFrean, M. & Downs, T. (1998). A simple cost function for boosting. Technical Report, Department of Computer\nScience and Electrical Engineering, University of Queensland.\nFreund, Y. & Schapire, R. (1994). A decision-theoretic generalization of on-line learning and an applica-\ntion to boosting. In Proceedings EuroCOLT\u201994: European Conference on Computational Learning Theory.\nLNCS.\nFreund, Y. & Schapire, R. (1996). Game theory, on-line prediction and boosting. In Proceedings COLT\u201986: Conf.\non Comput. Learning Theory (pp. 325\u2013332). New York, NY: ACM Press.\nFriedman, J. (1999). Greedy function approximation. Technical Report, Department of Statistics, Stanford\nUniversity.\nFriedman, J., Hastie, T., & Tibshirani, R. (1998). Additive logistic regression: A statistical view of boosting.\nTechnical Report, Department of Statistics, Sequoia Hall, Stanford University.\nFrie\u00df, T. & Harrison, R. (1998). Perceptrons in kernel feature space. Research Report RR-720, Department of\nAutomatic Control and Systems Engineering, University of Sheffield, Sheffield, UK.\nGrove, A. & Schuurmans, D. (1998). Boosting in the limit: Maximizing the margin of learned ensembles. In\nProceedings of the Fifteenth National Conference on Artifical Intelligence.\nKirkpatrick, S. (1984). Optimization by simulated annealing: Quantitative studies. J. Statistical Physics, 34, 975\u2013\n986.\nLeCun, Y., Jackel, L., Bottou, L., Cortes, C., Denker, J., Drucker, H., Guyon, I., M\u00a8 uller, U., S\u00a8 ackinger, E., Simard,\nP., & Vapnik, V. (1995). Learning algorithms for classification: A comparism on handwritten digit recognition.\nNeural Networks, 261\u2013276.\nMangasarian,O.(1965).Linearandnonlinearseparationofpatternsbylinearprogramming.OperationsResearch,\n13, 444\u2013452.\nMason, L., Bartlett, P. L., & Baxter, J. (2000a). Improved generalization through explicit optimization of margins.\nMachine Learning 38(3), 243\u2013255.\nMason,L.,Baxter,J.,Bartlett,P.L.,&Frean,M.(2000b).Functionalgradienttechniquesforcombininghypotheses.\nIn A. J. Smola, P. Bartlett, B. Sch\u00a8 olkopf, & C. Schuurmans (Eds.), Advances in Large Margin Classifiers.\nCambridge, MA: MIT Press.\nMoody, J. & Darken, C. (1989). Fast learning in networks of locally-tuned processing units. Neural Computation,\n1(2), 281\u2013294.\nM\u00a8 uller, K.-R., Smola, A., R\u00a8 atsch, G., Sch\u00a8 olkopf, B., Kohlmorgen, J., & Vapnik, V. (1998). Using support vector\nmachines for time series prediction. In B. Sch\u00a8 olkopf, C. Burges, & A. Smola (Eds.), Advances in Kernel\nMethods\u2014Support Vector Learning. Cambridge, MA: MIT Press.\nOnoda,T.,R\u00a8 atsch,G.,&M\u00a8 uller,K.-R.(1998).AnasymptoticanalysisofADABOOSTinthebinaryclassification\ncase. In L. Niklasson, M. Bod\u00b4 en, & T. Ziemke (Eds.), Proceedings ICANN\u201998: Int. Conf. on Artificial Neural\nNetworks (pp. 195\u2013200).\nOnoda, T., R\u00a8 atsch, G., & M\u00a8 uller, K.-R. (2000). An asymptotical analysis and improvement of ADABOOST in the\nbinary classification case. Journal of Japanese Society for AI, 15(2), 287\u2013296 (in Japanese).\nPress, W., Flannery, B., Teukolsky, S., & Vetterling, W. (1992). Numerical Recipes in C (2nd ed.). Cambridge:\nCambridge University Press.\nQuinlan, J. (1992). C4.5: Programs for Machine Learning. Los Altos, CA: Morgan Kaufmann.\nQuinlan,J.(1996).Boostingfirst-orderlearning.InS.Arikawa&A.Sharma(Eds.),LNAI,Vol.1160:Proceedings\nof the 7th International Workshop on Algorithmic Learning Theory (pp. 143\u2013155). Berlin: Springer."},{"page":34,"text":"320\nG. R\u00a8ATSCH, T. ONODA AND K.-R. M\u00a8ULLER\nR\u00a8 atsch, G. (1998). Ensemble learning methods for classification. Master\u2019s Thesis, Department of Computer Sci-\nence, University of Potsdam, Germany (in German).\nR\u00a8 atsch,G.,Onoda,T.,&M\u00a8 uller,K.-R.(1998).SoftmarginsforADABOOST.TechnicalReportNC-TR-1998-021,\nDepartment of Computer Science, Royal Holloway, University of London, Egham, UK.\nR\u00a8 atsch, G., Onoda, T., & M\u00a8 uller, K.-R. (1999). Regularizing ADABOOST. In M. Kearns, S. Solla, & D. Cohn\n(Eds.), Advances in Neural Information Processing Systems 11 (pp. 564\u2013570). Cambridge, MA: MIT Press.\nR\u00a8 atsch,G.,Sch\u00a8 olkopf,B.,Smola,A.,Mika,S.,Onoda,T.,&M\u00a8 uller,K.-R.(2000).Robustensemblelearning.InA.\nSmola,P.Bartlett,B.Sch\u00a8 olkopf,&D.Schuurmans(Eds.),AdvancesinLargeMarginClassifiers(pp.207\u2013219).\nCambridge, MA: MIT Press.\nR\u00a8 atsch,G.,Warmuth,M.,Mika,S.,Onoda,T.,Lemm,S.,&M\u00a8 uller,K.-R.(2000).Barrierboosting.InProceedings\nCOLT\u201900: Conference on Computational Learning Theory (pp. 170\u2013179). Los Altos, CA: Morgan Kaufmann.\nRokui, J. & Shimodaira, H. (1998). Improving the generalization performance of the minimum classification error\nlearning and its application to neural networks. In Proc. of the Int. Conf. on Neural Information Processing\n(ICONIP) (pp. 63\u201366). Japan, Kitakyushu.\nSchapire, R. (1999). Theoretical views of boosting. In Proceedings EuroCOLT\u201999: European Conference on\nComputational Learning Theory.\nSchapire,R.,Freund,Y.,Bartlett,P.,&Lee,W.(1997).Boostingthemargin:Anewexplanationfortheeffectiveness\nof voting methods. In Proceedings ICML\u201997: International Conference on Machine Learning (pp. 322\u2013330).\nLos Altos, CA: Morgan Kaufmann.\nSchapire,R.&Singer,Y.(1998).Improvedboostingalgorithmsusingconfidence-ratedpredictions.InProceedings\nCOLT\u201998: Conference on Computational Learning Theory (pp. 80\u201391).\nSch\u00a8 olkopf, B. (1997). Support Vector Learning. R. Oldenbourg Verlag, Berlin.\nSch\u00a8 olkopf, B., Smola, A., & Williamson, R. (2000). New support vector algorithms. Neural Computation. also\nNeuroCOLT TR\u201331\u201389, 12:1083\u20131121.\nSchwenk, H. & Bengio, Y. (1997). AdaBoosting neural networks. In W. Gerstner, A. Germond, M. Hasler, &\nJ.-D. Nicoud (Eds.), Proceedings ICANN\u201997: Int. Conf. on Artificial Neural Networks, Vol. 1327 of LNCS\n(pp. 967\u2013972). Berlin: Springer.\nSmola, A. J. (1998). Learning with kernels. Ph.D. Thesis, Technische Universit\u00a8 at Berlin.\nSmola, A., Sch\u00a8 olkopf, B., & M\u00a8 uller, K.-R. (1998). The connection between regularization operators and support\nvector kernels. Neural Networks, 11, 637\u2013649.\nTikhonov, A. & Arsenin, V. (1977). Solutions of Ill-Posed Problems. Washington, D.C.: W.H. Winston.\nVapnik, V. (1995). The Nature of Statistical Learning Theory. Berlin: Springer.\nWeston, J. (1999). LOO-support vector machines. In Proceedings of IJCNN\u201999.\nWeston, J., Gammerman, A., Stitson, M. O., Vapnik, V., Vovk, V., & Watkins, C. (1997). Density estimation using\nSV machines. Technical Report CSD-TR-97-23, Royal Holloway, University of London, Egham, UK.\nReceived August 14, 1998\nRevised June 14, 1999\nAccepted October 27, 1999\nFinal manuscript September 18, 2000"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914_Soft_Margins_for_AdaBoost\/links\/0046352aaf02269b25000000.pdf","widgetId":"rgw30_56ab1df743862"},"id":"rgw30_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=220343914&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw31_56ab1df743862"},"id":"rgw31_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=220343914&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":220343914,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":220343914,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2076161895,"url":"researcher\/2076161895_Rajmadhan_Ekambaram","fullname":"Rajmadhan Ekambaram","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70970717,"url":"researcher\/70970717_Sergiy_Fefilatyev","fullname":"Sergiy Fefilatyev","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":75003125,"url":"researcher\/75003125_Matthew_Shreve","fullname":"Matthew Shreve","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":10901919,"url":"researcher\/10901919_Kurt_Kramer","fullname":"Kurt Kramer","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":3,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":false,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Sep 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/282590246_Active_Cleaning_of_Label_Noise","usePlainButton":true,"publicationUid":282590246,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/282590246_Active_Cleaning_of_Label_Noise","title":"Active Cleaning of Label Noise","displayTitleAsLink":true,"authors":[{"id":2076161895,"url":"researcher\/2076161895_Rajmadhan_Ekambaram","fullname":"Rajmadhan Ekambaram","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70970717,"url":"researcher\/70970717_Sergiy_Fefilatyev","fullname":"Sergiy Fefilatyev","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":75003125,"url":"researcher\/75003125_Matthew_Shreve","fullname":"Matthew Shreve","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10901919,"url":"researcher\/10901919_Kurt_Kramer","fullname":"Kurt Kramer","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":6122168,"url":"researcher\/6122168_Lawrence_O_Hall","fullname":"Lawrence O. Hall","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":6403646,"url":"researcher\/6403646_Dmitry_B_Goldgof","fullname":"Dmitry B. Goldgof","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7239597,"url":"researcher\/7239597_Rangachar_Kasturi","fullname":"Rangachar Kasturi","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Mislabeled examples in the training data can severely affect the performance of supervised classifiers. In this paper, we present an approach to remove any mislabeled examples in the dataset by selecting suspicious examples as targets for inspection. We show that the large margin and soft margin principles used in support vector machines (SVM) have the characteristic of capturing the mislabeled examples as support vectors. Experimental results on two character recognition datasets show that one-class and two-class SVMs are able to capture around 85% and 99% of label noise examples, respectively, as their support vectors. We propose another new method that iteratively builds two-class SVM classifiers on the non-support vector examples from the training data followed by an expert manually verifying the support vectors based on their classification score to identify any mislabeled examples. We show that this method reduces the number of examples to be reviewed, as well as the parameter independence of this method, through experimental results on four data sets. So, by (re-)examining the labels of the selective support vectors, most noise can be removed. This can be quite advantageous when rapidly building a labeled data set.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/282590246_Active_Cleaning_of_Label_Noise","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":null,"publicationUid":282590246,"publicationUrl":"publication\/282590246_Active_Cleaning_of_Label_Noise","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=282590246&eventCode=","widgetId":"rgw35_56ab1df743862"},"id":"rgw35_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=282590246&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":220343914,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/282590246_Active_Cleaning_of_Label_Noise\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["This will create a group of base classifiers which 135 correctly predict the examples that have large weights. The work of Ratsch et al. [20] and Dietterich [21] show that AdaBoost tends to overfit in the presence of mislabeled examples. In order to avoid building base classifiers for noisy examples, a method was proposed by Cao et al. [22] to reduce the weights of the noisy examples using kNN and Expectation Maximization methods. "],"widgetId":"rgw36_56ab1df743862"},"id":"rgw36_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw34_56ab1df743862"},"id":"rgw34_56ab1df743862","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=282590246&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2071997355,"url":"researcher\/2071997355_Xiaohe_Wu","fullname":"Xiaohe Wu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":11736793,"url":"researcher\/11736793_Wangmeng_Zuo","fullname":"Wangmeng Zuo","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273617103618080%401442246810006_m"},{"id":2071950427,"url":"researcher\/2071950427_Yuanyuan_Zhu","fullname":"Yuanyuan Zhu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70465483,"url":"researcher\/70465483_Liang_Lin","fullname":"Liang Lin","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Apr 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":1,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/275280232_F-SVM_Combination_of_Feature_Transformation_and_SVM_Learning_via_Convex_Relaxation","usePlainButton":true,"publicationUid":275280232,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/275280232_F-SVM_Combination_of_Feature_Transformation_and_SVM_Learning_via_Convex_Relaxation","title":"F-SVM: Combination of Feature Transformation and SVM Learning via Convex Relaxation","displayTitleAsLink":true,"authors":[{"id":2071997355,"url":"researcher\/2071997355_Xiaohe_Wu","fullname":"Xiaohe Wu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":11736793,"url":"researcher\/11736793_Wangmeng_Zuo","fullname":"Wangmeng Zuo","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273617103618080%401442246810006_m"},{"id":2071950427,"url":"researcher\/2071950427_Yuanyuan_Zhu","fullname":"Yuanyuan Zhu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70465483,"url":"researcher\/70465483_Liang_Lin","fullname":"Liang Lin","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"The generalization error bound of support vector machine (SVM) depends on the\nratio of radius and margin, while standard SVM only considers the maximization\nof the margin but ignores the minimization of the radius. Several approaches\nhave been proposed to integrate radius and margin for joint learning of feature\ntransformation and SVM classifier. However, most of them either require the\nform of the transformation matrix to be diagonal, or are non-convex and\ncomputationally expensive. In this paper, we suggest a novel approximation for\nthe radius of minimum enclosing ball (MEB) in feature space, and then propose a\nconvex radius-margin based SVM model for joint learning of feature\ntransformation and SVM classifier, i.e., F-SVM. An alternating minimization\nmethod is adopted to solve the F-SVM model, where the feature transformation is\nupdatedvia gradient descent and the classifier is updated by employing the\nexisting SVM solver. By incorporating with kernel principal component analysis,\nF-SVM is further extended for joint learning of nonlinear transformation and\nclassifier. Experimental results on the UCI machine learning datasets and the\nLFW face datasets show that F-SVM outperforms the standard SVM and the existing\nradius-margin based SVMs, e.g., RMM, R-SVM+ and R-SVM+{\\mu}.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/275280232_F-SVM_Combination_of_Feature_Transformation_and_SVM_Learning_via_Convex_Relaxation","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Wangmeng_Zuo\/publication\/275280232_F-SVM_Combination_of_Feature_Transformation_and_SVM_Learning_via_Convex_Relaxation\/links\/55481aac0cf2e2031b3863ab.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Wangmeng_Zuo","sourceName":"Wangmeng Zuo","hasSourceUrl":true},"publicationUid":275280232,"publicationUrl":"publication\/275280232_F-SVM_Combination_of_Feature_Transformation_and_SVM_Learning_via_Convex_Relaxation","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/275280232_F-SVM_Combination_of_Feature_Transformation_and_SVM_Learning_via_Convex_Relaxation\/links\/55481aac0cf2e2031b3863ab\/smallpreview.png","linkId":"55481aac0cf2e2031b3863ab","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=275280232&reference=55481aac0cf2e2031b3863ab&eventCode=&origin=publication_list","widgetId":"rgw38_56ab1df743862"},"id":"rgw38_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=275280232&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"55481aac0cf2e2031b3863ab","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":220343914,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/275280232_F-SVM_Combination_of_Feature_Transformation_and_SVM_Learning_via_Convex_Relaxation\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["reason to choose them is that they had been widely adopted for evaluating SVM and kernel methods [43], [44], [45]. Table I provides a brief summary of these UCI datasets, which includes 6 2-class problems and 5 multi-class problems. "],"widgetId":"rgw39_56ab1df743862"},"id":"rgw39_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw37_56ab1df743862"},"id":"rgw37_56ab1df743862","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=275280232&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":70229010,"url":"researcher\/70229010_Anthony_Bagnall","fullname":"Anthony Bagnall","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272451472982016%401441968901105_m\/Anthony_Bagnall.png"},{"id":2050878419,"url":"researcher\/2050878419_Reda_Younsi","fullname":"Reda Younsi","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Sep 2014","journal":"Pattern Recognition","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/265788039_Ensembles_of_Random_Sphere_Cover_Classifiers","usePlainButton":true,"publicationUid":265788039,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"3.10","url":"publication\/265788039_Ensembles_of_Random_Sphere_Cover_Classifiers","title":"Ensembles of Random Sphere Cover Classifiers","displayTitleAsLink":true,"authors":[{"id":70229010,"url":"researcher\/70229010_Anthony_Bagnall","fullname":"Anthony Bagnall","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272451472982016%401441968901105_m\/Anthony_Bagnall.png"},{"id":2050878419,"url":"researcher\/2050878419_Reda_Younsi","fullname":"Reda Younsi","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Pattern Recognition 09\/2014; 49. DOI:10.1016\/j.patcog.2015.07.010"],"abstract":"We propose and evaluate alternative ensemble schemes for a new instance based\nlearning classifier, the Randomised Sphere Cover (RSC) classifier. RSC fuses\ninstances into spheres, then bases classification on distance to spheres rather\nthan distance to instances. The randomised nature of RSC makes it ideal for use\nin ensembles. We propose two ensemble methods tailored to the RSC classifier;\n$\\alpha \\beta$RSE, an ensemble based on instance resampling and $\\alpha$RSSE, a\nsubspace ensemble. We compare $\\alpha \\beta$RSE and $\\alpha$RSSE to tree based\nensembles on a set of UCI datasets and demonstrates that RSC ensembles perform\nsignificantly better than some of these ensembles, and not significantly worse\nthan the others. We demonstrate via a case study on six gene expression data\nsets that $\\alpha$RSSE can outperform other subspace ensemble methods on high\ndimensional data when used in conjunction with an attribute filter. Finally, we\nperform a set of Bias\/Variance decomposition experiments to analyse the source\nof improvement in comparison to a base classifier.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/265788039_Ensembles_of_Random_Sphere_Cover_Classifiers","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Anthony_Bagnall\/publication\/265788039_Ensembles_of_Random_Sphere_Cover_Classifiers\/links\/551bc6e40cf2fe6cbf75e774.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Anthony_Bagnall","sourceName":"Anthony Bagnall","hasSourceUrl":true},"publicationUid":265788039,"publicationUrl":"publication\/265788039_Ensembles_of_Random_Sphere_Cover_Classifiers","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/265788039_Ensembles_of_Random_Sphere_Cover_Classifiers\/links\/551bc6e40cf2fe6cbf75e774\/smallpreview.png","linkId":"551bc6e40cf2fe6cbf75e774","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=265788039&reference=551bc6e40cf2fe6cbf75e774&eventCode=&origin=publication_list","widgetId":"rgw41_56ab1df743862"},"id":"rgw41_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=265788039&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"551bc6e40cf2fe6cbf75e774","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":220343914,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/265788039_Ensembles_of_Random_Sphere_Cover_Classifiers\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["For alternative fusion schemes see [25]. Beyond simple accuracy comparison, there are three common approaches to analyse ensemble performance: diversity measures [28] [42]; margin theory [37] [33]; and BV decomposition [22] [43] [14] [5] [44] [2]. These have all been linked [42] [10]. "],"widgetId":"rgw42_56ab1df743862"},"id":"rgw42_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw40_56ab1df743862"},"id":"rgw40_56ab1df743862","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=265788039&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":220343914,"publicationLink":"publication\/220343914_Soft_Margins_for_AdaBoost","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw33_56ab1df743862"},"id":"rgw33_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=220343914&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=720","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":720,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw32_56ab1df743862"},"id":"rgw32_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=220343914&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"0046352aaf02269b25000000","name":"Klaus-Robert M\u00fcller","date":"Dec 13, 2013 ","nameLink":"profile\/Klaus-Robert_Mueller","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914_Soft_Margins_for_AdaBoost\/links\/0046352aaf02269b25000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914_Soft_Margins_for_AdaBoost\/links\/0046352aaf02269b25000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"0414edfdd9eb9d3c2114e3c7100a2fbb","showFileSizeNote":false,"fileSize":"263.01 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"0046352aaf02269b25000000","name":"Klaus-Robert M\u00fcller","date":"Dec 13, 2013 ","nameLink":"profile\/Klaus-Robert_Mueller","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914_Soft_Margins_for_AdaBoost\/links\/0046352aaf02269b25000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914_Soft_Margins_for_AdaBoost\/links\/0046352aaf02269b25000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"0414edfdd9eb9d3c2114e3c7100a2fbb","showFileSizeNote":false,"fileSize":"263.01 KB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[{"props":{"position":"float","orientation":"portrait","coords":"pag:3:rect:117.53,653.52,29.94,7.17","ordinal":"1"},"assetId":"AS:277559855075331@1443186834718"},{"props":{"position":"float","orientation":"portrait","coords":"pag:4:rect:117.53,633.89,29.69,7.17","ordinal":"2"},"assetId":"AS:277559855075333@1443186834775"},{"props":{"position":"float","orientation":"portrait","coords":"pag:7:rect:117.53,633.89,28.93,7.17","ordinal":"3"},"assetId":"AS:277559855075334@1443186834825"},{"props":{"position":"float","orientation":"portrait","coords":"pag:10:rect:117.53,312.40,28.96,7.17","ordinal":"4"},"assetId":"AS:277559855075335@1443186834906"},{"props":{"position":"float","orientation":"portrait","coords":"pag:11:rect:117.53,312.40,29.26,7.17","ordinal":"5"},"assetId":"AS:277559859269636@1443186835150"},{"props":{"position":"float","orientation":"portrait","coords":"pag:11:rect:117.53,528.89,29.72,7.17","ordinal":"6"},"assetId":"AS:277559859269666@1443186835860"},{"props":{"position":"float","orientation":"portrait","coords":"pag:12:rect:117.53,312.40,28.99,7.17","ordinal":"7"},"assetId":"AS:277559859269667@1443186835900"},{"props":{"position":"float","orientation":"portrait","coords":"pag:14:rect:117.53,313.40,29.84,7.17","ordinal":"8"},"assetId":"AS:277559859269670@1443186835956"},{"props":{"position":"float","orientation":"portrait","coords":"pag:14:rect:117.53,559.85,29.74,7.17","ordinal":"9"},"assetId":"AS:277559859269672@1443186836002"},{"props":{"position":"float","orientation":"portrait","coords":"pag:18:rect:117.53,467.40,33.04,7.17","ordinal":"0"},"assetId":"AS:277559863463939@1443186836156"},{"props":{"position":"float","orientation":"portrait","coords":"pag:19:rect:117.53,298.40,358.64,17.90","ordinal":"1"},"assetId":"AS:277559863463941@1443186836198"},{"props":{"position":"float","orientation":"portrait","coords":"pag:20:rect:117.53,376.40,33.00,7.17","ordinal":"2"},"assetId":"AS:277559863463942@1443186836250"},{"props":{"position":"float","orientation":"portrait","coords":"pag:21:rect:117.53,653.82,32.37,7.17","ordinal":"3"},"assetId":"AS:277559863463943@1443186836296"},{"props":{"position":"float","orientation":"portrait","coords":"pag:22:rect:117.53,673.74,33.00,7.17","ordinal":"4"},"assetId":"AS:277559863463946@1443186836345"},{"props":{"position":"float","orientation":"portrait","coords":"pag:24:rect:117.53,483.14,358.63,17.14","ordinal":"5"},"assetId":"AS:277559863463947@1443186836416"}],"figureAssetIds":["AS:277559855075331@1443186834718","AS:277559855075333@1443186834775","AS:277559855075334@1443186834825","AS:277559855075335@1443186834906","AS:277559859269636@1443186835150","AS:277559859269666@1443186835860","AS:277559859269667@1443186835900","AS:277559859269670@1443186835956","AS:277559859269672@1443186836002","AS:277559863463939@1443186836156","AS:277559863463941@1443186836198","AS:277559863463942@1443186836250","AS:277559863463943@1443186836296","AS:277559863463946@1443186836345","AS:277559863463947@1443186836416"],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=UFtuh5c56M7vsUxyHzcrqiE8DkoaMPoIOyp8toThfn7y1YRaNsw8Byxo2WYx6fLudpnQUo3YQ63kygjF5rGt9A.Mh1zhmjkU9Gq2UWRNYfOzXXS4YNBjdyFpdpMnmbgDoTADQa-sYXz9gruL8pmGCNKZje9bQFUHZDb6o5da7D6Gw","clickOnPill":"publication.PublicationFigures.html?_sg=wzWex0g2gn2SFEd0g5x2HU6oMmCt7TC9U4ikUnPlfyeDziw3V6dGskq3wjfhekk2Vi2mtJUF4PtW8BqeVmAYyQ.s40zlTp5ixmjvcJmNnJfxyESp7LJNShJsL_zCmKgC_Dnel3TjEGoiuKBCCRc0AUUTk3Ww0jJmrf8BX8_TQcJ0A"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1o9o3\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FKlaus-Robert_Mueller%2Fpublication%2F220343914_Soft_Margins_for_AdaBoost%2Flinks%2F0046352aaf02269b25000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1o9o3\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=eMZlqXYJO-1xAgShEVFVntjDXfJMSby7CQoANb7i1rh_lhwHSSyIJ924Dakoq7CpgvrjwSpm6ne6BFw4XiACaw","urlHash":"4da7d13e6d6ee8d8cab573bb78c93ad1","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=apXty2PQWNtncoiUxq5PND_zZKPrUcNQ5x0U-k-TymO5APd0EHpRq_C1zjW9O-8gNmaPG09LfxdnXseK7KpPQl056nbYjmNv2oJnvRpvybU.-t4ZxmVSYLvaV3_s5HznQ0KcWUsyyY4OAaRg9jo90AESUIs-sFePcT1zDT--6MRXJXrkv_yR6aiptGx6epQZPg.acTtJllXN43yZjUW_cKNacABy622_gtJBJGAUCPwOtLQZOB1NImilxyDD8o-RHtSggcI8VasKEfsMPpcbhx4Jg","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"0046352aaf02269b25000000","trackedDownloads":{"0046352aaf02269b25000000":{"v":false,"d":false}},"assetId":"AS:101937894658054@1401315296485","readerDocId":"2849244","assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":220343914,"commentCursorPromo":null,"widgetId":"rgw44_56ab1df743862"},"id":"rgw44_56ab1df743862","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FKlaus-Robert_Mueller%2Fpublication%2F220343914_Soft_Margins_for_AdaBoost%2Flinks%2F0046352aaf02269b25000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A101937894658054%401401315296485&publicationUid=220343914&linkId=0046352aaf02269b25000000&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Soft Margins for AdaBoost","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=4zvKl0YENRmh3EOyPcG1IrhhZdQk3VcklOA7QrVedswhIJSbGsdWQRl5ycVOVWdhug6ajmOchBb5z62ZBeD2s_Ou2JFAztr_cFc20Qx-xw0.62dFyaNuNh2QqfxlSbwTMXZ4esflt2jswTAvWkGNPK_X53F7dwuMVoHY2BZUBdW9Q5fNzrmTGUVSHnH9pe9qkw.U7kJIdTNVTyHUAdtrSop-sJnHwQ1yObZsuTZeE5dSmTRzR8Xn--pQMKrDXyfqXvkO_K87ZmpNbDbjOfqgEFF8A","publicationUid":220343914,"trackedDownloads":{"0046352aaf02269b25000000":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw46_56ab1df743862"},"id":"rgw46_56ab1df743862","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw47_56ab1df743862"},"id":"rgw47_56ab1df743862","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw48_56ab1df743862"},"id":"rgw48_56ab1df743862","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw49_56ab1df743862"},"id":"rgw49_56ab1df743862","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw50_56ab1df743862"},"id":"rgw50_56ab1df743862","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw45_56ab1df743862"},"id":"rgw45_56ab1df743862","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw43_56ab1df743862"},"id":"rgw43_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/220343914_Soft_Margins_for_AdaBoost","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1df743862"},"id":"rgw2_56ab1df743862","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":220343914},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=220343914&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1df743862"},"id":"rgw1_56ab1df743862","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"eaNxojqssxNSXNvDaHSOUXXLwwzP4\/EIfZ+mwcNPQtbFxFnO1WaLFVW4wsRkgVadd2nF3ITzQLCWz2k7yhpska9gwNPTBayKdyBBbgAvpS04hS01oSDi9EhVDeii1NXJ9xR\/G6WCYX3PUnTeOoipI\/zv1\/iv3OMhRpIohpgiIcn8Xf2ApnHo1wxxz\/\/mXk2IgygR\/\/s1N7SxPx1fjWyVVyG3hWXaP1fT7pORgwc5Tzb32RbXrWbAd3IimZBcIiUsJ+frwyLbDxE9xcBl+EfBPMuk58tWmEU5dvyF4Pjk0tU=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/220343914_Soft_Margins_for_AdaBoost\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Soft Margins for AdaBoost\" \/>\n<meta property=\"og:description\" content=\"Recently ensemble methods like ADABOOST have been applied successfully in many problems, while seemingly defying the problems of overfitting.\nADABOOST rarely overfits in the low noise regime,...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/220343914_Soft_Margins_for_AdaBoost\/links\/0046352aaf02269b25000000\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/220343914_Soft_Margins_for_AdaBoost\" \/>\n<meta property=\"rg:id\" content=\"PB:220343914\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1023\/A:1007618119488\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Soft Margins for AdaBoost\" \/>\n<meta name=\"citation_author\" content=\"Gunnar R\u00e4tsch\" \/>\n<meta name=\"citation_author\" content=\"Takashi Onoda\" \/>\n<meta name=\"citation_author\" content=\"Klaus-Robert M\u00fcller\" \/>\n<meta name=\"citation_publication_date\" content=\"2001\/03\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"Machine Learning\" \/>\n<meta name=\"citation_issn\" content=\"0885-6125\" \/>\n<meta name=\"citation_volume\" content=\"42\" \/>\n<meta name=\"citation_issue\" content=\"3\" \/>\n<meta name=\"citation_firstpage\" content=\"287\" \/>\n<meta name=\"citation_lastpage\" content=\"320\" \/>\n<meta name=\"citation_doi\" content=\"10.1023\/A:1007618119488\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Klaus-Robert_Mueller\/publication\/220343914_Soft_Margins_for_AdaBoost\/links\/0046352aaf02269b25000000.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/220343914_Soft_Margins_for_AdaBoost\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/220343914_Soft_Margins_for_AdaBoost\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/215868066921738\/styles\/pow\/publicliterature\/FigureList.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-06212ce2-6dfe-47e6-b61f-0dfa36376db2","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":992,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw51_56ab1df743862"},"id":"rgw51_56ab1df743862","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-06212ce2-6dfe-47e6-b61f-0dfa36376db2", "a680c84a970ae9f2bc297d4349fe28657ba2e1f1");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-06212ce2-6dfe-47e6-b61f-0dfa36376db2", "a680c84a970ae9f2bc297d4349fe28657ba2e1f1");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw52_56ab1df743862"},"id":"rgw52_56ab1df743862","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/220343914_Soft_Margins_for_AdaBoost","requestToken":"jbqFmAYL85Nouea0pZNYyc7uVECd3XTZ0aT03pt2PvPI1wGDfTipuorFs8SzoveUwFSCm2M\/LTLR\/H0FXTzxl\/NdkG1+TszpirM9CO6bLM7WSWhkVHigVh6ECReOw+RgRskWgNd9GkfK6UAO\/9n7i3kYFZ43Jrg5jXc+xePjSQ2gpwCTmx7pU+XDLhR6WncVPmk\/am1Y6jYruG\/T1EhVSQIXserNOVbBliQuqZ3ddobOl4W9FT56jm4anNvLhzBHATLzlAh\/ls34JYYW6iKoBecKREh\/IX7l\/Xihd556i\/c=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=c1OOqIOfU0ICdM0noikywxWM8dRyyW4P71aXc63jILRg_08XVockhDrxqTnJtGa2","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjIwMzQzOTE0X1NvZnRfTWFyZ2luc19mb3JfQWRhQm9vc3Q%3D","signupCallToAction":"Join for free","widgetId":"rgw54_56ab1df743862"},"id":"rgw54_56ab1df743862","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw53_56ab1df743862"},"id":"rgw53_56ab1df743862","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw55_56ab1df743862"},"id":"rgw55_56ab1df743862","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
