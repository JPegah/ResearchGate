<!DOCTYPE html> <html lang="en" class="" id="rgw30_56ab9ef632cec"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="plblw7X9hfUfXqsSqrMqfahp46AuIOPLOm6vRgkb4uHicfB8y1bGBlVatPsXC0Zd2wr+qB0q0aJy0PW5lS3dtAGW/slr9zfOnHau9Be5CAJewDBQizrwuG6sfld/Aa5BRL1fANkYOtTNAsBle5d63s0YMwo4RLKast9y7W3CmcFoEFFnwfrUbKBcGF/Dvlm0Ln1GclUubpxYr75WXDVab9BLOW+IxhMNNbPKrXPY7V7Z+CE5vHugW9VU7kdVk25DmvDKlGZFEFWAGHBEEOmpKFI7UjXU7aB67QPAQsiECGM="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-83cf3be7-dff2-4f12-9485-df2e8122a606",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/2239690_Variational_Inference_for_Bayesian_Mixtures_of_Factor_Analysers" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Variational Inference for Bayesian Mixtures of Factor Analysers" />
<meta property="og:description" content="We present an algorithm that infers the model structure of a mixture of factor analysers using an ecient and deterministic variational approximation to full Bayesian integration over model..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/2239690_Variational_Inference_for_Bayesian_Mixtures_of_Factor_Analysers/links/0e5fa369f0c41c4932de6c0a/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/2239690_Variational_Inference_for_Bayesian_Mixtures_of_Factor_Analysers" />
<meta property="rg:id" content="PB:2239690" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Variational Inference for Bayesian Mixtures of Factor Analysers" />
<meta name="citation_author" content="Zoubin Ghahramani" />
<meta name="citation_author" content="Matthew J. Beal" />
<meta name="citation_publication_date" content="2000/09/03" />
<meta name="citation_journal_title" content="Advances in neural information processing systems" />
<meta name="citation_issn" content="1049-5258" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/2239690_Variational_Inference_for_Bayesian_Mixtures_of_Factor_Analysers" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/2239690_Variational_Inference_for_Bayesian_Mixtures_of_Factor_Analysers" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Variational Inference for Bayesian Mixtures of Factor Analysers</title>
<meta name="description" content="Variational Inference for Bayesian Mixtures of Factor Analysers on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab9ef632cec" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab9ef632cec" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw8_56ab9ef632cec">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Variational%20Inference%20for%20Bayesian%20Mixtures%20of%20Factor%20Analysers&rft.title=Advances%20in%20Neural%20Information%20Processing%20Systems&rft.jtitle=Advances%20in%20Neural%20Information%20Processing%20Systems&rft.date=2000&rft.issn=1049-5258&rft.au=Zoubin%20Ghahramani%2CMatthew%20J.%20Beal&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Variational Inference for Bayesian Mixtures of Factor Analysers</h1> <meta itemprop="headline" content="Variational Inference for Bayesian Mixtures of Factor Analysers">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/2239690_Variational_Inference_for_Bayesian_Mixtures_of_Factor_Analysers/links/0e5fa369f0c41c4932de6c0a/smallpreview.png">  <div id="rgw11_56ab9ef632cec" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw12_56ab9ef632cec"> <a href="researcher/8159937_Zoubin_Ghahramani" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Zoubin Ghahramani</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw13_56ab9ef632cec">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/8159937_Zoubin_Ghahramani"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/8159937_Zoubin_Ghahramani" class="display-name">Zoubin Ghahramani</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw14_56ab9ef632cec"> <a href="researcher/8159938_Matthew_J_Beal" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Matthew J. Beal" alt="Matthew J. Beal" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Matthew J. Beal</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw15_56ab9ef632cec">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/8159938_Matthew_J_Beal"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Matthew J. Beal" alt="Matthew J. Beal" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/8159938_Matthew_J_Beal" class="display-name">Matthew J. Beal</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/1049-5258_Advances_in_neural_information_processing_systems"><span itemprop="name">Advances in neural information processing systems</span></a> </span>        <meta itemprop="datePublished" content="2000-09">  09/2000;               <div class="pub-source"> Source: <a href="http://citeseer.ist.psu.edu/322672.html" rel="nofollow">CiteSeer</a> </div>  </div> <div id="rgw16_56ab9ef632cec" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>We present an algorithm that infers the model structure of a mixture of factor analysers using an ecient and deterministic variational approximation to full Bayesian integration over model parameters. This procedure can automatically determine the optimal number of components and the local dimensionality of each component (i.e. the number of factors in each factor analyser). Alternatively it can be used to infer posterior distributions over number of components and dimensionalities. Since all parameters are integrated out the method is not prone to over tting. Using a stochastic procedure for adding components it is possible to perform the variational optimisation incrementally and to avoid local maxima. Results show that the method works very well in practice and correctly infers the number and dimensionality of nontrivial synthetic examples. By importance sampling from the variational approximation we show how to obtain unbiased estimates of the true evidence, the exa...</div> </p>  </div>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw29_56ab9ef632cec">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw28_56ab9ef632cec"  itemprop="articleBody">  <p>Page 1</p> <p>Variational Inference for Bayesian<br />Mixtures of Factor Analysers<br />Zoubin Ghahramani and Matthew J. Beal<br />Gatsby Computational Neuroscience Unit<br />University College London<br />17 Queen Square, London WC1N 3AR, England<br />{zoubin,m.beal}@gatsby.ucl.ac.uk http://www.gatsby.ucl.ac.uk<br />Abstract<br />We present an algorithm that infers the model structure of a mix-<br />ture of factor analysers using an efficient and deterministic varia-<br />tional approximation to full Bayesian integration over model pa-<br />rameters. This procedure can automatically determine the opti-<br />mal number of components and the local dimensionality of each<br />component (i.e. the number of factors in each factor analyser).<br />Alternatively it can be used to infer posterior distributions over<br />number of components and dimensionalities. Since all parameters<br />are integrated out the method is not prone to overfitting. Using a<br />stochastic procedure for adding components it is possible to per-<br />form the variational optimisation incrementally and to avoid local<br />maxima. Results show that the method works very well in practice<br />and correctly infers the number and dimensionality of nontrivial<br />synthetic examples.<br />By importance sampling from the variational approximation we<br />show how to obtain unbiased estimates of the true evidence, the<br />exact predictive density, and the KL divergence between the varia-<br />tional posterior and the true posterior, not only in this model but<br />for variational approximations in general.<br />1Introduction<br />Factor analysis (FA) is a method for modelling correlations in multidimensional<br />data. The model assumes that each p-dimensional data vector y was generated by<br />first linearly transforming a k &lt; p dimensional vector of unobserved independent<br />zero-mean unit-variance Gaussian sources, x, and then adding a p-dimensional zero-<br />mean Gaussian noise vector, n, with diagonal covariance matrix Ψ: i.e. y = Λx+n.<br />Integrating out x and n, the marginal density of y is Gaussian with zero mean<br />and covariance ΛΛT+ Ψ. The matrix Λ is known as the factor loading matrix.<br />Given data with a sample covariance matrix Σ, factor analysis finds the Λ and Ψ<br />that optimally fit Σ in the maximum likelihood sense. Since k &lt; p, a single factor<br />analyser can be seen as a reduced parametrisation of a full-covariance Gaussian.1<br />1Factor analysis and its relationship to principal components analysis (PCA) and mix-<br />ture models is reviewed in [10].</p>  <p>Page 2</p> <p>A mixture of factor analysers (MFA) models the density for y as a weighted average<br />of factor analyser densities<br />P(y|Λ,Ψ,π) =<br />S<br />?<br />s=1<br />P(s|π)P(y|s,Λs,Ψ),<br />(1)<br />where π is the vector of mixing proportions, s is a discrete indicator variable, and<br />Λsis the factor loading matrix for factor analyser s which includes a mean vector<br />for y.<br />By exploiting the factor analysis parameterisation of covariance matrices, a mix-<br />ture of factor analysers can be used to fit a mixture of Gaussians to correlated high<br />dimensional data without requiring O(p2) parameters or undesirable compromises<br />such as axis-aligned covariance matrices. In an MFA each Gaussian cluster has in-<br />trinsic dimensionality k (or ksif the dimensions are allowed to vary across clusters).<br />Consequently, the mixture of factor analysers simultaneously addresses the prob-<br />lems of clustering and local dimensionality reduction. When Ψ is a multiple of the<br />identity the model becomes a mixture of probabilistic PCAs. Tractable maximum<br />likelihood procedure for fitting MFA and MPCA models can be derived from the<br />Expectation Maximisation algorithm [4, 11].<br />The maximum likelihood (ML) approach to MFA can easily get caught in local<br />maxima.2Ueda et al. [12] provide an effective deterministic procedure for avoiding<br />local maxima by considering splitting a factor analyser in one part of space and<br />merging two in a another part. But splits and merges have to be considered simul-<br />taneously because the number of factor analysers has to stay the same since adding<br />a factor analyser is always expected to increase the training likelihood.<br />A fundamental problem with maximum likelihood approaches is that they fail to<br />take into account model complexity (i.e. the cost of coding the model parameters).<br />So more complex models are not penalised, which leads to overfitting and the in-<br />ability to determine the best model size and structure (or distributions thereof)<br />without resorting to costly cross-validation procedures. Bayesian approaches over-<br />come these problems by treating the parameters θ as unknown random variables<br />and averaging over the ensemble of models they define:<br />?<br />P(Y ) is the evidence for a data set Y = {y1,... ,yN}. Integrating out parameters<br />penalises models with more degrees of freedom since these models can a priori<br />model a larger range of data sets. All information inferred from the data about the<br />parameters is captured by the posterior distribution P(θ|Y ) rather than the ML<br />point estimateˆθ.3<br />While Bayesian theory deals with the problems of overfitting and model selec-<br />tion/averaging, in practice it is often computationally and analytically intractable to<br />perform the required integrals. For Gaussian mixture models Markov chain Monte<br />Carlo (MCMC) methods have been developed to approximate these integrals by<br />sampling [8, 7]. The main criticism of MCMC methods is that they are slow and<br />P(Y ) =<br />dθ P(Y |θ)P(θ).<br />(2)<br />2Technically, the log likelihood is not bounded above if no constraints are put on the<br />determinant of the component covariances. So the real ML objective for MFA is to find<br />the highest finite local maximum of the likelihood.<br />3We sometimes use θ to refer to the parameters and sometimes to all the unknown<br />quantities (parameters and hidden variables). Formally the only difference between the two<br />is that the number of hidden variables grows with N, whereas the number of parameters<br />usually does not.</p>  <p>Page 3</p> <p>it is usually difficult to assess convergence. Furthermore, the posterior density over<br />parameters is stored as a set of samples, which can be inefficient.<br />Another approach to Bayesian integration for Gaussian mixtures [9] is the Laplace<br />approximation which makes a local Gaussian approximation around a maximum a<br />posteriori parameter estimate. These approximations are based on large data limits<br />and can be poor, particularly for small data sets (for which, in principle, the advan-<br />tages of Bayesian integration over ML are largest). Local Gaussian approximations<br />are also poorly suited to bounded or positive parameters such as the mixing pro-<br />portions of the mixture model. Finally, it is difficult to see how this approach can<br />be applied to online incremental changes to model structure.<br />In this paper we employ a third approach to Bayesian inference: variational ap-<br />proximation. We form a lower bound on the log evidence using Jensen’s inequality:<br />?<br />which we seek to maximise. Maximising F is equivalent to minimising the KL-<br />divergence between Q(θ) and P(θ|Y ), so a tractable Q can be used as an approx-<br />imation to the intractable posterior. This approach draws its roots from one way<br />of deriving mean field approximations in physics, and has been used recently for<br />Bayesian inference [13, 5, 1].<br />The variational method has several advantages over MCMC and Laplace approxi-<br />mations. Unlike MCMC, convergence can be assessed easily by monitoring F. The<br />approximate posterior is encoded efficiently in Q(θ). Unlike Laplace approxima-<br />tions, the form of Q can be tailored to each parameter (in fact the optimal form<br />of Q for each parameter falls out of the optimisation), the approximation is global,<br />and Q optimises an objective function. Variational methods are generally fast, F<br />is guaranteed to increase monotonically and transparently incorporates model com-<br />plexity. To our knowledge, no one has done a full Bayesian analysis of mixtures of<br />factor analysers.<br />Of course, vis-a-vis MCMC, the main disadvantage of variational approximations<br />is that they are not guaranteed to find the exact posterior in the limit. However,<br />with a straightforward application of sampling, it is possible to take the result of<br />the variational optimisation and use it to sample from the exact posterior and exact<br />predictive density. This is described in section 5.<br />In the remainder of this paper we first describe the mixture of factor analysers in<br />more detail (section 2). We then derive the variational approximation (section 3).<br />We show empirically that the model can infer both the number of components and<br />their intrinsic dimensionalities, and is not prone to overfitting (section 6). Finally,<br />we conclude in section 7.<br />L ≡ lnP(Y ) = ln<br />dθ P(Y,θ) ≥<br />?<br />dθ Q(θ)lnP(Y,θ)<br />Q(θ)<br />≡ F,<br />(3)<br />2The Model<br />Starting from (1), the evidence for the Bayesian MFA is obtained by averaging the<br />likelihood under priors for the parameters (which have their own hyperparameters):<br />?<br />N<br />?<br />P(Y )=<br />dπP(π|α)<br />?<br />sn=1<br />?<br />dνP(ν|a,b)<br />?<br />?<br />dΛ P(Λ|ν) ·<br />n=1<br />S<br />?<br />P(sn|π)<br />dxnP(xn)P(yn|xn,sn,Λs,Ψ)<br />?<br />.<br />(4)</p>  <p>Page 4</p> <p>Here {α,a,b,Ψ} are hyperparameters4, ν are precision parameters (i.e. inverse vari-<br />ances) for the columns of Λ. The conditional independence relations between the<br />variables in this model are shown graphically in the usual belief network represen-<br />tation in Figure 1.<br />While arbitrary choices could be made for the<br />a,b<br />priors on the first line of (4), choosing priors that<br />are conjugate to the likelihood terms on the sec-<br />ond line of (4) greatly simplifies inference and<br />interpretability.5<br />symmetric Dirichlet, which is conjugate to the<br />multinomial P(s|π).<br />The prior for the factor loading matrix plays a<br />key role in this model. Each component of the<br />mixture has a Gaussian prior P(Λs|νs), where<br />each element of the vector νsis the precision of<br />a column of Λ. If one of these precisions νs<br />then the outgoing weights for factor xlwill go to<br />zero, which allows the model to reduce the in-<br />trinsic dimensionality of x if the data does not<br />warrant this added dimension. This method of<br />intrinsic dimensionality reduction has been used<br />by Bishop [2] for Bayesian PCA, and is closely<br />related to MacKay and Neal’s method for auto-<br />matic relevance determination (ARD) for inputs<br />to a neural network [6].<br />To avoid overfitting it is important to integrate out all parameters whose cardinality<br />scales with model complexity (i.e. number of components and their dimensionali-<br />ties). We therefore also integrate out the precisions using Gamma priors, P(ν|a,b).<br />ν1?<br />Λ1<br />α<br />?<br />Ψ<br />...?<br />ν2<br />νS<br />Λ2<br />ΛS<br />...?<br />yn<br />xn<br />sn<br />π<br />n=1...N<br />Figure 1:<br />variational Bayesian mixture of<br />factor analysers.<br />random variables, solid rectangles<br />denote hyperparameters, and the<br />dashed rectangle shows the plate<br />(i.e. repetitions) over the data.<br />Generative model for<br />Circles denote<br />So we choose P(π|α) to be<br />l→ ∞,<br />3 The Variational Approximation<br />Applying Jensen’s inequality repeatedly to the log evidence (4) we lower bound it<br />using the following factorisation of the distribution of parameters and hidden vari-<br />ables: Q(Λ)Q(π,ν)Q(s,x). Given this factorisation several additional factorisations<br />fall out of the conditional independencies in the model resulting in the variational<br />objective function:<br />?<br />s=1<br />N<br />?<br />+<br />dΛsQ(Λs)<br />F=<br />dπQ(π)lnP(π|α)<br />Q(π)<br />+<br />S<br />?<br />dπ Q(π)lnP(sn|π)<br />?<br />dνsQ(νs)<br />?<br />lnP(νs|a,b)<br />Q(νs)<br />?<br />+<br />?<br />dΛsQ(Λs)lnP(Λs|νs)<br />Q(Λs)<br />?<br />+<br />n=1<br />S<br />?<br />sn=1<br />Q(sn)<br />??<br />Q(sn)<br />?<br />+<br />dxnQ(xn|sn)ln<br />P(xn)<br />Q(xn|sn)<br />?<br />dxnQ(xn|sn)lnP(yn|xn,sn,Λs,Ψ)<br />?<br />(5)<br />The variational posteriors Q(·), as given in the Appendix, are derived by performing<br />a free-form extremisation of F w.r.t. Q. It is not difficult to show that these extrema<br />are indeed maxima of F. The optimal posteriors Q are of the same conjugate forms<br />as the priors. The model hyperparameters which govern the priors can be estimated<br />in the same fashion (see the Appendix).<br />4We currently do not integrate out Ψ, although this can also be done.<br />5Conjugate priors have the same effect as pseudo-observations.</p>  <p>Page 5</p> <p>4<br />When optimising F, occasionally one finds that for some s:?<br />the local data to overcome the dimensional complexity prior on the factor loading<br />matrices. So components of the mixture die of natural causes when they are no<br />longer needed. Removing these redundant components increases F.<br />Component birth does not happen spontaneously, so we introduce a heuristic.<br />Whenever F has stabilised we pick a parent-component stochastically with prob-<br />ability proportional to e−βFsand attempt to split it into two; Fsis the s-specific<br />contribution to F with the last bracketed term in (5) normalised by?<br />dom as it concentrates attempted births on components that are faring poorly. The<br />parameter distributions of the two Gaussians created from the split are initialised<br />by partitioning the responsibilities for the data, Q(sn), along a direction sampled<br />from the parent’s distribution. This usually causes F to decrease, so by monitoring<br />the future progress of F we can reject this attempted birth if F does not recover.<br />Although it is perfectly possible to start the model with many components and let<br />them die, it is computationally more efficient to start with one component and allow<br />it to spawn more when necessary.<br />Birth and Death<br />nQ(sn) = 0. These<br />zero responsibility components are the result of there being insufficient support from<br />nQ(sn).<br />This works better than both cycling through components and picking them at ran-<br />5 Exact Predictive Density, True Evidence, and KL<br />By importance sampling from the variational approximation we can obtain unbiased<br />estimates of three important quantities: the exact predictive density, the true log<br />evidence L, and the KL divergence between the variational posterior and the true<br />posterior. Letting θ = {Λ,π}, we sample θi∼ Q(θ). Each such sample is an instance<br />of a mixture of factor analysers with predictive density given by (1). We weight<br />these predictive densities by the importance weights wi = P(θi,Y )/Q(θi), which<br />are easy to evaluate. This results in a mixture of mixtures of factor analysers, and<br />will converge to the exact predictive density, P(y|Y ), as long as Q(θ) &gt; 0 wherever<br />P(θ|Y ) &gt; 0. The true log evidence can be similarly estimated by L = ln?w?, where<br />?·? denotes averaging over the importance samples. Finally, the KL divergence is<br />given by: KL(Q(θ)?P(θ|Y )) = ln?w? − ?lnw?.<br />This procedure has three significant properties. First, the same importance weights<br />can be used to estimate all three quantities. Second, while importance sampling<br />can work very poorly in high dimensions for ad hoc proposal distributions, here the<br />variational optimisation is used in a principled manner to pick Q to be a good ap-<br />proximation to P and therefore hopefully a good proposal distribution. Third, this<br />procedure can be applied to any variational approximation. A detailed exposition<br />can be found in [3].<br />6<br />Experiment 1: Discovering the number of components.<br />model on synthetic data generated from a mixture of 18 Gaussians with 50 points<br />per cluster (Figure 2, top left). The variational algorithm has little difficulty finding<br />the correct number of components and the birth heuristics are successful at avoiding<br />local maxima. After finding the 18 Gaussians repeated splits are attempted and<br />rejected. Finding a distribution over number of components using F is also simple.<br />Experiment 2: The shrinking spiral.<br />from a shrinking spiral from [12] as another test of how well the algorithm could<br />Results<br />We tested the<br />We used the dataset of 800 data points</p>  <p>Page 6</p> <p>Figure 2: (top) Exp 1: The frames from left to right are the data, and the 2 S.D. Gaussian<br />ellipses after 7, 14, 16 and 22 accepted births. (bottom) Exp 2: Shrinking spiral data<br />and 1 S.D. Gaussian ellipses after 6, 9, 12, and 17 accepted births. Note that the number<br />of Gaussians increases from left to right.<br />0 50010001500 2000<br />−7800<br />−7600<br />−7400<br />−7200<br />−7000<br />−6800<br />−6600<br />−6400<br />?<br />                   ????<br />?<br />???<br />???<br />???<br />???<br />???<br />???<br />1?7? 4? 3?2? 2?<br />    8??<br />    8??<br />   16??<br />   32??<br />   64??<br />  128??<br /> ?<br />    1???<br />1??<br />1?<br />1?<br />1?<br />       2???     1?<br />  2?<br />      4???<br />3?<br />4?<br />4?<br />2?<br />2?<br />2?<br />2<br />6?<br />7?<br />7?<br />3?<br />3?<br />3?<br />2?<br />2?<br />2?<br />intrinsic dimensionalities<br />  number?<br /> of points?<br />per cluster<br />Figure 3: (left) Exp 2: F as function of iteration for the spiral problem on a typical run.<br />Drops in F constitute component births. Thick lines are accepted attempts, thin lines are<br />rejected attempts. (middle) Exp 3: Means of the factor loading matrices. These results<br />are analogous to those given by Bishop [2] for Bayesian PCA. (right) Exp 3: Table with<br />learned number of Gaussians and dimensionalities as training set size increases. Boxes<br />represent model components that capture several of the clusters.<br />escape local maxima and how robust it was to initial conditions (Figure 2, bottom).<br />Again local maxima did not pose a problem and the algorithm always found between<br />12-14 Gaussians regardless of whether it was initialised with 0 or 200. These runs<br />took about 3-4 minutes on a 500MHz Alpha EV6 processor. A plot of F shows that<br />most of the compute time is spent on accepted moves (Figure 3, left).<br />Experiment 3: Discovering the local dimensionalities.<br />thetic data set of 300 data points in each of 6 Gaussians with intrinsic dimension-<br />alities (7 4 3 2 2 1) embedded in 10 dimensions. The variational Bayesian approach<br />correctly inferred both the number of Gaussians and their intrinsic dimensionalities<br />(Figure 3, middle). We varied the number of data points and found that as expected<br />with fewer points the data could not provide evidence for as many components and<br />intrinsic dimensions (Figure 3, right).<br />We generated a syn-<br />7<br />Search over model structures for MFAs is computationally intractable if each factor<br />analyser is allowed to have different intrinsic dimensionalities. In this paper we have<br />shown that the variational Bayesian approach can be used to efficiently infer this<br />model structure while avoiding overfitting and other deficiencies of ML approaches.<br />One attraction of our variational method, which can be exploited in other models,<br />is that once a factorisation of Q is assumed all inference is automatic and exact.<br />We can also use F to get a distribution over structures if desired. Finally we derive<br />Discussion</p>  <p>Page 7</p> <p>a generally applicable importance sampler that gives us unbiased estimates of the<br />true evidence, the exact predictive density, and the KL divergence between the<br />variational posterior and the true posterior.<br />Encouraged by the results on synthetic data, we have applied the Bayesian mixture<br />of factor analysers to a real-world unsupervised digit classification problem. We<br />will report the results of these experiments in a separate article.<br />Appendix: Optimal Q Distributions and Hyperparameters<br />Q(xn|sn) ∼ N(xn,s,Σs)<br />lnQ(sn) = [ψ(ωus) − ψ(ω)] +1<br />−1yn, Λ<br />q=Ψ<br />n=1<br />N<br />?<br />where {N,G,D} denote Normal, Gamma and Dirichlet distributions respectively, ?·? de-<br />notes expectation under the variational posterior, and ψ(x) is the digamma function<br />ψ(x) ≡<br />variance structure; even though each Λsis a p × q matrix, its covariance only has O(pq2)<br />parameters. Differentiating F with respect to the parameters, a and b, of the precision prior<br />we get fixed point equations ψ(a) = ?lnν? + lnb and b = a/?ν?. Similarly the fixed point<br />for the parameters of the Dirichlet prior is ψ(α) − ψ(α/S) +?[ψ(ωus) − ψ(ω)]/S = 0.<br />References<br />Q(Λs<br />q) ∼ N(Λ<br />s<br />q,Σq,s)Q(νs<br />l) ∼ G(as<br />l,bs<br />l)Q(π) ∼ D(ωu)<br />2ln|Σs| + ?lnP(yn|xn,sn,Λs,Ψ)? + c<br />?<br />q<br />Q(sn)?xnxn?? +diag?νs?, ωus =α<br />xn,s= ΣsΛ<br />s?Ψ<br />s<br />?<br />−1<br />N<br />?<br />Q(sn)ynxn,s?Σq,s<br />, as<br />l=a +p<br />2, bs<br />l=b +1<br />2<br />p<br />?<br />N<br />?<br />q=1<br />?Λs<br />ql<br />2?<br />Σs−1= ?Λs?Ψ<br />−1Λs? + I, Σq,s−1=Ψ<br />−1<br />qq<br />n=1<br />S+<br />n=1<br />Q(sn)<br />∂<br />∂xlnΓ(x). Note that the optimal distributions Q(Λs) have block diagonal co-<br />[1] H. Attias. Inferring parameters and structure of latent variable models by variational<br />Bayes. In Proc. 15th Conf. on Uncertainty in Artificial Intelligence, 1999.<br />[2] C.M. Bishop. Variational PCA. In Proc. Ninth Int. Conf. on Artificial Neural Net-<br />works. ICANN, 1999.<br />[3] Z. Ghahramani, H. Attias, and M.J. Beal.<br />Report GCNU-TR-1999-006, (in prep.) Gatsby Unit, Univ. College London, 1999.<br />[4] Z. Ghahramani and G.E. Hinton.<br />tor analyzers.Technical Report CRG-TR-96-1 [http://www.gatsby.ucl.ac.uk/<br />∼zoubin/papers/tr-96-1.ps.gz], Dept. of Comp. Sci., Univ. of Toronto, 1996.<br />[5] D.J.C. MacKay. Ensemble learning for hidden Markov models. Technical report,<br />Cavendish Laboratory, University of Cambridge, 1997.<br />[6] R.M. Neal.Assessing relevance determination methods using DELVE. In C.M. Bishop,<br />editor, Neural Networks and Machine Learning, pages 97–129. Springer-Verlag, 1998.<br />[7] C.E. Rasmussen. The infinite gaussian mixture model. In Adv. Neur. Inf. Proc. Sys.<br />12, Cambridge, MA, 2000. MIT Press.<br />[8] S. Richardson and P.J. Green. On Bayesian analysis of mixtures with an unknown<br />number of components. Journal Royal Stat. Society, Series B, 59(4):731–758, 1997.<br />[9] S.J. Roberts, D. Husmeier, I. Rezek, and W. Penny. Bayesian approaches to Gaussian<br />mixture modeling. IEEE PAMI, 20(11):1133–1142, 1998.<br />[10] S. T. Roweis and Z. Ghahramani. A unifying review of linear Gaussian models. Neural<br />Computation, 11(2):305–345, 1999.<br />[11] M.E. Tipping and C.M. Bishop. Mixtures of probabilistic principal component ana-<br />lyzers. Neural Computation, 11(2):443–482, 1999.<br />[12] N. Ueda, R. Nakano, Z. Ghahramani, and G.E. Hinton. SMEM algorithm for mixture<br />models. In Adv. Neur. Inf. Proc. Sys. 11, Cambridge, MA, 1999. MIT Press.<br />[13] S. Waterhouse, D.J.C. Mackay, and T. Robinson. Bayesian methods for mixtures of<br />experts. In Adv. Neur. Inf. Proc. Sys. 7, Cambridge, MA, 1995. MIT Press.<br />Learning model structure.Technical<br />The EM algorithm for mixtures of fac-</p>   </div> <div id="rgw21_56ab9ef632cec" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw22_56ab9ef632cec">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw23_56ab9ef632cec"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.4497&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Variational Inference for Bayesian Mixtures of Factor Analysers">Variational Inference for Bayesian Mixtures of Fac...</a> </div>  <div class="details">   Available from <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.4497&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow">psu.edu</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw25_56ab9ef632cec" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw26_56ab9ef632cec">  </ul> </div> </div>   <div id="rgw17_56ab9ef632cec" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw18_56ab9ef632cec"> <div> <h5> <a href="publication/2816888_Stochastic_Model_Selection_for_Bayesian_Mixtures_of_Factor_Analysers" class="color-inherit ga-similar-publication-title"><span class="publication-title">Stochastic Model Selection for Bayesian Mixtures of Factor Analysers</span></a>  </h5>  <div class="authors"> <a href="researcher/8738684_D_M_Titterington" class="authors ga-similar-publication-author">D. M. Titterington</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56ab9ef632cec"> <div> <h5> <a href="publication/2281698_Variational_Inference_for_Bayesian_Mixture_of_Factor_Analysers" class="color-inherit ga-similar-publication-title"><span class="publication-title">Variational Inference for Bayesian Mixture of Factor Analysers</span></a>  </h5>  <div class="authors"> <a href="researcher/8159937_Zoubin_Ghahramani" class="authors ga-similar-publication-author">Zoubin Ghahramani</a>, <a href="researcher/8159938_Matthew_J_Beal" class="authors ga-similar-publication-author">Matthew J. Beal</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw20_56ab9ef632cec"> <div> <h5> <a href="publication/256994163_Bayesian_mixtures_of_common_factor_analyzers_Model_variational_inference_and_applications" class="color-inherit ga-similar-publication-title"><span class="publication-title">Bayesian mixtures of common factor analyzers: Model, variational inference, and applications</span></a>  </h5>  <div class="authors"> <a href="researcher/2031225005_Xin_Wei" class="authors ga-similar-publication-author">Xin Wei</a>, <a href="researcher/43454484_Chunguang_Li" class="authors ga-similar-publication-author">Chunguang Li</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw31_56ab9ef632cec" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw32_56ab9ef632cec">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw33_56ab9ef632cec" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=mJF9k96uRFZenjjwmkmles7hBTO8KIeu77Ni1aFETFyPH8gbqSHI8FEB6Wjd2XaV" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="JpPJs8kiXOJepeTXr8R5BLHNT4dQpR5sBO5LHyIRsHJpeDxKtVYj75fHmdYM0XYUDc5KYgyUwEt/ZjikA1sjta/mZUjzXEG9VTTiBUaM4e2B1PkSxykL4p3rxCLI7aPi8MwsVAnxWFNMyppdecwK2KVB38VCJaiQyrZeeo2Jc7dN4t96jFS5xe1HwM0i8EEJjLbW2BvlMJHGVpI7ENGB7kUGn7LdDuIUvCrF6epniXrpfp73O0eTgycD3KeQo40M53avJAlHrW65kpZqfVfkM18aKqwT6Vg9KrraBRPwqlY="/> <input type="hidden" name="urlAfterLogin" value="publication/2239690_Variational_Inference_for_Bayesian_Mixtures_of_Factor_Analysers"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjIzOTY5MF9WYXJpYXRpb25hbF9JbmZlcmVuY2VfZm9yX0JheWVzaWFuX01peHR1cmVzX29mX0ZhY3Rvcl9BbmFseXNlcnM%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjIzOTY5MF9WYXJpYXRpb25hbF9JbmZlcmVuY2VfZm9yX0JheWVzaWFuX01peHR1cmVzX29mX0ZhY3Rvcl9BbmFseXNlcnM%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjIzOTY5MF9WYXJpYXRpb25hbF9JbmZlcmVuY2VfZm9yX0JheWVzaWFuX01peHR1cmVzX29mX0ZhY3Rvcl9BbmFseXNlcnM%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw34_56ab9ef632cec"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 403;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2198378204065/javascript/min/lib/error_logging.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"profileUrl":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2549355721578\/images\/template\/default\/profile\/profile_default_m.png","profileStats":[{"data":{"impactPoints":"374.75","widgetId":"rgw5_56ab9ef632cec"},"id":"rgw5_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorImpactPoints.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorImpactPoints.html?authorUid=8159937","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationCount":311,"widgetId":"rgw6_56ab9ef632cec"},"id":"rgw6_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorPublicationCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorPublicationCount.html?authorUid=8159937","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"followerCount":17,"widgetId":"rgw7_56ab9ef632cec"},"id":"rgw7_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorFollowerCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorFollowerCount.html?authorUid=8159937","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw4_56ab9ef632cec"},"id":"rgw4_56ab9ef632cec","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorBadge.html?authorUid=8159937","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw3_56ab9ef632cec"},"id":"rgw3_56ab9ef632cec","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=2239690","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":2239690,"title":"Variational Inference for Bayesian Mixtures of Factor Analysers","journalTitle":"Advances in neural information processing systems","journalDetailsTooltip":{"data":{"journalTitle":"Advances in neural information processing systems","journalAbbrev":"Adv Neural Inform Process Syst","publisher":"IEEE Conference on Neural Information Processing Systems--Natural and Synthetic, Massachusetts Institute of Technology Press","issn":"1049-5258","impactFactor":"0.00","fiveYearImpactFactor":"0.00","citedHalfLife":"0.00","immediacyIndex":"0.00","eigenFactor":"0.00","articleInfluence":"0.00","widgetId":"rgw9_56ab9ef632cec"},"id":"rgw9_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=1049-5258","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"09\/2000;","publicationDateRobot":"2000-09","article":"","journalTitle":"Advances in neural information processing systems","journalUrl":"journal\/1049-5258_Advances_in_neural_information_processing_systems"}},"source":{"sourceUrl":"http:\/\/citeseer.ist.psu.edu\/322672.html","sourceName":"CiteSeer"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Variational Inference for Bayesian Mixtures of Factor Analysers"},{"key":"rft.title","value":"Advances in Neural Information Processing Systems"},{"key":"rft.jtitle","value":"Advances in Neural Information Processing Systems"},{"key":"rft.date","value":"2000"},{"key":"rft.issn","value":"1049-5258"},{"key":"rft.au","value":"Zoubin Ghahramani,Matthew J. Beal"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw10_56ab9ef632cec"},"id":"rgw10_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=2239690","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":2239690,"peopleItems":[{"data":{"authorUrl":"researcher\/8159937_Zoubin_Ghahramani","authorNameOnPublication":"Zoubin Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Zoubin Ghahramani","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/8159937_Zoubin_Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw13_56ab9ef632cec"},"id":"rgw13_56ab9ef632cec","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=8159937&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw12_56ab9ef632cec"},"id":"rgw12_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=8159937&authorNameOnPublication=Zoubin%20Ghahramani","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/8159938_Matthew_J_Beal","authorNameOnPublication":"Matthew J. Beal","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Matthew J. Beal","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/8159938_Matthew_J_Beal","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw15_56ab9ef632cec"},"id":"rgw15_56ab9ef632cec","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=8159938&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw14_56ab9ef632cec"},"id":"rgw14_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=8159938&authorNameOnPublication=Matthew%20J.%20Beal","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw11_56ab9ef632cec"},"id":"rgw11_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=2239690&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":2239690,"abstract":"<noscript><\/noscript><div>We present an algorithm that infers the model structure of a mixture of factor analysers using an ecient and deterministic variational approximation to full Bayesian integration over model parameters. This procedure can automatically determine the optimal number of components and the local dimensionality of each component (i.e. the number of factors in each factor analyser). Alternatively it can be used to infer posterior distributions over number of components and dimensionalities. Since all parameters are integrated out the method is not prone to over tting. Using a stochastic procedure for adding components it is possible to perform the variational optimisation incrementally and to avoid local maxima. Results show that the method works very well in practice and correctly infers the number and dimensionality of nontrivial synthetic examples. By importance sampling from the variational approximation we show how to obtain unbiased estimates of the true evidence, the exa...<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw16_56ab9ef632cec"},"id":"rgw16_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=2239690","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/2239690_Variational_Inference_for_Bayesian_Mixtures_of_Factor_Analysers\/links\/0e5fa369f0c41c4932de6c0a\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw8_56ab9ef632cec"},"id":"rgw8_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=2239690&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":8738684,"url":"researcher\/8738684_D_M_Titterington","fullname":"D. M. Titterington","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Oct 2000","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/2816888_Stochastic_Model_Selection_for_Bayesian_Mixtures_of_Factor_Analysers","usePlainButton":true,"publicationUid":2816888,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/2816888_Stochastic_Model_Selection_for_Bayesian_Mixtures_of_Factor_Analysers","title":"Stochastic Model Selection for Bayesian Mixtures of Factor Analysers","displayTitleAsLink":true,"authors":[{"id":8738684,"url":"researcher\/8738684_D_M_Titterington","fullname":"D. M. Titterington","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/2816888_Stochastic_Model_Selection_for_Bayesian_Mixtures_of_Factor_Analysers","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/2816888_Stochastic_Model_Selection_for_Bayesian_Mixtures_of_Factor_Analysers\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab9ef632cec"},"id":"rgw18_56ab9ef632cec","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=2816888","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159938,"url":"researcher\/8159938_Matthew_J_Beal","fullname":"Matthew J. Beal","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jun 1999","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/2281698_Variational_Inference_for_Bayesian_Mixture_of_Factor_Analysers","usePlainButton":true,"publicationUid":2281698,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/2281698_Variational_Inference_for_Bayesian_Mixture_of_Factor_Analysers","title":"Variational Inference for Bayesian Mixture of Factor Analysers","displayTitleAsLink":true,"authors":[{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159938,"url":"researcher\/8159938_Matthew_J_Beal","fullname":"Matthew J. Beal","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/2281698_Variational_Inference_for_Bayesian_Mixture_of_Factor_Analysers","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/2281698_Variational_Inference_for_Bayesian_Mixture_of_Factor_Analysers\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab9ef632cec"},"id":"rgw19_56ab9ef632cec","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=2281698","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2031225005,"url":"researcher\/2031225005_Xin_Wei","fullname":"Xin Wei","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":43454484,"url":"researcher\/43454484_Chunguang_Li","fullname":"Chunguang Li","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Nov 2013","journal":"Signal Processing","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/256994163_Bayesian_mixtures_of_common_factor_analyzers_Model_variational_inference_and_applications","usePlainButton":true,"publicationUid":256994163,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.21","url":"publication\/256994163_Bayesian_mixtures_of_common_factor_analyzers_Model_variational_inference_and_applications","title":"Bayesian mixtures of common factor analyzers: Model, variational inference, and applications","displayTitleAsLink":true,"authors":[{"id":2031225005,"url":"researcher\/2031225005_Xin_Wei","fullname":"Xin Wei","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":43454484,"url":"researcher\/43454484_Chunguang_Li","fullname":"Chunguang Li","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Signal Processing 11\/2013; 93(11):2894\u20132905. DOI:10.1016\/j.sigpro.2013.04.007"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/256994163_Bayesian_mixtures_of_common_factor_analyzers_Model_variational_inference_and_applications","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/256994163_Bayesian_mixtures_of_common_factor_analyzers_Model_variational_inference_and_applications\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56ab9ef632cec"},"id":"rgw20_56ab9ef632cec","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=256994163","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw17_56ab9ef632cec"},"id":"rgw17_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=2239690&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":2239690,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":2239690,"publicationType":"article","linkId":"0e5fa369f0c41c4932de6c0a","fileName":"Variational Inference for Bayesian Mixtures of Factor Analysers","fileUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.65.4497&amp;rep=rep1&amp;type=pdf","name":"psu.edu","nameUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.65.4497&amp;rep=rep1&amp;type=pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw23_56ab9ef632cec"},"id":"rgw23_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=2239690&linkId=0e5fa369f0c41c4932de6c0a&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw22_56ab9ef632cec"},"id":"rgw22_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=2239690&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":0,"valueFormatted":"0","widgetId":"rgw24_56ab9ef632cec"},"id":"rgw24_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=2239690","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw21_56ab9ef632cec"},"id":"rgw21_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=2239690&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":2239690,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw26_56ab9ef632cec"},"id":"rgw26_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=2239690&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":0,"valueFormatted":"0","widgetId":"rgw27_56ab9ef632cec"},"id":"rgw27_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=2239690","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw25_56ab9ef632cec"},"id":"rgw25_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=2239690&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Variational Inference for Bayesian\nMixtures of Factor Analysers\nZoubin Ghahramani and Matthew J. Beal\nGatsby Computational Neuroscience Unit\nUniversity College London\n17 Queen Square, London WC1N 3AR, England\n{zoubin,m.beal}@gatsby.ucl.ac.uk http:\/\/www.gatsby.ucl.ac.uk\nAbstract\nWe present an algorithm that infers the model structure of a mix-\nture of factor analysers using an efficient and deterministic varia-\ntional approximation to full Bayesian integration over model pa-\nrameters. This procedure can automatically determine the opti-\nmal number of components and the local dimensionality of each\ncomponent (i.e. the number of factors in each factor analyser).\nAlternatively it can be used to infer posterior distributions over\nnumber of components and dimensionalities. Since all parameters\nare integrated out the method is not prone to overfitting. Using a\nstochastic procedure for adding components it is possible to per-\nform the variational optimisation incrementally and to avoid local\nmaxima. Results show that the method works very well in practice\nand correctly infers the number and dimensionality of nontrivial\nsynthetic examples.\nBy importance sampling from the variational approximation we\nshow how to obtain unbiased estimates of the true evidence, the\nexact predictive density, and the KL divergence between the varia-\ntional posterior and the true posterior, not only in this model but\nfor variational approximations in general.\n1Introduction\nFactor analysis (FA) is a method for modelling correlations in multidimensional\ndata. The model assumes that each p-dimensional data vector y was generated by\nfirst linearly transforming a k < p dimensional vector of unobserved independent\nzero-mean unit-variance Gaussian sources, x, and then adding a p-dimensional zero-\nmean Gaussian noise vector, n, with diagonal covariance matrix \u03a8: i.e. y = \u039bx+n.\nIntegrating out x and n, the marginal density of y is Gaussian with zero mean\nand covariance \u039b\u039bT+ \u03a8. The matrix \u039b is known as the factor loading matrix.\nGiven data with a sample covariance matrix \u03a3, factor analysis finds the \u039b and \u03a8\nthat optimally fit \u03a3 in the maximum likelihood sense. Since k < p, a single factor\nanalyser can be seen as a reduced parametrisation of a full-covariance Gaussian.1\n1Factor analysis and its relationship to principal components analysis (PCA) and mix-\nture models is reviewed in [10]."},{"page":2,"text":"A mixture of factor analysers (MFA) models the density for y as a weighted average\nof factor analyser densities\nP(y|\u039b,\u03a8,\u03c0) =\nS\n?\ns=1\nP(s|\u03c0)P(y|s,\u039bs,\u03a8),\n(1)\nwhere \u03c0 is the vector of mixing proportions, s is a discrete indicator variable, and\n\u039bsis the factor loading matrix for factor analyser s which includes a mean vector\nfor y.\nBy exploiting the factor analysis parameterisation of covariance matrices, a mix-\nture of factor analysers can be used to fit a mixture of Gaussians to correlated high\ndimensional data without requiring O(p2) parameters or undesirable compromises\nsuch as axis-aligned covariance matrices. In an MFA each Gaussian cluster has in-\ntrinsic dimensionality k (or ksif the dimensions are allowed to vary across clusters).\nConsequently, the mixture of factor analysers simultaneously addresses the prob-\nlems of clustering and local dimensionality reduction. When \u03a8 is a multiple of the\nidentity the model becomes a mixture of probabilistic PCAs. Tractable maximum\nlikelihood procedure for fitting MFA and MPCA models can be derived from the\nExpectation Maximisation algorithm [4, 11].\nThe maximum likelihood (ML) approach to MFA can easily get caught in local\nmaxima.2Ueda et al. [12] provide an effective deterministic procedure for avoiding\nlocal maxima by considering splitting a factor analyser in one part of space and\nmerging two in a another part. But splits and merges have to be considered simul-\ntaneously because the number of factor analysers has to stay the same since adding\na factor analyser is always expected to increase the training likelihood.\nA fundamental problem with maximum likelihood approaches is that they fail to\ntake into account model complexity (i.e. the cost of coding the model parameters).\nSo more complex models are not penalised, which leads to overfitting and the in-\nability to determine the best model size and structure (or distributions thereof)\nwithout resorting to costly cross-validation procedures. Bayesian approaches over-\ncome these problems by treating the parameters \u03b8 as unknown random variables\nand averaging over the ensemble of models they define:\n?\nP(Y ) is the evidence for a data set Y = {y1,... ,yN}. Integrating out parameters\npenalises models with more degrees of freedom since these models can a priori\nmodel a larger range of data sets. All information inferred from the data about the\nparameters is captured by the posterior distribution P(\u03b8|Y ) rather than the ML\npoint estimate\u02c6\u03b8.3\nWhile Bayesian theory deals with the problems of overfitting and model selec-\ntion\/averaging, in practice it is often computationally and analytically intractable to\nperform the required integrals. For Gaussian mixture models Markov chain Monte\nCarlo (MCMC) methods have been developed to approximate these integrals by\nsampling [8, 7]. The main criticism of MCMC methods is that they are slow and\nP(Y ) =\nd\u03b8 P(Y |\u03b8)P(\u03b8).\n(2)\n2Technically, the log likelihood is not bounded above if no constraints are put on the\ndeterminant of the component covariances. So the real ML objective for MFA is to find\nthe highest finite local maximum of the likelihood.\n3We sometimes use \u03b8 to refer to the parameters and sometimes to all the unknown\nquantities (parameters and hidden variables). Formally the only difference between the two\nis that the number of hidden variables grows with N, whereas the number of parameters\nusually does not."},{"page":3,"text":"it is usually difficult to assess convergence. Furthermore, the posterior density over\nparameters is stored as a set of samples, which can be inefficient.\nAnother approach to Bayesian integration for Gaussian mixtures [9] is the Laplace\napproximation which makes a local Gaussian approximation around a maximum a\nposteriori parameter estimate. These approximations are based on large data limits\nand can be poor, particularly for small data sets (for which, in principle, the advan-\ntages of Bayesian integration over ML are largest). Local Gaussian approximations\nare also poorly suited to bounded or positive parameters such as the mixing pro-\nportions of the mixture model. Finally, it is difficult to see how this approach can\nbe applied to online incremental changes to model structure.\nIn this paper we employ a third approach to Bayesian inference: variational ap-\nproximation. We form a lower bound on the log evidence using Jensen\u2019s inequality:\n?\nwhich we seek to maximise. Maximising F is equivalent to minimising the KL-\ndivergence between Q(\u03b8) and P(\u03b8|Y ), so a tractable Q can be used as an approx-\nimation to the intractable posterior. This approach draws its roots from one way\nof deriving mean field approximations in physics, and has been used recently for\nBayesian inference [13, 5, 1].\nThe variational method has several advantages over MCMC and Laplace approxi-\nmations. Unlike MCMC, convergence can be assessed easily by monitoring F. The\napproximate posterior is encoded efficiently in Q(\u03b8). Unlike Laplace approxima-\ntions, the form of Q can be tailored to each parameter (in fact the optimal form\nof Q for each parameter falls out of the optimisation), the approximation is global,\nand Q optimises an objective function. Variational methods are generally fast, F\nis guaranteed to increase monotonically and transparently incorporates model com-\nplexity. To our knowledge, no one has done a full Bayesian analysis of mixtures of\nfactor analysers.\nOf course, vis-a-vis MCMC, the main disadvantage of variational approximations\nis that they are not guaranteed to find the exact posterior in the limit. However,\nwith a straightforward application of sampling, it is possible to take the result of\nthe variational optimisation and use it to sample from the exact posterior and exact\npredictive density. This is described in section 5.\nIn the remainder of this paper we first describe the mixture of factor analysers in\nmore detail (section 2). We then derive the variational approximation (section 3).\nWe show empirically that the model can infer both the number of components and\ntheir intrinsic dimensionalities, and is not prone to overfitting (section 6). Finally,\nwe conclude in section 7.\nL \u2261 lnP(Y ) = ln\nd\u03b8 P(Y,\u03b8) \u2265\n?\nd\u03b8 Q(\u03b8)lnP(Y,\u03b8)\nQ(\u03b8)\n\u2261 F,\n(3)\n2The Model\nStarting from (1), the evidence for the Bayesian MFA is obtained by averaging the\nlikelihood under priors for the parameters (which have their own hyperparameters):\n?\nN\n?\nP(Y )=\nd\u03c0P(\u03c0|\u03b1)\n?\nsn=1\n?\nd\u03bdP(\u03bd|a,b)\n?\n?\nd\u039b P(\u039b|\u03bd) \u00b7\nn=1\nS\n?\nP(sn|\u03c0)\ndxnP(xn)P(yn|xn,sn,\u039bs,\u03a8)\n?\n.\n(4)"},{"page":4,"text":"Here {\u03b1,a,b,\u03a8} are hyperparameters4, \u03bd are precision parameters (i.e. inverse vari-\nances) for the columns of \u039b. The conditional independence relations between the\nvariables in this model are shown graphically in the usual belief network represen-\ntation in Figure 1.\nWhile arbitrary choices could be made for the\na,b\npriors on the first line of (4), choosing priors that\nare conjugate to the likelihood terms on the sec-\nond line of (4) greatly simplifies inference and\ninterpretability.5\nsymmetric Dirichlet, which is conjugate to the\nmultinomial P(s|\u03c0).\nThe prior for the factor loading matrix plays a\nkey role in this model. Each component of the\nmixture has a Gaussian prior P(\u039bs|\u03bds), where\neach element of the vector \u03bdsis the precision of\na column of \u039b. If one of these precisions \u03bds\nthen the outgoing weights for factor xlwill go to\nzero, which allows the model to reduce the in-\ntrinsic dimensionality of x if the data does not\nwarrant this added dimension. This method of\nintrinsic dimensionality reduction has been used\nby Bishop [2] for Bayesian PCA, and is closely\nrelated to MacKay and Neal\u2019s method for auto-\nmatic relevance determination (ARD) for inputs\nto a neural network [6].\nTo avoid overfitting it is important to integrate out all parameters whose cardinality\nscales with model complexity (i.e. number of components and their dimensionali-\nties). We therefore also integrate out the precisions using Gamma priors, P(\u03bd|a,b).\n\u03bd1?\n\u039b1\n\u03b1\n?\n\u03a8\n...?\n\u03bd2\n\u03bdS\n\u039b2\n\u039bS\n...?\nyn\nxn\nsn\n\u03c0\nn=1...N\nFigure 1:\nvariational Bayesian mixture of\nfactor analysers.\nrandom variables, solid rectangles\ndenote hyperparameters, and the\ndashed rectangle shows the plate\n(i.e. repetitions) over the data.\nGenerative model for\nCircles denote\nSo we choose P(\u03c0|\u03b1) to be\nl\u2192 \u221e,\n3 The Variational Approximation\nApplying Jensen\u2019s inequality repeatedly to the log evidence (4) we lower bound it\nusing the following factorisation of the distribution of parameters and hidden vari-\nables: Q(\u039b)Q(\u03c0,\u03bd)Q(s,x). Given this factorisation several additional factorisations\nfall out of the conditional independencies in the model resulting in the variational\nobjective function:\n?\ns=1\nN\n?\n+\nd\u039bsQ(\u039bs)\nF=\nd\u03c0Q(\u03c0)lnP(\u03c0|\u03b1)\nQ(\u03c0)\n+\nS\n?\nd\u03c0 Q(\u03c0)lnP(sn|\u03c0)\n?\nd\u03bdsQ(\u03bds)\n?\nlnP(\u03bds|a,b)\nQ(\u03bds)\n?\n+\n?\nd\u039bsQ(\u039bs)lnP(\u039bs|\u03bds)\nQ(\u039bs)\n?\n+\nn=1\nS\n?\nsn=1\nQ(sn)\n??\nQ(sn)\n?\n+\ndxnQ(xn|sn)ln\nP(xn)\nQ(xn|sn)\n?\ndxnQ(xn|sn)lnP(yn|xn,sn,\u039bs,\u03a8)\n?\n(5)\nThe variational posteriors Q(\u00b7), as given in the Appendix, are derived by performing\na free-form extremisation of F w.r.t. Q. It is not difficult to show that these extrema\nare indeed maxima of F. The optimal posteriors Q are of the same conjugate forms\nas the priors. The model hyperparameters which govern the priors can be estimated\nin the same fashion (see the Appendix).\n4We currently do not integrate out \u03a8, although this can also be done.\n5Conjugate priors have the same effect as pseudo-observations."},{"page":5,"text":"4\nWhen optimising F, occasionally one finds that for some s:?\nthe local data to overcome the dimensional complexity prior on the factor loading\nmatrices. So components of the mixture die of natural causes when they are no\nlonger needed. Removing these redundant components increases F.\nComponent birth does not happen spontaneously, so we introduce a heuristic.\nWhenever F has stabilised we pick a parent-component stochastically with prob-\nability proportional to e\u2212\u03b2Fsand attempt to split it into two; Fsis the s-specific\ncontribution to F with the last bracketed term in (5) normalised by?\ndom as it concentrates attempted births on components that are faring poorly. The\nparameter distributions of the two Gaussians created from the split are initialised\nby partitioning the responsibilities for the data, Q(sn), along a direction sampled\nfrom the parent\u2019s distribution. This usually causes F to decrease, so by monitoring\nthe future progress of F we can reject this attempted birth if F does not recover.\nAlthough it is perfectly possible to start the model with many components and let\nthem die, it is computationally more efficient to start with one component and allow\nit to spawn more when necessary.\nBirth and Death\nnQ(sn) = 0. These\nzero responsibility components are the result of there being insufficient support from\nnQ(sn).\nThis works better than both cycling through components and picking them at ran-\n5 Exact Predictive Density, True Evidence, and KL\nBy importance sampling from the variational approximation we can obtain unbiased\nestimates of three important quantities: the exact predictive density, the true log\nevidence L, and the KL divergence between the variational posterior and the true\nposterior. Letting \u03b8 = {\u039b,\u03c0}, we sample \u03b8i\u223c Q(\u03b8). Each such sample is an instance\nof a mixture of factor analysers with predictive density given by (1). We weight\nthese predictive densities by the importance weights wi = P(\u03b8i,Y )\/Q(\u03b8i), which\nare easy to evaluate. This results in a mixture of mixtures of factor analysers, and\nwill converge to the exact predictive density, P(y|Y ), as long as Q(\u03b8) > 0 wherever\nP(\u03b8|Y ) > 0. The true log evidence can be similarly estimated by L = ln?w?, where\n?\u00b7? denotes averaging over the importance samples. Finally, the KL divergence is\ngiven by: KL(Q(\u03b8)?P(\u03b8|Y )) = ln?w? \u2212 ?lnw?.\nThis procedure has three significant properties. First, the same importance weights\ncan be used to estimate all three quantities. Second, while importance sampling\ncan work very poorly in high dimensions for ad hoc proposal distributions, here the\nvariational optimisation is used in a principled manner to pick Q to be a good ap-\nproximation to P and therefore hopefully a good proposal distribution. Third, this\nprocedure can be applied to any variational approximation. A detailed exposition\ncan be found in [3].\n6\nExperiment 1: Discovering the number of components.\nmodel on synthetic data generated from a mixture of 18 Gaussians with 50 points\nper cluster (Figure 2, top left). The variational algorithm has little difficulty finding\nthe correct number of components and the birth heuristics are successful at avoiding\nlocal maxima. After finding the 18 Gaussians repeated splits are attempted and\nrejected. Finding a distribution over number of components using F is also simple.\nExperiment 2: The shrinking spiral.\nfrom a shrinking spiral from [12] as another test of how well the algorithm could\nResults\nWe tested the\nWe used the dataset of 800 data points"},{"page":6,"text":"Figure 2: (top) Exp 1: The frames from left to right are the data, and the 2 S.D. Gaussian\nellipses after 7, 14, 16 and 22 accepted births. (bottom) Exp 2: Shrinking spiral data\nand 1 S.D. Gaussian ellipses after 6, 9, 12, and 17 accepted births. Note that the number\nof Gaussians increases from left to right.\n0 50010001500 2000\n\u22127800\n\u22127600\n\u22127400\n\u22127200\n\u22127000\n\u22126800\n\u22126600\n\u22126400\n?\n                   ????\n?\n???\n???\n???\n???\n???\n???\n1?7? 4? 3?2? 2?\n    8??\n    8??\n   16??\n   32??\n   64??\n  128??\n ?\n    1???\n1??\n1?\n1?\n1?\n       2???     1?\n  2?\n      4???\n3?\n4?\n4?\n2?\n2?\n2?\n2\n6?\n7?\n7?\n3?\n3?\n3?\n2?\n2?\n2?\nintrinsic dimensionalities\n  number?\n of points?\nper cluster\nFigure 3: (left) Exp 2: F as function of iteration for the spiral problem on a typical run.\nDrops in F constitute component births. Thick lines are accepted attempts, thin lines are\nrejected attempts. (middle) Exp 3: Means of the factor loading matrices. These results\nare analogous to those given by Bishop [2] for Bayesian PCA. (right) Exp 3: Table with\nlearned number of Gaussians and dimensionalities as training set size increases. Boxes\nrepresent model components that capture several of the clusters.\nescape local maxima and how robust it was to initial conditions (Figure 2, bottom).\nAgain local maxima did not pose a problem and the algorithm always found between\n12-14 Gaussians regardless of whether it was initialised with 0 or 200. These runs\ntook about 3-4 minutes on a 500MHz Alpha EV6 processor. A plot of F shows that\nmost of the compute time is spent on accepted moves (Figure 3, left).\nExperiment 3: Discovering the local dimensionalities.\nthetic data set of 300 data points in each of 6 Gaussians with intrinsic dimension-\nalities (7 4 3 2 2 1) embedded in 10 dimensions. The variational Bayesian approach\ncorrectly inferred both the number of Gaussians and their intrinsic dimensionalities\n(Figure 3, middle). We varied the number of data points and found that as expected\nwith fewer points the data could not provide evidence for as many components and\nintrinsic dimensions (Figure 3, right).\nWe generated a syn-\n7\nSearch over model structures for MFAs is computationally intractable if each factor\nanalyser is allowed to have different intrinsic dimensionalities. In this paper we have\nshown that the variational Bayesian approach can be used to efficiently infer this\nmodel structure while avoiding overfitting and other deficiencies of ML approaches.\nOne attraction of our variational method, which can be exploited in other models,\nis that once a factorisation of Q is assumed all inference is automatic and exact.\nWe can also use F to get a distribution over structures if desired. Finally we derive\nDiscussion"},{"page":7,"text":"a generally applicable importance sampler that gives us unbiased estimates of the\ntrue evidence, the exact predictive density, and the KL divergence between the\nvariational posterior and the true posterior.\nEncouraged by the results on synthetic data, we have applied the Bayesian mixture\nof factor analysers to a real-world unsupervised digit classification problem. We\nwill report the results of these experiments in a separate article.\nAppendix: Optimal Q Distributions and Hyperparameters\nQ(xn|sn) \u223c N(xn,s,\u03a3s)\nlnQ(sn) = [\u03c8(\u03c9us) \u2212 \u03c8(\u03c9)] +1\n\u22121yn, \u039b\nq=\u03a8\nn=1\nN\n?\nwhere {N,G,D} denote Normal, Gamma and Dirichlet distributions respectively, ?\u00b7? de-\nnotes expectation under the variational posterior, and \u03c8(x) is the digamma function\n\u03c8(x) \u2261\nvariance structure; even though each \u039bsis a p \u00d7 q matrix, its covariance only has O(pq2)\nparameters. Differentiating F with respect to the parameters, a and b, of the precision prior\nwe get fixed point equations \u03c8(a) = ?ln\u03bd? + lnb and b = a\/?\u03bd?. Similarly the fixed point\nfor the parameters of the Dirichlet prior is \u03c8(\u03b1) \u2212 \u03c8(\u03b1\/S) +?[\u03c8(\u03c9us) \u2212 \u03c8(\u03c9)]\/S = 0.\nReferences\nQ(\u039bs\nq) \u223c N(\u039b\ns\nq,\u03a3q,s)Q(\u03bds\nl) \u223c G(as\nl,bs\nl)Q(\u03c0) \u223c D(\u03c9u)\n2ln|\u03a3s| + ?lnP(yn|xn,sn,\u039bs,\u03a8)? + c\n?\nq\nQ(sn)?xnxn?? +diag?\u03bds?, \u03c9us =\u03b1\nxn,s= \u03a3s\u039b\ns?\u03a8\ns\n?\n\u22121\nN\n?\nQ(sn)ynxn,s?\u03a3q,s\n, as\nl=a +p\n2, bs\nl=b +1\n2\np\n?\nN\n?\nq=1\n?\u039bs\nql\n2?\n\u03a3s\u22121= ?\u039bs?\u03a8\n\u22121\u039bs? + I, \u03a3q,s\u22121=\u03a8\n\u22121\nqq\nn=1\nS+\nn=1\nQ(sn)\n\u2202\n\u2202xln\u0393(x). Note that the optimal distributions Q(\u039bs) have block diagonal co-\n[1] H. Attias. Inferring parameters and structure of latent variable models by variational\nBayes. In Proc. 15th Conf. on Uncertainty in Artificial Intelligence, 1999.\n[2] C.M. Bishop. Variational PCA. In Proc. Ninth Int. Conf. on Artificial Neural Net-\nworks. ICANN, 1999.\n[3] Z. Ghahramani, H. Attias, and M.J. Beal.\nReport GCNU-TR-1999-006, (in prep.) Gatsby Unit, Univ. College London, 1999.\n[4] Z. Ghahramani and G.E. Hinton.\ntor analyzers.Technical Report CRG-TR-96-1 [http:\/\/www.gatsby.ucl.ac.uk\/\n\u223czoubin\/papers\/tr-96-1.ps.gz], Dept. of Comp. Sci., Univ. of Toronto, 1996.\n[5] D.J.C. MacKay. Ensemble learning for hidden Markov models. Technical report,\nCavendish Laboratory, University of Cambridge, 1997.\n[6] R.M. Neal.Assessing relevance determination methods using DELVE. In C.M. Bishop,\neditor, Neural Networks and Machine Learning, pages 97\u2013129. Springer-Verlag, 1998.\n[7] C.E. Rasmussen. The infinite gaussian mixture model. In Adv. Neur. Inf. Proc. Sys.\n12, Cambridge, MA, 2000. MIT Press.\n[8] S. Richardson and P.J. Green. On Bayesian analysis of mixtures with an unknown\nnumber of components. Journal Royal Stat. Society, Series B, 59(4):731\u2013758, 1997.\n[9] S.J. Roberts, D. Husmeier, I. Rezek, and W. Penny. Bayesian approaches to Gaussian\nmixture modeling. IEEE PAMI, 20(11):1133\u20131142, 1998.\n[10] S. T. Roweis and Z. Ghahramani. A unifying review of linear Gaussian models. Neural\nComputation, 11(2):305\u2013345, 1999.\n[11] M.E. Tipping and C.M. Bishop. Mixtures of probabilistic principal component ana-\nlyzers. Neural Computation, 11(2):443\u2013482, 1999.\n[12] N. Ueda, R. Nakano, Z. Ghahramani, and G.E. Hinton. SMEM algorithm for mixture\nmodels. In Adv. Neur. Inf. Proc. Sys. 11, Cambridge, MA, 1999. MIT Press.\n[13] S. Waterhouse, D.J.C. Mackay, and T. Robinson. Bayesian methods for mixtures of\nexperts. In Adv. Neur. Inf. Proc. Sys. 7, Cambridge, MA, 1995. MIT Press.\nLearning model structure.Technical\nThe EM algorithm for mixtures of fac-"}],"widgetId":"rgw28_56ab9ef632cec"},"id":"rgw28_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=2239690&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw29_56ab9ef632cec"},"id":"rgw29_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=2239690&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":2239690,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/2239690_Variational_Inference_for_Bayesian_Mixtures_of_Factor_Analysers","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab9ef632cec"},"id":"rgw2_56ab9ef632cec","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":2239690},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=2239690&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab9ef632cec"},"id":"rgw1_56ab9ef632cec","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"plblw7X9hfUfXqsSqrMqfahp46AuIOPLOm6vRgkb4uHicfB8y1bGBlVatPsXC0Zd2wr+qB0q0aJy0PW5lS3dtAGW\/slr9zfOnHau9Be5CAJewDBQizrwuG6sfld\/Aa5BRL1fANkYOtTNAsBle5d63s0YMwo4RLKast9y7W3CmcFoEFFnwfrUbKBcGF\/Dvlm0Ln1GclUubpxYr75WXDVab9BLOW+IxhMNNbPKrXPY7V7Z+CE5vHugW9VU7kdVk25DmvDKlGZFEFWAGHBEEOmpKFI7UjXU7aB67QPAQsiECGM=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/2239690_Variational_Inference_for_Bayesian_Mixtures_of_Factor_Analysers\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Variational Inference for Bayesian Mixtures of Factor Analysers\" \/>\n<meta property=\"og:description\" content=\"We present an algorithm that infers the model structure of a mixture of factor analysers using an ecient and deterministic variational approximation to full Bayesian integration over model...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/2239690_Variational_Inference_for_Bayesian_Mixtures_of_Factor_Analysers\/links\/0e5fa369f0c41c4932de6c0a\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/2239690_Variational_Inference_for_Bayesian_Mixtures_of_Factor_Analysers\" \/>\n<meta property=\"rg:id\" content=\"PB:2239690\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Variational Inference for Bayesian Mixtures of Factor Analysers\" \/>\n<meta name=\"citation_author\" content=\"Zoubin Ghahramani\" \/>\n<meta name=\"citation_author\" content=\"Matthew J. Beal\" \/>\n<meta name=\"citation_publication_date\" content=\"2000\/09\/03\" \/>\n<meta name=\"citation_journal_title\" content=\"Advances in neural information processing systems\" \/>\n<meta name=\"citation_issn\" content=\"1049-5258\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/2239690_Variational_Inference_for_Bayesian_Mixtures_of_Factor_Analysers\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/2239690_Variational_Inference_for_Bayesian_Mixtures_of_Factor_Analysers\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-83cf3be7-dff2-4f12-9485-df2e8122a606","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":388,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw30_56ab9ef632cec"},"id":"rgw30_56ab9ef632cec","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-83cf3be7-dff2-4f12-9485-df2e8122a606", "302c072b73c59cc3dfb1b4bbad9a0b4133939e2d");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-83cf3be7-dff2-4f12-9485-df2e8122a606", "302c072b73c59cc3dfb1b4bbad9a0b4133939e2d");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw31_56ab9ef632cec"},"id":"rgw31_56ab9ef632cec","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/2239690_Variational_Inference_for_Bayesian_Mixtures_of_Factor_Analysers","requestToken":"JpPJs8kiXOJepeTXr8R5BLHNT4dQpR5sBO5LHyIRsHJpeDxKtVYj75fHmdYM0XYUDc5KYgyUwEt\/ZjikA1sjta\/mZUjzXEG9VTTiBUaM4e2B1PkSxykL4p3rxCLI7aPi8MwsVAnxWFNMyppdecwK2KVB38VCJaiQyrZeeo2Jc7dN4t96jFS5xe1HwM0i8EEJjLbW2BvlMJHGVpI7ENGB7kUGn7LdDuIUvCrF6epniXrpfp73O0eTgycD3KeQo40M53avJAlHrW65kpZqfVfkM18aKqwT6Vg9KrraBRPwqlY=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=mJF9k96uRFZenjjwmkmles7hBTO8KIeu77Ni1aFETFyPH8gbqSHI8FEB6Wjd2XaV","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjIzOTY5MF9WYXJpYXRpb25hbF9JbmZlcmVuY2VfZm9yX0JheWVzaWFuX01peHR1cmVzX29mX0ZhY3Rvcl9BbmFseXNlcnM%3D","signupCallToAction":"Join for free","widgetId":"rgw33_56ab9ef632cec"},"id":"rgw33_56ab9ef632cec","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw32_56ab9ef632cec"},"id":"rgw32_56ab9ef632cec","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw34_56ab9ef632cec"},"id":"rgw34_56ab9ef632cec","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
