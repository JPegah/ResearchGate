<!DOCTYPE html> <html lang="en" class="" id="rgw44_56ab1df2a6baa"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="fzyIvuefiFHScPkYvswAQant09ZmvlXlOcmeeFLH+b+YViN5iqWas0O8CMXxn4UzrY2jIuVFCbPypCw/GS+iZNuILfa+3sTc4XPajW9ytW7yk9WpvHHfzscM2h7XkFiiNUz12Yl4/dwLJ2IrLdh1+oGLqNTsTV+aWzvcfBjargVbKl3XNsN1OS7uE+75fZ/4jbiHfujJgMg9dIRB+ErO2vrTtissGEfrle9ay9t9DCxa/HF3wbUK4T3ZEfLZ+7LvsMAbxxwk8/qAdL4YsfBGLrCva4UEKPLfCwSDh69A48g="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-88ac71cd-640f-45a2-9764-897fa35ab159",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Hilbert Space Methods for Reduced-Rank Gaussian Process Regression" />
<meta property="og:description" content="This paper proposes a novel scheme for reduced-rank Gaussian process
regression. The method is based on an approximate series expansion of the
covariance function in terms of an eigenfunction..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression/links/54a3dc790cf256bf8bb17c1c/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression" />
<meta property="rg:id" content="PB:259844876" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Hilbert Space Methods for Reduced-Rank Gaussian Process Regression" />
<meta name="citation_author" content="Arno Solin" />
<meta name="citation_author" content="Simo Särkkä" />
<meta name="citation_publication_date" content="2014/01/21" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Simo_Saerkkae/publication/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression/links/54a3dc790cf256bf8bb17c1c.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Hilbert Space Methods for Reduced-Rank Gaussian Process Regression (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Hilbert Space Methods for Reduced-Rank Gaussian Process Regression on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1df2a6baa" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1df2a6baa" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab1df2a6baa">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Hilbert%20Space%20Methods%20for%20Reduced-Rank%20Gaussian%20Process%20Regression&rft.date=2014&rft.au=Arno%20Solin%2CSimo%20S%C3%A4rkk%C3%A4&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Hilbert Space Methods for Reduced-Rank Gaussian Process Regression</h1> <meta itemprop="headline" content="Hilbert Space Methods for Reduced-Rank Gaussian Process Regression">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression/links/54a3dc790cf256bf8bb17c1c/smallpreview.png">  <div id="rgw7_56ab1df2a6baa" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab1df2a6baa" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Arno_Solin" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272475355349006%401441974595657_m/Arno_Solin.png" title="Arno Solin" alt="Arno Solin" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Arno Solin</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw9_56ab1df2a6baa" data-account-key="Arno_Solin">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Arno_Solin"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272475355349006%401441974595657_l/Arno_Solin.png" title="Arno Solin" alt="Arno Solin" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Arno_Solin" class="display-name">Arno Solin</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Aalto_University" title="Aalto University">Aalto University</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56ab1df2a6baa" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Simo_Saerkkae" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A277578301624325%401443191232558_m/Simo_Saerkkae.png" title="Simo Särkkä" alt="Simo Särkkä" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Simo Särkkä</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw11_56ab1df2a6baa" data-account-key="Simo_Saerkkae">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Simo_Saerkkae"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A277578301624325%401443191232558_l/Simo_Saerkkae.png" title="Simo Särkkä" alt="Simo Särkkä" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Simo_Saerkkae" class="display-name">Simo Särkkä</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Aalto_University" title="Aalto University">Aalto University</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">        <meta itemprop="datePublished" content="2014-01">  01/2014;               <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1401.5508" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw12_56ab1df2a6baa" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>This paper proposes a novel scheme for reduced-rank Gaussian process<br />
regression. The method is based on an approximate series expansion of the<br />
covariance function in terms of an eigenfunction expansion of the Laplace<br />
operator in a compact subset of $\mathbb{R}^d$. On this approximate eigenbasis<br />
the eigenvalues of the covariance function can be expressed as simple functions<br />
of the spectral density of the Gaussian process, which allows the GP inference<br />
to be solved under a computational cost scaling as $\mathcal{O}(nm^2)$<br />
(initial) and $\mathcal{O}(m^3)$ (hyperparameter learning) with $m$ basis<br />
functions and $n$ data points. The approach also allows for rigorous error<br />
analysis with Hilbert space theory, and we show that the approximation becomes<br />
exact when the size of the compact subset and the number of eigenfunctions go<br />
to infinity. The expansion generalizes to Hilbert spaces with an inner product<br />
which is defined as an integral over a specified input density. The method is<br />
compared to previously proposed methods theoretically and through empirical<br />
tests with simulated and real data.</div> </p>  </div>  </div>   </div>      <div class="action-container"> <div id="rgw13_56ab1df2a6baa" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw27_56ab1df2a6baa">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw36_56ab1df2a6baa">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Simo_Saerkkae/publication/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression/links/54a3dc790cf256bf8bb17c1c.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Simo_Saerkkae">Simo Särkkä</a>, <span class="js-publication-date"> Dec 31, 2014 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw38_56ab1df2a6baa" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw39_56ab1df2a6baa" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw40_56ab1df2a6baa" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw41_56ab1df2a6baa" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw42_56ab1df2a6baa" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw43_56ab1df2a6baa" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw37_56ab1df2a6baa" src="https://www.researchgate.net/c/o1o9o3/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FSimo_Saerkkae%2Fpublication%2F259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression%2Flinks%2F54a3dc790cf256bf8bb17c1c.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw26_56ab1df2a6baa"  itemprop="articleBody">  <p>Page 1</p> <p>Hilbert Space Methods for Reduced-Rank<br />Gaussian Process Regression<br />Arno Solin<br />Simo S¨ arkk¨ a<br />Department of Biomedical Engineering and Computational Science (BECS)<br />Aalto University, School of Science<br />P.O. Box 12200, FI-00076 Aalto, Finland<br />arno.solin@aalto.fi<br />simo.sarkka@aalto.fi<br />Abstract<br />This paper proposes a novel scheme for reduced-rank Gaussian process regression. The<br />method is based on an approximate series expansion of the covariance function in terms of<br />an eigenfunction expansion of the Laplace operator in a compact subset of Rd. On this ap-<br />proximate eigenbasis the eigenvalues of the covariance function can be expressed as simple<br />functions of the spectral density of the Gaussian process, which allows the GP inference<br />to be solved under a computational cost scaling as O(nm2) (initial) and O(m3) (hyper-<br />parameter learning) with m basis functions and n data points. The approach also allows<br />for rigorous error analysis with Hilbert space theory, and we show that the approximation<br />becomes exact when the size of the compact subset and the number of eigenfunctions go<br />to infinity. The expansion generalizes to Hilbert spaces with an inner product which is<br />defined as an integral over a specified input density. The method is compared to previously<br />proposed methods theoretically and through empirical tests with simulated and real data.<br />Keywords: Gaussian process regression, Laplace operator, eigenfunction expansion,<br />pseudo-differential operator, reduced-rank approximation<br />1. Introduction<br />Gaussian processes (GPs, Rasmussen and Williams, 2006) are powerful tools for non-<br />parametric Bayesian inference and learning. In GP regression the model functions f(x)<br />are assumed to be realizations from a Gaussian random process prior with a given covari-<br />ance function k(x,x?), and learning amounts to solving the posterior process given a set of<br />noisy measurements y1,y2,...,ynat some given test inputs. This model is often written in<br />the form<br />f ∼ GP(0,k(x,x?)),<br />yi= f(xi) + εi,<br />(1)<br />where εi∼ N(0,σ2<br />learning is the computational and memory requirements that scale as O(n3) and O(n2) in<br />a direct implementation. This limits the applicability of GPs when the number of training<br />samples n grows large. The computational requirements arise because in solving the GP<br />regression problem we need to invert the n×n Gram matrix K+σ2<br />which is an O(n3) operation in general.<br />n), for i = 1,2,...,n. One of the main limitations of GPs in machine<br />nI, where Kij= k(xi,xj),<br />c ?2014 Arno Solin and Simo S¨ arkk¨ a.<br />arXiv:1401.5508v1  [stat.ML]  21 Jan 2014</p>  <p>Page 2</p> <p>Solin and S¨ arkk¨ a<br />To overcome this problem, over the years, several schemes have been proposed. They<br />typically reduce the storage requirements to O(nm) and complexity to O(nm2), where<br />m &lt; n. Some early methods have been reviewed in Rasmussen and Williams (2006), and<br />Qui˜ nonero-Candela and Rasmussen (2005a) provide a unifying view on several methods.<br />From a spectral point of view, several of these methods (e.g., SOR, DTC, VAR, FIC) can<br />be interpreted as modifications to the so-called Nystr¨ om method (see Baker, 1977; Williams<br />and Seeger, 2001), a scheme for approximating the eigenspectrum.<br />For stationary covariance functions the spectral density of the covariance function can<br />be employed: in this context the spectral approach has mainly been considered in regular<br />grids, as this allows for the use of FFT-based methods for fast solutions (see Paciorek, 2007;<br />Fritz et al., 2009), and more recently in terms of converting GPs to state space models<br />(S¨ arkk¨ a and Hartikainen, 2012; S¨ arkk¨ a et al., 2013). Recently, L´ azaro-Gredilla et al. (2010)<br />proposed a sparse spectrum method where a randomly chosen set of spectral points span a<br />trigonometric basis for the problem.<br />The methods proposed in this article fall into the class of methods called reduced-rank<br />approximations (see, e.g., Rasmussen and Williams, 2006) which are based on approximat-<br />ing the Gram matrix K with a matrix˜K with a smaller rank m &lt; n. This allows for the<br />use of matrix inversion lemma (Woodbury formula) to speed up the computations. It is<br />well-known that the optimal reduced-rank approximation of the Gram (covariance) matrix<br />K with respect to the Frobenius norm is˜K = ΦΛΦT, where Λ is a diagonal matrix of the<br />leading m eigenvalues of K and Φ is the matrix of the corresponding orthonormal eigen-<br />vectors (Golub and Van Loan, 1996; Rasmussen and Williams, 2006). Yet, as computing<br />the eigendecomposition is an O(n3) operation, this provides no remedy as such.<br />In this work we propose a novel method for obtaining approximate eigendecompositions<br />of covariance functions in terms of an eigenfunction expansion of the Laplace operator in<br />a compact subset of Rd. The method is based on interpreting the covariance function<br />as the kernel of a pseudo-differential operator (Shubin, 1987) and approximating it using<br />Hilbert space methods (Courant and Hilbert, 2008; Showalter, 2010). This results in a<br />reduced-rank approximation for the covariance function. This path has not been explored<br />in GP regression context before, although the approach is closely related to the stochastic<br />partial differential equation based methods recently introduced to spatial statistics and GP<br />regression (Lindgren et al., 2011; S¨ arkk¨ a and Hartikainen, 2012; S¨ arkk¨ a et al., 2013). We also<br />show how the solution formally converges to the exact solution in well-defined conditions,<br />and provide theoretical and experimental comparisons to existing state-of-the-art methods.<br />This paper is structured as follows: In Section 2 we derive the approximative series<br />expansion of the covariance functions. Section 3 is dedicated to applying the approximation<br />scheme to GP regression and providing details of the computational benefits. We provide<br />a detailed analysis of the convergence of the method in Section 4. Section 5 and 6 provide<br />comparisons to existing methods, the former from a more theoretical point of view, whereas<br />the latter contains examples and comparative evaluation on several datasets. Finally the<br />properties of the method are summarized and discussed in Section 7.<br />2</p>  <p>Page 3</p> <p>Hilbert Space Methods for Reduced-Rank Gaussian Process Regression<br />2. Approximating the Covariance Function<br />In this section, we start by stating the assumptions and properties of the class of covariance<br />functions that we are considering, and show how a homogenous covariance function can<br />be considered as a pseudo-differential operator constructed as a series of Laplace operators.<br />Then we show how the pseudo-differential operators can be approximated with Hilbert space<br />methods on compact subsets of Rdor via inner products with integrable weight functions,<br />and discuss connections to Sturm–Liouville theory.<br />2.1 Spectral Densities of Homogeneous and Isotropic Gaussian Processes<br />In this work it is assumed that the covariance function is homogeneous (stationary), which<br />means that the covariance function k(x,x?) is actually a function of r = x − x?only. This<br />means that the covariance structure of the model function f(x) is the same regardless of<br />the absolute position in the input space (cf. Rasmussen and Williams, 2006). In this case<br />the covariance function can be equivalently represented in terms of the spectral density.<br />This results from the Bochner’s theorem (see, e.g., Akhiezer and Glazman, 1993; Da Prato<br />and Zabczyk, 1992) which states that an arbitrary positive definite function k(r) can be<br />represented as<br />1<br />(2π)d<br />where µ is a positive measure.<br />If the measure µ(ω) has a density, it is called the spectral density S(ω) corresponding to<br />the covariance function k(r). This gives rise to the Fourier duality of covariance and spectral<br />density, which is known as the Wiener–Khintchin theorem (Rasmussen and Williams, 2006),<br />giving the identities<br />?<br />From these identities it is easy to see that if the covariance function is isotropic, that is, it<br />only depends on the Euclidean norm ?r? such that k(r) ? k(?r?), then the spectral density<br />will also only depend on the norm of ω such that we can write S(ω) ? S(?ω?). In the<br />following we assume that the considered covariance functions are indeed isotropic, but the<br />approach can be generalized to more general homogenous covariance functions.<br />k(r) =<br />?<br />exp<br />?<br />iωTr<br />?<br />µ(dω),(2)<br />k(r) =<br />1<br />(2π)d<br />S(ω)eiωTrdωandS(ω) =<br />?<br />k(r)e−iωTrdr.<br />(3)<br />2.2 The Covariance Operator As a Pseudo-Differential Operator<br />Associated to each covariance function k(x,x?) we can also define a covariance operator K<br />as follows:<br />Kφ =<br />As we show in the next section, this interpretation allows us to approximate the covariance<br />operator using Hilbert space methods which are typically used for approximating differential<br />and pseudo-differential operators in the context of partial differential equations (Showalter,<br />2010). When the covariance function is homogenous, the corresponding operator will be<br />translation invariant thus allowing for Fourier-representation as a transfer function. This<br />transfer function is just the spectral density of the Gaussian process.<br />?<br />k(·,x?)φ(x?)dx?. (4)<br />3</p>  <p>Page 4</p> <p>Solin and S¨ arkk¨ a<br />Consider an isotropic covariance function k(x,x?) ? k(?r?) (recall that ?·? denotes<br />the Euclidean norm). The spectral density of the Gaussian process and thus the transfer<br />function corresponding to the covariance operator will now have the form S(?ω?). We can<br />formally write it as a function of ?ω?2such that<br />S(?ω?) = ψ(?ω?2).<br />Assume that the spectral density S(·) and hence ψ(·) are regular enough so that the spectral<br />density has the following polynomial expansion:<br />(5)<br />ψ(?ω?2) = a0+ a1?ω?2+ a2(?ω?2)2+ a3(?ω?2)3+ ··· .<br />Thus we also have<br />(6)<br />S(?ω?) = a0+ a1?ω?2+ a2(?ω?2)2+ a3(?ω?2)3+ ··· .<br />Recall that the transfer function corresponding to the Laplacian operator ∇2is −?ω?2in<br />the sense that<br />F[∇2f](ω) = −?ω?2F[f](ω),<br />where F[·] denotes the Fourier transform of its argument. If we take the Fourier transform<br />of (7), we get the following representation for the covariance operator K, which defines a<br />pseudo-differential operator (Shubin, 1987) as a formal series of Laplacian operators:<br />(7)<br />(8)<br />K = a0+ a1(−∇2) + a2(−∇2)2+ a3(−∇2)3+ ··· .<br />In the next section we will use this representation to form a series expansion approximation<br />for the covariance function.<br />(9)<br />2.3 Hilbert-Space Approximation of the Covariance Operator<br />We will now form a Hilbert-space approximation for the pseudo-differential operator defined<br />by (9). Let Ω ⊂ Rdbe a compact set, and consider the eigenvalue problem for the Laplacian<br />operators with Dirichlet boundary conditions (we could use other boundary conditions as<br />well):<br />?<br />φj(x) = 0,<br />Because −∇2is a positive definite Hermitian operator, the set of eigenfunctions φj(·) is<br />orthonormal with respect to the inner product<br />?<br />that is,<br />?<br />and all the eigenvalues λjare real and positive. The negative Laplace operator can then be<br />assigned the formal kernel<br />?<br />−∇2φj(x) = λjφj(x),x ∈ Ω<br />x ∈ ∂Ω.<br />(10)<br />?f,g? =<br />Ω<br />f(x)g(x)dx(11)<br />Ω<br />φi(x)φj(x)dx = δij,(12)<br />l(x,x?) =<br />j<br />λjφj(x)φj(x?)(13)<br />4</p>  <p>Page 5</p> <p>Hilbert Space Methods for Reduced-Rank Gaussian Process Regression<br />in the sense that<br />−∇2f(x) =<br />?<br />l(x,x?)f(x?)dx?,(14)<br />for sufficiently (weakly) differentiable functions f in the domain Ω assuming Dirichlet bound-<br />ary conditions.<br />If we consider the formal powers of this representation, due to orthonormality of the<br />basis, we can write the arbitrary operator power s = 1,2,... of the kernel as<br />?<br />This is again to be interpreted to mean that<br />?<br />for regular enough functions f and in the current domain with the assumed boundary<br />conditions.<br />This implies that on the domain Ω, assuming the boundary conditions, we also have<br />ls(x,x?) =<br />j<br />λs<br />jφj(x)φj(x?). (15)<br />−(∇2)sf(x) =ls(x,x?)f(x?)dx?, (16)<br />?a0+ a1(−∇2) + a2(−∇2)2+ a3(−∇2)3+ ···?f(x)<br />=<br />??a0+ a1l1(x,x?) + a2l2(x,x?) + a3l3(x,x?) + ···?f(x?)dx?.<br />The left hand side is just Kf via (9), on the domain with the boundary conditions, and<br />thus by comparing to (4) and using (15) we can conclude that<br />(17)<br />k(x,x?) ≈ a0+ a1l1(x,x?) + a2l2(x,x?) + a3l3(x,x?) + ···<br />=<br />?<br />j<br />?a0+ a1λ1<br />j+ a2λ2<br />j+ a3λ3<br />j+ ···?φj(x)φj(x?),<br />(18)<br />which is only an approximation to the covariance function due to restriction of the domain<br />to Ω and the boundary conditions. By letting ?ω?2= λjin (7) we now obtain<br />S(?λj) = a0+ a1λ1<br />and substituting this into (18) then leads to the approximation<br />j+ a2λ2<br />j+ a3λ3<br />j+ ··· .(19)<br />k(x,x?) ≈<br />?<br />j<br />S(?λj)φj(x)φj(x?),<br />(20)<br />where S(·) is the spectral density of the covariance function, λj is the jth eigenvalue and<br />φj(·) the eigenfunction of the Laplace operator in a given domain. These expressions tend<br />to be simple closed-form expressions.<br />The right hand side of (20) is very easy to evaluate, because it corresponds to evaluating<br />the spectral density at the square roots of the eigenvalues and multiplying them with the<br />eigenfunctions of the Laplace operator. Because the eigenvalues of the Laplacian operator<br />5</p>  <p>Page 6</p> <p>Solin and S¨ arkk¨ a<br />0<br />5?<br />ν → ∞<br />ν =7<br />2<br />ν =5<br />2<br />ν =3<br />2<br />ν =1<br />2<br />Exact<br />m = 12<br />m = 32<br />m = 64<br />m = 128<br />Figure 1: Approximations to covariance functions of the Mat´ ern class of various degrees<br />of smoothness; ν = 1/2 corresponds to the exponential Ornstein–Uhlenbeck covariance<br />function, and ν → ∞ to the squared exponential (exponentiated quadratic) covariance<br />function. Approximations are shown for 12, 32, 64, and 128 eigenfunctions.<br />are monotonically increasing with j and for bounded covariance functions the spectral den-<br />sity goes to zero fast with higher frequencies, we can expect to obtain a good approximation<br />of the right hand side by retaining only a finite number of terms in the series. However,<br />even with an infinite number of terms this is only an approximation, because we assumed a<br />compact domain with boundary conditions. The approximation can be, though, expected<br />to be good at the input values which are not near the boundary of Ω, where the Laplacian<br />was taken to be zero.<br />As an example, Figure 1 shows Mat´ ern covariance functions of various degrees of<br />smoothness ν (see, e.g., Rasmussen and Williams, 2006) and approximations for differ-<br />ent numbers of basis functions in the approximation. The basis consists of the functions<br />φj(x) = L−1/2sin(πj(x + L)/(2L)) and the eigenvalues were λj= (π j/(2L))2with L = 1<br />and ? = 0.1. For the squared exponential the approximation is indistinguishable from the<br />exact curve already at m = 12, whereas the less smooth functions require more terms.<br />2.4 Inner Product Point of View<br />Instead of considering a compact bounded set Ω, we can consider the same approximation<br />in terms of an inner product defined by an input density (Williams and Seeger, 2000). Let<br />the inner product be defined as<br />?<br />where w(x) is some positive weight function such that<br />inner product, we define the operator<br />?<br />This operator is self-adjoint with respect to the inner product, ?Kf,g? = ?f,Kg?, and<br />according to the spectral theorem there exists an orthonormal set of basis functions and<br />?f,g? =f(x)g(x)w(x)dx, (21)<br />?w(x)dx &lt; ∞. In terms of this<br />Kf =k(·,x)f(x)w(x)dx.(22)<br />6</p>  <p>Page 7</p> <p>Hilbert Space Methods for Reduced-Rank Gaussian Process Regression<br />positive constants, {ϕj(x),γj| j = 1,2,...}, that satisfies the eigenvalue equation<br />(Kϕj)(x) = γjϕj(x).<br />Thus k(x,x?) has the series expansion<br />(23)<br />k(x,x?) =<br />?<br />j<br />γjϕj(x)ϕj(x?). (24)<br />Similarly, we also have the Karhunen–Loeve expansion for a sample function f(x) with zero<br />mean and the above covariance function:<br />?<br />where fjs are independent zero mean Gaussian random variables with variances γj (see,<br />e.g., Lenk, 1991).<br />For the negative Laplacian the corresponding definition is<br />f(x) =<br />j<br />fjϕj(x), (25)<br />Df = −∇2[f w],(26)<br />which implies<br />?Df,g? = −<br />?<br />f(x)w(x)∇2[g(x)w(x)]dx, (27)<br />and the operator defined by (26) can be seen to be self-adjoint. Again, there exists an<br />orthonormal basis {φj(x) | j = 1,2,...} and positive eigenvalues λj which satisfy the<br />eigenvalue equation<br />(Dφj)(x) = λjφj(x).<br />Thus the kernel of D has a series expansion similar to Equation (13) and thus an approxi-<br />mation can be given in the same form as in Equation (20). In this case the approximation<br />error comes from approximating the Laplacian operator with the more smooth operator,<br />(28)<br />∇2f ≈ ∇2[f w], (29)<br />which is closely related to assumption of an input density w(x) for the Gaussian process.<br />However, when the weight function w(·) is close to constant in the area where the inputs<br />points are located, the approximation is accurate.<br />2.5 Connection to Sturm–Liouville Theory<br />The presented methodology is also related to the Sturm–Liouville theory arising in the<br />theory of partial differential equations (Courant and Hilbert, 2008). When the input x is<br />scalar, the eigenvalue problem in Equation (23) can be written in Sturm–Liouville form as<br />follows:<br />−d<br />dxdx<br />?<br />w2(x)dφj(x)<br />?<br />− w(x)d2w(x)<br />dx2<br />φj(x) = λjw(x)φj(x).(30)<br />The above equation can be solved for φj(x) and λj using numerical methods for Sturm–<br />Liouville equations. Also note that if we select w(x) = 1 in a finite set, we obtain the<br />7</p>  <p>Page 8</p> <p>Solin and S¨ arkk¨ a<br />(a) ν =1<br />2and ? = 0.5 (b) ν =3<br />2and ? = 0.5 (c) ν → ∞ and ? = 0.5<br />Figure 2: Approximate random draws of Gaussian processes with the Mat´ ern covariance<br />function on the hull of a unit sphere. The color scale and radius follow the process.<br />equation −d2/dx2φj(x) = λjφj(x) which is compatible with the results in the previous<br />section.<br />We consider the case where x ∈ Rdand w(x) is symmetric around the origin and thus<br />is only a function of the norm r = ?x? (i.e. has the form w(r)). The Laplacian in spherical<br />coordinates is<br />∇2f =<br />rd−1<br />∂r<br />1<br />∂<br />?<br />rd−1∂f<br />∂r<br />?<br />+1<br />r2∆Sd−1f,(31)<br />where ∆Sd−1 is the Laplace–Beltrami operator on Sd−1. Let us assume that φj(r,ξ) =<br />hj(r)g(ξ), where ξ denotes the angular variables. After some algebra, writing the equations<br />into Sturm–Liouville form yields for the radial part<br />?<br />−d<br />dr<br />w2(r)rdhj(r)<br />dr<br />?<br />−<br />?dw(r)<br />dr<br />w(r) +d2w(r)<br />dr2<br />w(r)r<br />?<br />hj(r) = λjw(r)rhj(r),(32)<br />and ∆Sd−1g(ξ) = 0 for the angular part. The solutions to the angular part are the Laplace’s<br />spherical harmonics. Note that if we assume that we have w(r) = 1 on some area of finite<br />radius, the first equation becomes (when d &gt; 1):<br />r2d2hj(r)<br />dr2<br />+ rdhj(r)<br />dr<br />+ r2λjhj(r) = 0. (33)<br />Figure 2 shows example Gaussian random field draws on a unit sphere, where the basis<br />functions are the Laplace spherical harmonics and the covariance functions of the Mat´ ern<br />class with different degrees of smoothness ν. Our approximation is straight-forward to apply<br />in any domain, where the eigendecomposition of the Laplacian can be formed.<br />3. Application of the Method to GP Regression<br />In this section we show how the approximation (20) can be used in Gaussian process regres-<br />sion. We also write down the expressions needed for hyperparameter learning and discuss<br />the computational requirements of the methods.<br />8</p>  <p>Page 9</p> <p>Hilbert Space Methods for Reduced-Rank Gaussian Process Regression<br />3.1 Gaussian Process Regression<br />GP regression is usually formulated as predicting an unknown scalar output f(x∗) associated<br />with a known input x∗∈ Rd, given a training data set D = {(xi,yi) | i = 1,2,...,n}. The<br />model functions f are assumed to be realizations of a Gaussian random process prior and<br />the observations corrupted by Gaussian noise:<br />f ∼ GP(0,k(x,x?))<br />yi= f(xi) + εi,<br />(34)<br />where εi∼ N(0,σ2<br />zero mean and the measurement errors are independent Gaussian, but the results of this<br />paper can be easily generalized to arbitrary mean functions and dependent Gaussian errors.<br />The direct solution to the GP regression problem (34) gives the predictions p(f(x∗) | D) =<br />N(f(x∗) | E[f(x∗)],V[f(x∗)]). The conditional mean and variance can be computed in<br />closed-form as (Rasmussen and Williams, 2006)<br />n). For notational simplicity the functions in the above model are a priori<br />E[f(x∗)] = kT<br />V[f(x∗)] = k(x∗,x∗) − kT<br />∗(K + σ2<br />nI)−1y,<br />∗(K + σ2<br />nI)−1k∗,<br />(35)<br />where Kij= k(xi,xj), k∗is an n-dimensional vector with the ith entry being k(x∗,xi), and<br />y is a vector of the n observations.<br />In order to avoid the n × n matrix inversion in (35), we use the approximation scheme<br />presented in the previous section and project the process to a truncated set of m basis<br />functions of the Laplacian as given in Equation (20) such that<br />f(x) ≈<br />m<br />?<br />j=1<br />fjφj(x), (36)<br />where fj ∼ N(0,S(?λj)). We can then form an approximate eigendecomposition of the<br />such that Λjj= S(?λj),j = 1,2,...,m. Here S(·) is the spectral density of the Gaussian<br />in the decomposition are given by the eigenvectors φj(x) of the Laplacian such that Φij=<br />φj(xi).<br />Using the matrix inversion lemma we rewrite (35) as follows:<br />matrix K ≈ ΦΛΦT, where Λ is a diagonal matrix of the leading m approximate eigenvalues<br />process and λjthe jth eigenvalue of the Laplace operator. The corresponding eigenvectors<br />E[f∗] ≈ φT<br />V[f∗] ≈ σ2<br />∗(ΦTΦ + σ2<br />nφT<br />nΛ−1)−1ΦTy,<br />nΛ−1)−1φ∗,<br />∗(ΦTΦ + σ2<br />(37)<br />where φ∗is an m-dimensional vector with the jth entry being φj(x∗). Thus, when the size<br />of the training set is higher than the number of required basis functions n &gt; m, the use of<br />this approximation is advantageous.<br />3.2 Learning the Hyperparameters<br />A common way to learn the hyperparameters θ of the covariance function (suppressed<br />earlier in the notation for brevity) and the noise variance σ2<br />nis by maximizing the marginal<br />9</p>  <p>Page 10</p> <p>Solin and S¨ arkk¨ a<br />likelihood function (Rasmussen and Williams, 2006; Qui˜ nonero-Candela and Rasmussen,<br />2005b). Let Q = K + σ2<br />nI for the full model, then the negative log marginal likelihood and<br />its derivatives are<br />L =1<br />∂L<br />∂θk<br />∂L<br />∂σ2<br />n<br />2log|Q| +1<br />=1<br />2Tr<br />=1<br />2yTQ−1y +n<br />Q−1∂Q<br />∂θk<br />2Tr?Q−1?−1<br />2log(2π), (38)<br />??<br />2yTQ−1Q−1y,<br />−1<br />2yTQ−1∂Q<br />∂θkQ−1y, (39)<br />(40)<br />and they can be combined with a conjugate gradient optimizer. The problem in this case is<br />the inversion of Q, which is an n × n matrix. And thus each step of running the optimizer<br />is O(n3). For our approximation scheme, let˜Q = ΦΛΦT+ σ2<br />in the above expressions gives us the following:<br />nI. Now replacing Q with˜Q<br />˜L =1<br />∂˜L<br />∂θk<br />∂˜L<br />∂σ2<br />n<br />2log|˜Q| +1<br />=1<br />2 ∂θk<br />=1<br />2 ∂σ2<br />2yT˜Q−1y +n<br />+1<br />2 ∂θk<br />+1<br />2 ∂σ2<br />2log(2π),(41)<br />∂ log|˜Q|<br />∂yT˜Q−1y<br />, (42)<br />∂ log|˜Q|<br />n<br />∂yT˜Q−1y<br />n<br />,(43)<br />where for the terms involving log|˜Q|:<br />log|˜Q| = (n − m)logσ2<br />n+ log|Z| +<br />m<br />?<br />j=1<br />logS(?λj),<br />?<br />(44)<br />∂ log|˜Q|<br />∂θk<br />=<br />m<br />?<br />j=1<br />S(?λj)−1∂S(?λj)<br />=n − m<br />σ2<br />n<br />∂θk<br />− σ2<br />nTrZ−1Λ−2∂Λ<br />∂θk<br />?<br />, (45)<br />∂ log|˜Q|<br />∂σ2<br />n<br />+ Tr?Z−1Λ−1?, (46)<br />and for the terms involving˜Q−1:<br />yT˜Q−1y =<br />1<br />σ2<br />n<br />?<br />yTy − yΦZ−1ΦTy<br />?<br />yTΦZ−1Λ−1Z−1ΦTy −<br />?<br />,(47)<br />∂yT˜Q−1y<br />∂θk<br />∂yT˜Q−1y<br />∂σ2<br />= −yTZ−1<br />Λ−2∂Λ<br />∂θk<br />?<br />Z−1y,(48)<br />n<br />=<br />1<br />σ2<br />n<br />1<br />σ2<br />n<br />yT˜Qy,(49)<br />where Z = σ2<br />can be avoided in many cases, and the inversion of Z can be carried out through Cholesky<br />factorization for numerical stability. This factorization (LLT= Z) can also be used for<br />nΛ−1+ ΦTΦ. For efficient implementation, matrix-to-matrix multiplications<br />10</p>  <p>Page 11</p> <p>Hilbert Space Methods for Reduced-Rank Gaussian Process Regression<br />the term log|Z| = 2?<br />Once the marginal likelihood and its derivatives are available, it is also possible to use<br />other methods for parameter inference such as Markov chain Monte Carlo methods (Liu,<br />2001; Brooks et al., 2011) including Hamiltonian Monte Carlo (HMC, Duane et al., 1987;<br />Neal, 2011) as well as numerous others.<br />jlogLjj, and Tr?Z−1Λ−1?<br />=?<br />j1/(ZjjΛjj) can be evaluated by<br />element-wise multiplication.<br />3.3 Discussion on the Computational Complexity<br />As can be noted from Equation (20), the basis functions in the reduced-rank approximation<br />do not depend on the hyperparameters of the covariance function. Thus it is enough to<br />calculate the product ΦTΦ only once, which means that the method has a overall asymp-<br />totic computational complexity of O(nm2). After this initial cost, evaluating the marginal<br />likelihood and the marginal likelihood gradient is an O(m3) operation—which in practice<br />comes from the Cholesky factorization of Z on each step.<br />If the number of observations n is so large that storing the n×m matrix Φ is not feasible,<br />the computations of ΦTΦ can be carried out in blocks. Storing the evaluated eigenfunctions<br />in Φ is not necessary, because the φj(x) are closed-form expressions that can be evaluated<br />when necessary. In practice, it might be preferable to cache the result of ΦTΦ (causing a<br />memory requirement scaling as O(m2)), but this is not required.<br />The computational complexity of conventional sparse GP approximations typically scale<br />as O(nm2) in time for each step of evaluating the marginal likelihood. The scaling in<br />demand of storage is O(nm). This comes from the inevitable cost of re-evaluating all results<br />involving the basis functions on each step and storing the matrices required for doing this.<br />This applies to all the methods that will be discussed in Section 5, with the exception of<br />SSGP, where the storage demand can be relaxed by re-evaluating the basis functions on<br />demand.<br />We can also consider the rather restricting, but in certain applications often encountered<br />case, where the measurements are constrained to a regular grid. This causes the product of<br />the orthonormal eigenfunction matrices ΦTΦ to be diagonal, avoiding the calculation of the<br />matrix inverse altogether. This relates to the FFT-based methods for GP regression (Pa-<br />ciorek, 2007; Fritz et al., 2009), and the projections to the basis functions can be evaluated<br />by fast Fourier transform in O(nlogn) time complexity.<br />4. Convergence Analysis<br />In this section we analyze the convergence of the proposed approximation when the size of<br />the domain Ω and the number of terms in the series grows to infinity. We start by analyzing<br />a univariate problem in the domain Ω = [−L,L] and with Dirichlet boundary conditions<br />and then generalize the result to d-dimensional cubes Ω = [−L1,L1] × ··· × [−Ld,Ld]. We<br />also discuss how the analysis could be extended to other types of basis functions.<br />11</p>  <p>Page 12</p> <p>Solin and S¨ arkk¨ a<br />4.1 Univariate Dirichlet Case<br />In the univariate case, the m-term approximation has the form<br />?km(x,x?) =<br />m<br />?<br />j=1<br />S(?λj)φj(x)φj(x?), (50)<br />where the eigenfunctions and eigenvalues are:<br />φj(x) =<br />1<br />√L<br />sin<br />?π j (x + L)<br />2L<br />?<br />andλj=<br />?π j<br />2L<br />?2<br />, for j = 1,2,....(51)<br />The true covariance function k(x,x?) is assumed to be stationary and have a spectral density<br />which is uniformly bounded S(ω) &lt; ∞, has at least two bounded derivatives |S?(ω)| &lt; ∞,<br />|S??(ω)| &lt; ∞, and has a bounded integral over the real axis?∞<br />and thus we are only interested in the case x,x?∈ [−?L,?L]. For the purposes of analysis we<br />The univariate convergence result can be summarized as the following theorem which is<br />proved in Appendix A.1.<br />−∞S(ω)dω &lt; ∞. We also<br />assume that our training and test sets are constrained in the area [−?L,?L], where?L &lt; L,<br />also assume that L is bounded below by a constant.<br />Theorem 4.1. There exists a constant C such that<br />???k(x,x?) −?km(x,x?)<br />which in turn implies that uniformly<br />??? ≤C<br />L+2<br />π<br />?∞<br />2L<br />π m<br />S(ω)dω, (52)<br />lim<br />L→∞<br />?<br />lim<br />m→∞<br />?km(x,x?)<br />?<br />= k(x,x?).(53)<br />Remark 4.2. Note that we cannot simply exchange the order of the limits in the above<br />theorem. However, the theorem does ensure the convergence of the approximation in the joint<br />limit m,L → ∞ provided that we add terms to the series fast enough such that m/L → ∞.<br />That is, in this limit, the approximation?km(x,x?) converges uniformly to k(x,x?).<br />As such, the results above only ensure the convergence of the prior covariance functions.<br />However, it turns out that this also ensures the convergence of the posterior as is summarized<br />in the following corollary.<br />Corollary 4.3. Because the Gaussian process regression equations only involve pointwise<br />evaluations of the kernels, it also follows that the posterior mean and covariance functions<br />converge uniformly to the exact solutions in the limit m,L → ∞.<br />4.2 Multivariate Cartesian Dirichlet Case<br />In order to generalize the results from the previous section, we turn our attention to a<br />d-dimensional inputs space with rectangular domain Ω = [−L1,L1] × ··· × [−Ld,Ld] with<br />12</p>  <p>Page 13</p> <p>Hilbert Space Methods for Reduced-Rank Gaussian Process Regression<br />Dirichlet boundary conditions. In this case we consider a truncated m = ˆ mdterm approxi-<br />mation of the form<br />?km(x,x?) =<br />ˆ m<br />?<br />j1,...,jd=1<br />S(?λj1,...,jd)φj1,...,jd(x)φj1,...,jd(x?)<br />(54)<br />with the eigenfunctions and eigenvalues<br />φj1,...,jd(x) =<br />d?<br />k=1<br />1<br />√Lk<br />sin<br />?π jk(xk+ Lk)<br />2Lk<br />?<br />andλj1,...,jd=<br />d<br />?<br />k=1<br />?π jk<br />2Lk<br />?2<br />.<br />(55)<br />The true covariance function k(x,x?) is assumed to be stationary and have a spectral den-<br />sity S(ω) which is two times differentiable and the derivatives are assumed to be bounded.<br />We also assume that the single-variable integrals are finite<br />in this case is equivalent to requiring that the integral over the whole space is finite<br />?<br />The following result for this d-dimensional case is proved in Appendix A.2.<br />?∞<br />−∞S(ω)dωk &lt; ∞, which<br />RdS(ω)dω &lt; ∞. Furthermore, we assume that the training and test sets are contained in<br />the d-dimensional cube [−?L,?L]dand that Lks are bounded from below.<br />Theorem 4.4. There exists a constant Cdsuch that<br />???k(x,x?) −?km(x,x?)<br />where L = minkLk, which in turn implies that uniformly<br />?<br />Remark 4.5. Analogously as in the one-dimensional case we cannot simply exchange the<br />order of the limits above. Furthermore, we need to add terms fast enough so that ˆ m/Lk→ ∞<br />when m,L1,...,Ld→ ∞.<br />Corollary 4.6. As in the one-dimensional case, the uniform convergence of the prior co-<br />variance function also implies uniform convergence of the posterior mean and covariance<br />in the limit m,L1,...,Ld→ ∞.<br />??? ≤Cd<br />L+<br />1<br />πd<br />?<br />?ω?≥π ˆ m<br />2L<br />S(ω)dω,<br />(56)<br />lim<br />L1,...,Ld→∞<br />lim<br />m→∞<br />?km(x,x?)<br />?<br />= k(x,x?).(57)<br />4.3 Other Domains<br />It would also be possible carry out similar convergence analysis, for example, in a spherical<br />domain. In that case the technical details become slightly more complicated, because instead<br />sinusoidals we will have Bessel functions and the eigenvalues no longer form a uniform grid.<br />This means that instead of Riemann integrals we need to consider weighted integrals where<br />the distribution of the zeros of Bessel functions is explicitly accounted for. It might also<br />be possible to use some more general theoretical results from mathematical analysis to<br />obtain the convergence results. However, due to these technical challenges more general<br />convergence proof will be developed elsewhere.<br />13</p>  <p>Page 14</p> <p>Solin and S¨ arkk¨ a<br />There is also a similar technical challenge in the analysis when the basis functions are<br />formed by assuming an input density (see Section 2.4) instead of a bounded domain. Because<br />explicit expressions for eigenfunctions and eigenvalues cannot be obtained in general, the<br />elementary proof methods which we used here cannot be applied. Therefore the convergence<br />analysis of this case is also left as a topic for future research.<br />5. Relationship to Other Methods<br />In this section we compare our method to existing sparse GP methods from a theoretical<br />point of view. We consider two different classes of approaches: a class of inducing input<br />methods based on the Nystr¨ om approximation (following the interpretation of Qui˜ nonero-<br />Candela and Rasmussen, 2005a), and direct spectral approximations.<br />5.1 Methods from the Nystr¨ om Family<br />A crude but rather effective scheme for approximating the eigendecomposition of the Gram<br />matrix is the Nystr¨ om method (see, e.g., Baker, 1977, for the integral approximation<br />scheme). This method is based on choosing a set of m inducing inputs xu and scaling<br />the corresponding eigendecomposition of their corresponding covariance matrix Ku,u to<br />match that of the actual covariance. The Nystr¨ om approximations to the jth eigenvalue<br />and eigenfunction are<br />?λj=1<br />?φj(x) =<br />mλu,j,<br />√m<br />λu,j<br />(58)<br />k(x,xu)φu,j,(59)<br />where λu,jand φu,jcorrespond to the jth eigenvalue and eigenvector of Ku,u. This scheme<br />was originally introduced to the GP context by Williams and Seeger (2001). They presented<br />a sparse scheme, where the resulting approximate prior covariance over the latent variables<br />is Kf,uK−1<br />u,uKu,f, which can be derived directly from Equations (58) and (59).<br />As discussed by Qui˜ nonero-Candela and Rasmussen (2005a), the Nystr¨ om method by<br />Williams and Seeger (2001) does not correspond to a well-formed probabilistic model. How-<br />ever, several methods modifying the inducing point approach are widely used. The Subset<br />of Regressors (SOR, Smola and Bartlett, 2001) method uses the Nystr¨ om approximation<br />scheme for approximating the whole covariance function,<br />kSOR(x,x?) =<br />m<br />?<br />j=1<br />?λj?φj(x)?φj(x?), (60)<br />whereas the sparse Nystr¨ om method (Williams and Seeger, 2001) only replaces the training<br />data covariance matrix. The SOR method is in this sense a complete Nystr¨ om approxima-<br />tion to the full GP problem. A method in-between is the Deterministic Training Conditional<br />(DTC, Csat´ o and Opper, 2002; Seeger et al., 2003) method which retains the true covari-<br />ance for the training data, but uses the approximate cross-covariances between training and<br />test data. For DTC, tampering with the covariance matrix causes the result not to actu-<br />ally be a Gaussian process. The Variational Approximation (VAR, Titsias, 2009) method<br />14</p>  <p>Page 15</p> <p>Hilbert Space Methods for Reduced-Rank Gaussian Process Regression<br />modifies the DTC method by an additional trace term in the likelihood that comes from<br />the variational bound.<br />The Fully Independent (Training) Conditional (FIC, Qui˜ nonero-Candela and Rasmussen,<br />2005a) method (originally introduced as Sparse Pseudo-Input GP by Snelson and Ghahra-<br />mani, 2006) is also based on the Nystr¨ om approximation but contains an additional diagonal<br />term replacing the diagonal of the approximate covariance matrix with the values from the<br />true covariance. The corresponding prior covariance function for FIC, is thus<br />kFIC(xi,xj) = kSOR(xi,xj) + δi,j(k(xi,xj) − kSOR(xi,xj)),(61)<br />where δi,jis the Kronecker delta.<br />Figure 3 illustrates the effect of the approximations compared to the exact correlation<br />structure in the GP. The dashed contours show the exact correlation contours computed<br />for three locations with the squared exponential covariance function. Figure 3a shows the<br />results for the FIC approximation with 16 inducing points (locations shown in the figure).<br />It is clear that the number of inducing points or their locations are not sufficient to capture<br />the correlation structure. For similar figures and discussion on the effects of the inducing<br />points, see Vanhatalo et al. (2010). This behavior is not unique to SOR or FIC, but applies<br />to all the methods from the Nystr¨ om family.<br />5.2 Direct Spectral Methods<br />The sparse spectrum GP (SSGP) method introduced by L´ azaro-Gredilla et al. (2010) uses<br />the spectral representation of the covariance function for drawing random samples from the<br />spectrum. These samples are used for representing the GP on a trigonometric basis<br />φ(x) =?cos(2π sT<br />1x)sin(2π sT<br />1x) ...cos(2π sT<br />hx) sin(2π sT<br />hx)?,(62)<br />where the spectral points sr,r = 1,2,...,h (2h = m), are sampled from the spectral density<br />of the original stationary covariance function (following the normalization convention used<br />in the original paper). The covariance function corresponding to the SSGP scheme is now<br />of the form<br />kSSGP(x,x?) =2σ2<br />m<br />φ(x)φT(x?) =σ2<br />h<br />h<br />?<br />r=1<br />cos<br />?<br />2π sT<br />r(x − x?)<br />?<br />, (63)<br />where σ2is the magnitude scale hyperparameter. This representation of the sparse spec-<br />trum method converges to the full GP in the limit of the number of spectral points going<br />to infinity, and it is the preferred formulation of the method in one or two dimensions (see<br />L´ azaro-Gredilla, 2010, for discussion). We can interpret the SSGP method in (63) as a<br />Monte Carlo approximation of the Wiener–Khintchin integral. In order to have a represen-<br />tative sample of the spectrum, the method typically requires the number of spectral points<br />to be large. For high-dimensional inputs the number of required spectral points becomes<br />overwhelming, and optimizing the spectral locations along with the hyperparameters at-<br />tractive. However, as argued by L´ azaro-Gredilla et al. (2010), this option does not converge<br />to the full GP and suffers from overfitting to the training data.<br />15</p>  <p>Page 16</p> <p>Solin and S¨ arkk¨ a<br />−L<br />0<br />L<br />−L<br />0<br />L<br />x1<br />x2<br />(a) SOR/FIC (grid)<br />−L<br />0<br />L<br />−L<br />0<br />L<br />x1<br />x2<br />(b) SOR/FIC (random)<br />−L<br />0<br />L<br />−L<br />0<br />L<br />x1<br />x2<br />(c) SSGP (random)<br />−L<br />0<br />L<br />−L<br />0<br />L<br />x1<br />x2<br />(d) The new method<br />(Cartesian)<br />−L<br />0<br />L<br />−L<br />0<br />L<br />x1<br />x2<br />(e) The new method<br />(extended Cartesian)<br />−L<br />0<br />L<br />−L<br />0<br />L<br />x1<br />x2<br />(f) The new method<br />(polar)<br />Figure 3: Correlation contours computed for three locations ( ) corresponding to the<br />squared exponential covariance function (exact contours dashed). The rank of each ap-<br />proximation is m = 16, and the locations of the inducing inputs are marked with blue stars<br />( ). The hyperparameters are the same in each figure. The domain boundary is shown in<br />thin grey (<br />) if extended outside the box.<br />Contours for the sparse spectrum SSGP method are visualized in Figure 3c. Here the<br />spectral points were chosen at random following L´ azaro-Gredilla (2010). Because the basis<br />functions are spanned using both sines and cosines, the number of spectral points was<br />h = 8 in order to match the rank m = 16. These results agree well with those presented in<br />the L´ azaro-Gredilla et al. (2010) for a one-dimensional example. For this particular set of<br />spectral points some directions of the contours happen to match the true values very well,<br />while other directions are completely off. Increasing the rank from 16 to 100 would give<br />comparable results to the other methods.<br />While SSGP is based on a sparse spectrum, the reduced-rank method proposed in this<br />paper aims to make the spectrum as ‘full’ as possible at a given rank. While SSGP can be<br />interpreted as a Monte Carlo integral approximation, the corresponding interpretation to<br />the proposed method would a numerical quadrature-based integral approximation (cf. the<br />16</p>  <p>Page 17</p> <p>Hilbert Space Methods for Reduced-Rank Gaussian Process Regression<br />convergence proof in Appendix A.1). Figure 3d shows the same contours obtained by the<br />proposed reduced-rank method. Here the eigendecomposition of the Laplace operator has<br />been obtained for the square Ω = [−L,L] × [−L,L] with Dirichlet boundary conditions.<br />The contours match well with the full solution towards the middle of the domain. The<br />boundary effects drive the process to zero, which is seen as distortion near the edges.<br />Figure 3e shows how extending the boundaries just by 25% and keeping the number<br />of basis functions fixed at 16, gives good results. The last Figure 3f corresponds to using<br />a disk shaped domain instead of the rectangular. The eigendecomposition of the Laplace<br />operator is done in polar coordinates, and the Dirichlet boundary is visualized by a circle<br />in the figure.<br />6. Experiments<br />In this section we aim to provide examples of the practical use of the proposed method,<br />and to compare it against other methods that are typically used in a similar setting. We<br />start with a small simulated one-dimensional dataset, and then provide more extensive<br />comparisons by using real-world data. We also consider an example of data, where the<br />input domain is the surface of a sphere, and conclude our comparison by using a very large<br />dataset to demonstrate what possibilities the computational benefits open.<br />6.1 Experimental Setup<br />For assessing the performance of different methods we use 10-fold cross-validation and<br />evaluate the following measures based on the validation set: the standardized mean squared<br />error (SMSE) and the mean standardized log loss (MSLL), respectively defined as:<br />SMSE =<br />n∗<br />?<br />i=1<br />(y∗i− µ∗i)2<br />Var[y]<br />,andMSLL =<br />1<br />2n∗<br />n∗<br />?<br />i=1<br />?(y∗i− µ∗i)2<br />σ2<br />∗i<br />+ log2πσ2<br />∗i<br />?<br />,<br />where µ∗i= E[f(x∗i)] and σ2<br />test sample i = 1,2,...,n∗, and y∗iis the actual test value. The training data variance is<br />denoted by Var[y]. For all experiments, the values reported are averages over ten repetitions.<br />We compare our solution to SOR, DTC, VAR, and FIC using the implementations<br />provided in the GPstuff software package (version 4.3.1, see Vanhatalo et al., 2013) for<br />Mathworks Matlab. The sparse spectrum SSGP method by L´ azaro-Gredilla et al. (2010) was<br />implemented into the GPstuff toolbox for the comparisons.1The reference implementation<br />was modified such that also non-ARD covariances could be accounted for.<br />The m inducing inputs for SOR, DTC, VAR, and FIC were chosen at random as a<br />subset from the training data and kept fixed between the methods. For low-dimensional<br />inputs, this tends to lead to good results and avoid over-fitting to the training data, while<br />optimizing the input locations alongside hyperparameters becomes the preferred approach<br />in high input dimensions (Qui˜ nonero-Candela and Rasmussen, 2005a). The results are<br />averaged over ten repetitions in order to present the average performance of the methods.<br />In Sections 6.2, 6.3, and 6.5, we used a Cartesian domain with Dirichlet boundary conditions<br />∗i= V[f(x∗i)] + σ2<br />nare the predictive mean and variance for<br />1. The implementation is based on the code available from Miguel L´ azaro-Gredilla: http://www.tsc.uc3m.<br />es/~miguel/downloads.php.<br />17</p>  <p>Page 18</p> <p>Solin and S¨ arkk¨ a<br />−101<br />−2<br />−1<br />0<br />1<br />2<br />Input, x<br />Output, y<br />Observations<br />Mean<br />95% region<br />Exact interval<br />(a) Gaussian process regression solution<br />? = 0.1<br />σ2= 12<br />True curve<br />Approximation<br />True value<br />σ2<br />n= 0.22<br />(b) Log marginal likelihood curves<br />Figure 4: (a) 256 data points generated from a GP with hyperparameters (σ2,?,σ2<br />(12,0.1,0.22), the full GP solution, and an approximate solution with m = 32. (b) Negative<br />marginal likelihood curves for the signal variance σ2, length-scale ?, and noise variance σ2<br />n) =<br />n.<br />for the new reduced-rank method. To avoid boundary effects, the domain was extended by<br />10% outside the inputs in each direction.<br />In the comparisons we followed the guidelines given by Chalupka et al. (2013) for making<br />comparisons between the actual performance of different methods. For hyperparameter op-<br />timization we used the fminunc routine in Matlab with a Quasi-Newton optimizer. We also<br />tested several other algorithms, but the results were not sensitive to the choice of optimizer.<br />The optimizer was run with a termination tolerance of 10−5on the target function value<br />and on the optimizer inputs. The number of required target function evaluations stayed<br />fairly constant for all the comparisons, making the comparisons for the hyperparameter<br />learning bespoke.<br />6.2 Toy Example<br />Figure 4 shows a simulated example, where 256 data points are drawn from a Gaussian pro-<br />cess prior with a squared exponential covariance function. We use the same parametrization<br />as Rasmussen and Williams (2006) and denote the signal variance σ2, length-scale ?, and<br />noise variance σ2<br />n. Figure 4b shows the negative marginal log likelihood curves both for the<br />full GP and the approximation with m = 32 basis functions. The likelihood curve approxi-<br />mations are almost exact and only differs from the full GP likelihood for small length-scales<br />(roughly for values smaller than 2L/m). Figure 4a shows the approximate GP solution.<br />The mean estimate follows the exact GP mean, and the shaded region showing the 95%<br />confidence area differs from the exact solution (dashed) only near the boundaries.<br />Figures 5a and 5b show the SMSE and MSLL values for m = 8,10,...,32 inducing<br />inputs and basis functions for the toy dataset from Figure 4.<br />proposed reduced rank method is fast and a soon as the number of eigenfunctions is large<br />The convergence of the<br />18</p>  <p>Page 19</p> <p>Hilbert Space Methods for Reduced-Rank Gaussian Process Regression<br />812 1620<br />m<br />242832<br />10−1.2<br />10−1<br />SMSE<br />(a) SMSE for the toy data<br />81216 20<br />m<br />2428 32<br />10−2<br />10−1<br />MSLL<br />FULL<br />NEW<br />SOR<br />DTC<br />VAR<br />FIC<br />SSGP<br />(b) MSLL for the toy data<br />100<br />101<br />102<br />10−0.6<br />10−0.4<br />Evaluation time (s)<br />SMSE<br />(c) SMSE for the precipitation data<br />100<br />101<br />102<br />10−0.2<br />10−0.1<br />100<br />Evaluation time (s)<br />MSLL<br />(d) MSLL for the precipitation data<br />Figure 5: Standardized mean squared error (SMSE) and mean standardized log loss (MSLL)<br />results for the toy data (d = 1, n = 256) from Figure 4 and the precipitation data (d = 2,<br />n = 5776) evaluated by 10-fold cross-validation and averaged over ten repetitions. The<br />evaluation time includes hyperparameter learning.<br />enough (m = 20) to account for the short length-scales, the approximation converges to the<br />exact full GP solution (shown by the dashed line).<br />In this case the SOR method that uses the Nystr¨ om approximation to directly approx-<br />imate the spectrum of the full GP (see Section 5) seems to give good results. However, as<br />the resulting approximation in SOR corresponds to a singular Gaussian distribution, the<br />predictive variance is underestimated. This can be seen in Figure 5b, where SOR seems to<br />give better results than the full GP. These results are however due to the smaller predictive<br />variance on the test set. DTC tries to fix this shortcoming of SOR—they are identical in<br />other respects except predictive variance evaluation—and while SOR and DTC give iden-<br />tical results in terms of SMSE, they differ in MSLL. We also note that additional trace<br />term in the marginal likelihood in VAR makes the likelihood surface flat, which explains<br />the differences in the results in comparison to DTC.<br />The sparse spectrum SSGP method did not perform well on average. Still, it can be<br />seen that it converges towards the performance of the full GP. The dependence on the<br />number of spectral points differs from the rest of the methods, and a rank of m = 32<br />is not enough to meet the other methods. However, in terms of best case performance<br />19</p>  <p>Page 20</p> <p>Solin and S¨ arkk¨ a<br />(a) Observation locations<br />015003000mm<br />(b) The full GP(c) The reduced-rank method<br />Figure 6: Interpolation of the yearly precipitation levels using reduced-rank GP regression.<br />Subfigure 6a shows the n = 5776 weather station locations. Subfigures 6b and 6c show the<br />results for the full GP model and the new reduced-rank GP method.<br />over the ten repetitions with different inducing inputs and spectral points, both FIC and<br />SSGP outperformed SOR, DTC, and VAR. Because of its ‘dense spectrum’ approach, the<br />proposed reduced-rank method is not sensitive to the choice of spectral points, and thus the<br />performance remained the same between repetitions. In terms of variance over the 10-fold<br />cross-validation folds, the methods in order of growing variance in the figure legend (the<br />variance approximately doubling between FULL and SSGP).<br />6.3 Precipitation Data<br />As a real-data example, we consider a precipitation data set that contain US annual pre-<br />cipitation summaries for year 1995 (d = 2 and n = 5776, available online, see Vanhatalo<br />et al., 2013). The observation locations are shown on a map in Figure 6a.<br />We limit the number of inducing inputs and spectral points to m = 128,192,...,512.<br />For the new method we additionally consider ranks m = 1024,1536,...,4096, and show<br />that this causes a computational burden of the same order as the conventional sparse GP<br />methods with smaller ms.<br />In order to demonstrate the computational benefits of the proposed model, we also<br />present the running time of the GP inference (including hyperparameter optimization). All<br />methods were implemented under a similar framework in the GPstuff package, and they all<br />employ similar reformulations for numerical stability. The key difference in the evaluation<br />times comes from hyperparameter optimization, where SOR, DTC, VAR, FIC, and SSGP<br />scale as O(nm2) for each evaluation of the marginal likelihood. The proposed reduced-rank<br />method scales as O(m3) for each evaluation (after an initial cost of O(nm2)).<br />Figures 5c and 5d show the SMSE and MSLL results for this data against evaluation<br />time.On this scale we note that the evaluation time and accuracy, both in terms of<br />SMSE and MSLL, are alike for SOR, DTC, VAR, and FIC. SSGP is faster to evaluate in<br />comparison with the Nystr¨ om family of methods, which comes from the simpler structure of<br />the approximation. Still, the number of required spectral points to meet a certain average<br />error level is larger for SSGP.<br />The results for the proposed reduced-rank method (NEW) show that with two input<br />dimensions, the required number of basis functions is larger. For the first seven points,<br />20</p>  <p>Page 21</p> <p>Hilbert Space Methods for Reduced-Rank Gaussian Process Regression<br />−20◦C<br />(a) The mean temperature<br />0◦C<br />30◦C<br />0◦C<br />2◦C<br />4◦C<br />6◦C<br />8◦C<br />(b) Standard deviation contours<br />Figure 7: Modeling of the yearly mean temperature on the spherical surface of the Earth<br />(n = 11028). Figure 7b shows the standard deviation contours which match well with the<br />continents.<br />we notice that even though the evaluation is two orders of magnitude faster, the method<br />performs only slightly worse in comparison to conventional sparse methods. By considering<br />higher ranks (the next seven points), the new method converges to the performance of the<br />full GP (both in SMSE and MSLL), while retaining a computational time comparable to<br />the conventional methods. This type of spatial medium-size GP regression problems can<br />thus be solved in seconds.<br />Figures 6b and 6c show interpolation of the precipitation levels using a full GP model<br />and the reduced-rank method (m = 1728), respectively. The results are practically identical,<br />as is easy to confirm from the color surfaces. Obtaining the reduced-rank result (including<br />initialization and hyperparameter learning) took slightly less than 30 seconds on a laptop<br />computer (MacBook Air, Late 2010 model, 2.13 GHz, 4 GB RAM), while the full GP<br />inference took approximately 18 minutes.<br />6.4 Temperature Data on the Surface of the Globe<br />We also demonstrate the use of the method in non-Cartesian coordinates. We consider<br />modeling of the spatial mean temperature over a number of n = 11028 locations around<br />the globe.2<br />As earlier demonstrated in Figure 2, we use the Laplace operator in spherical coordinates<br />as defined in (31). The eigenfunctions for the angular part are the Laplace’s spherical<br />harmonics. The evaluation of the approximation does not depend on the coordinate system,<br />and thus all the equations presented in the earlier sections remain valid. We use the squared<br />exponential covariance function and m = 1089 basis functions.<br />Figure 7 visualizes the modeling outcome. The results are visualized using an inter-<br />rupted projection (an adaption of the Goode homolosine projection) in order to preserve<br />the length-scale structure across the map.<br />which corresponds to the n = 11028 observation locations that are mostly spread over the<br />continents and western countries (the white areas in Figure 7b contain no observations).<br />The uncertainty is visualized in Figure 7b,<br />2. The data are available for download from US National Climatic Data Center: http://www7.ncdc.noaa.<br />gov/CDO/cdoselect.cmd (accessed January 3, 2014).<br />21</p>  <p>Page 22</p> <p>Solin and S¨ arkk¨ a<br />MethodSMSE MSLL<br />The reduced-rank method<br />Random subset (n = 500)<br />Random subset (n = 1000)<br />0.388 (0.007)<br />0.419 (0.035)<br />0.392 (0.022)<br />0.608 (0.009)<br />0.648 (0.014)<br />0.614 (0.010)<br />Table 1: Results for the apartment data set (d = 2, n = 102890) for predicting the<br />log-apartment prices across England and Wales. The results for the Standardized mean<br />squared error (SMSE) and mean standardized log loss (MSLL) were obtained by 10-fold<br />cross-validation, where the shown values are the mean (standard deviation parenthesised).<br />Obtaining the reduced-rank result (including initialization and hyperparameter learning)<br />took approximately 50 seconds on a laptop computer (MacBook Air, Late 2010 model,<br />2.13 GHz, 4 GB RAM), which scales with n in comparison to the evaluation time in the<br />previous section.<br />6.5 Apartment Price Data<br />In order to fully use the computational benefits of the method, we consider a large dataset.<br />We use records of sold apartments3in the UK for the period of February to October 2013.<br />The data consist of n = 102890 records for apartments, which were cross-referenced against<br />a postcode database to get the geographical coordinates on which the normalized logarithmic<br />prices were regressed. The dataset is similar to that used in Hensman et al. (2013), where<br />the records were for year 2012.<br />To account for both the national and regional variations in apartment prices, we used<br />two squared exponential covariance functions with different length-scales and magnitudes.<br />Additionally, a Gaussian noise term captures the variation that is not related to location<br />alone. Applying the reduced-rank methodology to a sum of covariances is straight-forward,<br />as the the kernel approximations share basis functions and only the spectra have to be<br />summed.<br />To validate the results, because the full GP solution is infeasible, we used the subset<br />of data approach as was done in Hensman et al. (2013). We solved the full GP problem<br />by considering subsets of n = 500 and n = 1000 data points randomly chosen from the<br />training set. For each fold in the cross-validation the results were averaged over ten choices<br />of subsets. The rank of the reduced-rank approximation was fixed at m = 1000 in order to<br />match with the larger of the two subsets.<br />Table 1 shows the SMSE and MSLL values for the apartment data. The results show<br />that the reduced rank method. The results show that the proposed method gives good<br />results in terms of both SMSE and MSLL, and the standard deviation between the folds is<br />also small. In this case the reduced-rank result (including initialization and hyperparameter<br />learning) took approximately 130 seconds on a laptop computer (MacBook Air, Late 2010<br />model, 2.13 GHz, 4 GB RAM).<br />3. The data are available from http://data.gov.uk/dataset/land-registry-monthly-price-paid-data/<br />(accessed January 6, 2014).<br />22</p>  <p>Page 23</p> <p>Hilbert Space Methods for Reduced-Rank Gaussian Process Regression<br />7. Conclusion and Discussion<br />In this paper we have proposed a novel approximation scheme for forming approximate<br />eigendecompositions of covariance functions in terms of the Laplace operator eigenbasis<br />and the spectral density of the covariance function. The eigenfunction decomposition of the<br />Laplacian can easily be formed in various domains, and the eigenfunctions are independent<br />of the choice of hyperparameters of the covariance.<br />An advantage of the method is that it has the ability to approximate the eigendecom-<br />position using only the eigendecomposition of the Laplacian and the spectral density of the<br />covariance function, both of which are closed-from expressions. This together with having<br />the eigenvectors in Φ mutually orthogonal and independent of the hyperparameters, is the<br />key to efficiency. This allows an implementation with a computational cost of O(nm2)<br />(initial) and O(m3) (marginal likelihood evaluation), with negligible memory requirements.<br />Of the infinite number of possible basis functions only an extremely small subset are<br />of any relevance to the GP being approximated. In GP regression the model functions<br />are conditioned on a covariance function (kernel), which imposes desired properties on the<br />solutions. We choose the basis functions such that they are as close as possible (w.r.t. the<br />Frobenius norm) to those of the particular covariance function. Our method gives the exact<br />eigendecomposition of a GP that has been constrained to be zero at the boundary of the<br />domain.<br />The method allows for theoretical analysis of the error induced by the truncation of<br />the series and the boundary effects. This is something new in this context and extremely<br />important, for example, in medical imaging applications. The approximative eigendecom-<br />position also opens a range of interesting possibilities for further analysis. In learning curve<br />estimation, the eigenvalues of the Gaussian process can now be directly approximated. For<br />example, we can approximate the Opper–Vivarelli bound (Opper and Vivarelli, 1999) as<br />?OV(n) ≈ σ2<br />n<br />?<br />j<br />S(?λj)<br />σ2<br />n+ nS(?λj).(64)<br />Sollich’s eigenvalue based bounds (Sollich and Halees, 2002) can be approximated and an-<br />alyzed in an analogous way.<br />However, some of these abilities come with a cost. As demonstrated throughout the<br />paper, restraining the domain to boundary conditions introduces edge effects. These are,<br />however, known and can be accounted for. Extrapolating with a stationary covariance<br />function outside the training inputs only causes the predictions to revert to the prior mean<br />and variance. Therefore we consider the boundary effects a minor problem for practical use.<br />A more severe limitation for applicability is the ‘full’ nature of the spectrum.<br />high-dimensional inputs the required number of basis functions grows large. There is, how-<br />ever, a substantial call for GPs in low-dimensional problems, especially in medical imaging<br />applications (typical number of training data points in millions) and spatial problems. Fur-<br />thermore, the mathematical formulation of the method provides a foundation for future<br />sparse methods to build upon. A step in this direction has been taken by L´ azaro-Gredilla<br />et al. (2010), which has shown good results in high-dimensional input spaces.<br />For<br />23</p>  <p>Page 24</p> <p>Solin and S¨ arkk¨ a<br />Appendix A. Proofs of Convergence Theorems<br />A.1 Proof of Theorem 4.1<br />The Wiener–Khinchin identity and the symmetry of the spectral density allows us to write<br />?∞<br />=1<br />π<br />0<br />k(x,x?) =<br />1<br />2π<br />−∞<br />S(ω) exp(−iω (x − x?))dω<br />?∞<br />S(ω) cos(ω (x − x?))dω.<br />(65)<br />In a one-dimensional domain Ω = [−L,L] with Dirichet boundary conditions we have an<br />m-term approximation of the form<br />?π j<br />?km(x,x?) =<br />m<br />?<br />j=1<br />S<br />2L<br />?<br />1<br />Lsin<br />?π j (x + L)<br />2L<br />?<br />sin<br />?π j (x?+ L)<br />2L<br />?<br />.<br />(66)<br />We start by showing the convergence by growing the domain and therefore first consider an<br />approximation with an infinite number of terms m = ∞:<br />∞<br />?<br />Lemma A.1. There exists a constant D1such that for all x,x?∈ [−?L,?L] we have<br />?<br />−1<br />?k∞(x,x?) =<br />j=1<br />S(?λj)φj(x)φj(x?). (67)<br />?????<br />∞<br />j=1<br />S<br />?π j<br />2L<br />?<br />1<br />Lsin<br />?π j (x + L)<br />2L<br />?<br />sin<br />?π j (x?+ L)<br />2L<br />?<br />π<br />?∞<br />0<br />S(ω) cos(ω (x − x?))dω<br />?????≤D1<br />L.<br />(68)<br />That is,<br />????k∞(x,x?) − k(x,x?)<br />?π j (x + L)<br />?π j (x − x?)<br />?π 2j<br />?π (2j − 1)<br />??? ≤D1<br />?π j (x?+ L)<br />1<br />2L<br />?π (2j − 1)<br />? ?<br />L,<br />for x,x?∈ [−?L,?L].<br />?<br />(69)<br />Proof. We can rewrite the summation in (68) as<br />∞<br />?<br />j=1<br />S<br />?π j<br />∞<br />?<br />2L<br />?<br />?π j<br />∞<br />?<br />∞<br />?<br />1<br />Lsin<br />?<br />?<br />2L<br />?<br />sin<br />2L<br />(70)<br />=<br />j=1<br />S<br />2L<br />cos<br />2L<br />?<br />−<br />1<br />2L<br />j=1<br />S<br />2L<br />?<br />− S<br />2L<br />?π 2j (x + x?)<br />??<br />cos<br />?π 2j (x + x?)<br />?<br />2L<br />?<br />−<br />1<br />2L<br />j=1<br />S<br />2L<br />cos<br />2L<br />− cos<br />?π (2j − 1)(x + x?)<br />2L<br />??<br />(71)<br />24</p>  <p>Page 25</p> <p>Hilbert Space Methods for Reduced-Rank Gaussian Process Regression<br />First consider the first term above in Equation (71). Let ∆ =<br />to have the form<br />π<br />2L, and thus it can be seen<br />1<br />π<br />∞<br />?<br />j=1<br />S (∆j) cos?∆j (x − x?)?∆,<br />(72)<br />which can be recognized as a Riemannian sum approximation to the integral1<br />x?))dω. Because we assume that x,x?∈ [−?L,?L], the integrand and its derivatives are<br />and hence we conclude that<br />π<br />?∞<br />0S(ω) cos(ω (x−<br />bounded and because the integral?∞<br />?????<br />for some constant D2.<br />The second summation term in Equation (71) can also be interpreted as a Riemann sum<br />if we set ∆ =π<br />L:<br />−∞S(ω)dω &lt; ∞, the Riemannian integral converges,<br />∞<br />?<br />j=1<br />S<br />?π j<br />2L<br />?<br />cos<br />?π j (x − x?)<br />2L<br />?<br />1<br />2L−1<br />π<br />?∞<br />0<br />S(ω) cos(ω (x − x?))dω<br />?????≤D2<br />L<br />(73)<br />1<br />2L<br />∞<br />?<br />j=1<br />[S (∆j) − S (∆j − ∆/2)] cos?∆(x + x?)?<br />∞<br />?<br />?∞<br />=<br />1<br />2L<br />j=1<br />1<br />∆[S (∆j) − S (∆j − ∆/2)] cos?∆(x + x?)?∆<br />2S?(ω) cos(ω (x + x?))dω.<br />≈<br />1<br />2L<br />0<br />(74)<br />Because we assumed that also the second derivative of S(·) is bounded, the derivative and<br />the Riemann sum converge (alternatively, we could analyze the sums as a Stieltjes integral<br />with respect to a differentiable function), and hence the exists a constant D?<br />3such that<br />?????<br />1<br />2L<br />∞<br />?<br />j=1<br />[S (∆j) − S (∆j − ∆/2)] cos?∆(x + x?)?<br />−<br />1<br />2L<br />?∞<br />0<br />2S?(ω) cos(ω (x + x?))dω<br />?????≤D?<br />3<br />L<br />(75)<br />But now because?∞<br />02S?(ω) cos(ω (x + x?))dω &lt; ∞, this actually implies that<br />?????<br />1<br />2L<br />∞<br />?<br />j=1<br />?<br />S<br />?π 2j<br />2L<br />?<br />− S<br />?π (2j − 1)<br />2L<br />??<br />cos<br />?π 2j (x + x?)<br />2L<br />??????≤D3<br />L<br />(76)<br />25</p>  <p>Page 26</p> <p>Solin and S¨ arkk¨ a<br />for some constant D3. For the last summation term in Equation (71) we get the interpre-<br />tation<br />?π (2j − 1)<br />1<br />2L<br />0<br />= −1<br />2L<br />0<br />1<br />2L<br />∞<br />?<br />≈<br />j=1<br />S<br />2L<br />? ?<br />?<br />cos<br />?π 2j (x + x?)<br />2L<br />?<br />− cos<br />?π (2j − 1)(x + x?)<br />2L<br />??<br />?∞<br />?∞<br />S(ω)2<br />d<br />dωcos(ω (x + x?))<br />?<br />dω<br />S(ω)2(x + x?) sin(ω (x + x?))dω,(77)<br />which by boundedness of x and x?implies<br />?????<br />for some constant D4. The result now follows by combining (73), (76), and (78) via the<br />triangle inequality.<br />1<br />2L<br />∞<br />?<br />j=1<br />S<br />?π (2j − 1)<br />2L<br />? ?<br />cos<br />?π 2j (x + x?)<br />2L<br />?<br />− cos<br />?π (2j − 1)(x + x?)<br />2L<br />???????≤D4<br />L<br />(78)<br />Let us now return to the original question, and consider what happens when we replace<br />the infinite sum approximation with a finite m number of terms. We are now interested in<br />?k∞(x,x?) −?km(x,x?) =<br />Lemma A.2. There exists a constant D5such that for all x,x?∈ [−?L,?L] we have<br />∞<br />?<br />j=m+1<br />S<br />?π j<br />2L<br />?<br />1<br />Lsin<br />?π j (x + L)<br />2L<br />?<br />sin<br />?π j (x?+ L)<br />2L<br />?<br />. (79)<br />????k∞(x,x?) −?km(x,x?)<br />??? ≤D5<br />L<br />+2<br />π<br />?∞<br />2L<br />π m<br />S(ω)dω.(80)<br />Proof. Because the sinusoidals are bounded by unity, we get<br />??????<br />The right-hand term can now be seen as Riemann sum approximation to the integral<br />?π j<br />∞<br />?<br />j=m+1<br />S<br />?π j<br />2L<br />?<br />1<br />Lsin<br />?π j (x + L)<br />2L<br />?<br />sin<br />?π j (x?+ L)<br />2L<br />???????<br />≤<br />??????<br />∞<br />?<br />j=m+1<br />S<br />?π j<br />2L<br />?<br />1<br />L<br />??????<br />.(81)<br />∞<br />?<br />j=m+1<br />S<br />2L<br />?<br />1<br />L≈2<br />π<br />?∞<br />2L<br />π m<br />S(ω)dω. (82)<br />Our assumptions ensure that this integral converges and hence there exists a constant D5<br />such that<br />?????<br />26<br />∞<br />?<br />j=m+1<br />S<br />?π j<br />2L<br />?<br />1<br />L−2<br />π<br />?∞<br />2L<br />π m<br />S(ω)dω<br />?????≤D5<br />L.<br />(83)</p>  <p>Page 27</p> <p>Hilbert Space Methods for Reduced-Rank Gaussian Process Regression<br />Hence by the triangle inequality we get<br />?????<br />∞<br />?<br />j=m+1<br />S<br />?π j<br />2L<br />?<br />1<br />L<br />?????=<br />≤<br />?????<br />∞<br />?<br />∞<br />?<br />+2<br />j=m+1<br />S<br />?π j<br />?π j<br />?∞<br />2L<br />2L<br />?<br />?<br />1<br />L−2<br />π<br />?∞<br />2L<br />?∞<br />2L<br />π m<br />S(ω)dω +2<br />π<br />?∞<br />2L<br />?∞<br />π m<br />S(ω)dω<br />?????<br />?????<br />j=m+1<br />S<br />2L<br />1<br />L−2<br />π<br />π m<br />S(ω)dω<br />?????+2<br />π<br />π m<br />2L<br />S(ω)dω<br />≤D5<br />Lπ<br />π m<br />S(ω)dω (84)<br />and thus the result follows.<br />The above result can now easily be combined to a proof of the one-dimensional conver-<br />gence theorem as follows:<br />Proof of Theorem 4.1. The first result follows by combining Lemmas A.1 and A.2 via the<br />triangle inequality. Because our assumptions imply that<br />?∞<br />lim<br />x→∞<br />x<br />S(ω)dω = 0,(85)<br />for any fixed L we have<br />lim<br />m→∞<br />?<br />E<br />L+2<br />π<br />?∞<br />2L<br />π m<br />S(ω)dω<br />?<br />→E<br />L.<br />(86)<br />If we now take the limit L → ∞, the second result in the theorem follows.<br />A.2 Proof of Theorem 4.4<br />When x ∈ Rd, the Wiener–Khinchin identity and symmetry of the spectral density imply<br />that<br />?<br />1<br />πd<br />00<br />k=1<br />k(x,x?) =<br />1<br />(2π)d<br />RdS(ω) exp(−iωT(x − x?))dω<br />?∞<br />=<br />?∞<br />···<br />S(ω)<br />d?<br />cos(ωk(xk− x?<br />k))dω1··· dωd.<br />(87)<br />The m = ˆ mdterm approximation now has the form<br />?km(x,x?) =<br />As in the one-dimensional problem we start by considering the case where ˆ m = ∞.<br />ˆ m<br />?<br />j1,...,jd=1<br />S<br />?π j1<br />2L1,...,π jd<br />2Ld<br />?<br />d?<br />k=1<br />1<br />Lk<br />sin<br />?π jk(xk+ Lk)<br />2Lk<br />?<br />sin<br />?π jk(x?<br />k+ Lk)<br />2Lk<br />?<br />.<br />(88)<br />27</p>  <p>Page 28</p> <p>Solin and S¨ arkk¨ a<br />Lemma A.3. There exists a constant D1such that for all x,x?∈ [−?L,?L]dwe have<br />?<br />1<br />πd<br />00<br />k=1<br />?????<br />∞<br />j1,...,jd=1<br />S<br />?π j1<br />2L1,...,π jd<br />2Ld<br />?<br />?∞<br />d?<br />k=1<br />1<br />Lk<br />sin<br />?π jk(xk+ Lk)<br />d?<br />2Lk<br />?<br />sin<br />?π jk(x?<br />k+ Lk)<br />2Lk<br />?????≤ D1<br />?<br />d<br />?<br />−<br />?∞<br />···<br />S(ω)<br />cos(ωk(x − x?))dω1··· dωd<br />k=1<br />1<br />Lk.(89)<br />That is,<br />????k∞(x,x?) − k(x,x?)<br />??? ≤ D1<br />d<br />?<br />k=1<br />1<br />Lk<br />for x,x?∈ [−?L,?L]d. (90)<br />Proof. We can separate the summation over j1in the summation term above as follows:<br />?<br />j1=1<br />∞<br />?<br />j2,...,jd=1<br />∞<br />?<br />S<br />?π j1<br />2L1,...,π jd<br />2Ld<br />?<br />d?<br />sin<br />?π j1(x1+ L1)<br />?π jk(xk+ Lk)<br />2L1<br />?<br />sin<br />?π j1(x?<br />?<br />1+ L1)<br />2L1<br />?π jk(x?<br />??<br />×<br />k=2<br />1<br />Lk<br />sin<br />2Lk<br />sin<br />k+ Lk)<br />2Lk<br />?<br />.(91)<br />By Lemma A.1 there now exists a constant D1,1such that<br />?????<br />−1<br />π<br />0<br />∞<br />?<br />j1=1<br />S<br />?π j1<br />2L1,...,π jd<br />2Ld<br />?<br />sin<br />?π j1(x1+ L1)<br />?∞<br />2L1<br />?<br />sin<br />?π j1(x?<br />?<br />1+ L1)<br />2L1<br />?<br />S<br />?<br />ω1,π j2<br />2L2,...,π jd<br />2Ld<br />cos(ω1(x1− x?<br />1))dω1<br />?????≤D1,1<br />L1<br />.(92)<br />The triangle inequality then gives<br />?????<br />∞<br />?<br />j1,...,jd=1<br />S<br />?π j1<br />2L1,...,π jd<br />2Ld<br />?<br />d?<br />?∞<br />?<br />?π jk(xk+ Lk)<br />?∞<br />k=1<br />1<br />Lk<br />sin<br />?π jk(xk+ Lk)<br />d?<br />2L2,...,π jd<br />2Ld<br />?<br />d?<br />2Lk<br />?<br />sin<br />?π jk(x?<br />k+ Lk)<br />2Lk<br />?????<br />1))dω1<br />?<br />−<br />1<br />πd<br />?∞<br />0<br />···<br />0<br />S(ω)<br />k=1<br />cos(ωj(xk− x?<br />?<br />?π jk(x?<br />k))dω1··· dωd<br />(93)<br />≤D1,1<br />L1<br />+<br />?????<br />1<br />π<br />∞<br />?<br />d?<br />1<br />πd<br />j2,...,jd=1<br />?∞<br />0<br />Sω1,π j2<br />cos(ω1(x1− x?<br />×<br />k=2<br />1<br />Lk<br />?∞<br />sin<br />2Lk<br />sin<br />k+ Lk)<br />2Lk<br />?<br />−<br />0<br />···<br />0<br />S(ω)<br />k=1<br />cos(ωk(xk− x?<br />k))dω1··· dωd<br />?????.(94)<br />We can now similarly bound with respect to the summations over j2,...,jdwhich leads to a<br />bound of the form<br />Ld. Taking D1= maxkD1,kleads to the desired result.<br />D1,1<br />L1+···+D1,d<br />28</p>  <p>Page 29</p> <p>Hilbert Space Methods for Reduced-Rank Gaussian Process Regression<br />Now we can consider what happens in the finite truncation of the series. That is, we<br />analyze the following residual sum<br />?k∞(x,x?) −?km(x,x?)<br />=<br />S<br />2Ld<br />k=1<br />Lk<br />∞<br />?<br />j1,...,jd= ˆ m+1<br />?π j1<br />2L1,...,π jd<br />?<br />d?<br />1<br />sin<br />?π jk(xk+ Lk)<br />2Lk<br />?<br />sin<br />?π jk(x?<br />k+ Lk)<br />2Lk<br />?<br />(95)<br />.<br />Lemma A.4. The exists a constant D2such that for all x,x?∈ [−?L,?L]dwe have<br />????k∞(x,x?) −?km(x,x?)<br />??? ≤D2<br />L<br />+<br />1<br />πd<br />?<br />?ω?≥π ˆ m<br />2L<br />S(ω)dω,<br />(96)<br />where L = minkLk.<br />Proof. We can write the following bound<br />?????<br />≤<br />∞<br />?<br />j1,...,jd= ˆ m+1<br />S<br />?π j1<br />?????<br />π<br />2L1giving<br />?π j1<br />2L1,...,π jd<br />2Ld<br />?<br />d?<br />?π j1<br />k=1<br />1<br />Lk<br />sin<br />?π jk(xk+ Lk)<br />?<br />k=1<br />2Lk<br />?<br />sin<br />?π jk(x?<br />k+ Lk)<br />2Lk<br />??????<br />∞<br />?<br />j1,...,jd= ˆ m+1<br />S<br />2L1,...,π jd<br />2Ld<br />d?<br />1<br />Lk<br />?????.(97)<br />The summation over the index j1can now be interpreted as a Riemann integral approxi-<br />mation with ∆ =<br />?????<br />−2<br />π<br />j2,...,jd= ˆ m+1<br />2L1<br />∞<br />?<br />j1,...,jd= ˆ m+1<br />S<br />2L1,...,π jd<br />2Ld<br />?<br />d?<br />k=1<br />1<br />Lk<br />∞<br />?<br />?∞<br />π ˆ m<br />S<br />?<br />ω1,π j2<br />2L2,...,π jd<br />2Ld<br />?<br />dω1<br />d?<br />k=2<br />1<br />Lk<br />?????≤D2,1<br />L1<br />.(98)<br />Using a similar argument again, we get<br />?????<br />π2<br />j3,...,jd= ˆ m+1<br />2<br />π<br />∞<br />?<br />j2,...,jd= ˆ m+1<br />?∞<br />2L1<br />∞<br />?<br />π ˆ m<br />S<br />?<br />?∞<br />2L1<br />ω1,π j2<br />2L2,...,π jd<br />2Ld<br />?<br />dω1<br />d?<br />k=2<br />1<br />Lk<br />−22<br />π ˆ m<br />?∞<br />2L2<br />π ˆ m<br />S<br />?<br />ω1,ω2,π j3<br />2L3,...,π jd<br />2Ld<br />?<br />dω1dω2<br />d?<br />k=3<br />1<br />Lk<br />?????≤D2,2<br />L2<br />. (99)<br />After repeating this for all the indexes, by forming a telescoping sum of the terms and<br />applying the triangle inequality then gives<br />?????<br />−<br />π<br />π ˆ m<br />2L1<br />2Ld<br />∞<br />?<br />j1,...,jd= ˆ m+1<br />S<br />?π j1<br />2L1,...,π jd<br />2Ld<br />?d?∞<br />?<br />d?<br />k=1<br />1<br />Lk<br />?2<br />···<br />?∞<br />π ˆ m<br />S(ω1,...,ωd)dω1··· dωd<br />?????≤<br />d<br />?<br />k=1<br />D2,k<br />Lk<br />.(100)<br />29</p>  <p>Page 30</p> <p>Solin and S¨ arkk¨ a<br />Applying the triangle inequality again gives<br />?????<br />∞<br />?<br />j1,...,jd= ˆ m+1<br />S<br />?π j1<br />2L1,...,π jd<br />2Ld<br />?<br />d?<br />k=1<br />1<br />Lk<br />?2<br />?????<br />≤<br />d<br />?<br />k=1<br />D2,k<br />Lk<br />+<br />π<br />?d?∞<br />π ˆ m<br />2L1<br />···<br />?∞<br />2Ld<br />π ˆ m<br />S(ω1,...,ωd)dω1··· dωd.(101)<br />By interpreting the latter integral as being over the positive exterior of a rectangular hy-<br />percuboid and bounding it by a integral over exterior of a hypersphere which fits inside the<br />cuboid, we can bound the expression by<br />d<br />?<br />k=1<br />D2,k<br />Lk<br />+<br />1<br />πd<br />?<br />?ω?≥π ˆ m<br />2L<br />S(ω)dω.(102)<br />The first term can be further bounded by replacing Lks with their minimum L and by<br />defining a new constant D2which is d times the maximum of D2,k. This leads to the final<br />form of the result.<br />Proof of Theorem 4.4. Analogous to the one-dimensional case. That is, we combine the<br />results of the above lemmas using the triangle inequality.<br />References<br />Naum I. Akhiezer and Izrail’ M. Glazman. Theory of Linear Operators in Hilbert Space.<br />Dover, New York, 1993.<br />Christopher T. H. Baker. The Numerical Treatment of Integral Equations. Clarendon press,<br />Oxford, 1977.<br />Steve Brooks, Andrew Gelman, Galin L. Jones, and Xiao-Li Meng. Handbook of Markov<br />Chain Monte Carlo. Chapman &amp; Hall/CRC, 2011.<br />Krzysztof Chalupka, Christopher K. I. Williams, and Iain Murray. A framework for evaluat-<br />ing approximation methods for Gaussian process regression. Journal of Machine Learning<br />Research, 14:333–350, 2013.<br />Richard Courant and David Hilbert. Methods of Mathematical Physics, volume 1. Wiley-<br />VCH, 2008.<br />Lehel Csat´ o and Manfred Opper. Sparse online Gaussian processes. Neural Computation,<br />14(3):641–668, 2002.<br />Giuseppe Da Prato and Jerzy Zabczyk. Stochastic Equations in Infinite Dimensions, vol-<br />ume 45 of Encyclopedia of Mathematics and its Applications. Cambridge University Press,<br />1992.<br />30</p>  <p>Page 31</p> <p>Hilbert Space Methods for Reduced-Rank Gaussian Process Regression<br />Simon Duane, Anthony D. Kennedy, Brian J. Pendleton, and Duncan Roweth. Hybrid<br />Monte Carlo. Physics Letters B, 195(2):216–222, 1987.<br />Jochen Fritz, Insa Neuweiler, and Wolfgang Nowak. Application of FFT-based algorithms<br />for large-scale universal kriging problems.<br />2009.<br />Mathematical Geosciences, 41(5):509–533,<br />Gene H. Golub and Charles F. Van Loan. Matrix Computations. Johns Hopkins University<br />Press, Baltimore, third edition, 1996.<br />James Hensman, Nicol` o Fusi, and Neil D. Lawrence. Gaussian processes for big data. In<br />Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence (UAI 2013),<br />pages 282–290, 2013.<br />Miguel L´ azaro-Gredilla. Sparse Gaussian Processes for Large-Scale Machine Learning. PhD<br />thesis, Universidad Carlos III de Madrid, 2010.<br />Miguel L´ azaro-Gredilla, Joaquin Qui˜ nonero-Candela, Carl Edward Rasmussen, and<br />An´ ıbal R. Figueiras-Vidal. Sparse spectrum Gaussian process regression. Journal of<br />Machine Learning Research, 11:1865–1881, 2010.<br />Peter J. Lenk. Towards a practicable Bayesian nonparametric density estimator. Biometrika,<br />78(3):531–543, 1991.<br />Finn Lindgren, H˚ avard Rue, and Johan Lindstr¨ om. An explicit link between Gaussian<br />fields and Gaussian Markov random fields: The stochastic partial differential equation<br />approach. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73<br />(4):423–498, 2011.<br />Jun S. Liu. Monte Carlo Strategies in Scientific Computing. Springer, New York, 2001.<br />Radford M. Neal. MCMC using Hamiltonian dynamics. In Steve Brooks, Andrew Gelman,<br />Galin L. Jones, and Xiao-Li Meng, editors, Handbook of Markov Chain Monte Carlo,<br />chapter 5. Chapman &amp; Hall/CRC, 2011.<br />Manfred Opper and Francesco Vivarelli. General bounds on Bayes errors for regression with<br />Gaussian processes. In Advances in Neural Information Processing Systems, volume 11,<br />pages 302–308, 1999.<br />Christopher J. Paciorek. Bayesian smoothing with Gaussian processes using Fourier basis<br />functions in the spectralGP package. Journal of Statistical Software, 19(2):1–38, 2007.<br />Joaquin Qui˜ nonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approx-<br />imate Gaussian process regression. Journal of Machine Learning Research, 6:1939–1959,<br />2005a.<br />Joaquin Qui˜ nonero-Candela and Carl Edward Rasmussen. Analysis of some methods for<br />reduced rank Gaussian process regression. In Switching and Learning in Feedback Systems,<br />volume 3355 of Lecture Notes in Computer Science, pages 98–127. Springer, 2005b.<br />31</p>  <p>Page 32</p> <p>Solin and S¨ arkk¨ a<br />Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine<br />Learning. The MIT Press, 2006.<br />Simo S¨ arkk¨ a and Jouni Hartikainen. Infinite-dimensional Kalman filtering approach to<br />spatio-temporal Gaussian process regression. In Proceedings of the Fifteenth International<br />Conference on Artificial Intelligence and Statistics (AISTATS 2012), volume 22 of JMLR<br />Workshop and Conference Proceedings, pages 993–1001, 2012.<br />Simo S¨ arkk¨ a, Arno Solin, and Jouni Hartikainen. Spatiotemporal learning via infinite-<br />dimensional Bayesian filtering and smoothing. IEEE Signal Processing Magazine, 30(4):<br />51–61, 2013.<br />Matthias Seeger, Christopher K. I. Williams, and Neil D. Lawrence. Fast forward selection<br />to speed up sparse Gaussian process regression. In Proceedings of the 9th International<br />Workshop on Artificial Intelligence and Statistics (AISTATS 2003), 2003.<br />Ralph E. Showalter. Hilbert Space Methods in Partial Differential Equations. Dover Publi-<br />cations, 2010.<br />Mikhail A. Shubin. Pseudodifferential Operators and Spectral Theory. Springer Series in<br />Soviet Mathematics. Springer-Verlag, 1987.<br />Alex J. Smola and Peter Bartlett. Sparse greedy Gaussian process regression. In Advances<br />in Neural Information Processing Systems, volume 13, 2001.<br />Edward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs.<br />In Advances in Neural Information Processing Systems, volume 18, pages 1259–1266,<br />2006.<br />Peter Sollich and Anason Halees. Learning curves for Gaussian process regression: Approx-<br />imations and bounds. Neural Computation, 14(6):1393–1428, 2002.<br />Michalis K. Titsias. Variational learning of inducing variables in sparse Gaussian processes.<br />In Proceedings of the 12th International Conference on Artificial Intelligence and Statis-<br />tics (AISTATS 2009), volume 5 of JMLR Workshop and Conference Proceedings, pages<br />567–574, 2009.<br />Jarno Vanhatalo, Ville Pietil¨ ainen, and Aki Vehtari. Approximate inference for disease<br />mapping with sparse Gaussian processes. Statistics in Medicine, 29(15):1580–1607, 2010.<br />Jarno Vanhatalo, Jaakko Riihim¨ aki, Jouni Hartikainen, Pasi Jyl¨ anki, Ville Tolvanen, and<br />Aki Vehtari. GPstuff: Bayesian modeling with Gaussian processes. Journal of Machine<br />Learning Research, 14:1175–1179, 2013.<br />Christopher K. I. Williams and Matthias Seeger. The effect of the input density distribu-<br />tion on kernel-based classifiers. In Proceedings of the 17th International Conference on<br />Machine Learning, 2000.<br />Christopher K. I. Williams and Matthias Seeger. Using the Nystr¨ om method to speed up<br />kernel machines. In Advances in Neural Information Processing Systems, volume 13,<br />2001.<br />32</p>  <a href="https://www.researchgate.net/profile/Simo_Saerkkae/publication/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression/links/54a3dc790cf256bf8bb17c1c.pdf">Download full-text</a> </div> <div id="rgw18_56ab1df2a6baa" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw19_56ab1df2a6baa">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw20_56ab1df2a6baa"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Simo_Saerkkae/publication/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression/links/54a3dc790cf256bf8bb17c1c.pdf" class="publication-viewer" title="54a3dc790cf256bf8bb17c1c.pdf">54a3dc790cf256bf8bb17c1c.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Simo_Saerkkae">Simo Särkkä</a> &middot; Dec 31, 2014 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw21_56ab1df2a6baa"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://de.arxiv.org/pdf/1401.5508" target="_blank" rel="nofollow" class="publication-viewer" title="Hilbert Space Methods for Reduced-Rank Gaussian Process Regression">Hilbert Space Methods for Reduced-Rank Gaussian Pr...</a> </div>  <div class="details">   Available from <a href="http://de.arxiv.org/pdf/1401.5508" target="_blank" rel="nofollow">de.arxiv.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw28_56ab1df2a6baa" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (3) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw29_56ab1df2a6baa" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   "  id="rgw30_56ab1df2a6baa" >  <div class="indent-left">  <div id="rgw31_56ab1df2a6baa" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/277959163_Computationally_Efficient_Bayesian_Learning_of_Gaussian_Process_State_Space_Models">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Thomas_Schoen3" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Thomas B. Schön </div> </div>   </div>  </div>  <div class="indent-right">      </div>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/277959163_Computationally_Efficient_Bayesian_Learning_of_Gaussian_Process_State_Space_Models"> <span class="publication-title js-publication-title">Computationally Efficient Bayesian Learning of Gaussian Process State Space Models</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2066255729_Andreas_Svensson" class="authors js-author-name ga-publications-authors">Andreas Svensson</a> &middot;     <a href="researcher/71647262_Arno_Solin" class="authors js-author-name ga-publications-authors">Arno Solin</a> &middot;     <a href="researcher/69762528_Simo_Saerkkae" class="authors js-author-name ga-publications-authors">Simo Särkkä</a> &middot;     <a href="researcher/35821922_Thomas_B_Schoen" class="authors js-author-name ga-publications-authors">Thomas B. Schön</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Gaussian processes allow for flexible specification of prior assumptions of unknown dynamics in state space models. We present a procedure for efficient Bayesian learning in Gaussian process state space models, where the representation is formed by projecting the problem onto a set of approximate eigenfunctions derived from the prior covariance structure. Learning under this family of models can be conducted using a carefully crafted particle MCMC algorithm. This scheme is computationally efficient and yet allows for a fully Bayesian treatment of the problem. Compared to conventional system identification tools or existing learning methods, we show competitive performance and reliable quantification of uncertainties in the model. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Jun 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Thomas_Schoen3/publication/277959163_Computationally_Efficient_Bayesian_Learning_of_Gaussian_Process_State_Space_Models/links/559bf2c708ae0035df233e68.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   "  id="rgw32_56ab1df2a6baa" >  <div class="indent-left">  <div id="rgw33_56ab1df2a6baa" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Maurizio_Filippone" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Maurizio Filippone </div> </div>   </div>  </div>  <div class="indent-right">      </div>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes"> <span class="publication-title js-publication-title">MCMC for Variationally Sparse Gaussian Processes</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/71410443_James_Hensman" class="authors js-author-name ga-publications-authors">James Hensman</a> &middot;     <a href="researcher/2048369253_Alexander_G_de_G_Matthews" class="authors js-author-name ga-publications-authors">Alexander G. de G. Matthews</a> &middot;     <a href="researcher/70871340_Maurizio_Filippone" class="authors js-author-name ga-publications-authors">Maurizio Filippone</a> &middot;     <a href="researcher/8159937_Zoubin_Ghahramani" class="authors js-author-name ga-publications-authors">Zoubin Ghahramani</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Gaussian process (GP) models form a core part of probabilistic machine
learning. Considerable research effort has been made into attacking three
issues with GP models: how to compute efficiently when the number of data is
large; how to approximate the posterior when the likelihood is not Gaussian and
how to estimate covariance function parameter posteriors. This paper
simultaneously addresses these, using a variational approximation to the
posterior which is sparse in support of the function but otherwise free-form.
The result is a Hybrid Monte-Carlo sampling scheme which allows for a
non-Gaussian approximation over the function values and covariance parameters
simultaneously, with efficient computations based on inducing-point sparse GPs.
Code to replicate each experiment in this paper will be available shortly. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Jun 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Maurizio_Filippone/publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes/links/558a857008aee1fc9174ea84.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   "  id="rgw34_56ab1df2a6baa" >  <div class="indent-left">  <div id="rgw35_56ab1df2a6baa" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/283335187_Blitzkriging_Kronecker-structured_Stochastic_Gaussian_Processes">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1510.07965" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: de.arxiv.org </div> </div>   </div>  </div>  <div class="indent-right">      </div>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/283335187_Blitzkriging_Kronecker-structured_Stochastic_Gaussian_Processes"> <span class="publication-title js-publication-title">Blitzkriging: Kronecker-structured Stochastic Gaussian Processes</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2052101055_Thomas_Nickson" class="authors js-author-name ga-publications-authors">Thomas Nickson</a> &middot;     <a href="researcher/2051950921_Tom_Gunter" class="authors js-author-name ga-publications-authors">Tom Gunter</a> &middot;     <a href="researcher/2051935693_Chris_Lloyd" class="authors js-author-name ga-publications-authors">Chris Lloyd</a> &middot;     <a href="researcher/2051939493_Michael_A_Osborne" class="authors js-author-name ga-publications-authors">Michael A Osborne</a> &middot;     <a href="researcher/2083725021_Stephen_Roberts" class="authors js-author-name ga-publications-authors">Stephen Roberts</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> We present Blitzkriging, a new approach to fast inference for Gaussian
processes, applicable to regression, optimisation and classification.
State-of-the-art (stochastic) inference for Gaussian processes on very large
datasets scales cubically in the number of &#39;inducing inputs&#39;, variables
introduced to factorise the model. Blitzkriging shares state-of-the-art scaling
with data, but reduces the scaling in the number of inducing points to
approximately linear. Further, in contrast to other methods, Blitzkriging: does
not force the data to conform to any particular structure (including
grid-like); reduces reliance on error-prone optimisation of inducing point
locations; and is able to learn rich (covariance) structure from the data. We
demonstrate the benefits of our approach on real data in regression,
time-series prediction and signal-interpolation experiments. </span> </div>    <div class="publication-meta publication-meta">  <span class="ico-publication-preview reset-background"></span> Preview    &middot; Article &middot; Oct 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  </ul>    <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw23_56ab1df2a6baa" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw24_56ab1df2a6baa">  </ul> </div> </div>   <div id="rgw14_56ab1df2a6baa" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw15_56ab1df2a6baa"> <div> <h5> <a href="publication/247114955_Partial_Difierential_Equations_and_Hilbert_Space_Methods" class="color-inherit ga-similar-publication-title"><span class="publication-title">Partial Difierential Equations and Hilbert Space Methods</span></a>  </h5>  <div class="authors"> <a href="researcher/2019303243_K_E_Gustafson" class="authors ga-similar-publication-author">K. E. Gustafson</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw16_56ab1df2a6baa"> <div> <h5> <a href="publication/265370475_Hilbert_space_methods_in_science_and_engineering" class="color-inherit ga-similar-publication-title"><span class="publication-title">Hilbert space methods in science and engineering</span></a>  </h5>  <div class="authors"> <a href="researcher/2053707769_Laszlo_Mate" class="authors ga-similar-publication-author">László Máté</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56ab1df2a6baa"> <div> <h5> <a href="publication/252175544_Hilbert_Space_Methods_in_Quantum_Mechanics_by_Werner_O_Amrein" class="color-inherit ga-similar-publication-title"><span class="publication-title">Hilbert Space Methods in Quantum Mechanics, by Werner O. Amrein</span></a>  </h5>  <div class="authors"> <a href="researcher/2049218031_S_Tiwari" class="authors ga-similar-publication-author">S. Tiwari</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw45_56ab1df2a6baa" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw46_56ab1df2a6baa">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw47_56ab1df2a6baa" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=45x6obpuLdqjZOiscV521Ie3ZsOESWx25FF_Yv6HsQtGIguLHHMzidoM4t9RxPPe" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="uHkDs/WDdmzEjcw7PPnKQeAMc9/KyPD18Yo2m6T70QUFhVVsaKWEdLoUBPTRq365Nogl3VP97hwHKc4lK30/LjEiKEfhctYApG0izkET7OfC9lm6nEokH59jO7WbIToX4bgrQ+Rn4Zodz3tp8R81soSS1/XBW1NaD/a6KLOIsWdOVlpdtwfPI5cQCN9mMVOPAyJC2NG+uuaB1kBMWmxVGDDcIMtwy6ZVqxsZvC5sS9F3AUJlY2T//qYWrk3chxKdPlL9JdiPCq98DooA9AywV/1k9I4dvb3/q0ayPwAZVw0="/> <input type="hidden" name="urlAfterLogin" value="publication/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjU5ODQ0ODc2X0hpbGJlcnRfU3BhY2VfTWV0aG9kc19mb3JfUmVkdWNlZC1SYW5rX0dhdXNzaWFuX1Byb2Nlc3NfUmVncmVzc2lvbg%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjU5ODQ0ODc2X0hpbGJlcnRfU3BhY2VfTWV0aG9kc19mb3JfUmVkdWNlZC1SYW5rX0dhdXNzaWFuX1Byb2Nlc3NfUmVncmVzc2lvbg%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjU5ODQ0ODc2X0hpbGJlcnRfU3BhY2VfTWV0aG9kc19mb3JfUmVkdWNlZC1SYW5rX0dhdXNzaWFuX1Byb2Nlc3NfUmVncmVzc2lvbg%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw48_56ab1df2a6baa"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 1098;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Arno Solin","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272475355349006%401441974595657_m\/Arno_Solin.png","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Arno_Solin","institution":"Aalto University","institutionUrl":false,"widgetId":"rgw4_56ab1df2a6baa"},"id":"rgw4_56ab1df2a6baa","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=2027626","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab1df2a6baa"},"id":"rgw3_56ab1df2a6baa","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=259844876","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":259844876,"title":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"01\/2014;","publicationDateRobot":"2014-01","article":""}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1401.5508","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression"},{"key":"rft.date","value":"2014"},{"key":"rft.au","value":"Arno Solin,Simo S\u00e4rkk\u00e4"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56ab1df2a6baa"},"id":"rgw6_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=259844876","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":259844876,"peopleItems":[{"data":{"authorNameOnPublication":"Arno Solin","accountUrl":"profile\/Arno_Solin","accountKey":"Arno_Solin","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272475355349006%401441974595657_m\/Arno_Solin.png","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Arno Solin","profile":{"professionalInstitution":{"professionalInstitutionName":"Aalto University","professionalInstitutionUrl":"institution\/Aalto_University"}},"professionalInstitutionName":"Aalto University","professionalInstitutionUrl":"institution\/Aalto_University","url":"profile\/Arno_Solin","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272475355349006%401441974595657_l\/Arno_Solin.png","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Arno_Solin","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw9_56ab1df2a6baa"},"id":"rgw9_56ab1df2a6baa","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=2027626&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Aalto University","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":2,"accountCount":2,"publicationUid":259844876,"widgetId":"rgw8_56ab1df2a6baa"},"id":"rgw8_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=2027626&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=2&accountCount=2&publicationUid=259844876","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Simo S\u00e4rkk\u00e4","accountUrl":"profile\/Simo_Saerkkae","accountKey":"Simo_Saerkkae","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277578301624325%401443191232558_m\/Simo_Saerkkae.png","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Simo S\u00e4rkk\u00e4","profile":{"professionalInstitution":{"professionalInstitutionName":"Aalto University","professionalInstitutionUrl":"institution\/Aalto_University"}},"professionalInstitutionName":"Aalto University","professionalInstitutionUrl":"institution\/Aalto_University","url":"profile\/Simo_Saerkkae","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277578301624325%401443191232558_l\/Simo_Saerkkae.png","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Simo_Saerkkae","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw11_56ab1df2a6baa"},"id":"rgw11_56ab1df2a6baa","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=4101234&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Aalto University","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":2,"accountCount":2,"publicationUid":259844876,"widgetId":"rgw10_56ab1df2a6baa"},"id":"rgw10_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=4101234&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=2&accountCount=2&publicationUid=259844876","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab1df2a6baa"},"id":"rgw7_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=259844876&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":259844876,"abstract":"<noscript><\/noscript><div>This paper proposes a novel scheme for reduced-rank Gaussian process<br \/>\nregression. The method is based on an approximate series expansion of the<br \/>\ncovariance function in terms of an eigenfunction expansion of the Laplace<br \/>\noperator in a compact subset of $\\mathbb{R}^d$. On this approximate eigenbasis<br \/>\nthe eigenvalues of the covariance function can be expressed as simple functions<br \/>\nof the spectral density of the Gaussian process, which allows the GP inference<br \/>\nto be solved under a computational cost scaling as $\\mathcal{O}(nm^2)$<br \/>\n(initial) and $\\mathcal{O}(m^3)$ (hyperparameter learning) with $m$ basis<br \/>\nfunctions and $n$ data points. The approach also allows for rigorous error<br \/>\nanalysis with Hilbert space theory, and we show that the approximation becomes<br \/>\nexact when the size of the compact subset and the number of eigenfunctions go<br \/>\nto infinity. The expansion generalizes to Hilbert spaces with an inner product<br \/>\nwhich is defined as an integral over a specified input density. The method is<br \/>\ncompared to previously proposed methods theoretically and through empirical<br \/>\ntests with simulated and real data.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw12_56ab1df2a6baa"},"id":"rgw12_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=259844876","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression\/links\/54a3dc790cf256bf8bb17c1c\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw13_56ab1df2a6baa"},"id":"rgw13_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab1df2a6baa"},"id":"rgw5_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=259844876&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2019303243,"url":"researcher\/2019303243_K_E_Gustafson","fullname":"K. E. Gustafson","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 1980","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/247114955_Partial_Difierential_Equations_and_Hilbert_Space_Methods","usePlainButton":true,"publicationUid":247114955,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/247114955_Partial_Difierential_Equations_and_Hilbert_Space_Methods","title":"Partial Difierential Equations and Hilbert Space Methods","displayTitleAsLink":true,"authors":[{"id":2019303243,"url":"researcher\/2019303243_K_E_Gustafson","fullname":"K. E. Gustafson","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/247114955_Partial_Difierential_Equations_and_Hilbert_Space_Methods","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/247114955_Partial_Difierential_Equations_and_Hilbert_Space_Methods\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw15_56ab1df2a6baa"},"id":"rgw15_56ab1df2a6baa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=247114955","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2053707769,"url":"researcher\/2053707769_Laszlo_Mate","fullname":"L\u00e1szl\u00f3 M\u00e1t\u00e9","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/265370475_Hilbert_space_methods_in_science_and_engineering","usePlainButton":true,"publicationUid":265370475,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/265370475_Hilbert_space_methods_in_science_and_engineering","title":"Hilbert space methods in science and engineering","displayTitleAsLink":true,"authors":[{"id":2053707769,"url":"researcher\/2053707769_Laszlo_Mate","fullname":"L\u00e1szl\u00f3 M\u00e1t\u00e9","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/265370475_Hilbert_space_methods_in_science_and_engineering","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/265370475_Hilbert_space_methods_in_science_and_engineering\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56ab1df2a6baa"},"id":"rgw16_56ab1df2a6baa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=265370475","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2049218031,"url":"researcher\/2049218031_S_Tiwari","fullname":"S. Tiwari","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2011","journal":"Contemporary Physics","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/252175544_Hilbert_Space_Methods_in_Quantum_Mechanics_by_Werner_O_Amrein","usePlainButton":true,"publicationUid":252175544,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.96","url":"publication\/252175544_Hilbert_Space_Methods_in_Quantum_Mechanics_by_Werner_O_Amrein","title":"Hilbert Space Methods in Quantum Mechanics, by Werner O. Amrein","displayTitleAsLink":true,"authors":[{"id":2049218031,"url":"researcher\/2049218031_S_Tiwari","fullname":"S. Tiwari","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Contemporary Physics 01\/2011; 52(1):81-82. DOI:10.1080\/00107514.2010.514059"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/252175544_Hilbert_Space_Methods_in_Quantum_Mechanics_by_Werner_O_Amrein","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/252175544_Hilbert_Space_Methods_in_Quantum_Mechanics_by_Werner_O_Amrein\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab1df2a6baa"},"id":"rgw17_56ab1df2a6baa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=252175544","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw14_56ab1df2a6baa"},"id":"rgw14_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=259844876&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":259844876,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":259844876,"publicationType":"article","linkId":"54a3dc790cf256bf8bb17c1c","fileName":"54a3dc790cf256bf8bb17c1c.pdf","fileUrl":"profile\/Simo_Saerkkae\/publication\/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression\/links\/54a3dc790cf256bf8bb17c1c.pdf","name":"Simo S\u00e4rkk\u00e4","nameUrl":"profile\/Simo_Saerkkae","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Dec 31, 2014","fileSize":"3.32 MB","widgetId":"rgw20_56ab1df2a6baa"},"id":"rgw20_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=259844876&linkId=54a3dc790cf256bf8bb17c1c&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":259844876,"publicationType":"article","linkId":"02e61cf60cf2f7e88e77399f","fileName":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression","fileUrl":"http:\/\/de.arxiv.org\/pdf\/1401.5508","name":"de.arxiv.org","nameUrl":"http:\/\/de.arxiv.org\/pdf\/1401.5508","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw21_56ab1df2a6baa"},"id":"rgw21_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=259844876&linkId=02e61cf60cf2f7e88e77399f&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw19_56ab1df2a6baa"},"id":"rgw19_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=259844876&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":19,"valueFormatted":"19","widgetId":"rgw22_56ab1df2a6baa"},"id":"rgw22_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=259844876","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw18_56ab1df2a6baa"},"id":"rgw18_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=259844876&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":259844876,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw24_56ab1df2a6baa"},"id":"rgw24_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=259844876&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":19,"valueFormatted":"19","widgetId":"rgw25_56ab1df2a6baa"},"id":"rgw25_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=259844876","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw23_56ab1df2a6baa"},"id":"rgw23_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=259844876&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Hilbert Space Methods for Reduced-Rank\nGaussian Process Regression\nArno Solin\nSimo S\u00a8 arkk\u00a8 a\nDepartment of Biomedical Engineering and Computational Science (BECS)\nAalto University, School of Science\nP.O. Box 12200, FI-00076 Aalto, Finland\narno.solin@aalto.fi\nsimo.sarkka@aalto.fi\nAbstract\nThis paper proposes a novel scheme for reduced-rank Gaussian process regression. The\nmethod is based on an approximate series expansion of the covariance function in terms of\nan eigenfunction expansion of the Laplace operator in a compact subset of Rd. On this ap-\nproximate eigenbasis the eigenvalues of the covariance function can be expressed as simple\nfunctions of the spectral density of the Gaussian process, which allows the GP inference\nto be solved under a computational cost scaling as O(nm2) (initial) and O(m3) (hyper-\nparameter learning) with m basis functions and n data points. The approach also allows\nfor rigorous error analysis with Hilbert space theory, and we show that the approximation\nbecomes exact when the size of the compact subset and the number of eigenfunctions go\nto infinity. The expansion generalizes to Hilbert spaces with an inner product which is\ndefined as an integral over a specified input density. The method is compared to previously\nproposed methods theoretically and through empirical tests with simulated and real data.\nKeywords: Gaussian process regression, Laplace operator, eigenfunction expansion,\npseudo-differential operator, reduced-rank approximation\n1. Introduction\nGaussian processes (GPs, Rasmussen and Williams, 2006) are powerful tools for non-\nparametric Bayesian inference and learning. In GP regression the model functions f(x)\nare assumed to be realizations from a Gaussian random process prior with a given covari-\nance function k(x,x?), and learning amounts to solving the posterior process given a set of\nnoisy measurements y1,y2,...,ynat some given test inputs. This model is often written in\nthe form\nf \u223c GP(0,k(x,x?)),\nyi= f(xi) + \u03b5i,\n(1)\nwhere \u03b5i\u223c N(0,\u03c32\nlearning is the computational and memory requirements that scale as O(n3) and O(n2) in\na direct implementation. This limits the applicability of GPs when the number of training\nsamples n grows large. The computational requirements arise because in solving the GP\nregression problem we need to invert the n\u00d7n Gram matrix K+\u03c32\nwhich is an O(n3) operation in general.\nn), for i = 1,2,...,n. One of the main limitations of GPs in machine\nnI, where Kij= k(xi,xj),\nc ?2014 Arno Solin and Simo S\u00a8 arkk\u00a8 a.\narXiv:1401.5508v1  [stat.ML]  21 Jan 2014"},{"page":2,"text":"Solin and S\u00a8 arkk\u00a8 a\nTo overcome this problem, over the years, several schemes have been proposed. They\ntypically reduce the storage requirements to O(nm) and complexity to O(nm2), where\nm < n. Some early methods have been reviewed in Rasmussen and Williams (2006), and\nQui\u02dc nonero-Candela and Rasmussen (2005a) provide a unifying view on several methods.\nFrom a spectral point of view, several of these methods (e.g., SOR, DTC, VAR, FIC) can\nbe interpreted as modifications to the so-called Nystr\u00a8 om method (see Baker, 1977; Williams\nand Seeger, 2001), a scheme for approximating the eigenspectrum.\nFor stationary covariance functions the spectral density of the covariance function can\nbe employed: in this context the spectral approach has mainly been considered in regular\ngrids, as this allows for the use of FFT-based methods for fast solutions (see Paciorek, 2007;\nFritz et al., 2009), and more recently in terms of converting GPs to state space models\n(S\u00a8 arkk\u00a8 a and Hartikainen, 2012; S\u00a8 arkk\u00a8 a et al., 2013). Recently, L\u00b4 azaro-Gredilla et al. (2010)\nproposed a sparse spectrum method where a randomly chosen set of spectral points span a\ntrigonometric basis for the problem.\nThe methods proposed in this article fall into the class of methods called reduced-rank\napproximations (see, e.g., Rasmussen and Williams, 2006) which are based on approximat-\ning the Gram matrix K with a matrix\u02dcK with a smaller rank m < n. This allows for the\nuse of matrix inversion lemma (Woodbury formula) to speed up the computations. It is\nwell-known that the optimal reduced-rank approximation of the Gram (covariance) matrix\nK with respect to the Frobenius norm is\u02dcK = \u03a6\u039b\u03a6T, where \u039b is a diagonal matrix of the\nleading m eigenvalues of K and \u03a6 is the matrix of the corresponding orthonormal eigen-\nvectors (Golub and Van Loan, 1996; Rasmussen and Williams, 2006). Yet, as computing\nthe eigendecomposition is an O(n3) operation, this provides no remedy as such.\nIn this work we propose a novel method for obtaining approximate eigendecompositions\nof covariance functions in terms of an eigenfunction expansion of the Laplace operator in\na compact subset of Rd. The method is based on interpreting the covariance function\nas the kernel of a pseudo-differential operator (Shubin, 1987) and approximating it using\nHilbert space methods (Courant and Hilbert, 2008; Showalter, 2010). This results in a\nreduced-rank approximation for the covariance function. This path has not been explored\nin GP regression context before, although the approach is closely related to the stochastic\npartial differential equation based methods recently introduced to spatial statistics and GP\nregression (Lindgren et al., 2011; S\u00a8 arkk\u00a8 a and Hartikainen, 2012; S\u00a8 arkk\u00a8 a et al., 2013). We also\nshow how the solution formally converges to the exact solution in well-defined conditions,\nand provide theoretical and experimental comparisons to existing state-of-the-art methods.\nThis paper is structured as follows: In Section 2 we derive the approximative series\nexpansion of the covariance functions. Section 3 is dedicated to applying the approximation\nscheme to GP regression and providing details of the computational benefits. We provide\na detailed analysis of the convergence of the method in Section 4. Section 5 and 6 provide\ncomparisons to existing methods, the former from a more theoretical point of view, whereas\nthe latter contains examples and comparative evaluation on several datasets. Finally the\nproperties of the method are summarized and discussed in Section 7.\n2"},{"page":3,"text":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression\n2. Approximating the Covariance Function\nIn this section, we start by stating the assumptions and properties of the class of covariance\nfunctions that we are considering, and show how a homogenous covariance function can\nbe considered as a pseudo-differential operator constructed as a series of Laplace operators.\nThen we show how the pseudo-differential operators can be approximated with Hilbert space\nmethods on compact subsets of Rdor via inner products with integrable weight functions,\nand discuss connections to Sturm\u2013Liouville theory.\n2.1 Spectral Densities of Homogeneous and Isotropic Gaussian Processes\nIn this work it is assumed that the covariance function is homogeneous (stationary), which\nmeans that the covariance function k(x,x?) is actually a function of r = x \u2212 x?only. This\nmeans that the covariance structure of the model function f(x) is the same regardless of\nthe absolute position in the input space (cf. Rasmussen and Williams, 2006). In this case\nthe covariance function can be equivalently represented in terms of the spectral density.\nThis results from the Bochner\u2019s theorem (see, e.g., Akhiezer and Glazman, 1993; Da Prato\nand Zabczyk, 1992) which states that an arbitrary positive definite function k(r) can be\nrepresented as\n1\n(2\u03c0)d\nwhere \u00b5 is a positive measure.\nIf the measure \u00b5(\u03c9) has a density, it is called the spectral density S(\u03c9) corresponding to\nthe covariance function k(r). This gives rise to the Fourier duality of covariance and spectral\ndensity, which is known as the Wiener\u2013Khintchin theorem (Rasmussen and Williams, 2006),\ngiving the identities\n?\nFrom these identities it is easy to see that if the covariance function is isotropic, that is, it\nonly depends on the Euclidean norm ?r? such that k(r) ? k(?r?), then the spectral density\nwill also only depend on the norm of \u03c9 such that we can write S(\u03c9) ? S(?\u03c9?). In the\nfollowing we assume that the considered covariance functions are indeed isotropic, but the\napproach can be generalized to more general homogenous covariance functions.\nk(r) =\n?\nexp\n?\ni\u03c9Tr\n?\n\u00b5(d\u03c9),(2)\nk(r) =\n1\n(2\u03c0)d\nS(\u03c9)ei\u03c9Trd\u03c9andS(\u03c9) =\n?\nk(r)e\u2212i\u03c9Trdr.\n(3)\n2.2 The Covariance Operator As a Pseudo-Differential Operator\nAssociated to each covariance function k(x,x?) we can also define a covariance operator K\nas follows:\nK\u03c6 =\nAs we show in the next section, this interpretation allows us to approximate the covariance\noperator using Hilbert space methods which are typically used for approximating differential\nand pseudo-differential operators in the context of partial differential equations (Showalter,\n2010). When the covariance function is homogenous, the corresponding operator will be\ntranslation invariant thus allowing for Fourier-representation as a transfer function. This\ntransfer function is just the spectral density of the Gaussian process.\n?\nk(\u00b7,x?)\u03c6(x?)dx?. (4)\n3"},{"page":4,"text":"Solin and S\u00a8 arkk\u00a8 a\nConsider an isotropic covariance function k(x,x?) ? k(?r?) (recall that ?\u00b7? denotes\nthe Euclidean norm). The spectral density of the Gaussian process and thus the transfer\nfunction corresponding to the covariance operator will now have the form S(?\u03c9?). We can\nformally write it as a function of ?\u03c9?2such that\nS(?\u03c9?) = \u03c8(?\u03c9?2).\nAssume that the spectral density S(\u00b7) and hence \u03c8(\u00b7) are regular enough so that the spectral\ndensity has the following polynomial expansion:\n(5)\n\u03c8(?\u03c9?2) = a0+ a1?\u03c9?2+ a2(?\u03c9?2)2+ a3(?\u03c9?2)3+ \u00b7\u00b7\u00b7 .\nThus we also have\n(6)\nS(?\u03c9?) = a0+ a1?\u03c9?2+ a2(?\u03c9?2)2+ a3(?\u03c9?2)3+ \u00b7\u00b7\u00b7 .\nRecall that the transfer function corresponding to the Laplacian operator \u22072is \u2212?\u03c9?2in\nthe sense that\nF[\u22072f](\u03c9) = \u2212?\u03c9?2F[f](\u03c9),\nwhere F[\u00b7] denotes the Fourier transform of its argument. If we take the Fourier transform\nof (7), we get the following representation for the covariance operator K, which defines a\npseudo-differential operator (Shubin, 1987) as a formal series of Laplacian operators:\n(7)\n(8)\nK = a0+ a1(\u2212\u22072) + a2(\u2212\u22072)2+ a3(\u2212\u22072)3+ \u00b7\u00b7\u00b7 .\nIn the next section we will use this representation to form a series expansion approximation\nfor the covariance function.\n(9)\n2.3 Hilbert-Space Approximation of the Covariance Operator\nWe will now form a Hilbert-space approximation for the pseudo-differential operator defined\nby (9). Let \u03a9 \u2282 Rdbe a compact set, and consider the eigenvalue problem for the Laplacian\noperators with Dirichlet boundary conditions (we could use other boundary conditions as\nwell):\n?\n\u03c6j(x) = 0,\nBecause \u2212\u22072is a positive definite Hermitian operator, the set of eigenfunctions \u03c6j(\u00b7) is\northonormal with respect to the inner product\n?\nthat is,\n?\nand all the eigenvalues \u03bbjare real and positive. The negative Laplace operator can then be\nassigned the formal kernel\n?\n\u2212\u22072\u03c6j(x) = \u03bbj\u03c6j(x),x \u2208 \u03a9\nx \u2208 \u2202\u03a9.\n(10)\n?f,g? =\n\u03a9\nf(x)g(x)dx(11)\n\u03a9\n\u03c6i(x)\u03c6j(x)dx = \u03b4ij,(12)\nl(x,x?) =\nj\n\u03bbj\u03c6j(x)\u03c6j(x?)(13)\n4"},{"page":5,"text":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression\nin the sense that\n\u2212\u22072f(x) =\n?\nl(x,x?)f(x?)dx?,(14)\nfor sufficiently (weakly) differentiable functions f in the domain \u03a9 assuming Dirichlet bound-\nary conditions.\nIf we consider the formal powers of this representation, due to orthonormality of the\nbasis, we can write the arbitrary operator power s = 1,2,... of the kernel as\n?\nThis is again to be interpreted to mean that\n?\nfor regular enough functions f and in the current domain with the assumed boundary\nconditions.\nThis implies that on the domain \u03a9, assuming the boundary conditions, we also have\nls(x,x?) =\nj\n\u03bbs\nj\u03c6j(x)\u03c6j(x?). (15)\n\u2212(\u22072)sf(x) =ls(x,x?)f(x?)dx?, (16)\n?a0+ a1(\u2212\u22072) + a2(\u2212\u22072)2+ a3(\u2212\u22072)3+ \u00b7\u00b7\u00b7?f(x)\n=\n??a0+ a1l1(x,x?) + a2l2(x,x?) + a3l3(x,x?) + \u00b7\u00b7\u00b7?f(x?)dx?.\nThe left hand side is just Kf via (9), on the domain with the boundary conditions, and\nthus by comparing to (4) and using (15) we can conclude that\n(17)\nk(x,x?) \u2248 a0+ a1l1(x,x?) + a2l2(x,x?) + a3l3(x,x?) + \u00b7\u00b7\u00b7\n=\n?\nj\n?a0+ a1\u03bb1\nj+ a2\u03bb2\nj+ a3\u03bb3\nj+ \u00b7\u00b7\u00b7?\u03c6j(x)\u03c6j(x?),\n(18)\nwhich is only an approximation to the covariance function due to restriction of the domain\nto \u03a9 and the boundary conditions. By letting ?\u03c9?2= \u03bbjin (7) we now obtain\nS(?\u03bbj) = a0+ a1\u03bb1\nand substituting this into (18) then leads to the approximation\nj+ a2\u03bb2\nj+ a3\u03bb3\nj+ \u00b7\u00b7\u00b7 .(19)\nk(x,x?) \u2248\n?\nj\nS(?\u03bbj)\u03c6j(x)\u03c6j(x?),\n(20)\nwhere S(\u00b7) is the spectral density of the covariance function, \u03bbj is the jth eigenvalue and\n\u03c6j(\u00b7) the eigenfunction of the Laplace operator in a given domain. These expressions tend\nto be simple closed-form expressions.\nThe right hand side of (20) is very easy to evaluate, because it corresponds to evaluating\nthe spectral density at the square roots of the eigenvalues and multiplying them with the\neigenfunctions of the Laplace operator. Because the eigenvalues of the Laplacian operator\n5"},{"page":6,"text":"Solin and S\u00a8 arkk\u00a8 a\n0\n5?\n\u03bd \u2192 \u221e\n\u03bd =7\n2\n\u03bd =5\n2\n\u03bd =3\n2\n\u03bd =1\n2\nExact\nm = 12\nm = 32\nm = 64\nm = 128\nFigure 1: Approximations to covariance functions of the Mat\u00b4 ern class of various degrees\nof smoothness; \u03bd = 1\/2 corresponds to the exponential Ornstein\u2013Uhlenbeck covariance\nfunction, and \u03bd \u2192 \u221e to the squared exponential (exponentiated quadratic) covariance\nfunction. Approximations are shown for 12, 32, 64, and 128 eigenfunctions.\nare monotonically increasing with j and for bounded covariance functions the spectral den-\nsity goes to zero fast with higher frequencies, we can expect to obtain a good approximation\nof the right hand side by retaining only a finite number of terms in the series. However,\neven with an infinite number of terms this is only an approximation, because we assumed a\ncompact domain with boundary conditions. The approximation can be, though, expected\nto be good at the input values which are not near the boundary of \u03a9, where the Laplacian\nwas taken to be zero.\nAs an example, Figure 1 shows Mat\u00b4 ern covariance functions of various degrees of\nsmoothness \u03bd (see, e.g., Rasmussen and Williams, 2006) and approximations for differ-\nent numbers of basis functions in the approximation. The basis consists of the functions\n\u03c6j(x) = L\u22121\/2sin(\u03c0j(x + L)\/(2L)) and the eigenvalues were \u03bbj= (\u03c0 j\/(2L))2with L = 1\nand ? = 0.1. For the squared exponential the approximation is indistinguishable from the\nexact curve already at m = 12, whereas the less smooth functions require more terms.\n2.4 Inner Product Point of View\nInstead of considering a compact bounded set \u03a9, we can consider the same approximation\nin terms of an inner product defined by an input density (Williams and Seeger, 2000). Let\nthe inner product be defined as\n?\nwhere w(x) is some positive weight function such that\ninner product, we define the operator\n?\nThis operator is self-adjoint with respect to the inner product, ?Kf,g? = ?f,Kg?, and\naccording to the spectral theorem there exists an orthonormal set of basis functions and\n?f,g? =f(x)g(x)w(x)dx, (21)\n?w(x)dx < \u221e. In terms of this\nKf =k(\u00b7,x)f(x)w(x)dx.(22)\n6"},{"page":7,"text":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression\npositive constants, {\u03d5j(x),\u03b3j| j = 1,2,...}, that satisfies the eigenvalue equation\n(K\u03d5j)(x) = \u03b3j\u03d5j(x).\nThus k(x,x?) has the series expansion\n(23)\nk(x,x?) =\n?\nj\n\u03b3j\u03d5j(x)\u03d5j(x?). (24)\nSimilarly, we also have the Karhunen\u2013Loeve expansion for a sample function f(x) with zero\nmean and the above covariance function:\n?\nwhere fjs are independent zero mean Gaussian random variables with variances \u03b3j (see,\ne.g., Lenk, 1991).\nFor the negative Laplacian the corresponding definition is\nf(x) =\nj\nfj\u03d5j(x), (25)\nDf = \u2212\u22072[f w],(26)\nwhich implies\n?Df,g? = \u2212\n?\nf(x)w(x)\u22072[g(x)w(x)]dx, (27)\nand the operator defined by (26) can be seen to be self-adjoint. Again, there exists an\northonormal basis {\u03c6j(x) | j = 1,2,...} and positive eigenvalues \u03bbj which satisfy the\neigenvalue equation\n(D\u03c6j)(x) = \u03bbj\u03c6j(x).\nThus the kernel of D has a series expansion similar to Equation (13) and thus an approxi-\nmation can be given in the same form as in Equation (20). In this case the approximation\nerror comes from approximating the Laplacian operator with the more smooth operator,\n(28)\n\u22072f \u2248 \u22072[f w], (29)\nwhich is closely related to assumption of an input density w(x) for the Gaussian process.\nHowever, when the weight function w(\u00b7) is close to constant in the area where the inputs\npoints are located, the approximation is accurate.\n2.5 Connection to Sturm\u2013Liouville Theory\nThe presented methodology is also related to the Sturm\u2013Liouville theory arising in the\ntheory of partial differential equations (Courant and Hilbert, 2008). When the input x is\nscalar, the eigenvalue problem in Equation (23) can be written in Sturm\u2013Liouville form as\nfollows:\n\u2212d\ndxdx\n?\nw2(x)d\u03c6j(x)\n?\n\u2212 w(x)d2w(x)\ndx2\n\u03c6j(x) = \u03bbjw(x)\u03c6j(x).(30)\nThe above equation can be solved for \u03c6j(x) and \u03bbj using numerical methods for Sturm\u2013\nLiouville equations. Also note that if we select w(x) = 1 in a finite set, we obtain the\n7"},{"page":8,"text":"Solin and S\u00a8 arkk\u00a8 a\n(a) \u03bd =1\n2and ? = 0.5 (b) \u03bd =3\n2and ? = 0.5 (c) \u03bd \u2192 \u221e and ? = 0.5\nFigure 2: Approximate random draws of Gaussian processes with the Mat\u00b4 ern covariance\nfunction on the hull of a unit sphere. The color scale and radius follow the process.\nequation \u2212d2\/dx2\u03c6j(x) = \u03bbj\u03c6j(x) which is compatible with the results in the previous\nsection.\nWe consider the case where x \u2208 Rdand w(x) is symmetric around the origin and thus\nis only a function of the norm r = ?x? (i.e. has the form w(r)). The Laplacian in spherical\ncoordinates is\n\u22072f =\nrd\u22121\n\u2202r\n1\n\u2202\n?\nrd\u22121\u2202f\n\u2202r\n?\n+1\nr2\u2206Sd\u22121f,(31)\nwhere \u2206Sd\u22121 is the Laplace\u2013Beltrami operator on Sd\u22121. Let us assume that \u03c6j(r,\u03be) =\nhj(r)g(\u03be), where \u03be denotes the angular variables. After some algebra, writing the equations\ninto Sturm\u2013Liouville form yields for the radial part\n?\n\u2212d\ndr\nw2(r)rdhj(r)\ndr\n?\n\u2212\n?dw(r)\ndr\nw(r) +d2w(r)\ndr2\nw(r)r\n?\nhj(r) = \u03bbjw(r)rhj(r),(32)\nand \u2206Sd\u22121g(\u03be) = 0 for the angular part. The solutions to the angular part are the Laplace\u2019s\nspherical harmonics. Note that if we assume that we have w(r) = 1 on some area of finite\nradius, the first equation becomes (when d > 1):\nr2d2hj(r)\ndr2\n+ rdhj(r)\ndr\n+ r2\u03bbjhj(r) = 0. (33)\nFigure 2 shows example Gaussian random field draws on a unit sphere, where the basis\nfunctions are the Laplace spherical harmonics and the covariance functions of the Mat\u00b4 ern\nclass with different degrees of smoothness \u03bd. Our approximation is straight-forward to apply\nin any domain, where the eigendecomposition of the Laplacian can be formed.\n3. Application of the Method to GP Regression\nIn this section we show how the approximation (20) can be used in Gaussian process regres-\nsion. We also write down the expressions needed for hyperparameter learning and discuss\nthe computational requirements of the methods.\n8"},{"page":9,"text":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression\n3.1 Gaussian Process Regression\nGP regression is usually formulated as predicting an unknown scalar output f(x\u2217) associated\nwith a known input x\u2217\u2208 Rd, given a training data set D = {(xi,yi) | i = 1,2,...,n}. The\nmodel functions f are assumed to be realizations of a Gaussian random process prior and\nthe observations corrupted by Gaussian noise:\nf \u223c GP(0,k(x,x?))\nyi= f(xi) + \u03b5i,\n(34)\nwhere \u03b5i\u223c N(0,\u03c32\nzero mean and the measurement errors are independent Gaussian, but the results of this\npaper can be easily generalized to arbitrary mean functions and dependent Gaussian errors.\nThe direct solution to the GP regression problem (34) gives the predictions p(f(x\u2217) | D) =\nN(f(x\u2217) | E[f(x\u2217)],V[f(x\u2217)]). The conditional mean and variance can be computed in\nclosed-form as (Rasmussen and Williams, 2006)\nn). For notational simplicity the functions in the above model are a priori\nE[f(x\u2217)] = kT\nV[f(x\u2217)] = k(x\u2217,x\u2217) \u2212 kT\n\u2217(K + \u03c32\nnI)\u22121y,\n\u2217(K + \u03c32\nnI)\u22121k\u2217,\n(35)\nwhere Kij= k(xi,xj), k\u2217is an n-dimensional vector with the ith entry being k(x\u2217,xi), and\ny is a vector of the n observations.\nIn order to avoid the n \u00d7 n matrix inversion in (35), we use the approximation scheme\npresented in the previous section and project the process to a truncated set of m basis\nfunctions of the Laplacian as given in Equation (20) such that\nf(x) \u2248\nm\n?\nj=1\nfj\u03c6j(x), (36)\nwhere fj \u223c N(0,S(?\u03bbj)). We can then form an approximate eigendecomposition of the\nsuch that \u039bjj= S(?\u03bbj),j = 1,2,...,m. Here S(\u00b7) is the spectral density of the Gaussian\nin the decomposition are given by the eigenvectors \u03c6j(x) of the Laplacian such that \u03a6ij=\n\u03c6j(xi).\nUsing the matrix inversion lemma we rewrite (35) as follows:\nmatrix K \u2248 \u03a6\u039b\u03a6T, where \u039b is a diagonal matrix of the leading m approximate eigenvalues\nprocess and \u03bbjthe jth eigenvalue of the Laplace operator. The corresponding eigenvectors\nE[f\u2217] \u2248 \u03c6T\nV[f\u2217] \u2248 \u03c32\n\u2217(\u03a6T\u03a6 + \u03c32\nn\u03c6T\nn\u039b\u22121)\u22121\u03a6Ty,\nn\u039b\u22121)\u22121\u03c6\u2217,\n\u2217(\u03a6T\u03a6 + \u03c32\n(37)\nwhere \u03c6\u2217is an m-dimensional vector with the jth entry being \u03c6j(x\u2217). Thus, when the size\nof the training set is higher than the number of required basis functions n > m, the use of\nthis approximation is advantageous.\n3.2 Learning the Hyperparameters\nA common way to learn the hyperparameters \u03b8 of the covariance function (suppressed\nearlier in the notation for brevity) and the noise variance \u03c32\nnis by maximizing the marginal\n9"},{"page":10,"text":"Solin and S\u00a8 arkk\u00a8 a\nlikelihood function (Rasmussen and Williams, 2006; Qui\u02dc nonero-Candela and Rasmussen,\n2005b). Let Q = K + \u03c32\nnI for the full model, then the negative log marginal likelihood and\nits derivatives are\nL =1\n\u2202L\n\u2202\u03b8k\n\u2202L\n\u2202\u03c32\nn\n2log|Q| +1\n=1\n2Tr\n=1\n2yTQ\u22121y +n\nQ\u22121\u2202Q\n\u2202\u03b8k\n2Tr?Q\u22121?\u22121\n2log(2\u03c0), (38)\n??\n2yTQ\u22121Q\u22121y,\n\u22121\n2yTQ\u22121\u2202Q\n\u2202\u03b8kQ\u22121y, (39)\n(40)\nand they can be combined with a conjugate gradient optimizer. The problem in this case is\nthe inversion of Q, which is an n \u00d7 n matrix. And thus each step of running the optimizer\nis O(n3). For our approximation scheme, let\u02dcQ = \u03a6\u039b\u03a6T+ \u03c32\nin the above expressions gives us the following:\nnI. Now replacing Q with\u02dcQ\n\u02dcL =1\n\u2202\u02dcL\n\u2202\u03b8k\n\u2202\u02dcL\n\u2202\u03c32\nn\n2log|\u02dcQ| +1\n=1\n2 \u2202\u03b8k\n=1\n2 \u2202\u03c32\n2yT\u02dcQ\u22121y +n\n+1\n2 \u2202\u03b8k\n+1\n2 \u2202\u03c32\n2log(2\u03c0),(41)\n\u2202 log|\u02dcQ|\n\u2202yT\u02dcQ\u22121y\n, (42)\n\u2202 log|\u02dcQ|\nn\n\u2202yT\u02dcQ\u22121y\nn\n,(43)\nwhere for the terms involving log|\u02dcQ|:\nlog|\u02dcQ| = (n \u2212 m)log\u03c32\nn+ log|Z| +\nm\n?\nj=1\nlogS(?\u03bbj),\n?\n(44)\n\u2202 log|\u02dcQ|\n\u2202\u03b8k\n=\nm\n?\nj=1\nS(?\u03bbj)\u22121\u2202S(?\u03bbj)\n=n \u2212 m\n\u03c32\nn\n\u2202\u03b8k\n\u2212 \u03c32\nnTrZ\u22121\u039b\u22122\u2202\u039b\n\u2202\u03b8k\n?\n, (45)\n\u2202 log|\u02dcQ|\n\u2202\u03c32\nn\n+ Tr?Z\u22121\u039b\u22121?, (46)\nand for the terms involving\u02dcQ\u22121:\nyT\u02dcQ\u22121y =\n1\n\u03c32\nn\n?\nyTy \u2212 y\u03a6Z\u22121\u03a6Ty\n?\nyT\u03a6Z\u22121\u039b\u22121Z\u22121\u03a6Ty \u2212\n?\n,(47)\n\u2202yT\u02dcQ\u22121y\n\u2202\u03b8k\n\u2202yT\u02dcQ\u22121y\n\u2202\u03c32\n= \u2212yTZ\u22121\n\u039b\u22122\u2202\u039b\n\u2202\u03b8k\n?\nZ\u22121y,(48)\nn\n=\n1\n\u03c32\nn\n1\n\u03c32\nn\nyT\u02dcQy,(49)\nwhere Z = \u03c32\ncan be avoided in many cases, and the inversion of Z can be carried out through Cholesky\nfactorization for numerical stability. This factorization (LLT= Z) can also be used for\nn\u039b\u22121+ \u03a6T\u03a6. For efficient implementation, matrix-to-matrix multiplications\n10"},{"page":11,"text":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression\nthe term log|Z| = 2?\nOnce the marginal likelihood and its derivatives are available, it is also possible to use\nother methods for parameter inference such as Markov chain Monte Carlo methods (Liu,\n2001; Brooks et al., 2011) including Hamiltonian Monte Carlo (HMC, Duane et al., 1987;\nNeal, 2011) as well as numerous others.\njlogLjj, and Tr?Z\u22121\u039b\u22121?\n=?\nj1\/(Zjj\u039bjj) can be evaluated by\nelement-wise multiplication.\n3.3 Discussion on the Computational Complexity\nAs can be noted from Equation (20), the basis functions in the reduced-rank approximation\ndo not depend on the hyperparameters of the covariance function. Thus it is enough to\ncalculate the product \u03a6T\u03a6 only once, which means that the method has a overall asymp-\ntotic computational complexity of O(nm2). After this initial cost, evaluating the marginal\nlikelihood and the marginal likelihood gradient is an O(m3) operation\u2014which in practice\ncomes from the Cholesky factorization of Z on each step.\nIf the number of observations n is so large that storing the n\u00d7m matrix \u03a6 is not feasible,\nthe computations of \u03a6T\u03a6 can be carried out in blocks. Storing the evaluated eigenfunctions\nin \u03a6 is not necessary, because the \u03c6j(x) are closed-form expressions that can be evaluated\nwhen necessary. In practice, it might be preferable to cache the result of \u03a6T\u03a6 (causing a\nmemory requirement scaling as O(m2)), but this is not required.\nThe computational complexity of conventional sparse GP approximations typically scale\nas O(nm2) in time for each step of evaluating the marginal likelihood. The scaling in\ndemand of storage is O(nm). This comes from the inevitable cost of re-evaluating all results\ninvolving the basis functions on each step and storing the matrices required for doing this.\nThis applies to all the methods that will be discussed in Section 5, with the exception of\nSSGP, where the storage demand can be relaxed by re-evaluating the basis functions on\ndemand.\nWe can also consider the rather restricting, but in certain applications often encountered\ncase, where the measurements are constrained to a regular grid. This causes the product of\nthe orthonormal eigenfunction matrices \u03a6T\u03a6 to be diagonal, avoiding the calculation of the\nmatrix inverse altogether. This relates to the FFT-based methods for GP regression (Pa-\nciorek, 2007; Fritz et al., 2009), and the projections to the basis functions can be evaluated\nby fast Fourier transform in O(nlogn) time complexity.\n4. Convergence Analysis\nIn this section we analyze the convergence of the proposed approximation when the size of\nthe domain \u03a9 and the number of terms in the series grows to infinity. We start by analyzing\na univariate problem in the domain \u03a9 = [\u2212L,L] and with Dirichlet boundary conditions\nand then generalize the result to d-dimensional cubes \u03a9 = [\u2212L1,L1] \u00d7 \u00b7\u00b7\u00b7 \u00d7 [\u2212Ld,Ld]. We\nalso discuss how the analysis could be extended to other types of basis functions.\n11"},{"page":12,"text":"Solin and S\u00a8 arkk\u00a8 a\n4.1 Univariate Dirichlet Case\nIn the univariate case, the m-term approximation has the form\n?km(x,x?) =\nm\n?\nj=1\nS(?\u03bbj)\u03c6j(x)\u03c6j(x?), (50)\nwhere the eigenfunctions and eigenvalues are:\n\u03c6j(x) =\n1\n\u221aL\nsin\n?\u03c0 j (x + L)\n2L\n?\nand\u03bbj=\n?\u03c0 j\n2L\n?2\n, for j = 1,2,....(51)\nThe true covariance function k(x,x?) is assumed to be stationary and have a spectral density\nwhich is uniformly bounded S(\u03c9) < \u221e, has at least two bounded derivatives |S?(\u03c9)| < \u221e,\n|S??(\u03c9)| < \u221e, and has a bounded integral over the real axis?\u221e\nand thus we are only interested in the case x,x?\u2208 [\u2212?L,?L]. For the purposes of analysis we\nThe univariate convergence result can be summarized as the following theorem which is\nproved in Appendix A.1.\n\u2212\u221eS(\u03c9)d\u03c9 < \u221e. We also\nassume that our training and test sets are constrained in the area [\u2212?L,?L], where?L < L,\nalso assume that L is bounded below by a constant.\nTheorem 4.1. There exists a constant C such that\n???k(x,x?) \u2212?km(x,x?)\nwhich in turn implies that uniformly\n??? \u2264C\nL+2\n\u03c0\n?\u221e\n2L\n\u03c0 m\nS(\u03c9)d\u03c9, (52)\nlim\nL\u2192\u221e\n?\nlim\nm\u2192\u221e\n?km(x,x?)\n?\n= k(x,x?).(53)\nRemark 4.2. Note that we cannot simply exchange the order of the limits in the above\ntheorem. However, the theorem does ensure the convergence of the approximation in the joint\nlimit m,L \u2192 \u221e provided that we add terms to the series fast enough such that m\/L \u2192 \u221e.\nThat is, in this limit, the approximation?km(x,x?) converges uniformly to k(x,x?).\nAs such, the results above only ensure the convergence of the prior covariance functions.\nHowever, it turns out that this also ensures the convergence of the posterior as is summarized\nin the following corollary.\nCorollary 4.3. Because the Gaussian process regression equations only involve pointwise\nevaluations of the kernels, it also follows that the posterior mean and covariance functions\nconverge uniformly to the exact solutions in the limit m,L \u2192 \u221e.\n4.2 Multivariate Cartesian Dirichlet Case\nIn order to generalize the results from the previous section, we turn our attention to a\nd-dimensional inputs space with rectangular domain \u03a9 = [\u2212L1,L1] \u00d7 \u00b7\u00b7\u00b7 \u00d7 [\u2212Ld,Ld] with\n12"},{"page":13,"text":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression\nDirichlet boundary conditions. In this case we consider a truncated m = \u02c6 mdterm approxi-\nmation of the form\n?km(x,x?) =\n\u02c6 m\n?\nj1,...,jd=1\nS(?\u03bbj1,...,jd)\u03c6j1,...,jd(x)\u03c6j1,...,jd(x?)\n(54)\nwith the eigenfunctions and eigenvalues\n\u03c6j1,...,jd(x) =\nd?\nk=1\n1\n\u221aLk\nsin\n?\u03c0 jk(xk+ Lk)\n2Lk\n?\nand\u03bbj1,...,jd=\nd\n?\nk=1\n?\u03c0 jk\n2Lk\n?2\n.\n(55)\nThe true covariance function k(x,x?) is assumed to be stationary and have a spectral den-\nsity S(\u03c9) which is two times differentiable and the derivatives are assumed to be bounded.\nWe also assume that the single-variable integrals are finite\nin this case is equivalent to requiring that the integral over the whole space is finite\n?\nThe following result for this d-dimensional case is proved in Appendix A.2.\n?\u221e\n\u2212\u221eS(\u03c9)d\u03c9k < \u221e, which\nRdS(\u03c9)d\u03c9 < \u221e. Furthermore, we assume that the training and test sets are contained in\nthe d-dimensional cube [\u2212?L,?L]dand that Lks are bounded from below.\nTheorem 4.4. There exists a constant Cdsuch that\n???k(x,x?) \u2212?km(x,x?)\nwhere L = minkLk, which in turn implies that uniformly\n?\nRemark 4.5. Analogously as in the one-dimensional case we cannot simply exchange the\norder of the limits above. Furthermore, we need to add terms fast enough so that \u02c6 m\/Lk\u2192 \u221e\nwhen m,L1,...,Ld\u2192 \u221e.\nCorollary 4.6. As in the one-dimensional case, the uniform convergence of the prior co-\nvariance function also implies uniform convergence of the posterior mean and covariance\nin the limit m,L1,...,Ld\u2192 \u221e.\n??? \u2264Cd\nL+\n1\n\u03c0d\n?\n?\u03c9?\u2265\u03c0 \u02c6 m\n2L\nS(\u03c9)d\u03c9,\n(56)\nlim\nL1,...,Ld\u2192\u221e\nlim\nm\u2192\u221e\n?km(x,x?)\n?\n= k(x,x?).(57)\n4.3 Other Domains\nIt would also be possible carry out similar convergence analysis, for example, in a spherical\ndomain. In that case the technical details become slightly more complicated, because instead\nsinusoidals we will have Bessel functions and the eigenvalues no longer form a uniform grid.\nThis means that instead of Riemann integrals we need to consider weighted integrals where\nthe distribution of the zeros of Bessel functions is explicitly accounted for. It might also\nbe possible to use some more general theoretical results from mathematical analysis to\nobtain the convergence results. However, due to these technical challenges more general\nconvergence proof will be developed elsewhere.\n13"},{"page":14,"text":"Solin and S\u00a8 arkk\u00a8 a\nThere is also a similar technical challenge in the analysis when the basis functions are\nformed by assuming an input density (see Section 2.4) instead of a bounded domain. Because\nexplicit expressions for eigenfunctions and eigenvalues cannot be obtained in general, the\nelementary proof methods which we used here cannot be applied. Therefore the convergence\nanalysis of this case is also left as a topic for future research.\n5. Relationship to Other Methods\nIn this section we compare our method to existing sparse GP methods from a theoretical\npoint of view. We consider two different classes of approaches: a class of inducing input\nmethods based on the Nystr\u00a8 om approximation (following the interpretation of Qui\u02dc nonero-\nCandela and Rasmussen, 2005a), and direct spectral approximations.\n5.1 Methods from the Nystr\u00a8 om Family\nA crude but rather effective scheme for approximating the eigendecomposition of the Gram\nmatrix is the Nystr\u00a8 om method (see, e.g., Baker, 1977, for the integral approximation\nscheme). This method is based on choosing a set of m inducing inputs xu and scaling\nthe corresponding eigendecomposition of their corresponding covariance matrix Ku,u to\nmatch that of the actual covariance. The Nystr\u00a8 om approximations to the jth eigenvalue\nand eigenfunction are\n?\u03bbj=1\n?\u03c6j(x) =\nm\u03bbu,j,\n\u221am\n\u03bbu,j\n(58)\nk(x,xu)\u03c6u,j,(59)\nwhere \u03bbu,jand \u03c6u,jcorrespond to the jth eigenvalue and eigenvector of Ku,u. This scheme\nwas originally introduced to the GP context by Williams and Seeger (2001). They presented\na sparse scheme, where the resulting approximate prior covariance over the latent variables\nis Kf,uK\u22121\nu,uKu,f, which can be derived directly from Equations (58) and (59).\nAs discussed by Qui\u02dc nonero-Candela and Rasmussen (2005a), the Nystr\u00a8 om method by\nWilliams and Seeger (2001) does not correspond to a well-formed probabilistic model. How-\never, several methods modifying the inducing point approach are widely used. The Subset\nof Regressors (SOR, Smola and Bartlett, 2001) method uses the Nystr\u00a8 om approximation\nscheme for approximating the whole covariance function,\nkSOR(x,x?) =\nm\n?\nj=1\n?\u03bbj?\u03c6j(x)?\u03c6j(x?), (60)\nwhereas the sparse Nystr\u00a8 om method (Williams and Seeger, 2001) only replaces the training\ndata covariance matrix. The SOR method is in this sense a complete Nystr\u00a8 om approxima-\ntion to the full GP problem. A method in-between is the Deterministic Training Conditional\n(DTC, Csat\u00b4 o and Opper, 2002; Seeger et al., 2003) method which retains the true covari-\nance for the training data, but uses the approximate cross-covariances between training and\ntest data. For DTC, tampering with the covariance matrix causes the result not to actu-\nally be a Gaussian process. The Variational Approximation (VAR, Titsias, 2009) method\n14"},{"page":15,"text":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression\nmodifies the DTC method by an additional trace term in the likelihood that comes from\nthe variational bound.\nThe Fully Independent (Training) Conditional (FIC, Qui\u02dc nonero-Candela and Rasmussen,\n2005a) method (originally introduced as Sparse Pseudo-Input GP by Snelson and Ghahra-\nmani, 2006) is also based on the Nystr\u00a8 om approximation but contains an additional diagonal\nterm replacing the diagonal of the approximate covariance matrix with the values from the\ntrue covariance. The corresponding prior covariance function for FIC, is thus\nkFIC(xi,xj) = kSOR(xi,xj) + \u03b4i,j(k(xi,xj) \u2212 kSOR(xi,xj)),(61)\nwhere \u03b4i,jis the Kronecker delta.\nFigure 3 illustrates the effect of the approximations compared to the exact correlation\nstructure in the GP. The dashed contours show the exact correlation contours computed\nfor three locations with the squared exponential covariance function. Figure 3a shows the\nresults for the FIC approximation with 16 inducing points (locations shown in the figure).\nIt is clear that the number of inducing points or their locations are not sufficient to capture\nthe correlation structure. For similar figures and discussion on the effects of the inducing\npoints, see Vanhatalo et al. (2010). This behavior is not unique to SOR or FIC, but applies\nto all the methods from the Nystr\u00a8 om family.\n5.2 Direct Spectral Methods\nThe sparse spectrum GP (SSGP) method introduced by L\u00b4 azaro-Gredilla et al. (2010) uses\nthe spectral representation of the covariance function for drawing random samples from the\nspectrum. These samples are used for representing the GP on a trigonometric basis\n\u03c6(x) =?cos(2\u03c0 sT\n1x)sin(2\u03c0 sT\n1x) ...cos(2\u03c0 sT\nhx) sin(2\u03c0 sT\nhx)?,(62)\nwhere the spectral points sr,r = 1,2,...,h (2h = m), are sampled from the spectral density\nof the original stationary covariance function (following the normalization convention used\nin the original paper). The covariance function corresponding to the SSGP scheme is now\nof the form\nkSSGP(x,x?) =2\u03c32\nm\n\u03c6(x)\u03c6T(x?) =\u03c32\nh\nh\n?\nr=1\ncos\n?\n2\u03c0 sT\nr(x \u2212 x?)\n?\n, (63)\nwhere \u03c32is the magnitude scale hyperparameter. This representation of the sparse spec-\ntrum method converges to the full GP in the limit of the number of spectral points going\nto infinity, and it is the preferred formulation of the method in one or two dimensions (see\nL\u00b4 azaro-Gredilla, 2010, for discussion). We can interpret the SSGP method in (63) as a\nMonte Carlo approximation of the Wiener\u2013Khintchin integral. In order to have a represen-\ntative sample of the spectrum, the method typically requires the number of spectral points\nto be large. For high-dimensional inputs the number of required spectral points becomes\noverwhelming, and optimizing the spectral locations along with the hyperparameters at-\ntractive. However, as argued by L\u00b4 azaro-Gredilla et al. (2010), this option does not converge\nto the full GP and suffers from overfitting to the training data.\n15"},{"page":16,"text":"Solin and S\u00a8 arkk\u00a8 a\n\u2212L\n0\nL\n\u2212L\n0\nL\nx1\nx2\n(a) SOR\/FIC (grid)\n\u2212L\n0\nL\n\u2212L\n0\nL\nx1\nx2\n(b) SOR\/FIC (random)\n\u2212L\n0\nL\n\u2212L\n0\nL\nx1\nx2\n(c) SSGP (random)\n\u2212L\n0\nL\n\u2212L\n0\nL\nx1\nx2\n(d) The new method\n(Cartesian)\n\u2212L\n0\nL\n\u2212L\n0\nL\nx1\nx2\n(e) The new method\n(extended Cartesian)\n\u2212L\n0\nL\n\u2212L\n0\nL\nx1\nx2\n(f) The new method\n(polar)\nFigure 3: Correlation contours computed for three locations ( ) corresponding to the\nsquared exponential covariance function (exact contours dashed). The rank of each ap-\nproximation is m = 16, and the locations of the inducing inputs are marked with blue stars\n( ). The hyperparameters are the same in each figure. The domain boundary is shown in\nthin grey (\n) if extended outside the box.\nContours for the sparse spectrum SSGP method are visualized in Figure 3c. Here the\nspectral points were chosen at random following L\u00b4 azaro-Gredilla (2010). Because the basis\nfunctions are spanned using both sines and cosines, the number of spectral points was\nh = 8 in order to match the rank m = 16. These results agree well with those presented in\nthe L\u00b4 azaro-Gredilla et al. (2010) for a one-dimensional example. For this particular set of\nspectral points some directions of the contours happen to match the true values very well,\nwhile other directions are completely off. Increasing the rank from 16 to 100 would give\ncomparable results to the other methods.\nWhile SSGP is based on a sparse spectrum, the reduced-rank method proposed in this\npaper aims to make the spectrum as \u2018full\u2019 as possible at a given rank. While SSGP can be\ninterpreted as a Monte Carlo integral approximation, the corresponding interpretation to\nthe proposed method would a numerical quadrature-based integral approximation (cf. the\n16"},{"page":17,"text":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression\nconvergence proof in Appendix A.1). Figure 3d shows the same contours obtained by the\nproposed reduced-rank method. Here the eigendecomposition of the Laplace operator has\nbeen obtained for the square \u03a9 = [\u2212L,L] \u00d7 [\u2212L,L] with Dirichlet boundary conditions.\nThe contours match well with the full solution towards the middle of the domain. The\nboundary effects drive the process to zero, which is seen as distortion near the edges.\nFigure 3e shows how extending the boundaries just by 25% and keeping the number\nof basis functions fixed at 16, gives good results. The last Figure 3f corresponds to using\na disk shaped domain instead of the rectangular. The eigendecomposition of the Laplace\noperator is done in polar coordinates, and the Dirichlet boundary is visualized by a circle\nin the figure.\n6. Experiments\nIn this section we aim to provide examples of the practical use of the proposed method,\nand to compare it against other methods that are typically used in a similar setting. We\nstart with a small simulated one-dimensional dataset, and then provide more extensive\ncomparisons by using real-world data. We also consider an example of data, where the\ninput domain is the surface of a sphere, and conclude our comparison by using a very large\ndataset to demonstrate what possibilities the computational benefits open.\n6.1 Experimental Setup\nFor assessing the performance of different methods we use 10-fold cross-validation and\nevaluate the following measures based on the validation set: the standardized mean squared\nerror (SMSE) and the mean standardized log loss (MSLL), respectively defined as:\nSMSE =\nn\u2217\n?\ni=1\n(y\u2217i\u2212 \u00b5\u2217i)2\nVar[y]\n,andMSLL =\n1\n2n\u2217\nn\u2217\n?\ni=1\n?(y\u2217i\u2212 \u00b5\u2217i)2\n\u03c32\n\u2217i\n+ log2\u03c0\u03c32\n\u2217i\n?\n,\nwhere \u00b5\u2217i= E[f(x\u2217i)] and \u03c32\ntest sample i = 1,2,...,n\u2217, and y\u2217iis the actual test value. The training data variance is\ndenoted by Var[y]. For all experiments, the values reported are averages over ten repetitions.\nWe compare our solution to SOR, DTC, VAR, and FIC using the implementations\nprovided in the GPstuff software package (version 4.3.1, see Vanhatalo et al., 2013) for\nMathworks Matlab. The sparse spectrum SSGP method by L\u00b4 azaro-Gredilla et al. (2010) was\nimplemented into the GPstuff toolbox for the comparisons.1The reference implementation\nwas modified such that also non-ARD covariances could be accounted for.\nThe m inducing inputs for SOR, DTC, VAR, and FIC were chosen at random as a\nsubset from the training data and kept fixed between the methods. For low-dimensional\ninputs, this tends to lead to good results and avoid over-fitting to the training data, while\noptimizing the input locations alongside hyperparameters becomes the preferred approach\nin high input dimensions (Qui\u02dc nonero-Candela and Rasmussen, 2005a). The results are\naveraged over ten repetitions in order to present the average performance of the methods.\nIn Sections 6.2, 6.3, and 6.5, we used a Cartesian domain with Dirichlet boundary conditions\n\u2217i= V[f(x\u2217i)] + \u03c32\nnare the predictive mean and variance for\n1. The implementation is based on the code available from Miguel L\u00b4 azaro-Gredilla: http:\/\/www.tsc.uc3m.\nes\/~miguel\/downloads.php.\n17"},{"page":18,"text":"Solin and S\u00a8 arkk\u00a8 a\n\u2212101\n\u22122\n\u22121\n0\n1\n2\nInput, x\nOutput, y\nObservations\nMean\n95% region\nExact interval\n(a) Gaussian process regression solution\n? = 0.1\n\u03c32= 12\nTrue curve\nApproximation\nTrue value\n\u03c32\nn= 0.22\n(b) Log marginal likelihood curves\nFigure 4: (a) 256 data points generated from a GP with hyperparameters (\u03c32,?,\u03c32\n(12,0.1,0.22), the full GP solution, and an approximate solution with m = 32. (b) Negative\nmarginal likelihood curves for the signal variance \u03c32, length-scale ?, and noise variance \u03c32\nn) =\nn.\nfor the new reduced-rank method. To avoid boundary effects, the domain was extended by\n10% outside the inputs in each direction.\nIn the comparisons we followed the guidelines given by Chalupka et al. (2013) for making\ncomparisons between the actual performance of different methods. For hyperparameter op-\ntimization we used the fminunc routine in Matlab with a Quasi-Newton optimizer. We also\ntested several other algorithms, but the results were not sensitive to the choice of optimizer.\nThe optimizer was run with a termination tolerance of 10\u22125on the target function value\nand on the optimizer inputs. The number of required target function evaluations stayed\nfairly constant for all the comparisons, making the comparisons for the hyperparameter\nlearning bespoke.\n6.2 Toy Example\nFigure 4 shows a simulated example, where 256 data points are drawn from a Gaussian pro-\ncess prior with a squared exponential covariance function. We use the same parametrization\nas Rasmussen and Williams (2006) and denote the signal variance \u03c32, length-scale ?, and\nnoise variance \u03c32\nn. Figure 4b shows the negative marginal log likelihood curves both for the\nfull GP and the approximation with m = 32 basis functions. The likelihood curve approxi-\nmations are almost exact and only differs from the full GP likelihood for small length-scales\n(roughly for values smaller than 2L\/m). Figure 4a shows the approximate GP solution.\nThe mean estimate follows the exact GP mean, and the shaded region showing the 95%\nconfidence area differs from the exact solution (dashed) only near the boundaries.\nFigures 5a and 5b show the SMSE and MSLL values for m = 8,10,...,32 inducing\ninputs and basis functions for the toy dataset from Figure 4.\nproposed reduced rank method is fast and a soon as the number of eigenfunctions is large\nThe convergence of the\n18"},{"page":19,"text":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression\n812 1620\nm\n242832\n10\u22121.2\n10\u22121\nSMSE\n(a) SMSE for the toy data\n81216 20\nm\n2428 32\n10\u22122\n10\u22121\nMSLL\nFULL\nNEW\nSOR\nDTC\nVAR\nFIC\nSSGP\n(b) MSLL for the toy data\n100\n101\n102\n10\u22120.6\n10\u22120.4\nEvaluation time (s)\nSMSE\n(c) SMSE for the precipitation data\n100\n101\n102\n10\u22120.2\n10\u22120.1\n100\nEvaluation time (s)\nMSLL\n(d) MSLL for the precipitation data\nFigure 5: Standardized mean squared error (SMSE) and mean standardized log loss (MSLL)\nresults for the toy data (d = 1, n = 256) from Figure 4 and the precipitation data (d = 2,\nn = 5776) evaluated by 10-fold cross-validation and averaged over ten repetitions. The\nevaluation time includes hyperparameter learning.\nenough (m = 20) to account for the short length-scales, the approximation converges to the\nexact full GP solution (shown by the dashed line).\nIn this case the SOR method that uses the Nystr\u00a8 om approximation to directly approx-\nimate the spectrum of the full GP (see Section 5) seems to give good results. However, as\nthe resulting approximation in SOR corresponds to a singular Gaussian distribution, the\npredictive variance is underestimated. This can be seen in Figure 5b, where SOR seems to\ngive better results than the full GP. These results are however due to the smaller predictive\nvariance on the test set. DTC tries to fix this shortcoming of SOR\u2014they are identical in\nother respects except predictive variance evaluation\u2014and while SOR and DTC give iden-\ntical results in terms of SMSE, they differ in MSLL. We also note that additional trace\nterm in the marginal likelihood in VAR makes the likelihood surface flat, which explains\nthe differences in the results in comparison to DTC.\nThe sparse spectrum SSGP method did not perform well on average. Still, it can be\nseen that it converges towards the performance of the full GP. The dependence on the\nnumber of spectral points differs from the rest of the methods, and a rank of m = 32\nis not enough to meet the other methods. However, in terms of best case performance\n19"},{"page":20,"text":"Solin and S\u00a8 arkk\u00a8 a\n(a) Observation locations\n015003000mm\n(b) The full GP(c) The reduced-rank method\nFigure 6: Interpolation of the yearly precipitation levels using reduced-rank GP regression.\nSubfigure 6a shows the n = 5776 weather station locations. Subfigures 6b and 6c show the\nresults for the full GP model and the new reduced-rank GP method.\nover the ten repetitions with different inducing inputs and spectral points, both FIC and\nSSGP outperformed SOR, DTC, and VAR. Because of its \u2018dense spectrum\u2019 approach, the\nproposed reduced-rank method is not sensitive to the choice of spectral points, and thus the\nperformance remained the same between repetitions. In terms of variance over the 10-fold\ncross-validation folds, the methods in order of growing variance in the figure legend (the\nvariance approximately doubling between FULL and SSGP).\n6.3 Precipitation Data\nAs a real-data example, we consider a precipitation data set that contain US annual pre-\ncipitation summaries for year 1995 (d = 2 and n = 5776, available online, see Vanhatalo\net al., 2013). The observation locations are shown on a map in Figure 6a.\nWe limit the number of inducing inputs and spectral points to m = 128,192,...,512.\nFor the new method we additionally consider ranks m = 1024,1536,...,4096, and show\nthat this causes a computational burden of the same order as the conventional sparse GP\nmethods with smaller ms.\nIn order to demonstrate the computational benefits of the proposed model, we also\npresent the running time of the GP inference (including hyperparameter optimization). All\nmethods were implemented under a similar framework in the GPstuff package, and they all\nemploy similar reformulations for numerical stability. The key difference in the evaluation\ntimes comes from hyperparameter optimization, where SOR, DTC, VAR, FIC, and SSGP\nscale as O(nm2) for each evaluation of the marginal likelihood. The proposed reduced-rank\nmethod scales as O(m3) for each evaluation (after an initial cost of O(nm2)).\nFigures 5c and 5d show the SMSE and MSLL results for this data against evaluation\ntime.On this scale we note that the evaluation time and accuracy, both in terms of\nSMSE and MSLL, are alike for SOR, DTC, VAR, and FIC. SSGP is faster to evaluate in\ncomparison with the Nystr\u00a8 om family of methods, which comes from the simpler structure of\nthe approximation. Still, the number of required spectral points to meet a certain average\nerror level is larger for SSGP.\nThe results for the proposed reduced-rank method (NEW) show that with two input\ndimensions, the required number of basis functions is larger. For the first seven points,\n20"},{"page":21,"text":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression\n\u221220\u25e6C\n(a) The mean temperature\n0\u25e6C\n30\u25e6C\n0\u25e6C\n2\u25e6C\n4\u25e6C\n6\u25e6C\n8\u25e6C\n(b) Standard deviation contours\nFigure 7: Modeling of the yearly mean temperature on the spherical surface of the Earth\n(n = 11028). Figure 7b shows the standard deviation contours which match well with the\ncontinents.\nwe notice that even though the evaluation is two orders of magnitude faster, the method\nperforms only slightly worse in comparison to conventional sparse methods. By considering\nhigher ranks (the next seven points), the new method converges to the performance of the\nfull GP (both in SMSE and MSLL), while retaining a computational time comparable to\nthe conventional methods. This type of spatial medium-size GP regression problems can\nthus be solved in seconds.\nFigures 6b and 6c show interpolation of the precipitation levels using a full GP model\nand the reduced-rank method (m = 1728), respectively. The results are practically identical,\nas is easy to confirm from the color surfaces. Obtaining the reduced-rank result (including\ninitialization and hyperparameter learning) took slightly less than 30 seconds on a laptop\ncomputer (MacBook Air, Late 2010 model, 2.13 GHz, 4 GB RAM), while the full GP\ninference took approximately 18 minutes.\n6.4 Temperature Data on the Surface of the Globe\nWe also demonstrate the use of the method in non-Cartesian coordinates. We consider\nmodeling of the spatial mean temperature over a number of n = 11028 locations around\nthe globe.2\nAs earlier demonstrated in Figure 2, we use the Laplace operator in spherical coordinates\nas defined in (31). The eigenfunctions for the angular part are the Laplace\u2019s spherical\nharmonics. The evaluation of the approximation does not depend on the coordinate system,\nand thus all the equations presented in the earlier sections remain valid. We use the squared\nexponential covariance function and m = 1089 basis functions.\nFigure 7 visualizes the modeling outcome. The results are visualized using an inter-\nrupted projection (an adaption of the Goode homolosine projection) in order to preserve\nthe length-scale structure across the map.\nwhich corresponds to the n = 11028 observation locations that are mostly spread over the\ncontinents and western countries (the white areas in Figure 7b contain no observations).\nThe uncertainty is visualized in Figure 7b,\n2. The data are available for download from US National Climatic Data Center: http:\/\/www7.ncdc.noaa.\ngov\/CDO\/cdoselect.cmd (accessed January 3, 2014).\n21"},{"page":22,"text":"Solin and S\u00a8 arkk\u00a8 a\nMethodSMSE MSLL\nThe reduced-rank method\nRandom subset (n = 500)\nRandom subset (n = 1000)\n0.388 (0.007)\n0.419 (0.035)\n0.392 (0.022)\n0.608 (0.009)\n0.648 (0.014)\n0.614 (0.010)\nTable 1: Results for the apartment data set (d = 2, n = 102890) for predicting the\nlog-apartment prices across England and Wales. The results for the Standardized mean\nsquared error (SMSE) and mean standardized log loss (MSLL) were obtained by 10-fold\ncross-validation, where the shown values are the mean (standard deviation parenthesised).\nObtaining the reduced-rank result (including initialization and hyperparameter learning)\ntook approximately 50 seconds on a laptop computer (MacBook Air, Late 2010 model,\n2.13 GHz, 4 GB RAM), which scales with n in comparison to the evaluation time in the\nprevious section.\n6.5 Apartment Price Data\nIn order to fully use the computational benefits of the method, we consider a large dataset.\nWe use records of sold apartments3in the UK for the period of February to October 2013.\nThe data consist of n = 102890 records for apartments, which were cross-referenced against\na postcode database to get the geographical coordinates on which the normalized logarithmic\nprices were regressed. The dataset is similar to that used in Hensman et al. (2013), where\nthe records were for year 2012.\nTo account for both the national and regional variations in apartment prices, we used\ntwo squared exponential covariance functions with different length-scales and magnitudes.\nAdditionally, a Gaussian noise term captures the variation that is not related to location\nalone. Applying the reduced-rank methodology to a sum of covariances is straight-forward,\nas the the kernel approximations share basis functions and only the spectra have to be\nsummed.\nTo validate the results, because the full GP solution is infeasible, we used the subset\nof data approach as was done in Hensman et al. (2013). We solved the full GP problem\nby considering subsets of n = 500 and n = 1000 data points randomly chosen from the\ntraining set. For each fold in the cross-validation the results were averaged over ten choices\nof subsets. The rank of the reduced-rank approximation was fixed at m = 1000 in order to\nmatch with the larger of the two subsets.\nTable 1 shows the SMSE and MSLL values for the apartment data. The results show\nthat the reduced rank method. The results show that the proposed method gives good\nresults in terms of both SMSE and MSLL, and the standard deviation between the folds is\nalso small. In this case the reduced-rank result (including initialization and hyperparameter\nlearning) took approximately 130 seconds on a laptop computer (MacBook Air, Late 2010\nmodel, 2.13 GHz, 4 GB RAM).\n3. The data are available from http:\/\/data.gov.uk\/dataset\/land-registry-monthly-price-paid-data\/\n(accessed January 6, 2014).\n22"},{"page":23,"text":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression\n7. Conclusion and Discussion\nIn this paper we have proposed a novel approximation scheme for forming approximate\neigendecompositions of covariance functions in terms of the Laplace operator eigenbasis\nand the spectral density of the covariance function. The eigenfunction decomposition of the\nLaplacian can easily be formed in various domains, and the eigenfunctions are independent\nof the choice of hyperparameters of the covariance.\nAn advantage of the method is that it has the ability to approximate the eigendecom-\nposition using only the eigendecomposition of the Laplacian and the spectral density of the\ncovariance function, both of which are closed-from expressions. This together with having\nthe eigenvectors in \u03a6 mutually orthogonal and independent of the hyperparameters, is the\nkey to efficiency. This allows an implementation with a computational cost of O(nm2)\n(initial) and O(m3) (marginal likelihood evaluation), with negligible memory requirements.\nOf the infinite number of possible basis functions only an extremely small subset are\nof any relevance to the GP being approximated. In GP regression the model functions\nare conditioned on a covariance function (kernel), which imposes desired properties on the\nsolutions. We choose the basis functions such that they are as close as possible (w.r.t. the\nFrobenius norm) to those of the particular covariance function. Our method gives the exact\neigendecomposition of a GP that has been constrained to be zero at the boundary of the\ndomain.\nThe method allows for theoretical analysis of the error induced by the truncation of\nthe series and the boundary effects. This is something new in this context and extremely\nimportant, for example, in medical imaging applications. The approximative eigendecom-\nposition also opens a range of interesting possibilities for further analysis. In learning curve\nestimation, the eigenvalues of the Gaussian process can now be directly approximated. For\nexample, we can approximate the Opper\u2013Vivarelli bound (Opper and Vivarelli, 1999) as\n?OV(n) \u2248 \u03c32\nn\n?\nj\nS(?\u03bbj)\n\u03c32\nn+ nS(?\u03bbj).(64)\nSollich\u2019s eigenvalue based bounds (Sollich and Halees, 2002) can be approximated and an-\nalyzed in an analogous way.\nHowever, some of these abilities come with a cost. As demonstrated throughout the\npaper, restraining the domain to boundary conditions introduces edge effects. These are,\nhowever, known and can be accounted for. Extrapolating with a stationary covariance\nfunction outside the training inputs only causes the predictions to revert to the prior mean\nand variance. Therefore we consider the boundary effects a minor problem for practical use.\nA more severe limitation for applicability is the \u2018full\u2019 nature of the spectrum.\nhigh-dimensional inputs the required number of basis functions grows large. There is, how-\never, a substantial call for GPs in low-dimensional problems, especially in medical imaging\napplications (typical number of training data points in millions) and spatial problems. Fur-\nthermore, the mathematical formulation of the method provides a foundation for future\nsparse methods to build upon. A step in this direction has been taken by L\u00b4 azaro-Gredilla\net al. (2010), which has shown good results in high-dimensional input spaces.\nFor\n23"},{"page":24,"text":"Solin and S\u00a8 arkk\u00a8 a\nAppendix A. Proofs of Convergence Theorems\nA.1 Proof of Theorem 4.1\nThe Wiener\u2013Khinchin identity and the symmetry of the spectral density allows us to write\n?\u221e\n=1\n\u03c0\n0\nk(x,x?) =\n1\n2\u03c0\n\u2212\u221e\nS(\u03c9) exp(\u2212i\u03c9 (x \u2212 x?))d\u03c9\n?\u221e\nS(\u03c9) cos(\u03c9 (x \u2212 x?))d\u03c9.\n(65)\nIn a one-dimensional domain \u03a9 = [\u2212L,L] with Dirichet boundary conditions we have an\nm-term approximation of the form\n?\u03c0 j\n?km(x,x?) =\nm\n?\nj=1\nS\n2L\n?\n1\nLsin\n?\u03c0 j (x + L)\n2L\n?\nsin\n?\u03c0 j (x?+ L)\n2L\n?\n.\n(66)\nWe start by showing the convergence by growing the domain and therefore first consider an\napproximation with an infinite number of terms m = \u221e:\n\u221e\n?\nLemma A.1. There exists a constant D1such that for all x,x?\u2208 [\u2212?L,?L] we have\n?\n\u22121\n?k\u221e(x,x?) =\nj=1\nS(?\u03bbj)\u03c6j(x)\u03c6j(x?). (67)\n?????\n\u221e\nj=1\nS\n?\u03c0 j\n2L\n?\n1\nLsin\n?\u03c0 j (x + L)\n2L\n?\nsin\n?\u03c0 j (x?+ L)\n2L\n?\n\u03c0\n?\u221e\n0\nS(\u03c9) cos(\u03c9 (x \u2212 x?))d\u03c9\n?????\u2264D1\nL.\n(68)\nThat is,\n????k\u221e(x,x?) \u2212 k(x,x?)\n?\u03c0 j (x + L)\n?\u03c0 j (x \u2212 x?)\n?\u03c0 2j\n?\u03c0 (2j \u2212 1)\n??? \u2264D1\n?\u03c0 j (x?+ L)\n1\n2L\n?\u03c0 (2j \u2212 1)\n? ?\nL,\nfor x,x?\u2208 [\u2212?L,?L].\n?\n(69)\nProof. We can rewrite the summation in (68) as\n\u221e\n?\nj=1\nS\n?\u03c0 j\n\u221e\n?\n2L\n?\n?\u03c0 j\n\u221e\n?\n\u221e\n?\n1\nLsin\n?\n?\n2L\n?\nsin\n2L\n(70)\n=\nj=1\nS\n2L\ncos\n2L\n?\n\u2212\n1\n2L\nj=1\nS\n2L\n?\n\u2212 S\n2L\n?\u03c0 2j (x + x?)\n??\ncos\n?\u03c0 2j (x + x?)\n?\n2L\n?\n\u2212\n1\n2L\nj=1\nS\n2L\ncos\n2L\n\u2212 cos\n?\u03c0 (2j \u2212 1)(x + x?)\n2L\n??\n(71)\n24"},{"page":25,"text":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression\nFirst consider the first term above in Equation (71). Let \u2206 =\nto have the form\n\u03c0\n2L, and thus it can be seen\n1\n\u03c0\n\u221e\n?\nj=1\nS (\u2206j) cos?\u2206j (x \u2212 x?)?\u2206,\n(72)\nwhich can be recognized as a Riemannian sum approximation to the integral1\nx?))d\u03c9. Because we assume that x,x?\u2208 [\u2212?L,?L], the integrand and its derivatives are\nand hence we conclude that\n\u03c0\n?\u221e\n0S(\u03c9) cos(\u03c9 (x\u2212\nbounded and because the integral?\u221e\n?????\nfor some constant D2.\nThe second summation term in Equation (71) can also be interpreted as a Riemann sum\nif we set \u2206 =\u03c0\nL:\n\u2212\u221eS(\u03c9)d\u03c9 < \u221e, the Riemannian integral converges,\n\u221e\n?\nj=1\nS\n?\u03c0 j\n2L\n?\ncos\n?\u03c0 j (x \u2212 x?)\n2L\n?\n1\n2L\u22121\n\u03c0\n?\u221e\n0\nS(\u03c9) cos(\u03c9 (x \u2212 x?))d\u03c9\n?????\u2264D2\nL\n(73)\n1\n2L\n\u221e\n?\nj=1\n[S (\u2206j) \u2212 S (\u2206j \u2212 \u2206\/2)] cos?\u2206(x + x?)?\n\u221e\n?\n?\u221e\n=\n1\n2L\nj=1\n1\n\u2206[S (\u2206j) \u2212 S (\u2206j \u2212 \u2206\/2)] cos?\u2206(x + x?)?\u2206\n2S?(\u03c9) cos(\u03c9 (x + x?))d\u03c9.\n\u2248\n1\n2L\n0\n(74)\nBecause we assumed that also the second derivative of S(\u00b7) is bounded, the derivative and\nthe Riemann sum converge (alternatively, we could analyze the sums as a Stieltjes integral\nwith respect to a differentiable function), and hence the exists a constant D?\n3such that\n?????\n1\n2L\n\u221e\n?\nj=1\n[S (\u2206j) \u2212 S (\u2206j \u2212 \u2206\/2)] cos?\u2206(x + x?)?\n\u2212\n1\n2L\n?\u221e\n0\n2S?(\u03c9) cos(\u03c9 (x + x?))d\u03c9\n?????\u2264D?\n3\nL\n(75)\nBut now because?\u221e\n02S?(\u03c9) cos(\u03c9 (x + x?))d\u03c9 < \u221e, this actually implies that\n?????\n1\n2L\n\u221e\n?\nj=1\n?\nS\n?\u03c0 2j\n2L\n?\n\u2212 S\n?\u03c0 (2j \u2212 1)\n2L\n??\ncos\n?\u03c0 2j (x + x?)\n2L\n??????\u2264D3\nL\n(76)\n25"},{"page":26,"text":"Solin and S\u00a8 arkk\u00a8 a\nfor some constant D3. For the last summation term in Equation (71) we get the interpre-\ntation\n?\u03c0 (2j \u2212 1)\n1\n2L\n0\n= \u22121\n2L\n0\n1\n2L\n\u221e\n?\n\u2248\nj=1\nS\n2L\n? ?\n?\ncos\n?\u03c0 2j (x + x?)\n2L\n?\n\u2212 cos\n?\u03c0 (2j \u2212 1)(x + x?)\n2L\n??\n?\u221e\n?\u221e\nS(\u03c9)2\nd\nd\u03c9cos(\u03c9 (x + x?))\n?\nd\u03c9\nS(\u03c9)2(x + x?) sin(\u03c9 (x + x?))d\u03c9,(77)\nwhich by boundedness of x and x?implies\n?????\nfor some constant D4. The result now follows by combining (73), (76), and (78) via the\ntriangle inequality.\n1\n2L\n\u221e\n?\nj=1\nS\n?\u03c0 (2j \u2212 1)\n2L\n? ?\ncos\n?\u03c0 2j (x + x?)\n2L\n?\n\u2212 cos\n?\u03c0 (2j \u2212 1)(x + x?)\n2L\n???????\u2264D4\nL\n(78)\nLet us now return to the original question, and consider what happens when we replace\nthe infinite sum approximation with a finite m number of terms. We are now interested in\n?k\u221e(x,x?) \u2212?km(x,x?) =\nLemma A.2. There exists a constant D5such that for all x,x?\u2208 [\u2212?L,?L] we have\n\u221e\n?\nj=m+1\nS\n?\u03c0 j\n2L\n?\n1\nLsin\n?\u03c0 j (x + L)\n2L\n?\nsin\n?\u03c0 j (x?+ L)\n2L\n?\n. (79)\n????k\u221e(x,x?) \u2212?km(x,x?)\n??? \u2264D5\nL\n+2\n\u03c0\n?\u221e\n2L\n\u03c0 m\nS(\u03c9)d\u03c9.(80)\nProof. Because the sinusoidals are bounded by unity, we get\n??????\nThe right-hand term can now be seen as Riemann sum approximation to the integral\n?\u03c0 j\n\u221e\n?\nj=m+1\nS\n?\u03c0 j\n2L\n?\n1\nLsin\n?\u03c0 j (x + L)\n2L\n?\nsin\n?\u03c0 j (x?+ L)\n2L\n???????\n\u2264\n??????\n\u221e\n?\nj=m+1\nS\n?\u03c0 j\n2L\n?\n1\nL\n??????\n.(81)\n\u221e\n?\nj=m+1\nS\n2L\n?\n1\nL\u22482\n\u03c0\n?\u221e\n2L\n\u03c0 m\nS(\u03c9)d\u03c9. (82)\nOur assumptions ensure that this integral converges and hence there exists a constant D5\nsuch that\n?????\n26\n\u221e\n?\nj=m+1\nS\n?\u03c0 j\n2L\n?\n1\nL\u22122\n\u03c0\n?\u221e\n2L\n\u03c0 m\nS(\u03c9)d\u03c9\n?????\u2264D5\nL.\n(83)"},{"page":27,"text":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression\nHence by the triangle inequality we get\n?????\n\u221e\n?\nj=m+1\nS\n?\u03c0 j\n2L\n?\n1\nL\n?????=\n\u2264\n?????\n\u221e\n?\n\u221e\n?\n+2\nj=m+1\nS\n?\u03c0 j\n?\u03c0 j\n?\u221e\n2L\n2L\n?\n?\n1\nL\u22122\n\u03c0\n?\u221e\n2L\n?\u221e\n2L\n\u03c0 m\nS(\u03c9)d\u03c9 +2\n\u03c0\n?\u221e\n2L\n?\u221e\n\u03c0 m\nS(\u03c9)d\u03c9\n?????\n?????\nj=m+1\nS\n2L\n1\nL\u22122\n\u03c0\n\u03c0 m\nS(\u03c9)d\u03c9\n?????+2\n\u03c0\n\u03c0 m\n2L\nS(\u03c9)d\u03c9\n\u2264D5\nL\u03c0\n\u03c0 m\nS(\u03c9)d\u03c9 (84)\nand thus the result follows.\nThe above result can now easily be combined to a proof of the one-dimensional conver-\ngence theorem as follows:\nProof of Theorem 4.1. The first result follows by combining Lemmas A.1 and A.2 via the\ntriangle inequality. Because our assumptions imply that\n?\u221e\nlim\nx\u2192\u221e\nx\nS(\u03c9)d\u03c9 = 0,(85)\nfor any fixed L we have\nlim\nm\u2192\u221e\n?\nE\nL+2\n\u03c0\n?\u221e\n2L\n\u03c0 m\nS(\u03c9)d\u03c9\n?\n\u2192E\nL.\n(86)\nIf we now take the limit L \u2192 \u221e, the second result in the theorem follows.\nA.2 Proof of Theorem 4.4\nWhen x \u2208 Rd, the Wiener\u2013Khinchin identity and symmetry of the spectral density imply\nthat\n?\n1\n\u03c0d\n00\nk=1\nk(x,x?) =\n1\n(2\u03c0)d\nRdS(\u03c9) exp(\u2212i\u03c9T(x \u2212 x?))d\u03c9\n?\u221e\n=\n?\u221e\n\u00b7\u00b7\u00b7\nS(\u03c9)\nd?\ncos(\u03c9k(xk\u2212 x?\nk))d\u03c91\u00b7\u00b7\u00b7 d\u03c9d.\n(87)\nThe m = \u02c6 mdterm approximation now has the form\n?km(x,x?) =\nAs in the one-dimensional problem we start by considering the case where \u02c6 m = \u221e.\n\u02c6 m\n?\nj1,...,jd=1\nS\n?\u03c0 j1\n2L1,...,\u03c0 jd\n2Ld\n?\nd?\nk=1\n1\nLk\nsin\n?\u03c0 jk(xk+ Lk)\n2Lk\n?\nsin\n?\u03c0 jk(x?\nk+ Lk)\n2Lk\n?\n.\n(88)\n27"},{"page":28,"text":"Solin and S\u00a8 arkk\u00a8 a\nLemma A.3. There exists a constant D1such that for all x,x?\u2208 [\u2212?L,?L]dwe have\n?\n1\n\u03c0d\n00\nk=1\n?????\n\u221e\nj1,...,jd=1\nS\n?\u03c0 j1\n2L1,...,\u03c0 jd\n2Ld\n?\n?\u221e\nd?\nk=1\n1\nLk\nsin\n?\u03c0 jk(xk+ Lk)\nd?\n2Lk\n?\nsin\n?\u03c0 jk(x?\nk+ Lk)\n2Lk\n?????\u2264 D1\n?\nd\n?\n\u2212\n?\u221e\n\u00b7\u00b7\u00b7\nS(\u03c9)\ncos(\u03c9k(x \u2212 x?))d\u03c91\u00b7\u00b7\u00b7 d\u03c9d\nk=1\n1\nLk.(89)\nThat is,\n????k\u221e(x,x?) \u2212 k(x,x?)\n??? \u2264 D1\nd\n?\nk=1\n1\nLk\nfor x,x?\u2208 [\u2212?L,?L]d. (90)\nProof. We can separate the summation over j1in the summation term above as follows:\n?\nj1=1\n\u221e\n?\nj2,...,jd=1\n\u221e\n?\nS\n?\u03c0 j1\n2L1,...,\u03c0 jd\n2Ld\n?\nd?\nsin\n?\u03c0 j1(x1+ L1)\n?\u03c0 jk(xk+ Lk)\n2L1\n?\nsin\n?\u03c0 j1(x?\n?\n1+ L1)\n2L1\n?\u03c0 jk(x?\n??\n\u00d7\nk=2\n1\nLk\nsin\n2Lk\nsin\nk+ Lk)\n2Lk\n?\n.(91)\nBy Lemma A.1 there now exists a constant D1,1such that\n?????\n\u22121\n\u03c0\n0\n\u221e\n?\nj1=1\nS\n?\u03c0 j1\n2L1,...,\u03c0 jd\n2Ld\n?\nsin\n?\u03c0 j1(x1+ L1)\n?\u221e\n2L1\n?\nsin\n?\u03c0 j1(x?\n?\n1+ L1)\n2L1\n?\nS\n?\n\u03c91,\u03c0 j2\n2L2,...,\u03c0 jd\n2Ld\ncos(\u03c91(x1\u2212 x?\n1))d\u03c91\n?????\u2264D1,1\nL1\n.(92)\nThe triangle inequality then gives\n?????\n\u221e\n?\nj1,...,jd=1\nS\n?\u03c0 j1\n2L1,...,\u03c0 jd\n2Ld\n?\nd?\n?\u221e\n?\n?\u03c0 jk(xk+ Lk)\n?\u221e\nk=1\n1\nLk\nsin\n?\u03c0 jk(xk+ Lk)\nd?\n2L2,...,\u03c0 jd\n2Ld\n?\nd?\n2Lk\n?\nsin\n?\u03c0 jk(x?\nk+ Lk)\n2Lk\n?????\n1))d\u03c91\n?\n\u2212\n1\n\u03c0d\n?\u221e\n0\n\u00b7\u00b7\u00b7\n0\nS(\u03c9)\nk=1\ncos(\u03c9j(xk\u2212 x?\n?\n?\u03c0 jk(x?\nk))d\u03c91\u00b7\u00b7\u00b7 d\u03c9d\n(93)\n\u2264D1,1\nL1\n+\n?????\n1\n\u03c0\n\u221e\n?\nd?\n1\n\u03c0d\nj2,...,jd=1\n?\u221e\n0\nS\u03c91,\u03c0 j2\ncos(\u03c91(x1\u2212 x?\n\u00d7\nk=2\n1\nLk\n?\u221e\nsin\n2Lk\nsin\nk+ Lk)\n2Lk\n?\n\u2212\n0\n\u00b7\u00b7\u00b7\n0\nS(\u03c9)\nk=1\ncos(\u03c9k(xk\u2212 x?\nk))d\u03c91\u00b7\u00b7\u00b7 d\u03c9d\n?????.(94)\nWe can now similarly bound with respect to the summations over j2,...,jdwhich leads to a\nbound of the form\nLd. Taking D1= maxkD1,kleads to the desired result.\nD1,1\nL1+\u00b7\u00b7\u00b7+D1,d\n28"},{"page":29,"text":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression\nNow we can consider what happens in the finite truncation of the series. That is, we\nanalyze the following residual sum\n?k\u221e(x,x?) \u2212?km(x,x?)\n=\nS\n2Ld\nk=1\nLk\n\u221e\n?\nj1,...,jd= \u02c6 m+1\n?\u03c0 j1\n2L1,...,\u03c0 jd\n?\nd?\n1\nsin\n?\u03c0 jk(xk+ Lk)\n2Lk\n?\nsin\n?\u03c0 jk(x?\nk+ Lk)\n2Lk\n?\n(95)\n.\nLemma A.4. The exists a constant D2such that for all x,x?\u2208 [\u2212?L,?L]dwe have\n????k\u221e(x,x?) \u2212?km(x,x?)\n??? \u2264D2\nL\n+\n1\n\u03c0d\n?\n?\u03c9?\u2265\u03c0 \u02c6 m\n2L\nS(\u03c9)d\u03c9,\n(96)\nwhere L = minkLk.\nProof. We can write the following bound\n?????\n\u2264\n\u221e\n?\nj1,...,jd= \u02c6 m+1\nS\n?\u03c0 j1\n?????\n\u03c0\n2L1giving\n?\u03c0 j1\n2L1,...,\u03c0 jd\n2Ld\n?\nd?\n?\u03c0 j1\nk=1\n1\nLk\nsin\n?\u03c0 jk(xk+ Lk)\n?\nk=1\n2Lk\n?\nsin\n?\u03c0 jk(x?\nk+ Lk)\n2Lk\n??????\n\u221e\n?\nj1,...,jd= \u02c6 m+1\nS\n2L1,...,\u03c0 jd\n2Ld\nd?\n1\nLk\n?????.(97)\nThe summation over the index j1can now be interpreted as a Riemann integral approxi-\nmation with \u2206 =\n?????\n\u22122\n\u03c0\nj2,...,jd= \u02c6 m+1\n2L1\n\u221e\n?\nj1,...,jd= \u02c6 m+1\nS\n2L1,...,\u03c0 jd\n2Ld\n?\nd?\nk=1\n1\nLk\n\u221e\n?\n?\u221e\n\u03c0 \u02c6 m\nS\n?\n\u03c91,\u03c0 j2\n2L2,...,\u03c0 jd\n2Ld\n?\nd\u03c91\nd?\nk=2\n1\nLk\n?????\u2264D2,1\nL1\n.(98)\nUsing a similar argument again, we get\n?????\n\u03c02\nj3,...,jd= \u02c6 m+1\n2\n\u03c0\n\u221e\n?\nj2,...,jd= \u02c6 m+1\n?\u221e\n2L1\n\u221e\n?\n\u03c0 \u02c6 m\nS\n?\n?\u221e\n2L1\n\u03c91,\u03c0 j2\n2L2,...,\u03c0 jd\n2Ld\n?\nd\u03c91\nd?\nk=2\n1\nLk\n\u221222\n\u03c0 \u02c6 m\n?\u221e\n2L2\n\u03c0 \u02c6 m\nS\n?\n\u03c91,\u03c92,\u03c0 j3\n2L3,...,\u03c0 jd\n2Ld\n?\nd\u03c91d\u03c92\nd?\nk=3\n1\nLk\n?????\u2264D2,2\nL2\n. (99)\nAfter repeating this for all the indexes, by forming a telescoping sum of the terms and\napplying the triangle inequality then gives\n?????\n\u2212\n\u03c0\n\u03c0 \u02c6 m\n2L1\n2Ld\n\u221e\n?\nj1,...,jd= \u02c6 m+1\nS\n?\u03c0 j1\n2L1,...,\u03c0 jd\n2Ld\n?d?\u221e\n?\nd?\nk=1\n1\nLk\n?2\n\u00b7\u00b7\u00b7\n?\u221e\n\u03c0 \u02c6 m\nS(\u03c91,...,\u03c9d)d\u03c91\u00b7\u00b7\u00b7 d\u03c9d\n?????\u2264\nd\n?\nk=1\nD2,k\nLk\n.(100)\n29"},{"page":30,"text":"Solin and S\u00a8 arkk\u00a8 a\nApplying the triangle inequality again gives\n?????\n\u221e\n?\nj1,...,jd= \u02c6 m+1\nS\n?\u03c0 j1\n2L1,...,\u03c0 jd\n2Ld\n?\nd?\nk=1\n1\nLk\n?2\n?????\n\u2264\nd\n?\nk=1\nD2,k\nLk\n+\n\u03c0\n?d?\u221e\n\u03c0 \u02c6 m\n2L1\n\u00b7\u00b7\u00b7\n?\u221e\n2Ld\n\u03c0 \u02c6 m\nS(\u03c91,...,\u03c9d)d\u03c91\u00b7\u00b7\u00b7 d\u03c9d.(101)\nBy interpreting the latter integral as being over the positive exterior of a rectangular hy-\npercuboid and bounding it by a integral over exterior of a hypersphere which fits inside the\ncuboid, we can bound the expression by\nd\n?\nk=1\nD2,k\nLk\n+\n1\n\u03c0d\n?\n?\u03c9?\u2265\u03c0 \u02c6 m\n2L\nS(\u03c9)d\u03c9.(102)\nThe first term can be further bounded by replacing Lks with their minimum L and by\ndefining a new constant D2which is d times the maximum of D2,k. This leads to the final\nform of the result.\nProof of Theorem 4.4. Analogous to the one-dimensional case. That is, we combine the\nresults of the above lemmas using the triangle inequality.\nReferences\nNaum I. Akhiezer and Izrail\u2019 M. Glazman. Theory of Linear Operators in Hilbert Space.\nDover, New York, 1993.\nChristopher T. H. Baker. The Numerical Treatment of Integral Equations. Clarendon press,\nOxford, 1977.\nSteve Brooks, Andrew Gelman, Galin L. Jones, and Xiao-Li Meng. Handbook of Markov\nChain Monte Carlo. Chapman & Hall\/CRC, 2011.\nKrzysztof Chalupka, Christopher K. I. Williams, and Iain Murray. A framework for evaluat-\ning approximation methods for Gaussian process regression. Journal of Machine Learning\nResearch, 14:333\u2013350, 2013.\nRichard Courant and David Hilbert. Methods of Mathematical Physics, volume 1. Wiley-\nVCH, 2008.\nLehel Csat\u00b4 o and Manfred Opper. Sparse online Gaussian processes. Neural Computation,\n14(3):641\u2013668, 2002.\nGiuseppe Da Prato and Jerzy Zabczyk. Stochastic Equations in Infinite Dimensions, vol-\nume 45 of Encyclopedia of Mathematics and its Applications. Cambridge University Press,\n1992.\n30"},{"page":31,"text":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression\nSimon Duane, Anthony D. Kennedy, Brian J. Pendleton, and Duncan Roweth. Hybrid\nMonte Carlo. Physics Letters B, 195(2):216\u2013222, 1987.\nJochen Fritz, Insa Neuweiler, and Wolfgang Nowak. Application of FFT-based algorithms\nfor large-scale universal kriging problems.\n2009.\nMathematical Geosciences, 41(5):509\u2013533,\nGene H. Golub and Charles F. Van Loan. Matrix Computations. Johns Hopkins University\nPress, Baltimore, third edition, 1996.\nJames Hensman, Nicol` o Fusi, and Neil D. Lawrence. Gaussian processes for big data. In\nProceedings of the 29th Conference on Uncertainty in Artificial Intelligence (UAI 2013),\npages 282\u2013290, 2013.\nMiguel L\u00b4 azaro-Gredilla. Sparse Gaussian Processes for Large-Scale Machine Learning. PhD\nthesis, Universidad Carlos III de Madrid, 2010.\nMiguel L\u00b4 azaro-Gredilla, Joaquin Qui\u02dc nonero-Candela, Carl Edward Rasmussen, and\nAn\u00b4 \u0131bal R. Figueiras-Vidal. Sparse spectrum Gaussian process regression. Journal of\nMachine Learning Research, 11:1865\u20131881, 2010.\nPeter J. Lenk. Towards a practicable Bayesian nonparametric density estimator. Biometrika,\n78(3):531\u2013543, 1991.\nFinn Lindgren, H\u02da avard Rue, and Johan Lindstr\u00a8 om. An explicit link between Gaussian\nfields and Gaussian Markov random fields: The stochastic partial differential equation\napproach. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73\n(4):423\u2013498, 2011.\nJun S. Liu. Monte Carlo Strategies in Scientific Computing. Springer, New York, 2001.\nRadford M. Neal. MCMC using Hamiltonian dynamics. In Steve Brooks, Andrew Gelman,\nGalin L. Jones, and Xiao-Li Meng, editors, Handbook of Markov Chain Monte Carlo,\nchapter 5. Chapman & Hall\/CRC, 2011.\nManfred Opper and Francesco Vivarelli. General bounds on Bayes errors for regression with\nGaussian processes. In Advances in Neural Information Processing Systems, volume 11,\npages 302\u2013308, 1999.\nChristopher J. Paciorek. Bayesian smoothing with Gaussian processes using Fourier basis\nfunctions in the spectralGP package. Journal of Statistical Software, 19(2):1\u201338, 2007.\nJoaquin Qui\u02dc nonero-Candela and Carl Edward Rasmussen. A unifying view of sparse approx-\nimate Gaussian process regression. Journal of Machine Learning Research, 6:1939\u20131959,\n2005a.\nJoaquin Qui\u02dc nonero-Candela and Carl Edward Rasmussen. Analysis of some methods for\nreduced rank Gaussian process regression. In Switching and Learning in Feedback Systems,\nvolume 3355 of Lecture Notes in Computer Science, pages 98\u2013127. Springer, 2005b.\n31"},{"page":32,"text":"Solin and S\u00a8 arkk\u00a8 a\nCarl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine\nLearning. The MIT Press, 2006.\nSimo S\u00a8 arkk\u00a8 a and Jouni Hartikainen. Infinite-dimensional Kalman filtering approach to\nspatio-temporal Gaussian process regression. In Proceedings of the Fifteenth International\nConference on Artificial Intelligence and Statistics (AISTATS 2012), volume 22 of JMLR\nWorkshop and Conference Proceedings, pages 993\u20131001, 2012.\nSimo S\u00a8 arkk\u00a8 a, Arno Solin, and Jouni Hartikainen. Spatiotemporal learning via infinite-\ndimensional Bayesian filtering and smoothing. IEEE Signal Processing Magazine, 30(4):\n51\u201361, 2013.\nMatthias Seeger, Christopher K. I. Williams, and Neil D. Lawrence. Fast forward selection\nto speed up sparse Gaussian process regression. In Proceedings of the 9th International\nWorkshop on Artificial Intelligence and Statistics (AISTATS 2003), 2003.\nRalph E. Showalter. Hilbert Space Methods in Partial Differential Equations. Dover Publi-\ncations, 2010.\nMikhail A. Shubin. Pseudodifferential Operators and Spectral Theory. Springer Series in\nSoviet Mathematics. Springer-Verlag, 1987.\nAlex J. Smola and Peter Bartlett. Sparse greedy Gaussian process regression. In Advances\nin Neural Information Processing Systems, volume 13, 2001.\nEdward Snelson and Zoubin Ghahramani. Sparse Gaussian processes using pseudo-inputs.\nIn Advances in Neural Information Processing Systems, volume 18, pages 1259\u20131266,\n2006.\nPeter Sollich and Anason Halees. Learning curves for Gaussian process regression: Approx-\nimations and bounds. Neural Computation, 14(6):1393\u20131428, 2002.\nMichalis K. Titsias. Variational learning of inducing variables in sparse Gaussian processes.\nIn Proceedings of the 12th International Conference on Artificial Intelligence and Statis-\ntics (AISTATS 2009), volume 5 of JMLR Workshop and Conference Proceedings, pages\n567\u2013574, 2009.\nJarno Vanhatalo, Ville Pietil\u00a8 ainen, and Aki Vehtari. Approximate inference for disease\nmapping with sparse Gaussian processes. Statistics in Medicine, 29(15):1580\u20131607, 2010.\nJarno Vanhatalo, Jaakko Riihim\u00a8 aki, Jouni Hartikainen, Pasi Jyl\u00a8 anki, Ville Tolvanen, and\nAki Vehtari. GPstuff: Bayesian modeling with Gaussian processes. Journal of Machine\nLearning Research, 14:1175\u20131179, 2013.\nChristopher K. I. Williams and Matthias Seeger. The effect of the input density distribu-\ntion on kernel-based classifiers. In Proceedings of the 17th International Conference on\nMachine Learning, 2000.\nChristopher K. I. Williams and Matthias Seeger. Using the Nystr\u00a8 om method to speed up\nkernel machines. In Advances in Neural Information Processing Systems, volume 13,\n2001.\n32"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Simo_Saerkkae\/publication\/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression\/links\/54a3dc790cf256bf8bb17c1c.pdf","widgetId":"rgw26_56ab1df2a6baa"},"id":"rgw26_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=259844876&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw27_56ab1df2a6baa"},"id":"rgw27_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=259844876&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":259844876,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":259844876,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2066255729,"url":"researcher\/2066255729_Andreas_Svensson","fullname":"Andreas Svensson","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A279258531418122%401443591830745_m\/Andreas_Svensson2.png"},{"id":71647262,"url":"researcher\/71647262_Arno_Solin","fullname":"Arno Solin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69762528,"url":"researcher\/69762528_Simo_Saerkkae","fullname":"Simo S\u00e4rkk\u00e4","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":35821922,"url":"researcher\/35821922_Thomas_B_Schoen","fullname":"Thomas B. Sch\u00f6n","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[[]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Jun 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":2,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/277959163_Computationally_Efficient_Bayesian_Learning_of_Gaussian_Process_State_Space_Models","usePlainButton":true,"publicationUid":277959163,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/277959163_Computationally_Efficient_Bayesian_Learning_of_Gaussian_Process_State_Space_Models","title":"Computationally Efficient Bayesian Learning of Gaussian Process State Space Models","displayTitleAsLink":true,"authors":[{"id":2066255729,"url":"researcher\/2066255729_Andreas_Svensson","fullname":"Andreas Svensson","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A279258531418122%401443591830745_m\/Andreas_Svensson2.png"},{"id":71647262,"url":"researcher\/71647262_Arno_Solin","fullname":"Arno Solin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69762528,"url":"researcher\/69762528_Simo_Saerkkae","fullname":"Simo S\u00e4rkk\u00e4","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":35821922,"url":"researcher\/35821922_Thomas_B_Schoen","fullname":"Thomas B. Sch\u00f6n","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Gaussian processes allow for flexible specification of prior assumptions of unknown dynamics in state space models. We present a procedure for efficient Bayesian learning in Gaussian process state space models, where the representation is formed by projecting the problem onto a set of approximate eigenfunctions derived from the prior covariance structure. Learning under this family of models can be conducted using a carefully crafted particle MCMC algorithm. This scheme is computationally efficient and yet allows for a fully Bayesian treatment of the problem. Compared to conventional system identification tools or existing learning methods, we show competitive performance and reliable quantification of uncertainties in the model.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/277959163_Computationally_Efficient_Bayesian_Learning_of_Gaussian_Process_State_Space_Models","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Thomas_Schoen3\/publication\/277959163_Computationally_Efficient_Bayesian_Learning_of_Gaussian_Process_State_Space_Models\/links\/559bf2c708ae0035df233e68.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Thomas_Schoen3","sourceName":"Thomas B. Sch\u00f6n","hasSourceUrl":true},"publicationUid":277959163,"publicationUrl":"publication\/277959163_Computationally_Efficient_Bayesian_Learning_of_Gaussian_Process_State_Space_Models","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/277959163_Computationally_Efficient_Bayesian_Learning_of_Gaussian_Process_State_Space_Models\/links\/559bf2c708ae0035df233e68\/smallpreview.png","linkId":"559bf2c708ae0035df233e68","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=277959163&reference=559bf2c708ae0035df233e68&eventCode=&origin=publication_list","widgetId":"rgw31_56ab1df2a6baa"},"id":"rgw31_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=277959163&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"559bf2c708ae0035df233e68","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":259844876,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/277959163_Computationally_Efficient_Bayesian_Learning_of_Gaussian_Process_State_Space_Models\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw30_56ab1df2a6baa"},"id":"rgw30_56ab1df2a6baa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=277959163&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":71410443,"url":"researcher\/71410443_James_Hensman","fullname":"James Hensman","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277099177889798%401443077000994_m"},{"id":2048369253,"url":"researcher\/2048369253_Alexander_G_de_G_Matthews","fullname":"Alexander G. de G. Matthews","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70871340,"url":"researcher\/70871340_Maurizio_Filippone","fullname":"Maurizio Filippone","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[[]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Jun 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":1,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes","usePlainButton":true,"publicationUid":278332447,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes","title":"MCMC for Variationally Sparse Gaussian Processes","displayTitleAsLink":true,"authors":[{"id":71410443,"url":"researcher\/71410443_James_Hensman","fullname":"James Hensman","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277099177889798%401443077000994_m"},{"id":2048369253,"url":"researcher\/2048369253_Alexander_G_de_G_Matthews","fullname":"Alexander G. de G. Matthews","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70871340,"url":"researcher\/70871340_Maurizio_Filippone","fullname":"Maurizio Filippone","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Gaussian process (GP) models form a core part of probabilistic machine\nlearning. Considerable research effort has been made into attacking three\nissues with GP models: how to compute efficiently when the number of data is\nlarge; how to approximate the posterior when the likelihood is not Gaussian and\nhow to estimate covariance function parameter posteriors. This paper\nsimultaneously addresses these, using a variational approximation to the\nposterior which is sparse in support of the function but otherwise free-form.\nThe result is a Hybrid Monte-Carlo sampling scheme which allows for a\nnon-Gaussian approximation over the function values and covariance parameters\nsimultaneously, with efficient computations based on inducing-point sparse GPs.\nCode to replicate each experiment in this paper will be available shortly.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Maurizio_Filippone\/publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes\/links\/558a857008aee1fc9174ea84.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Maurizio_Filippone","sourceName":"Maurizio Filippone","hasSourceUrl":true},"publicationUid":278332447,"publicationUrl":"publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes\/links\/558a857008aee1fc9174ea84\/smallpreview.png","linkId":"558a857008aee1fc9174ea84","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=278332447&reference=558a857008aee1fc9174ea84&eventCode=&origin=publication_list","widgetId":"rgw33_56ab1df2a6baa"},"id":"rgw33_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=278332447&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"558a857008aee1fc9174ea84","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":259844876,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw32_56ab1df2a6baa"},"id":"rgw32_56ab1df2a6baa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=278332447&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithSlurp","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2052101055,"url":"researcher\/2052101055_Thomas_Nickson","fullname":"Thomas Nickson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2051950921,"url":"researcher\/2051950921_Tom_Gunter","fullname":"Tom Gunter","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2051935693,"url":"researcher\/2051935693_Chris_Lloyd","fullname":"Chris Lloyd","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2051939493,"url":"researcher\/2051939493_Michael_A_Osborne","fullname":"Michael A Osborne","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":[[]],"isFulltext":false,"isSlurp":true,"isNoText":false,"publicationType":"Article","publicationDate":"Oct 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283335187_Blitzkriging_Kronecker-structured_Stochastic_Gaussian_Processes","usePlainButton":true,"publicationUid":283335187,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/283335187_Blitzkriging_Kronecker-structured_Stochastic_Gaussian_Processes","title":"Blitzkriging: Kronecker-structured Stochastic Gaussian Processes","displayTitleAsLink":true,"authors":[{"id":2052101055,"url":"researcher\/2052101055_Thomas_Nickson","fullname":"Thomas Nickson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2051950921,"url":"researcher\/2051950921_Tom_Gunter","fullname":"Tom Gunter","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2051935693,"url":"researcher\/2051935693_Chris_Lloyd","fullname":"Chris Lloyd","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2051939493,"url":"researcher\/2051939493_Michael_A_Osborne","fullname":"Michael A Osborne","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083725021,"url":"researcher\/2083725021_Stephen_Roberts","fullname":"Stephen Roberts","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"We present Blitzkriging, a new approach to fast inference for Gaussian\nprocesses, applicable to regression, optimisation and classification.\nState-of-the-art (stochastic) inference for Gaussian processes on very large\ndatasets scales cubically in the number of 'inducing inputs', variables\nintroduced to factorise the model. Blitzkriging shares state-of-the-art scaling\nwith data, but reduces the scaling in the number of inducing points to\napproximately linear. Further, in contrast to other methods, Blitzkriging: does\nnot force the data to conform to any particular structure (including\ngrid-like); reduces reliance on error-prone optimisation of inducing point\nlocations; and is able to learn rich (covariance) structure from the data. We\ndemonstrate the benefits of our approach on real data in regression,\ntime-series prediction and signal-interpolation experiments.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283335187_Blitzkriging_Kronecker-structured_Stochastic_Gaussian_Processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1510.07965","sourceName":"de.arxiv.org","hasSourceUrl":true},"publicationUid":283335187,"publicationUrl":"publication\/283335187_Blitzkriging_Kronecker-structured_Stochastic_Gaussian_Processes","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/283335187_Blitzkriging_Kronecker-structured_Stochastic_Gaussian_Processes\/links\/563bf2b808ae405111a77d9e\/smallpreview.png","linkId":"563bf2b808ae405111a77d9e","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=283335187&reference=563bf2b808ae405111a77d9e&eventCode=&origin=publication_list","widgetId":"rgw35_56ab1df2a6baa"},"id":"rgw35_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=283335187&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":259844876,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283335187_Blitzkriging_Kronecker-structured_Stochastic_Gaussian_Processes\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw34_56ab1df2a6baa"},"id":"rgw34_56ab1df2a6baa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=283335187&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":259844876,"publicationLink":"publication\/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression","hasShowMore":false,"newOffset":3,"pageSize":10,"widgetId":"rgw29_56ab1df2a6baa"},"id":"rgw29_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=259844876&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=3","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":3,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw28_56ab1df2a6baa"},"id":"rgw28_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=259844876&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"54a3dc790cf256bf8bb17c1c","name":"Simo S\u00e4rkk\u00e4","date":"Dec 31, 2014 ","nameLink":"profile\/Simo_Saerkkae","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Simo_Saerkkae\/publication\/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression\/links\/54a3dc790cf256bf8bb17c1c.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Simo_Saerkkae\/publication\/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression\/links\/54a3dc790cf256bf8bb17c1c.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"2de34aa11b5254acd03472fff9da8adf","showFileSizeNote":false,"fileSize":"3.32 MB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"54a3dc790cf256bf8bb17c1c","name":"Simo S\u00e4rkk\u00e4","date":"Dec 31, 2014 ","nameLink":"profile\/Simo_Saerkkae","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Simo_Saerkkae\/publication\/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression\/links\/54a3dc790cf256bf8bb17c1c.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Simo_Saerkkae\/publication\/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression\/links\/54a3dc790cf256bf8bb17c1c.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"2de34aa11b5254acd03472fff9da8adf","showFileSizeNote":false,"fileSize":"3.32 MB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=4siALQ7V3ax4mjbotGqahB58KVkX26iamsl6ARb1SUbtKhfPA1k7LtsOuXjq8iH11FHhtKcrpg97Ursusf3V1g.thRvMh-cI7p6uP2Ex1rLyzeneNweynaMClpVzDSGHCjrNznFH4CcauFs0yrH2iijzsjoWZSzjCxi5iJv9hiP0A","clickOnPill":"publication.PublicationFigures.html?_sg=uqlPYVxDdo_kyk0wdMd2kJplwMJsntxbmu67ee3e60BfLJmW1by2WzFGIHtENPZ-SbgCB8VwqTMs6A6tUl671A.9NIOZCJqu_Xvo7f7dwJKtGfcv0CpgJr31TEsyvzwZqjzUfZhZd1FDX-UX03z8z2xVtHAa22uLhPORp4gILwlRg"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1o9o3\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FSimo_Saerkkae%2Fpublication%2F259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression%2Flinks%2F54a3dc790cf256bf8bb17c1c.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1o9o3\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=j_U5wxNwQ-RibAe08mx-Cky7UcIyTrHpCq_TfPhBDtAkzNo7MFd2RZCcVCzuFsmXhs5Wn4QqRZtgMV4pOFiChg","urlHash":"1076d7bb99add0b94dc42aeea425f739","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=HB0-EmYpoTQKBRBPildQBiNVtS1AZC9jHdHpmwoKNKX0vfPOD7ZBObO3IuQof9Ii8akO1OUggRMLMs9pyYLpCGw-WckL91A7y1lv3aje2Mg.hhWBgISKRsB31X2RcxCKvX0iibHactaL4OCV3UUL0e219UxQi87g45Sjpwj9w-nm1AZjSUQh3Eiq2d42uOa3Iw.VOg1lcsYV7mwLRd8IhJs0h79Ha_wTdhNABTiGhxwG7TfWXcODtW2Ej7HBUGAihOubBXpRKkuUovx-93zIbmAPQ","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"54a3dc790cf256bf8bb17c1c","trackedDownloads":{"54a3dc790cf256bf8bb17c1c":{"v":false,"d":false}},"assetId":"AS:180411883859968@1420024953129","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":259844876,"commentCursorPromo":null,"widgetId":"rgw37_56ab1df2a6baa"},"id":"rgw37_56ab1df2a6baa","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FSimo_Saerkkae%2Fpublication%2F259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression%2Flinks%2F54a3dc790cf256bf8bb17c1c.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A180411883859968%401420024953129&publicationUid=259844876&linkId=54a3dc790cf256bf8bb17c1c&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=KY7thOuIQ2ltD1vsU1luf_4agi5bfwyXbNFioRAPEFF5QOV6zWUr_yBw2QdC79QmJoYG046oC97fNTXR8jf8eLmml4NedFxPw42u_LYHN9w.TAvqzF6qQbNVhbnjF9Lng092Uk7r6wjPmHF-0_MU_t70Mb7NdPv1vRRAsfQXdAj_u-fFK0nBO_MlfiXLdnWyNg.r6KnLHkYPfMstLDBR6BlWe9b7SGatUyiRH9A3V-Lwk5xaybePOztHAFRC414Y1p4RDbtn1qxINxTmsOjXwrxVg","publicationUid":259844876,"trackedDownloads":{"54a3dc790cf256bf8bb17c1c":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw39_56ab1df2a6baa"},"id":"rgw39_56ab1df2a6baa","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw40_56ab1df2a6baa"},"id":"rgw40_56ab1df2a6baa","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw41_56ab1df2a6baa"},"id":"rgw41_56ab1df2a6baa","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw42_56ab1df2a6baa"},"id":"rgw42_56ab1df2a6baa","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw43_56ab1df2a6baa"},"id":"rgw43_56ab1df2a6baa","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw38_56ab1df2a6baa"},"id":"rgw38_56ab1df2a6baa","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw36_56ab1df2a6baa"},"id":"rgw36_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1df2a6baa"},"id":"rgw2_56ab1df2a6baa","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":259844876},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=259844876&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1df2a6baa"},"id":"rgw1_56ab1df2a6baa","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"fzyIvuefiFHScPkYvswAQant09ZmvlXlOcmeeFLH+b+YViN5iqWas0O8CMXxn4UzrY2jIuVFCbPypCw\/GS+iZNuILfa+3sTc4XPajW9ytW7yk9WpvHHfzscM2h7XkFiiNUz12Yl4\/dwLJ2IrLdh1+oGLqNTsTV+aWzvcfBjargVbKl3XNsN1OS7uE+75fZ\/4jbiHfujJgMg9dIRB+ErO2vrTtissGEfrle9ay9t9DCxa\/HF3wbUK4T3ZEfLZ+7LvsMAbxxwk8\/qAdL4YsfBGLrCva4UEKPLfCwSDh69A48g=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression\" \/>\n<meta property=\"og:description\" content=\"This paper proposes a novel scheme for reduced-rank Gaussian process\nregression. The method is based on an approximate series expansion of the\ncovariance function in terms of an eigenfunction...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression\/links\/54a3dc790cf256bf8bb17c1c\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression\" \/>\n<meta property=\"rg:id\" content=\"PB:259844876\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Hilbert Space Methods for Reduced-Rank Gaussian Process Regression\" \/>\n<meta name=\"citation_author\" content=\"Arno Solin\" \/>\n<meta name=\"citation_author\" content=\"Simo S\u00e4rkk\u00e4\" \/>\n<meta name=\"citation_publication_date\" content=\"2014\/01\/21\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Simo_Saerkkae\/publication\/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression\/links\/54a3dc790cf256bf8bb17c1c.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-88ac71cd-640f-45a2-9764-897fa35ab159","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":1028,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw44_56ab1df2a6baa"},"id":"rgw44_56ab1df2a6baa","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-88ac71cd-640f-45a2-9764-897fa35ab159", "52ce96c3bd4a645d7919a3c2d8fec3c87b472835");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-88ac71cd-640f-45a2-9764-897fa35ab159", "52ce96c3bd4a645d7919a3c2d8fec3c87b472835");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw45_56ab1df2a6baa"},"id":"rgw45_56ab1df2a6baa","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/259844876_Hilbert_Space_Methods_for_Reduced-Rank_Gaussian_Process_Regression","requestToken":"uHkDs\/WDdmzEjcw7PPnKQeAMc9\/KyPD18Yo2m6T70QUFhVVsaKWEdLoUBPTRq365Nogl3VP97hwHKc4lK30\/LjEiKEfhctYApG0izkET7OfC9lm6nEokH59jO7WbIToX4bgrQ+Rn4Zodz3tp8R81soSS1\/XBW1NaD\/a6KLOIsWdOVlpdtwfPI5cQCN9mMVOPAyJC2NG+uuaB1kBMWmxVGDDcIMtwy6ZVqxsZvC5sS9F3AUJlY2T\/\/qYWrk3chxKdPlL9JdiPCq98DooA9AywV\/1k9I4dvb3\/q0ayPwAZVw0=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=45x6obpuLdqjZOiscV521Ie3ZsOESWx25FF_Yv6HsQtGIguLHHMzidoM4t9RxPPe","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjU5ODQ0ODc2X0hpbGJlcnRfU3BhY2VfTWV0aG9kc19mb3JfUmVkdWNlZC1SYW5rX0dhdXNzaWFuX1Byb2Nlc3NfUmVncmVzc2lvbg%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw47_56ab1df2a6baa"},"id":"rgw47_56ab1df2a6baa","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw46_56ab1df2a6baa"},"id":"rgw46_56ab1df2a6baa","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw48_56ab1df2a6baa"},"id":"rgw48_56ab1df2a6baa","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
