<!DOCTYPE html> <html lang="en" class="" id="rgw50_56ab1d387cccd"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="Uvn7OhsHRATb/gDLGbiQuAyy2JTuPJdCdxdfgb+lLRir/aa9wnm/KXZhNxyglvlADvhE7CR5bu2v7L+y8rfyqg9xS3B1fs9yLSlZJiZnrHIrOmR1IllGMibuTp2OWNdqWenRbrKH0B7lQOLKPmnwFKhL4w971H4SXJ5V6pfRv/39aVJgX6trwfikPsLBH+g95VFzn0EvfOc3cm/RfhVjJTlF/g9QyI3LNs2pzHg9zMpMskGIPYyKWjIbd3Ehiq3d6ThLfhpLI59U68CL/H9l5+WfOBt8kqekQEprn2bxlcw="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-ffd03e95-573f-4db1-8b1b-e9f32412bb2b",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/221618332_Online_Learning_for_Latent_Dirichlet_Allocation" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Online Learning for Latent Dirichlet Allocation" />
<meta property="og:description" content="We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/221618332_Online_Learning_for_Latent_Dirichlet_Allocation/links/0a85e52e37bbcf0247000000/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/221618332_Online_Learning_for_Latent_Dirichlet_Allocation" />
<meta property="rg:id" content="PB:221618332" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Online Learning for Latent Dirichlet Allocation" />
<meta name="citation_author" content="Matthew D. Hoffman" />
<meta name="citation_author" content="David M. Blei" />
<meta name="citation_author" content="Francis R. Bach" />
<meta name="citation_conference_title" content="Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada." />
<meta name="citation_publication_date" content="2010/11/24" />
<meta name="citation_journal_title" content="Advances in neural information processing systems" />
<meta name="citation_issn" content="1049-5258" />
<meta name="citation_volume" content="23" />
<meta name="citation_firstpage" content="856" />
<meta name="citation_lastpage" content="864" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Francis_Bach/publication/221618332_Online_Learning_for_Latent_Dirichlet_Allocation/links/0a85e52e37bbcf0247000000.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/221618332_Online_Learning_for_Latent_Dirichlet_Allocation" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/221618332_Online_Learning_for_Latent_Dirichlet_Allocation" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Conference Paper');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Online Learning for Latent Dirichlet Allocation (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Online Learning for Latent Dirichlet Allocation on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1d387cccd" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1d387cccd" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab1d387cccd">  <div class="type-label"> Conference Paper   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Online%20Learning%20for%20Latent%20Dirichlet%20Allocation&rft.title=Advances%20in%20Neural%20Information%20Processing%20Systems&rft.jtitle=Advances%20in%20Neural%20Information%20Processing%20Systems&rft.volume=23&rft.date=2010&rft.pages=856-864&rft.issn=1049-5258&rft.au=Matthew%20D.%20Hoffman%2CDavid%20M.%20Blei%2CFrancis%20R.%20Bach&rft.genre=inProceedings"></span> <h1 class="pub-title" itemprop="name">Online Learning for Latent Dirichlet Allocation</h1> <meta itemprop="headline" content="Online Learning for Latent Dirichlet Allocation">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/221618332_Online_Learning_for_Latent_Dirichlet_Allocation/links/0a85e52e37bbcf0247000000/smallpreview.png">  <div id="rgw8_56ab1d387cccd" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56ab1d387cccd"> <a href="researcher/37863250_Matthew_D_Hoffman" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Matthew D. Hoffman" alt="Matthew D. Hoffman" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Matthew D. Hoffman</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw10_56ab1d387cccd">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/37863250_Matthew_D_Hoffman"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Matthew D. Hoffman" alt="Matthew D. Hoffman" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/37863250_Matthew_D_Hoffman" class="display-name">Matthew D. Hoffman</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56ab1d387cccd"> <a href="researcher/2064238818_David_M_Blei" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="David M. Blei" alt="David M. Blei" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">David M. Blei</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56ab1d387cccd">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2064238818_David_M_Blei"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="David M. Blei" alt="David M. Blei" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2064238818_David_M_Blei" class="display-name">David M. Blei</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw13_56ab1d387cccd" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Francis_Bach" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272393490923548%401441955077520_m" title="Francis Bach" alt="Francis Bach" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Francis Bach</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw14_56ab1d387cccd" data-account-key="Francis_Bach">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Francis_Bach"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272393490923548%401441955077520_l" title="Francis Bach" alt="Francis Bach" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Francis_Bach" class="display-name">Francis Bach</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Ecole_Normale_Superieure_de_Paris" title="Ecole Normale Supérieure de Paris">Ecole Normale Supérieure de Paris</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">         Conference: Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada.      <div class="pub-source"> Source: <a href="http://dblp.uni-trier.de/db/conf/nips/nips2010.html#HoffmanBB10" rel="nofollow">DBLP</a> </div>  </div> <div id="rgw15_56ab1d387cccd" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time. 1</div> </p>  </div>  </div>   </div>      <div class="action-container"> <div id="rgw16_56ab1d387cccd" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw30_56ab1d387cccd">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw42_56ab1d387cccd">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Francis_Bach/publication/221618332_Online_Learning_for_Latent_Dirichlet_Allocation/links/0a85e52e37bbcf0247000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Francis_Bach">Francis Bach</a>, <span class="js-publication-date"> Jan 25, 2014 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw44_56ab1d387cccd" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw45_56ab1d387cccd" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw46_56ab1d387cccd" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw47_56ab1d387cccd" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw48_56ab1d387cccd" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw49_56ab1d387cccd" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw43_56ab1d387cccd" src="https://www.researchgate.net/c/o1o9o3/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FFrancis_Bach%2Fpublication%2F221618332_Online_Learning_for_Latent_Dirichlet_Allocation%2Flinks%2F0a85e52e37bbcf0247000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw29_56ab1d387cccd"  itemprop="articleBody">  <p>Page 1</p> <p>Online Learning for Latent Dirichlet Allocation<br />Matthew D. Hoffman<br />Department of Computer Science<br />Princeton University<br />Princeton, NJ<br />mdhoffma@cs.princeton.edu<br />David M. Blei<br />Department of Computer Science<br />Princeton University<br />Princeton, NJ<br />blei@cs.princeton.edu<br />Francis Bach<br />INRIA—Ecole Normale Sup´ erieure<br />Paris, France<br />francis.bach@ens.fr<br />Abstract<br />We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Al-<br />location (LDA). Online LDA is based on online stochastic optimization with a<br />natural gradient step, which we show converges to a local optimum of the VB<br />objective function. It can handily analyze massive document collections, includ-<br />ing those arriving in a stream. We study the performance of online LDA in several<br />ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia<br />in a single pass. We demonstrate that online LDA finds topic models as good or<br />better than those found with batch VB, and in a fraction of the time.<br />1Introduction<br />Hierarchical Bayesian modeling has become a mainstay in machine learning and applied statistics.<br />Bayesian models provide a natural way to encode assumptions about observed data, and analysis<br />proceeds by examining the posterior distribution of model parameters and latent variables condi-<br />tioned on a set of observations. For example, research in probabilistic topic modeling—the applica-<br />tion we will focus on in this paper—revolves around fitting complex hierarchical Bayesian models<br />to large collections of documents. In a topic model, the posterior distribution reveals latent semantic<br />structure that can be used for many applications.<br />For topic models and many other Bayesian models of interest, however, the posterior is intractable<br />to compute and researchers must appeal to approximate posterior inference. Modern approximate<br />posterior inference algorithms fall in two categories—sampling approaches and optimization ap-<br />proaches. Sampling approaches are usually based on Markov Chain Monte Carlo (MCMC) sam-<br />pling, where a Markov chain is defined whose stationary distribution is the posterior of interest. Op-<br />timization approaches are usually based on variational inference, which is called variational Bayes<br />(VB) when used in a Bayesian hierarchical model. Whereas MCMC methods seek to generate inde-<br />pendent samples from the posterior, VB optimizes a simplified parametric distribution to be close in<br />Kullback-Leibler divergence to the posterior. Although the choice of approximate posterior intro-<br />duces bias, VB is empirically shown to be faster than and as accurate as MCMC, which makes it an<br />attractive option when applying Bayesian models to large datasets [1, 2, 3].<br />Nonetheless, large scale data analysis with VB can be computationally difficult. Standard “batch”<br />VB algorithms iterate between analyzing each observation and updating dataset-wide variational<br />parameters. The per-iteration cost of batch algorithms can quickly become impractical for very large<br />datasets. In topic modeling applications, this issue is particularly relevant—topic modeling promises<br />1</p>  <p>Page 2</p> <p>4096<br />systems<br />health<br />communication<br />service<br />billion<br />language<br />care<br />road<br />8192<br />service<br />systems<br />health<br />companies<br />market<br />communication<br />company<br />billion<br />12288<br />service<br />systems<br />companies<br />business<br />company<br />billion<br />health<br />industry<br />16384<br />service<br />companies<br />systems<br />business<br />company<br />industry<br />market<br />billion<br />32768<br />business<br />service<br />companies<br />industry<br />company<br />management<br />systems<br />services<br />49152<br />business<br />service<br />companies<br />industry<br />services<br />company<br />management<br />public<br />2048<br />systems<br />road<br />made<br />service<br />announced<br />national<br />west<br />language<br />65536<br />business<br />industry<br />service<br />companies<br />services<br />company<br />management<br />public<br />Documents<br />analyzed<br />Top eight<br />words<br />Documents seen (log scale)<br />Perplexity<br />600<br />650<br />700<br />750<br />800<br />850<br />900<br />103.5<br />104<br />104.5<br />105<br />105.5<br />106<br />106.5<br />Batch 98K<br />Online 98K<br />Online 3.3M<br />Figure 1: Top: Perplexity on held-out Wikipedia documents as a function of number of documents<br />analyzed, i.e., the number of E steps. Online VB run on 3.3 million unique Wikipedia articles is<br />compared with online VB run on 98,000 Wikipedia articles and with the batch algorithm run on the<br />same 98,000 articles. The online algorithms converge much faster than the batch algorithm does.<br />Bottom: Evolution of a topic about business as online LDA sees more and more documents.<br />to summarize the latent structure of massive document collections that cannot be annotated by hand.<br />A central research problem for topic modeling is to efficiently fit models to larger corpora [4, 5].<br />To this end, we develop an online variational Bayes algorithm for latent Dirichlet allocation (LDA),<br />one of the simplest topic models and one on which many others are based. Our algorithm is based on<br />online stochastic optimization, which has been shown to produce good parameter estimates dramat-<br />ically faster than batch algorithms on large datasets [6]. Online LDA handily analyzes massive col-<br />lections of documents and, moreover, online LDA need not locally store or collect the documents—<br />each can arrive in a stream and be discarded after one look.<br />In the subsequent sections, we derive online LDA and show that it converges to a stationary point<br />of the variational objective function. We study the performance of online LDA in several ways,<br />including by fitting a topic model to 3.3M articles from Wikipedia without looking at the same<br />article twice. We show that online LDA finds topic models as good as or better than those found<br />with batch VB, and in a fraction of the time (see figure 1). Online variational Bayes is a practical<br />new method for estimating the posterior of complex hierarchical Bayesian models.<br />2 Online variational Bayes for latent Dirichlet allocation<br />Latent Dirichlet Allocation (LDA) [7] is a Bayesian probabilistic model of text documents. It as-<br />sumes a collection of K “topics.” Each topic defines a multinomial distribution over the vocabulary<br />and is assumed to have been drawn from a Dirichlet, βk ∼ Dirichlet(η). Given the topics, LDA<br />assumes the following generative process for each document d. First, draw a distribution over topics<br />θd∼ Dirichlet(α). Then, for each word i in the document, draw a topic index zdi∈ {1,...,K}<br />fromthetopicweightszdi∼ θdanddrawtheobservedwordwdifromtheselectedtopic, wdi∼ βzdi.<br />For simplicity, we assume symmetric priors on θ and β, but this assumption is easy to relax [8].<br />Note that if we sum over the topic assignments z, then we get p(wdi|θd,β) =?<br />factorization of the matrix of word counts n (where ndwis the number of times word w appears in<br />document d) into a matrix of topic weights θ and a dictionary of topics β [9]. Our work can thus<br />kθdkβkw. This<br />leads to the “multinomial PCA” interpretation of LDA; we can think of LDA as a probabilistic<br />2</p>  <p>Page 3</p> <p>be seen as an extension of online matrix factorization techniques that optimize squared error [10] to<br />more general probabilistic formulations.<br />We can analyze a corpus of documents with LDA by examining the posterior distribution of the<br />topics β, topic proportions θ, and topic assignments z conditioned on the documents. This reveals<br />latent structure in the collection that can be used for prediction or data exploration. This posterior<br />cannot be computed directly [7], and is usually approximated using Markov Chain Monte Carlo<br />(MCMC) methods or variational inference. Both classes of methods are effective, but both present<br />significant computational challenges in the face of massive data sets.Developing scalable approxi-<br />mate inference methods for topic models is an active area of research [3, 4, 5, 11].<br />To this end, we develop online variational inference for LDA, an approximate posterior inference<br />algorithm that can analyze massive collections of documents. We first review the traditional vari-<br />ational Bayes algorithm for LDA and its objective function, then present our online method, and<br />show that it converges to a stationary point of the same objective function.<br />2.1 Batch variational Bayes for LDA<br />In Variational Bayesian inference (VB) the true posterior is approximated by a simpler distribution<br />q(z,θ,β), which is indexed by a set of free parameters [12, 13]. These parameters are optimized to<br />maximize the Evidence Lower BOund (ELBO):<br />logp(w|α,η) ≥L(w,φ,γ,λ) ? Eq[logp(w,z,θ,β|α,η)] − Eq[logq(z,θ,β)].<br />Maximizing the ELBO is equivalent to minimizing the KL divergence between q(z,θ,β) and the<br />posterior p(z,θ,β|w,α,η). Following [7], we choose a fully factorized distribution q of the form<br />q(zdi= k) = φdwdik;q(θd) = Dirichlet(θd;γd);<br />Theposteriorovertheper-wordtopicassignmentsz isparameterizedbyφ, theposteriorovertheper-<br />document topic weights θ is parameterized by θ, and the posterior over the topics β is parameterized<br />by λ. As a shorthand, we refer to λ as “the topics.” Equation 1 factorizes to<br />L(w,φ,γ,λ) =?<br />Notice we have brought the per-corpus terms into the summation over documents, and divided them<br />by the number of documents D. This step will help us to derive an online inference algorithm.<br />We now expand the expectations above to be functions of the variational parameters. This reveals<br />that the variational objective relies only on ndw, the number of times word w appears in document<br />d. When using VB—as opposed to MCMC—documents can be summarized by their word counts,<br />L =?<br />+ (?<br />??<br />the contribution of document d to the ELBO.<br />L can be optimized using coordinate ascent over the variational parameters φ,γ,λ [7]:<br />φdwk∝ exp{Eq[logθdk]+Eq[logβkw]};<br />The expectations under q of logθ and logβ are<br />Eq[logθdk] = Ψ(γdk) − Ψ(?K<br />The updates in equation 5 are guaranteed to converge to a stationary point of the ELBO. By analogy<br />to the Expectation-Maximization (EM) algorithm [14], we can partition these updates into an “E”<br />step—iteratively updating γ and φ until convergence, holding λ fixed—and an “M” step—updating<br />λ given φ. In practice, this algorithm converges to a better solution if we reinitialize γ and φ before<br />each E step. Algorithm 1 outlines batch VB for LDA.<br />(1)<br />q(βk) = Dirichlet(βk;λk),<br />(2)<br />d<br />?Eq[logp(wd|θd,zd,β)] + Eq[logp(zd|θd)] − Eq[logq(zd)]<br />+ Eq[logp(θd|α)] − Eq[logq(θd)] + (Eq[logp(β|η)] − Eq[logq(β)])/D?.<br />(3)<br />d<br />− logΓ(?<br />+ logΓ(Kα) − K logΓ(α) + (logΓ(Wη) − W logΓ(η))/D<br />d?(nd,φd,γd,λ),<br />where W is the size of the vocabulary and D is the number of documents. ?(nd,φd,γd,λ) denotes<br />?<br />wndw<br />?<br />kφdwk(Eq[logθdk] + Eq[logβkw] − logφdwk)<br />k(α − γdk)Eq[logθdk] + logΓ(γdk)<br />w(η − λkw)Eq[logβkw] + logΓ(λkw))/D<br />kγdk) +?<br />k−logΓ(?<br />wλkw) +?<br />(4)<br />γdk= α+?<br />Eq[logβkw] = Ψ(λkw) − Ψ(?W<br />wndwφdwk;λkw= η+?<br />dndwφdwk.<br />(5)<br />i=1γdi);<br />i=1λki),<br />(6)<br />where Ψ denotes the digamma function (the first derivative of the logarithm of the gamma function).<br />3</p>  <p>Page 4</p> <p>Algorithm 1 Batch variational Bayes for LDA<br />Initialize λ randomly.<br />while relative improvement in L(w,φ,γ,λ) &gt; 0.00001 do<br />E step:<br />for d = 1 to D do<br />Initialize γdk= 1. (The constant 1 is arbitrary.)<br />repeat<br />Set φdwk∝ exp{Eq[logθdk] + Eq[logβkw]}<br />Set γdk= α +?<br />end for<br />M step:<br />Set λkw= η +?<br />wφdwkndw<br />until<br />1<br />K<br />?<br />k|change inγdk| &lt; 0.00001<br />dndwφdwk<br />end while<br />2.2 Online variational inference for LDA<br />Algorithm 1 has constant memory requirements and empirically converges faster than batch col-<br />lapsed Gibbs sampling [3]. However, it still requires a full pass through the entire corpus each<br />iteration. It can therefore be slow to apply to very large datasets, and is not naturally suited to set-<br />tings where new data is constantly arriving. We propose an online variational inference algorithm<br />for fitting λ, the parameters to the variational posterior over the topic distributions β. Our algorithm<br />is nearly as simple as the batch VB algorithm, but converges much faster for large datasets.<br />A good setting of the topics λ is one for which the ELBO L is as high as possible after fitting the<br />per-document variational parameters γ and φ with the E step defined in algorithm 1. Let γ(nd,λ)<br />and φ(nd,λ) be the values of γdand φdproduced by the E step. Our goal is to set λ to maximize<br />L(n,λ) ??<br />where ?(nd,γd,φd,λ) is the dth document’s contribution to the variational bound in equation 4.<br />This is analogous to the goal of least-squares matrix factorization, although the ELBO for LDA is<br />less convenient to work with than a simple squared loss function such as the one in [10].<br />Online VB for LDA (“online LDA”) is described in algorithm 2. As the tth vector of word counts<br />ntis observed, we perform an E step to find locally optimal values of γtand φt, holding λ fixed.<br />We then compute˜λ, the setting of λ that would be optimal (given φt) if our entire corpus consisted<br />of the single document ntrepeated D times. D is the number of unique documents available to the<br />algorithm, e.g. the size of a corpus. (In the true online case D → ∞, corresponding to empirical<br />Bayes estimation of β.) We then update λ using a weighted average of its previous value and˜λ.<br />The weight given to˜λ is given by ρt? (τ0+ t)−κ, where κ ∈ (0.5,1] controls the rate at which<br />old values of˜λ are forgotten and τ0 ≥ 0 slows down the early iterations of the algorithm. The<br />condition that κ ∈ (0.5,1] is needed to guarantee convergence. We show in section 2.3 that online<br />LDA corresponds to a stochastic natural gradient algorithm on the variational objective L [15, 16].<br />This algorithm closely resembles one proposed in [16] for online VB on models with hidden data—<br />the most important difference is that we use an approximate E step to optimize γtand φt, since we<br />cannot compute the conditional distribution p(zt,θt|β,nt,α) exactly.<br />d?(nd,γ(nd,λ),φ(nd,λ),λ),<br />(7)<br />Mini-batches.<br />update to reduce noise [6, 17]. In online LDA, this means computing˜λ using S &gt; 1 observations:<br />A common technique in stochastic learning is to consider multiple observations per<br />˜λkw= η +D<br />S<br />?<br />sntskφtskw,<br />(8)<br />where ntsis the sth document in mini-batch t. The variational parameters φtsand γtsfor this<br />document are fit with a normal E step. Note that we recover batch VB when S = D and κ = 0.<br />Hyperparameter estimation.<br />α and η can be fit given γ and λ using a linear-time Newton-Raphson method [7]. We can likewise<br />In batch variational LDA, point estimates of the hyperparameters<br />4</p>  <p>Page 5</p> <p>Algorithm 2 Online variational Bayes for LDA<br />Define ρt? (τ0+ t)−κ<br />Initialize λ randomly.<br />for t = 0 to ∞ do<br />E step:<br />Initialize γtk= 1. (The constant 1 is arbitrary.)<br />repeat<br />Set φtwk∝ exp{Eq[logθtk] + Eq[logβkw]}<br />Set γtk= α +?<br />M step:<br />Compute˜λkw= η + Dntwφtwk<br />Set λ = (1 − ρt)λ + ρt˜λ.<br />end for<br />wφtwkntw<br />until<br />1<br />K<br />?<br />k|change inγtk| &lt; 0.00001<br />incorporate updates for α and η into online LDA:<br />α ← α − ρt˜ α(γt);η ← η − ρt˜ η(λ),<br />(9)<br />where ˜ α(γt) is the inverse of the Hessian times the gradient ∇α?(nt,γt,φt,λ), ˜ η(λ) is the inverse<br />of the Hessian times the gradient ∇ηL, and ρt? (τ0+ t)−κas elsewhere.<br />2.3 Analysis of convergence<br />In this section we show that algorithm 2 converges to a stationary point of the objective defined in<br />equation 7. Since variational inference replaces sampling with optimization, we can use results from<br />stochastic optimization to analyze online LDA. Stochastic optimization algorithms optimize an ob-<br />jective using noisy estimates of its gradient [18]. Although there is no explicit gradient computation,<br />algorithm 2 can be interpreted as a stochastic natural gradient algorithm [16, 15].<br />We begin by deriving a related first-order stochastic gradient algorithm for LDA. Let g(n) denote<br />the population distribution over documents n from which we will repeatedly sample documents:<br />?D<br />I[n = nd] is 1 if n = ndand 0 otherwise. If this population consists of the D documents in the<br />corpus, then we can rewrite equation 7 as<br />g(n) ?<br />1<br />D<br />d=1I[n = nd].<br />(10)<br />L(g,λ) ? DEg[?(n,γ(n,λ),φ(n,λ),λ)|λ].<br />(11)<br />where ? is defined as in equation 3. We can optimize equation 11 over λ by repeatedly drawing an<br />observation nt∼ g, computing γt? γ(nt,λ) and φt? φ(nt,λ), and applying the update<br />λ ← λ + ρtD∇λ?(nt,γt,φt,λ)<br />where ρt ? (τ0+ t)−κas in algorithm 2. If we condition on the current value of λ and<br />treat γtand φtas random variables drawn at the same time as each observed document nt, then<br />Eg[D∇λ?(nt,γt,φt,λ)|λ] = ∇λ<br />∞, the analysis in [19] shows both that λ converges and that the gradient ∇λ<br />converges to 0, and thus that λ converges to a stationary point.1<br />The update in equation 12 only makes use of first-order gradient information. Stochastic gradient<br />algorithms can be sped up by multiplying the gradient by the inverse of an appropriate positive<br />definite matrix H [19]. One choice for H is the Hessian of the objective function. In variational<br />inference, an alternative is to use the Fisher information matrix of the variational distribution q (i.e.,<br />the Hessian of the log of the variational probability density function), which corresponds to using<br />(12)<br />?<br />d?(nd,γd,φd,λ). Thus, since?∞<br />t=0ρt= ∞ and?∞<br />t=0ρ2<br />t&lt;<br />?<br />d?(nd,γd,φd,λ)<br />1Although we use a deterministic procedure to compute γ and φ given n and λ, this analysis can also be<br />applied if γ and φ are optimized using a randomized algorithm. We address this case in the supplement.<br />5</p>  <p>Page 6</p> <p>a natural gradient method instead of a (quasi-) Newton method [16, 15]. Following the analysis in<br />[16], the gradient of the per-document ELBO ? can be written as<br />=?W<br />∂λkv∂λkw(−λkv/D + η/D + ntvφtvk),<br />where we have used the fact that Eq[logβkv] is the derivative of the log-normalizer of q(logβk). By<br />definition, multiplying equation 13 by the inverse of the Fisher information matrix yields<br />??<br />Multiplying equation 14 by ρtD and adding it to λkwyields the update for λ in algorithm 2. Thus<br />we can interpret our algorithm as a stochastic natural gradient algorithm, as in [16].<br />∂?(nt,γt,φt,λ)<br />∂λkw<br />v=1<br />v=1−∂2log q(βk)<br />∂Eq[log βkv]<br />∂λkw<br />(−λkv/D + η/D + ntvφtvk)<br />=?W<br />(13)<br />−∂2log q(log βk)<br />∂λk∂λT<br />k<br />?−1∂?(nt,γt,φt,λ)<br />∂λk<br />?<br />w<br />= −λkw/D + η/D + ntwφtwk.<br />(14)<br />3 Related work<br />Comparison with other stochastic learning algorithms.<br />timization setup, the number of parameters to be fit does not depend on the number of observations<br />[19]. However, some learning algorithms must also fit a set of per-observation parameters (such<br />as the per-document variational parameters γdand φdin LDA). The problem is addressed by on-<br />line coordinate ascent algorithms such as those described in [20, 21, 16, 17, 10]. The goal of these<br />algorithms is to set the global parameters so that the objective is as good as possible once the per-<br />observation parameters are optimized. Most of these approaches assume the computability of a<br />unique optimum for the per-observation parameters, which is not available for LDA.<br />In the standard stochastic gradient op-<br />Efficient sampling methods.<br />approximate inference algorithms for LDA. Collapsed Gibbs Sampling (CGS) is a popular MCMC<br />approach that samples from the posterior over topic assignments z by repeatedly sampling the topic<br />assignment zdiconditioned on the data and all other topic assignments [22].<br />One online MCMC approach adapts CGS by sampling topic assignments zdibased on the topic<br />assignments and data for all previously analyzed words, instead of all other words in the corpus [23].<br />This algorithm is fast and has constant memory requirements, but is not guaranteed to converge to<br />the posterior. Two alternative online MCMC approaches were considered in [24]. The first, called<br />incremental LDA, periodically resamples the topic assignments for previously analyzed words. The<br />second approach uses particle filtering instead of CGS. In a study in [24], none of these three online<br />MCMC algorithms performed as well as batch CGS.<br />Instead of online methods, the authors of [4] used parallel computing to apply LDA to large corpora.<br />They developed two approximate parallel CGS schemes for LDA that gave similar predictive per-<br />formance on held-out documents to batch CGS. However, they require parallel hardware, and their<br />complexity and memory costs still scale linearly with the number of documents.<br />Except for the algorithm in [23] (which is not guaranteed to converge), all of the MCMC algorithms<br />described above have memory costs that scale linearly with the number of documents analyzed. By<br />contrast, batch VB can be implemented using constant memory, and parallelizes easily. As we will<br />show in the next section, its online counterpart is even faster.<br />Markov Chain Monte Carlo (MCMC) methods form one class of<br />4 Experiments<br />We ran several experiments to evaluate online LDA’s efficiency and effectiveness. The first set of<br />experiments compares algorithms 1 and 2 on static datasets. The second set of experiments evaluates<br />online VB in the setting where new documents are constantly being observed. Both algorithms were<br />implemented in Python using Numpy. The implementations are as similar as possible.2<br />2Open-source Python implementations of batch and online LDA can be found at http://www.cs.<br />princeton.edu/˜mdhoffma.<br />6</p>  <p>Page 7</p> <p>Table 1: Best settings of κ and τ0for various mini-batch sizes S, with resulting perplexities on<br />Nature and Wikipedia corpora.<br />Best parameter settings for Nature corpus<br />S<br />14 16 64<br />κ<br />0.9 0.80.80.7<br />τ0<br />10241024 10241024<br />Perplexity1132 108710521053<br />Best parameter settings for Wikipedia corpus<br />S<br />14 1664<br />κ<br />0.90.9 0.80.7<br />τ0<br />1024102410241024<br />Perplexity675 640611 595<br />256<br />0.6<br />1024<br />1042<br />1024<br />0.5<br />256<br />1031<br />4096<br />0.5<br />64<br />1030<br />16384<br />0.5<br />1<br />1046<br />256<br />0.6<br />1024<br />588<br />1024<br />0.5<br />1024<br />584<br />4096<br />0.5<br />64<br />580<br />16384<br />0.5<br />1<br />584<br />Time in seconds (log scale)<br />Perplexity<br />1500<br />2000<br />2500<br />101<br />102<br />103<br />104<br />Batch size<br />00001<br />00016<br />00256<br />01024<br />04096<br />16384<br />batch10K<br />batch98K<br />Time in seconds (log scale)<br />Perplexity<br />600<br />700<br />800<br />900<br />1000<br />101<br />102<br />103<br />104<br />Batch size<br />00001<br />00016<br />00256<br />01024<br />04096<br />16384<br />batch10K<br />batch98K<br />Figure 2: Held-out perplexity obtained on the Nature (left) and Wikipedia (right) corpora as a func-<br />tion of CPU time. For moderately large mini-batch sizes, online LDA finds solutions as good as<br />those that the batch LDA finds, but with much less computation. When fit to a 10,000-document<br />subset of the training corpus batch LDA’s speed improves, but its performance suffers.<br />We use perplexity on held-out data as a measure of model fit. Perplexity is defined as the geometric<br />mean of the inverse marginal probability of each word in the held-out set of documents:<br />perplexity(ntest,λ,α) ? exp<br />where nitestdenotes the vector of word counts for the ith document. Since we cannot directly<br />compute logp(ntest<br />i<br />|α,β), we use a lower bound on perplexity as a proxy:<br />perplexity(ntest,λ,α) ≤ exp<br />?<br />−(?<br />ilogp(ntest<br />i<br />|α,β))/(?<br />i,wntest<br />iw)<br />?<br />(15)<br />?<br />−(?<br />iEq[logp(ntest<br />i<br />,θi,zi|α,β)] − Eq[logq(θi,zi)])(?<br />i,wntest<br />iw)<br />(16)<br />?<br />.<br />The per-document parameters γiand φifor the variational distributions q(θi) and q(zi) are fit using<br />the E step in algorithm 2. The topics λ are fit to a training set of documents and then held fixed. In<br />all experiments α and η are fixed at 0.01 and the number of topics K = 100.<br />There is some question as to the meaningfulness of perplexity as a metric for comparing different<br />topic models [25]. Held-out likelihood metrics are nonetheless well suited to measuring how well<br />an inference algorithm accomplishes the specific optimization task defined by a model.<br />Evaluating learning parameters.<br />Online LDA introduces several learning parameters: κ ∈<br />(0.5,1], which controls how quickly old information is forgotten; τ0≥ 0, which downweights early<br />iterations; and the mini-batch size S, which controls how many documents are used each iteration.<br />Although online LDA converges to a stationary point for any valid κ, τ0, and S, the quality of this<br />stationary point and the speed of convergence may depend on how the learning parameters are set.<br />We evaluated a range of settings of the learning parameters κ, τ0, and S on two corpora: 352,549<br />documents from the journal Nature3and 100,000 documents downloaded from the English ver-<br />3For the Nature articles, we removed all words not in a pruned vocabulary of 4,253 words.<br />7</p>  <p>Page 8</p> <p>sion of Wikipedia4. For each corpus, we set aside a 1,000-document test set and a separate<br />1,000-document validation set. We then ran online LDA for five hours on the remaining docu-<br />ments from each corpus for κ ∈ {0.5,0.6,0.7,0.8,0.9,1.0}, τ0 ∈ {1,4,16,64,256,1024}, and<br />S ∈ {1,4,16,64,256,1024,4096,16384}, for a total of 288 runs per corpus. After five hours of<br />CPU time, we computed perplexity on the test sets for the topics λ obtained at the end of each fit.<br />Table 1 summarizes the best settings for each corpus of κ and τ0for a range of settings of S. The<br />supplement includes a more exhaustive summary. The best learning parameter settings for both<br />corpora were κ = 0.5, τ0= 64, and S = 4096. The best settings of κ and τ0are consistent across<br />thetwocorpora. Formini-batchsizesfrom256to16384thereislittledifferenceinperplexityscores.<br />Severaltrendsemergefromtheseresults. Highervaluesofthelearningrateκandthedownweighting<br />parameter τ0lead to better performance for small mini-batch sizes S, but worse performance for<br />larger values of S. Mini-batch sizes of at least 256 documents outperform smaller mini-batch sizes.<br />Comparing batch and online on fixed corpora. To compare batch LDA to online LDA, we evalu-<br />ated held-out perplexity as a function of time on the Nature and Wikipedia corpora above. We tried<br />various mini-batch sizes from 1 to 16,384, using the best learning parameters for each mini-batch<br />size found in the previous study of the Nature corpus. We also evaluated batch LDA fit to a 10,000-<br />document subset of the training corpus. We computed perplexity on a separate validation set from<br />the test set used in the previous experiment. Each algorithm ran for 24 hours of CPU time.<br />Figure 2 summarizes the results. On the larger Nature corpus, online LDA finds a solution as good as<br />the batch algorithm’s with much less computation. On the smaller Wikipedia corpus, the online al-<br />gorithm finds a better solution than the batch algorithm does. The batch algorithm converges quickly<br />on the 10,000-document corpora, but makes less accurate predictions on held-out documents.<br />True online. To demonstrate the ability of online VB to perform in a true online setting, we wrote a<br />Python script to continually download and analyze mini-batches of articles chosen at random from<br />a list of approximately 3.3 million Wikipedia articles. This script can download and analyze about<br />60,000 articles an hour. It completed a pass through all 3.3 million articles in under three days. The<br />amount of time needed to download an article and convert it to a vector of word counts is comparable<br />to the amount of time that the online LDA algorithm takes to analyze it.<br />We ran online LDA with κ = 0.5, τ0= 1024, and S = 1024. Figure 1 shows the evolution of the<br />perplexity obtained on the held-out validation set of 1,000 Wikipedia articles by the online algorithm<br />as a function of number of articles seen. Shown for comparison is the perplexity obtained by the<br />online algorithm (with the same parameters) fit to only 98,000 Wikipedia articles, and that obtained<br />by the batch algorithm fit to the same 98,000 articles.<br />The online algorithm outperforms the batch algorithm regardless of which training dataset is used,<br />but it does best with access to a constant stream of novel documents. The batch algorithm’s failure<br />to outperform the online algorithm on limited data may be due to stochastic gradient’s robustness<br />to local optima [19]. The online algorithm converged after analyzing about half of the 3.3 million<br />articles. Even one iteration of the batch algorithm over that many articles would have taken days.<br />5Discussion<br />We have developed online variational Bayes (VB) for LDA. This algorithm requires only a few<br />more lines of code than the traditional batch VB of [7], and is handily applied to massive and<br />streaming document collections. Online VB for LDA approximates the posterior as well as previous<br />approaches in a fraction of the time. The approach we used to derive an online version of batch VB<br />for LDA is general (and simple) enough to apply to a wide variety of hierarchical Bayesian models.<br />Acknowledgments<br />D.M. Blei is supported by ONR 175-6343, NSF CAREER 0745520, AFOSR 09NL202, the Alfred<br />P. Sloan foundation, and a grant from Google. F. Bach is supported by ANR (MGA project).<br />4For the Wikipedia articles, we removed all words not from a fixed vocabulary of 7,995 common words.<br />Thisvocabularywasobtainedbyremovingwordslessthan3characterslongfromalistofthe10,000mostcom-<br />mon words in Project Gutenberg texts obtained from http://en.wiktionary.org/wiki/Wiktionary:Frequency lists.<br />8</p>  <p>Page 9</p> <p>References<br />[1] M. Braun and J. McAuliffe. Variational inference for large-scale models of discrete choice. arXiv,<br />(0712.2526), 2008.<br />[2] D. Blei and M. Jordan. Variational methods for the Dirichlet process. In Proc. 21st Int’l Conf. on Machine<br />Learning, 2004.<br />[3] A. Asuncion, M. Welling, P. Smyth, and Y.W. Teh. On smoothing and inference for topic models. In<br />Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence, 2009.<br />[4] D. Newman, A. Asuncion, P. Smyth, and M. Welling. Distributed inference for latent Dirichlet allocation.<br />In Neural Information Processing Systems, 2007.<br />[5] Feng Yan, Ningyi Xu, and Yuan Qi. Parallel inference for latent Dirichlet allocation on graphics process-<br />ing units. In Advances in Neural Information Processing Systems 22, pages 2134–2142, 2009.<br />[6] L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In Advances in Neural Information<br />Processing Systems, volume 20, pages 161–168. NIPS Foundation (http://books.nips.cc), 2008.<br />[7] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research,<br />3:993–1022, January 2003.<br />[8] Hanna Wallach, David Mimno, and Andrew McCallum. Rethinking lda: Why priors matter. In Advances<br />in Neural Information Processing Systems 22, pages 1973–1981, 2009.<br />[9] W.Buntine. VariationalextentionstoEMandmultinomialPCA. InEuropeanConf.onMachineLearning,<br />2002.<br />[10] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse coding.<br />Journal of Machine Learning Research, 11(1):19–60, 2010.<br />[11] L. Yao, D. Mimno, and A. McCallum. Efficient methods for topic model inference on streaming document<br />collections. In KDD 2009: Proc. 15th ACM SIGKDD int’l Conf. on Knowledge discovery and data<br />mining, pages 937–946, 2009.<br />[12] M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical<br />models. Machine Learning, 37:183–233, 1999.<br />[13] H. Attias. A variational Bayesian framework for graphical models. In Advances in Neural Information<br />Processing Systems 12, 2000.<br />[14] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the EM algorithm.<br />Journal of the Royal Statistical Society, Series B, 39:1–38, 1977.<br />[15] L. Bottou and N. Murata. Stochastic approximations and efficient learning. The Handbook of Brain<br />Theory and Neural Networks, Second edition. The MIT Press, Cambridge, MA, 2002.<br />[16] M.A. Sato. Online model selection based on the variational Bayes. Neural Computation, 13(7):1649–<br />1681, 2001.<br />[17] P. Liang and D. Klein. Online EM for unsupervised models. In Proc. Human Language Technologies: The<br />2009AnnualConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics,<br />pages 611–619, 2009.<br />[18] H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics,<br />22(3):400–407, 1951.<br />[19] L. Bottou. Online learning and stochastic approximations. Cambridge University Press, Cambridge, UK,<br />1998.<br />[20] R.M. Neal and G.E. Hinton. A view of the EM algorithm that justifies incremental, sparse, and other<br />variants. Learning in graphical models, 89:355–368, 1998.<br />[21] M.A. Sato and S. Ishii. On-line EM algorithm for the normalized Gaussian network. Neural Computation,<br />12(2):407–432, 2000.<br />[22] T. Griffiths and M. Steyvers. Finding scientific topics. Proc. National Academy of Science, 2004.<br />[23] X. Song, C.Y. Lin, B.L. Tseng, and M.T. Sun. Modeling and predicting personal information dissemi-<br />nation behavior. In KDD 2005: Proc. 11th ACM SIGKDD int’l Conf. on Knowledge discovery and data<br />mining. ACM, 2005.<br />[24] K.R. Canini, L. Shi, and T.L. Griffiths. Online inference of topics with latent Dirichlet allocation. In<br />Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 5, 2009.<br />[25] J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and D. Blei. Reading tea leaves: How humans interpret<br />topic models. In Advances in Neural Information Processing Systems 21 (NIPS), 2009.<br />9</p>  <a href="https://www.researchgate.net/profile/Francis_Bach/publication/221618332_Online_Learning_for_Latent_Dirichlet_Allocation/links/0a85e52e37bbcf0247000000.pdf">Download full-text</a> </div> <div id="rgw21_56ab1d387cccd" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw22_56ab1d387cccd">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw23_56ab1d387cccd"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Francis_Bach/publication/221618332_Online_Learning_for_Latent_Dirichlet_Allocation/links/0a85e52e37bbcf0247000000.pdf" class="publication-viewer" title="0a85e52e37bbcf0247000000.pdf">0a85e52e37bbcf0247000000.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Francis_Bach">Francis Bach</a> &middot; May 23, 2014 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw24_56ab1d387cccd"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://videolectures.net/site/normal_dl/tag=83534/nips2010_1291.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Online Learning for Latent Dirichlet Allocation">Online Learning for Latent Dirichlet Allocation</a> </div>  <div class="details">   Available from <a href="http://videolectures.net/site/normal_dl/tag=83534/nips2010_1291.pdf" target="_blank" rel="nofollow">videolectures.net</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw31_56ab1d387cccd" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (165) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw32_56ab1d387cccd" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw33_56ab1d387cccd" >  <div class="indent-left">  <div id="rgw34_56ab1d387cccd" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview preview-not-available ga-publication-viewer-not-available js-publication-item-fulltext-content" href="publication/283455111_Getting_the_Agenda_Right_Measuring_Media_Agenda_using_Topic_Models">       </a>    </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw35_56ab1d387cccd">  <li class="citation-context-item"> "Following [7], we set the hyperparameter α=50/T (controls topicper-document distribution) and β=0.01 (controls per-topic word sparsity). To optimize online learning hyperparameters [11], we conduct a grid search using perplexity [2] as the measure of model quality. The chosen parameters values are S=1000 (number of documents used to update the model in each step), τ0=1000 and κ=0.5 (parameters controlling the relative weight of new evidence in each update). " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Conference Paper:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/283455111_Getting_the_Agenda_Right_Measuring_Media_Agenda_using_Topic_Models"> <span class="publication-title js-publication-title">Getting the Agenda Right: Measuring Media Agenda using Topic Models</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2046950725_Damir_Korencic" class="authors js-author-name ga-publications-authors">Damir Korenčić</a> &middot;     <a href="researcher/2054738072_Strahil_Ristov" class="authors js-author-name ga-publications-authors">Strahil Ristov</a> &middot;     <a href="researcher/2083879645_Jan_Snajder" class="authors js-author-name ga-publications-authors">Jan Šnajder</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Agenda setting is the theory of how issue salience is transferred from the media to media audience. An agenda-setting study requires one to define a set of issues and to measure their salience. We propose a semi-supervised approach based on topic modeling for exploring a news corpus and measuring the media agenda by tagging news articles with issues. The approach relies on an off-the-shelf Latent Dirichlet Allocation topic model, manual labeling of topics, and topic model customization. In preliminary evaluation, the tagger achieves a micro F1-score of 0.85 and outperforms the supervised baselines, suggesting that it could be successfully used for agenda-setting studies. </span> </div>    <div class="publication-meta publication-meta">    No preview  &middot; Conference Paper &middot; Oct 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw36_56ab1d387cccd" >  <div class="indent-left">  <div id="rgw37_56ab1d387cccd" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/281262457_Necessary_and_Sufficient_Conditions_and_a_Provably_Efficient_Algorithm_for_Separable_Topic_Discovery">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Venkatesh_Saligrama" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Venkatesh Saligrama </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw38_56ab1d387cccd">  <li class="citation-context-item"> "The communication cost is only the &quot; partial &quot; projection values and is therefore insignificant [5] and does not scale as the number of observations N, M increases. • In an online setting in which the documents are streamed in an online fashion [20], we only need to keep all the projection values and update the projection values (hence the empirical solid angle estimates) when new documents arrive. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/281262457_Necessary_and_Sufficient_Conditions_and_a_Provably_Efficient_Algorithm_for_Separable_Topic_Discovery"> <span class="publication-title js-publication-title">Necessary and Sufficient Conditions and a Provably Efficient Algorithm for Separable Topic Discovery</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2004349196_Weicong_Ding" class="authors js-author-name ga-publications-authors">Weicong Ding</a> &middot;     <a href="researcher/10237219_Prakash_Ishwar" class="authors js-author-name ga-publications-authors">Prakash Ishwar</a> &middot;     <a href="researcher/12917297_Venkatesh_Saligrama" class="authors js-author-name ga-publications-authors">Venkatesh Saligrama</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> We develop necessary and sufficient conditions and a novel provably
consistent and efficient algorithm for discovering topics (latent factors) from
observations (documents) that are realized from a probabilistic mixture of
shared latent factors that have certain properties. Our focus is on the class
of topic models in which each shared latent factor contains a novel word that
is unique to that factor, a property that has come to be known as separability.
Our algorithm is based on the key insight that the novel words correspond to
the extreme points of the convex hull formed by the row-vectors of a suitably
normalized word co-occurrence matrix. We leverage this geometric insight to
establish polynomial computation and sample complexity bounds based on a few
isotropic random projections of the rows of the normalized word co-occurrence
matrix. Our proposed random-projections-based algorithm is naturally amenable
to an efficient distributed implementation and is attractive for modern
web-scale distributed data mining applications. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Aug 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Venkatesh_Saligrama/publication/281262457_Necessary_and_Sufficient_Conditions_and_a_Provably_Efficient_Algorithm_for_Separable_Topic_Discovery/links/5612819608ae400c16aefb00.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw39_56ab1d387cccd" >  <div class="indent-left">  <div id="rgw40_56ab1d387cccd" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/280221142_Incremental_Variational_Inference_for_Latent_Dirichlet_Allocation">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1507.05016" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: de.arxiv.org </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw41_56ab1d387cccd">  <li class="citation-context-item"> "Typically, mini-batches are used to stabilize the gradients. An interesting property of SVI is that it corresponds to natural gradients with respect to the variational distribution (Hoffman et al., 2010). " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/280221142_Incremental_Variational_Inference_for_Latent_Dirichlet_Allocation"> <span class="publication-title js-publication-title">Incremental Variational Inference for Latent Dirichlet Allocation</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/11633355_Cedric_Archambeau" class="authors js-author-name ga-publications-authors">Cedric Archambeau</a> &middot;     <a href="researcher/2078144841_Beyza_Ermis" class="authors js-author-name ga-publications-authors">Beyza Ermis</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> We introduce incremental variational inference and apply it to latent
Dirichlet allocation (LDA). Incremental variational inference is inspired by
incremental EM and provides an alternative to stochastic variational inference.
Incremental LDA can process massive document collections, does not require to
set a learning rate, con- verges faster to a local optimum of the variational
bound and enjoys the attractive property of monotonically increasing it. We
study the performance of incremental LDA on large bench- mark data sets. We
further introduce a stochastic approximation of incremental variational
inference which extends to the asynchronous distributed setting. The resulting
distributed algorithm achieves comparable performance as single host
incremental variational inference, but with a significant speed-up. </span> </div>    <div class="publication-meta publication-meta">  <span class="ico-publication-preview reset-background"></span> Preview    &middot; Article &middot; Jul 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw26_56ab1d387cccd" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw27_56ab1d387cccd">  </ul> </div> </div>   <div id="rgw17_56ab1d387cccd" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw18_56ab1d387cccd"> <div> <h5> <a href="publication/281059020_End-to-end_Learning_of_Latent_Dirichlet_Allocation_by_Mirror-Descent_Back_Propagation" class="color-inherit ga-similar-publication-title"><span class="publication-title">End-to-end Learning of Latent Dirichlet Allocation by Mirror-Descent Back Propagation</span></a>  </h5>  <div class="authors"> <a href="researcher/2071533424_Jianshu_Chen" class="authors ga-similar-publication-author">Jianshu Chen</a>, <a href="researcher/2079644034_Ji_He" class="authors ga-similar-publication-author">Ji He</a>, <a href="researcher/2008319155_Yelong_Shen" class="authors ga-similar-publication-author">Yelong Shen</a>, <a href="researcher/2079652231_Lin_Xiao" class="authors ga-similar-publication-author">Lin Xiao</a>, <a href="researcher/9570563_Xiaodong_He" class="authors ga-similar-publication-author">Xiaodong He</a>, <a href="researcher/64076211_Jianfeng_Gao" class="authors ga-similar-publication-author">Jianfeng Gao</a>, <a href="researcher/2079671084_Xinying_Song" class="authors ga-similar-publication-author">Xinying Song</a>, <a href="researcher/7763935_Li_Deng" class="authors ga-similar-publication-author">Li Deng</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56ab1d387cccd"> <div> <h5> <a href="publication/282374777_Cluster-based_architecture_for_parallel_learning_of_latent_dirichlet_allocation" class="color-inherit ga-similar-publication-title"><span class="publication-title">Cluster-based architecture for parallel learning of latent dirichlet allocation</span></a>  </h5>  <div class="authors"> <a href="researcher/2082050680_X_Tu" class="authors ga-similar-publication-author">X. Tu</a>, <a href="researcher/2082000929_J_Chen" class="authors ga-similar-publication-author">J. Chen</a>, <a href="researcher/2082065965_L_Yang" class="authors ga-similar-publication-author">L. Yang</a>, <a href="researcher/2081988169_J_Yan" class="authors ga-similar-publication-author">J. Yan</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw20_56ab1d387cccd"> <div> <h5> <a href="publication/274343371_Mining_Marketing_Meaning_from_Online_Chatter_Strategic_Brand_Analysis_of_Big_Data_Using_Latent_Dirichlet_Allocation" class="color-inherit ga-similar-publication-title"><span class="publication-title">Mining Marketing Meaning from Online Chatter: Strategic Brand Analysis of Big Data Using Latent Dirichlet Allocation</span></a>  </h5>  <div class="authors"> <a href="researcher/80863429_Seshadri_Tirunillai" class="authors ga-similar-publication-author">Seshadri Tirunillai</a>, <a href="researcher/2054469486_Gerard_J_Tellis" class="authors ga-similar-publication-author">Gerard J. Tellis</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw51_56ab1d387cccd" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw52_56ab1d387cccd">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw53_56ab1d387cccd" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=vLGonTq1OFN83iAcHNv48F8zNnWmzzX0nQt0H9BFDHWU4tTpcB_rBckLvKqo8MDx" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="2k3dOSSKPidQIx8LH0MMHnfxZeU5WDJfwkMjKgGqEqCm/Rs2yHtqCEIe/h1QaxR8UZSeHhXU7BdaHJfBu1noinwf5kWaQo7v7iNxVJKFAiiKdykyL3eQ0EQuK+jMQJF05mXKRh4BSACME98psmPuiX5C9dujWn8ntQfdvRyFnGTyrxYoga8FoRxy0Lv7oS9L5eUFivaRrjgqvOVlTM0qVZ93wb0CJsZDJnwSnfsknSBxbC0ToGFh7VzmZ7ZPIj9eCDruapGyW6ymR/w8QZirrP2uT9vLczF0keETKxf6znI="/> <input type="hidden" name="urlAfterLogin" value="publication/221618332_Online_Learning_for_Latent_Dirichlet_Allocation"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjIxNjE4MzMyX09ubGluZV9MZWFybmluZ19mb3JfTGF0ZW50X0RpcmljaGxldF9BbGxvY2F0aW9u"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjIxNjE4MzMyX09ubGluZV9MZWFybmluZ19mb3JfTGF0ZW50X0RpcmljaGxldF9BbGxvY2F0aW9u"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjIxNjE4MzMyX09ubGluZV9MZWFybmluZ19mb3JfTGF0ZW50X0RpcmljaGxldF9BbGxvY2F0aW9u"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw54_56ab1d387cccd"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 1022;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Francis Bach","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272393490923548%401441955077520_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Francis_Bach","institution":"Ecole Normale Sup\u00e9rieure de Paris","institutionUrl":false,"widgetId":"rgw4_56ab1d387cccd"},"id":"rgw4_56ab1d387cccd","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=1858471","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab1d387cccd"},"id":"rgw3_56ab1d387cccd","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=221618332","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":221618332,"title":"Online Learning for Latent Dirichlet Allocation","journalTitle":"Advances in neural information processing systems","journalDetailsTooltip":{"data":{"journalTitle":"Advances in neural information processing systems","journalAbbrev":"Adv Neural Inform Process Syst","publisher":"IEEE Conference on Neural Information Processing Systems--Natural and Synthetic, Massachusetts Institute of Technology Press","issn":"1049-5258","impactFactor":"0.00","fiveYearImpactFactor":"0.00","citedHalfLife":"0.00","immediacyIndex":"0.00","eigenFactor":"0.00","articleInfluence":"0.00","widgetId":"rgw6_56ab1d387cccd"},"id":"rgw6_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=1049-5258","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Conference Paper","details":{"conferenceInfos":"Conference: Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada."},"source":{"sourceUrl":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2010.html#HoffmanBB10","sourceName":"DBLP"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Online Learning for Latent Dirichlet Allocation"},{"key":"rft.title","value":"Advances in Neural Information Processing Systems"},{"key":"rft.jtitle","value":"Advances in Neural Information Processing Systems"},{"key":"rft.volume","value":"23"},{"key":"rft.date","value":"2010"},{"key":"rft.pages","value":"856-864"},{"key":"rft.issn","value":"1049-5258"},{"key":"rft.au","value":"Matthew D. Hoffman,David M. Blei,Francis R. Bach"},{"key":"rft.genre","value":"inProceedings"}],"widgetId":"rgw7_56ab1d387cccd"},"id":"rgw7_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=221618332","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":221618332,"peopleItems":[{"data":{"authorUrl":"researcher\/37863250_Matthew_D_Hoffman","authorNameOnPublication":"Matthew D. Hoffman","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Matthew D. Hoffman","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/37863250_Matthew_D_Hoffman","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw10_56ab1d387cccd"},"id":"rgw10_56ab1d387cccd","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=37863250&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw9_56ab1d387cccd"},"id":"rgw9_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=37863250&authorNameOnPublication=Matthew%20D.%20Hoffman","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/2064238818_David_M_Blei","authorNameOnPublication":"David M. Blei","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"David M. Blei","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2064238818_David_M_Blei","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56ab1d387cccd"},"id":"rgw12_56ab1d387cccd","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2064238818&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56ab1d387cccd"},"id":"rgw11_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2064238818&authorNameOnPublication=David%20M.%20Blei","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Francis Bach","accountUrl":"profile\/Francis_Bach","accountKey":"Francis_Bach","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272393490923548%401441955077520_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Francis Bach","profile":{"professionalInstitution":{"professionalInstitutionName":"Ecole Normale Sup\u00e9rieure de Paris","professionalInstitutionUrl":"institution\/Ecole_Normale_Superieure_de_Paris"}},"professionalInstitutionName":"Ecole Normale Sup\u00e9rieure de Paris","professionalInstitutionUrl":"institution\/Ecole_Normale_Superieure_de_Paris","url":"profile\/Francis_Bach","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272393490923548%401441955077520_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Francis_Bach","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw14_56ab1d387cccd"},"id":"rgw14_56ab1d387cccd","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1858471&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Ecole Normale Sup\u00e9rieure de Paris","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":1,"publicationUid":221618332,"widgetId":"rgw13_56ab1d387cccd"},"id":"rgw13_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1858471&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=1&publicationUid=221618332","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab1d387cccd"},"id":"rgw8_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=221618332&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":221618332,"abstract":"<noscript><\/noscript><div>We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time. 1<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw15_56ab1d387cccd"},"id":"rgw15_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=221618332","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/221618332_Online_Learning_for_Latent_Dirichlet_Allocation\/links\/0a85e52e37bbcf0247000000\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw16_56ab1d387cccd"},"id":"rgw16_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab1d387cccd"},"id":"rgw5_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=221618332&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2071533424,"url":"researcher\/2071533424_Jianshu_Chen","fullname":"Jianshu Chen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079644034,"url":"researcher\/2079644034_Ji_He","fullname":"Ji He","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2008319155,"url":"researcher\/2008319155_Yelong_Shen","fullname":"Yelong Shen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2079652231,"url":"researcher\/2079652231_Lin_Xiao","fullname":"Lin Xiao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":4,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Aug 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/281059020_End-to-end_Learning_of_Latent_Dirichlet_Allocation_by_Mirror-Descent_Back_Propagation","usePlainButton":true,"publicationUid":281059020,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/281059020_End-to-end_Learning_of_Latent_Dirichlet_Allocation_by_Mirror-Descent_Back_Propagation","title":"End-to-end Learning of Latent Dirichlet Allocation by Mirror-Descent Back Propagation","displayTitleAsLink":true,"authors":[{"id":2071533424,"url":"researcher\/2071533424_Jianshu_Chen","fullname":"Jianshu Chen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079644034,"url":"researcher\/2079644034_Ji_He","fullname":"Ji He","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2008319155,"url":"researcher\/2008319155_Yelong_Shen","fullname":"Yelong Shen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079652231,"url":"researcher\/2079652231_Lin_Xiao","fullname":"Lin Xiao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9570563,"url":"researcher\/9570563_Xiaodong_He","fullname":"Xiaodong He","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":64076211,"url":"researcher\/64076211_Jianfeng_Gao","fullname":"Jianfeng Gao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079671084,"url":"researcher\/2079671084_Xinying_Song","fullname":"Xinying Song","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7763935,"url":"researcher\/7763935_Li_Deng","fullname":"Li Deng","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/281059020_End-to-end_Learning_of_Latent_Dirichlet_Allocation_by_Mirror-Descent_Back_Propagation","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/281059020_End-to-end_Learning_of_Latent_Dirichlet_Allocation_by_Mirror-Descent_Back_Propagation\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab1d387cccd"},"id":"rgw18_56ab1d387cccd","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=281059020","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2082050680,"url":"researcher\/2082050680_X_Tu","fullname":"X. Tu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2082000929,"url":"researcher\/2082000929_J_Chen","fullname":"J. Chen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2082065965,"url":"researcher\/2082065965_L_Yang","fullname":"L. Yang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2081988169,"url":"researcher\/2081988169_J_Yan","fullname":"J. Yan","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2015","journal":"Journal of Computational Information Systems","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/282374777_Cluster-based_architecture_for_parallel_learning_of_latent_dirichlet_allocation","usePlainButton":true,"publicationUid":282374777,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/282374777_Cluster-based_architecture_for_parallel_learning_of_latent_dirichlet_allocation","title":"Cluster-based architecture for parallel learning of latent dirichlet allocation","displayTitleAsLink":true,"authors":[{"id":2082050680,"url":"researcher\/2082050680_X_Tu","fullname":"X. Tu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2082000929,"url":"researcher\/2082000929_J_Chen","fullname":"J. Chen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2082065965,"url":"researcher\/2082065965_L_Yang","fullname":"L. Yang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2081988169,"url":"researcher\/2081988169_J_Yan","fullname":"J. Yan","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of Computational Information Systems 01\/2015; 11(2):399-407. DOI:10.12733\/jcis12531"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/282374777_Cluster-based_architecture_for_parallel_learning_of_latent_dirichlet_allocation","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/282374777_Cluster-based_architecture_for_parallel_learning_of_latent_dirichlet_allocation\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab1d387cccd"},"id":"rgw19_56ab1d387cccd","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=282374777","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":80863429,"url":"researcher\/80863429_Seshadri_Tirunillai","fullname":"Seshadri Tirunillai","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2054469486,"url":"researcher\/2054469486_Gerard_J_Tellis","fullname":"Gerard J. Tellis","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Aug 2014","journal":"Journal of Marketing Research","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/274343371_Mining_Marketing_Meaning_from_Online_Chatter_Strategic_Brand_Analysis_of_Big_Data_Using_Latent_Dirichlet_Allocation","usePlainButton":true,"publicationUid":274343371,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.52","url":"publication\/274343371_Mining_Marketing_Meaning_from_Online_Chatter_Strategic_Brand_Analysis_of_Big_Data_Using_Latent_Dirichlet_Allocation","title":"Mining Marketing Meaning from Online Chatter: Strategic Brand Analysis of Big Data Using Latent Dirichlet Allocation","displayTitleAsLink":true,"authors":[{"id":80863429,"url":"researcher\/80863429_Seshadri_Tirunillai","fullname":"Seshadri Tirunillai","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2054469486,"url":"researcher\/2054469486_Gerard_J_Tellis","fullname":"Gerard J. Tellis","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of Marketing Research 08\/2014; 51(4):463-479. DOI:10.1509\/jmr.12.0106"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/274343371_Mining_Marketing_Meaning_from_Online_Chatter_Strategic_Brand_Analysis_of_Big_Data_Using_Latent_Dirichlet_Allocation","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/274343371_Mining_Marketing_Meaning_from_Online_Chatter_Strategic_Brand_Analysis_of_Big_Data_Using_Latent_Dirichlet_Allocation\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56ab1d387cccd"},"id":"rgw20_56ab1d387cccd","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=274343371","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw17_56ab1d387cccd"},"id":"rgw17_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=221618332&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":221618332,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":221618332,"publicationType":"inProceedings","linkId":"0a85e52e37bbcf0247000000","fileName":"0a85e52e37bbcf0247000000.pdf","fileUrl":"profile\/Francis_Bach\/publication\/221618332_Online_Learning_for_Latent_Dirichlet_Allocation\/links\/0a85e52e37bbcf0247000000.pdf","name":"Francis Bach","nameUrl":"profile\/Francis_Bach","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"May 23, 2014","fileSize":"658.22 KB","widgetId":"rgw23_56ab1d387cccd"},"id":"rgw23_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=221618332&linkId=0a85e52e37bbcf0247000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":221618332,"publicationType":"inProceedings","linkId":"0ffbec360cf22ec95c078a72","fileName":"Online Learning for Latent Dirichlet Allocation","fileUrl":"http:\/\/videolectures.net\/site\/normal_dl\/tag=83534\/nips2010_1291.pdf","name":"videolectures.net","nameUrl":"http:\/\/videolectures.net\/site\/normal_dl\/tag=83534\/nips2010_1291.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw24_56ab1d387cccd"},"id":"rgw24_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=221618332&linkId=0ffbec360cf22ec95c078a72&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw22_56ab1d387cccd"},"id":"rgw22_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=221618332&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":718,"valueFormatted":"718","widgetId":"rgw25_56ab1d387cccd"},"id":"rgw25_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=221618332","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw21_56ab1d387cccd"},"id":"rgw21_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=221618332&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":221618332,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw27_56ab1d387cccd"},"id":"rgw27_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=221618332&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":718,"valueFormatted":"718","widgetId":"rgw28_56ab1d387cccd"},"id":"rgw28_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=221618332","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw26_56ab1d387cccd"},"id":"rgw26_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=221618332&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Online Learning for Latent Dirichlet Allocation\nMatthew D. Hoffman\nDepartment of Computer Science\nPrinceton University\nPrinceton, NJ\nmdhoffma@cs.princeton.edu\nDavid M. Blei\nDepartment of Computer Science\nPrinceton University\nPrinceton, NJ\nblei@cs.princeton.edu\nFrancis Bach\nINRIA\u2014Ecole Normale Sup\u00b4 erieure\nParis, France\nfrancis.bach@ens.fr\nAbstract\nWe develop an online variational Bayes (VB) algorithm for Latent Dirichlet Al-\nlocation (LDA). Online LDA is based on online stochastic optimization with a\nnatural gradient step, which we show converges to a local optimum of the VB\nobjective function. It can handily analyze massive document collections, includ-\ning those arriving in a stream. We study the performance of online LDA in several\nways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia\nin a single pass. We demonstrate that online LDA finds topic models as good or\nbetter than those found with batch VB, and in a fraction of the time.\n1Introduction\nHierarchical Bayesian modeling has become a mainstay in machine learning and applied statistics.\nBayesian models provide a natural way to encode assumptions about observed data, and analysis\nproceeds by examining the posterior distribution of model parameters and latent variables condi-\ntioned on a set of observations. For example, research in probabilistic topic modeling\u2014the applica-\ntion we will focus on in this paper\u2014revolves around fitting complex hierarchical Bayesian models\nto large collections of documents. In a topic model, the posterior distribution reveals latent semantic\nstructure that can be used for many applications.\nFor topic models and many other Bayesian models of interest, however, the posterior is intractable\nto compute and researchers must appeal to approximate posterior inference. Modern approximate\nposterior inference algorithms fall in two categories\u2014sampling approaches and optimization ap-\nproaches. Sampling approaches are usually based on Markov Chain Monte Carlo (MCMC) sam-\npling, where a Markov chain is defined whose stationary distribution is the posterior of interest. Op-\ntimization approaches are usually based on variational inference, which is called variational Bayes\n(VB) when used in a Bayesian hierarchical model. Whereas MCMC methods seek to generate inde-\npendent samples from the posterior, VB optimizes a simplified parametric distribution to be close in\nKullback-Leibler divergence to the posterior. Although the choice of approximate posterior intro-\nduces bias, VB is empirically shown to be faster than and as accurate as MCMC, which makes it an\nattractive option when applying Bayesian models to large datasets [1, 2, 3].\nNonetheless, large scale data analysis with VB can be computationally difficult. Standard \u201cbatch\u201d\nVB algorithms iterate between analyzing each observation and updating dataset-wide variational\nparameters. The per-iteration cost of batch algorithms can quickly become impractical for very large\ndatasets. In topic modeling applications, this issue is particularly relevant\u2014topic modeling promises\n1"},{"page":2,"text":"4096\nsystems\nhealth\ncommunication\nservice\nbillion\nlanguage\ncare\nroad\n8192\nservice\nsystems\nhealth\ncompanies\nmarket\ncommunication\ncompany\nbillion\n12288\nservice\nsystems\ncompanies\nbusiness\ncompany\nbillion\nhealth\nindustry\n16384\nservice\ncompanies\nsystems\nbusiness\ncompany\nindustry\nmarket\nbillion\n32768\nbusiness\nservice\ncompanies\nindustry\ncompany\nmanagement\nsystems\nservices\n49152\nbusiness\nservice\ncompanies\nindustry\nservices\ncompany\nmanagement\npublic\n2048\nsystems\nroad\nmade\nservice\nannounced\nnational\nwest\nlanguage\n65536\nbusiness\nindustry\nservice\ncompanies\nservices\ncompany\nmanagement\npublic\nDocuments\nanalyzed\nTop eight\nwords\nDocuments seen (log scale)\nPerplexity\n600\n650\n700\n750\n800\n850\n900\n103.5\n104\n104.5\n105\n105.5\n106\n106.5\nBatch 98K\nOnline 98K\nOnline 3.3M\nFigure 1: Top: Perplexity on held-out Wikipedia documents as a function of number of documents\nanalyzed, i.e., the number of E steps. Online VB run on 3.3 million unique Wikipedia articles is\ncompared with online VB run on 98,000 Wikipedia articles and with the batch algorithm run on the\nsame 98,000 articles. The online algorithms converge much faster than the batch algorithm does.\nBottom: Evolution of a topic about business as online LDA sees more and more documents.\nto summarize the latent structure of massive document collections that cannot be annotated by hand.\nA central research problem for topic modeling is to efficiently fit models to larger corpora [4, 5].\nTo this end, we develop an online variational Bayes algorithm for latent Dirichlet allocation (LDA),\none of the simplest topic models and one on which many others are based. Our algorithm is based on\nonline stochastic optimization, which has been shown to produce good parameter estimates dramat-\nically faster than batch algorithms on large datasets [6]. Online LDA handily analyzes massive col-\nlections of documents and, moreover, online LDA need not locally store or collect the documents\u2014\neach can arrive in a stream and be discarded after one look.\nIn the subsequent sections, we derive online LDA and show that it converges to a stationary point\nof the variational objective function. We study the performance of online LDA in several ways,\nincluding by fitting a topic model to 3.3M articles from Wikipedia without looking at the same\narticle twice. We show that online LDA finds topic models as good as or better than those found\nwith batch VB, and in a fraction of the time (see figure 1). Online variational Bayes is a practical\nnew method for estimating the posterior of complex hierarchical Bayesian models.\n2 Online variational Bayes for latent Dirichlet allocation\nLatent Dirichlet Allocation (LDA) [7] is a Bayesian probabilistic model of text documents. It as-\nsumes a collection of K \u201ctopics.\u201d Each topic defines a multinomial distribution over the vocabulary\nand is assumed to have been drawn from a Dirichlet, \u03b2k \u223c Dirichlet(\u03b7). Given the topics, LDA\nassumes the following generative process for each document d. First, draw a distribution over topics\n\u03b8d\u223c Dirichlet(\u03b1). Then, for each word i in the document, draw a topic index zdi\u2208 {1,...,K}\nfromthetopicweightszdi\u223c \u03b8danddrawtheobservedwordwdifromtheselectedtopic, wdi\u223c \u03b2zdi.\nFor simplicity, we assume symmetric priors on \u03b8 and \u03b2, but this assumption is easy to relax [8].\nNote that if we sum over the topic assignments z, then we get p(wdi|\u03b8d,\u03b2) =?\nfactorization of the matrix of word counts n (where ndwis the number of times word w appears in\ndocument d) into a matrix of topic weights \u03b8 and a dictionary of topics \u03b2 [9]. Our work can thus\nk\u03b8dk\u03b2kw. This\nleads to the \u201cmultinomial PCA\u201d interpretation of LDA; we can think of LDA as a probabilistic\n2"},{"page":3,"text":"be seen as an extension of online matrix factorization techniques that optimize squared error [10] to\nmore general probabilistic formulations.\nWe can analyze a corpus of documents with LDA by examining the posterior distribution of the\ntopics \u03b2, topic proportions \u03b8, and topic assignments z conditioned on the documents. This reveals\nlatent structure in the collection that can be used for prediction or data exploration. This posterior\ncannot be computed directly [7], and is usually approximated using Markov Chain Monte Carlo\n(MCMC) methods or variational inference. Both classes of methods are effective, but both present\nsignificant computational challenges in the face of massive data sets.Developing scalable approxi-\nmate inference methods for topic models is an active area of research [3, 4, 5, 11].\nTo this end, we develop online variational inference for LDA, an approximate posterior inference\nalgorithm that can analyze massive collections of documents. We first review the traditional vari-\national Bayes algorithm for LDA and its objective function, then present our online method, and\nshow that it converges to a stationary point of the same objective function.\n2.1 Batch variational Bayes for LDA\nIn Variational Bayesian inference (VB) the true posterior is approximated by a simpler distribution\nq(z,\u03b8,\u03b2), which is indexed by a set of free parameters [12, 13]. These parameters are optimized to\nmaximize the Evidence Lower BOund (ELBO):\nlogp(w|\u03b1,\u03b7) \u2265L(w,\u03c6,\u03b3,\u03bb) ? Eq[logp(w,z,\u03b8,\u03b2|\u03b1,\u03b7)] \u2212 Eq[logq(z,\u03b8,\u03b2)].\nMaximizing the ELBO is equivalent to minimizing the KL divergence between q(z,\u03b8,\u03b2) and the\nposterior p(z,\u03b8,\u03b2|w,\u03b1,\u03b7). Following [7], we choose a fully factorized distribution q of the form\nq(zdi= k) = \u03c6dwdik;q(\u03b8d) = Dirichlet(\u03b8d;\u03b3d);\nTheposteriorovertheper-wordtopicassignmentsz isparameterizedby\u03c6, theposteriorovertheper-\ndocument topic weights \u03b8 is parameterized by \u03b8, and the posterior over the topics \u03b2 is parameterized\nby \u03bb. As a shorthand, we refer to \u03bb as \u201cthe topics.\u201d Equation 1 factorizes to\nL(w,\u03c6,\u03b3,\u03bb) =?\nNotice we have brought the per-corpus terms into the summation over documents, and divided them\nby the number of documents D. This step will help us to derive an online inference algorithm.\nWe now expand the expectations above to be functions of the variational parameters. This reveals\nthat the variational objective relies only on ndw, the number of times word w appears in document\nd. When using VB\u2014as opposed to MCMC\u2014documents can be summarized by their word counts,\nL =?\n+ (?\n??\nthe contribution of document d to the ELBO.\nL can be optimized using coordinate ascent over the variational parameters \u03c6,\u03b3,\u03bb [7]:\n\u03c6dwk\u221d exp{Eq[log\u03b8dk]+Eq[log\u03b2kw]};\nThe expectations under q of log\u03b8 and log\u03b2 are\nEq[log\u03b8dk] = \u03a8(\u03b3dk) \u2212 \u03a8(?K\nThe updates in equation 5 are guaranteed to converge to a stationary point of the ELBO. By analogy\nto the Expectation-Maximization (EM) algorithm [14], we can partition these updates into an \u201cE\u201d\nstep\u2014iteratively updating \u03b3 and \u03c6 until convergence, holding \u03bb fixed\u2014and an \u201cM\u201d step\u2014updating\n\u03bb given \u03c6. In practice, this algorithm converges to a better solution if we reinitialize \u03b3 and \u03c6 before\neach E step. Algorithm 1 outlines batch VB for LDA.\n(1)\nq(\u03b2k) = Dirichlet(\u03b2k;\u03bbk),\n(2)\nd\n?Eq[logp(wd|\u03b8d,zd,\u03b2)] + Eq[logp(zd|\u03b8d)] \u2212 Eq[logq(zd)]\n+ Eq[logp(\u03b8d|\u03b1)] \u2212 Eq[logq(\u03b8d)] + (Eq[logp(\u03b2|\u03b7)] \u2212 Eq[logq(\u03b2)])\/D?.\n(3)\nd\n\u2212 log\u0393(?\n+ log\u0393(K\u03b1) \u2212 K log\u0393(\u03b1) + (log\u0393(W\u03b7) \u2212 W log\u0393(\u03b7))\/D\nd?(nd,\u03c6d,\u03b3d,\u03bb),\nwhere W is the size of the vocabulary and D is the number of documents. ?(nd,\u03c6d,\u03b3d,\u03bb) denotes\n?\nwndw\n?\nk\u03c6dwk(Eq[log\u03b8dk] + Eq[log\u03b2kw] \u2212 log\u03c6dwk)\nk(\u03b1 \u2212 \u03b3dk)Eq[log\u03b8dk] + log\u0393(\u03b3dk)\nw(\u03b7 \u2212 \u03bbkw)Eq[log\u03b2kw] + log\u0393(\u03bbkw))\/D\nk\u03b3dk) +?\nk\u2212log\u0393(?\nw\u03bbkw) +?\n(4)\n\u03b3dk= \u03b1+?\nEq[log\u03b2kw] = \u03a8(\u03bbkw) \u2212 \u03a8(?W\nwndw\u03c6dwk;\u03bbkw= \u03b7+?\ndndw\u03c6dwk.\n(5)\ni=1\u03b3di);\ni=1\u03bbki),\n(6)\nwhere \u03a8 denotes the digamma function (the first derivative of the logarithm of the gamma function).\n3"},{"page":4,"text":"Algorithm 1 Batch variational Bayes for LDA\nInitialize \u03bb randomly.\nwhile relative improvement in L(w,\u03c6,\u03b3,\u03bb) > 0.00001 do\nE step:\nfor d = 1 to D do\nInitialize \u03b3dk= 1. (The constant 1 is arbitrary.)\nrepeat\nSet \u03c6dwk\u221d exp{Eq[log\u03b8dk] + Eq[log\u03b2kw]}\nSet \u03b3dk= \u03b1 +?\nend for\nM step:\nSet \u03bbkw= \u03b7 +?\nw\u03c6dwkndw\nuntil\n1\nK\n?\nk|change in\u03b3dk| < 0.00001\ndndw\u03c6dwk\nend while\n2.2 Online variational inference for LDA\nAlgorithm 1 has constant memory requirements and empirically converges faster than batch col-\nlapsed Gibbs sampling [3]. However, it still requires a full pass through the entire corpus each\niteration. It can therefore be slow to apply to very large datasets, and is not naturally suited to set-\ntings where new data is constantly arriving. We propose an online variational inference algorithm\nfor fitting \u03bb, the parameters to the variational posterior over the topic distributions \u03b2. Our algorithm\nis nearly as simple as the batch VB algorithm, but converges much faster for large datasets.\nA good setting of the topics \u03bb is one for which the ELBO L is as high as possible after fitting the\nper-document variational parameters \u03b3 and \u03c6 with the E step defined in algorithm 1. Let \u03b3(nd,\u03bb)\nand \u03c6(nd,\u03bb) be the values of \u03b3dand \u03c6dproduced by the E step. Our goal is to set \u03bb to maximize\nL(n,\u03bb) ??\nwhere ?(nd,\u03b3d,\u03c6d,\u03bb) is the dth document\u2019s contribution to the variational bound in equation 4.\nThis is analogous to the goal of least-squares matrix factorization, although the ELBO for LDA is\nless convenient to work with than a simple squared loss function such as the one in [10].\nOnline VB for LDA (\u201conline LDA\u201d) is described in algorithm 2. As the tth vector of word counts\nntis observed, we perform an E step to find locally optimal values of \u03b3tand \u03c6t, holding \u03bb fixed.\nWe then compute\u02dc\u03bb, the setting of \u03bb that would be optimal (given \u03c6t) if our entire corpus consisted\nof the single document ntrepeated D times. D is the number of unique documents available to the\nalgorithm, e.g. the size of a corpus. (In the true online case D \u2192 \u221e, corresponding to empirical\nBayes estimation of \u03b2.) We then update \u03bb using a weighted average of its previous value and\u02dc\u03bb.\nThe weight given to\u02dc\u03bb is given by \u03c1t? (\u03c40+ t)\u2212\u03ba, where \u03ba \u2208 (0.5,1] controls the rate at which\nold values of\u02dc\u03bb are forgotten and \u03c40 \u2265 0 slows down the early iterations of the algorithm. The\ncondition that \u03ba \u2208 (0.5,1] is needed to guarantee convergence. We show in section 2.3 that online\nLDA corresponds to a stochastic natural gradient algorithm on the variational objective L [15, 16].\nThis algorithm closely resembles one proposed in [16] for online VB on models with hidden data\u2014\nthe most important difference is that we use an approximate E step to optimize \u03b3tand \u03c6t, since we\ncannot compute the conditional distribution p(zt,\u03b8t|\u03b2,nt,\u03b1) exactly.\nd?(nd,\u03b3(nd,\u03bb),\u03c6(nd,\u03bb),\u03bb),\n(7)\nMini-batches.\nupdate to reduce noise [6, 17]. In online LDA, this means computing\u02dc\u03bb using S > 1 observations:\nA common technique in stochastic learning is to consider multiple observations per\n\u02dc\u03bbkw= \u03b7 +D\nS\n?\nsntsk\u03c6tskw,\n(8)\nwhere ntsis the sth document in mini-batch t. The variational parameters \u03c6tsand \u03b3tsfor this\ndocument are fit with a normal E step. Note that we recover batch VB when S = D and \u03ba = 0.\nHyperparameter estimation.\n\u03b1 and \u03b7 can be fit given \u03b3 and \u03bb using a linear-time Newton-Raphson method [7]. We can likewise\nIn batch variational LDA, point estimates of the hyperparameters\n4"},{"page":5,"text":"Algorithm 2 Online variational Bayes for LDA\nDefine \u03c1t? (\u03c40+ t)\u2212\u03ba\nInitialize \u03bb randomly.\nfor t = 0 to \u221e do\nE step:\nInitialize \u03b3tk= 1. (The constant 1 is arbitrary.)\nrepeat\nSet \u03c6twk\u221d exp{Eq[log\u03b8tk] + Eq[log\u03b2kw]}\nSet \u03b3tk= \u03b1 +?\nM step:\nCompute\u02dc\u03bbkw= \u03b7 + Dntw\u03c6twk\nSet \u03bb = (1 \u2212 \u03c1t)\u03bb + \u03c1t\u02dc\u03bb.\nend for\nw\u03c6twkntw\nuntil\n1\nK\n?\nk|change in\u03b3tk| < 0.00001\nincorporate updates for \u03b1 and \u03b7 into online LDA:\n\u03b1 \u2190 \u03b1 \u2212 \u03c1t\u02dc \u03b1(\u03b3t);\u03b7 \u2190 \u03b7 \u2212 \u03c1t\u02dc \u03b7(\u03bb),\n(9)\nwhere \u02dc \u03b1(\u03b3t) is the inverse of the Hessian times the gradient \u2207\u03b1?(nt,\u03b3t,\u03c6t,\u03bb), \u02dc \u03b7(\u03bb) is the inverse\nof the Hessian times the gradient \u2207\u03b7L, and \u03c1t? (\u03c40+ t)\u2212\u03baas elsewhere.\n2.3 Analysis of convergence\nIn this section we show that algorithm 2 converges to a stationary point of the objective defined in\nequation 7. Since variational inference replaces sampling with optimization, we can use results from\nstochastic optimization to analyze online LDA. Stochastic optimization algorithms optimize an ob-\njective using noisy estimates of its gradient [18]. Although there is no explicit gradient computation,\nalgorithm 2 can be interpreted as a stochastic natural gradient algorithm [16, 15].\nWe begin by deriving a related first-order stochastic gradient algorithm for LDA. Let g(n) denote\nthe population distribution over documents n from which we will repeatedly sample documents:\n?D\nI[n = nd] is 1 if n = ndand 0 otherwise. If this population consists of the D documents in the\ncorpus, then we can rewrite equation 7 as\ng(n) ?\n1\nD\nd=1I[n = nd].\n(10)\nL(g,\u03bb) ? DEg[?(n,\u03b3(n,\u03bb),\u03c6(n,\u03bb),\u03bb)|\u03bb].\n(11)\nwhere ? is defined as in equation 3. We can optimize equation 11 over \u03bb by repeatedly drawing an\nobservation nt\u223c g, computing \u03b3t? \u03b3(nt,\u03bb) and \u03c6t? \u03c6(nt,\u03bb), and applying the update\n\u03bb \u2190 \u03bb + \u03c1tD\u2207\u03bb?(nt,\u03b3t,\u03c6t,\u03bb)\nwhere \u03c1t ? (\u03c40+ t)\u2212\u03baas in algorithm 2. If we condition on the current value of \u03bb and\ntreat \u03b3tand \u03c6tas random variables drawn at the same time as each observed document nt, then\nEg[D\u2207\u03bb?(nt,\u03b3t,\u03c6t,\u03bb)|\u03bb] = \u2207\u03bb\n\u221e, the analysis in [19] shows both that \u03bb converges and that the gradient \u2207\u03bb\nconverges to 0, and thus that \u03bb converges to a stationary point.1\nThe update in equation 12 only makes use of first-order gradient information. Stochastic gradient\nalgorithms can be sped up by multiplying the gradient by the inverse of an appropriate positive\ndefinite matrix H [19]. One choice for H is the Hessian of the objective function. In variational\ninference, an alternative is to use the Fisher information matrix of the variational distribution q (i.e.,\nthe Hessian of the log of the variational probability density function), which corresponds to using\n(12)\n?\nd?(nd,\u03b3d,\u03c6d,\u03bb). Thus, since?\u221e\nt=0\u03c1t= \u221e and?\u221e\nt=0\u03c12\nt<\n?\nd?(nd,\u03b3d,\u03c6d,\u03bb)\n1Although we use a deterministic procedure to compute \u03b3 and \u03c6 given n and \u03bb, this analysis can also be\napplied if \u03b3 and \u03c6 are optimized using a randomized algorithm. We address this case in the supplement.\n5"},{"page":6,"text":"a natural gradient method instead of a (quasi-) Newton method [16, 15]. Following the analysis in\n[16], the gradient of the per-document ELBO ? can be written as\n=?W\n\u2202\u03bbkv\u2202\u03bbkw(\u2212\u03bbkv\/D + \u03b7\/D + ntv\u03c6tvk),\nwhere we have used the fact that Eq[log\u03b2kv] is the derivative of the log-normalizer of q(log\u03b2k). By\ndefinition, multiplying equation 13 by the inverse of the Fisher information matrix yields\n??\nMultiplying equation 14 by \u03c1tD and adding it to \u03bbkwyields the update for \u03bb in algorithm 2. Thus\nwe can interpret our algorithm as a stochastic natural gradient algorithm, as in [16].\n\u2202?(nt,\u03b3t,\u03c6t,\u03bb)\n\u2202\u03bbkw\nv=1\nv=1\u2212\u22022log q(\u03b2k)\n\u2202Eq[log \u03b2kv]\n\u2202\u03bbkw\n(\u2212\u03bbkv\/D + \u03b7\/D + ntv\u03c6tvk)\n=?W\n(13)\n\u2212\u22022log q(log \u03b2k)\n\u2202\u03bbk\u2202\u03bbT\nk\n?\u22121\u2202?(nt,\u03b3t,\u03c6t,\u03bb)\n\u2202\u03bbk\n?\nw\n= \u2212\u03bbkw\/D + \u03b7\/D + ntw\u03c6twk.\n(14)\n3 Related work\nComparison with other stochastic learning algorithms.\ntimization setup, the number of parameters to be fit does not depend on the number of observations\n[19]. However, some learning algorithms must also fit a set of per-observation parameters (such\nas the per-document variational parameters \u03b3dand \u03c6din LDA). The problem is addressed by on-\nline coordinate ascent algorithms such as those described in [20, 21, 16, 17, 10]. The goal of these\nalgorithms is to set the global parameters so that the objective is as good as possible once the per-\nobservation parameters are optimized. Most of these approaches assume the computability of a\nunique optimum for the per-observation parameters, which is not available for LDA.\nIn the standard stochastic gradient op-\nEfficient sampling methods.\napproximate inference algorithms for LDA. Collapsed Gibbs Sampling (CGS) is a popular MCMC\napproach that samples from the posterior over topic assignments z by repeatedly sampling the topic\nassignment zdiconditioned on the data and all other topic assignments [22].\nOne online MCMC approach adapts CGS by sampling topic assignments zdibased on the topic\nassignments and data for all previously analyzed words, instead of all other words in the corpus [23].\nThis algorithm is fast and has constant memory requirements, but is not guaranteed to converge to\nthe posterior. Two alternative online MCMC approaches were considered in [24]. The first, called\nincremental LDA, periodically resamples the topic assignments for previously analyzed words. The\nsecond approach uses particle filtering instead of CGS. In a study in [24], none of these three online\nMCMC algorithms performed as well as batch CGS.\nInstead of online methods, the authors of [4] used parallel computing to apply LDA to large corpora.\nThey developed two approximate parallel CGS schemes for LDA that gave similar predictive per-\nformance on held-out documents to batch CGS. However, they require parallel hardware, and their\ncomplexity and memory costs still scale linearly with the number of documents.\nExcept for the algorithm in [23] (which is not guaranteed to converge), all of the MCMC algorithms\ndescribed above have memory costs that scale linearly with the number of documents analyzed. By\ncontrast, batch VB can be implemented using constant memory, and parallelizes easily. As we will\nshow in the next section, its online counterpart is even faster.\nMarkov Chain Monte Carlo (MCMC) methods form one class of\n4 Experiments\nWe ran several experiments to evaluate online LDA\u2019s efficiency and effectiveness. The first set of\nexperiments compares algorithms 1 and 2 on static datasets. The second set of experiments evaluates\nonline VB in the setting where new documents are constantly being observed. Both algorithms were\nimplemented in Python using Numpy. The implementations are as similar as possible.2\n2Open-source Python implementations of batch and online LDA can be found at http:\/\/www.cs.\nprinceton.edu\/\u02dcmdhoffma.\n6"},{"page":7,"text":"Table 1: Best settings of \u03ba and \u03c40for various mini-batch sizes S, with resulting perplexities on\nNature and Wikipedia corpora.\nBest parameter settings for Nature corpus\nS\n14 16 64\n\u03ba\n0.9 0.80.80.7\n\u03c40\n10241024 10241024\nPerplexity1132 108710521053\nBest parameter settings for Wikipedia corpus\nS\n14 1664\n\u03ba\n0.90.9 0.80.7\n\u03c40\n1024102410241024\nPerplexity675 640611 595\n256\n0.6\n1024\n1042\n1024\n0.5\n256\n1031\n4096\n0.5\n64\n1030\n16384\n0.5\n1\n1046\n256\n0.6\n1024\n588\n1024\n0.5\n1024\n584\n4096\n0.5\n64\n580\n16384\n0.5\n1\n584\nTime in seconds (log scale)\nPerplexity\n1500\n2000\n2500\n101\n102\n103\n104\nBatch size\n00001\n00016\n00256\n01024\n04096\n16384\nbatch10K\nbatch98K\nTime in seconds (log scale)\nPerplexity\n600\n700\n800\n900\n1000\n101\n102\n103\n104\nBatch size\n00001\n00016\n00256\n01024\n04096\n16384\nbatch10K\nbatch98K\nFigure 2: Held-out perplexity obtained on the Nature (left) and Wikipedia (right) corpora as a func-\ntion of CPU time. For moderately large mini-batch sizes, online LDA finds solutions as good as\nthose that the batch LDA finds, but with much less computation. When fit to a 10,000-document\nsubset of the training corpus batch LDA\u2019s speed improves, but its performance suffers.\nWe use perplexity on held-out data as a measure of model fit. Perplexity is defined as the geometric\nmean of the inverse marginal probability of each word in the held-out set of documents:\nperplexity(ntest,\u03bb,\u03b1) ? exp\nwhere nitestdenotes the vector of word counts for the ith document. Since we cannot directly\ncompute logp(ntest\ni\n|\u03b1,\u03b2), we use a lower bound on perplexity as a proxy:\nperplexity(ntest,\u03bb,\u03b1) \u2264 exp\n?\n\u2212(?\nilogp(ntest\ni\n|\u03b1,\u03b2))\/(?\ni,wntest\niw)\n?\n(15)\n?\n\u2212(?\niEq[logp(ntest\ni\n,\u03b8i,zi|\u03b1,\u03b2)] \u2212 Eq[logq(\u03b8i,zi)])(?\ni,wntest\niw)\n(16)\n?\n.\nThe per-document parameters \u03b3iand \u03c6ifor the variational distributions q(\u03b8i) and q(zi) are fit using\nthe E step in algorithm 2. The topics \u03bb are fit to a training set of documents and then held fixed. In\nall experiments \u03b1 and \u03b7 are fixed at 0.01 and the number of topics K = 100.\nThere is some question as to the meaningfulness of perplexity as a metric for comparing different\ntopic models [25]. Held-out likelihood metrics are nonetheless well suited to measuring how well\nan inference algorithm accomplishes the specific optimization task defined by a model.\nEvaluating learning parameters.\nOnline LDA introduces several learning parameters: \u03ba \u2208\n(0.5,1], which controls how quickly old information is forgotten; \u03c40\u2265 0, which downweights early\niterations; and the mini-batch size S, which controls how many documents are used each iteration.\nAlthough online LDA converges to a stationary point for any valid \u03ba, \u03c40, and S, the quality of this\nstationary point and the speed of convergence may depend on how the learning parameters are set.\nWe evaluated a range of settings of the learning parameters \u03ba, \u03c40, and S on two corpora: 352,549\ndocuments from the journal Nature3and 100,000 documents downloaded from the English ver-\n3For the Nature articles, we removed all words not in a pruned vocabulary of 4,253 words.\n7"},{"page":8,"text":"sion of Wikipedia4. For each corpus, we set aside a 1,000-document test set and a separate\n1,000-document validation set. We then ran online LDA for five hours on the remaining docu-\nments from each corpus for \u03ba \u2208 {0.5,0.6,0.7,0.8,0.9,1.0}, \u03c40 \u2208 {1,4,16,64,256,1024}, and\nS \u2208 {1,4,16,64,256,1024,4096,16384}, for a total of 288 runs per corpus. After five hours of\nCPU time, we computed perplexity on the test sets for the topics \u03bb obtained at the end of each fit.\nTable 1 summarizes the best settings for each corpus of \u03ba and \u03c40for a range of settings of S. The\nsupplement includes a more exhaustive summary. The best learning parameter settings for both\ncorpora were \u03ba = 0.5, \u03c40= 64, and S = 4096. The best settings of \u03ba and \u03c40are consistent across\nthetwocorpora. Formini-batchsizesfrom256to16384thereislittledifferenceinperplexityscores.\nSeveraltrendsemergefromtheseresults. Highervaluesofthelearningrate\u03baandthedownweighting\nparameter \u03c40lead to better performance for small mini-batch sizes S, but worse performance for\nlarger values of S. Mini-batch sizes of at least 256 documents outperform smaller mini-batch sizes.\nComparing batch and online on fixed corpora. To compare batch LDA to online LDA, we evalu-\nated held-out perplexity as a function of time on the Nature and Wikipedia corpora above. We tried\nvarious mini-batch sizes from 1 to 16,384, using the best learning parameters for each mini-batch\nsize found in the previous study of the Nature corpus. We also evaluated batch LDA fit to a 10,000-\ndocument subset of the training corpus. We computed perplexity on a separate validation set from\nthe test set used in the previous experiment. Each algorithm ran for 24 hours of CPU time.\nFigure 2 summarizes the results. On the larger Nature corpus, online LDA finds a solution as good as\nthe batch algorithm\u2019s with much less computation. On the smaller Wikipedia corpus, the online al-\ngorithm finds a better solution than the batch algorithm does. The batch algorithm converges quickly\non the 10,000-document corpora, but makes less accurate predictions on held-out documents.\nTrue online. To demonstrate the ability of online VB to perform in a true online setting, we wrote a\nPython script to continually download and analyze mini-batches of articles chosen at random from\na list of approximately 3.3 million Wikipedia articles. This script can download and analyze about\n60,000 articles an hour. It completed a pass through all 3.3 million articles in under three days. The\namount of time needed to download an article and convert it to a vector of word counts is comparable\nto the amount of time that the online LDA algorithm takes to analyze it.\nWe ran online LDA with \u03ba = 0.5, \u03c40= 1024, and S = 1024. Figure 1 shows the evolution of the\nperplexity obtained on the held-out validation set of 1,000 Wikipedia articles by the online algorithm\nas a function of number of articles seen. Shown for comparison is the perplexity obtained by the\nonline algorithm (with the same parameters) fit to only 98,000 Wikipedia articles, and that obtained\nby the batch algorithm fit to the same 98,000 articles.\nThe online algorithm outperforms the batch algorithm regardless of which training dataset is used,\nbut it does best with access to a constant stream of novel documents. The batch algorithm\u2019s failure\nto outperform the online algorithm on limited data may be due to stochastic gradient\u2019s robustness\nto local optima [19]. The online algorithm converged after analyzing about half of the 3.3 million\narticles. Even one iteration of the batch algorithm over that many articles would have taken days.\n5Discussion\nWe have developed online variational Bayes (VB) for LDA. This algorithm requires only a few\nmore lines of code than the traditional batch VB of [7], and is handily applied to massive and\nstreaming document collections. Online VB for LDA approximates the posterior as well as previous\napproaches in a fraction of the time. The approach we used to derive an online version of batch VB\nfor LDA is general (and simple) enough to apply to a wide variety of hierarchical Bayesian models.\nAcknowledgments\nD.M. Blei is supported by ONR 175-6343, NSF CAREER 0745520, AFOSR 09NL202, the Alfred\nP. Sloan foundation, and a grant from Google. F. Bach is supported by ANR (MGA project).\n4For the Wikipedia articles, we removed all words not from a fixed vocabulary of 7,995 common words.\nThisvocabularywasobtainedbyremovingwordslessthan3characterslongfromalistofthe10,000mostcom-\nmon words in Project Gutenberg texts obtained from http:\/\/en.wiktionary.org\/wiki\/Wiktionary:Frequency lists.\n8"},{"page":9,"text":"References\n[1] M. Braun and J. McAuliffe. Variational inference for large-scale models of discrete choice. arXiv,\n(0712.2526), 2008.\n[2] D. Blei and M. Jordan. Variational methods for the Dirichlet process. In Proc. 21st Int\u2019l Conf. on Machine\nLearning, 2004.\n[3] A. Asuncion, M. Welling, P. Smyth, and Y.W. Teh. On smoothing and inference for topic models. In\nProceedings of the 25th Conference on Uncertainty in Artificial Intelligence, 2009.\n[4] D. Newman, A. Asuncion, P. Smyth, and M. Welling. Distributed inference for latent Dirichlet allocation.\nIn Neural Information Processing Systems, 2007.\n[5] Feng Yan, Ningyi Xu, and Yuan Qi. Parallel inference for latent Dirichlet allocation on graphics process-\ning units. In Advances in Neural Information Processing Systems 22, pages 2134\u20132142, 2009.\n[6] L. Bottou and O. Bousquet. The tradeoffs of large scale learning. In Advances in Neural Information\nProcessing Systems, volume 20, pages 161\u2013168. NIPS Foundation (http:\/\/books.nips.cc), 2008.\n[7] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research,\n3:993\u20131022, January 2003.\n[8] Hanna Wallach, David Mimno, and Andrew McCallum. Rethinking lda: Why priors matter. In Advances\nin Neural Information Processing Systems 22, pages 1973\u20131981, 2009.\n[9] W.Buntine. VariationalextentionstoEMandmultinomialPCA. InEuropeanConf.onMachineLearning,\n2002.\n[10] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse coding.\nJournal of Machine Learning Research, 11(1):19\u201360, 2010.\n[11] L. Yao, D. Mimno, and A. McCallum. Efficient methods for topic model inference on streaming document\ncollections. In KDD 2009: Proc. 15th ACM SIGKDD int\u2019l Conf. on Knowledge discovery and data\nmining, pages 937\u2013946, 2009.\n[12] M. Jordan, Z. Ghahramani, T. Jaakkola, and L. Saul. Introduction to variational methods for graphical\nmodels. Machine Learning, 37:183\u2013233, 1999.\n[13] H. Attias. A variational Bayesian framework for graphical models. In Advances in Neural Information\nProcessing Systems 12, 2000.\n[14] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the EM algorithm.\nJournal of the Royal Statistical Society, Series B, 39:1\u201338, 1977.\n[15] L. Bottou and N. Murata. Stochastic approximations and efficient learning. The Handbook of Brain\nTheory and Neural Networks, Second edition. The MIT Press, Cambridge, MA, 2002.\n[16] M.A. Sato. Online model selection based on the variational Bayes. Neural Computation, 13(7):1649\u2013\n1681, 2001.\n[17] P. Liang and D. Klein. Online EM for unsupervised models. In Proc. Human Language Technologies: The\n2009AnnualConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics,\npages 611\u2013619, 2009.\n[18] H. Robbins and S. Monro. A stochastic approximation method. The Annals of Mathematical Statistics,\n22(3):400\u2013407, 1951.\n[19] L. Bottou. Online learning and stochastic approximations. Cambridge University Press, Cambridge, UK,\n1998.\n[20] R.M. Neal and G.E. Hinton. A view of the EM algorithm that justifies incremental, sparse, and other\nvariants. Learning in graphical models, 89:355\u2013368, 1998.\n[21] M.A. Sato and S. Ishii. On-line EM algorithm for the normalized Gaussian network. Neural Computation,\n12(2):407\u2013432, 2000.\n[22] T. Griffiths and M. Steyvers. Finding scientific topics. Proc. National Academy of Science, 2004.\n[23] X. Song, C.Y. Lin, B.L. Tseng, and M.T. Sun. Modeling and predicting personal information dissemi-\nnation behavior. In KDD 2005: Proc. 11th ACM SIGKDD int\u2019l Conf. on Knowledge discovery and data\nmining. ACM, 2005.\n[24] K.R. Canini, L. Shi, and T.L. Griffiths. Online inference of topics with latent Dirichlet allocation. In\nProceedings of the International Conference on Artificial Intelligence and Statistics, volume 5, 2009.\n[25] J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and D. Blei. Reading tea leaves: How humans interpret\ntopic models. In Advances in Neural Information Processing Systems 21 (NIPS), 2009.\n9"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Francis_Bach\/publication\/221618332_Online_Learning_for_Latent_Dirichlet_Allocation\/links\/0a85e52e37bbcf0247000000.pdf","widgetId":"rgw29_56ab1d387cccd"},"id":"rgw29_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=221618332&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw30_56ab1d387cccd"},"id":"rgw30_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=221618332&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":221618332,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":221618332,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2046950725,"url":"researcher\/2046950725_Damir_Korencic","fullname":"Damir Koren\u010di\u0107","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A275045947801613%401442587472394_m\/Damir_Korencic.png"},{"id":2054738072,"url":"researcher\/2054738072_Strahil_Ristov","fullname":"Strahil Ristov","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083879645,"url":"researcher\/2083879645_Jan_Snajder","fullname":"Jan \u0160najder","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":false,"isSlurp":false,"isNoText":true,"publicationType":"Conference Paper","publicationDate":"Oct 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283455111_Getting_the_Agenda_Right_Measuring_Media_Agenda_using_Topic_Models","usePlainButton":true,"publicationUid":283455111,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/283455111_Getting_the_Agenda_Right_Measuring_Media_Agenda_using_Topic_Models","title":"Getting the Agenda Right: Measuring Media Agenda using Topic Models","displayTitleAsLink":true,"authors":[{"id":2046950725,"url":"researcher\/2046950725_Damir_Korencic","fullname":"Damir Koren\u010di\u0107","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A275045947801613%401442587472394_m\/Damir_Korencic.png"},{"id":2054738072,"url":"researcher\/2054738072_Strahil_Ristov","fullname":"Strahil Ristov","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083879645,"url":"researcher\/2083879645_Jan_Snajder","fullname":"Jan \u0160najder","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["2015 Workshop on Topic Models: Post-Processing and Applications, Melbourne, VIC, Australia; 10\/2015"],"abstract":"Agenda setting is the theory of how issue salience is transferred from the media to media audience. An agenda-setting study requires one to define a set of issues and to measure their salience. We propose a semi-supervised approach based on topic modeling for exploring a news corpus and measuring the media agenda by tagging news articles with issues. The approach relies on an off-the-shelf Latent Dirichlet Allocation topic model, manual labeling of topics, and topic model customization. In preliminary evaluation, the tagger achieves a micro F1-score of 0.85 and outperforms the supervised baselines, suggesting that it could be successfully used for agenda-setting studies.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283455111_Getting_the_Agenda_Right_Measuring_Media_Agenda_using_Topic_Models","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":null,"publicationUid":283455111,"publicationUrl":"publication\/283455111_Getting_the_Agenda_Right_Measuring_Media_Agenda_using_Topic_Models","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=283455111&eventCode=","widgetId":"rgw34_56ab1d387cccd"},"id":"rgw34_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=283455111&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":221618332,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283455111_Getting_the_Agenda_Right_Measuring_Media_Agenda_using_Topic_Models\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Following [7], we set the hyperparameter \u03b1=50\/T (controls topicper-document distribution) and \u03b2=0.01 (controls per-topic word sparsity). To optimize online learning hyperparameters [11], we conduct a grid search using perplexity [2] as the measure of model quality. The chosen parameters values are S=1000 (number of documents used to update the model in each step), \u03c40=1000 and \u03ba=0.5 (parameters controlling the relative weight of new evidence in each update). "],"widgetId":"rgw35_56ab1d387cccd"},"id":"rgw35_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw33_56ab1d387cccd"},"id":"rgw33_56ab1d387cccd","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=283455111&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2004349196,"url":"researcher\/2004349196_Weicong_Ding","fullname":"Weicong Ding","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10237219,"url":"researcher\/10237219_Prakash_Ishwar","fullname":"Prakash Ishwar","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":12917297,"url":"researcher\/12917297_Venkatesh_Saligrama","fullname":"Venkatesh Saligrama","last":true,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A283790606913537%401444672361257_m"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Aug 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/281262457_Necessary_and_Sufficient_Conditions_and_a_Provably_Efficient_Algorithm_for_Separable_Topic_Discovery","usePlainButton":true,"publicationUid":281262457,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/281262457_Necessary_and_Sufficient_Conditions_and_a_Provably_Efficient_Algorithm_for_Separable_Topic_Discovery","title":"Necessary and Sufficient Conditions and a Provably Efficient Algorithm for Separable Topic Discovery","displayTitleAsLink":true,"authors":[{"id":2004349196,"url":"researcher\/2004349196_Weicong_Ding","fullname":"Weicong Ding","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10237219,"url":"researcher\/10237219_Prakash_Ishwar","fullname":"Prakash Ishwar","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":12917297,"url":"researcher\/12917297_Venkatesh_Saligrama","fullname":"Venkatesh Saligrama","last":true,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A283790606913537%401444672361257_m"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"We develop necessary and sufficient conditions and a novel provably\nconsistent and efficient algorithm for discovering topics (latent factors) from\nobservations (documents) that are realized from a probabilistic mixture of\nshared latent factors that have certain properties. Our focus is on the class\nof topic models in which each shared latent factor contains a novel word that\nis unique to that factor, a property that has come to be known as separability.\nOur algorithm is based on the key insight that the novel words correspond to\nthe extreme points of the convex hull formed by the row-vectors of a suitably\nnormalized word co-occurrence matrix. We leverage this geometric insight to\nestablish polynomial computation and sample complexity bounds based on a few\nisotropic random projections of the rows of the normalized word co-occurrence\nmatrix. Our proposed random-projections-based algorithm is naturally amenable\nto an efficient distributed implementation and is attractive for modern\nweb-scale distributed data mining applications.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/281262457_Necessary_and_Sufficient_Conditions_and_a_Provably_Efficient_Algorithm_for_Separable_Topic_Discovery","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Venkatesh_Saligrama\/publication\/281262457_Necessary_and_Sufficient_Conditions_and_a_Provably_Efficient_Algorithm_for_Separable_Topic_Discovery\/links\/5612819608ae400c16aefb00.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Venkatesh_Saligrama","sourceName":"Venkatesh Saligrama","hasSourceUrl":true},"publicationUid":281262457,"publicationUrl":"publication\/281262457_Necessary_and_Sufficient_Conditions_and_a_Provably_Efficient_Algorithm_for_Separable_Topic_Discovery","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/281262457_Necessary_and_Sufficient_Conditions_and_a_Provably_Efficient_Algorithm_for_Separable_Topic_Discovery\/links\/5612819608ae400c16aefb00\/smallpreview.png","linkId":"5612819608ae400c16aefb00","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=281262457&reference=5612819608ae400c16aefb00&eventCode=&origin=publication_list","widgetId":"rgw37_56ab1d387cccd"},"id":"rgw37_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=281262457&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"5612819608ae400c16aefb00","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":221618332,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/281262457_Necessary_and_Sufficient_Conditions_and_a_Provably_Efficient_Algorithm_for_Separable_Topic_Discovery\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["The communication cost is only the \" partial \" projection values and is therefore insignificant [5] and does not scale as the number of observations N, M increases. \u2022 In an online setting in which the documents are streamed in an online fashion [20], we only need to keep all the projection values and update the projection values (hence the empirical solid angle estimates) when new documents arrive. "],"widgetId":"rgw38_56ab1d387cccd"},"id":"rgw38_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw36_56ab1d387cccd"},"id":"rgw36_56ab1d387cccd","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=281262457&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithSlurp","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":11633355,"url":"researcher\/11633355_Cedric_Archambeau","fullname":"Cedric Archambeau","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272180487389207%401441904293508_m"},{"id":2078144841,"url":"researcher\/2078144841_Beyza_Ermis","fullname":"Beyza Ermis","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":false,"isSlurp":true,"isNoText":false,"publicationType":"Article","publicationDate":"Jul 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/280221142_Incremental_Variational_Inference_for_Latent_Dirichlet_Allocation","usePlainButton":true,"publicationUid":280221142,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/280221142_Incremental_Variational_Inference_for_Latent_Dirichlet_Allocation","title":"Incremental Variational Inference for Latent Dirichlet Allocation","displayTitleAsLink":true,"authors":[{"id":11633355,"url":"researcher\/11633355_Cedric_Archambeau","fullname":"Cedric Archambeau","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272180487389207%401441904293508_m"},{"id":2078144841,"url":"researcher\/2078144841_Beyza_Ermis","fullname":"Beyza Ermis","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"We introduce incremental variational inference and apply it to latent\nDirichlet allocation (LDA). Incremental variational inference is inspired by\nincremental EM and provides an alternative to stochastic variational inference.\nIncremental LDA can process massive document collections, does not require to\nset a learning rate, con- verges faster to a local optimum of the variational\nbound and enjoys the attractive property of monotonically increasing it. We\nstudy the performance of incremental LDA on large bench- mark data sets. We\nfurther introduce a stochastic approximation of incremental variational\ninference which extends to the asynchronous distributed setting. The resulting\ndistributed algorithm achieves comparable performance as single host\nincremental variational inference, but with a significant speed-up.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/280221142_Incremental_Variational_Inference_for_Latent_Dirichlet_Allocation","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1507.05016","sourceName":"de.arxiv.org","hasSourceUrl":true},"publicationUid":280221142,"publicationUrl":"publication\/280221142_Incremental_Variational_Inference_for_Latent_Dirichlet_Allocation","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/280221142_Incremental_Variational_Inference_for_Latent_Dirichlet_Allocation\/links\/55b8205608aed621de05bd9f\/smallpreview.png","linkId":"55b8205608aed621de05bd9f","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=280221142&reference=55b8205608aed621de05bd9f&eventCode=&origin=publication_list","widgetId":"rgw40_56ab1d387cccd"},"id":"rgw40_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=280221142&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":221618332,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/280221142_Incremental_Variational_Inference_for_Latent_Dirichlet_Allocation\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Typically, mini-batches are used to stabilize the gradients. An interesting property of SVI is that it corresponds to natural gradients with respect to the variational distribution (Hoffman et al., 2010). "],"widgetId":"rgw41_56ab1d387cccd"},"id":"rgw41_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw39_56ab1d387cccd"},"id":"rgw39_56ab1d387cccd","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=280221142&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":221618332,"publicationLink":"publication\/221618332_Online_Learning_for_Latent_Dirichlet_Allocation","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw32_56ab1d387cccd"},"id":"rgw32_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=221618332&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=165","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":165,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw31_56ab1d387cccd"},"id":"rgw31_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=221618332&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"0a85e52e37bbcf0247000000","name":"Francis Bach","date":"Jan 25, 2014 ","nameLink":"profile\/Francis_Bach","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Francis_Bach\/publication\/221618332_Online_Learning_for_Latent_Dirichlet_Allocation\/links\/0a85e52e37bbcf0247000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Francis_Bach\/publication\/221618332_Online_Learning_for_Latent_Dirichlet_Allocation\/links\/0a85e52e37bbcf0247000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"8f8347268c38fab18d0428ad243fddd8","showFileSizeNote":false,"fileSize":"658.22 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"0a85e52e37bbcf0247000000","name":"Francis Bach","date":"Jan 25, 2014 ","nameLink":"profile\/Francis_Bach","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Francis_Bach\/publication\/221618332_Online_Learning_for_Latent_Dirichlet_Allocation\/links\/0a85e52e37bbcf0247000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Francis_Bach\/publication\/221618332_Online_Learning_for_Latent_Dirichlet_Allocation\/links\/0a85e52e37bbcf0247000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"8f8347268c38fab18d0428ad243fddd8","showFileSizeNote":false,"fileSize":"658.22 KB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Conference Paper","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=U6AtdzWFK_SrEUbAIWFpdgiS9PT5-tipUZb0uTQTeDf2Tp81Z6aOPrA6jBT6TWw5uY9toX_7BhiiyEno6wfKrA.UFRE2SJ3BWVEPLkAbPxNB2ad82XJ6aYDMSgZjI6xjuDiBzxbpo3YL0B9_2-VldTd9hp3_obiuImCNG_xi_XEXg","clickOnPill":"publication.PublicationFigures.html?_sg=a6dMQNgZrerv5FepL5uhdNKpURysvHNy2mI51DjUB54NLRaAmNwXaUB9Ey-aI5UYI2Zf77xvwOSWGkyalrq8lw.MczUvDLHqkTOvLFkqazY2qWTw1EiCDkkOjJE2tIhXMzKlkkQ6kk2dGp0ThdjryfJ58FyBt-egMF68qOKg_000A"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1o9o3\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FFrancis_Bach%2Fpublication%2F221618332_Online_Learning_for_Latent_Dirichlet_Allocation%2Flinks%2F0a85e52e37bbcf0247000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1o9o3\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=1nzJzigooK_yG4P8bnK1rQ0xxs9SEYOJuNxADLMlvwefEgDcsJ28xpgAHFEQmPJhBB9I2dFOr1ZS5KN_zybxgQ","urlHash":"7c9d2204d75653a27e92a7370f0cb259","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=yK8SGl681ISZXX80GEN5GPlU4Jev5hdXlM2QNGMyixlVharkwWOR28Yx3OfvR6pAXyt3ZmUpZWwfCwTUI75obPWBuNoWrmDyc7ZVRH5V47o.w5v8q8R_4fVXuVSmxjgiyJnQEw4kYz6MpA2BWZiPfIeuFriV4KotmALEyC034GzXx1kSDI8QiDZeEtcA3nEmpg.3vP0FitEBBDkRjpgq9zfdoukShkM9sLjRQ_J5-_2QhhT1RKVKkA8OvzzdFymtk4KwKWavl8Jxa7TzyDPxvPWOA","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"0a85e52e37bbcf0247000000","trackedDownloads":{"0a85e52e37bbcf0247000000":{"v":false,"d":false}},"assetId":"AS:99820089905157@1400810372265","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":221618332,"commentCursorPromo":null,"widgetId":"rgw43_56ab1d387cccd"},"id":"rgw43_56ab1d387cccd","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FFrancis_Bach%2Fpublication%2F221618332_Online_Learning_for_Latent_Dirichlet_Allocation%2Flinks%2F0a85e52e37bbcf0247000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A99820089905157%401400810372265&publicationUid=221618332&linkId=0a85e52e37bbcf0247000000&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Online Learning for Latent Dirichlet Allocation","publicationType":"Conference Paper","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=EyRcjtqL3UTEWj4VYVyY4Ozmuho3qCGu5d71wwyTbBSj1s-zwzyRsap-xw9v5M7k_kt_Irag2WnJhmg1iguSNjWnYd1btihXeP78YB24oIY.zjKR7CnIk0W0ugi6AZVhqqCTHqDhZouwXb3oWCGAJPMMZrKdGzfYAxui2puEGMoS2gnziaNrgu2leCzNMbOLPw.mQwS7fftkWkTBYvNQDGOK5T-ZvYbu7JafM2ywFPlvuuHlFnJ5Gmq8aI5CtfyDVI5vrFSXPFlme2AQAO1iYmzcw","publicationUid":221618332,"trackedDownloads":{"0a85e52e37bbcf0247000000":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw45_56ab1d387cccd"},"id":"rgw45_56ab1d387cccd","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw46_56ab1d387cccd"},"id":"rgw46_56ab1d387cccd","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw47_56ab1d387cccd"},"id":"rgw47_56ab1d387cccd","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw48_56ab1d387cccd"},"id":"rgw48_56ab1d387cccd","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw49_56ab1d387cccd"},"id":"rgw49_56ab1d387cccd","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw44_56ab1d387cccd"},"id":"rgw44_56ab1d387cccd","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw42_56ab1d387cccd"},"id":"rgw42_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/221618332_Online_Learning_for_Latent_Dirichlet_Allocation","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1d387cccd"},"id":"rgw2_56ab1d387cccd","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":221618332},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=221618332&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1d387cccd"},"id":"rgw1_56ab1d387cccd","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"Uvn7OhsHRATb\/gDLGbiQuAyy2JTuPJdCdxdfgb+lLRir\/aa9wnm\/KXZhNxyglvlADvhE7CR5bu2v7L+y8rfyqg9xS3B1fs9yLSlZJiZnrHIrOmR1IllGMibuTp2OWNdqWenRbrKH0B7lQOLKPmnwFKhL4w971H4SXJ5V6pfRv\/39aVJgX6trwfikPsLBH+g95VFzn0EvfOc3cm\/RfhVjJTlF\/g9QyI3LNs2pzHg9zMpMskGIPYyKWjIbd3Ehiq3d6ThLfhpLI59U68CL\/H9l5+WfOBt8kqekQEprn2bxlcw=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/221618332_Online_Learning_for_Latent_Dirichlet_Allocation\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Online Learning for Latent Dirichlet Allocation\" \/>\n<meta property=\"og:description\" content=\"We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/221618332_Online_Learning_for_Latent_Dirichlet_Allocation\/links\/0a85e52e37bbcf0247000000\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/221618332_Online_Learning_for_Latent_Dirichlet_Allocation\" \/>\n<meta property=\"rg:id\" content=\"PB:221618332\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Online Learning for Latent Dirichlet Allocation\" \/>\n<meta name=\"citation_author\" content=\"Matthew D. Hoffman\" \/>\n<meta name=\"citation_author\" content=\"David M. Blei\" \/>\n<meta name=\"citation_author\" content=\"Francis R. Bach\" \/>\n<meta name=\"citation_conference_title\" content=\"Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada.\" \/>\n<meta name=\"citation_publication_date\" content=\"2010\/11\/24\" \/>\n<meta name=\"citation_journal_title\" content=\"Advances in neural information processing systems\" \/>\n<meta name=\"citation_issn\" content=\"1049-5258\" \/>\n<meta name=\"citation_volume\" content=\"23\" \/>\n<meta name=\"citation_firstpage\" content=\"856\" \/>\n<meta name=\"citation_lastpage\" content=\"864\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Francis_Bach\/publication\/221618332_Online_Learning_for_Latent_Dirichlet_Allocation\/links\/0a85e52e37bbcf0247000000.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/221618332_Online_Learning_for_Latent_Dirichlet_Allocation\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/221618332_Online_Learning_for_Latent_Dirichlet_Allocation\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Conference Paper');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-ffd03e95-573f-4db1-8b1b-e9f32412bb2b","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":1000,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw50_56ab1d387cccd"},"id":"rgw50_56ab1d387cccd","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-ffd03e95-573f-4db1-8b1b-e9f32412bb2b", "494f9a776a791dc39f532175b8a796db0068decd");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-ffd03e95-573f-4db1-8b1b-e9f32412bb2b", "494f9a776a791dc39f532175b8a796db0068decd");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw51_56ab1d387cccd"},"id":"rgw51_56ab1d387cccd","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/221618332_Online_Learning_for_Latent_Dirichlet_Allocation","requestToken":"2k3dOSSKPidQIx8LH0MMHnfxZeU5WDJfwkMjKgGqEqCm\/Rs2yHtqCEIe\/h1QaxR8UZSeHhXU7BdaHJfBu1noinwf5kWaQo7v7iNxVJKFAiiKdykyL3eQ0EQuK+jMQJF05mXKRh4BSACME98psmPuiX5C9dujWn8ntQfdvRyFnGTyrxYoga8FoRxy0Lv7oS9L5eUFivaRrjgqvOVlTM0qVZ93wb0CJsZDJnwSnfsknSBxbC0ToGFh7VzmZ7ZPIj9eCDruapGyW6ymR\/w8QZirrP2uT9vLczF0keETKxf6znI=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=vLGonTq1OFN83iAcHNv48F8zNnWmzzX0nQt0H9BFDHWU4tTpcB_rBckLvKqo8MDx","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjIxNjE4MzMyX09ubGluZV9MZWFybmluZ19mb3JfTGF0ZW50X0RpcmljaGxldF9BbGxvY2F0aW9u","signupCallToAction":"Join for free","widgetId":"rgw53_56ab1d387cccd"},"id":"rgw53_56ab1d387cccd","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw52_56ab1d387cccd"},"id":"rgw52_56ab1d387cccd","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw54_56ab1d387cccd"},"id":"rgw54_56ab1d387cccd","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Conference Paper","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
