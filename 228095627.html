<!DOCTYPE html> <html lang="en" class="" id="rgw39_56ab1b7347e32"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="zuJolFWa48xFbSPB1ULCfRiWmjH3+d016lBhYHzVKDpshBMUX4AyEkH2J9yO+kIhb4CMnY1Tm8SoAHA4NpTpZK1lD/bCyKVMlnQ6qKf451pUhqRLj4OuL+Sl4tBDYhFN6AWVJdn/o257E4o+6Wr6NwnXge+PBvMAfCxBJPVcpgedKlKOXROc9bOuwISBh4g9dZKl/lU7YgexVRsKKmdXThxuWb5JyNPIds8PrtDjYGls9QKAUWBg34IeOeoWpXcMCpFI8aWljqSffdYkcz85o8w9drPcZNxEbSGRQJIOnTQ="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-2380e38d-7b3c-4301-a9e0-8d688355fba4",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/228095627_Sparse_Stochastic_Inference_for_Latent_Dirichlet_allocation" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Sparse Stochastic Inference for Latent Dirichlet allocation" />
<meta property="og:description" content="We present a hybrid algorithm for Bayesian topic models that combines the
efficiency of sparse Gibbs sampling with the scalability of online stochastic
inference. We used our algorithm to analyze..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/228095627_Sparse_Stochastic_Inference_for_Latent_Dirichlet_allocation/links/02b045870cf27908e9dda48a/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/228095627_Sparse_Stochastic_Inference_for_Latent_Dirichlet_allocation" />
<meta property="rg:id" content="PB:228095627" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Sparse Stochastic Inference for Latent Dirichlet allocation" />
<meta name="citation_author" content="David Mimno" />
<meta name="citation_author" content="Matt Hoffman" />
<meta name="citation_author" content="David Blei" />
<meta name="citation_publication_date" content="2012/06/27" />
<meta name="citation_volume" content="2" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/228095627_Sparse_Stochastic_Inference_for_Latent_Dirichlet_allocation" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/228095627_Sparse_Stochastic_Inference_for_Latent_Dirichlet_allocation" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Sparse Stochastic Inference for Latent Dirichlet allocation</title>
<meta name="description" content="Sparse Stochastic Inference for Latent Dirichlet allocation on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1b7347e32" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1b7347e32" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab1b7347e32">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Sparse%20Stochastic%20Inference%20for%20Latent%20Dirichlet%20allocation&rft.title=Proceedings%20of%20the%2029th%20International%20Conference%20on%20Machine%20Learning%2C%20ICML%202012&rft.jtitle=Proceedings%20of%20the%2029th%20International%20Conference%20on%20Machine%20Learning%2C%20ICML%202012&rft.volume=2&rft.date=2012&rft.au=David%20Mimno%2CMatt%20Hoffman%2CDavid%20Blei&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Sparse Stochastic Inference for Latent Dirichlet allocation</h1> <meta itemprop="headline" content="Sparse Stochastic Inference for Latent Dirichlet allocation">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/228095627_Sparse_Stochastic_Inference_for_Latent_Dirichlet_allocation/links/02b045870cf27908e9dda48a/smallpreview.png">  <div id="rgw7_56ab1b7347e32" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab1b7347e32" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/David_Mimno" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A278460808351751%401443401638524_m" title="David Mimno" alt="David Mimno" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">David Mimno</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw9_56ab1b7347e32" data-account-key="David_Mimno">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/David_Mimno"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A278460808351751%401443401638524_l" title="David Mimno" alt="David Mimno" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/David_Mimno" class="display-name">David Mimno</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Cornell_University" title="Cornell University">Cornell University</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56ab1b7347e32"> <a href="researcher/79325033_Matt_Hoffman" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Matt Hoffman" alt="Matt Hoffman" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Matt Hoffman</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw11_56ab1b7347e32">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/79325033_Matt_Hoffman"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Matt Hoffman" alt="Matt Hoffman" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/79325033_Matt_Hoffman" class="display-name">Matt Hoffman</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56ab1b7347e32"> <a href="researcher/2064238818_David_Blei" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="David Blei" alt="David Blei" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">David Blei</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw13_56ab1b7347e32">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2064238818_David_Blei"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="David Blei" alt="David Blei" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2064238818_David_Blei" class="display-name">David Blei</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">     Proceedings of the 29th International Conference on Machine Learning, ICML 2012   <meta itemprop="datePublished" content="2012-06">  06/2012;  2.             <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1206.6425" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw14_56ab1b7347e32" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>We present a hybrid algorithm for Bayesian topic models that combines the<br />
efficiency of sparse Gibbs sampling with the scalability of online stochastic<br />
inference. We used our algorithm to analyze a corpus of 1.2 million books (33<br />
billion words) with thousands of topics. Our approach reduces the bias of<br />
variational inference and generalizes to many Bayesian hidden-variable models.</div> </p>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw27_56ab1b7347e32">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw26_56ab1b7347e32"  itemprop="articleBody">  <p>Page 1</p> <p>Sparse stochastic inference for latent Dirichlet allocation<br />David Mimno<br />Princeton U., Dept. of Computer Science, 35 Olden St., Princeton, NJ 08540, USA<br />mimno@cs.princeton.edu<br />Matthew D. Hoffman<br />Columbia U., Dept. of Statistics, Room 1005 SSW, MC 4690 1255 Amsterdam Ave. New York, NY 10027<br />mdhoffma@cs.princeton.edu<br />David M. Blei<br />Princeton U., Dept. of Computer Science, 35 Olden St., Princeton, NJ 08540, USA<br />blei@cs.princeton.edu<br />Abstract<br />We present a hybrid algorithm for Bayesian<br />topic models that combines the efficiency of<br />sparse Gibbs sampling with the scalability of<br />online stochastic inference. We used our algo-<br />rithm to analyze a corpus of 1.2 million books<br />(33 billion words) with thousands of topics.<br />Our approach reduces the bias of variational<br />inference and generalizes to many Bayesian<br />hidden-variable models.<br />1. Introduction<br />Topic models are hierarchical Bayesian models of doc-<br />ument collections (Blei et al., 2003). They can uncover<br />the main themes that pervade a corpus and then use<br />those themes to help organize, search, and explore the<br />documents. In topic modeling, a “topic” is a distri-<br />bution over a fixed vocabulary and each document ex-<br />hibits the topics with different proportions. Both the<br />topics and the topic proportions of documents are hid-<br />den variables. Inferring the conditional distribution of<br />these variables given an observed set of documents is<br />the central computational problem.<br />In this paper, we develop a posterior inference method<br />for topic modeling that can find large numbers of top-<br />ics in massive collections of documents. We demon-<br />strate our approach by analyzing a collection of 1.2<br />million out-of-copyright books, comprising 33 billion<br />observed words. Using our algorithm, we fit a topic<br />model to this corpus with thousands of topics. We il-<br />lustrate the most frequent words from several of these<br />topics in Table 1.<br />Appearing in Proceedings of the 29thInternational Confer-<br />ence on Machine Learning, Edinburgh, Scotland, UK, 2012.<br />Copyright 2012 by the author(s)/owner(s).<br />Our algorithm builds on variational inference (Jor-<br />dan et al., 1999). In variational inference, we define<br />a parameterized family of distributions over the hid-<br />den structure—in this case topics and document-topic<br />proportions—and then optimize the parameters to find<br />a member of the family that is close to the poste-<br />rior. Traditional variational inference for topic model-<br />ing uses coordinate ascent. The algorithm alternates<br />between estimating document-topic proportions under<br />the current settings of the topics and re-estimating the<br />topics based on the estimated document proportions.<br />This requires multiple passes through an entire collec-<br />tion, which is not practical when working with very<br />large corpora.<br />Table 1. Randomly selected topics from a 2000-topic model<br />trained on a library of 1.2 million out-of-copyright books.<br />killed wounded sword slain arms military rifle wounds loss<br />human Plato Socrates universe philosophical minds ethics<br />inflammation affected abdomen ulcer circulation heart<br />ships fleet sea shore Admiral vessels land boats admiral<br />sister child tears pleasure daughters loves wont sigh warm<br />sentence clause syllable singular examples clauses syllables<br />provinces princes nations imperial possessions invasion<br />women Quebec Women Iroquois husbands thirty whom<br />steam engines power piston boilers plant supplied chimney<br />lines points direction planes Lines scale sections extending<br />Recently, Hoffman et al. (2010) introduced Online<br />LDA, a stochastic gradient optimization algorithm for<br />topic modeling. The algorithm repeatedly subsamples<br />a small set of documents from the collection and then<br />updates the topics from an analysis of the subsam-<br />ple. This method uses less memory than the standard<br />approach because we do not need to store topic pro-<br />portions for the full corpus. It also converges faster<br />because we update topics more frequently. However,<br />while it handles large corpora it does not scale to large</p>  <p>Page 2</p> <p>Sparse stochastic inference for latent Dirichlet allocation<br />numbers of topics.<br />Our algorithm builds on this method by using sam-<br />pling to introduce a second source of stochasticity into<br />the gradient. This approach lets us take advantage of<br />sparse computation, scaling sublinearly with the num-<br />ber of topics. Using this algorithm, we can fit topic<br />models to large collections with many topics.<br />2. Hybrid stochastic-MCMC inference<br />We model each of the D documents in a corpus as a<br />mixture of K topics. This topic model can be divided<br />into corpus-level global variables and document-level<br />local variables. The global variables are K topic-word<br />distributions β1,...,βKover the V -dimensional vocab-<br />ulary, each drawn from a Dirichlet prior with param-<br />eter η. For a document d of length Nd, the local vari-<br />ables are (a) a distribution over topics θddrawn from<br />a Dirichlet prior with parameter α and (b) Ndtoken-<br />topic indicator variables zd1,...,zdNddrawn from θd.<br />Our goal is to estimate the posterior distribution of the<br />hidden variables given an observed corpus. We will use<br />variational inference. Unlike standard mean-field vari-<br />ational inference, but similar to Griffiths &amp; Steyvers<br />(2004) and Teh et al. (2006), we will marginalize out<br />the topic proportions θd. Thus we need to approxi-<br />mate the posterior over the topic assignments zdand<br />the topics β1:K.<br />We will use a variational distribution of the form<br />q(z1,...,zD,β1,...,βK) =?<br />This factorization differs from the usual mean-field<br />family for topic models. Rather than defining a distri-<br />bution that factorizes over individual tokens, we treat<br />each document’s sequence of topic indicator variables<br />zd as a unit. As a result q(zd) will be a single dis-<br />tribution over the KNdpossible topic configurations,<br />rather than a product of Nd distributions, each over<br />K possible values.<br />dq(zd)?<br />kq(βk).(1)<br />We now derive an algorithm that uses Gibbs sampling<br />to estimate variational expectations of the local vari-<br />ables and a stochastic natural gradient step to update<br />the variational distribution of global variables. A lower<br />bound on the marginal log probability of the observed<br />words given the hyperparameters is<br />logp(w|α,η) ≥<br />?<br />+<br />d<br />Eqlog<br />?<br />p(zd|α)<br />?<br />i<br />βzdiwdi<br />?<br />(2)<br />?<br />k<br />Eqlogp(βk|η) + H(q),<br />where H(q) denotes the entropy of q.<br />Following Bishop (2006), the optimal variational dis-<br />tribution over topic configurations for a document,<br />holding all other variational distributions fixed, is<br />q?(zd) ∝ exp{Eq(¬zd)[logp(zd|α)p(wd|zd,β)]}<br />=<br />Γ(Kα + Nd)<br />?<br />where Ia=b is 1 if a = b and 0 otherwise, and ¬zd<br />denotes the set of all unobserved variables besides zd.<br />We can compute Eq. 4 for a specific topic configura-<br />tion zd, but we cannot tractably normalize it to get<br />the distribution q?(zd) over all KNdconfigurations.<br />(3)<br />Γ(Kα)<br />?<br />k<br />Γ(α +?<br />iIzdi=k)<br />Γ(α)<br />(4)<br />×<br />i<br />expEq[logβzdiwdi]<br />The optimal variational distribution over topic-word<br />distributions, holding the other distributions fixed, is<br />the kernel of a Dirichlet distribution with parameters<br />?<br />This expression includes the expectation under q of<br />the number of tokens of type w assigned to topic k.<br />Computing this expectation would require evaluating<br />the intractable distribution q?(zd).<br />λkw= η +<br />d<br />?<br />i<br />Eq[Izdi=kIwdi=w]. (5)<br />2.1. Online stochastic inference<br />We optimize the variational topic-word parameters<br />λkwusing stochastic gradient ascent. Stochastic gra-<br />dient ascent iteratively updates parameters with noisy<br />estimates of the gradient. We obtain these noisy esti-<br />mates by subsampling the data (Sato, 2001; Hoffman<br />et al., 2010).<br />We first recast the variational objective in Eq. 2 as<br />a summation over per-document terms ?d, so that the<br />full gradient with respect to λkis the sum?<br />full gradient by sampling a minibatch of documents<br />B and then scaling the sum of the document-specific<br />gradients to match the total size of the corpus,<br />?<br />d<br />∂<br />∂λk?d.<br />We can then generate a noisy approximation to this<br />?<br />d<br />∂<br />∂λk?d= E<br />D<br />|B|<br />?<br />d∈B<br />∂<br />∂λk?d<br />?<br />. (6)<br />(The expectation is with respect to the random sample<br />B.) Pushing the per-topic terms in Eq. 2 inside the<br />summation over documents and removing terms not<br />involving λkwwe obtain<br />?<br />+1<br />D<br />?d=<br />w<br />?<br />Eq[Ndkw] +1<br />?<br />D(η − λkw)<br />?<br />?<br />Eq[logβkw] (7)<br />logΓ(?<br />wλkw) −<br />w<br />logΓ(λkw)<br />?</p>  <p>Page 3</p> <p>Sparse stochastic inference for latent Dirichlet allocation<br />Algorithm<br />variational-Gibbs inference.<br />for t ∈ 1,...,∞ do<br />ρt←<br />sample minibatch B<br />for d ∈ B do<br />initialize z0<br />discard B burn-in sweeps<br />for sample s ∈ 1,...,S do<br />for token i ∈ 1,...,Nddo<br />sample zs<br />end for<br />end for<br />end for<br />λt<br />end for<br />1 Algorithmfor hybrid stochastic<br />?<br />1<br />t0+t<br />?κ<br />d<br />di∝ (α + Ndk)eEq[log βkw]<br />kw← (1 − ρt)λt−1<br />kw+ ρt<br />?<br />η +<br />D<br />|B|ˆ Nkw<br />?<br />where Eq[Ndkw] =?<br />can be factored into the product of a matrix and a<br />vector. The matrix, which contains derivatives of the<br />digamma function, is the Fisher information matrix<br />for the topic parameters. Element w of the vector is<br />iEq[Izdi=kIwdi=w]. The gradient<br />of Eq. 7 with respect to the parameters λk1,...,λkV<br />Eq[Ndkw] +1<br />D(η − λkw). (8)<br />Premultiplying the gradient of an objective function by<br />the inverse Fisher information matrix of the distribu-<br />tion being optimized (in our case the variational distri-<br />bution q) results in the natural gradient (Sato, 2001).<br />Since our gradient is the product of the Fisher informa-<br />tion matrix and a vector, the natural gradient is there-<br />fore simply Eq. 8 (Hoffman et al., 2010). Compared to<br />the standard Euclidean gradient, the natural gradient<br />offers both faster convergence (because it takes into<br />account the information geometry of the variational<br />distribution) and cheaper computation (because the<br />vector in Eq. 8 is a simple linear function).<br />2.2. MCMC within stochastic inference<br />We cannot evaluate the expectation in Eq. 8 because<br />we would have to consider a combinatorial number of<br />topic configurations zd. To use stochastic gradient as-<br />cent, however, we only need an approximation to this<br />expectation. We use Markov chain Monte Carlo to<br />sample topic configurations from q?(zd). We then use<br />the empirical average of these samples to estimate the<br />expectations needed for Eq. 8.<br />Gibbs sampling for a document starts with a random<br />initialization of the topic indicator variables zd. We<br />then iteratively resample the topic indicator at each<br />position from the conditional distribution over that<br />position given the remaining topic indicator variables:<br />q?(zdi= k|z\i) ∝ (α +?<br />j?=iIzj=k)exp{Eq[logβkwdi]},<br />(9)<br />where the expectation of the log probability of word<br />w given a topic k is Ψ(λkw) − Ψ(?<br />urations. Once we have saved S samples {z}1,...,S, we<br />can define approximate sufficient statistics<br />w?λkw?). After B<br />burn-in sweeps, we begin saving sampled topic config-<br />Eq[Ndkw] ≈ˆ Nkw=1<br />S<br />?<br />s<br />?<br />d∈B<br />?<br />i<br />Izs<br />di=kIwdi=w. (10)<br />Using MCMC estimates adds noise to our gradient,<br />but allows us to use a collapsed objective function that<br />does not represent document-topic proportions θd. In<br />addition, an average over a finite set of samples pro-<br />vides a sparse estimate of the gradient: for many words<br />and topics, our estimate of Eq[Ndkw] will be zero.<br />2.3. Algorithm<br />We have defined a natural gradient and a method for<br />approximating the sufficient statistics of that gradient.<br />For a sequence of learning rates ρt= (t0+ t)−κ, the<br />following update will lead to a stationary point:<br />?<br />?<br />λt<br />kw← λt−1<br />kw+ ρt<br />D<br />|B|<br />?<br />d∈B<br />ˆ Ndkw+1<br />D(η − λkw)<br />?<br />= (1 − ρt)λt−1<br />kw+ ρt<br />η +D<br />|B|<br />?<br />d∈B<br />ˆ Ndkw<br />?<br />. (11)<br />This update results in Algorithm 1. Two implementa-<br />tion details that result in sparse computations can be<br />found in Appendix A. This online algorithm has the<br />important advantage over Online LDA of preserving<br />sparsity in the topic-word parameters, so that λkw= η<br />for most values of k and w. Sparsity increases the effi-<br />ciency of updates to λkand of Gibbs sampling for zd.<br />Previous variational methods lead to dense updates to<br />KV topic parameters, making them expensive to ap-<br />ply to large vocabularies and large numbers of topics.<br />Our method, in contrast, is able to exploit the sparsity<br />exhibited by samples from the variational distribution<br />q?, resulting in much more efficient updates.<br />3. Related Work<br />This paper combines two sources of zero-mean noise in<br />constructing an approximate gradient for a variational<br />inference algorithm: subsampling of data, and Monte<br />Carlo inference. These sources of variance have been</p>  <p>Page 4</p> <p>Sparse stochastic inference for latent Dirichlet allocation<br />used individually in previous work. Stochastic approx-<br />imation EM (SAEM, Delyon et al., 1999) combines an<br />EM algorithm with a stochastic online inference proce-<br />dure. SAEM does not subsample data, but rather in-<br />terpolates between Monte Carlo estimates of the com-<br />plete data. Kuhn &amp; Lavielle (2004) extend SAEM to<br />use MCMC estimates. Similarly, online EM (Capp´ e &amp;<br />Moulines, 2009) sub-samples data but preserves stan-<br />dard inference procedures for local variables.<br />Standard collapsed Gibbs sampling uses multiple<br />sweeps over the entire corpus, representing topic-word<br />distributions using the topic-word assignment vari-<br />ables of the entire corpus except for the current token.<br />As a result, topic assignment variables must in theory<br />be sampled sequentially, although parallel approxima-<br />tions work well empirically (Asuncion et al., 2008). In<br />contrast, Algorithm 1 treats topic-word distributions<br />as a global variable distinct from the local token-topic<br />assignment variables, and so can parallelize trivially.<br />In this work we integrate over document-topic pro-<br />portions θdwithin a variational algorithm. Collapsed<br />variational inference (Teh et al., 2006) also analytically<br />marginalizes over the topic proportions, but still main-<br />tains a fully factorized distribution over topic assign-<br />ments at each position. The method described here<br />does not restrict itself to such factored distributions,<br />and therefore reduces bias, but this reduction may be<br />offset by the bias we introduce when we initialize the<br />Gibbs chain.<br />4. Empirical Results<br />In this section we compare the sampled online algo-<br />rithm to two related online methods and measure the<br />effect of model parameters. We use a selection of met-<br />rics to evaluate models.<br />4.1. Evaluation<br />Held-out probability.<br />the semantic structure of a corpus should place more<br />of its probability mass on sensible documents than<br />on random sequences of words. We can use this as-<br />sumption to compare different models by asking each<br />model to estimate the probability of a previously un-<br />seen document. A better model should, on average,<br />assign higher probability to real documents than a<br />lower-quality model. We evaluate held-out probabil-<br />ity using the left-to-right sequential sampling method<br />(Wallach et al., 2009; Buntine, 2009). For each trained<br />model we generate point estimates of the topic-word<br />probabilities ˜ p(w|k). We then process each document<br />by iterating through the tokens w1,...,wNd. At each<br />A model that characterizes<br />position i we calculate the marginal probability<br />?<br />We then sample ziproportional to the terms of that<br />summation and continue to the next token.1In order<br />to normalize for document lengths, we divide the sum<br />of the logs of the marginal probabilities by Nd.<br />˜ p(wi|w&lt;i) =<br />k<br />p(zi= k|w&lt;i,z&lt;i,α)˜ p(wi|k).(12)<br />Coherence.<br />quality of a topic by approximating the experience of<br />a user viewing the W most probable words for the<br />topic (Mimno et al., 2011). It is related to point-wise<br />mutual information (Newman et al., 2010). Let D(w)<br />be the document frequencies for each word w, that is,<br />the number of documents containing one or more to-<br />kens of type w, and let D(w1,w2) be the number of<br />documents containing at least one token of w1and of<br />w2. For each pair of words w1,w2in the top W list,<br />we calculate the number of documents that contain at<br />least one token of the higher ranked word w1that also<br />contain at least one token of the lower ranked word<br />w2:<br />?<br />where ? is a small constant used to avoid log zero.<br />Values closer to zero indicate greater co-occurrence.<br />Unlike held-out probability, which reports scores for<br />held-out documents, coherence reports scores for indi-<br />vidual topics.<br />This metric measures the semantic<br />C(W) =<br />i<br />?<br />j&lt;i<br />logD(wi,wj) + ?<br />D(wj)<br />(13)<br />Wallclock time.<br />as efficiently as possible. In addition to model qual-<br />ity metrics, we are therefore also interested in total<br />computation time.<br />Our goal is to train useful models<br />4.2. Comparison to Online VB<br />Our first corpus consists of 350,000 research articles<br />from three major journals: Science, Nature, and the<br />Proceedings of the National Academy of Sciences of<br />the USA. We use a vocabulary with 19,000 distinct<br />words, including selected multi-word terms. We train<br />models on 90% of the Science/Nature/PNAS corpus,<br />holding out the remaining documents for testing pur-<br />poses. We save topic-word parameters<br />epochs consisting of 500,000 documents.<br />ˆ Nkw after<br />Sampled online variational Bayesian inference com-<br />pares well in terms of wallclock time to standard online<br />VB inference, particularly with respect to the number<br />of topics K. Figure 1 shows results comparing stan-<br />dard online VB inference to sampled online inference<br />1We set the document-topic hyperparameter α = 0.1.</p>  <p>Page 5</p> <p>Sparse stochastic inference for latent Dirichlet allocation<br />Topics<br />Sec<br />10<br />20<br />30<br />40<br />G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G<br />G G G G G<br />G<br />G G G GGG G G G GG G G GG<br />G G G G G G G G G G G G G GGG G G GGG G G<br />GG G G G<br />G G G G<br />G GG G<br />G<br />GG GG G<br />G<br />G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G<br />G G G G G G G G G G G G G G G G G G G G G G G<br />G G G G G G G G G G G G G G G G G G G G G G G<br />G<br />G<br />G G G G G G G G G<br />G<br />G G G G G G G G G G G G G G G G G G G G G G G G<br />G G G G G G G G G G G G G G G G G G G G G G G G G<br />GG G G GG G G G GG G GG GG G G G G G G G<br />G G G G G G G G G G G G G G G G GG G G G G G<br />G G G<br />G<br />G G G G G G G G G G G G G G G G G G G GG<br />G<br />G<br />G<br />G<br />G G G G G G G G G G G G G G<br />G G G G G G G G G G G G G G G G G G G G G G G G G<br />G GG G G G G G G G G G G G G G G G G G G G G G G G<br />G G G G G G G G G G G G G G G G G G G G G G G G GG G<br />2004006008001000<br />Method<br />G<br />VB<br />G<br />Sampled<br />Figure 1. Comparison of seconds per mini-batch between<br />online variational Bayes (Hoffman et al., 2010) and sampled<br />online inference (this paper). Online VB is linear in K,<br />while sampled inference takes advantage of sparsity.<br />for K up to 1000. Each iteration consists of a mini-<br />batch of 100 documents.<br />takes time linear in K, while wallclock time for sam-<br />pled online inference grows more slowly.<br />Standard online inference<br />We would like to know if there is a difference in the<br />quality of models trained through the hybrid sampled<br />variational algorithm and the online LDA algorithm.<br />We compare an implementation of Online LDA that<br />tries to be as close as possible to the sampled online<br />implementation, but using a dense VB update instead<br />of a sparse sampled update for the local variables. In<br />particular, the number of coordinate ascent steps in<br />standard VB is equal to the number of Gibbs sweeps<br />in the sampled algorithm.<br />Per-topic coherence for K = 200 is shown in Fig-<br />ure 2. Sampled online inference produces fewer very<br />poor topics. This difference is significant under a two-<br />sample t-test (p &lt; 0.001) and does not decrease with<br />additional training epochs. Sampled online inference<br />also assigns greater held-out probability than Online<br />LDA for every test document, by a wide margin. We<br />evaluated several possible reasons for this difference in<br />performance. Held-out probability estimation can be<br />affected by evaluation-time smoothing parameter set-<br />tings, but we found both models were affected equally.<br />The log probability of a document is the sum of the<br />log probabilities of its words.<br />one model assigned very small probability to a hand-<br />ful of tokens, those words could significantly affect the<br />overall score, but the difference in log probability was<br />consistent across many tokens. The scale of parame-<br />ters might not be comparable, but as both methods<br />use the same learning schedule, the sum of the trained<br />parameters λkwis nearly identical.<br />It is possible that if<br />The main difference appears to be the entropy of the<br />Coherence<br />Algorithm<br />SampOnline<br />VB<br />GGG<br />GGGGGGGGGGG G GGGGGG<br />−1200 −1000−800 −600−400−200<br />Figure 2. The new sampled online algorithm produces<br />fewer low-quality topics than Online LDA at K = 200.<br />Heldout log likelihood is much worse for Online LDA.<br />Coherence<br />Algorithm<br />SampOnline<br />SMC<br />GGGGG G GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG G GGGGGGGG G GGGGG<br />G GGGGGG G G GGG G GGGGG G GGGGGGGG GGGGGGGGGGGGG GGGGGGGGGG GGGGGGGGGG<br />−1200−1000−800−600 −400−200<br />HeldOut<br />Algorithm<br />SampOnline<br />SMC<br />G GGGGG GGGGG GGG G GGGGGGGG GGGGGGGGGGGGGG G GG GGG GGGGGGGGGGGGGGGGGGGGGGG GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG G GGGGGGGGGGGGGGGGGGGGGGGGGGGGGG<br />GGGG GGGGGGGGGGGGGGGG GGGGG GGGG GGGGGGGGGGGGGGGGGGGGGGGGG GGG GGGG GGGGGG GGGGGGGGGGGG GGGGGG GGGGGGGGGGGG GGGGGGGGGGGGG GGGGGGGG G GGGGGGG GGGGGGG GGGGGGGGGGGGGGGGGGGGGGGGG<br />−9.5−9.0−8.5−8.0−7.5−7.0<br />Figure 3. Sampled online inference performs better than<br />one pass of sequential Monte Carlo, after processing a com-<br />parable number of documents with K = 200.<br />topic distributions: the sampled-online algorithm pro-<br />duces less concentrated distributions (mean entropy<br />6.8 ± 0.46) than standard online LDA (mean entropy<br />6.0±0.58). This result could indicate that coordinate<br />ascent over the local variables for Online LDA is not<br />converging.<br />4.3. Comparison to Sequential Monte Carlo<br />Sequential Monte Carlo is an online algorithm similar<br />to Gibbs sampling in that it represents topics using<br />sums over assignment variables (Ahmed et al., 2012).<br />A Gibbs sampler starts with a random initialization<br />for all hidden variables and sweeps repeatedly over the<br />entire data set, updating each variable given the cur-<br />rent value of all other variables. SMC samples values<br />for hidden variables in sequential order, conditioning<br />only on previously-seen variables.<br />keep multiple sampling states or “particles”, but this<br />process adds both computation and significant book-<br />keeping complexity. Ahmed et al. (2012) use a single<br />SMC state.<br />It is common to<br />In order to compare SMC to the sampled online al-<br />gorithm, we ran 10 independent SMC samplers over<br />the Science/Nature/PNAS dataset, with documents</p>  <p>Page 6</p> <p>Sparse stochastic inference for latent Dirichlet allocation<br />ordered randomly. We also ran 10 independent sam-<br />pled trainers, stopping after a number of documents<br />had been sampled equivalent to the size of the corpus.<br />In order to make the comparison more fair, we allowed<br />the SMC sampler to sweep through each document the<br />same number of times as the sampled online algorithm,<br />but only the final topic configuration of a document<br />was available to the subsequent documents.2Results<br />for K = 200 are shown in Figure 3. SMC has con-<br />sistently worse per-topic coherence and per-document<br />held-out log probability.<br />rithm in this paper differs from SMC in that the contri-<br />bution of local token-topic assignment variables decays<br />according to the learning rate schedule, so that more<br />recently sampled documents can have greater weight<br />than earlier documents. This decay allows sampled on-<br />line inference to “forget” its initial topics, unlike SMC,<br />which weights all documents equally.<br />The sampled online algo-<br />4.4. Effect of parameter settings<br />Number of samples.<br />rithm we initialize3the topic indicator variables z for<br />a document and then perform several Gibbs sweeps.<br />In each sweep we resample the value of each topic in-<br />dicator variable in turn. We introduce bias when we<br />initialize, so we discard B “burn-in” sweeps and use<br />values of z saved after S additional sweeps to calculate<br />the gradient. Since performance is linear in the total<br />number of sweeps B +S, we want to find the smallest<br />number of sweeps that does not sacrifice performance.<br />In the inner loop of our algo-<br />We consider nine settings of the pair (B,S). Under<br />the first three settings we save one sweep and vary<br />the number of burn-in sweeps: (1,1), (2,1), (3,1). For<br />the second three settings we perform five sweeps, vary-<br />ing how many we discard: (2,3), (3,2), (4,1). The final<br />three settings fix B = S and consider larger total num-<br />bers of sweeps: (5,5), (10,10), (20,20). We evaluate<br />each setting after processing 500,000 documents.<br />Performance was similar across settings with the fol-<br />lowing exceptions, which were significant at p &lt; 0.001<br />under a two-sample t-test. The two-sweep setting (1,1)<br />had better topic coherence but worse held-out proba-<br />bility than the all other settings. The (5,5) setting<br />had the best mean held-out probability, but it was<br />not significantly better than (10,10) and (20,20). The<br />2Note that SMC has an advantage.<br />online algorithm we Gibbs sample each document within a<br />mini-batch independently, while in SMC, documents “see”<br />results from all previous documents.<br />3We initialize by sampling each token conditioned on<br />the topics of the previous tokens in the document:<br />p(zdi= k) ∝ (α +?<br />In the sampled<br />j&lt;iIzdj=k)p(wdi|k).<br />many-sweep settings (5,5), (10,10), (20,20) had worse<br />topic coherence than the other settings, with many vis-<br />ibly low-quality topics. These results suggest that 3–5<br />sweeps is sufficient.<br />Topic-word smoothing.<br />tion eΨ(x). This function approaches x −1<br />large, but for values of x near 0, it is non-linear. For<br />example, eΨ(0.05)is 1034times greater than eΨ(0.01). If<br />the values of topic parameters are in this range, a mi-<br />nuscule increase in the parameter for word w in topic<br />k can cause a profound change in the sampling distri-<br />bution for that word: all subsequent tokens of type w<br />will be assigned to topic k with probability near 1.0.<br />Eq. 9 involves the func-<br />2as x gets<br />In general, the randomness introduced by sampling<br />topic assignments helps to avoid becoming trapped in<br />local maxima. When parameters are near zero, how-<br />ever, random decisions early in the inference process<br />risk becoming permanent.<br />ing parameter η can push parameter values away<br />from this explosive region. We measured coherence<br />for six settings of the topic-word hyperparameter η,<br />{0.1,0.2,0.3,0.4,0.5,0.6}. At η = 0.1, a common value<br />for batch variational inference, many topics are visi-<br />bly nonsensical. Average coherence improves signifi-<br />cantly for each increasing value of η ∈ {0.2,0.3,0.4}<br />(p &lt; 0.001). There is no significant difference in aver-<br />age coherence for η ∈ {0.4,0.5,0.6}.<br />The topic-word smooth-<br />Forgetting factors.<br />rate ρt = (t0+ t)−κand its relation to the corpus<br />size D. We fix κ = 0.6 and vary the offset param-<br />eter t0∈ {3000,15000,30000,150000,300000}, saving<br />topic parameters after five training epochs of 500,000<br />documents each. There was no significant difference<br />in average topic coherence.<br />We now consider the learning<br />The learning rate, however, is not the only factor that<br />determines the magnitude of parameter updates. Eq.<br />11 also includes the size of the corpus D. If the cor-<br />pus is larger, we will take larger steps, regardless of<br />the contents of the mini-batch. The offset parameter<br />t0 had no significant effect on coherence for the full<br />corpus, but it may have an effect if we also vary the<br />corpus size.<br />We simulate different size corpora by subsampling the<br />full data set. Results are shown in Figure 4 for models<br />trained on one half, one quarter, and one eighth of the<br />corpus. Each corpus is a subset of the next larger cor-<br />pus. In the smallest corpus (12.5%), the model with<br />t0= 300000 is significantly worse than other settings<br />(p &lt; 0.001). Otherwise, there is no significant differ-<br />ence in average topic coherence.</p>  <p>Page 7</p> <p>Sparse stochastic inference for latent Dirichlet allocation<br />t0<br />Coherence<br />−1000<br />−800<br />−600<br />−400<br />−200<br /> 12.5<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G<br />G G<br />G<br />300015000 30000 150000 300000<br /> 25.0<br />G<br />G<br />G<br />G G<br />G<br />G<br />300015000 30000 150000 300000<br /> 50.0<br />G<br />G<br />G<br />G<br />G<br />G<br />300015000 30000 150000 300000<br />100.0<br />G<br />G<br />300015000 30000 150000 300000<br />Figure 4. Topic quality is lowest for large values of t0, but only in small corpora. Panels represent the proportion of<br />training data used. Each panel shows coherence values for five K = 100 topic models with varying learning rates.<br />4.5. Scalability<br />Pre-1922 books.<br />the method, we modeled a collection of 1.2 million out-<br />of-copyright books. Topic models are useful in char-<br />acterizing the contents of the corpus and supporting<br />browsing applications: even scanning titles for a col-<br />lection of this size is impossible for one person. Pre-<br />vious approaches to million-book digital libraries have<br />focused on keyword search and word frequency his-<br />tograms (Michel et al., 2011). Such methods do not<br />account for variability in meaning or context. There is<br />no guarantee that the words being counted match the<br />meaning assumed by the user. In contrast, an interface<br />based on a topic model could, for example, distinguish<br />uses of the word “strain” in immunology, mechanical<br />engineering, and cookery.<br />To demonstrate the scalability of<br />We divide each book into 10-page sections, resulting in<br />44 million “documents” with a vocabulary size of 216.<br />We trained models with K ∈ {100,500,1000,2000}.<br />Randomly selected example topics are shown in Ta-<br />ble 1, illustrating the average level of topic quality.<br />Models are sparse: at K = 2000, less than 1% of the<br />2000·216possible topic-word parameters are non-zero.<br />The algorithm scales well as K increases. The number<br />of milliseconds taken to process a sequence of 10,000<br />documents was similar for K = 1000 and 2000, despite<br />doubling the number of topics.<br />5. Conclusions<br />Stochastic online inference allows us to scale topic<br />modeling to large document sets. Sparse Gibbs sam-<br />pling allows us to scale to large numbers of topics.<br />The algorithm presented in this paper combines the<br />advantages of these two methods. As a result, models<br />can be trained on vast, open-ended corpora without<br />requiring access to vast computer clusters. If parallel<br />architectures are available, we can trivially parallelize<br />computation within each mini-batch. As this work is<br />related to the Online LDA algorithm of Hoffman et al.<br />(2010), extensions to that model are also applicable,<br />such as adaptive scheduling algorithms (Wahabzada &amp;<br />Kersting, 2011). The use of MCMC within stochastic<br />variational inference reduces one source of bias in es-<br />timating local variables. Although we have focused on<br />text analysis applications, this hybrid method gener-<br />alizes to a broad class of Bayesian models.<br />Acknowledgments<br />John Langford, Iain Murray, Charles Sutton provided<br />helpful comments.Yahoo!<br />computational resources. DM is supported by a CRA<br />CI fellowship.MDH is supported by NSF ATM-<br />0934516, DOE DE-SC0002099, and IES R305D100017.<br />DMB is supported by ONR N00014-11-1-0651, NSF<br />CAREER 0745520, AFOSR FA9550-09-1-0668, the<br />Alfred P. Sloan foundation, and a grant from Google.<br />and PICSciE provided<br />References<br />Ahmed,<br />Narayanamurthy, Shravan, and Smola, Alexander. Scal-<br />able inference in latent variable models. In WSDM, 2012.<br />Amr,Aly, Mohamed,Gonzalez, Joseph,<br />Asuncion, Arthur, Smyth, Padhraic, and Welling, Max.<br />Asynchronous distributed learning of topic models. In<br />NIPS, 2008.<br />Bishop, Christopher M. Pattern Recognition and Machine<br />Learning. Springer, 2006.<br />Blei, David, Ng, Andrew, and Jordan, Michael. Latent<br />Dirichlet allocation. Journal of Machine Learning Re-<br />search, 3:993–1022, January 2003.<br />Buntine, Wray L. Estimating likelihoods for topic models.<br />In Asian Conference on Machine Learning, 2009.<br />Capp´ e, Olivier and Moulines, Eric. Online EM algorithm<br />for latent data models. Journal of the Royal Statistical<br />Society Series B, 71(3):593–613, 2009.<br />Delyon, Bernard, Lavielle, Marc, and Moulines, Eric. Con-<br />vergence of a stochastic approximation version of the<br />EM algorithm. Annals of Statistics, 27(1):94–128, 1999.</p>  <p>Page 8</p> <p>Sparse stochastic inference for latent Dirichlet allocation<br />Griffiths, Thomas L. and Steyvers, Mark. Finding scientific<br />topics. PNAS, 101(suppl. 1):5228–5235, 2004.<br />Hoffman, Matthew, Blei, David, and Bach, Francis. Online<br />learning for latent dirichlet allocation. In NIPS, 2010.<br />Jordan, Michael, Ghahramani, Zoubin, Jaakkola, Tommi,<br />and Saul, Laurence. Introduction to variational methods<br />for graphical models.Machine Learning, 37:183–233,<br />1999.<br />Kuhn, Estelle and Lavielle, Marc. Coupling a stochastic<br />approximation version of EM with an MCMC procedure.<br />ESAIM: Probability and Statistics, 8:115–131, August<br />2004.<br />Michel,<br />Aviva Presser, Veres, Adrian, Gray, Matthew K.,<br />Team, The Google Books, Pickett, Joseph P., Hoiberg,<br />Dale, Clancy, Dan, Norvig, Peter, Orwant, Jon, Pinker,<br />Steven, Nowak, Martin A., , and Aiden, Erez Lieber-<br />man. Quantitative analysis of culture using millions of<br />digitized books. Science, 311, 2011.<br />Jean-Baptiste,Shen,YuanKui,Aiden,<br />Mimno, David, Wallach, Hanna, Talley, Edmund, Leen-<br />ders, Miriam, and McCallum, Andrew. Optimizing se-<br />mantic coherence in topic models. In EMNLP, 2011.<br />Newman, David, Lau, Jey Han, Grieser, Karl, and Bald-<br />win, Timothy. Automatic evaluation of topic coherence.<br />In Human Language Technologies: The Annual Confer-<br />ence of the North American Chapter of the Association<br />for Computational Linguistics, 2010.<br />Sato, M.A. Online model selection based on the variational<br />Bayes. Neural Computation, 13(7):1649–1681, 2001.<br />Teh, Yee-Whye, Newman, David, and Welling, Max. A col-<br />lapsed variational bayesian inference algorithm for latent<br />dirichlet allocation. In NIPS, 2006.<br />Wahabzada, Mirwaes and Kersting, Kristian. Larger resid-<br />uals, less work: Active document scheduling for latent<br />Dirichlet allocation. In ECML/PKDD, 2011.<br />Wallach, Hanna, Murray, Iain, Salakhutdinov, Ruslan, and<br />Mimno, David. Evaluation methods for topic models. In<br />ICML, 2009.<br />A. Sparse computation<br />Sparse sampling over topics.<br />(α + Ndk)eEq[log βkw]requires calculating the normal-<br />izing constant Z =?<br />O(k) if we can represent the topic-word parameters<br />λkwsparsely. The smoothing parameter η can be fac-<br />tored out of Equation 11 as long as we assume that all<br />initial values λ0<br />kw≥ η. Rearranging this equation to<br />separate the Dirichlet hyperparameter η<br />Sampling zs<br />di∝<br />k(α + Ndk)eEq[log βkw]. This cal-<br />culation can be accomplished in time much less than<br />λt<br />kw← η + (1 − ρt)?λt−1<br />kw− η?+ ρt<br />D<br />|B|NS<br />kw<br />(14)<br />shows that we can define an alternative parameter<br />˜ Nt<br />kw− η that represents the “non-smoothing”<br />portion of the variational Dirichlet parameter, and ig-<br />nore the contribution of the smoothing parameter until<br />it is time to calculate expectations.<br />kw= λt<br />For any given w, it is likely that most values of˜ Nkw<br />will be zero. We can therefore rewrite the normalizing<br />constant as<br />?<br />?<br />The second summation does not depend on any word-<br />specific variables, and can therefore be calculated and<br />then updated incrementally as Ndkchanges. The first<br />summation is non-zero only for k such that˜ Nkw&gt; 0.<br />Z =<br />k<br />α + Ndk<br />eΨ(V η+˜Nk◦)<br />?<br />eΨ(η+˜Nkw)− eΨ(η)?<br />α + Ndk<br />eΨ(V η+˜<br />+<br />k<br />Nk◦)eΨ(η).(15)<br />Sparse updates in the vocabulary.<br />that a typical mini-batch will contain a small fraction<br />of the words in the vocabulary. Eq. 11, however, up-<br />dates˜ Nkwfor all words, even words that do not occur<br />in the current mini-batch. Expanding the recursive<br />definition of˜ Nt<br />?<br />We expect<br />kw, and lettingˆ Nt<br />kw=<br />D<br />|B|NS<br />kw,<br />˜ Nt<br />kw= ρtˆ Nt<br />kw+ (1 − ρt)ρt−1ˆ Nt−1<br />kw+ (1 − ρt−1)(...)<br />?<br />(16)<br />= ρtˆ Nt<br />kw+ (1 − ρt)ρt−1ˆ Nt−1<br />kw+ (1 − ρt)(1 − ρt−1)...<br />(17)<br />Dividing both sides by?t<br />˜ Nt<br />kw<br />?t<br />i=1(1 − ρi),<br />ρtˆ Nt<br />kw<br />i=1(1 − ρi)+<br />ρt−2ˆ Nt−2<br />?t−2<br />i=1(1 − ρi), the update be-<br />i=1(1 − ρi)<br />=<br />?t<br />+<br />ρt−1ˆ Nt−1<br />?t−1<br />+ ....<br />kw<br />i=1(1 − ρi)<br />(18)<br />kw<br />i=1(1 − ρi)<br />Defining a variable πt=?t<br />comes<br />˜ Nt<br />πt<br />kw<br />=<br />˜ Nt−1<br />kw<br />πt−1<br />+ρtˆ Nt<br />kw<br />πt<br />. (19)<br />This update is sparse:<br />zero ndw will be modified.<br />pectation of p(w|k), we compute Ψ<br />ΨWη + πt<br />w<br />πt<br />only elements with non-<br />To calculate the ex-<br />?<br />η + πt<br />Nt<br />πt<br />kw<br />?<br />−<br />?<br />?<br />Nt<br />kw<br />?<br />.<br />The scale factor πtcan become small after several hun-<br />dred mini-batches. We periodically “reset” this pa-<br />rameter by setting all stored values to˜ Nt<br />avoiding the possibility of numerical instability.<br />kw= πt<br />˜<br />πt,<br />Nt<br />kw</p>   </div> <div id="rgw19_56ab1b7347e32" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw20_56ab1b7347e32">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw21_56ab1b7347e32"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://arxiv.org/pdf/1206.6425" target="_blank" rel="nofollow" class="publication-viewer" title="Sparse Stochastic Inference for Latent Dirichlet allocation">Sparse Stochastic Inference for Latent Dirichlet a...</a> </div>  <div class="details">   Available from <a href="http://arxiv.org/pdf/1206.6425" target="_blank" rel="nofollow">arxiv.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw28_56ab1b7347e32" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (22) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw29_56ab1b7347e32" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw30_56ab1b7347e32" >  <div class="indent-left">  <div id="rgw31_56ab1b7347e32" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/David_Knowles2" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: David A. Knowles </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw32_56ab1b7347e32">  <li class="citation-context-item"> "Meanwhile the topic modelling community has taken great strides developing stochastic variational inference methods for latent Dirichlet allocation (Blei et al., 2003), encouraged by the availability of large corpora of text. The idea was initially proposed in Hoffman et al. (2010), and refined in Mimno et al. (2012) where the sparse updates of Gibbs sampling were leveraged to scale inference on just a single machine to 1.2 million books. The latter idea allows nontruncated online learning (Wang &amp; Blei, 2012) of Bayesian non-parametric models, though only the hierachical Dirichlet process (Teh et al., 2004) was demonstrated. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process"> <span class="publication-title js-publication-title">An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2076919269_Amar_Shah" class="authors js-author-name ga-publications-authors">Amar Shah</a> &middot;     <a href="researcher/54244192_David_A_Knowles" class="authors js-author-name ga-publications-authors">David A. Knowles</a> &middot;     <a href="researcher/8159937_Zoubin_Ghahramani" class="authors js-author-name ga-publications-authors">Zoubin Ghahramani</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Stochastic variational inference (SVI) is emerging as the most promising
candidate for scaling inference in Bayesian probabilistic models to large
datasets. However, the performance of these methods has been assessed primarily
in the context of Bayesian topic models, particularly latent Dirichlet
allocation (LDA). Deriving several new algorithms, and using synthetic, image
and genomic datasets, we investigate whether the understanding gleaned from LDA
applies in the setting of sparse latent factor models, specifically beta
process factor analysis (BPFA). We demonstrate that the big picture is
consistent: using Gibbs sampling within SVI to maintain certain posterior
dependencies is extremely effective. However, we find that different posterior
dependencies are important in BPFA relative to LDA. Particularly,
approximations able to model intra-local variable dependence perform best. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Jun 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/David_Knowles2/publication/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process/links/55a53c7b08aef604aa042e0b.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw33_56ab1b7347e32" >  <div class="indent-left">  <div id="rgw34_56ab1b7347e32" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/268451692_A_Nonparametric_Bayesian_Approach_Toward_Stacked_Convolutional_Independent_Component_Analysis">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Sotirios_Chatzis" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Sotirios P Chatzis </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw35_56ab1b7347e32">  <li class="citation-context-item"> "We devise an efficient inference algorithm for our model under a hybrid variational inference paradigm, similar to [25]. In contrast to traditional variational inference algorithms, which require imposition of truncation thresholds for the model or the variational distribution over the extracted features [14], our method adapts model complexity on the fly. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/268451692_A_Nonparametric_Bayesian_Approach_Toward_Stacked_Convolutional_Independent_Component_Analysis"> <span class="publication-title js-publication-title">A Nonparametric Bayesian Approach Toward Stacked Convolutional Independent Component Analysis</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/58723617_Sotirios_P_Chatzis" class="authors js-author-name ga-publications-authors">Sotirios P. Chatzis</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Unsupervised feature learning algorithms based on convolutional formulations
of independent components analysis (ICA) have been demonstrated to yield
state-of-the-art results in several action recognition benchmarks. However,
existing approaches do not allow for the number of latent components (features)
to be automatically inferred from the data in an unsupervised manner. This is a
significant disadvantage of the state-of-the-art, as it results in considerable
burden imposed on researchers and practitioners, who must resort to tedious
cross-validation procedures to obtain the optimal number of latent features. To
resolve these issues, in this paper we introduce a convolutional nonparametric
Bayesian sparse ICA architecture for overcomplete feature learning from
high-dimensional data. Our method utilizes an Indian buffet process prior to
facilitate inference of the appropriate number of latent features under a
hybrid variational inference algorithm, scalable to massive datasets. As we
show, our model can be naturally used to obtain deep unsupervised hierarchical
feature extractors, by greedily stacking successive model layers, similar to
existing approaches. In addition, inference for this model is completely
heuristics-free; thus, it obviates the need of tedious parameter tuning, which
is a major challenge most deep learning approaches are faced with. We evaluate
our method on several action recognition benchmarks, and exhibit its advantages
over the state-of-the-art. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Nov 2014  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Sotirios_Chatzis/publication/268451692_A_Nonparametric_Bayesian_Approach_Toward_Stacked_Convolutional_Independent_Component_Analysis/links/54dfb7420cf29666378be5e1.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw36_56ab1b7347e32" >  <div class="indent-left">  <div id="rgw37_56ab1b7347e32" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/236687792_Stochastic_Collapsed_Variational_Bayesian_Inference_for_Latent_Dirichlet_Allocation">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fexport.arxiv.org%2Fpdf%2F1305.2452" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: export.arxiv.org </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw38_56ab1b7347e32">  <li class="citation-context-item"> "For variational inference , perhaps the most important advantage of the collapsed representation is that the variational bound is strictly better than for the uncollapsed representation, leading to the potential to learn more accurate topic models [24]. The existing online inference algorithms for LDA do not fully take advantage of the collapsed representation – although the sparse online LDA algorithm of Mimno et al. [16] collapses out per-document parameters θ, the topics themselves are not collapsed so there is no improvement in the variational bound. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/236687792_Stochastic_Collapsed_Variational_Bayesian_Inference_for_Latent_Dirichlet_Allocation"> <span class="publication-title js-publication-title">Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet Allocation</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/69562933_James_Foulds" class="authors js-author-name ga-publications-authors">James Foulds</a> &middot;     <a href="researcher/69897643_Levi_Boyles" class="authors js-author-name ga-publications-authors">Levi Boyles</a> &middot;     <a href="researcher/70373032_Christopher_Dubois" class="authors js-author-name ga-publications-authors">Christopher Dubois</a> &middot;     <a href="researcher/66646470_Padhraic_Smyth" class="authors js-author-name ga-publications-authors">Padhraic Smyth</a> &middot;     <a href="researcher/69847505_Max_Welling" class="authors js-author-name ga-publications-authors">Max Welling</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> In the internet era there has been an explosion in the amount of digital text
information available, leading to difficulties of scale for traditional
inference algorithms for topic models. Recent advances in stochastic
variational inference algorithms for latent Dirichlet allocation (LDA) have
made it feasible to learn topic models on large-scale corpora, but these
methods do not currently take full advantage of the collapsed representation of
the model. We propose a stochastic algorithm for collapsed variational Bayesian
inference for LDA, which is simpler and more efficient than the state of the
art method. We show connections between collapsed variational Bayesian
inference and MAP estimation for LDA, and leverage these connections to prove
convergence properties of the proposed algorithm. In experiments on large-scale
text corpora, the algorithm was found to converge faster and often to a better
solution than the previous method. Human-subject experiments also demonstrated
that the method can learn coherent topics in seconds on small corpora,
facilitating the use of topic models in interactive document analysis software. </span> </div>    <div class="publication-meta publication-meta">  <span class="ico-publication-preview reset-background"></span> Preview    &middot; Article &middot; May 2013  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw23_56ab1b7347e32" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw24_56ab1b7347e32">  </ul> </div> </div>   <div id="rgw15_56ab1b7347e32" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw16_56ab1b7347e32"> <div> <h5> <a href="publication/236687792_Stochastic_Collapsed_Variational_Bayesian_Inference_for_Latent_Dirichlet_Allocation" class="color-inherit ga-similar-publication-title"><span class="publication-title">Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet Allocation</span></a>  </h5>  <div class="authors"> <a href="researcher/69562933_James_Foulds" class="authors ga-similar-publication-author">James Foulds</a>, <a href="researcher/69897643_Levi_Boyles" class="authors ga-similar-publication-author">Levi Boyles</a>, <a href="researcher/70373032_Christopher_Dubois" class="authors ga-similar-publication-author">Christopher Dubois</a>, <a href="researcher/66646470_Padhraic_Smyth" class="authors ga-similar-publication-author">Padhraic Smyth</a>, <a href="researcher/69847505_Max_Welling" class="authors ga-similar-publication-author">Max Welling</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56ab1b7347e32"> <div> <h5> <a href="publication/221620432_Parallel_Inference_for_Latent_Dirichlet_Allocation_on_Graphics_Processing_Units" class="color-inherit ga-similar-publication-title"><span class="publication-title">Parallel Inference for Latent Dirichlet Allocation on Graphics Processing Units.</span></a>  </h5>  <div class="authors"> <a href="researcher/70220978_Feng_Yan" class="authors ga-similar-publication-author">Feng Yan</a>, <a href="researcher/12303177_Ningyi_Xu" class="authors ga-similar-publication-author">Ningyi Xu</a>, <a href="researcher/2042684600_Yuan_Qi" class="authors ga-similar-publication-author">Yuan Qi</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56ab1b7347e32"> <div> <h5> <a href="publication/221619731_Distributed_Inference_for_Latent_Dirichlet_Allocation" class="color-inherit ga-similar-publication-title"><span class="publication-title">Distributed Inference for Latent Dirichlet Allocation.</span></a>  </h5>  <div class="authors"> <a href="researcher/61748486_David_Newman" class="authors ga-similar-publication-author">David Newman</a>, <a href="researcher/69884878_Arthur_U_Asuncion" class="authors ga-similar-publication-author">Arthur U. Asuncion</a>, <a href="researcher/66646470_Padhraic_Smyth" class="authors ga-similar-publication-author">Padhraic Smyth</a>, <a href="researcher/69847505_Max_Welling" class="authors ga-similar-publication-author">Max Welling</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw40_56ab1b7347e32" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw41_56ab1b7347e32">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw42_56ab1b7347e32" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=WidOUoJdjaHp4FgSZgc1yVcOfBVX1wob1sfyJloDe37oBWDfBxPRtjuclpTukNxu" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="6kJ9MbCuTUca96w1K2bGB6PwxOByq0nWelHcmxvxS7hC2oS7XvGccw98tMBSrRWqUVa6HRFZb+/c5YJxaZMG8ygrV8YnUcDBL1Rmo7sPC1SA2jHPorupWzuMyJQxnIoDxeeazIqU+RHzeYGQP7iX0266/m5QeUkR2TSUzvOSEuCFDEbxYvAGU7W7LseLW92NWqNl1fm8yfD+utIz6C4xD9Dq1reakMi948xSsfdcdUXMPh7fTgm1l9pF7jDRBTg6ZLw3fDifRivLs90qARyx/f5x+ct/X9bSik5v6oTv1Yg="/> <input type="hidden" name="urlAfterLogin" value="publication/228095627_Sparse_Stochastic_Inference_for_Latent_Dirichlet_allocation"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjI4MDk1NjI3X1NwYXJzZV9TdG9jaGFzdGljX0luZmVyZW5jZV9mb3JfTGF0ZW50X0RpcmljaGxldF9hbGxvY2F0aW9u"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjI4MDk1NjI3X1NwYXJzZV9TdG9jaGFzdGljX0luZmVyZW5jZV9mb3JfTGF0ZW50X0RpcmljaGxldF9hbGxvY2F0aW9u"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjI4MDk1NjI3X1NwYXJzZV9TdG9jaGFzdGljX0luZmVyZW5jZV9mb3JfTGF0ZW50X0RpcmljaGxldF9hbGxvY2F0aW9u"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw43_56ab1b7347e32"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 704;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"David Mimno","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278460808351751%401443401638524_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/David_Mimno","institution":"Cornell University","institutionUrl":false,"widgetId":"rgw4_56ab1b7347e32"},"id":"rgw4_56ab1b7347e32","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=4733989","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab1b7347e32"},"id":"rgw3_56ab1b7347e32","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=228095627","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":228095627,"title":"Sparse Stochastic Inference for Latent Dirichlet allocation","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"Proceedings of the 29th International Conference on Machine Learning, ICML 2012","publicationDate":"06\/2012;","publicationDateRobot":"2012-06","article":"2."}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1206.6425","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Sparse Stochastic Inference for Latent Dirichlet allocation"},{"key":"rft.title","value":"Proceedings of the 29th International Conference on Machine Learning, ICML 2012"},{"key":"rft.jtitle","value":"Proceedings of the 29th International Conference on Machine Learning, ICML 2012"},{"key":"rft.volume","value":"2"},{"key":"rft.date","value":"2012"},{"key":"rft.au","value":"David Mimno,Matt Hoffman,David Blei"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56ab1b7347e32"},"id":"rgw6_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=228095627","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":228095627,"peopleItems":[{"data":{"authorNameOnPublication":"David Mimno","accountUrl":"profile\/David_Mimno","accountKey":"David_Mimno","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278460808351751%401443401638524_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"David Mimno","profile":{"professionalInstitution":{"professionalInstitutionName":"Cornell University","professionalInstitutionUrl":"institution\/Cornell_University"}},"professionalInstitutionName":"Cornell University","professionalInstitutionUrl":"institution\/Cornell_University","url":"profile\/David_Mimno","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278460808351751%401443401638524_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"David_Mimno","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw9_56ab1b7347e32"},"id":"rgw9_56ab1b7347e32","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=4733989&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Cornell University","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":1,"publicationUid":228095627,"widgetId":"rgw8_56ab1b7347e32"},"id":"rgw8_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=4733989&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=1&publicationUid=228095627","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/79325033_Matt_Hoffman","authorNameOnPublication":"Matt Hoffman","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Matt Hoffman","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/79325033_Matt_Hoffman","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw11_56ab1b7347e32"},"id":"rgw11_56ab1b7347e32","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=79325033&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw10_56ab1b7347e32"},"id":"rgw10_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=79325033&authorNameOnPublication=Matt%20Hoffman","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/2064238818_David_Blei","authorNameOnPublication":"David Blei","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"David Blei","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2064238818_David_Blei","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw13_56ab1b7347e32"},"id":"rgw13_56ab1b7347e32","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2064238818&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw12_56ab1b7347e32"},"id":"rgw12_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2064238818&authorNameOnPublication=David%20Blei","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab1b7347e32"},"id":"rgw7_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=228095627&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":228095627,"abstract":"<noscript><\/noscript><div>We present a hybrid algorithm for Bayesian topic models that combines the<br \/>\nefficiency of sparse Gibbs sampling with the scalability of online stochastic<br \/>\ninference. We used our algorithm to analyze a corpus of 1.2 million books (33<br \/>\nbillion words) with thousands of topics. Our approach reduces the bias of<br \/>\nvariational inference and generalizes to many Bayesian hidden-variable models.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw14_56ab1b7347e32"},"id":"rgw14_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=228095627","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/228095627_Sparse_Stochastic_Inference_for_Latent_Dirichlet_allocation\/links\/02b045870cf27908e9dda48a\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw5_56ab1b7347e32"},"id":"rgw5_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=228095627&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":69562933,"url":"researcher\/69562933_James_Foulds","fullname":"James Foulds","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69897643,"url":"researcher\/69897643_Levi_Boyles","fullname":"Levi Boyles","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70373032,"url":"researcher\/70373032_Christopher_Dubois","fullname":"Christopher Dubois","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":66646470,"url":"researcher\/66646470_Padhraic_Smyth","fullname":"Padhraic Smyth","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"May 2013","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/236687792_Stochastic_Collapsed_Variational_Bayesian_Inference_for_Latent_Dirichlet_Allocation","usePlainButton":true,"publicationUid":236687792,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/236687792_Stochastic_Collapsed_Variational_Bayesian_Inference_for_Latent_Dirichlet_Allocation","title":"Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet Allocation","displayTitleAsLink":true,"authors":[{"id":69562933,"url":"researcher\/69562933_James_Foulds","fullname":"James Foulds","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69897643,"url":"researcher\/69897643_Levi_Boyles","fullname":"Levi Boyles","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70373032,"url":"researcher\/70373032_Christopher_Dubois","fullname":"Christopher Dubois","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":66646470,"url":"researcher\/66646470_Padhraic_Smyth","fullname":"Padhraic Smyth","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69847505,"url":"researcher\/69847505_Max_Welling","fullname":"Max Welling","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/236687792_Stochastic_Collapsed_Variational_Bayesian_Inference_for_Latent_Dirichlet_Allocation","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/236687792_Stochastic_Collapsed_Variational_Bayesian_Inference_for_Latent_Dirichlet_Allocation\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56ab1b7347e32"},"id":"rgw16_56ab1b7347e32","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=236687792","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":70220978,"url":"researcher\/70220978_Feng_Yan","fullname":"Feng Yan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":12303177,"url":"researcher\/12303177_Ningyi_Xu","fullname":"Ningyi Xu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2042684600,"url":"researcher\/2042684600_Yuan_Qi","fullname":"Yuan Qi","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Conference Paper","publicationDate":"Jan 2009","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/221620432_Parallel_Inference_for_Latent_Dirichlet_Allocation_on_Graphics_Processing_Units","usePlainButton":true,"publicationUid":221620432,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/221620432_Parallel_Inference_for_Latent_Dirichlet_Allocation_on_Graphics_Processing_Units","title":"Parallel Inference for Latent Dirichlet Allocation on Graphics Processing Units.","displayTitleAsLink":true,"authors":[{"id":70220978,"url":"researcher\/70220978_Feng_Yan","fullname":"Feng Yan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":12303177,"url":"researcher\/12303177_Ningyi_Xu","fullname":"Ningyi Xu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2042684600,"url":"researcher\/2042684600_Yuan_Qi","fullname":"Yuan Qi","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009. Proceedings of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada.; 01\/2009"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/221620432_Parallel_Inference_for_Latent_Dirichlet_Allocation_on_Graphics_Processing_Units","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/221620432_Parallel_Inference_for_Latent_Dirichlet_Allocation_on_Graphics_Processing_Units\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab1b7347e32"},"id":"rgw17_56ab1b7347e32","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=221620432","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":61748486,"url":"researcher\/61748486_David_Newman","fullname":"David Newman","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69884878,"url":"researcher\/69884878_Arthur_U_Asuncion","fullname":"Arthur U. Asuncion","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":66646470,"url":"researcher\/66646470_Padhraic_Smyth","fullname":"Padhraic Smyth","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69847505,"url":"researcher\/69847505_Max_Welling","fullname":"Max Welling","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Conference Paper","publicationDate":"Jan 2007","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/221619731_Distributed_Inference_for_Latent_Dirichlet_Allocation","usePlainButton":true,"publicationUid":221619731,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/221619731_Distributed_Inference_for_Latent_Dirichlet_Allocation","title":"Distributed Inference for Latent Dirichlet Allocation.","displayTitleAsLink":true,"authors":[{"id":61748486,"url":"researcher\/61748486_David_Newman","fullname":"David Newman","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69884878,"url":"researcher\/69884878_Arthur_U_Asuncion","fullname":"Arthur U. Asuncion","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":66646470,"url":"researcher\/66646470_Padhraic_Smyth","fullname":"Padhraic Smyth","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69847505,"url":"researcher\/69847505_Max_Welling","fullname":"Max Welling","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Advances in Neural Information Processing Systems 20, Proceedings of the Twenty-First Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 3-6, 2007; 01\/2007"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/221619731_Distributed_Inference_for_Latent_Dirichlet_Allocation","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/221619731_Distributed_Inference_for_Latent_Dirichlet_Allocation\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab1b7347e32"},"id":"rgw18_56ab1b7347e32","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=221619731","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw15_56ab1b7347e32"},"id":"rgw15_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=228095627&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":228095627,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":228095627,"publicationType":"article","linkId":"02b045870cf27908e9dda48a","fileName":"Sparse Stochastic Inference for Latent Dirichlet allocation","fileUrl":"http:\/\/arxiv.org\/pdf\/1206.6425","name":"arxiv.org","nameUrl":"http:\/\/arxiv.org\/pdf\/1206.6425","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw21_56ab1b7347e32"},"id":"rgw21_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=228095627&linkId=02b045870cf27908e9dda48a&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw20_56ab1b7347e32"},"id":"rgw20_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=228095627&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":2,"valueFormatted":"2","widgetId":"rgw22_56ab1b7347e32"},"id":"rgw22_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=228095627","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw19_56ab1b7347e32"},"id":"rgw19_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=228095627&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":228095627,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw24_56ab1b7347e32"},"id":"rgw24_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=228095627&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":2,"valueFormatted":"2","widgetId":"rgw25_56ab1b7347e32"},"id":"rgw25_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=228095627","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw23_56ab1b7347e32"},"id":"rgw23_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=228095627&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Sparse stochastic inference for latent Dirichlet allocation\nDavid Mimno\nPrinceton U., Dept. of Computer Science, 35 Olden St., Princeton, NJ 08540, USA\nmimno@cs.princeton.edu\nMatthew D. Hoffman\nColumbia U., Dept. of Statistics, Room 1005 SSW, MC 4690 1255 Amsterdam Ave. New York, NY 10027\nmdhoffma@cs.princeton.edu\nDavid M. Blei\nPrinceton U., Dept. of Computer Science, 35 Olden St., Princeton, NJ 08540, USA\nblei@cs.princeton.edu\nAbstract\nWe present a hybrid algorithm for Bayesian\ntopic models that combines the efficiency of\nsparse Gibbs sampling with the scalability of\nonline stochastic inference. We used our algo-\nrithm to analyze a corpus of 1.2 million books\n(33 billion words) with thousands of topics.\nOur approach reduces the bias of variational\ninference and generalizes to many Bayesian\nhidden-variable models.\n1. Introduction\nTopic models are hierarchical Bayesian models of doc-\nument collections (Blei et al., 2003). They can uncover\nthe main themes that pervade a corpus and then use\nthose themes to help organize, search, and explore the\ndocuments. In topic modeling, a \u201ctopic\u201d is a distri-\nbution over a fixed vocabulary and each document ex-\nhibits the topics with different proportions. Both the\ntopics and the topic proportions of documents are hid-\nden variables. Inferring the conditional distribution of\nthese variables given an observed set of documents is\nthe central computational problem.\nIn this paper, we develop a posterior inference method\nfor topic modeling that can find large numbers of top-\nics in massive collections of documents. We demon-\nstrate our approach by analyzing a collection of 1.2\nmillion out-of-copyright books, comprising 33 billion\nobserved words. Using our algorithm, we fit a topic\nmodel to this corpus with thousands of topics. We il-\nlustrate the most frequent words from several of these\ntopics in Table 1.\nAppearing in Proceedings of the 29thInternational Confer-\nence on Machine Learning, Edinburgh, Scotland, UK, 2012.\nCopyright 2012 by the author(s)\/owner(s).\nOur algorithm builds on variational inference (Jor-\ndan et al., 1999). In variational inference, we define\na parameterized family of distributions over the hid-\nden structure\u2014in this case topics and document-topic\nproportions\u2014and then optimize the parameters to find\na member of the family that is close to the poste-\nrior. Traditional variational inference for topic model-\ning uses coordinate ascent. The algorithm alternates\nbetween estimating document-topic proportions under\nthe current settings of the topics and re-estimating the\ntopics based on the estimated document proportions.\nThis requires multiple passes through an entire collec-\ntion, which is not practical when working with very\nlarge corpora.\nTable 1. Randomly selected topics from a 2000-topic model\ntrained on a library of 1.2 million out-of-copyright books.\nkilled wounded sword slain arms military rifle wounds loss\nhuman Plato Socrates universe philosophical minds ethics\ninflammation affected abdomen ulcer circulation heart\nships fleet sea shore Admiral vessels land boats admiral\nsister child tears pleasure daughters loves wont sigh warm\nsentence clause syllable singular examples clauses syllables\nprovinces princes nations imperial possessions invasion\nwomen Quebec Women Iroquois husbands thirty whom\nsteam engines power piston boilers plant supplied chimney\nlines points direction planes Lines scale sections extending\nRecently, Hoffman et al. (2010) introduced Online\nLDA, a stochastic gradient optimization algorithm for\ntopic modeling. The algorithm repeatedly subsamples\na small set of documents from the collection and then\nupdates the topics from an analysis of the subsam-\nple. This method uses less memory than the standard\napproach because we do not need to store topic pro-\nportions for the full corpus. It also converges faster\nbecause we update topics more frequently. However,\nwhile it handles large corpora it does not scale to large"},{"page":2,"text":"Sparse stochastic inference for latent Dirichlet allocation\nnumbers of topics.\nOur algorithm builds on this method by using sam-\npling to introduce a second source of stochasticity into\nthe gradient. This approach lets us take advantage of\nsparse computation, scaling sublinearly with the num-\nber of topics. Using this algorithm, we can fit topic\nmodels to large collections with many topics.\n2. Hybrid stochastic-MCMC inference\nWe model each of the D documents in a corpus as a\nmixture of K topics. This topic model can be divided\ninto corpus-level global variables and document-level\nlocal variables. The global variables are K topic-word\ndistributions \u03b21,...,\u03b2Kover the V -dimensional vocab-\nulary, each drawn from a Dirichlet prior with param-\neter \u03b7. For a document d of length Nd, the local vari-\nables are (a) a distribution over topics \u03b8ddrawn from\na Dirichlet prior with parameter \u03b1 and (b) Ndtoken-\ntopic indicator variables zd1,...,zdNddrawn from \u03b8d.\nOur goal is to estimate the posterior distribution of the\nhidden variables given an observed corpus. We will use\nvariational inference. Unlike standard mean-field vari-\national inference, but similar to Griffiths & Steyvers\n(2004) and Teh et al. (2006), we will marginalize out\nthe topic proportions \u03b8d. Thus we need to approxi-\nmate the posterior over the topic assignments zdand\nthe topics \u03b21:K.\nWe will use a variational distribution of the form\nq(z1,...,zD,\u03b21,...,\u03b2K) =?\nThis factorization differs from the usual mean-field\nfamily for topic models. Rather than defining a distri-\nbution that factorizes over individual tokens, we treat\neach document\u2019s sequence of topic indicator variables\nzd as a unit. As a result q(zd) will be a single dis-\ntribution over the KNdpossible topic configurations,\nrather than a product of Nd distributions, each over\nK possible values.\ndq(zd)?\nkq(\u03b2k).(1)\nWe now derive an algorithm that uses Gibbs sampling\nto estimate variational expectations of the local vari-\nables and a stochastic natural gradient step to update\nthe variational distribution of global variables. A lower\nbound on the marginal log probability of the observed\nwords given the hyperparameters is\nlogp(w|\u03b1,\u03b7) \u2265\n?\n+\nd\nEqlog\n?\np(zd|\u03b1)\n?\ni\n\u03b2zdiwdi\n?\n(2)\n?\nk\nEqlogp(\u03b2k|\u03b7) + H(q),\nwhere H(q) denotes the entropy of q.\nFollowing Bishop (2006), the optimal variational dis-\ntribution over topic configurations for a document,\nholding all other variational distributions fixed, is\nq?(zd) \u221d exp{Eq(\u00aczd)[logp(zd|\u03b1)p(wd|zd,\u03b2)]}\n=\n\u0393(K\u03b1 + Nd)\n?\nwhere Ia=b is 1 if a = b and 0 otherwise, and \u00aczd\ndenotes the set of all unobserved variables besides zd.\nWe can compute Eq. 4 for a specific topic configura-\ntion zd, but we cannot tractably normalize it to get\nthe distribution q?(zd) over all KNdconfigurations.\n(3)\n\u0393(K\u03b1)\n?\nk\n\u0393(\u03b1 +?\niIzdi=k)\n\u0393(\u03b1)\n(4)\n\u00d7\ni\nexpEq[log\u03b2zdiwdi]\nThe optimal variational distribution over topic-word\ndistributions, holding the other distributions fixed, is\nthe kernel of a Dirichlet distribution with parameters\n?\nThis expression includes the expectation under q of\nthe number of tokens of type w assigned to topic k.\nComputing this expectation would require evaluating\nthe intractable distribution q?(zd).\n\u03bbkw= \u03b7 +\nd\n?\ni\nEq[Izdi=kIwdi=w]. (5)\n2.1. Online stochastic inference\nWe optimize the variational topic-word parameters\n\u03bbkwusing stochastic gradient ascent. Stochastic gra-\ndient ascent iteratively updates parameters with noisy\nestimates of the gradient. We obtain these noisy esti-\nmates by subsampling the data (Sato, 2001; Hoffman\net al., 2010).\nWe first recast the variational objective in Eq. 2 as\na summation over per-document terms ?d, so that the\nfull gradient with respect to \u03bbkis the sum?\nfull gradient by sampling a minibatch of documents\nB and then scaling the sum of the document-specific\ngradients to match the total size of the corpus,\n?\nd\n\u2202\n\u2202\u03bbk?d.\nWe can then generate a noisy approximation to this\n?\nd\n\u2202\n\u2202\u03bbk?d= E\nD\n|B|\n?\nd\u2208B\n\u2202\n\u2202\u03bbk?d\n?\n. (6)\n(The expectation is with respect to the random sample\nB.) Pushing the per-topic terms in Eq. 2 inside the\nsummation over documents and removing terms not\ninvolving \u03bbkwwe obtain\n?\n+1\nD\n?d=\nw\n?\nEq[Ndkw] +1\n?\nD(\u03b7 \u2212 \u03bbkw)\n?\n?\nEq[log\u03b2kw] (7)\nlog\u0393(?\nw\u03bbkw) \u2212\nw\nlog\u0393(\u03bbkw)\n?"},{"page":3,"text":"Sparse stochastic inference for latent Dirichlet allocation\nAlgorithm\nvariational-Gibbs inference.\nfor t \u2208 1,...,\u221e do\n\u03c1t\u2190\nsample minibatch B\nfor d \u2208 B do\ninitialize z0\ndiscard B burn-in sweeps\nfor sample s \u2208 1,...,S do\nfor token i \u2208 1,...,Nddo\nsample zs\nend for\nend for\nend for\n\u03bbt\nend for\n1 Algorithmfor hybrid stochastic\n?\n1\nt0+t\n?\u03ba\nd\ndi\u221d (\u03b1 + Ndk)eEq[log \u03b2kw]\nkw\u2190 (1 \u2212 \u03c1t)\u03bbt\u22121\nkw+ \u03c1t\n?\n\u03b7 +\nD\n|B|\u02c6 Nkw\n?\nwhere Eq[Ndkw] =?\ncan be factored into the product of a matrix and a\nvector. The matrix, which contains derivatives of the\ndigamma function, is the Fisher information matrix\nfor the topic parameters. Element w of the vector is\niEq[Izdi=kIwdi=w]. The gradient\nof Eq. 7 with respect to the parameters \u03bbk1,...,\u03bbkV\nEq[Ndkw] +1\nD(\u03b7 \u2212 \u03bbkw). (8)\nPremultiplying the gradient of an objective function by\nthe inverse Fisher information matrix of the distribu-\ntion being optimized (in our case the variational distri-\nbution q) results in the natural gradient (Sato, 2001).\nSince our gradient is the product of the Fisher informa-\ntion matrix and a vector, the natural gradient is there-\nfore simply Eq. 8 (Hoffman et al., 2010). Compared to\nthe standard Euclidean gradient, the natural gradient\noffers both faster convergence (because it takes into\naccount the information geometry of the variational\ndistribution) and cheaper computation (because the\nvector in Eq. 8 is a simple linear function).\n2.2. MCMC within stochastic inference\nWe cannot evaluate the expectation in Eq. 8 because\nwe would have to consider a combinatorial number of\ntopic configurations zd. To use stochastic gradient as-\ncent, however, we only need an approximation to this\nexpectation. We use Markov chain Monte Carlo to\nsample topic configurations from q?(zd). We then use\nthe empirical average of these samples to estimate the\nexpectations needed for Eq. 8.\nGibbs sampling for a document starts with a random\ninitialization of the topic indicator variables zd. We\nthen iteratively resample the topic indicator at each\nposition from the conditional distribution over that\nposition given the remaining topic indicator variables:\nq?(zdi= k|z\\i) \u221d (\u03b1 +?\nj?=iIzj=k)exp{Eq[log\u03b2kwdi]},\n(9)\nwhere the expectation of the log probability of word\nw given a topic k is \u03a8(\u03bbkw) \u2212 \u03a8(?\nurations. Once we have saved S samples {z}1,...,S, we\ncan define approximate sufficient statistics\nw?\u03bbkw?). After B\nburn-in sweeps, we begin saving sampled topic config-\nEq[Ndkw] \u2248\u02c6 Nkw=1\nS\n?\ns\n?\nd\u2208B\n?\ni\nIzs\ndi=kIwdi=w. (10)\nUsing MCMC estimates adds noise to our gradient,\nbut allows us to use a collapsed objective function that\ndoes not represent document-topic proportions \u03b8d. In\naddition, an average over a finite set of samples pro-\nvides a sparse estimate of the gradient: for many words\nand topics, our estimate of Eq[Ndkw] will be zero.\n2.3. Algorithm\nWe have defined a natural gradient and a method for\napproximating the sufficient statistics of that gradient.\nFor a sequence of learning rates \u03c1t= (t0+ t)\u2212\u03ba, the\nfollowing update will lead to a stationary point:\n?\n?\n\u03bbt\nkw\u2190 \u03bbt\u22121\nkw+ \u03c1t\nD\n|B|\n?\nd\u2208B\n\u02c6 Ndkw+1\nD(\u03b7 \u2212 \u03bbkw)\n?\n= (1 \u2212 \u03c1t)\u03bbt\u22121\nkw+ \u03c1t\n\u03b7 +D\n|B|\n?\nd\u2208B\n\u02c6 Ndkw\n?\n. (11)\nThis update results in Algorithm 1. Two implementa-\ntion details that result in sparse computations can be\nfound in Appendix A. This online algorithm has the\nimportant advantage over Online LDA of preserving\nsparsity in the topic-word parameters, so that \u03bbkw= \u03b7\nfor most values of k and w. Sparsity increases the effi-\nciency of updates to \u03bbkand of Gibbs sampling for zd.\nPrevious variational methods lead to dense updates to\nKV topic parameters, making them expensive to ap-\nply to large vocabularies and large numbers of topics.\nOur method, in contrast, is able to exploit the sparsity\nexhibited by samples from the variational distribution\nq?, resulting in much more efficient updates.\n3. Related Work\nThis paper combines two sources of zero-mean noise in\nconstructing an approximate gradient for a variational\ninference algorithm: subsampling of data, and Monte\nCarlo inference. These sources of variance have been"},{"page":4,"text":"Sparse stochastic inference for latent Dirichlet allocation\nused individually in previous work. Stochastic approx-\nimation EM (SAEM, Delyon et al., 1999) combines an\nEM algorithm with a stochastic online inference proce-\ndure. SAEM does not subsample data, but rather in-\nterpolates between Monte Carlo estimates of the com-\nplete data. Kuhn & Lavielle (2004) extend SAEM to\nuse MCMC estimates. Similarly, online EM (Capp\u00b4 e &\nMoulines, 2009) sub-samples data but preserves stan-\ndard inference procedures for local variables.\nStandard collapsed Gibbs sampling uses multiple\nsweeps over the entire corpus, representing topic-word\ndistributions using the topic-word assignment vari-\nables of the entire corpus except for the current token.\nAs a result, topic assignment variables must in theory\nbe sampled sequentially, although parallel approxima-\ntions work well empirically (Asuncion et al., 2008). In\ncontrast, Algorithm 1 treats topic-word distributions\nas a global variable distinct from the local token-topic\nassignment variables, and so can parallelize trivially.\nIn this work we integrate over document-topic pro-\nportions \u03b8dwithin a variational algorithm. Collapsed\nvariational inference (Teh et al., 2006) also analytically\nmarginalizes over the topic proportions, but still main-\ntains a fully factorized distribution over topic assign-\nments at each position. The method described here\ndoes not restrict itself to such factored distributions,\nand therefore reduces bias, but this reduction may be\noffset by the bias we introduce when we initialize the\nGibbs chain.\n4. Empirical Results\nIn this section we compare the sampled online algo-\nrithm to two related online methods and measure the\neffect of model parameters. We use a selection of met-\nrics to evaluate models.\n4.1. Evaluation\nHeld-out probability.\nthe semantic structure of a corpus should place more\nof its probability mass on sensible documents than\non random sequences of words. We can use this as-\nsumption to compare different models by asking each\nmodel to estimate the probability of a previously un-\nseen document. A better model should, on average,\nassign higher probability to real documents than a\nlower-quality model. We evaluate held-out probabil-\nity using the left-to-right sequential sampling method\n(Wallach et al., 2009; Buntine, 2009). For each trained\nmodel we generate point estimates of the topic-word\nprobabilities \u02dc p(w|k). We then process each document\nby iterating through the tokens w1,...,wNd. At each\nA model that characterizes\nposition i we calculate the marginal probability\n?\nWe then sample ziproportional to the terms of that\nsummation and continue to the next token.1In order\nto normalize for document lengths, we divide the sum\nof the logs of the marginal probabilities by Nd.\n\u02dc p(wi|w<i) =\nk\np(zi= k|w<i,z<i,\u03b1)\u02dc p(wi|k).(12)\nCoherence.\nquality of a topic by approximating the experience of\na user viewing the W most probable words for the\ntopic (Mimno et al., 2011). It is related to point-wise\nmutual information (Newman et al., 2010). Let D(w)\nbe the document frequencies for each word w, that is,\nthe number of documents containing one or more to-\nkens of type w, and let D(w1,w2) be the number of\ndocuments containing at least one token of w1and of\nw2. For each pair of words w1,w2in the top W list,\nwe calculate the number of documents that contain at\nleast one token of the higher ranked word w1that also\ncontain at least one token of the lower ranked word\nw2:\n?\nwhere ? is a small constant used to avoid log zero.\nValues closer to zero indicate greater co-occurrence.\nUnlike held-out probability, which reports scores for\nheld-out documents, coherence reports scores for indi-\nvidual topics.\nThis metric measures the semantic\nC(W) =\ni\n?\nj<i\nlogD(wi,wj) + ?\nD(wj)\n(13)\nWallclock time.\nas efficiently as possible. In addition to model qual-\nity metrics, we are therefore also interested in total\ncomputation time.\nOur goal is to train useful models\n4.2. Comparison to Online VB\nOur first corpus consists of 350,000 research articles\nfrom three major journals: Science, Nature, and the\nProceedings of the National Academy of Sciences of\nthe USA. We use a vocabulary with 19,000 distinct\nwords, including selected multi-word terms. We train\nmodels on 90% of the Science\/Nature\/PNAS corpus,\nholding out the remaining documents for testing pur-\nposes. We save topic-word parameters\nepochs consisting of 500,000 documents.\n\u02c6 Nkw after\nSampled online variational Bayesian inference com-\npares well in terms of wallclock time to standard online\nVB inference, particularly with respect to the number\nof topics K. Figure 1 shows results comparing stan-\ndard online VB inference to sampled online inference\n1We set the document-topic hyperparameter \u03b1 = 0.1."},{"page":5,"text":"Sparse stochastic inference for latent Dirichlet allocation\nTopics\nSec\n10\n20\n30\n40\nG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G\nG G G G G\nG\nG G G GGG G G G GG G G GG\nG G G G G G G G G G G G G GGG G G GGG G G\nGG G G G\nG G G G\nG GG G\nG\nGG GG G\nG\nG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G GG G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G G\nG G G G G G G G G G G G G G G G G G G G G G G\nG G G G G G G G G G G G G G G G G G G G G G G\nG\nG\nG G G G G G G G G\nG\nG G G G G G G G G G G G G G G G G G G G G G G G\nG G G G G G G G G G G G G G G G G G G G G G G G G\nGG G G GG G G G GG G GG GG G G G G G G G\nG G G G G G G G G G G G G G G G GG G G G G G\nG G G\nG\nG G G G G G G G G G G G G G G G G G G GG\nG\nG\nG\nG\nG G G G G G G G G G G G G G\nG G G G G G G G G G G G G G G G G G G G G G G G G\nG GG G G G G G G G G G G G G G G G G G G G G G G G\nG G G G G G G G G G G G G G G G G G G G G G G G GG G\n2004006008001000\nMethod\nG\nVB\nG\nSampled\nFigure 1. Comparison of seconds per mini-batch between\nonline variational Bayes (Hoffman et al., 2010) and sampled\nonline inference (this paper). Online VB is linear in K,\nwhile sampled inference takes advantage of sparsity.\nfor K up to 1000. Each iteration consists of a mini-\nbatch of 100 documents.\ntakes time linear in K, while wallclock time for sam-\npled online inference grows more slowly.\nStandard online inference\nWe would like to know if there is a difference in the\nquality of models trained through the hybrid sampled\nvariational algorithm and the online LDA algorithm.\nWe compare an implementation of Online LDA that\ntries to be as close as possible to the sampled online\nimplementation, but using a dense VB update instead\nof a sparse sampled update for the local variables. In\nparticular, the number of coordinate ascent steps in\nstandard VB is equal to the number of Gibbs sweeps\nin the sampled algorithm.\nPer-topic coherence for K = 200 is shown in Fig-\nure 2. Sampled online inference produces fewer very\npoor topics. This difference is significant under a two-\nsample t-test (p < 0.001) and does not decrease with\nadditional training epochs. Sampled online inference\nalso assigns greater held-out probability than Online\nLDA for every test document, by a wide margin. We\nevaluated several possible reasons for this difference in\nperformance. Held-out probability estimation can be\naffected by evaluation-time smoothing parameter set-\ntings, but we found both models were affected equally.\nThe log probability of a document is the sum of the\nlog probabilities of its words.\none model assigned very small probability to a hand-\nful of tokens, those words could significantly affect the\noverall score, but the difference in log probability was\nconsistent across many tokens. The scale of parame-\nters might not be comparable, but as both methods\nuse the same learning schedule, the sum of the trained\nparameters \u03bbkwis nearly identical.\nIt is possible that if\nThe main difference appears to be the entropy of the\nCoherence\nAlgorithm\nSampOnline\nVB\nGGG\nGGGGGGGGGGG G GGGGGG\n\u22121200 \u22121000\u2212800 \u2212600\u2212400\u2212200\nFigure 2. The new sampled online algorithm produces\nfewer low-quality topics than Online LDA at K = 200.\nHeldout log likelihood is much worse for Online LDA.\nCoherence\nAlgorithm\nSampOnline\nSMC\nGGGGG G GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG G GGGGGGGG G GGGGG\nG GGGGGG G G GGG G GGGGG G GGGGGGGG GGGGGGGGGGGGG GGGGGGGGGG GGGGGGGGGG\n\u22121200\u22121000\u2212800\u2212600 \u2212400\u2212200\nHeldOut\nAlgorithm\nSampOnline\nSMC\nG GGGGG GGGGG GGG G GGGGGGGG GGGGGGGGGGGGGG G GG GGG GGGGGGGGGGGGGGGGGGGGGGG GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG G GGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGGGG GGGGGGGGGGGGGGGG GGGGG GGGG GGGGGGGGGGGGGGGGGGGGGGGGG GGG GGGG GGGGGG GGGGGGGGGGGG GGGGGG GGGGGGGGGGGG GGGGGGGGGGGGG GGGGGGGG G GGGGGGG GGGGGGG GGGGGGGGGGGGGGGGGGGGGGGGG\n\u22129.5\u22129.0\u22128.5\u22128.0\u22127.5\u22127.0\nFigure 3. Sampled online inference performs better than\none pass of sequential Monte Carlo, after processing a com-\nparable number of documents with K = 200.\ntopic distributions: the sampled-online algorithm pro-\nduces less concentrated distributions (mean entropy\n6.8 \u00b1 0.46) than standard online LDA (mean entropy\n6.0\u00b10.58). This result could indicate that coordinate\nascent over the local variables for Online LDA is not\nconverging.\n4.3. Comparison to Sequential Monte Carlo\nSequential Monte Carlo is an online algorithm similar\nto Gibbs sampling in that it represents topics using\nsums over assignment variables (Ahmed et al., 2012).\nA Gibbs sampler starts with a random initialization\nfor all hidden variables and sweeps repeatedly over the\nentire data set, updating each variable given the cur-\nrent value of all other variables. SMC samples values\nfor hidden variables in sequential order, conditioning\nonly on previously-seen variables.\nkeep multiple sampling states or \u201cparticles\u201d, but this\nprocess adds both computation and significant book-\nkeeping complexity. Ahmed et al. (2012) use a single\nSMC state.\nIt is common to\nIn order to compare SMC to the sampled online al-\ngorithm, we ran 10 independent SMC samplers over\nthe Science\/Nature\/PNAS dataset, with documents"},{"page":6,"text":"Sparse stochastic inference for latent Dirichlet allocation\nordered randomly. We also ran 10 independent sam-\npled trainers, stopping after a number of documents\nhad been sampled equivalent to the size of the corpus.\nIn order to make the comparison more fair, we allowed\nthe SMC sampler to sweep through each document the\nsame number of times as the sampled online algorithm,\nbut only the final topic configuration of a document\nwas available to the subsequent documents.2Results\nfor K = 200 are shown in Figure 3. SMC has con-\nsistently worse per-topic coherence and per-document\nheld-out log probability.\nrithm in this paper differs from SMC in that the contri-\nbution of local token-topic assignment variables decays\naccording to the learning rate schedule, so that more\nrecently sampled documents can have greater weight\nthan earlier documents. This decay allows sampled on-\nline inference to \u201cforget\u201d its initial topics, unlike SMC,\nwhich weights all documents equally.\nThe sampled online algo-\n4.4. Effect of parameter settings\nNumber of samples.\nrithm we initialize3the topic indicator variables z for\na document and then perform several Gibbs sweeps.\nIn each sweep we resample the value of each topic in-\ndicator variable in turn. We introduce bias when we\ninitialize, so we discard B \u201cburn-in\u201d sweeps and use\nvalues of z saved after S additional sweeps to calculate\nthe gradient. Since performance is linear in the total\nnumber of sweeps B +S, we want to find the smallest\nnumber of sweeps that does not sacrifice performance.\nIn the inner loop of our algo-\nWe consider nine settings of the pair (B,S). Under\nthe first three settings we save one sweep and vary\nthe number of burn-in sweeps: (1,1), (2,1), (3,1). For\nthe second three settings we perform five sweeps, vary-\ning how many we discard: (2,3), (3,2), (4,1). The final\nthree settings fix B = S and consider larger total num-\nbers of sweeps: (5,5), (10,10), (20,20). We evaluate\neach setting after processing 500,000 documents.\nPerformance was similar across settings with the fol-\nlowing exceptions, which were significant at p < 0.001\nunder a two-sample t-test. The two-sweep setting (1,1)\nhad better topic coherence but worse held-out proba-\nbility than the all other settings. The (5,5) setting\nhad the best mean held-out probability, but it was\nnot significantly better than (10,10) and (20,20). The\n2Note that SMC has an advantage.\nonline algorithm we Gibbs sample each document within a\nmini-batch independently, while in SMC, documents \u201csee\u201d\nresults from all previous documents.\n3We initialize by sampling each token conditioned on\nthe topics of the previous tokens in the document:\np(zdi= k) \u221d (\u03b1 +?\nIn the sampled\nj<iIzdj=k)p(wdi|k).\nmany-sweep settings (5,5), (10,10), (20,20) had worse\ntopic coherence than the other settings, with many vis-\nibly low-quality topics. These results suggest that 3\u20135\nsweeps is sufficient.\nTopic-word smoothing.\ntion e\u03a8(x). This function approaches x \u22121\nlarge, but for values of x near 0, it is non-linear. For\nexample, e\u03a8(0.05)is 1034times greater than e\u03a8(0.01). If\nthe values of topic parameters are in this range, a mi-\nnuscule increase in the parameter for word w in topic\nk can cause a profound change in the sampling distri-\nbution for that word: all subsequent tokens of type w\nwill be assigned to topic k with probability near 1.0.\nEq. 9 involves the func-\n2as x gets\nIn general, the randomness introduced by sampling\ntopic assignments helps to avoid becoming trapped in\nlocal maxima. When parameters are near zero, how-\never, random decisions early in the inference process\nrisk becoming permanent.\ning parameter \u03b7 can push parameter values away\nfrom this explosive region. We measured coherence\nfor six settings of the topic-word hyperparameter \u03b7,\n{0.1,0.2,0.3,0.4,0.5,0.6}. At \u03b7 = 0.1, a common value\nfor batch variational inference, many topics are visi-\nbly nonsensical. Average coherence improves signifi-\ncantly for each increasing value of \u03b7 \u2208 {0.2,0.3,0.4}\n(p < 0.001). There is no significant difference in aver-\nage coherence for \u03b7 \u2208 {0.4,0.5,0.6}.\nThe topic-word smooth-\nForgetting factors.\nrate \u03c1t = (t0+ t)\u2212\u03baand its relation to the corpus\nsize D. We fix \u03ba = 0.6 and vary the offset param-\neter t0\u2208 {3000,15000,30000,150000,300000}, saving\ntopic parameters after five training epochs of 500,000\ndocuments each. There was no significant difference\nin average topic coherence.\nWe now consider the learning\nThe learning rate, however, is not the only factor that\ndetermines the magnitude of parameter updates. Eq.\n11 also includes the size of the corpus D. If the cor-\npus is larger, we will take larger steps, regardless of\nthe contents of the mini-batch. The offset parameter\nt0 had no significant effect on coherence for the full\ncorpus, but it may have an effect if we also vary the\ncorpus size.\nWe simulate different size corpora by subsampling the\nfull data set. Results are shown in Figure 4 for models\ntrained on one half, one quarter, and one eighth of the\ncorpus. Each corpus is a subset of the next larger cor-\npus. In the smallest corpus (12.5%), the model with\nt0= 300000 is significantly worse than other settings\n(p < 0.001). Otherwise, there is no significant differ-\nence in average topic coherence."},{"page":7,"text":"Sparse stochastic inference for latent Dirichlet allocation\nt0\nCoherence\n\u22121000\n\u2212800\n\u2212600\n\u2212400\n\u2212200\n 12.5\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG G\nG\n300015000 30000 150000 300000\n 25.0\nG\nG\nG\nG G\nG\nG\n300015000 30000 150000 300000\n 50.0\nG\nG\nG\nG\nG\nG\n300015000 30000 150000 300000\n100.0\nG\nG\n300015000 30000 150000 300000\nFigure 4. Topic quality is lowest for large values of t0, but only in small corpora. Panels represent the proportion of\ntraining data used. Each panel shows coherence values for five K = 100 topic models with varying learning rates.\n4.5. Scalability\nPre-1922 books.\nthe method, we modeled a collection of 1.2 million out-\nof-copyright books. Topic models are useful in char-\nacterizing the contents of the corpus and supporting\nbrowsing applications: even scanning titles for a col-\nlection of this size is impossible for one person. Pre-\nvious approaches to million-book digital libraries have\nfocused on keyword search and word frequency his-\ntograms (Michel et al., 2011). Such methods do not\naccount for variability in meaning or context. There is\nno guarantee that the words being counted match the\nmeaning assumed by the user. In contrast, an interface\nbased on a topic model could, for example, distinguish\nuses of the word \u201cstrain\u201d in immunology, mechanical\nengineering, and cookery.\nTo demonstrate the scalability of\nWe divide each book into 10-page sections, resulting in\n44 million \u201cdocuments\u201d with a vocabulary size of 216.\nWe trained models with K \u2208 {100,500,1000,2000}.\nRandomly selected example topics are shown in Ta-\nble 1, illustrating the average level of topic quality.\nModels are sparse: at K = 2000, less than 1% of the\n2000\u00b7216possible topic-word parameters are non-zero.\nThe algorithm scales well as K increases. The number\nof milliseconds taken to process a sequence of 10,000\ndocuments was similar for K = 1000 and 2000, despite\ndoubling the number of topics.\n5. Conclusions\nStochastic online inference allows us to scale topic\nmodeling to large document sets. Sparse Gibbs sam-\npling allows us to scale to large numbers of topics.\nThe algorithm presented in this paper combines the\nadvantages of these two methods. As a result, models\ncan be trained on vast, open-ended corpora without\nrequiring access to vast computer clusters. If parallel\narchitectures are available, we can trivially parallelize\ncomputation within each mini-batch. As this work is\nrelated to the Online LDA algorithm of Hoffman et al.\n(2010), extensions to that model are also applicable,\nsuch as adaptive scheduling algorithms (Wahabzada &\nKersting, 2011). The use of MCMC within stochastic\nvariational inference reduces one source of bias in es-\ntimating local variables. Although we have focused on\ntext analysis applications, this hybrid method gener-\nalizes to a broad class of Bayesian models.\nAcknowledgments\nJohn Langford, Iain Murray, Charles Sutton provided\nhelpful comments.Yahoo!\ncomputational resources. DM is supported by a CRA\nCI fellowship.MDH is supported by NSF ATM-\n0934516, DOE DE-SC0002099, and IES R305D100017.\nDMB is supported by ONR N00014-11-1-0651, NSF\nCAREER 0745520, AFOSR FA9550-09-1-0668, the\nAlfred P. Sloan foundation, and a grant from Google.\nand PICSciE provided\nReferences\nAhmed,\nNarayanamurthy, Shravan, and Smola, Alexander. Scal-\nable inference in latent variable models. In WSDM, 2012.\nAmr,Aly, Mohamed,Gonzalez, Joseph,\nAsuncion, Arthur, Smyth, Padhraic, and Welling, Max.\nAsynchronous distributed learning of topic models. In\nNIPS, 2008.\nBishop, Christopher M. Pattern Recognition and Machine\nLearning. Springer, 2006.\nBlei, David, Ng, Andrew, and Jordan, Michael. Latent\nDirichlet allocation. Journal of Machine Learning Re-\nsearch, 3:993\u20131022, January 2003.\nBuntine, Wray L. Estimating likelihoods for topic models.\nIn Asian Conference on Machine Learning, 2009.\nCapp\u00b4 e, Olivier and Moulines, Eric. Online EM algorithm\nfor latent data models. Journal of the Royal Statistical\nSociety Series B, 71(3):593\u2013613, 2009.\nDelyon, Bernard, Lavielle, Marc, and Moulines, Eric. Con-\nvergence of a stochastic approximation version of the\nEM algorithm. Annals of Statistics, 27(1):94\u2013128, 1999."},{"page":8,"text":"Sparse stochastic inference for latent Dirichlet allocation\nGriffiths, Thomas L. and Steyvers, Mark. Finding scientific\ntopics. PNAS, 101(suppl. 1):5228\u20135235, 2004.\nHoffman, Matthew, Blei, David, and Bach, Francis. Online\nlearning for latent dirichlet allocation. In NIPS, 2010.\nJordan, Michael, Ghahramani, Zoubin, Jaakkola, Tommi,\nand Saul, Laurence. Introduction to variational methods\nfor graphical models.Machine Learning, 37:183\u2013233,\n1999.\nKuhn, Estelle and Lavielle, Marc. Coupling a stochastic\napproximation version of EM with an MCMC procedure.\nESAIM: Probability and Statistics, 8:115\u2013131, August\n2004.\nMichel,\nAviva Presser, Veres, Adrian, Gray, Matthew K.,\nTeam, The Google Books, Pickett, Joseph P., Hoiberg,\nDale, Clancy, Dan, Norvig, Peter, Orwant, Jon, Pinker,\nSteven, Nowak, Martin A., , and Aiden, Erez Lieber-\nman. Quantitative analysis of culture using millions of\ndigitized books. Science, 311, 2011.\nJean-Baptiste,Shen,YuanKui,Aiden,\nMimno, David, Wallach, Hanna, Talley, Edmund, Leen-\nders, Miriam, and McCallum, Andrew. Optimizing se-\nmantic coherence in topic models. In EMNLP, 2011.\nNewman, David, Lau, Jey Han, Grieser, Karl, and Bald-\nwin, Timothy. Automatic evaluation of topic coherence.\nIn Human Language Technologies: The Annual Confer-\nence of the North American Chapter of the Association\nfor Computational Linguistics, 2010.\nSato, M.A. Online model selection based on the variational\nBayes. Neural Computation, 13(7):1649\u20131681, 2001.\nTeh, Yee-Whye, Newman, David, and Welling, Max. A col-\nlapsed variational bayesian inference algorithm for latent\ndirichlet allocation. In NIPS, 2006.\nWahabzada, Mirwaes and Kersting, Kristian. Larger resid-\nuals, less work: Active document scheduling for latent\nDirichlet allocation. In ECML\/PKDD, 2011.\nWallach, Hanna, Murray, Iain, Salakhutdinov, Ruslan, and\nMimno, David. Evaluation methods for topic models. In\nICML, 2009.\nA. Sparse computation\nSparse sampling over topics.\n(\u03b1 + Ndk)eEq[log \u03b2kw]requires calculating the normal-\nizing constant Z =?\nO(k) if we can represent the topic-word parameters\n\u03bbkwsparsely. The smoothing parameter \u03b7 can be fac-\ntored out of Equation 11 as long as we assume that all\ninitial values \u03bb0\nkw\u2265 \u03b7. Rearranging this equation to\nseparate the Dirichlet hyperparameter \u03b7\nSampling zs\ndi\u221d\nk(\u03b1 + Ndk)eEq[log \u03b2kw]. This cal-\nculation can be accomplished in time much less than\n\u03bbt\nkw\u2190 \u03b7 + (1 \u2212 \u03c1t)?\u03bbt\u22121\nkw\u2212 \u03b7?+ \u03c1t\nD\n|B|NS\nkw\n(14)\nshows that we can define an alternative parameter\n\u02dc Nt\nkw\u2212 \u03b7 that represents the \u201cnon-smoothing\u201d\nportion of the variational Dirichlet parameter, and ig-\nnore the contribution of the smoothing parameter until\nit is time to calculate expectations.\nkw= \u03bbt\nFor any given w, it is likely that most values of\u02dc Nkw\nwill be zero. We can therefore rewrite the normalizing\nconstant as\n?\n?\nThe second summation does not depend on any word-\nspecific variables, and can therefore be calculated and\nthen updated incrementally as Ndkchanges. The first\nsummation is non-zero only for k such that\u02dc Nkw> 0.\nZ =\nk\n\u03b1 + Ndk\ne\u03a8(V \u03b7+\u02dcNk\u25e6)\n?\ne\u03a8(\u03b7+\u02dcNkw)\u2212 e\u03a8(\u03b7)?\n\u03b1 + Ndk\ne\u03a8(V \u03b7+\u02dc\n+\nk\nNk\u25e6)e\u03a8(\u03b7).(15)\nSparse updates in the vocabulary.\nthat a typical mini-batch will contain a small fraction\nof the words in the vocabulary. Eq. 11, however, up-\ndates\u02dc Nkwfor all words, even words that do not occur\nin the current mini-batch. Expanding the recursive\ndefinition of\u02dc Nt\n?\nWe expect\nkw, and letting\u02c6 Nt\nkw=\nD\n|B|NS\nkw,\n\u02dc Nt\nkw= \u03c1t\u02c6 Nt\nkw+ (1 \u2212 \u03c1t)\u03c1t\u22121\u02c6 Nt\u22121\nkw+ (1 \u2212 \u03c1t\u22121)(...)\n?\n(16)\n= \u03c1t\u02c6 Nt\nkw+ (1 \u2212 \u03c1t)\u03c1t\u22121\u02c6 Nt\u22121\nkw+ (1 \u2212 \u03c1t)(1 \u2212 \u03c1t\u22121)...\n(17)\nDividing both sides by?t\n\u02dc Nt\nkw\n?t\ni=1(1 \u2212 \u03c1i),\n\u03c1t\u02c6 Nt\nkw\ni=1(1 \u2212 \u03c1i)+\n\u03c1t\u22122\u02c6 Nt\u22122\n?t\u22122\ni=1(1 \u2212 \u03c1i), the update be-\ni=1(1 \u2212 \u03c1i)\n=\n?t\n+\n\u03c1t\u22121\u02c6 Nt\u22121\n?t\u22121\n+ ....\nkw\ni=1(1 \u2212 \u03c1i)\n(18)\nkw\ni=1(1 \u2212 \u03c1i)\nDefining a variable \u03c0t=?t\ncomes\n\u02dc Nt\n\u03c0t\nkw\n=\n\u02dc Nt\u22121\nkw\n\u03c0t\u22121\n+\u03c1t\u02c6 Nt\nkw\n\u03c0t\n. (19)\nThis update is sparse:\nzero ndw will be modified.\npectation of p(w|k), we compute \u03a8\n\u03a8W\u03b7 + \u03c0t\nw\n\u03c0t\nonly elements with non-\nTo calculate the ex-\n?\n\u03b7 + \u03c0t\nNt\n\u03c0t\nkw\n?\n\u2212\n?\n?\nNt\nkw\n?\n.\nThe scale factor \u03c0tcan become small after several hun-\ndred mini-batches. We periodically \u201creset\u201d this pa-\nrameter by setting all stored values to\u02dc Nt\navoiding the possibility of numerical instability.\nkw= \u03c0t\n\u02dc\n\u03c0t,\nNt\nkw"}],"widgetId":"rgw26_56ab1b7347e32"},"id":"rgw26_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=228095627&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw27_56ab1b7347e32"},"id":"rgw27_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=228095627&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":228095627,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":228095627,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2076919269,"url":"researcher\/2076919269_Amar_Shah","fullname":"Amar Shah","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":54244192,"url":"researcher\/54244192_David_A_Knowles","fullname":"David A. Knowles","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Jun 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process","usePlainButton":true,"publicationUid":279309917,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process","title":"An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process","displayTitleAsLink":true,"authors":[{"id":2076919269,"url":"researcher\/2076919269_Amar_Shah","fullname":"Amar Shah","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":54244192,"url":"researcher\/54244192_David_A_Knowles","fullname":"David A. Knowles","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Stochastic variational inference (SVI) is emerging as the most promising\ncandidate for scaling inference in Bayesian probabilistic models to large\ndatasets. However, the performance of these methods has been assessed primarily\nin the context of Bayesian topic models, particularly latent Dirichlet\nallocation (LDA). Deriving several new algorithms, and using synthetic, image\nand genomic datasets, we investigate whether the understanding gleaned from LDA\napplies in the setting of sparse latent factor models, specifically beta\nprocess factor analysis (BPFA). We demonstrate that the big picture is\nconsistent: using Gibbs sampling within SVI to maintain certain posterior\ndependencies is extremely effective. However, we find that different posterior\ndependencies are important in BPFA relative to LDA. Particularly,\napproximations able to model intra-local variable dependence perform best.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/David_Knowles2\/publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process\/links\/55a53c7b08aef604aa042e0b.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/David_Knowles2","sourceName":"David A. Knowles","hasSourceUrl":true},"publicationUid":279309917,"publicationUrl":"publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process\/links\/55a53c7b08aef604aa042e0b\/smallpreview.png","linkId":"55a53c7b08aef604aa042e0b","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=279309917&reference=55a53c7b08aef604aa042e0b&eventCode=&origin=publication_list","widgetId":"rgw31_56ab1b7347e32"},"id":"rgw31_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=279309917&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"55a53c7b08aef604aa042e0b","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":228095627,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Meanwhile the topic modelling community has taken great strides developing stochastic variational inference methods for latent Dirichlet allocation (Blei et al., 2003), encouraged by the availability of large corpora of text. The idea was initially proposed in Hoffman et al. (2010), and refined in Mimno et al. (2012) where the sparse updates of Gibbs sampling were leveraged to scale inference on just a single machine to 1.2 million books. The latter idea allows nontruncated online learning (Wang & Blei, 2012) of Bayesian non-parametric models, though only the hierachical Dirichlet process (Teh et al., 2004) was demonstrated. "],"widgetId":"rgw32_56ab1b7347e32"},"id":"rgw32_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw30_56ab1b7347e32"},"id":"rgw30_56ab1b7347e32","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=279309917&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":58723617,"url":"researcher\/58723617_Sotirios_P_Chatzis","fullname":"Sotirios P. Chatzis","last":true,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272504967135238%401441981656662_m\/Sotirios_Chatzis.png"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Nov 2014","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/268451692_A_Nonparametric_Bayesian_Approach_Toward_Stacked_Convolutional_Independent_Component_Analysis","usePlainButton":true,"publicationUid":268451692,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/268451692_A_Nonparametric_Bayesian_Approach_Toward_Stacked_Convolutional_Independent_Component_Analysis","title":"A Nonparametric Bayesian Approach Toward Stacked Convolutional Independent Component Analysis","displayTitleAsLink":true,"authors":[{"id":58723617,"url":"researcher\/58723617_Sotirios_P_Chatzis","fullname":"Sotirios P. Chatzis","last":true,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272504967135238%401441981656662_m\/Sotirios_Chatzis.png"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Unsupervised feature learning algorithms based on convolutional formulations\nof independent components analysis (ICA) have been demonstrated to yield\nstate-of-the-art results in several action recognition benchmarks. However,\nexisting approaches do not allow for the number of latent components (features)\nto be automatically inferred from the data in an unsupervised manner. This is a\nsignificant disadvantage of the state-of-the-art, as it results in considerable\nburden imposed on researchers and practitioners, who must resort to tedious\ncross-validation procedures to obtain the optimal number of latent features. To\nresolve these issues, in this paper we introduce a convolutional nonparametric\nBayesian sparse ICA architecture for overcomplete feature learning from\nhigh-dimensional data. Our method utilizes an Indian buffet process prior to\nfacilitate inference of the appropriate number of latent features under a\nhybrid variational inference algorithm, scalable to massive datasets. As we\nshow, our model can be naturally used to obtain deep unsupervised hierarchical\nfeature extractors, by greedily stacking successive model layers, similar to\nexisting approaches. In addition, inference for this model is completely\nheuristics-free; thus, it obviates the need of tedious parameter tuning, which\nis a major challenge most deep learning approaches are faced with. We evaluate\nour method on several action recognition benchmarks, and exhibit its advantages\nover the state-of-the-art.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/268451692_A_Nonparametric_Bayesian_Approach_Toward_Stacked_Convolutional_Independent_Component_Analysis","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Sotirios_Chatzis\/publication\/268451692_A_Nonparametric_Bayesian_Approach_Toward_Stacked_Convolutional_Independent_Component_Analysis\/links\/54dfb7420cf29666378be5e1.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Sotirios_Chatzis","sourceName":"Sotirios P Chatzis","hasSourceUrl":true},"publicationUid":268451692,"publicationUrl":"publication\/268451692_A_Nonparametric_Bayesian_Approach_Toward_Stacked_Convolutional_Independent_Component_Analysis","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/268451692_A_Nonparametric_Bayesian_Approach_Toward_Stacked_Convolutional_Independent_Component_Analysis\/links\/54dfb7420cf29666378be5e1\/smallpreview.png","linkId":"54dfb7420cf29666378be5e1","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=268451692&reference=54dfb7420cf29666378be5e1&eventCode=&origin=publication_list","widgetId":"rgw34_56ab1b7347e32"},"id":"rgw34_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=268451692&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"54dfb7420cf29666378be5e1","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":228095627,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/268451692_A_Nonparametric_Bayesian_Approach_Toward_Stacked_Convolutional_Independent_Component_Analysis\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["We devise an efficient inference algorithm for our model under a hybrid variational inference paradigm, similar to [25]. In contrast to traditional variational inference algorithms, which require imposition of truncation thresholds for the model or the variational distribution over the extracted features [14], our method adapts model complexity on the fly. "],"widgetId":"rgw35_56ab1b7347e32"},"id":"rgw35_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw33_56ab1b7347e32"},"id":"rgw33_56ab1b7347e32","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=268451692&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithSlurp","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":69562933,"url":"researcher\/69562933_James_Foulds","fullname":"James Foulds","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69897643,"url":"researcher\/69897643_Levi_Boyles","fullname":"Levi Boyles","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70373032,"url":"researcher\/70373032_Christopher_Dubois","fullname":"Christopher Dubois","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":66646470,"url":"researcher\/66646470_Padhraic_Smyth","fullname":"Padhraic Smyth","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":false,"isSlurp":true,"isNoText":false,"publicationType":"Article","publicationDate":"May 2013","journal":null,"showEnrichedPublicationItem":false,"citationCount":9,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/236687792_Stochastic_Collapsed_Variational_Bayesian_Inference_for_Latent_Dirichlet_Allocation","usePlainButton":true,"publicationUid":236687792,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/236687792_Stochastic_Collapsed_Variational_Bayesian_Inference_for_Latent_Dirichlet_Allocation","title":"Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet Allocation","displayTitleAsLink":true,"authors":[{"id":69562933,"url":"researcher\/69562933_James_Foulds","fullname":"James Foulds","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69897643,"url":"researcher\/69897643_Levi_Boyles","fullname":"Levi Boyles","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70373032,"url":"researcher\/70373032_Christopher_Dubois","fullname":"Christopher Dubois","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":66646470,"url":"researcher\/66646470_Padhraic_Smyth","fullname":"Padhraic Smyth","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69847505,"url":"researcher\/69847505_Max_Welling","fullname":"Max Welling","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"In the internet era there has been an explosion in the amount of digital text\ninformation available, leading to difficulties of scale for traditional\ninference algorithms for topic models. Recent advances in stochastic\nvariational inference algorithms for latent Dirichlet allocation (LDA) have\nmade it feasible to learn topic models on large-scale corpora, but these\nmethods do not currently take full advantage of the collapsed representation of\nthe model. We propose a stochastic algorithm for collapsed variational Bayesian\ninference for LDA, which is simpler and more efficient than the state of the\nart method. We show connections between collapsed variational Bayesian\ninference and MAP estimation for LDA, and leverage these connections to prove\nconvergence properties of the proposed algorithm. In experiments on large-scale\ntext corpora, the algorithm was found to converge faster and often to a better\nsolution than the previous method. Human-subject experiments also demonstrated\nthat the method can learn coherent topics in seconds on small corpora,\nfacilitating the use of topic models in interactive document analysis software.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/236687792_Stochastic_Collapsed_Variational_Bayesian_Inference_for_Latent_Dirichlet_Allocation","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fexport.arxiv.org%2Fpdf%2F1305.2452","sourceName":"export.arxiv.org","hasSourceUrl":true},"publicationUid":236687792,"publicationUrl":"publication\/236687792_Stochastic_Collapsed_Variational_Bayesian_Inference_for_Latent_Dirichlet_Allocation","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/236687792_Stochastic_Collapsed_Variational_Bayesian_Inference_for_Latent_Dirichlet_Allocation\/links\/035900f80cf2e6adb8b5c962\/smallpreview.png","linkId":"035900f80cf2e6adb8b5c962","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=236687792&reference=035900f80cf2e6adb8b5c962&eventCode=&origin=publication_list","widgetId":"rgw37_56ab1b7347e32"},"id":"rgw37_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=236687792&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":228095627,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/236687792_Stochastic_Collapsed_Variational_Bayesian_Inference_for_Latent_Dirichlet_Allocation\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["For variational inference , perhaps the most important advantage of the collapsed representation is that the variational bound is strictly better than for the uncollapsed representation, leading to the potential to learn more accurate topic models [24]. The existing online inference algorithms for LDA do not fully take advantage of the collapsed representation \u2013 although the sparse online LDA algorithm of Mimno et al. [16] collapses out per-document parameters \u03b8, the topics themselves are not collapsed so there is no improvement in the variational bound. "],"widgetId":"rgw38_56ab1b7347e32"},"id":"rgw38_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw36_56ab1b7347e32"},"id":"rgw36_56ab1b7347e32","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=236687792&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":228095627,"publicationLink":"publication\/228095627_Sparse_Stochastic_Inference_for_Latent_Dirichlet_allocation","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw29_56ab1b7347e32"},"id":"rgw29_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=228095627&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=22","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":22,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw28_56ab1b7347e32"},"id":"rgw28_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=228095627&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/228095627_Sparse_Stochastic_Inference_for_Latent_Dirichlet_allocation","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1b7347e32"},"id":"rgw2_56ab1b7347e32","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":228095627},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=228095627&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1b7347e32"},"id":"rgw1_56ab1b7347e32","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"zuJolFWa48xFbSPB1ULCfRiWmjH3+d016lBhYHzVKDpshBMUX4AyEkH2J9yO+kIhb4CMnY1Tm8SoAHA4NpTpZK1lD\/bCyKVMlnQ6qKf451pUhqRLj4OuL+Sl4tBDYhFN6AWVJdn\/o257E4o+6Wr6NwnXge+PBvMAfCxBJPVcpgedKlKOXROc9bOuwISBh4g9dZKl\/lU7YgexVRsKKmdXThxuWb5JyNPIds8PrtDjYGls9QKAUWBg34IeOeoWpXcMCpFI8aWljqSffdYkcz85o8w9drPcZNxEbSGRQJIOnTQ=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/228095627_Sparse_Stochastic_Inference_for_Latent_Dirichlet_allocation\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Sparse Stochastic Inference for Latent Dirichlet allocation\" \/>\n<meta property=\"og:description\" content=\"We present a hybrid algorithm for Bayesian topic models that combines the\nefficiency of sparse Gibbs sampling with the scalability of online stochastic\ninference. We used our algorithm to analyze...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/228095627_Sparse_Stochastic_Inference_for_Latent_Dirichlet_allocation\/links\/02b045870cf27908e9dda48a\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/228095627_Sparse_Stochastic_Inference_for_Latent_Dirichlet_allocation\" \/>\n<meta property=\"rg:id\" content=\"PB:228095627\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Sparse Stochastic Inference for Latent Dirichlet allocation\" \/>\n<meta name=\"citation_author\" content=\"David Mimno\" \/>\n<meta name=\"citation_author\" content=\"Matt Hoffman\" \/>\n<meta name=\"citation_author\" content=\"David Blei\" \/>\n<meta name=\"citation_publication_date\" content=\"2012\/06\/27\" \/>\n<meta name=\"citation_volume\" content=\"2\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/228095627_Sparse_Stochastic_Inference_for_Latent_Dirichlet_allocation\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/228095627_Sparse_Stochastic_Inference_for_Latent_Dirichlet_allocation\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-2380e38d-7b3c-4301-a9e0-8d688355fba4","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":688,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw39_56ab1b7347e32"},"id":"rgw39_56ab1b7347e32","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-2380e38d-7b3c-4301-a9e0-8d688355fba4", "078bc9643310c0b67943aa7568f1cab0df6a1901");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-2380e38d-7b3c-4301-a9e0-8d688355fba4", "078bc9643310c0b67943aa7568f1cab0df6a1901");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw40_56ab1b7347e32"},"id":"rgw40_56ab1b7347e32","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/228095627_Sparse_Stochastic_Inference_for_Latent_Dirichlet_allocation","requestToken":"6kJ9MbCuTUca96w1K2bGB6PwxOByq0nWelHcmxvxS7hC2oS7XvGccw98tMBSrRWqUVa6HRFZb+\/c5YJxaZMG8ygrV8YnUcDBL1Rmo7sPC1SA2jHPorupWzuMyJQxnIoDxeeazIqU+RHzeYGQP7iX0266\/m5QeUkR2TSUzvOSEuCFDEbxYvAGU7W7LseLW92NWqNl1fm8yfD+utIz6C4xD9Dq1reakMi948xSsfdcdUXMPh7fTgm1l9pF7jDRBTg6ZLw3fDifRivLs90qARyx\/f5x+ct\/X9bSik5v6oTv1Yg=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=WidOUoJdjaHp4FgSZgc1yVcOfBVX1wob1sfyJloDe37oBWDfBxPRtjuclpTukNxu","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjI4MDk1NjI3X1NwYXJzZV9TdG9jaGFzdGljX0luZmVyZW5jZV9mb3JfTGF0ZW50X0RpcmljaGxldF9hbGxvY2F0aW9u","signupCallToAction":"Join for free","widgetId":"rgw42_56ab1b7347e32"},"id":"rgw42_56ab1b7347e32","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw41_56ab1b7347e32"},"id":"rgw41_56ab1b7347e32","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw43_56ab1b7347e32"},"id":"rgw43_56ab1b7347e32","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
