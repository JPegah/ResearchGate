<!DOCTYPE html> <html lang="en" class="" id="rgw44_56ab2027499d2"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="O6k4fS3u6bbAwQ0iF3BlsV1PLI3m3zVG1q2IzMc6JryuMN7JyY4p/MeN1/o4UkULp0PM+vrdOkdF9ZPpTLLFTUHHhyHyl6L2nWwR8H62ZJ0mJsdWn6OLXkwBamnTOHPSdKXFmrJKHL/q/DnTJtsp4WltLY8JBK+kNGXbte7r/hQo7U6+Xnv5GnnKOXmTxc0RX5/sOTyh4GmfQY6S94BqOQpafXGzl4O+tmTJEqv+D5jeEM1y+syCW7/oYF2LBiTQJRhuUn5ruqNCi9idLRes5DCc4pAht4pF8U489zMzvYs="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-b4d1d89e-970d-4b58-a442-f95cb0c7dd9b",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/45280500_HypE_An_Algorithm_for_Fast_Hypervolume-Based_Many-Objective_Optimization" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization" />
<meta property="og:description" content="In the field of evolutionary multi-criterion optimization, the hypervolume indicator is the only single set quality measure that is known to be strictly monotonic with regard to Pareto dominance:..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/45280500_HypE_An_Algorithm_for_Fast_Hypervolume-Based_Many-Objective_Optimization/links/0e60a57ef0c4cf5df7c57d44/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/45280500_HypE_An_Algorithm_for_Fast_Hypervolume-Based_Many-Objective_Optimization" />
<meta property="rg:id" content="PB:45280500" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1162/EVCO_a_00009" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization" />
<meta name="citation_author" content="Johannes Bader" />
<meta name="citation_author" content="Eckart Zitzler" />
<meta name="citation_pmid" content="20649424" />
<meta name="citation_publication_date" content="2011/03/01" />
<meta name="citation_journal_title" content="Evolutionary Computation" />
<meta name="citation_issn" content="1530-9304" />
<meta name="citation_volume" content="19" />
<meta name="citation_issue" content="1" />
<meta name="citation_firstpage" content="45" />
<meta name="citation_lastpage" content="76" />
<meta name="citation_doi" content="10.1162/EVCO_a_00009" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/45280500_HypE_An_Algorithm_for_Fast_Hypervolume-Based_Many-Objective_Optimization" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/45280500_HypE_An_Algorithm_for_Fast_Hypervolume-Based_Many-Objective_Optimization" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/212307534887204/styles/modules/publictopics.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization</title>
<meta name="description" content="HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab2027499d2" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab2027499d2" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab2027499d2">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1162%2FEVCO_a_00009&rft.atitle=HypE%3A%20An%20Algorithm%20for%20Fast%20Hypervolume-Based%20Many-Objective%20Optimization&rft.title=Evolutionary%20computation&rft.jtitle=Evolutionary%20computation&rft.volume=19&rft.issue=1&rft.date=2011&rft.pages=45-76&rft.issn=1530-9304&rft.au=Johannes%20Bader%2CEckart%20Zitzler&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization</h1> <meta itemprop="headline" content="HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/45280500_HypE_An_Algorithm_for_Fast_Hypervolume-Based_Many-Objective_Optimization/links/0e60a57ef0c4cf5df7c57d44/smallpreview.png">  <div id="rgw8_56ab2027499d2" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56ab2027499d2"> <a href="researcher/55315225_Johannes_Bader" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Johannes Bader" alt="Johannes Bader" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Johannes Bader</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw10_56ab2027499d2">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/55315225_Johannes_Bader"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Johannes Bader" alt="Johannes Bader" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/55315225_Johannes_Bader" class="display-name">Johannes Bader</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56ab2027499d2" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Eckart_Zitzler" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Eckart Zitzler" alt="Eckart Zitzler" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Eckart Zitzler</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw12_56ab2027499d2" data-account-key="Eckart_Zitzler">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Eckart_Zitzler"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Eckart Zitzler" alt="Eckart Zitzler" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Eckart_Zitzler" class="display-name">Eckart Zitzler</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Paedagogische_Hochschule_Bern" title="Pädagogische Hochschule Bern">Pädagogische Hochschule Bern</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">  <div> Computer Engineering and Networks Laboratory, ETH Zurich, 8092 Zurich, Switzerland.  </div>      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/1530-9304_Evolutionary_Computation"><span itemprop="name">Evolutionary Computation</span></a> </span>    (Impact Factor: 2.37).     <meta itemprop="datePublished" content="2011-03">  03/2011;  19(1):45-76.    DOI:&nbsp;10.1162/EVCO_a_00009           <div class="pub-source"> Source: <a href="http://www.ncbi.nlm.nih.gov/pubmed/20649424" rel="nofollow">PubMed</a> </div>  </div> <div id="rgw13_56ab2027499d2" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>In the field of evolutionary multi-criterion optimization, the hypervolume indicator is the only single set quality measure that is known to be strictly monotonic with regard to Pareto dominance: whenever a Pareto set approximation entirely dominates another one, then the indicator value of the dominant set will also be better. This property is of high interest and relevance for problems involving a large number of objective functions. However, the high computational effort required for hypervolume calculation has so far prevented the full exploitation of this indicator's potential; current hypervolume-based search algorithms are limited to problems with only a few objectives. This paper addresses this issue and proposes a fast search algorithm that uses Monte Carlo simulation to approximate the exact hypervolume values. The main idea is not that the actual indicator values are important, but rather that the rankings of solutions induced by the hypervolume indicator. In detail, we present HypE, a hypervolume estimation algorithm for multi-objective optimization, by which the accuracy of the estimates and the available computing resources can be traded off; thereby, not only do many-objective problems become feasible with hypervolume-based search, but also the runtime can be flexibly adapted. Moreover, we show how the same principle can be used to statistically compare the outcomes of different multi-objective optimizers with respect to the hypervolume--so far, statistical testing has been restricted to scenarios with few objectives. The experimental results indicate that HypE is highly effective for many-objective problems in comparison to existing multi-objective evolutionary algorithms. HypE is available for download at http://www.tik.ee.ethz.ch/sop/download/supplementary/hype/.</div> </p>  </div>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw26_56ab2027499d2">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw25_56ab2027499d2"  itemprop="articleBody">  <p>Page 1</p> <p>1<br />HypE: An Algorithm for Fast Hypervolume-Based<br />Many-Objective Optimization<br />Johannes Bader and Eckart Zitzler<br />Computer Engineering and Networks Laboratory, ETH Zurich, 8092 Zurich, Switzerland<br />{johannes.bader,eckart.zitzler}@tik.ee.ethz.ch<br />TIK-Report No. 286<br />November 26, 2008<br />Abstract—In the field of evolutionary multi-criterion optimiza-<br />tion, the hypervolume indicator is the only single set quality<br />measure that is known to be strictly monotonic with regard to<br />Pareto dominance: whenever a Pareto set approximation entirely<br />dominates another one, then also the indicator value of the former<br />will be better. This property is of high interest and relevance<br />for problems involving a large number of objective functions.<br />However, the high computational effort required for hypervolume<br />calculation has so far prevented to fully exploit the potential of<br />this indicator; current hypervolume-based search algorithms are<br />limited to problems with only a few objectives.<br />This paper addresses this issue and proposes a fast search<br />algorithm that uses Monte Carlo simulation to approximate the<br />exact hypervolume values. The main idea is that not the actual<br />indicator values are important, but rather the rankings of solu-<br />tions induced by the hypervolume indicator. In detail, we present<br />HypE, a hypervolume estimation algorithm for multiobjective<br />optimization, by which the accuracy of the estimates and the<br />available computing resources can be traded off; thereby, not<br />only many-objective problems become feasible with hypervolume-<br />based search, but also the runtime can be flexibly adapted.<br />Moreover, we show how the same principle can be used to<br />statistically compare the outcomes of different multiobjective<br />optimizers with respect to the hypervolume—so far, statistical<br />testing has been restricted to scenarios with few objectives. The<br />experimental results indicate that HypE is highly effective for<br />many-objective problems in comparison to existing multiobjective<br />evolutionary algorithms.<br />HypE is available for download at http://www.tik.ee.ethz.ch/<br />sop/download/supplementary/hype/.<br />I. MOTIVATION<br />By far most studies in the field of evolutionary multiobjec-<br />tive optimization (EMO) are concerned with the following set<br />problem: find a set of solutions that as a whole represents a<br />good approximation of the Pareto-optimal set. To this end, the<br />original multiobjective problem consisting of<br />• the decision space X,<br />• the objective space Z = Rn,<br />• a vector function f = (f1,f2,...,fn) comprising n<br />objective functions fi: X → R, which are without loss<br />of generality to be minimized, and<br />• a relation ≤ on Z, which induces a preference relation<br />? on X with a ? b :⇔ f(a) ≤ f(b) for a,b ∈ X,<br />is usually transformed into a single-objective set problem [46].<br />The search space Ψ of the resulting set problem includes<br />all possible Pareto set approximations1, i.e., Ψ contains all<br />multisets over X. The preference relation ? can be used to<br />define a corresponding set preference relation ? on Ψ where<br />A ? B :⇔ ∀b ∈ B ∃a ∈ A : a ? b<br />for all Pareto set approximations A,B ∈ Ψ. In the following,<br />we will assume that weak Pareto dominance is the underlying<br />preference relation, cf. [46].2<br />A key question when tackling such a set problem is how<br />to define the optimization criterion. Many multiobjective evo-<br />lutionary algorithms (MOEAs) implement a combination of<br />Pareto dominance on sets and a diversity measure based on<br />Euclidean distance in the objective space, e.g., NSGA-II [13]<br />and SPEA2 [42]. While these methods have been successfully<br />employed in various biobjective optimization scenarios, they<br />appear to have difficulties when the number of objectives<br />increases [34]. As a consequence, researchers have tried to<br />develop alternative concepts, and a recent trend is to use<br />set quality measures, also denoted as quality indicators, for<br />search—so far, they have mainly been used for performance<br />assessment. Of particular interest in this context is the hy-<br />pervolume indicator [43], [45] as it is the only quality in-<br />dicator known to be fully sensitive to Pareto dominance—a<br />property especially desirable when many objective functions<br />are involved.<br />Several hypervolume-based MOEAs have been proposed<br />meanwhile, e.g., [15], [23], [9], but their main drawback is<br />their extreme computational overhead. Although there have<br />been recent studies presenting improved algorithms for hy-<br />pervolume calculation, currently high-dimensional problems<br />with six or more objectives are infeasible for these MOEAs.<br />Therefore, the question is whether and how fast hypervolume-<br />based search algorithms can be designed that exploit the<br />(1)<br />1Here, a Pareto set approximation may also contain dominated solutions as<br />well as duplicates, in contrast to the notation in [47].<br />2For reasons of simplicity, we will use the term ‘u weakly dominates v’<br />resp. ‘u dominates v’ independently of whether u and v are elements of X,<br />Z, or Ψ. For instance, A weakly dominates b with A ∈ Ψ and b ∈ X means<br />A ? {b} and a dominates z with a ∈ X and z ∈ Z means f(a) ≤ z ∧ z ?≤<br />f(a).</p>  <p>Page 2</p> <p>advantages of the hypervolume indicator and at the same time<br />are scalable with respect to the number of objectives.<br />A first attempt in this direction has been presented in [1].<br />The main idea is to estimate—by means of Monte Carlo<br />simulation—the ranking of the individuals that is induced by<br />the hypervolume indicator and not to determine the exact indi-<br />cator values. This paper proposes an advanced method called<br />HypE (Hypervolume Estimation Algorithm for Multiobjective<br />Optimization) that is based on the same idea, but uses more<br />effective fitness assignment and sampling strategies. In detail,<br />the main contributions of this work can be summarized as<br />follows:<br />1) A novel method to assign fitness values to individuals<br />based on the hypervolume indicator—for both mating<br />and environmental selection;<br />2) A hypervolume-based search algorithm (HypE) using<br />Monte Carlo simulation that can be applied to problems<br />with arbitrarily many objectives;<br />3) A statistical testing procedure that allows to compare<br />the outcomes of different multiobjective optimizers with<br />respect to the hypervolume indicator in many-objective<br />scenarios.<br />As we will show in the follwing, the proposed search algo-<br />rithm can be easily tuned regarding the available computing<br />resources and the number of objectives involved. Thereby,<br />it opens a new perspective on how to treat many-objective<br />problems, and the presented concepts may also be helpful<br />for other types of quality indicators to be integrated in the<br />optimization process.<br />II. A BRIEF REVIEW OF HYPERVOLUME-RELATED<br />RESEARCH<br />The hypervolume indicator was originally proposed and<br />employed in [44], [45] to quantitatively compare the outcomes<br />of different MOEAs. In these two first publications, the<br />indicator was denoted as ‘size of the space covered’, and later<br />also other terms such as ‘hyperarea metric’ [33], ‘S-metric’<br />[38], ‘hypervolume indicator’ [47], and hypervolume measure<br />[4] were used. Besides the names, there are also different<br />definitions available, based on polytopes [45], the Lebesgue<br />measure [27], [26], [18], or the attainment function [40].<br />As to hypervolume calculation, the first algorithms [39],<br />[26] operated recursively and in each recursion step the num-<br />ber of objectives was decremented; the underlying principle is<br />known as ‘hypervolume by slicing objectives’ approach [36].<br />While the method used in [44], [45] was never published<br />(only the source code is publicly available [39]), Knowles<br />independently proposed and described a similar method in<br />[26]. A few years later, this approach was the first time studied<br />systematically and heuristics to accelerate the computation<br />were proposed in [36]. All these algorithms have a worst-<br />case runtime complexity that is exponential in the number of<br />objecives, more specifically O(Nn−1) where N is the number<br />of solutions considered [26], [36]. A different approach was<br />presented by Fleischer [18] who mistakenly claimed a polyno-<br />mial worst-case runtime complexity—While [35] showed that<br />it is exponential in n as well. Recently, advanced algorithms<br />for hypervolume calculation have been proposed, a dimension-<br />sweep method [19] with a worst-case runtime complexity of<br />O(Nn−2logN), and a specialized algorithm related to the<br />Klee measure problem [5] the runtime of which is in the<br />worst case of order O(N logN + Nn/2). Furthermore, Yang<br />and Ding [37] described an algorithm for which they claim a<br />worst-case runtime complexity of O((n/2)N). The fact that<br />there is no exact polynomial algorithm available gave rise<br />to the hypothesis that this problem in general is hard to<br />solve, although the tighest known lower bound is of order<br />Ω(N logN) [3]. New results substantiate this hypothesis:<br />Bringmann and Friedrich [8] have proven that the problem<br />of computing the hypervolume is #P-complete, i.e., it is<br />expected that no polynomial algorithm exists since this would<br />imply NP = P.<br />The complexity of the hypervolume calculation in terms<br />of programming and computation time may explain why this<br />measure was seldom used until 2003. However, this changed<br />with the advent of theoretical studies that provided evidence<br />for a unique property of this indicator [24], [47], [18]: it is the<br />only indicator known to be strictly monotonic with respect to<br />Pareto dominance and thereby guaranteeing that the Pareto-<br />optimal front achieves the maximum hypervolume possible,<br />while any worse set will be assigned a worse indicator value.<br />This property is especially desirable with many-objective<br />problems and since classical MOEAs have been shown to have<br />difficulties in such scenarios [34], a trend can be observed<br />in the literature to directly use the hypervolume indicator for<br />search.<br />Knowles and Corne [26], [25] were the first to propose the<br />integration of the hypervolume indicator into the optimization<br />process. In particular, they described a strategy to maintain<br />a separate, bounded archive of nondominated solutions based<br />on the hypervolume indicator. Huband et al. [22] presented<br />an MOEA which includes a modified SPEA2 environmental<br />selection procedure where a hypervolume-related measure<br />replaces the original density estimation technique. In [41],<br />the binary hypervolume indicator was used to compare in-<br />dividuals and to assign corresponding fitness values within a<br />general indicator-based evolutionary algorithm (IBEA). The<br />first MOEA tailored specifically to the hypervolume indicator<br />was described in [15]; it combines nondominated sorting with<br />the hypervolume indicator and considers one offspring per<br />generation (steady state). Similar fitness assignment strategies<br />were later adopted in [40], [23], and also other search al-<br />gorithms were proposed where the hypervolume indicator is<br />partially used for search guidance [29], [28]. Moreover, spe-<br />cific aspects like hypervolume-based environmental selection<br />[7], cf. Section III-B, and explicit gradient determination for<br />hypervolume landscapes [16] have been investigated recently.<br />To date, the hypervolume indicator is one of the most<br />popular set quality measures. For instance, almost one fourth<br />of the papers published in the proceedings of the EMO 2007<br />conference [30] report on the use of or are dedicated to the<br />hypervolume indicator. However, there are still two major<br />drawbacks that current research acitivities try to tackle: (i)<br />2</p>  <p>Page 3</p> <p>the high computation effort and (ii) the bias of the indicator in<br />terms of user preferences. The former issue has been addressed<br />in different ways: by automatically reducing the number of<br />objectives [9] and by approximating the indicator values using<br />Monte Carlo methods [17], [1], [11]. Everson et al. [17] used<br />a basic Monte Carlo technique for performance assessment<br />in order to estimate the values of the binary hypervolume<br />indicator [38]; with their approach the error ratio is not<br />polynomially bounded. In contrast, the scheme presented in<br />[8] is a fully polynomial randomized approximation scheme<br />where the error ratio is polynomial in the input size. The<br />issue of statistically comparing hypervolume estimates was<br />not addressed in these two papers. Another study [1]—a<br />precursor study for the present paper—employed Monte Carlo<br />simulation for fast hypervolume-based search. As to the bias<br />issue, first proof-of-principle results have been presented in<br />[40] that demonstrate that and how the hypervolume indicator<br />can be adapted to different user preferences.<br />III. HYPERVOLUME-BASED FITNESS ASSIGNMENT<br />When considering the hypervolume indicator as the objec-<br />tive function of the underlying set problem, the main question<br />is how to make use of this measure within a multiobjective<br />optimizer to guide the search. In the context of an MOEA,<br />this refers to selection and one can distinguish two situations:<br />1) The selection of solutions to be varied (mating selec-<br />tion).<br />2) The selection of solutions to be kept in memory (envi-<br />ronmental selection).<br />Since the indicator as such operates on (multi)sets of solu-<br />tions, while selection considers single solutions, a strategy<br />for assigning fitness values to solutions is required. Most<br />hypervolume-based algorithms first perform a nondominated<br />sorting and then rank solutions within a particular front accord-<br />ing to the hypervolume loss that results from the removal of<br />a specific solution [25], [15], [23], [1]. In the following, we<br />propose a generalized fitness assignment strategy that takes<br />into account the entire objective space weakly dominated by<br />a population. We will first provide a basic scheme for mating<br />selection and then present an extension for environmental<br />selection. Afterwards, we briefly discuss how the fitness values<br />can be computed exactly using a slightly modified hypervol-<br />ume calculation algorithm.<br />A. Basic Scheme for Mating Selection<br />To begin with, we formally define the hypervolumeindicator<br />as a basis for the following discussions. Different definitions<br />can be found in the literature, and we here use the one from<br />[46] which draws upon the Lebesgue measure as proposed in<br />[27] and considers a reference set of objective vectors.<br />Definition III.1. Let A ∈ Ψ be a Pareto set approximation<br />and R ⊂ Z be a reference set of mutually nondominating<br />objective vectors. Then the hypervolume indicator IH can be<br />defined as<br />IH(A,R) := λ(H(A,R))<br />(2)<br />where<br />H(A,R) := {z ∈ Z ; ∃a ∈ A∃r ∈ R : f(a) ≤ z ≤ r}<br />and λ is the Lebesgue measure with λ(H(A,R))<br />?<br />The set H(A,R) denotes the set of objective vectors that are<br />enclosed by the front f(A) given by A and the reference set<br />R.<br />The subspace H(A,R) of Z can be further split into<br />partitions H(S,A,R), each associated with a specific subset<br />S ⊆ A:<br />H(S,A,R) := [<br />H({s},R)] \ [<br />(3)<br />=<br />Rn1H(A,R)(z)dz and 1H(A,R)being the characteristic func-<br />tion of H(A,R).<br />?<br />s∈S<br />?<br />a∈A\S<br />H({a},R)]<br />(4)<br />The set H(S,A,R) ⊆ Z represents the portion of the objective<br />space that is jointly weakly dominated by the solutions in S<br />and not weakly dominated by any other solution in A. It holds<br />˙?<br />which is illustrated in Fig. 1(a). That the partitions are disjoint<br />can be easily shown: Assume that there are two non-identical<br />subsets S1,S2of A for which H(S1,A,R)∩H(S2,A,R) ?= ∅;<br />since the sets are not identical, there exists with loss of gener-<br />ality an element a ∈ S1which is not contained in S2; from the<br />above definition follows that H({a},R) ⊇ H(S1,A,R) and<br />therefore H({a},R) ∩ H(S2,A,R) ?= ∅; the latter statement<br />leads to a contradiction since H({a},R) cannot be part of<br />H(S2,A,R) when a ?∈ S2.<br />In practice, it is infeasible to determine all distinct<br />H(S,A,R) due to combinatorial explosion. Instead, we will<br />consider a more compact splitting of the dominated objective<br />space that refers to single solutions:<br />S⊆A<br />H(S,A,R) = H(A,R)<br />(5)<br />Hi(a,A,R) :=<br />?<br />a∈S<br />|S|=i<br />S⊆A<br />H(S,A,R)<br />(6)<br />According<br />the portion of the objective space that is jointly and<br />solely weakly dominated by a and any i − 1 further<br />solutions from A, see Fig. 1(b). Note that the sets<br />H1(a,A,R),H2(a,A,R),...,H|A|(a,A,R) are disjoint for<br />a given a ∈ A, i.e.,˙?<br />fixed i and different solutions a,b ∈ A. This slightly different<br />notion has reduced the number of subspaces to be considered<br />from 2|A|for H(S,A,R) to |A|2for Hi(a,A,R).<br />Now, given an arbitrary population P<br />obtains for each solution a contained in P<br />(λ(H1(a,P,R)),λ(H2(a,P,R)),...,λ(H|P|(a,P,R)))<br />hypervolume contributions. These vectors can be used to<br />assign fitness values to solutions; Subsection III-C describes<br />how the corresponding values λ(Hi(a,A,R))<br />computed. While most hypervolume-based search algorithms<br />only take the first components, i.e., λ(H1(a,P,R)), into<br />tothis definition,<br />Hi(a,A,R)<br />standsfor<br />1≤i≤|A|Hi(a,A,R) = H({a},R), while<br />the sets Hi(a,A,R) and Hi(b,A,R) may be overlapping for<br />∈<br />Ψ one<br />a vector<br />of<br />can be<br />3</p>  <p>Page 4</p> <p>( )<br />f a<br />( )<br />f b<br />( )<br />f c<br />( )<br />f d<br />({ , }, , }<br />H b c A R<br />({ , , , }, , }<br />H a b c dA R<br />( , }<br />H A R<br />({ }, , }<br />H dA R<br />{ }<br />rR<br />=<br />(a) The relationship between H(A,R) and H(S,A,R)<br />( )<br />f a<br />( )<br />f b<br />( )<br />f c<br />( )<br />f d<br />4( , , )<br />H c A R<br />H c A R<br />({ , , , }, , )<br />H a b c dA R<br />=<br />3( , , )({ , , }, , )<br />({ , , }, , )<br />H b c d<br />+<br />H a b c A R<br />A R<br />=<br />2( , , )<br />H c A R<br />({ , }, , )<br />({ , }, , )<br />H c d<br />+<br />H b c A R<br />A R<br />=<br />1( , , )<br />H c A R<br />({ }, , )<br />H c A R<br />=<br />r<br />(b) The relationship between H(S,A,R) and Hi(a,A,R)<br />Figure 1.<br />reference set R = {r}.<br />Illustration of the notions of H(A,R), H(S,A,R), and Hi(a,A,R) in the objective space for a Pareto set approximation A = {a,b,c,d} and<br />( )<br />f a<br />( )<br />f b<br />( )<br />f c<br />( )<br />f d<br />( , , )<br />hI a P R<br />( , , )<br />1<br />2<br />1<br />2<br />1<br />1<br />1 2<br />1 3<br />1 3<br />1 3<br />1 4<br />1 4<br />hI c P R<br />{ }<br />rR<br />=<br />Figure 2. Illustration of the basic fitness assignment scheme where the fitness<br />Fa of a solution a is set to Fa= Ih(a,P,R).<br />account, we here propose the following scheme to aggregate<br />the hypervolume contributions into a single scalar value.<br />Definition III.2. Let A ∈ Ψ and R ⊂ Z. Then the function<br />Ihwith<br />|A|<br />?<br />gives for each solution a ∈ A the hypervolume that can<br />be attributed to a with regard to the overall hypervolume<br />IH(A,R).<br />Ih(a,A,R) :=<br />i=1<br />1<br />iλ(Hi(a,A,R))<br />(7)<br />The motivation behind this definition is simple: the hyper-<br />volume contribution of each partition H(S,A,R) is shared<br />equally among the dominating solutions s ∈ S. That means the<br />portion of Z solely weakly dominated by a specific solution<br />a is fully attributed to a, the portion of Z that a weakly<br />dominates together with another solution b is attributed half<br />to a and so forth—the principle is illustrated in Fig. 2.<br />Thereby, the overall hypervolume is distributed among the<br />distinct solutions according to their hypervolume contributions<br />as the following theorem shows (the proof can be found in<br />the appendix). Note that this scheme does not require that<br />the solutions of the considered Pareto set approximation A<br />are mutually non-dominating; it applies to nondominated and<br />dominated solutions alike.<br />( )<br />f a<br />( )<br />f b<br />( )<br />f c<br />( )<br />f d<br />{ }<br />rR<br />=<br />( , , )<br />hI a A Rconst<br />=<br />∑<br />Figure 3.<br />the population members (left). The sizes of the points correlate with the<br />corresponding selection probabilities. As one can see on the right, the overall<br />selection probability for the shaded area does not change when dominated<br />solutions are added to the population.<br />Shows for an example population the selection probabilities for<br />Theorem III.3. Let A ∈ Ψ and R ⊂ Z. Then it holds<br />IH(A,R) =<br />?<br />a∈A<br />Ih(a,A,R)<br />(8)<br />This aggregation method has some desirable properties that<br />make it well suited to mating selection where the fitness Fa<br />of a population member a ∈ P is Fa= Ih(a,P,R) and the<br />corresponding selection probability pa equals Fa/IH(P,R).<br />As Fig. 3 demonstrates, the accumulated selection probability<br />remains the same for any subspace H({a},R) with a ∈ P,<br />independently of how many individuals b ∈ P are mapped<br />to H({a},R) and how the individuals are located within<br />H({a},R). This can be formally stated in the next theorem;<br />the proof can again be found in the appendix.<br />Theorem III.4. Let A ∈ Ψ and R ⊂ Z. For every a ∈ A and<br />all multisets B1,B2∈ Ψ with {a} ? B1and {a} ? B2holds<br />?<br />b1∈{a}∪B1<br />Ih(b1,{a}∪B1,R) =<br />?<br />b2∈{a}∪B2<br />Ih(b2,{a}∪B2,R)<br />(9)<br />Since the selection probability per subspace is constant as<br />long as the overall hypervolume value does not change, adding<br />dominated solutions to the population leads to a redistribution<br />of the selection probabilities and thereby implements a natural<br />4</p>  <p>Page 5</p> <p>Table I<br />COMPARISON OF THREE FITNESS ASSIGNMENT SCHEMES: (1) CONSTANT<br />FITNESS, (2) NONDOMINATED SORTING PLUS λ(H1(a,P,R)), AND (3)<br />THE PROPOSED METHOD. EACH VALUE GIVES THE PERCENTAGE OF CASES<br />WHERE THE METHOD ASSOCIATED WITH THAT ROW YIELDS A HIGHER<br />HYPERVOLUME VALUE THAN THE METHOD ASSOCIATED WITH THE<br />CORRESPONDING COLUMN.<br />versusconstant (1)standard (2)new (3)<br />constant (1)-44%28%<br />standard (2)56%-37%<br />new (3)72%63%-<br />niching mechanism. Another advantage of this fitness assign-<br />ment scheme is that it takes all hypervolume contributions<br />Hi(a,P,R) for 1 ≤ i ≤ |P| into account. As will be discussed<br />in Section IV, this allows to more accurately estimate the<br />ranking of the individuals according to their fitness values<br />when using Monte Carlo simulation.<br />In order to study the usefulness of this fitness assignment<br />strategy, we consider the following experiment. A standard<br />evolutionary algorithm implementing pure nondominated sort-<br />ing fitness is applied to a selected test function (biobjective<br />WFG1 [21] using the setting as described in Section VI) and<br />run for 100 generations. Then, mating selection is carried out<br />on the resulting population, i.e., the individuals are reevaluated<br />using the fitness scheme under consideration and offspring is<br />generated employingbinary tournamentselection with replace-<br />ment and corresponding variation operators. The hypervolume<br />of the (multi)set of offspring is taken as an indicator for the<br />effectiveness of the fitness assignment scheme. By compar-<br />ing the resulting hypervolume values for different strategies<br />(constant fitness leading to uniform selection, nondominated<br />sorting plus λ(H1(a,P,R)), and the proposed fitness accord-<br />ing to Def. III.2) and for 100 repetitions of this experiment, we<br />can investigate the influence of the fitness assignment strategy<br />on the mating selection process.<br />The Quade test, a modification of Friedman’s test which has<br />more power when comparing few treatments [10], reveals that<br />there are significant differences in the quality of the generated<br />offspring populations at a signficance level of 0.01 (test statis-<br />tics: T3 = 12.2). Performing post-hoc pairwise comparisons<br />following [10] using the same significance level as in the<br />Quade test provides evidence that the proposed fitness strategy<br />can be advantageous over the other two strategies, cf. Table I;<br />in the considered setting, the hypervolume values achieved<br />are significantly better. Comparing the standard hypervolume-<br />based fitness with constant fitness, the former outperforms the<br />latter significantly. Nevertheless, also the required computation<br />resources need to be taken into account. That means in practice<br />that the advantage over uniform selection may diminish when<br />fitness computation becomes expensive. This aspect will be<br />investigated in Section VI.<br />Next, we will extend and generalize the fitness assignment<br />scheme with regard to the environmental selection phase.<br />B. Extended Scheme for Environmental Selection<br />In the context of hypervolume-based multiobjective search,<br />environmental selection can be formulated in terms of the<br />hypervolume subset selection problem (HSSP).<br />Definition III.5. Let A ∈ Ψ, R ⊂ Z, and k ∈ {0,1,...,|A|}.<br />The hypervolume subset selection problem (HSSP) is defined<br />as the problem of finding a subset A?⊆ A with |A?| = |A|−k<br />such that the overall hypervolume loss is minimum, i.e.,<br />IH(A?,R) =max<br />A??⊆A<br />|A??|=|A|−k<br />IH(A??,R)<br />(10)<br />Here, we assume that parents and offspring have been<br />merged into a single population P which then needs to be<br />truncated by removing k solutions. Since dominated solutions<br />in the population do not affect the overall hypervolume, they<br />can be deleted first; therefore, we assume in the following<br />that all solutions in P are incomparable3or indifferent4to<br />each other.<br />If k = 1, then HSSP can be solved exactly by removing<br />that solution a from the population P with the lowest value<br />λ(H1(a,P,R)); this is the principle implemented in most<br />hypervolume-based MOEAs which consider one offspring per<br />generation, e.g., [25], [15], [23]. However, it has been recently<br />shown that exchanging only one solution in the population<br />like in steady state MOEAs (k = 1) may lead to premature<br />convergence to a local optimum in the hypervolume landscape<br />[46]. This problem can be avoided when generating at least as<br />many offspring as parents are available, i.e., k ≥ |P|/2.<br />For arbitrary values of k, dynamic programming can be<br />used to solve HSSP in a biobjective setting; in the presence<br />of three or more objectives, it is an open problem whether<br />HSSP becomes NP-hard. In practice, a greedy heuristic is<br />employed to obtain an approximation [41], [9]: all solutions<br />are evaluated with respect to their usefulness and the l least<br />important solutions are removed where l is a prespecified<br />parameter. Most popular are the following two approaches:<br />1) Iterative (l = 1): The greedy heuristics is applied k<br />times in a row; each time, the worst solution is removed<br />and afterwards the remaining solutions are re-evaluated.<br />2) One shot (l = k): The greedy heuristics is only applied<br />once; the solutions are evaluated and the k worst solu-<br />tions are removed in one step.<br />Best results are usually obtained using the iterative approach,<br />as the re-evaluation increases the quality of the generated<br />approximation.In contrast, the one-shot approach substantially<br />reduces the computation effort, but the quality of the resulting<br />subset is lower. In the context of density-based MOEAs, the<br />first approach is for instance used in SPEA2, while the second<br />is employed in NSGA-II.<br />The key issue with respect to the above greedy strategy<br />is how to evaluate the usefulness of a solution. The scheme<br />3Two solutions a,b ∈ X are called incomparable if and only if neither<br />weakly dominates the other one, i.e., a ?? b and b ?? a<br />4Two solutions a,b ∈ X are called indifferent if and only if both weakly<br />dominate other one, i.e., a ? b and b ? a<br />5</p>  <p>Page 6</p> <p>r<br />( )<br />f a<br />( )<br />f b<br />( )<br />f c<br />( )<br />f d<br />1<br />p=<br />1/3<br />p=<br />0<br />p=<br />0<br />p=<br />({ , , , }, )<br />H a b c dR<br />({ , , }, }<br />H a b c R<br />({ , , }, )<br />H b c dR<br />Figure 4.<br />{a,b,c,d},R = {r} and shows (i) which portion of the objective space<br />remains dominated if any two solutions are removed from A (shaded area),<br />and (ii) the probabilities p that a particular area that can be attributed to a ∈ A<br />is lost if a is removed from A together with any other solution in A.<br />The figure is based on the previous example with A =<br />presented in Def. III.2 has the drawback that portions of<br />the objective space are taken into account that for sure will<br />not change. Consider, for instance, a population with four<br />solutions as shown in Fig. 4; when two solutions need to<br />be removed (k = 2), then the subspaces H({a,b,c},P,R),<br />H({b,c,d},P,R), and H({a,b,c,d},P,R) remain weakly<br />dominated independently of which solutions are deleted. This<br />observation led to the idea of considering the expected loss<br />in hypervolume that can be attributed to a particular solution<br />when exactly k solutions are removed. In detail, we consider<br />for each a ∈ P the average hypervolume loss over all subsets<br />S ⊆ P that contain a and k − 1 further solutions; this value<br />can be easily computed by slightly extending the scheme from<br />Def. III.2 as follows.<br />Definition III.6. Let A ∈ Ψ, R ⊂ Z, and k ∈ {0,1,...,|A|}.<br />Then the function Ik<br />⎡<br />⎣<br />where S = {S ⊆ A; a ∈ S ∧ |S| = k} contains all subsets of<br />A that include a and have cardinality k gives for each solution<br />a ∈ A the expected hypervolume loss that can be attributed<br />to a when a and k − 1 uniformly randomly chosen solutions<br />from A are removed from A.<br />hwith<br />Ik<br />h(a,A,R) :=<br />1<br />|S|<br />?<br />S∈S<br />⎢<br />?<br />a∈T<br />T⊆S<br />1<br />|T|λ?H(T,A,R)?<br />⎤<br />⎦<br />⎥<br />(11)<br />Notice<br />I|A|<br />can be regarded as a generalization of the scheme presented<br />in Def. III.2 and the commonly used fitness assignment<br />strategy for hypervolume-based search [25], [15], [23], [1].<br />The next theorem shows how to calculate Ik<br />averaging over all subsets S ∈ S; the proof can be found in<br />the appendix.<br />that<br />I1<br />h(a,A,R)<br />Ih(a,A,R), i.e., this modified scheme<br />=<br />λ(H1(a,A,R))<br />and<br />h(a,A,R)=<br />h(a,A,R) without<br />Theorem III.7. Let A ∈ Ψ, R ⊂ Z, and k ∈ {0,1,...,|A|}.<br />Then it holds<br />Ik<br />h(a,A,R) =<br />k<br />?<br />i=1<br />αi<br />iλ(Hi(a,A,R))<br />(12)<br />Table II<br />COMPARISON OF GREEDY STRATEGIES FOR THE HSSP (ITERATIVE VS.<br />ONE SHOT) USING THE NEW (Ik<br />FITNESS (I1<br />CONSIDERED AS WELL. THE FIRST COLUMN GIVES THE PORTION OF<br />CASES AN OPTIMAL SUBSET WAS GENERATED; THE SECOND COLUMN<br />PROVIDES THE AVERAGE DIFFERENCE IN HYPERVOLUME BETWEEN<br />OPTIMAL AND GENERATED SUBSET. THE LAST TWO COLUMNS REFLECT<br />THE DIRECT COMPARISONS BETWEEN THE TWO FITNESS SCHEMES FOR<br />EACH GREEDY APPROACH (ITERATIVE, ONE SHOT) SEPARATELY; THEY<br />GIVE THE PERCENTAGES OF CASES WHERE THE CORRESPONDING<br />METHOD WAS BETTER THAN OR EQUAL TO THE OTHER ONE.<br />h) AND THE STANDARD HYPERVOLUME<br />h); AS A REFERENCE, PURELY RANDOM DELETIONS ARE<br />greedy strategyoptimum founddistancebetterequal<br />iterative with Ik<br />iterative with I1<br />h<br />59.8 %<br />44.5 %<br />1.09 10−3<br />2.59 10−3<br />30.3 %<br />3.17 %<br />66.5 %<br />66.5 %<br />h<br />one shot with Ik<br />one shot with I1<br />h<br />16.9 %<br />3.4 %<br />39.3 10−3<br />69.6 10−3<br />65.2 %<br />11.1 %<br />23.7 %<br />23.7 %<br />h<br />uniformly random0.381 %257 10−3<br />where<br />αi:=<br />i−1<br />?<br />j=1<br />k − j<br />|A| − j<br />(13)<br />Next, we will study the effectiveness of Ik<br />approximating the optimal HSSP solution. To this end, we<br />assume that for the iterative greedy strategy (l = 1) in the first<br />round the values Ik<br />round the values Ik−1<br />h<br />(a,A,R), and so forth; each time an<br />individual assigned the lowest value is selected for removal.<br />For the one-step greedy method (l = k), only the Ik<br />values are considered.<br />Table II provides a comparison of the different techniques<br />for 100,000 randomly chosen Pareto set approximations A ∈<br />Ψ containing ten incomparable solutions, where the ten points<br />are randomly distributed on a three dimensional unit simplex,<br />i.e., we consider a three objective scenario. The parameter k<br />was set to 5, so that half of the solutions needed to be removed.<br />The relatively small numbers were chosen to allow to compute<br />the optimal subsets by enumeration. Thereby, the maximum<br />hypervolume values achievable could be determined.<br />The comparison reveals that the new fitness assignment<br />scheme is in the considered scenario more effective in approx-<br />imating HSSP than the standard scheme. The mean relative<br />distance (see Table II) to the optimal solution is about 60%<br />smaller than the distance achieved using I1<br />and about 44% smaller in the one shot case. Furthermore, the<br />optimum was found much more often in comparison to the<br />standard fitness: 34% more often for the iterative approach<br />and 497% in the one shot scenario.<br />Finally, note that the proposed evaluation function Ik<br />be combined with nondominated sorting for environmental<br />selection, cf. Section V, similarly to [15], [23], [9], [40],<br />[1]. One reason is computation time: with nondominated<br />sorting the worst dominated solutions can be removed quickly<br />without invoking the hypervolume calculation algorithm; this<br />advantage mainly applies to low-dimensional problems and to<br />the early stage of the search process. Another reason is that<br />the full benefits of the scheme proposed in Def. III.6 can be<br />h(a,A,R) for<br />h(a,A,R) are considered, in the second<br />h(a,A,R)<br />hin the iterative case<br />hwill<br />6</p>  <p>Page 7</p> <p>exploited when the Pareto set approximation A under consid-<br />eration only contains incomparable and indifferent solutions;<br />otherwise, it cannot be guaranteed that nondominatedsolutions<br />are preferred over dominated ones.<br />C. Exact Calculation of Ik<br />h<br />In this subsection, we tackle the question of how to calculate<br />the fitness values for a given population P ∈ Ψ. We present<br />an algorithm that determines the values Ik<br />elements a ∈ P and a fixed k—in the case of mating<br />selection k equals |P|, in the case of environmental selection<br />k gives the number of solutions to be removed from P. It<br />operates according to the ’hypervolume by slicing objectives’<br />principle [39], [26], [36], but differs from existing methods<br />in that it allows (i) to consider a set R of reference points<br />and (ii) to compute all fitness values, e.g., the I1<br />values for k = 1, in parallel for any number of objectives<br />instead of subsequently as in [4]. Although it looks at all<br />partitions H(S,P,R) with S ⊆ P explicitly, the worst-case<br />runtime complexity is not affected by this; it is of order<br />O(|P|n+n|P|log|P|) assuming that sorting of the solutions<br />in all dimensions is carried out as a preprocessing step. Clearly,<br />this is only feasible for a low number of objectives, and the<br />next section discusses how the fitness values can be estimated<br />using Monte Carlo methods.<br />Details of the procedure are given by Algorithms 1 and 2.<br />Algorithm 1 just provides the top level call to the recursive<br />function doSlicing and returns a fitness assignment F, a<br />multiset containing for each a ∈ P a corresponding pair<br />(a,v) where v is the fitness value. Note that n at Line 3<br />denotes the number of objectives. Algorithm 2 recursively<br />cuts the dominated space into hyperrectangles and returns<br />a (partial) fitness assignment F?. At each recursion level, a<br />scan is performed along a specific objective—given by i—<br />with u∗representing the current scan position. The vector<br />(z1,...,zn) contains for all dimensions the scan positions,<br />and at each invocation of doSlicing solutions (more precisely:<br />their objective vectors) and reference points are filtered out<br />according to these scan positions (Lines 3 and 4) where also<br />dominated solutions may be selected in contrast to [39], [26],<br />[36]. Furthermore, the partial volume V is updated before<br />recursively invoking Algorithm 2 based on the distance to the<br />next scan position. At the lowest recursion level (i = 0), the<br />variable V gives the hypervolume of the partition H(A,P,R),<br />i.e., V = λ(H(A,P,R)) where A stands for the remaining so-<br />lutions fulfilling the bounds given by the vector (z1,...,zn)—<br />UP contains the objective vectors corresponding to A, cf.<br />Line 3. Since the fitness according to Def. III.6 is additive with<br />respect to the partitions, for each a ∈ A the partial fitness value<br />v can be updated by adding<br />is a multiset, i.e., it may contain indifferent solutions or even<br />duplicates; therefore, all the other sets in the algorithms are<br />multisets.<br />The following example illustrates the working principle of<br />the hypervolume computation.<br />h(a,P,R) for all<br />h(a,P,R)<br />α|UP|<br />|UP|V . Note that the population<br />Example III.8.<br />Consider thethree-objectivescenario<br />Algorithm 1 Hypervolume-based Fitness Value Computation<br />Require: population P ∈ Ψ, reference set R ⊆ Z, fitness<br />parameter k ∈ N<br />1: procedure computeHypervolume(P, R, k)<br />2:<br />3:<br />return doSlicing(F,R,k,n,1,(∞,∞,...,∞));<br />4: end procedure<br />F ←?<br />a∈P{(a,0)}<br />Algorithm 2 Recursive Objective Space Partitioning<br />Require: current fitness assignment F, reference set R ⊆ Z,<br />fitness parameter k ∈ N, recursion level i, partial volume<br />V ∈ R, scan positions (z1,...,zn) ∈ Rn<br />1: procedure doSlicing(F, R, k, i, V , (z1,...,zn))<br />2:<br />/∗ filter out relevant solutions and reference points ∗/<br />3:<br />4:<br />5:<br />if i = 0 ∧ UR ?= ∅ then<br />6:<br />/∗ end of recursion reached ∗/<br />7:<br />j=1<br />(k − j)/(|F| − j)<br />8:<br />/∗ update hypervolumes of filtered solutions ∗/<br />9:<br />F?← ∅<br />10:<br />for all (a,v) ∈ F do<br />11:<br />if ∀1 ≤ j ≤ n : fj(a) ≤ zj then<br />12:<br />F?← F?∪ {(a,v +<br />13:<br />else<br />14:<br />F?← F?∪ {(a,v)}<br />15:<br />end if<br />16:<br />end for<br />17:<br />else if i &gt; 0 then<br />18:<br />/∗ recursion continues ∗/<br />19:<br />F?← F<br />20:<br />U ← UP ∪ UR<br />21:<br />/∗ scan current dimension in ascending order ∗/<br />22:<br />while U ?= ∅ do<br />23:<br />u∗← min(u1,...,un)∈Uui<br />24:<br />U?← {(u1,...,un) ∈ U |ui&gt; u∗}<br />25:<br />if U??= ∅ then<br />26:<br />27:<br />F?← doSlicing(F?, R, k, i − 1, V?,<br />28:<br />(z1,...,zi−1,u∗,zi+1,...,zn) )<br />29:<br />end if<br />30:<br />U = U?<br />31:<br />end while<br />32:<br />end if<br />33:<br />return F?<br />34: end procedure<br />UP ←?<br />(a,v)∈F, ∀i&lt;j≤n: fj(a)≤zj{f(a)}<br />(r1,...,rn)∈R, ∀i&lt;j≤n: rj≥zj{(r1,...,rn)}<br />UR ←?<br />α ←?|UP|−1<br />α<br />|UP|V )}<br />V?= V ·?(min(u?<br />1,...,u?<br />n)∈U? u?<br />i) − u∗?<br />depicted in Fig. 5 where the population contains four<br />solutions a,b,c,d the objective vectors of which are<br />f(a)=(−10,−3,−2),f(b)<br />(−6,−8,−10),f(d) = (−4,−5,−11) and the reference<br />set includes two points r = (−2,0,0),s = (0,−3,−4).<br />Furthermore, let the parameter k be 2.<br />In the first call of doSlicing, it holds i = 3 and U contains<br />all objective vectors associated with the population and all<br />reference points. The following representation shows U with<br />=(−8,−1,−8),f(c)=<br />7</p>  <p>Page 8</p> <p>r<br />s<br />r<br />s<br />0<br />?<br />s<br />r<br />3f<br />1f<br />2f<br />s<br />s<br />r<br />( )<br />f a<br />( )<br />f a<br />( )<br />f b<br />( )<br />f b<br />( )<br />f b<br />( )<br />f b<br />( )<br />f c<br />( )<br />f c<br />( )<br />f c<br />( )<br />f c<br />( )<br />f d<br />( )<br />f d<br />( )<br />f d<br />( )<br />f d<br />Figure 5.<br />one looks from (∞,−∞,∞) to the origin. First, the dominated polytope is cut along the third dimension leading to five slices, which are again cut along<br />the second dimension and finally along the first dimension. In contrast to existing ’Hypervolume by Slicing Objectives’ algorithms, also dominated points are<br />carried along.<br />Illustration of the principle underlying Algorithm 2 where one looks from (−∞,−∞,−∞) on the front except for the lower left picture where<br />its elements sorted in ascending order according to their third<br />vector components:<br />U<br />=<br />f(d) :<br />f(c) :<br />f(b) :<br />(−4,−5,−11) ↓<br />(−6,−8,−10)<br />(−8,−1,−8)<br />(−0,−3,−4)<br />(−10,−3,−2)<br />(−2,0,0)<br />s :<br />f(a) :<br />r :<br />(14)<br />Hence, in the first two iterations of the loop beginning at<br />Line 22 the variable u∗is assigned to f3(d) = −11 resp.<br />u∗= f3(c) = −10. Within the third iteration, U is reduced to<br />{f(a),f(b),r,s} which yields u∗= f3(b) = −8 and in turn<br />V?= 1 · (−4 − (−8)) = 4 with the current vector of scan<br />positions being (z1,z2,z3) = (∞,∞,−8); these values are<br />passed to the next recursion level i = 2 where U is initialized<br />at Line 20 as follows (this time sorted according to the second<br />dimension):<br />U<br />=<br />f(c) :<br />f(d) :<br />(−6,−8,−10) ↓<br />(−4,−5,−11)<br />(0,−3,−4)<br />(−8,−1,−8)<br />(−2,0,0)<br />s :<br />f(b) :<br />r :<br />(15)<br />Now, after three iterations of the loop at Line 22 with<br />u∗= f2(c) = −8, u∗= f2(d) = −5, and u∗= s2= −3, re-<br />spectively, U is reduced in the fourth iteration to {f(b),r} and<br />u∗is set to f2(b) = −1. As a result, V?= 1·4·(0−(−1)) = 4<br />and (z1,z2,z3) = (∞,−1,−8) which are the parameters for<br />the next recursive invocation of doSlicing where U is set to:<br />U<br />=<br />f(b) :<br />f(c) :<br />f(d) :<br />(−8,−1,−8) ↓<br />(−6,−8,−10)<br />(−4,−5,−11)<br />(−2,0,0)<br />r :<br />(16)<br />At this recursion level with i = 1, in the second iteration it<br />holds u∗= f1(c) = −6 and V?= 1· 4· 1· (−4− (−6)) = 8.<br />When calling doSlicing at this stage, the last recursion level<br />is reached (i = 0): First, α is computed based on the<br />population size N = 4, the number of individuals dominating<br />the hyperrectangle (|UP| = 2), and the fitness parameter<br />k = 2, which yields α = 1/3; then for b and c, the fitness<br />values are increased by adding α · V/|UP| = 4/3.<br />Applying this procedure to all slices at a particular recur-<br />sion level identifies all hyperrectangles which constitute the<br />portion of the objective space enclosed by the population and<br />the reference set.<br />IV. ESTIMATING HYPERVOLUME CONTRIBUTIONS USING<br />CARLO SIMULATION<br />As outlined above, the computation of the proposed<br />hypervolume-based fitness scheme is that expensive that only<br />problems with at maximum four or five objectives are tractable<br />within reasonable time limits. However, in the context of<br />randomized search heuristics one may argue that the exact<br />fitness values are not crucial and approximated values may<br />be sufficient; furthermore, if using pure rank-based selection<br />schemes, then only the resulting order of the individuals<br />matters. These considerations lead to the idea of estimating<br />the hypervolume contributions by means of Monte Carlo<br />simulation.<br />The basic principle is described and investigated in the<br />following subsection, while more advanced sampling strategies<br />8</p>  <p>Page 9</p> <p>that automatically adjust the number of samples to be drawn<br />are discussed in the second part of this section.<br />A. Basic Concept<br />To approximate the fitness values according to Defini-<br />tion III.6, we need to estimate the Lebesgue measures of the<br />domains Hi(a,P,R) where P ∈ Ψ is the population. Since<br />these domains are all integrable, their Lebesgue measure can<br />be approximated by means of Monte Carlo simulation.<br />For this purpose, a sampling space S ⊆ Z has to be<br />defined with the following properties: (i) the hypervolume<br />of S can easily be computed, (ii) samples from the space<br />S can be generated fast, and (iii) S is a superset of the<br />domains Hi(a,P,R) the hypervolumes of which one would<br />like to approximate. The latter condition is met by setting<br />S = H(P,R), but since it is hard both to calculate the<br />Lebesgue measure of this sampling space and to draw samples<br />from it, we propose using the axis-aligned minimum bounding<br />box containing the Hi(a,P,R) subspaces instead, i.e.:<br />S := {(z1,...,zn) ∈ Z |∀1 ≤ i ≤ n : li≤ zi≤ ui}<br />where<br />li<br />:=mina∈Pfi(a)<br />ui<br />:=max(r1,...,rn)∈Rri<br />for 1 ≤ i ≤ n. Hence, the volume V of the sampling space S<br />is given by V =?n<br />objective vectors s1,...,sM from S uniformly at random.<br />For each sj it is checked whether it lies in any partition<br />Hi(a,P,R) for 1 ≤ i ≤ k and a ∈ P. This can be determined<br />in two steps: first, it is verified that sjis ’below’ the reference<br />set R, i.e., it exists r ∈ R that is dominated by sj; second,<br />it is verified that the multiset A of those population members<br />dominating sj is not empty. If both conditions are fulfilled,<br />then we know that—given A—the sampling point sj lies in<br />all partitions Hi(a,P,R) where i = |A| and a ∈ A. This<br />situation will be denoted as a hit regarding the ith partition of<br />a. If any of the above two conditions is not fulfilled, then we<br />call sj a miss. Let X(i,a)<br />j<br />denote the corresponding random<br />variable that is equal to 1 in case of a hit of sj regarding the<br />ith partition of a and 0 otherwise.<br />Based on the M sampling points, we obtain an estimate<br />for λ(Hi(a,P,R)) by simply counting the number of hits and<br />multiplying the hit ratio with the volume of the sampling box:<br />(17)<br />(18)<br />i=1max{0,ui− li}.<br />Now given S, sampling is carried out by selecting M<br />ˆλ?Hi(a,P,R)?=<br />?M<br />j=1X(i,a))<br />M<br />j<br />· V<br />(19)<br />This value approaches the exact value λ(Hi(a,P,R)) with<br />increasing M by the law of large numbers. Due to the linearity<br />of the expectation operator, the fitness scheme according to<br />Eq. (11) can be approximated by replacing the Lebesgue<br />measure with the respective estimates given by Eq. (19):<br />ˆIk<br />h(a,P,R) =<br />k<br />?<br />i=1<br />αi<br />i<br />·<br />??M<br />j=1X(i,a))<br />M<br />j<br />V<br />?<br />(20)<br />Algorithm 3 Hypervolume-based Fitness Value Estimation<br />Require: population P ∈ Ψ, reference set R ⊆ Z, fitness<br />parameter k ∈ N, number of sampling points M ∈ N<br />1: procedure estimateHypervolume(P, R, k, M)<br />2:<br />/∗ determine sampling box S ∗/<br />3:<br />for i ← 1,n do<br />4:<br />li= mina∈Pfi(a)<br />5:<br />ui= max(r1,...,rn)∈Rri<br />6:<br />end for<br />7:<br />S ← [l1,u1] × ··· × [ln,un]<br />8:<br />9:<br />/∗ reset fitness assignment ∗/<br />10:<br />11:<br />/∗ perform sampling ∗/<br />12:<br />for j ← 1,M do<br />13:<br />choose s ∈ S uniformly at random<br />14:<br />if ∃r ∈ R : s ≤ r then<br />15:<br />16:<br />if |UP| ≤ k then<br />17:<br />/∗ hit in a relevant partition ∗/<br />18:<br />l=1<br />19:<br />/∗ update hypervolume estimates ∗/<br />20:<br />F?← ∅<br />21:<br />for all (a,v) ∈ F do<br />22:<br />if f(a) ≤ s then<br />23:<br />F?← F?∪ {(a,v +<br />24:<br />else<br />25:<br />F?← F?∪ {(a,v)}<br />26:<br />end if<br />27:<br />end for<br />28:<br />F ← F?<br />29:<br />end if<br />30:<br />end if<br />31:<br />end for<br />32:<br />return F<br />33: end procedure<br />V ←?n<br />F ←?<br />i=1max{0,(ui− li)}<br />a∈P{(a,0)}<br />UP ←?<br />α ←?|UP|−1<br />a∈P, f(a)≤s{f(a)}<br />k−l<br />|P|−l<br />α<br />|UP|·V<br />M)}<br />The details of estimation procedure are described by Algo-<br />rithm 3 which returns a fitness assignment, i.e., for each<br />a ∈ P the corresponding hypervolume estimateˆIk<br />will be later used by the evolutionary algorithm presented in<br />Section V. Note that the partitions Hi(a,P,R) with i &gt; k do<br />not need to be considered for the fitness calculation as they do<br />not contribute to the Ik<br />cf. Def. III.6.<br />In order to study how closely the sample size M and the<br />accuracy of the estimates is related, a simple experiment was<br />carried out: ten imaginary individuals a ∈ A were generated,<br />the objective vectors f(a) of which are uniformly distributed<br />at random on a three dimensional unit simplex, similarly to<br />the experiments presented in Table II. These individuals were<br />then ranked on the one hand according to the estimatesˆI|A|<br />and on the other hand with respect to the exact values I|A|<br />The closer the former ranking is to the latter ranking, the<br />higher is the accuracy of the estimation procedure given by<br />Algorithm 3. To quantify the differences between the two<br />h(a,P,R). It<br />hvalues that we would like to estimate,<br />h<br />h.<br />9</p>  <p>Page 10</p> <p>Table III<br />ACCURACY OF THE RANKING OF 10 INDIVIDUALS ACCORDING TOˆI10<br />(20) IN COMPARISON TO I10<br />h<br />PERCENTAGES REPRESENT THE NUMBER OF PAIRS OF INDIVIDUALS<br />RANKED CORRECTLY.<br />h<br />FOR DIFFERENT SAMPLE SIZES. THE<br />number of samples M<br />ranking accuracy<br />101<br />102<br />103<br />104<br />105<br />106<br />107<br />56.0%<br />74.1%<br />89.9%<br />96.9%<br />99.2%<br />99.8%<br />100.0 %<br />rankings, we calculated the percentage of all pairs (i,j) with<br />1 ≤ i &lt; j ≤ |A| where the individuals at the ith position<br />and the jth position in the ranking according to I|A|<br />same order in the ranking according toˆI|A|<br />experiment was repeated for different numbers of sampling<br />points as shown in Table III. The experimental results indicate<br />that 10.000 samples are necessary to achieve an error below<br />5% and that 10.000.000 sampling point are sufficient in this<br />setting to obtain the exact ranking.<br />h<br />have the<br />h, see [31]. The<br />B. Adaptive Sampling<br />Seeing the close relationship between sample size and ac-<br />curacy, one may ask whether M can be adjusted automatically<br />on the basis of confidence intervals. That means sampling is<br />stopped as soon as the statistical confidence in the estimated<br />fitness values reaches a prespecified level. The hope is that<br />thereby the estimation is less sensitive to the choice of M and<br />that the number of drawn samples can be reduced.<br />Using the normal approximation of the distribution of an<br />estimateˆIk<br />that the true value Ik<br />h(a,A,R), we can state there is a probability of L,<br />h(a,A,R) is in<br />Ik<br />h(a,A,R) ∈ˆIk<br />h(a,A,R) ± z1/2+L/2<br />?<br />?<br />Var?ˆIk<br />h(a,A,R)?<br />(21)<br />where zβdenotes the β-percentile of a standard normal distri-<br />bution and?<br />Based on this confidence interval, we can derive a lower<br />bound for the probability Cw, that the individual with the<br />smallest estimated contributionˆIk<br />the hypervolume (see Appendix B1a). Let A = {a1,...,a|A|}<br />with a1≤ ai∀i ∈ {1,...,|A|}, then<br />Var?ˆIk<br />h(a,A,R)?<br />denotes the estimated variance<br />of the estimate according to (30), see Appendix B for details.<br />h(a,A,R) contributes least to<br />Cw≥ 1 − (1 − L)/2 −<br />|A|<br />?<br />i=2<br />P?Ik<br />h(ai,A,R) &gt; B?<br />(22)<br />where B denotes the upper end of confidence interval (21)<br />for confidence level (L + 1)/2. Similiarly, a lower bound for<br />the confidence Cr, that the ranking of individuals is correct<br />follows as<br />Cr≥ 1 −<br />|A|−1<br />?<br />|A|<br />?<br />i=1<br />P(Ik<br />h(ai,A,R) &lt; Bi)<br />−<br />i=2<br />P(Ik<br />h(ai,A,R) &gt; Bi−1)<br />(23)<br />(see Appendix B1c for details, including the meaning of Bi).<br />The lower bound (22) can be used to limit the number of<br />samples used to estimate Ik<br />selection step, and (23) to limit the number of samples in<br />the mating selection. Algorithm 4 gives an adaptive sam-<br />pling procedure—based on the confidence levels—that re-<br />sembles Algorithm 3. In contrast to the latter, the elements<br />(a,v,(h1,...,h|P|)) of F take a third component that records<br />the number of hits hi per partition Hi(a,A,R), which are<br />needed to calculate the confidence level.<br />In the following we investigate to what extend the adaptive<br />adjustment of sampling size reduces the total number of<br />samples needed. For this purpose, assume from a Pareto-front<br />approximation A, some individuals are removed one by one.<br />In each removal step, the confidence level Cw is calculated<br />after a certain number of samples Θ are drawn, here set to<br />t = 100. If Cw exceeds a user defined level L, then the<br />adaptive sampling stops. Otherwise it continues, periodically<br />recalculating the confidence level after Θ samples until either<br />the confidence exceeds L or the maximum number of samples<br />Mmax is reached.<br />We consider the following scenario: 20 individuals a ∈ A<br />are generated whose objective values are set uniformly at<br />random on a 3 dimensional unit simplex. From these 20<br />individuals, 10 are removed one by one based on the smallest<br />indicator value Ik<br />|A| − 10. The resultant set of 10 individuals Aref acts as<br />reference to assess the quality of the two sampling strategies.<br />The same procedure is then carried out for the estimated<br />indicatorˆIk<br />as well as using an adaptive number of samples according<br />to Algorithm 4. In the latter case, the maximum number of<br />samples Mmax is set to M and the desired confidence level<br />L is set to 0.99. The quality of the resulting set Aadaptiveis<br />assessed by counting the percentage of individuals which are<br />in Aadaptivebut not in Aref, i.e., |Aref∩Aadaptive|/|Aref|. In<br />the same way the percentage is calculated for the set Aconstant<br />that results from applying the constant sampling strategy.<br />Figure 6 shows the percentages obtained for both the<br />constant and adaptive sampling approach. For small maximum<br />number of samples Mmax, the adaptive sampling algorithm<br />hardly ever reaches the desired confidence level L. Hence,<br />both the number of samples and ranking accuracy is similar<br />to the constant approach. However, as the maximum sample<br />size increases, the more often L is reached. In this case, the<br />number of samples is only half the number needed by the<br />algorithm based on the constant sampling strategy while the<br />accuracy of the estimate is only marginaly affected. Since the<br />confidence level is set to a relatively high value, the accuracy<br />h(a,A,R) in the environmental<br />h(a,A,R) with R = (2,2,2) and k =<br />h(a,A,R) using a constant number of samples M<br />10</p>  <p>Page 11</p> <p>is only affected very slightly. This indicates that using adaptive<br />sampling might be beneficial to speed up sampling when the<br />number of samples is large.<br />102103104 105 106107<br />0.93<br />0.95<br />0.97<br />0.99<br />total number of samples<br />correct decisions<br />adaptive sampling<br />constant sampling<br />related sample sizes<br />Figure 6.<br />for the constant and adaptive sampling strategy. As the grey arrows indicate,<br />the latter uses less samples than the constant approach if the sample size is<br />large enough. On the other hand, the two approaches differ only slightly when<br />few samples are used.<br />Ranking accuracy with increasing (maximum) number of samples<br />V. HYPE: HYPERVOLUME ESTIMATION ALGORITHM FOR<br />MULTIOBJECTIVE OPTIMIZATION<br />In this section, we describe an evolutionary algorithm<br />named HypE (Hypervolume Estimation Algorithm for Mul-<br />tiobjective Optimization) which is based on the fitness assign-<br />ment schemes presented in the previous sections. When the<br />number of objectives is small (≤ 3), the hypervolume values<br />Ik<br />are estimated based on Algorithm 3.<br />The main loop of HypE is given by Algorithm 5. It<br />reflects a standard evolutionary algorithm and consists of<br />the successive application of mating selection (Algorithm 6),<br />variation, and environmental selection (Algorithm 7). As to<br />mating selection, binary tournament selection is proposed here,<br />although any other selection scheme could be used as well. The<br />procedure variation encapsulates the application of mutation<br />and recombination operators to generate N offspring. Finally,<br />environmental selection aims at selecting the most promising<br />N solutions from the multiset-union of parent population<br />and offspring; more precisely, it creates a new population by<br />carrying out the following two steps:<br />hare computed exactly using Algorithm 1, otherwise they<br />1) First, the union of parents and offspring is divided into<br />disjoint partitions using the principle of nondominated<br />sorting [20], [13], also known as dominance depth. Start-<br />ing with the lowest dominance depth level, the partitions<br />are moved one by one to the new population as long<br />as the first partition is reached that cannot be transfered<br />completely. This corresponds to the scheme used in most<br />hypervolume-based multiobjective optimizers [15], [23],<br />[9].<br />Algorithm 4 Hypervolume-based Fitness Value Estimation<br />With Adaptive Sampling<br />Require: population P ∈ Ψ, reference set R ⊆ Z, fitness<br />parameter k ∈ N, maximum number of sampling points<br />Mmax∈ N, desired confidence L, sampling interval Θ<br />1: procedure estimateHypervolumeAS(P, R, k, Mmax, L)<br />2:<br />/∗ determine sampling box S ∗/<br />3:<br />for i ← 1,n do<br />4:<br />li= mina∈Pfi(a)<br />5:<br />ui= max(r1,...,rn)∈R ri<br />6:<br />end for<br />7:<br />S ← [l1,u1] × ··· × [ln,un]<br />8:<br />9:<br />/∗ reset fitness assignement<br />10:<br />third component: number of hits per Hi(a,P,R) ∗/<br />11:<br />12:<br />/∗ compute α-vector according to Equation (13) ∗/<br />13:<br />α ← (0,...,0)<br />14:<br />for i ← 1,k do<br />15:<br />l=1<br />|A|−l<br />16:<br />end for<br />17:<br />/∗ perform sampling ∗/<br />18:<br />C ← 0 /∗ either Cw(env. sel.) or Cr(mat. sel) ∗/<br />19:<br />j ← 1<br />20:<br />while j ≤ Mmaxand C &lt; L do<br />21:<br />choose s ∈ S uniformly at random<br />22:<br />if ∃r ∈ R : s ≤ r then<br />23:<br />F?← ∅<br />24:<br />/∗ update estimates and hit counts ∗/<br />25:<br />for all (a,v,(h1,...,h|P|)) ∈ F do<br />26:<br />if ∀1 ≤ j ≤ n : fj(a) ≤ sj then<br />27:<br />(h?<br />28:<br />h?<br />29:<br />F?← F?∪ {(a,v +αA<br />30:<br />31:<br />else<br />32:<br />F?← F?∪ {(a,v,(h1,...,h|P|))}<br />33:<br />end if<br />34:<br />end for<br />35:<br />F ← F?<br />36:<br />end if<br />37:<br />/∗ Recalculate confidence level reached ∗/<br />38:<br />if<br />mod (j,Θ) = 0 then<br />39:<br />C ← confidence according to (22) or (23)<br />40:<br />end if<br />41:<br />end while<br />42:<br />return F<br />43: end procedure<br />V ←?n<br />F ←?<br />i=1max{0,(ui− li)}<br />a∈P{(a,0,(0,...,0))}<br />αi←?k−1<br />k−l<br />1,...,h?<br />|A|← h?<br />|P|) ← (h1,...,h|P|)<br />|A|+ 1<br />|A|·V<br />(h?<br />M,<br />1,...,h?<br />|P|))}<br />2) The partition that only fits partially into the new pop-<br />ulation is then processed using the method presented<br />in Section III-B. In each step, the fitness values for<br />the partition under consideration are computed and the<br />individual with the worst fitness is removed—if multiple<br />individuals share the same minimal fitness, then one of<br />them is selected uniformly at random. This procedure<br />11</p>  <p>Page 12</p> <p>Algorithm 5 HypE Main Loop<br />Require: reference set R ⊆ Z, population size N ∈ N,<br />number of generations gmax, number of sampling points<br />M ∈ N<br />1: initialize population P by selecting N solutions from X<br />uniformly at random<br />2: g ← 0<br />3: while g ≤ gmaxdo<br />4:<br />P?← matingSelection(P,R,N,M)<br />5:<br />P??← variation(P?,N)<br />6:<br />P ← environmentalSelection(P ∪ P??,R,N,M)<br />7:<br />g ← g + 1<br />8: end while<br />is repeated until the partition has been reduced to the<br />desired size, i.e., until it fits into the remaining slots left<br />in the new population.<br />Concerning the fitness assignment, the number of objectives<br />determines whether the exact or the estimated Ik<br />are considered. If less than four objectives are involved,<br />we recommend to employ Algorithm 1, otherwise to use<br />Algorithm 3. The latter works with a fixed number of sampling<br />points to estimate the hypervolume values Ik<br />confidence of the decision to be made; hence, the variance of<br />the estimates does not need to be calculated and it is sufficient<br />to update for each sample drawn an array storing the fitness<br />values of the population members.<br />Instead of Algorithm 3, one may also apply the adaptive<br />sampling routine described in Algorithm 4 when estimating<br />the hypervolume contributions. To this end, the variance of the<br />estimates is calculated after a certain number of initial samples<br />and from this, the confidence level is determined. If this lies<br />below a user-defined level, then the sampling process contin-<br />ues. Since this process can last arbitrarily long (the difference<br />between the hypervolume contributions of two solutions can<br />be arbitrarily small), an upper bound for the maximal number<br />of samples Mmaxhas to be defined. If this number is reached,<br />a decision is made based on the current estimates regardless<br />of the confidence level. The main advantage of this adaptive<br />procedure is that it is robust with respect to the choice of the<br />sample size M. Its main disadvantage is the need to store for<br />each population member the number of hits in all domains<br />Hi(a,P,R), which slows down the sampling considerably, as<br />will be shown in Section VI-C.<br />hvalues<br />h, regardless of the<br />VI. EXPERIMENTS<br />This section serves two goals: (i) to investigate the influence<br />of specific algorithmic concepts (fitness, sample size, adaptive<br />sampling) on the performance of HypE, and (ii) to study the<br />effectiveness of HypE in comparison to existing MOEAs. A<br />difficulty that arises in this context is how to statistically com-<br />pare the quality of Pareto-set approximations with respect to<br />the hypervolume indicator when a large number of objectives<br />(n ≥ 5) is considered. In this case, exact computation of<br />the hypervolume becomes infeasible; to this end, we propose<br />Algorithm 6 HypEMating Selection<br />Require: population P ∈ Ψ, reference set R ⊆ Z, number of<br />offspring N ∈ N, number of sampling points M ∈ N<br />1: procedure matingSelection(P,R,N,M)<br />2:<br />if n ≤ 3 then<br />3:<br />F ← computeHypervolume(P,R,N)<br />4:<br />else<br />5:<br />F ← estimateHypervolume(P,R,N,M)<br />6:<br />end if<br />7:<br />Q ← ∅<br />8:<br />while |Q| &lt; N do<br />9:<br />choose (a,va),(b,vb) ∈ F uniformly at random<br />10:<br />if va&gt; vbthen<br />11:<br />Q ← Q ∪ {a}<br />12:<br />else<br />13:<br />Q ← Q ∪ {b}<br />14:<br />end if<br />15:<br />end while<br />16:<br />return Q<br />17: end procedure<br />Monte Carlo sampling using appropriate statistical tools as<br />detailed below.<br />A. Experimental Setup<br />HypE is implemented within the PISA framework [6] and<br />tested in two versions: the first (HypE) uses fitness-based<br />mating selection as described in Algorithm 6, while the second<br />(HypE*) employs a uniform mating selection scheme where<br />all individuals have the same probability of being chosen<br />for reproduction. Unless stated otherwise, for sampling the<br />number of sampling points is fixed to M = 10,000 and<br />Mmax = 20,000 respectively, both kept constant during a<br />run.<br />HypE and HypE* are compared to three popular MOEAs,<br />namely NSGA-II [13], SPEA2 [42], and IBEA (in combination<br />with the ε-indicator) [41]. Since these algorithms are not<br />designed to optimize the hypervolume, it cannot be expected<br />that they perform particularly well when measuring the quality<br />of the approximation in terms of the hypervolume indicator.<br />Nevertheless, they serve as an important reference as they are<br />considerably faster than hypervolume-based search algorithms<br />and therefore can execute a substantially larger number of gen-<br />erations when keeping the available computation time fixed.<br />On the other hand, dedicated hypervolume-based methods are<br />included in the comparisons. The algorithms proposed in [15],<br />[23], [9] use the same fitness assignment scheme which can be<br />mimicked by means of a HypE variant that only uses the I1<br />values for fitness assignment, i.e., k is set to 1, and employs the<br />routine for exact hypervolume calculation (Algorithm 1). We<br />will refer to this approach as RHV (regular hypervolume-based<br />algorithm)—the acronym RHV* stands for the variant that<br />uses uniform selection for mating. However, we do not provide<br />comparisons to the original implementations of [15], [23], [9]<br />because the focus is on the fitness assignment principles and<br />h<br />12</p>  <p>Page 13</p> <p>Algorithm 7 HypE Environmental Selection<br />Require: population P ∈ Ψ, reference set R ⊆ Z, number of<br />offspring N ∈ N, number of sampling points M ∈ N<br />1: procedure environmentalSelection(P,R,N,M)<br />2:<br />P?← P /∗ remaining population members ∗/<br />3:<br />Q ← ∅ /∗ new population ∗/<br />4:<br />Q?← ∅ /∗ current nondominated set ∗/<br />5:<br />/∗ iteratively copy nondominated sets to Q ∗/<br />6:<br />repeat<br />7:<br />Q ← Q ∪ Q?<br />8:<br />/∗ determine current nondominated set in P?∗/<br />9:<br />Q?,P??← ∅<br />10:<br />for all a ∈ P?do<br />11:<br />if ∀b ∈ P?: b ? a ⇒ a ? b then<br />12:<br />Q?← Q?∪ {a}<br />13:<br />else<br />14:<br />P??← P??∪ {a}<br />15:<br />end if<br />16:<br />end for<br />17:<br />P?← P??<br />18:<br />until |Q| + |Q?| ≥ N ∨ P?= ∅<br />19:<br />/∗ truncate last non-fitting nondominated set Q?∗/<br />20:<br />k = |Q| + |Q?| − N<br />21:<br />while k &gt; 0 do<br />22:<br />if n ≤ 3 then<br />23:<br />F ← computeHypervolume(Q?,R,k)<br />24:<br />else<br />25:<br />F ← estimateHypervolume(Q?,R,k,M)<br />26:<br />end if<br />27:<br />/∗ remove worst solution from Q?∗/<br />28:<br />Q?← ∅<br />29:<br />removed ← false<br />30:<br />for all (a,v) ∈ F do<br />31:<br />if removed = true ∨ v ?= min(a,v)∈F{v} then<br />32:<br />Q?← Q?∪ {a}<br />33:<br />else<br />34:<br />removed ← true<br />35:<br />end if<br />36:<br />end for<br />37:<br />k ← k − 1<br />38:<br />end while<br />39:<br />Q ← Q ∪ Q?<br />40:<br />return Q<br />41: end procedure<br />not on specific data structures for fast hypervolume calculation<br />as in [15] or specific variation operators as in [23]. Further-<br />more, we consider the sampling-based optimizer proposed<br />in [1], here denoted as SHV (sampling-based hypervolume-<br />oriented algorithm); it more or less corresponds to RHV<br />with adaptive sampling. Finally, to study the influence of<br />the nondominated sorting we also include a simple HypE<br />variant named RS (random selection) where all individuals<br />are assigned the same constant fitness value. Thereby, the<br />selection pressure is only maintained by the nondominated<br />sorting carried out during the environmental selection phase.<br />As basis for the comparisons, the DTLZ [14], the WFG [21],<br />and the knapsack [45] test problem suites are considered since<br />they allow the number of objectives to be scaled arbitrarily—<br />here, ranging from 2 to 50 objectives. For the DTLZ problem,<br />the number of decision variables is set to 300, while for the<br />WFG problems individual values are used, see Table IV. As<br />to the knapsack problem, we used 400 items which where<br />modified with mutation probability 1 by one-bit mutation<br />and by one-point crossover with probability 0.5. For each<br />benchmark function, 30 runs are carried out per algorithm<br />using a population size of N = 50 and a maximum number<br />gmax = 200 of generations (unless the computation time is<br />fixed). The individuals are represented by real vectors, where<br />a polynomial distribution is used for mutation and the SBX-<br />20 operator for recombination [12]. The recombination and<br />mutation probabilities are set according to [14].<br />B. Statistical Comparison Methodology<br />The quality of the Pareto-set approximations are assessed<br />using the hypervolume indicator, where for less than 6 objec-<br />tives the indicator values are calculated exactly and otherwise<br />approximated by Monte Carlo sampling as described in [2].<br />When sampling is used, uncertainty of measurement is in-<br />troduced which can be expressed by the standard deviation<br />of the sampled value u(ˆIH(A,R)) = IH(A,R)?p(1 − p)/n,<br />andˆIH the hypervolume estimate. Unless otherwise noted,<br />1,000,000 samples are used per Pareto-set approximation.<br />For a typical hit probability between 10% to 90% observed,<br />this leads to a very small uncertainty below 10−3in relation<br />to IH. Therefore, it is highly unlikely that the uncertainty<br />will influence the statistical test applyied to the hypervolume<br />estimates and if it does nonetheless, the statistical tests become<br />over-conservative. Hence, we do not consider uncertainty in<br />the following tests.<br />Let Aiwith 1 ≤ i ≤ l denote the algorithms to be compared.<br />For each algorithm Ai, the same number r of independent<br />runs are carried out for 200 generations. For formal reason,<br />the null hypothesis that all algorithms are equally well suited<br />to approximate the Pareto-opimal set is investigated first, using<br />the Kruskal-Wallis test at a significance level of α = 0.01 [10].<br />This hypothesis could be rejected in all test cases described<br />below. Thereafter, for all pairs of algorithms the difference in<br />median of the hypervolume is compared.<br />To test the difference for significance, the Conover-Inman<br />procedure is applied with the same α level as in the Kruskal-<br />Wallis test [10]. Let δi,jbe 1, if Aiturns out to be significantly<br />better than Aj and 0 otherwise. Based on δi,j, for each<br />algorithm Ai the performance index P(Ai) is determined as<br />follows:<br />where p denotes the hit probability of the sampling process<br />P(Ai) =<br />l<br />?<br />j=1<br />j=i<br />δi,j<br />(24)<br />This value reveals how many other algorithms are better<br />than the corresponding algorithm on the specific test case.<br />The smaller the index, the better the algorithm; an index of<br />13</p>  <p>Page 14</p> <p>Table IV<br />NUMBER OF DECISION VARIABLES AND THEIR DECOMPOSITION INTO POSITION AND DISTANCE VARIABLES AS USED FOR THE WFG TEST FUNCTIONS<br />DEPENDING ON THE NUMBER OF OBJECTIVES.<br />Objective Space Dimensions (n)<br />2d3d 5d7d10d 25d50d<br />distance parameters<br />position parameters<br />20204258<br />12<br />5076<br />24<br />150<br />494489<br />decision variables242450 7059 100199<br />zero means that no other algorithm generated significantly<br />better Pareto-set approximations in terms of the hypervolume<br />indicator.<br />C. Results<br />In the following, we discuss the experimental results<br />grouped according to the foci of the investigations.<br />1) Constant Versus Adaptive Sampling: First, the issue of<br />adaptive sampling is examined. We compare HypE with its<br />counterpart that uses the adaptive sampling strategy described<br />in Algorithm 4 in both environmental selection (employ-<br />ing Equation (22)) and mating selection (emplyoing Equa-<br />tion (23)) with confidence level 0.1 and maximal number of<br />samples of M = 20,000. As mentioned above, standard HypE<br />is run with a sample size of M = 10,000 and a constant<br />number of 200 generations.<br />The experiments indicate that for the 17 test problems each<br />instantiated with 5, 7, 10, 25, and 50 objectives, the adaptive<br />strategy beats the constant sampling method in 12 cases, which<br />is vice versa in 10 cases better than adaptive sampling. In the<br />remaining 63 instances, the two versions of HypE do not differ<br />significantly in terms of the hypervolumes achieved.<br />While both strategy show about the same performance, it is<br />expected that adaptive sampling needs less computation time<br />because fewer samples are used. However, this is not the case:<br />the adaptive sampling takes between 2.4 and 5.2 times longer<br />than its counterpart, the difference increasing with the number<br />of objectives. Two main reasons can be mentioned to explain<br />the additional computation time:<br />• The confidence levels are seldom reached. This means<br />the maximum number of samples are drawn with the<br />overhead of calculating the variance of the points from<br />time to time. Figure 7 shows the average number of<br />samples drawn for the DTLZ 2 test problem with 5<br />objectives. While at the initial stage of the run about<br />50% of the maximum number of samples are used,<br />the percentage steadily increases to over 95% after 100<br />generations.<br />• Calculating the variances requires to take track of the<br />hits in each partition Hi(a,A,R) for every population<br />member a. This slows down the sampling process con-<br />siderably.<br />These observations lead to the conclusion that the benefits of<br />adaptive sampling are mainly compensated by the additional<br />computational overhead. For this reason, only HypE with a<br />0 20 40 6080 100120140 160 180200<br />8<br />10<br />12<br />14<br />16<br />18<br />20x 103<br />generation<br />number of samples per removal<br />Figure 7.<br />HypE with significance level 0.05 and a maximum of 20,000 samples. The<br />testproblem is DTLZ 2 with 5 objectives and the average is based on 30 runs.<br />Average number of samples used per removal for adaptive<br />constant sample size is considered in the following compar-<br />isons. Future research may be necessary to investigate how<br />adaptivity could be integrated more efficiently. For instance,<br />as Figure 7 indicates, one could use adaptive sampling in an<br />early stage of the evolutionary run, say in the first twenty<br />generations, and then switch to constant sampling. Further-<br />more, adaptive sampling might pay off when the user defined<br />confidence L is close to 1 and large number of samples need<br />to be drawn.<br />2) Exact Hypervolume Computation Versus Sampling:<br />Next, we compare HypE with RHV—due to the large com-<br />putation effort caused by the exact hypervolume calculation<br />only on a single test problem, namely DTLZ2 with 2, 3, 4,<br />and 5 objectives. Both HypE and HypE* are run with exact<br />fitness calculation (Algorithm 1) as well as with the estimation<br />procedure (Algorithm 3); the former variants are marked with<br />a trailing ’e’, while the latter variants are marked with a trailing<br />’-s’. All algorithms run for 200 generations, per algorithm 30<br />runs were performed.<br />Figure 8 shows the hypervolume values normalized for<br />each test problem instance separately. As one may expect,<br />HypE beats HypE*. Moreover, fitness-based mating selection<br />is beneficial to both HypE and RHV. The two best variants,<br />HypE-e and RHV, reach about the same hypervolume values,<br />independently of the number of objectives. Although HypE<br />reaches a better hypervolume median for all four number<br />of objectives, the difference is never significant<br />5. . Hence,<br />5According to the Kruskal-Wallis test described in Section VI-B with<br />confidence level 0.01.<br />14</p>  <p>Page 15</p> <p>235710 2550<br />0<br />1<br />2<br />3<br />4<br />5<br />6<br />dimension<br />mean performance<br />HypE<br />HypE*<br />IBEA<br />RHVNSGA-II<br />SPEA2<br />RS<br />Figure 9. Mean performance score over all test problems for different number<br />of objectives. The smaller the score, the better the Pareto-set approximation<br />in terms of hypervolume.<br />HypE can be considered an adequate alternative to the regular<br />hypervolume algorithms; the main advantage though becomes<br />evident when the respective fitness measures need to be<br />estimated, see below.<br />3) HypE Versus Other MOEAs: Now we compare HypE<br />and HypE*, both using a constant number of samples, to other<br />multiobjective evolutionary algorithms. Table V on pages 23–<br />25 shows the performance score and mean hypervolume on the<br />17 test problems mentioned in the Experimental Setup Section.<br />Except on few testproblems HypE is better than HypE*. HypE<br />reaches the best performance score overall. Summing up all<br />performance scores, HypE yields the best total (76), followed<br />by HypE* (143), IBEA (171) and the method proposed in<br />[1] (295). SPEA2 and NSGA-II reach almost the same score<br />(413 and 421 respectively), clearly outperforming the random<br />selection (626).<br />In order to better visualize the performance index, we<br />show to Figures where the index is summarized for dif-<br />ferent test-problems and number of objectives respectively.<br />Figure 9 shows the average performance over all testproblems<br />for different number of objectives. Except for two objective<br />problems, HypE yields the best score, increasing its lead<br />in higher dimensions. The version using uniform mating<br />selection, HypE*, is outperformed by IBEA for two to seven<br />objectives and only thereafter reaches a similar score as HypE.<br />This indicates, that using non-uniform mating selection is<br />particularly advantageous for small number of objectives.<br />Next we look at the performance score for the individual<br />test-problems. Figure 10 shows the average index over all<br />number of objectives. For DTLZ2, 4, 5 and 7, knapsack and<br />WFG8, IBEA outperforms HypE, for DTLZ7 and knapsack,<br />SHV as well is better than HypE. On the remaining 11<br />testproblems, HypE reaches the best mean performance.<br />Note that the above comparison is carried out for the case<br />all algorithms run for the same number of generations and<br />HypE needs longer execution time, e.g., in comparison to<br />SPEA2 or NSGA-II. We therefore investigate in the following,<br />whether NSGA-II and SPEA2 will not overtake HypE given<br />D1 D2 D3 D4 D5 D6 D7 K1 W1W2W3W4W5W6W7W8W9<br />testproblem<br />0<br />1<br />2<br />3<br />4<br />5<br />6<br />mean performance<br /> <br /> <br />RHV<br />IBEA<br />NSGA−II<br />RS<br />SPEA HypE HypE*<br />Figure 10.<br />problems, namely DTLZ (Dx), WFG (Wx) and knapsack (K1). The values of<br />HypE+ are connected by a solid line to easier assess the score.<br />Mean performance score over all dimensions for different test<br />a constant amount of time. Figure 11 shows the hypervolume<br />of the Pareto-set approximations over time for HypE using<br />the exact fitness values as well as the estimated values for<br />different samples sizes M. Although only the results on WFG9<br />are shown, the same experiments were repeated on DTLZ2,<br />DTLZ7, WFG3 and WFG6 and provided similar outcomes.<br />Even though SPEA2, NSGA-II and even IBEA are able to<br />process twice as many generations as the exact HypE, they do<br />not reach its hypervolume. In the three dimensional example<br />used, HypE can be run sufficiently fast without approximating<br />the fitness values. Nevertheless, the sampled version is used as<br />well to show the dependency of the execution time and quality<br />on the number of samples M. Via M, the execution time<br />of HypE can be traded off against the quality of the Pareto-<br />set approximation. The fewer samples are used, the more the<br />behavior of HypE resembles random selection. On the other<br />hand by increasing M, the quality of exact calculation can be<br />achieved, increasing the execution time, though. For example,<br />with M = 1,000, HypE is able to carry out nearly the<br />same number of generations as SPEA2 or NSGA-II, but the<br />Pareto-set is just as good as when 100,000 samples are used,<br />producing only a fifteenth the number of generations. In the<br />example given, M = 10,000 represents the best compromise,<br />but the number of samples should be increased in two cases:<br />(i) the fitness evaluation takes more time. This will affect<br />the faster algorithm much more and increasing the number of<br />samples will influence the execution time much less. (ii) More<br />generations are used. In this case, HypE using more samples<br />might overtake the faster versions with fewer samples, since<br />those are more vulnerable to stagnation.<br />VII. CONCLUSIONS<br />This paper proposes HypE (Hypervolume Estimation Algo-<br />rithm for Multiobjective Optimization), a novel hypervolume-<br />based multiobjective evolutionary algorithm that can be ap-<br />plied to problems with arbitrary numbers of objective func-<br />tions. It incorporates a new fitness assignment scheme based<br />15</p>  <p>Page 16</p> <p>HypE*-s<br />HypE*-e<br />RHV*<br />HypE-s<br />HypE-e<br />RHV<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />0.8<br />0.9<br />1<br />Values<br />HypE*-s<br />HypE*-e<br />RHV*<br />HypE-s<br />HypE-e<br />RHV<br />HypE*-s<br />HypE*-e<br />RHV*<br />HypE-s<br />HypE-e<br />RHV<br />HypE*-s<br />HypE*-e<br />RHV*<br />HypE-s<br />HypE-e<br />RHV<br />2 objectives 3 objectives4 objectives 5 objectives<br />Figure 8.<br />3, 4, and 5 objectives. For presentation reasons, the hypervolume values are normalized to the minimal and maximal values observed per problem instance.<br />Comparison of the hypervolume indicator values for different variants of HypE and the regular hypervolume algorithm (RHV) on DTLZ2 with 2,<br />minutes<br />hypervolume<br /> <br />0<br /> <br />1658<br />135<br />522<br />2353<br />2434<br />2125<br />12345678910<br />  .475<br />.500<br />  .525<br />  .550<br />522<br />2024<br />881<br />134<br />1123<br />minutes<br />0123456789 10<br />SHV-1k<br />SHV-10k<br />SHV-100k<br />HypE-1k<br />HypE-100k<br />HypE-10k<br />IBEA<br />NSGA-II<br />SPEA2<br />SHV-10k<br />HypE-e<br />Figure 11.<br />The test problem is WFG9 for three objectives. HypE is compared to the algorithms presented in Section VI, where the results are split in two figures with<br />identical axis for the sake of clarity. The numbers at the left border of the figures indicate the total number of generations.<br />Hypervolume process over ten minutes of HypE+ for different samples sizes x in thousands (Hy-xk) as well as using the exact values (Hy-e).<br />on the Lebesgue measure, where this measure can be both ex-<br />actly calculated and estimated by means of Monte Carlo sam-<br />pling. The latter allows to trade-off fitness accuracy versus the<br />overall computing time budget which renders hypervolume-<br />based search possible also for many-objective problems, in<br />contrast to [15], [23], [9]. HypE is available for download at<br />http://www.tik.ee.ethz.ch/sop/download/supplementary/hype/.<br />HypE was compared to various state-of-the-art MOEAs<br />with regard to the hypervolume indicator values of the gen-<br />erated Pareto-set approximations—on the DTLZ [14], the<br />WFG [21], and the knapsack [45] test problem suites. The<br />simulations results indicate that HypE is a highly competitive<br />multiobjective search algorithm; in the considered setting the<br />Pareto front approximations obtained by HypE reached the<br />best hypervolume value in 6 out of 7 cases averaged over all<br />testproblems.<br />A promising direction of future research is the development<br />of advanced adaptive sampling strategies that exploit the avail-<br />able computing resources most effectively, such as increasing<br />the number of samples towards the end of the evolutionary<br />run.<br />ACKNOWLEDGEMENTS<br />Johannes Bader has been supported by the Indo-Swiss Joint<br />Research Program under grant IT14.<br />REFERENCES<br />[1] J. Bader, K. Deb, and E. Zitzler. Faster Hypervolume-based Search using<br />Monte Carlo Sampling. In Conference on Multiple Criteria Decision<br />Making (MCDM 2008). Springer, 2008. to appear.<br />16</p>  <p>Page 17</p> <p>[2] J. Bader and E. Zitzler. HypE: Fast Hypervolume-Based Multiobjective<br />Search Using Monte Carlo Sampling.<br />Technische Informatik und Kommunikationsnetze, ETH Zürich, April<br />2006.<br />[3] N. Beume, C. M. Fonseca, M. Lopez-Ibanez, L. Paquete, and J. Vahren-<br />hold. On the Complexity of Computing the Hypervolume Indicator.<br />Technical Report CI-235/07, University of Dortmund, December 2007.<br />[4] N. Beume, B. Naujoks, and M. Emmerich. SMS-EMOA: Multiobjective<br />selection based on dominated hypervolume.<br />Operational Research, 181:1653–1669, 2007.<br />[5] N. Beume and G. Rudolph. Faster S-Metric Calculation by Considering<br />Dominated Hypervolume as Klee’s Measure Problem. Technical Report<br />CI-216/06, Sonderforschungsbereich 531 Computational Intelligence,<br />Universität Dortmund, 2006. shorter version published at IASTED<br />International Conference on Computational Intelligence (CI 2006).<br />[6] S. Bleuler, M. Laumanns, L. Thiele, and E. Zitzler. PISA—A Plat-<br />form and Programming Language Independent Interface for Search<br />Algorithms. In C. M. Fonseca, P. J. Fleming, E. Zitzler, K. Deb,<br />and L. Thiele, editors, Conference on Evolutionary Multi-Criterion<br />Optimization (EMO 2003), volume 2632 of LNCS, pages 494–508,<br />Berlin, 2003. Springer.<br />[7] L. Bradstreet, L. Barone, and L. While. Maximising Hypervolume for<br />Selection in Multi-objective Evolutionary Algorithms. In 2006 IEEE<br />Congress on Evolutionary Computation (CEC 2006), pages 6208–6215,<br />Vancouver, BC, Canada, 2006. IEEE.<br />[8] K. Bringmann and T. Friedrich. Approximating the volume of unions<br />and intersections of high-dimensional geometric objects. In Proc. of the<br />19th International Symposium on Algorithms and Computation (ISAAC<br />2008), Berlin, Germany, 2008. Springer.<br />[9] D. Brockhoff and E. Zitzler. Improving Hypervolume-based Multiobjec-<br />tive Evolutionary Algorithms by Using Objective Reduction Methods. In<br />Congress on Evolutionary Computation (CEC 2007), pages 2086–2093.<br />IEEE Press, 2007.<br />[10] W. J. Conover. Practical Nonparametric Statistics. John Wiley, 3 edition,<br />1999.<br />[11] J. Cooper and T. Friedrich. The Hypervolume Indicator and Klee’s<br />Measure Problem. Submitted to ICALP 2008.<br />[12] K. Deb.<br />Multi-objective optimization using evolutionary algorithms.<br />Wiley, Chichester, UK, 2001.<br />[13] K. Deb, S. Agrawal, A. Pratap, and T. Meyarivan. A Fast Elitist Non-<br />Dominated Sorting Genetic Algorithm for Multi-Objective Optimization:<br />NSGA-II.In M. Schoenauer et al., editors, Conference on Parallel<br />Problem Solving from Nature (PPSN VI), volume 1917 of LNCS, pages<br />849–858. Springer, 2000.<br />[14] K. Deb, L. Thiele, M. Laumanns, and E. Zitzler. Scalable Test Problems<br />for Evolutionary Multi-Objective Optimization. In A. Abraham, R. Jain,<br />and R. Goldberg, editors, Evolutionary Multiobjective Optimization:<br />Theoretical Advances and Applications, chapter 6, pages 105–145.<br />Springer, 2005.<br />[15] M. Emmerich, N. Beume, and B. Naujoks. An EMO Algorithm Using<br />the Hypervolume Measure as Selection Criterion.<br />Evolutionary Multi-Criterion Optimization (EMO 2005), volume 3410<br />of LNCS, pages 62–76. Springer, 2005.<br />[16] M. Emmerich, A. Deutz, and N. Beume. Gradient-Based/Evolutionary<br />Relay Hybrid for Computing Pareto Front Approximations Maximizing<br />the S-Metric. In Hybrid Metaheuristics, pages 140–156. Springer, 2007.<br />[17] R. Everson, J. Fieldsend, and S. Singh. Full Elite-Sets for Multiobjective<br />Optimisation. In I.C. Parmee, editor, Proceedings of the fifth interna-<br />tional conference on adaptive computing in design and manufacture<br />(ADCM 2002), pages 343–354, London, UK, 2002. Springer.<br />[18] M. Fleischer. The measure of Pareto optima. Applications to multi-<br />objective metaheuristics. In C. M. Fonseca et al., editors, Conference on<br />Evolutionary Multi-Criterion Optimization (EMO 2003), volume 2632<br />of LNCS, pages 519–533, Faro, Portugal, 2003. Springer.<br />[19] C. M. Fonseca, L. Paquete, and M. López-Ibáñez.<br />Dimension-Sweep Algorithm for the Hypervolume Indicator.<br />Congress on Evolutionary Computation (CEC 2006), pages 1157–1163,<br />Sheraton Vancouver Wall Centre Hotel, Vancouver, BC Canada, 2006.<br />IEEE Press.<br />[20] D. E. Goldberg.<br />Genetic Algorithms in Search, Optimization, and<br />Machine Learning. Addison-Wesley, Reading, Massachusetts, 1989.<br />TIK Report 286, Institut für<br />European Journal on<br />In Conference on<br />An Improved<br />In<br />[21] S. Huband, P. Hingston, L. Barone, and L. While.<br />Multiobjective Test Problems and a Scalable Test Problem Toolkit. IEEE<br />Transactions on Evolutionary Computation, 10(5):477–506, 2006.<br />[22] S. Huband, P. Hingston, L. White, and L. Barone.<br />Strategy with Probabilistic Mutation for Multi-Objective Optimisation.<br />In Proceedings of the 2003 Congress on Evolutionary Computation<br />(CEC 2003), volume 3, pages 2284–2291, Canberra, Australia, 2003.<br />IEEE Press.<br />[23] C. Igel, N. Hansen, and S. Roth. Covariance Matrix Adaptation for<br />Multi-objective Optimization. Evolutionary Computation, 15(1):1–28,<br />2007.<br />[24] J. Knowles and D. Corne. On Metrics for Comparing Non-Dominated<br />Sets. In Congress on Evolutionary Computation (CEC 2002), pages<br />711–716, Piscataway, NJ, 2002. IEEE Press.<br />[25] J. Knowles and D. Corne. Properties of an Adaptive Archiving<br />Algorithm for Storing Nondominated Vectors. IEEE Transactions on<br />Evolutionary Computation, 7(2):100–116, 2003.<br />[26] J. D. Knowles. Local-Search and Hybrid Evolutionary Algorithms for<br />Pareto Optimization. PhD thesis, University of Reading, 2002.<br />[27] M. Laumanns, G. Rudolph, and H.-P. Schwefel.<br />Pareto Set: Concepts, Diversity Issues, and Performance Assessment.<br />Technical Report CI-7299, University of Dortmund, 1999.<br />[28] S. Mostaghim, J. Branke, and H. Schmeck.<br />Swarm Optimization on Computer Grids. In Proceedings of the 9th<br />annual conference on Genetic and evolutionary computation (GECCO<br />2007), pages 869–875, New York, NY, USA, 2007. ACM.<br />[29] M. Nicolini. A Two-Level Evolutionary Approach to Multi-criterion<br />Optimization of Water Supply Systems. In Conference on Evolutionary<br />Multi-Criterion Optimization (EMO 2005), volume 3410 of LNCS, pages<br />736–751. Springer, 2005.<br />[30] S. Obayashi, K. K. Deb, C. Poloni, T. Hiroyasu, and T. Murata, editors.<br />Conference on Evolutionary Multi-Criterion Optimization (EMO 2007),<br />volume 4403 of LNCS, Berlin, Germany, 2007. Springer.<br />[31] J. Scharnow, K. Tinnefeld, and I. Wegener. The Analysis of Evolu-<br />tionary Algorithms on Sorting and Shortest Paths Problems. Journal of<br />Mathematical Modelling and Algorithms, 3(4):349–366, 2004. Online<br />Date Tuesday, December 28, 2004.<br />[32] J. R. Swisher and S. H. Jacobson. A survey of ranking, selection, and<br />multiple comparison procedures for discrete-event simulation. In WSC<br />’99: Proceedings of the 31st conference on Winter simulation, pages<br />492–501, New York, NY, USA, 1999. ACM.<br />[33] D. A. Van Veldhuizen. Multiobjective Evolutionary Algorithms: Classi-<br />fications, Analyses, and New Innovations. PhD thesis, Graduate School<br />of Engineering, Air Force Institute of Technology, Air University, 1999.<br />[34] T. Wagner, N. Beume, and B. Naujoks.<br />and Indicator-based Methods in Many-objective Optimization.<br />S. Obayashi et al., editors, Conference on Evolutionary Multi-Criterion<br />Optimization (EMO 2007), volume 4403 of LNCS, pages 742–756,<br />Berlin Heidelberg, Germany, 2007. Springer.<br />lished as internal report of Sonderforschungsbereich 531 Computational<br />Intelligence CI-217/06, Universität Dortmund, September 2006.<br />[35] L. While. A New Analysis of the LebMeasure Algorithm for Calculating<br />Hypervolume. In Conference on Evolutionary Multi-Criterion Optimiza-<br />tion (EMO 2005), volume 3410 of LNCS, pages 326–340, Guanajuato,<br />México, 2005. Springer.<br />[36] L. While, P. Hingston, L. Barone, and S. Huband. A Faster Algorithm<br />for Calculating Hypervolume.<br />Computation, 10(1):29–38, 2006.<br />[37] Q. Yang and S. Ding.Novel Algorithm to Calculate Hypervolume<br />Indicator of Pareto Approximation Set. In Advanced Intelligent Com-<br />puting Theories and Applications. With Aspects of Theoretical and<br />Methodological Issues, Third International Conference on Intelligent<br />Computing (ICIC 2007), volume 2, pages 235–244, 2007.<br />[38] E. Zitzler.<br />Evolutionary Algorithms for Multiobjective Optimization:<br />Methods and Applications. PhD thesis, ETH Zurich, Switzerland, 1999.<br />[39] E. Zitzler. Hypervolume metric calculation. ftp://ftp.tik.ee.ethz.ch/pub/<br />people/zitzler/hypervol.c, 2001.<br />[40] E. Zitzler, D. Brockhoff, and L. Thiele. The Hypervolume Indicator<br />Revisited: On the Design of Pareto-compliant Indicators Via Weighted<br />Integration. In S. Obayashi et al., editors, Conference on Evolutionary<br />A Review of<br />An Evolution<br />Approximating the<br />Multi-objective Particle<br />Pareto-, Aggregation-,<br />In<br />extended version pub-<br />IEEE Transactions on Evolutionary<br />17</p>  <p>Page 18</p> <p>Multi-Criterion Optimization (EMO 2007), volume 4403 of LNCS, pages<br />862–876, Berlin, 2007. Springer.<br />[41] E. Zitzler and S. Künzli. Indicator-Based Selection in Multiobjective<br />Search.In Conference on Parallel Problem Solving from Nature<br />(PPSN VIII), volume 3242 of LNCS, pages 832–842. Springer, 2004.<br />[42] E. Zitzler, M. Laumanns, and L. Thiele. SPEA2: Improving the Strength<br />Pareto Evolutionary Algorithm for Multiobjective Optimization. In K.C.<br />Giannakoglou et al., editors, Evolutionary Methods for Design, Optimi-<br />sation and Control with Application to Industrial Problems (EUROGEN<br />2001), pages 95–100. International Center for Numerical Methods in<br />Engineering (CIMNE), 2002.<br />[43] E. Zitzler and L. Thiele. An Evolutionary Approach for Multiobjective<br />Optimization: The Strength Pareto Approach.<br />Computer Engineering and Networks Laboratory, ETH Zurich, May<br />1998.<br />[44] E. Zitzler and L. Thiele. Multiobjective Optimization Using Evolution-<br />ary Algorithms - A Comparative Case Study. In Conference on Parallel<br />Problem Solving from Nature (PPSN V), pages 292–301, Amsterdam,<br />1998.<br />[45] E. Zitzler and L. Thiele. Multiobjective Evolutionary Algorithms: A<br />Comparative Case Study and the Strength Pareto Approach.<br />Transactions on Evolutionary Computation, 3(4):257–271, 1999.<br />[46] E. Zitzler, L. Thiele, and J. Bader. On Set-Based Multiobjective Opti-<br />mization. Technical Report 300, Computer Engineering and Networks<br />Laboratory, ETH Zurich, February 2008.<br />[47] E. Zitzler, L. Thiele, M. Laumanns, C. M. Fonseca, and V. Grunert<br />da Fonseca. Performance Assessment of Multiobjective Optimizers: An<br />Analysis and Review. IEEE Transactions on Evolutionary Computation,<br />7(2):117–132, 2003.<br />Technical Report 43,<br />IEEE<br />APPENDIX<br />A. Proofs for Section III<br />Proof of Theorem III.3, page 4: According to 5 it holds:<br />IH(A,R) = λ?H(A,R))<br />= λ<br />H(S,A,R)<br />?˙?<br />S⊆A<br />?<br />.<br />By dividing the subsets into groups of equal size, one obtains<br />?<br />= λ<br />˙?<br />1≤i≤|A|<br />˙?<br />|S|=i<br />S⊆A<br />H(S,A,R)<br />?<br />which can be rewritten as<br />=<br />|A|<br />?<br />i=1<br />λ<br />? ?<br />|S|=i<br />S⊆A<br />H(S,A,R)<br />?<br />because the inner unions are all disjoint. Now, for each subset<br />of size i we count the Lebesque measure once for each element<br />and then divide by 1/i:<br />=<br />|A|<br />?<br />i=1<br />1<br />i<br />?<br />a∈A<br />λ<br />? ?<br />|S|=i<br />a∈S<br />S⊆A<br />H(S,A,R)<br />?<br />.<br />Changing the order of the sums results in<br />=<br />?<br />a∈A<br />|A|<br />?<br />i=1<br />1<br />iλ<br />? ?<br />|S|=i<br />a∈S<br />S⊆A<br />H(S,A,R)<br />?<br />and using Definition III.2 we obtain<br />=<br />?<br />a∈A<br />Ih(a,A,R)<br />which concludes the proof.<br />Proof of Theorem III.4, page 4: From Theorem III.3 we<br />know that<br />?<br />which—following Definition III.1—equals<br />b1∈{a}∪B1<br />Ih(b1,{a} ∪ B1,R) = IH({a} ∪ B1,R)<br />= λ?H({a} ∪ B1,R)?.<br />Since {a} ? B1, it holds H(b,R) ⊆ H(A,R) for all b ∈ B1<br />and therefore the above formula can be simplified to<br />= λ?H({a},R)?<br />The same holds analogically for the right-hand side of the<br />equation in Theorem III.4 which proves the claim.<br />Proof of Theorem III.7, page 4: Definition III.6 states<br />that<br />⎡<br />⎣<br />where S denotes the set of subsets of A, that contain k<br />elements, one of which is individual a, i.e., S = {S ⊆ A;a ∈<br />S ∧ |S| = k}. Inserting the definition of S leads to<br />1<br />|S|<br />S∈A<br />|S|=k<br />a∈S<br />To combine the two summations of the previous equa-<br />tion, let o(T) denote the number of times the summand<br />1<br />Ik<br />h(a,A,R) =<br />1<br />|S|<br />?<br />S∈S<br />⎢<br />?<br />a∈T<br />T⊆S<br />1<br />|T|λ?H(T,A,R)?<br />⎤<br />⎦<br />⎥<br />=<br />?<br />?<br />a∈T<br />T⊆S<br />1<br />|T|λ?H(T,A,R)?<br />(25)<br />|T|λ?H(T,A,R)?is added for the same set T<br />=<br />|S|<br />1<br />?<br />a∈T<br />T⊆A<br />o(T)1<br />|T|λ?H(T,A,R)?<br />splitting up into summation over subsets of equal size gives<br />=<br />1<br />|S|<br />k<br />?<br />i=1<br />1<br />i<br />?<br />|T|=i<br />a∈T<br />T⊆A<br />o(T)λ?H(T,A,R)?<br />For symmetry reasons, each subset T with cardinality |T| = i<br />has the same number of occurences o(T) =: oi<br />=<br />1<br />|S|<br />k<br />?<br />i=1<br />oi<br />i<br />?<br />|T|=i<br />a∈T<br />T⊆A<br />λ?H(T,A,R)?<br />18</p>  <p>Page 19</p> <p>A<br />S<br />T<br />k=6<br />|A|=10<br />i=4<br />a<br />a<br />a<br />Figure 12.<br />sets contain a. Given one particular T of size i there exist?|A|−i<br />T is a subset of S, which in turn is a subset of A; all three<br />k−i<br />?<br />subsets<br />S ⊆ A of size k which are a superset of T.<br />since all H(T,A,R) in the sum are disjoint according to (5)<br />=<br />1<br />|S|<br />k<br />?<br />i=1<br />oi<br />iλ<br />? ?<br />|T|=i<br />a∈T<br />T⊆A<br />H(T,A,R)<br />?<br />and according to Equation (6)<br />=<br />k<br />?<br />i=1<br />oi<br />i · |S|λ?Hi(a,A,R)?<br />After having transformed the original equation, we deter-<br />mine the number oi, i.e., the number of times the term<br />1<br />appears in (25). The term is added once<br />every time the corresponding set T is a subset of S. Hence,<br />o(T) with |T| = i corresponds to the number of sets S that are<br />a superset of T. As depicted in Figure A, T defines i elements<br />of S and the remaining k − i elements can be chosen freely<br />from the |A| − i elements in A that are not yet in S.<br />Therefore, there exist?|A|−i<br />?|A|−i<br />oi<br />|S|=<br />k−1<br />=(|A| − i)!(k − 1)!((|A| − 1) − (k − 1))!<br />(k − i)!((|A| − i) − (k − i))!(|A| − 1)!<br />=(|A| − i)!(k − 1)!<br />(|A| − 1)!(k − i)!<br />=<br />(|A| − 1)(|A| − 2)···(|A| − (i − 1)<br />=<br />|A| − j<br />= αi<br />|T|λ?H(T,A,R)?<br />k−i<br />?subsets S ∈ S that contain one<br />particular T with |T| = i and a ∈ T. Therefore, o(T) = oi=<br />k−i<br />Hence<br />?|A|−i<br />?. Likewise, the total number of sets S is |S| =?|A|−1<br />?<br />k−1<br />?.<br />k−i<br />?|A|−1<br />?<br />(k − 1)(k − 2)···(k − (i − 1))<br />i−1<br />?<br />j=1<br />k − j<br />Therefore<br />Ik<br />h(a,A,R) =<br />k<br />?<br />i=1<br />αi<br />iλ?Hi(a,A,R)?<br />which concludes the proof.<br />B. Derivation of Confidence Intervals used in IV-B<br />Here, we consider confidence intervals forˆIk<br />order to adaptively determine the number of sampling points<br />h(a,A,R) in<br />to be drawn in order to reach a user predefined confidence<br />level. However, there are also other uses: for instance, to<br />indicate the quality when reporting the values to a decision<br />maker or carrying out statistical tests; or to adapt the selection<br />probabilites for fitness proportionate selection meaning that<br />high-fitness individuals are penalized whenever the fitness<br />value comes along with a high uncertainty.<br />The confidence intervals can be derived by determining the<br />distribution of a sampled value. To this end, a new discrete<br />random variable Z is introduced with the property Pr(Z =<br />αi/i) = piwhere pidenotes the probability of a hit in the ith<br />partition, i.e., a hit in Hi(a,A,R); picorresponds to the ratio<br />between the volume of the particular partition and the volume<br />of the sampling space. Formula (20) can now be rewritten as<br />a sum of M random variables Zi distributed independently<br />identically as Z<br />ˆIk<br />h(a,A,R) =V<br />M<br />M<br />?<br />s=1<br />Zi<br />(26)<br />According to the central limit theorem, this estimate is ap-<br />proximately normally distributed with mean Ik<br />the following variance:<br />h(a,A,R and<br />Var?ˆIk<br />h(a,A,R)?=V2<br />M2<br />?<br />k<br />?<br />k<br />?<br />i=1<br />α2<br />i2Var(Pi)+<br />i<br />(27)<br />2<br />k<br />?<br />i=1j=i+1<br />αiαj<br />ij<br />Cov(Pi,Pj)<br />?<br />(28)<br />where Pi denotes the number of hits in the ith partition.<br />The variance of Pifollows from the binomial distribution as<br />Var(Pi) = Mpi(1 − pi) and the covariance of Piand Pj is<br />M<br />?<br />because Cov(X(i)<br />Since the probabilities pi are unknown, the estimates ˆ pi =<br />Pi/M are used instead. This of course presupposes counting<br />the number of hits in a particular partition. The estimated<br />variance according to Eq. (28) becomes<br />Cov(Pi,Pj) =<br />f=1<br />M<br />?<br />g ) = −pipj if i = j and 0 otherwise.<br />g=1<br />Cov?X(i)<br />f,X(j)<br />g<br />?= −Mpipj<br />(29)<br />f,X(j)<br />?<br />Var?ˆIk<br />h(a,A,R)?=V2<br />M<br />?<br />k<br />?<br />i=1<br />α2<br />i2Pi(1 − Pi) − 2<br />i<br />k<br />?<br />i=1<br />k<br />?<br />j=i+1<br />αiαj<br />ij<br />PiPj<br />?<br />(30)<br />The normal approximation with the mean and variance just<br />derived can now be used to give the confidence interval for a<br />particular estimate, i.e.,<br />Ik<br />h(a,A,R) ∈ˆIk<br />h(a,A,R) ± z1−perr/2<br />?<br />?<br />Var?ˆIk<br />h(a,A,R)?<br />(31)<br />where zβ denotes the β-percentile of a standard normal<br />distribution and perr the probability of error.<br />19</p>  <p>Page 20</p> <p>1) Ranking and Selection: In order to use the confidence<br />interval of Equation 31 for adaptive sampling, we will distin-<br />guish three situations—depending on how the fitness estimates<br />ˆIk<br />h(a,A,R) of a population A are used:<br />• The individual of A with the smallest value needs to be<br />identified for removal: This occurs when applying the<br />iterative greedy strategy for environmental selection.<br />• The subset A?of A compromising the k individuals<br />with the smallest fitness values should be determined for<br />removal: This occurs when applying the one-shot greedy<br />strategy for environmental selection.<br />• The population members a ∈ A should be ranked<br />according to their Ik<br />when using rank-based mating selection, e.g., tournament<br />selection.<br />h(a,A,R) values: This is the case<br />In all three scenarios, one is interested in the probability that<br />the selection and ranking respectively is done correctly, i.e.,<br />corresponds to the one that would result when using the exact<br />values Ik<br />which then can be used as a stopping criterion for the sampling<br />process.<br />Consider first the simple case of deciding between the<br />estimate of two individuals a,b ∈ A whether either is<br />smaller. Let the indicator values be v1 := Ik<br />v2 := Ik<br />and ˆ v2 :=ˆIk<br />is selected. Furthermore, let δ := v1− v2 denote difference<br />between the indicator values andˆδ := ˆ v1− ˆ v2the difference<br />between the estimates. By examining the distribution of δ<br />givenˆδ, the probability of correct selection Cp corresponds<br />to the probability P(δ &lt; 0|ˆδ).<br />The difference δ is approximately normally distributed<br />around ˆ v1 − ˆ v2 due to the same arguments used above.<br />Since the same samples are used to estimate v1 and v2, the<br />corresponding estimates are not independent—the variance<br />of the difference is Var(δ) ≈ σ2<br />The first two summands can be determined according to<br />(30), the correlation ρ is more difficult to determine as it<br />can vary greatly as Fig. 13 reveals. The correlation can be<br />estimated iteratively, but this implies updating ρ for all pairs<br />of individuals after each sample drawn. For two reasons this is<br />impractical: Firstly, keeping track of all correlations requires a<br />lot of memory and processing time compared to the sampling<br />process itself and will significantly slow down the estimation<br />process; secondly, the exact error probability is not crucial,<br />and an approximation of it or a lower bound is adequate for<br />the intended applications.<br />Therefore, we propose using an upper bound for the correct<br />probability, based solely on the variance of the estimates. If<br />the exact value v1is smaller than a value B and v2is bigger<br />than B, then clearly v1&lt; v2. We can therefore give a lower<br />h(a,A,R). This probability is derived in the following<br />h(a,A,R) and<br />h(a,A,R)<br />h(b,A,R) and their estimates be ˆ v1 :=ˆIk<br />h(b,A,R) with ˆ v1 &lt; ˆ v2, hence individual a<br />ˆ v1+ σ2<br />ˆ v2− 2σˆ v1σˆ v2ρˆ v1,ˆ v2.<br />−0.4−0.20 0.2<br />correlation<br />0.40.6 0.81<br />0<br />50<br />100<br />150<br />200<br />250<br />300<br />350<br />400<br />frequency<br />Figure 13.<br />a given Pareto-set approximation introduced in Table II. The closer to points<br />are, the higher the correlation between their estimates becomes. Contrariwise,<br />the estimates can be negatively correlated for points which lie far apart.<br />Correlation between fitness estimates of different individuals of<br />101<br />102<br />103<br />104<br />105<br />0.4<br />0.5<br />0.6<br />0.7<br />0.8<br />0.9<br />1<br />Samples<br />Pairwise Confidence<br /> <br /> <br />Exact<br />Lower Bound<br />Figure 14.<br />randomly selected individuals of a Pareto set approximation as in Table II.<br />The solid line shows the exact confidence interval calculated based on<br />the correlation information. The dashed course represents the lower bound<br />according to (33).<br />Shows the confidence interval for a comparison between two<br />bound for ordering the two individuals correctly by<br />P(v1&lt; v2|ˆ v1&lt; ˆ v2) ≥ P(v1&lt; B|ˆ v1) · P(v2&gt; B|v1&lt; B, ˆ v2)<br />(32)<br />where B =?ˆ v1+ ˆ v2<br />P(v1&lt; v2|ˆ v1&lt; ˆ v2) ≥ 1 − P(v1&gt; B|ˆ v1) − P(v2&lt; B|ˆ v2)<br />?/2. (32) in turn can be lower bounded,<br />which finally leads to<br />(33)<br />In (33) all probabilities involved are pairwise independent<br />and hence can be determined using the normal distribution<br />with the variance given in Eq. (30). Figure B1 shows the<br />estimated confidence according to (33) in comparison to the<br />exact confidence level.<br />After these general considerations, we can now provide<br />specific lower bounds for the three situations mentioned above.<br />a) Selecting the Worst Individual: There exist many<br />procedures in the field of ranking and selection [32]. However,<br />they often assume that the estimates are independent or at<br />20</p>  <p>Page 21</p> <p>selecting the worstselecting the s worst ranking<br />?<br />?<br />?<br />increasing value<br />B<br />1ˆ v<br />2ˆ v<br />2ˆ v<br />|A|<br />ˆ v<br />1ˆ v<br />1ˆ v<br />sˆ v<br />s+1<br />ˆ v<br />|A|<br />ˆ v<br />?<br />|A|<br />ˆ v<br />1<br />ˆ<br />v<br />ˆ<br />v<br />2<br />ss<br />B<br />++<br />=<br />1<br />B<br />2<br />B<br />| |-1<br />A<br />B<br />Figure 15.<br />Section B1: When selecting the worst point, the threshold is determined by<br />its estimated variance (left illustration); when the s worst points need to be<br />selected (middle) or the points have to be ranked, then the threshold value(s)<br />lie halfway between two estimates.<br />Shows the threshold for values for the three cases discussed in<br />least that their correlation matrix has special properties like<br />sphericity. Unfortunately, these assumptions are not met in<br />our case. Moreover, determining the necesarry correlations<br />between estimates slows down the sampling process consid-<br />erably.<br />We therefore propose to use a rough lower bound for the<br />confidence Cw. Though it is a quite conservative bound, the<br />sampling speed is in a much minor degree affected compared<br />to tighter approximations which consider the correlation. The<br />idea is again to determine a threshold to which the estimates<br />are compared.<br />Let vi = Ik<br />different individuals ai ∈ A with 1 ≤ i ≤ |A| and ˆ vi<br />the sampled estimates thereof. Let ˆ v1 &lt; ˆ v2··· &lt;ˆI|A|. The<br />confidence Cw of correctly selecting the worst individuals is<br />P(?|A|<br />determine a lower bound for Cw. Firstly, we calculate the<br />confidence interval of the worst estimate ˆ v1according to (31)<br />at the level (L + 1)/2, where L denotes the user defined<br />confidence level for the selection problem. Let the threshold<br />B then be the upper endpoint of this interval. Hence, v1will<br />be smaller than B approximately with probability (L + 1)/2<br />given ˆ v1.<br />For the remaining estimates ˆ viwith i &gt; 1 we then compute<br />the probability P(vi ≥ B|ˆ vi), that their exact value vi is<br />bigger than the threshold B by using once again the normal<br />approximation (shown as arrows in Fig. 15). Clearly, the<br />bigger ˆ vi, the bigger the probability becomes.<br />Based on L and P(vi ≥ B|ˆ vi,i ≥ 2), the probability<br />of obtaining a correct selection outcome is then calculated<br />analogically to (32) (all following probabilities are under<br />the condition of known ˆ vi, which is omitted to facilitate<br />readability):<br />h(ai,A,R) denote the indicator values for<br />i=2vi&gt; v1|ˆ v1,..., ˆ v|A|).<br />Left hand side of Figure 15 illustrates the procedure to<br />Cw= P<br />⎛<br />⎝<br />⎛<br />|A|<br />?<br />i=2<br />vi&gt; v1<br />⎞<br />⎠<br />(34)<br />≥ P(v1&lt; B) · P(v2≥ B|v1&lt; B)···<br />(35)<br />P<br />⎝v|A|≥ B|v1&lt; B,<br />|A|−1<br />?<br />i=2<br />vi≥ B<br />⎞<br />⎠<br />(36)<br />which in turn can be lower bounded by<br />Cw≥ 1 − (1 − L)/2 −<br />|A|<br />?<br />i=2<br />P(vi&gt; B)<br />(37)<br />In the latter approximation, all factors P(vi &gt; B) can be<br />determined using the normal approximation and the variance<br />derived in (28), not involving any correlation between different<br />estimates.<br />b) Selecting the s Worst Individuals: Let again ˆ v1 &lt;<br />ˆ v2 ≤ ··· ≤ ˆ v|A|denote the estimates of vi = Ik<br />with ai∈ A. In this section we consider the problem of not<br />only removing the worst point, but the setˆAw= {a1,...,as}<br />of those s individuals with the worst estimated indicator<br />values. On this problem, the confidence Csshall be determined<br />that the set ˆAw corresponds to the set Aw resulting form<br />ranking the individuals according to the exact indicator values<br />vi. Hence, Csis equal to the probability P(ˆAw= Aw).<br />The middle part of Figure 15 illustrates how a lower bound<br />for the probability can be obtained. This time, the threshold<br />B is set to lie between the best estimate still to be removed ˆ vs<br />and the worst estimate which remains in the set ˆ vs+1. Hence,<br />B = (ˆ vs+1+ ˆ vs)/2. A lower bound for the confidence of<br />correctly selecting the set Aw is obtained by multiplying the<br />probabilities, that vi is smaller than B for all individuals of<br />ˆ Awand bigger than B for all ai∈ A\ˆ Aw:<br />Cs= Pc(ˆAw= Aw)<br />?<br />|A|<br />?<br />which can be further simplified by the lower bounded<br />h(ai,A,R),<br />(38)<br />≥<br />s?<br />i=1<br />Pvi&lt; B|<br />i−1<br />?<br />j=1<br />vj&lt; B<br />?<br />·<br />(39)<br />i=s+1<br />P<br />?<br />vi&gt; B|<br />s?<br />j=1<br />vj&lt; B,<br />i−1<br />?<br />k=1<br />vk&gt; B<br />?<br />(40)<br />≥ 1 −<br />s<br />?<br />i=1<br />P(vi&gt; B) −<br />|A|<br />?<br />i=s+1<br />P(vi&lt; B)<br />(41)<br />c) Ranking Individuals: Given a ranking of estimated in-<br />dicator values ˆ v1&lt; ˆ v2··· &lt; ˆ v|A|, where ˆ vi:=ˆIk<br />before. We are interested in finding the confidence Cr, that this<br />ranking represents the ranking of the exact values vi. To this<br />end, we introduce multiple thresholds Biwith 1 ≤ i ≤ |A|−1<br />between each pair ˆ vi and ˆ vi+1 of consecutive estimates (see<br />right hand side of Figure 15). Hence, Bi= (ˆ vi+ ˆ vi+1)/2.<br />h(ai,A,R) as<br />21</p>  <p>Page 22</p> <p>Using Bi, a lower bound is<br />Cr= P(∀i ≤ j : vi&lt; vj)<br />(42)<br />≥ P(v1&lt; B1)<br />· P(v|A|&gt; B|A|−1)<br />which is again lower bounded by<br />|A|−1<br />?<br />i=2<br />(P(vi&gt; Bi) · P(vi&lt; Bi+1)) (43)<br />(44)<br />≥ 1 −<br />|A|−1<br />?<br />i=1<br />P(vi&lt; Bi) −<br />|A|<br />?<br />i=2<br />P(vi&gt; Bi−1)<br />(45)<br />22</p>  <p>Page 23</p> <p>Table V: Comparison of HypE to different MOEAs with respect to the<br />hypervolume indicator. The first number represents the performance<br />score P, which stands for the number of participants significantly<br />dominating the selected algorithm. The number in brackets denote the<br />hypervolume value, normalized to the minimum and maximum value<br />observed on the test problem.<br />Problem SHVIBEANSGA-IIRSSPEA2HypEHypE*<br />2 objectives<br />DTLZ 1<br />DTLZ 2<br />DTLZ 3<br />DTLZ 4<br />DTLZ 5<br />DTLZ 6<br />DTLZ 7<br />Knapsack<br />WFG 1<br />WFG 2<br />WFG 3<br />WFG 4<br />WFG 5<br />WFG 6<br />WFG 7<br />WFG 8<br />WFG 9<br />3 (0.286)<br />2 (0.438)<br />6 (0.265)<br />1 (0.848)<br />2 (0.489)<br />2 (0.670)<br />0 (0.945)<br />2 (0.523)<br />4 (0.567)<br />1 (0.987)<br />2 (0.994)<br />0 (0.964)<br />3 (0.994)<br />2 (0.945)<br />3 (0.929)<br />3 (0.431)<br />1 (0.920)<br />0 (0.667)<br />0 (0.871)<br />0 (0.759)<br />0 (0.928)<br />0 (0.931)<br />0 (0.914)<br />1 (0.898)<br />0 (0.631)<br />0 (0.949)<br />4 (0.962)<br />0 (0.997)<br />0 (0.969)<br />0 (0.997)<br />0 (0.975)<br />0 (0.988)<br />0 (0.675)<br />0 (0.939)<br />2 (0.441)<br />5 (0.306)<br />1 (0.596)<br />3 (0.732)<br />5 (0.361)<br />5 (0.326)<br />6 (0.739)<br />0 (0.603)<br />1 (0.792)<br />3 (0.974)<br />4 (0.991)<br />4 (0.891)<br />5 (0.992)<br />4 (0.932)<br />1 (0.946)<br />1 (0.536)<br />4 (0.891)<br />3 (0.306)<br />5 (0.278)<br />3 (0.452)<br />3 (0.834)<br />6 (0.279)<br />4 (0.388)<br />2 (0.818)<br />3 (0.493)<br />6 (0.160)<br />6 (0.702)<br />6 (0.559)<br />6 (0.314)<br />6 (0.402)<br />6 (0.418)<br />6 (0.294)<br />3 (0.367)<br />6 (0.313)<br />3 (0.343)<br />2 (0.431)<br />1 (0.578)<br />2 (0.769)<br />2 (0.463)<br />6 (0.229)<br />4 (0.817)<br />0 (0.574)<br />1 (0.776)<br />4 (0.969)<br />4 (0.990)<br />4 (0.898)<br />2 (0.995)<br />4 (0.930)<br />2 (0.939)<br />1 (0.514)<br />4 (0.878)<br />1 (0.545)<br />1 (0.682)<br />3 (0.454)<br />1 (0.779)<br />1 (0.724)<br />1 (0.856)<br />2 (0.853)<br />0 (0.633)<br />2 (0.744)<br />0 (0.990)<br />0 (0.997)<br />0 (0.968)<br />0 (0.998)<br />1 (0.955)<br />1 (0.947)<br />0 (0.683)<br />1 (0.924)<br />3 (0.279)<br />4 (0.362)<br />2 (0.483)<br />3 (0.711)<br />4 (0.428)<br />2 (0.659)<br />1 (0.876)<br />0 (0.630)<br />4 (0.557)<br />0 (0.989)<br />2 (0.994)<br />0 (0.963)<br />2 (0.995)<br />2 (0.942)<br />4 (0.920)<br />1 (0.549)<br />0 (0.931)<br />3 objectives<br />DTLZ 1<br />DTLZ 2<br />DTLZ 3<br />DTLZ 4<br />DTLZ 5<br />DTLZ 6<br />DTLZ 7<br />Knapsack<br />WFG 1<br />WFG 2<br />WFG 3<br />WFG 4<br />WFG 5<br />WFG 6<br />WFG 7<br />WFG 8<br />WFG 9<br />3 (0.313)<br />2 (0.995)<br />3 (0.210)<br />1 (0.945)<br />1 (0.991)<br />2 (0.971)<br />0 (0.993)<br />2 (0.441)<br />4 (0.792)<br />0 (0.556)<br />2 (0.995)<br />0 (0.978)<br />2 (0.988)<br />2 (0.959)<br />1 (0.965)<br />2 (0.887)<br />1 (0.954)<br />1 (0.505)<br />0 (0.998)<br />1 (0.495)<br />0 (0.989)<br />0 (0.994)<br />0 (0.990)<br />1 (0.987)<br />0 (0.544)<br />3 (0.811)<br />3 (0.475)<br />3 (0.981)<br />3 (0.955)<br />3 (0.952)<br />2 (0.955)<br />3 (0.950)<br />0 (0.922)<br />3 (0.914)<br />6 (0.168)<br />5 (0.683)<br />3 (0.179)<br />3 (0.777)<br />5 (0.696)<br />6 (0.151)<br />6 (0.633)<br />1 (0.462)<br />3 (0.827)<br />3 (0.406)<br />4 (0.966)<br />5 (0.708)<br />4 (0.884)<br />4 (0.914)<br />5 (0.770)<br />4 (0.842)<br />5 (0.735)<br />0 (0.607)<br />6 (0.491)<br />0 (0.679)<br />3 (0.774)<br />6 (0.374)<br />5 (0.237)<br />4 (0.794)<br />6 (0.322)<br />6 (0.207)<br />6 (0.261)<br />6 (0.689)<br />6 (0.220)<br />6 (0.343)<br />6 (0.415)<br />6 (0.183)<br />6 (0.301)<br />6 (0.283)<br />5 (0.275)<br />4 (0.888)<br />3 (0.216)<br />2 (0.860)<br />4 (0.882)<br />4 (0.266)<br />5 (0.722)<br />1 (0.441)<br />1 (0.881)<br />2 (0.441)<br />4 (0.966)<br />4 (0.740)<br />5 (0.877)<br />5 (0.879)<br />4 (0.858)<br />5 (0.780)<br />4 (0.766)<br />1 (0.395)<br />1 (0.996)<br />2 (0.398)<br />0 (0.987)<br />2 (0.990)<br />0 (0.991)<br />3 (0.970)<br />0 (0.550)<br />0 (0.985)<br />0 (0.446)<br />0 (0.999)<br />1 (0.975)<br />0 (0.991)<br />0 (0.987)<br />0 (0.988)<br />0 (0.906)<br />0 (0.972)<br />continued on next page<br />3 (0.336)<br />3 (0.994)<br />3 (0.196)<br />2 (0.922)<br />3 (0.989)<br />3 (0.967)<br />2 (0.980)<br />0 (0.473)<br />1 (0.894)<br />0 (0.372)<br />1 (0.998)<br />0 (0.979)<br />0 (0.991)<br />1 (0.981)<br />2 (0.958)<br />3 (0.870)<br />1 (0.956)<br />23</p>  <p>Page 24</p> <p>continued from previous page<br />Problem SHVIBEA NSGA-IIRS SPEA2HypEHypE*<br />5 objectives<br />DTLZ 1<br />DTLZ 2<br />DTLZ 3<br />DTLZ 4<br />DTLZ 5<br />DTLZ 6<br />DTLZ 7<br />Knapsack<br />WFG 1<br />WFG 2<br />WFG 3<br />WFG 4<br />WFG 5<br />WFG 6<br />WFG 7<br />WFG 8<br />WFG 9<br />2 (0.927)<br />1 (0.998)<br />2 (0.754)<br />1 (0.997)<br />0 (0.997)<br />3 (0.964)<br />0 (0.988)<br />0 (0.676)<br />4 (0.766)<br />0 (0.671)<br />6 (0.339)<br />0 (0.965)<br />5 (0.754)<br />0 (0.953)<br />0 (0.921)<br />0 (0.847)<br />5 (0.496)<br />3 (0.905)<br />0 (0.999)<br />1 (0.786)<br />0 (0.998)<br />0 (0.998)<br />1 (0.979)<br />0 (0.986)<br />0 (0.862)<br />5 (0.703)<br />0 (0.533)<br />0 (0.974)<br />3 (0.894)<br />1 (0.971)<br />0 (0.949)<br />1 (0.822)<br />0 (0.856)<br />2 (0.720)<br />5 (0.831)<br />4 (0.808)<br />6 (0.365)<br />4 (0.749)<br />4 (0.854)<br />5 (0.428)<br />6 (0.478)<br />2 (0.163)<br />2 (0.832)<br />0 (0.644)<br />3 (0.946)<br />5 (0.711)<br />4 (0.892)<br />4 (0.913)<br />2 (0.774)<br />4 (0.685)<br />4 (0.645)<br />6 (0.548)<br />6 (0.324)<br />4 (0.529)<br />5 (0.558)<br />6 (0.403)<br />6 (0.311)<br />4 (0.672)<br />2 (0.235)<br />6 (0.291)<br />6 (0.351)<br />5 (0.760)<br />6 (0.241)<br />6 (0.303)<br />6 (0.392)<br />6 (0.157)<br />6 (0.309)<br />6 (0.138)<br />4 (0.869)<br />5 (0.795)<br />4 (0.520)<br />6 (0.537)<br />5 (0.841)<br />4 (0.597)<br />5 (0.569)<br />1 (0.369)<br />2 (0.820)<br />0 (0.624)<br />4 (0.932)<br />4 (0.741)<br />3 (0.911)<br />5 (0.872)<br />4 (0.745)<br />5 (0.588)<br />3 (0.667)<br />0 (0.968)<br />2 (0.998)<br />0 (0.824)<br />2 (0.992)<br />2 (0.996)<br />0 (0.988)<br />2 (0.868)<br />2 (0.242)<br />0 (0.973)<br />0 (0.557)<br />0 (0.977)<br />1 (0.948)<br />0 (0.978)<br />1 (0.948)<br />2 (0.784)<br />2 (0.825)<br />0 (0.937)<br />1 (0.961)<br />3 (0.998)<br />1 (0.768)<br />2 (0.992)<br />2 (0.995)<br />1 (0.977)<br />2 (0.862)<br />2 (0.256)<br />1 (0.951)<br />3 (0.503)<br />0 (0.971)<br />1 (0.949)<br />1 (0.975)<br />2 (0.940)<br />5 (0.700)<br />3 (0.809)<br />0 (0.956)<br />7 objectives<br />DTLZ 1<br />DTLZ 2<br />DTLZ 3<br />DTLZ 4<br />DTLZ 5<br />DTLZ 6<br />DTLZ 7<br />Knapsack<br />WFG 1<br />WFG 2<br />WFG 3<br />WFG 4<br />WFG 5<br />WFG 6<br />WFG 7<br />WFG 8<br />WFG 9<br />2 (0.962)<br />3 (0.998)<br />1 (0.951)<br />1 (0.999)<br />1 (0.997)<br />3 (0.954)<br />0 (0.981)<br />0 (0.745)<br />4 (0.647)<br />0 (0.632)<br />6 (0.105)<br />3 (0.888)<br />6 (0.042)<br />0 (0.978)<br />1 (0.688)<br />0 (0.933)<br />5 (0.385)<br />2 (0.960)<br />0 (1.000)<br />1 (0.958)<br />0 (1.000)<br />0 (0.997)<br />2 (0.983)<br />1 (0.958)<br />0 (0.768)<br />5 (0.649)<br />0 (0.747)<br />2 (0.975)<br />2 (0.919)<br />2 (0.982)<br />0 (0.967)<br />3 (0.657)<br />1 (0.905)<br />2 (0.681)<br />5 (0.950)<br />5 (0.808)<br />5 (0.589)<br />4 (0.902)<br />4 (0.888)<br />5 (0.635)<br />5 (0.348)<br />2 (0.235)<br />2 (0.814)<br />1 (0.409)<br />3 (0.961)<br />4 (0.688)<br />4 (0.905)<br />4 (0.940)<br />0 (0.813)<br />4 (0.709)<br />3 (0.679)<br />6 (0.563)<br />6 (0.340)<br />6 (0.438)<br />6 (0.569)<br />6 (0.502)<br />6 (0.397)<br />4 (0.559)<br />2 (0.226)<br />6 (0.189)<br />5 (0.155)<br />5 (0.709)<br />6 (0.200)<br />5 (0.406)<br />6 (0.453)<br />6 (0.207)<br />6 (0.366)<br />6 (0.119)<br />2 (0.961)<br />4 (0.850)<br />4 (0.723)<br />5 (0.814)<br />4 (0.899)<br />4 (0.756)<br />5 (0.352)<br />2 (0.272)<br />2 (0.812)<br />0 (0.837)<br />4 (0.958)<br />4 (0.694)<br />3 (0.938)<br />5 (0.921)<br />3 (0.658)<br />5 (0.537)<br />3 (0.683)<br />0 (0.995)<br />1 (0.999)<br />0 (0.973)<br />2 (0.999)<br />0 (0.997)<br />0 (0.993)<br />2 (0.877)<br />2 (0.276)<br />0 (0.956)<br />0 (0.528)<br />0 (0.983)<br />0 (0.956)<br />0 (0.986)<br />0 (0.974)<br />1 (0.713)<br />2 (0.863)<br />0 (0.928)<br />0 (0.995)<br />1 (0.999)<br />1 (0.952)<br />2 (0.999)<br />1 (0.997)<br />1 (0.988)<br />2 (0.870)<br />4 (0.212)<br />1 (0.937)<br />0 (0.630)<br />0 (0.982)<br />0 (0.952)<br />0 (0.987)<br />3 (0.967)<br />5 (0.606)<br />2 (0.874)<br />0 (0.943)<br />10 objectives<br />DTLZ 1<br />DTLZ 2<br />DTLZ 3<br />DTLZ 4<br />DTLZ 5<br />DTLZ 6<br />DTLZ 7<br />Knapsack<br />WFG 1<br />WFG 2<br />WFG 3<br />WFG 4<br />WFG 5<br />WFG 6<br />WFG 7<br />WFG 8<br />WFG 9<br />3 (0.981)<br />3 (0.999)<br />3 (0.951)<br />2 (1.000)<br />3 (0.951)<br />4 (0.497)<br />0 (0.986)<br />0 (0.568)<br />6 (0.402)<br />0 (0.971)<br />6 (0.088)<br />3 (0.698)<br />6 (0.014)<br />3 (0.934)<br />1 (0.686)<br />0 (0.956)<br />5 (0.222)<br />5 (0.971)<br />2 (1.000)<br />1 (0.990)<br />0 (1.000)<br />0 (0.998)<br />2 (0.987)<br />1 (0.831)<br />0 (0.529)<br />4 (0.843)<br />0 (0.988)<br />1 (0.973)<br />2 (0.896)<br />2 (0.979)<br />1 (0.949)<br />4 (0.464)<br />1 (0.903)<br />3 (0.584)<br />4 (0.986)<br />5 (0.825)<br />5 (0.676)<br />4 (0.988)<br />4 (0.899)<br />4 (0.706)<br />4 (0.137)<br />2 (0.149)<br />2 (0.932)<br />0 (0.978)<br />3 (0.947)<br />3 (0.708)<br />4 (0.832)<br />4 (0.896)<br />1 (0.604)<br />4 (0.689)<br />3 (0.644)<br />6 (0.590)<br />6 (0.290)<br />6 (0.358)<br />6 (0.560)<br />6 (0.471)<br />6 (0.276)<br />6 (0.057)<br />4 (0.119)<br />5 (0.562)<br />5 (0.020)<br />5 (0.792)<br />6 (0.207)<br />5 (0.365)<br />6 (0.449)<br />6 (0.077)<br />6 (0.221)<br />6 (0.109)<br />2 (0.990)<br />4 (0.868)<br />4 (0.750)<br />5 (0.960)<br />4 (0.892)<br />3 (0.769)<br />4 (0.166)<br />2 (0.173)<br />2 (0.937)<br />2 (0.962)<br />4 (0.933)<br />5 (0.669)<br />3 (0.913)<br />5 (0.865)<br />4 (0.473)<br />5 (0.438)<br />2 (0.676)<br />0 (0.999)<br />0 (1.000)<br />0 (0.994)<br />1 (1.000)<br />0 (0.998)<br />0 (0.994)<br />2 (0.744)<br />5 (0.068)<br />0 (0.977)<br />0 (0.981)<br />0 (0.980)<br />0 (0.950)<br />0 (0.987)<br />0 (0.959)<br />0 (0.683)<br />2 (0.883)<br />1 (0.893)<br />continued on next page<br />0 (0.999)<br />0 (1.000)<br />1 (0.990)<br />0 (1.000)<br />1 (0.997)<br />1 (0.992)<br />1 (0.781)<br />5 (0.060)<br />0 (0.975)<br />1 (0.966)<br />1 (0.976)<br />0 (0.955)<br />0 (0.989)<br />1 (0.949)<br />3 (0.548)<br />2 (0.875)<br />0 (0.925)<br />24</p>  <p>Page 25</p> <p>continued from previous page<br />ProblemSHVIBEANSGA-IIRSSPEA2HypEHypE*<br />25 objectives<br />DTLZ 1<br />DTLZ 2<br />DTLZ 3<br />DTLZ 4<br />DTLZ 5<br />DTLZ 6<br />DTLZ 7<br />Knapsack<br />WFG 1<br />WFG 2<br />WFG 3<br />WFG 4<br />WFG 5<br />WFG 6<br />WFG 7<br />WFG 8<br />WFG 9<br />4 (0.994)<br />3 (0.999)<br />3 (0.967)<br />3 (1.000)<br />5 (0.781)<br />6 (0.286)<br />0 (0.973)<br />0 (0.000)<br />6 (0.183)<br />0 (0.951)<br />6 (0.037)<br />6 (0.063)<br />6 (0.003)<br />3 (0.932)<br />3 (0.286)<br />0 (0.924)<br />5 (0.118)<br />5 (0.987)<br />2 (1.000)<br />2 (0.999)<br />2 (1.000)<br />2 (0.996)<br />2 (0.993)<br />0 (0.966)<br />4 (0.000)<br />4 (0.930)<br />0 (0.951)<br />0 (0.983)<br />2 (0.890)<br />3 (0.832)<br />0 (0.959)<br />4 (0.183)<br />0 (0.909)<br />3 (0.531)<br />2 (1.000)<br />4 (0.965)<br />4 (0.930)<br />4 (1.000)<br />3 (0.949)<br />3 (0.957)<br />3 (0.856)<br />5 (0.000)<br />0 (0.971)<br />2 (0.935)<br />3 (0.965)<br />3 (0.541)<br />4 (0.796)<br />5 (0.913)<br />2 (0.386)<br />4 (0.517)<br />3 (0.580)<br />6 (0.657)<br />6 (0.301)<br />6 (0.455)<br />6 (0.546)<br />6 (0.457)<br />5 (0.412)<br />2 (0.893)<br />3 (0.000)<br />5 (0.815)<br />6 (0.072)<br />5 (0.758)<br />5 (0.170)<br />5 (0.227)<br />6 (0.579)<br />6 (0.081)<br />6 (0.189)<br />5 (0.133)<br />3 (0.998)<br />5 (0.882)<br />5 (0.827)<br />5 (0.991)<br />4 (0.808)<br />4 (0.830)<br />4 (0.671)<br />6 (0.000)<br />3 (0.965)<br />2 (0.933)<br />3 (0.963)<br />4 (0.432)<br />2 (0.915)<br />3 (0.926)<br />4 (0.185)<br />5 (0.305)<br />2 (0.681)<br />0 (1.000)<br />0 (1.000)<br />0 (0.999)<br />0 (1.000)<br />0 (0.999)<br />0 (0.999)<br />2 (0.889)<br />1 (0.000)<br />0 (0.972)<br />2 (0.934)<br />1 (0.974)<br />0 (0.941)<br />0 (0.989)<br />0 (0.961)<br />0 (0.707)<br />2 (0.817)<br />0 (0.893)<br />0 (1.000)<br />0 (1.000)<br />0 (1.000)<br />0 (1.000)<br />1 (0.999)<br />0 (0.998)<br />3 (0.825)<br />2 (0.000)<br />0 (0.973)<br />2 (0.928)<br />1 (0.977)<br />0 (0.945)<br />0 (0.989)<br />0 (0.962)<br />1 (0.479)<br />3 (0.792)<br />1 (0.848)<br />50 objectives<br />DTLZ 1<br />DTLZ 2<br />DTLZ 3<br />DTLZ 4<br />DTLZ 5<br />DTLZ 6<br />DTLZ 7<br />Knapsack<br />WFG 1<br />WFG 2<br />WFG 3<br />WFG 4<br />WFG 5<br />WFG 6<br />WFG 7<br />WFG 8<br />WFG 9<br />4 (0.992)<br />3 (1.000)<br />3 (0.984)<br />2 (1.000)<br />5 (0.477)<br />6 (0.112)<br />1 (0.767)<br />0 (0.000)<br />6 (0.210)<br />3 (0.538)<br />6 (0.059)<br />6 (0.011)<br />6 (0.003)<br />4 (0.933)<br />1 (0.312)<br />1 (0.669)<br />5 (0.250)<br />5 (0.985)<br />2 (1.000)<br />2 (1.000)<br />2 (1.000)<br />2 (0.996)<br />2 (0.995)<br />0 (0.966)<br />4 (0.000)<br />4 (0.869)<br />0 (0.962)<br />0 (0.981)<br />2 (0.783)<br />2 (0.940)<br />2 (0.963)<br />5 (0.026)<br />0 (0.913)<br />3 (0.597)<br />2 (1.000)<br />4 (0.998)<br />3 (0.988)<br />4 (1.000)<br />3 (0.954)<br />3 (0.979)<br />5 (0.233)<br />5 (0.000)<br />2 (0.962)<br />0 (0.959)<br />2 (0.972)<br />3 (0.268)<br />4 (0.789)<br />4 (0.941)<br />3 (0.208)<br />4 (0.341)<br />3 (0.559)<br />6 (0.566)<br />6 (0.375)<br />6 (0.518)<br />6 (0.517)<br />5 (0.425)<br />5 (0.399)<br />4 (0.254)<br />3 (0.000)<br />4 (0.823)<br />6 (0.076)<br />5 (0.731)<br />5 (0.118)<br />5 (0.416)<br />6 (0.663)<br />5 (0.022)<br />6 (0.147)<br />6 (0.166)<br />3 (0.999)<br />5 (0.917)<br />5 (0.891)<br />5 (0.999)<br />4 (0.752)<br />4 (0.839)<br />6 (0.020)<br />6 (0.000)<br />2 (0.961)<br />0 (0.952)<br />2 (0.973)<br />3 (0.258)<br />3 (0.913)<br />2 (0.961)<br />4 (0.034)<br />5 (0.233)<br />2 (0.727)<br />1 (1.000)<br />0 (1.000)<br />0 (1.000)<br />0 (1.000)<br />0 (0.999)<br />0 (0.998)<br />2 (0.684)<br />1 (0.000)<br />0 (0.971)<br />2 (0.945)<br />0 (0.976)<br />0 (0.944)<br />1 (0.987)<br />0 (0.974)<br />0 (0.581)<br />1 (0.602)<br />0 (0.907)<br />0 (1.000)<br />0 (1.000)<br />0 (1.000)<br />0 (1.000)<br />0 (0.999)<br />1 (0.998)<br />3 (0.675)<br />2 (0.000)<br />0 (0.970)<br />3 (0.943)<br />0 (0.979)<br />1 (0.908)<br />0 (0.989)<br />0 (0.976)<br />1 (0.378)<br />2 (0.579)<br />0 (0.903)<br />25</p>   </div> <div id="rgw18_56ab2027499d2" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw19_56ab2027499d2">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw20_56ab2027499d2"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://e-collection.ethbib.ethz.ch/eserv/eth:30945/eth-30945-01.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization">HypE: An Algorithm for Fast Hypervolume-Based Many...</a> </div>  <div class="details">   Available from <a href="http://e-collection.ethbib.ethz.ch/eserv/eth:30945/eth-30945-01.pdf" target="_blank" rel="nofollow">ethz.ch</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw33_56ab2027499d2" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (244) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw34_56ab2027499d2" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw35_56ab2027499d2" >  <div class="indent-left">  <div id="rgw36_56ab2027499d2" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/291337172_A_Self-Organizing_Multiobjective_Evolutionary_Algorithm">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Hu_Zhang12" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Hu Zhang </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw37_56ab2027499d2">  <li class="citation-context-item"> "Multiobjective evolutionary algorithms (MOEAs) have been accepted as a major approach for approximating the PF[2],[3], and various MOEAs are proposed[4]–[6]. Three classes of widely investigated and used MOEAs are the Pareto dominance-based[7]–[9], performance indicator- based[10],[11]and decomposition-based approaches[12]–[15]. An efficient MOEA should make use of problem specific knowledge to guide its search. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/291337172_A_Self-Organizing_Multiobjective_Evolutionary_Algorithm"> <span class="publication-title js-publication-title">A Self-Organizing Multiobjective Evolutionary Algorithm</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2001967256_Hu_Zhang" class="authors js-author-name ga-publications-authors">Hu Zhang</a> &middot;     <a href="researcher/12244323_Aimin_Zhou" class="authors js-author-name ga-publications-authors">Aimin Zhou</a> &middot;     <a href="researcher/32568397_Shenmin_Song" class="authors js-author-name ga-publications-authors">Shenmin Song</a> &middot;     <a href="researcher/2095136196_Qingfu_Zhang" class="authors js-author-name ga-publications-authors">Qingfu Zhang</a> &middot;     <a href="researcher/8941014_Xiao-Zhi_Gao" class="authors js-author-name ga-publications-authors">Xiao-Zhi Gao</a> &middot;     <a href="researcher/2095138924_Jun_Zhang" class="authors js-author-name ga-publications-authors">Jun Zhang</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Under mild conditions, the Pareto front (Pareto set) of a continuous m-objective optimization problem forms an (m−1)-dimensional piecewise continuous manifold. Based on this property, this paper proposes a self-organizing multiobjective evolutionary algorithm. At each generation, a self-organizing mapping method with (m − 1) latent variables is applied to establish the neighborhood relationship among current solutions. A solution is only allowed to mate with its neighboring solutions to generate a new solution. To reduce the computational overhead, the self-organizing training step and the evolution step are conducted in an alternative manner. In other words, the self-organizing training is performed only one single step at each generation. The proposed algorithm has been applied to a number of test instances, and compared with some state-of the-art multiobjective evolutionary methods. The results have demonstrated its advantages over other approaches. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Jan 2016  &middot; IEEE Transactions on Evolutionary Computation  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Hu_Zhang12/publication/291337172_A_Self-Organizing_Multiobjective_Evolutionary_Algorithm/links/56a5ce6708ae232fb209759a.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw38_56ab2027499d2" >  <div class="indent-left">  <div id="rgw39_56ab2027499d2" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/291186153_The_Bi-Objective_Quadratic_Multiple_Knapsack_Problem_Model_and_Heuristics">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Jin-Kao_Hao" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Jin-Kao Hao </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw40_56ab2027499d2">  <li class="citation-context-item"> ", where R is a reference set which is the best-identified approximation complied from the approximation sets of all tested configurations. Intuitively, this indicator measures the portion of the objective space that is dominated by R but not by A. The hypervolume indicator is one of the most commonly used measures for evaluating the multi-objective optimization algorithms, since it is the only unary measure which is consistent with the Pareto dominance relation[5], i.e., it allows to obtain a total order between approximation sets. In our experiments, these two indicators were computed based on the normalized objective vectors of the non-dominated solutions. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/291186153_The_Bi-Objective_Quadratic_Multiple_Knapsack_Problem_Model_and_Heuristics"> <span class="publication-title js-publication-title">The Bi-Objective Quadratic Multiple Knapsack Problem: Model and Heuristics</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2051767641_Yuning_Chen" class="authors js-author-name ga-publications-authors">Yuning Chen</a> &middot;     <a href="researcher/6546247_Jin-Kao_Hao" class="authors js-author-name ga-publications-authors">Jin-Kao Hao</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> The single objective quadratic multiple knapsack problem (QMKP) is a useful model to formulate a number of practical problems. However, it is not suitable for situations where more than one objective needs to be considered. In this paper, we extend the single objective QMKP to the bi-objective case such that we simultaneously maximize the total profit of the items packed into the knapsacks and the ’makespan’ (the gain of the least profit knapsack). Given the imposing computational challenge, we propose a hybrid two-stage (HTS) algorithm to approximate the Pareto front of the bi-objective QMKP. HTS combines two different and complementary search methods - scalarizing memetic search (first stage) and Pareto local search (second stage). Experimental assessments on a set of 60 problem instances show that HTS dominates a standard multi-objective evolutionary algorithm (NSGA II), and two simplified variants of HTS. We also present a comparison with two state-of-the-art algorithms for the single objective QMKP to assess the quality of the extreme solutions of the approximated Pareto front. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Jan 2016  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Jin-Kao_Hao/publication/291186153_The_Bi-Objective_Quadratic_Multiple_Knapsack_Problem_Model_and_Heuristics/links/56a230a608ae2afab8867dfb.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw41_56ab2027499d2" >  <div class="indent-left">  <div id="rgw42_56ab2027499d2" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/288827687_Multiobjective_evolutionary_algorithm_based_on_multimethod_with_dynamic_resources_allocation">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Wali_Mashwani2" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Wali Khan Mashwani </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw43_56ab2027499d2">  <li class="citation-context-item"> "Dur to population based nature, they can find a diversified set of solutions for the given MOPs in single simulation run unlike traditional techniques [18] [9] [19] [11] [20]. In general, classical MOEAs can be divided into three main different classes, namely, the Pareto dominance based MOEAs (e.g., [21] [22] [23] [24] [25] [26] [27] [28] [29]), the decomposition based MOEAs (e.g., [30–40,19,41,42]), and Indicator Based algorithms (e.g., [43] [44] [45] [46] [47] [48] [49]). Among above mentioned three classes, Pareto dominance based MOEAs are very commonly used in the existing specialized literature of EC. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/288827687_Multiobjective_evolutionary_algorithm_based_on_multimethod_with_dynamic_resources_allocation"> <span class="publication-title js-publication-title">Multiobjective evolutionary algorithm based on multimethod with dynamic resources allocation</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2034216062_Wali_Khan_Mashwani" class="authors js-author-name ga-publications-authors">Wali Khan Mashwani</a> &middot;     <a href="researcher/2077832430_Abdel_Salhi" class="authors js-author-name ga-publications-authors">Abdel Salhi</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract"></a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract"></a><br/>   </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Jan 2016  &middot; Applied Soft Computing  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Wali_Mashwani2/publication/288827687_Multiobjective_evolutionary_algorithm_based_on_multimethod_with_dynamic_resources_allocation/links/5684c29008ae1975839389fa.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw22_56ab2027499d2" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw23_56ab2027499d2">  </ul> </div> </div>  <div id="rgw27_56ab2027499d2" class="questions-container"> <h2>Questions & Answers about this publication</h2> <ul class="c-list post-list">  <li id="rgw28_56ab2027499d2" class="c-list-item topics-post-feed-item ">   <div class="topics-post-feed-item-reason"><a href="profile/Carlos_Azevedo2" class="js-reason-profile">Carlos R. B. Azevedo</a> added an answer in <a href="topic/ant_colony_optimization">Ant Colony Optimization</a>:</div>   <a class="js-question-title topics-post-feed-item-title" href="post/What_are_the_most_interesting_non-metaphor-based_metaheuristics_out_there">What are the most interesting non-metaphor-based metaheuristics out there?</a> <div class="topics-post-feed-item-text expandable-body rich-text-unstyled topics-post-feed-item-has-comment"> <div class="expander-container js-expander-container js-expander-collapsed"> <p>We are all familiar with the proliferation of metaphor-based metaheuristics.  From the stalwarts of Simulated Annealing, Genetic Algorithms, Ant Colony Optimization, and Particle Swarm Optimization, we have many others such as Harmony Search, Imperialist Competitive Algorithm, and Fireworks Algorithms.  It is not even certain that some of these metaphors (e.g. fireworks) perform optimization in the real-world.</p>
<p>More important than mimicking a metaphor, our goal is to perform optimization.  Focusing on this task, non-metaphor-based techniques such as Tabu Search and Estimation of Distribution Algorithms have been developed to address specific aspects of search spaces.</p>
<p>Can anyone tell me more about other interesting non-metaphor-based metaheuristics, and the specific optimization task that they are designed to address?</p> </div> </div>    <div class="topics-post-feed-item-comments">  <div id="rgw29_56ab2027499d2" class="topics-post-feed-item-comment"> <div class="topics-post-feed-item-comment-author truncate-single-line"> <span class="topics-post-feed-item-info-authorname"><a href="profile/Carlos_Azevedo2">Carlos R. B. Azevedo</a></span> </div> <div class="clearfix"> <div class="topics-post-feed-item-comment-text expandable-body rich-text-unstyled"> <div class="expander-container js-expander-container js-expander-collapsed"><p>Dear Stephen,</p>
<p>This is a very good question because it has been increasingly difficult to find novel research in metaheuristics without recurring to inspirations of any kind. Attempts of unifying the various flavors of metaphor-based metaheuristics into an unique framework have appeared e.g. in Taillard et al. (2001).<br /><br />On the other hand, I think many researchers have turned on improving metaheuristics for a vast range of domains, by integrating concepts that do not rely on metaphors.</p>
<p>One example of this are indicator-based metaheuristics for Multi-Objective Optimization (MOO), whose main challenge is on designing efficient ways of measuring the quality of sets of mutually non-inferior solutions in order to approximate the Pareto Frontier. Although many indicator-based MOO algorithms are still grounded in evolutionary-inspired frameworks, the focus is entirely on how to design heuristic operators that take advantage of the information provided by such indicators. See e.g. Bader and Zitzler (2008).</p>
<p>Now, to answer your question, I can point out a few interesting modern metaheuristics not entirely focused on metaphors:</p>
<p>- <strong>Natural evolution strategies</strong>: these are methods that update candidate solutions in continous search spaces in the direction of the natural gradient (a more general gradient estimation based on differential geometry, see Amari and Douglas (1998)). A recent example can be found in Sun et al. (2009) for numerical optimization problems and a non-markovian double pole balancing application.</p>
<p>- <strong>Cross entropy methods</strong>: given a parametric family of distributions and an adaptive threshold on the objective function values sampled so far, these methods can be viewed as the higher-level strategy of finding the distribution representing the elite solutions by minimizing the relative entropy. Rubinstein (1999) applied CE both for continuous and combinatorial optimization (TSP, shortest path). In my opinion, CE could also be viewed as an Estimation of Distribution Algorithm (EDA).</p>
<p>- <strong>Particle Filter methods</strong>: these were presented as a "reformulation of stochastic global optimization as a filtering problem" and have been applied for parametric density estimation. See Stinis (2009). Again, IMO, these are closely related to EDAs, although having a much more formal flavor and convergence guarantees.</p>
<p>- <strong>Quantum annealing</strong>: these methods can be considered as the quantum counterpart to simulated annealing, one of the most popular and well-succeeded metaheuristics. Now, these are interesting because they can be implemented in a quantum device for actually physically performing the steps required to achieve a configuration of minimal potential energy. That's the approach the canadian company D-Wave claims to have implemented by using an adiabatic quantum computation processor. See e.g. Finnila et al. (1994).</p>
<p>A very good recent survey on metaheuristics for optimization appeared in Boussaïd, Lepagnot, and Siarry (2013), where you can find several examples not mentioned in your question but that came out long ago (the following references can be found in the original paper):</p>
<p>- <strong>Greedy Randomized Adaptive Search Procedure</strong> (GRASP) after Feo and Resende (1989) who designed such methods mainly for "set covering problems that arise from Steiner triple systems"</p>
<p>- <strong>Noising Method</strong> (NM) after Charon and Hudry (1993) that applied it for "the clique partitioning of a graph"</p>
<p>- <strong>Variable Neighborhood Search</strong> (VNS) after Hansen and Mladenovic (1997) for the traveling salesman problem</p>
<p>- <strong>Guided local search</strong> (GLS) after C. Voudouris, E. Tsang (1999) also for TSP</p>
<p>- <strong>Iterated local search</strong> (ITL) after Stützle (1998) for TSP, quadratic assignment problems, and flow shop problems</p>
<p>- <strong>Scatter Search</strong> (SS) and <strong>Path Relinking</strong> (PR) after Glover (1997) are methods closely-related with evolutionary-inspired algorithms, the difference being that principled heuristics are provided for combining multiple candidate solutions to produce novel ones.<br /><br />Hope that helps.</p></div> </div> </div> <div id="rgw30_56ab2027499d2" class="post-attachments-list">     <ul class="publication-attachments-list attachments-category">   <li class="c-list-item li-publication  "  id="rgw31_56ab2027499d2" >  <div class="indent-left">  <div id="rgw32_56ab2027499d2" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/1782110_Quantum_Annealing_A_New_Method_for_Minimizing_Multidimensional_Functions">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Maria_Gomez41" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Maria A. Gomez </div> </div>   </div>  </div>  <div class="indent-right">    </div>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/1782110_Quantum_Annealing_A_New_Method_for_Minimizing_Multidimensional_Functions"> <span class="publication-title js-publication-title">Quantum Annealing: A New Method for Minimizing Multidimensional Functions</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/6167779_A_B_Finnila" class="authors js-author-name ga-publications-authors">A. B. Finnila</a> &middot;     <a href="researcher/12219872_M_A_Gomez" class="authors js-author-name ga-publications-authors">M. A. Gomez</a> &middot;     <a href="researcher/28779477_C_Sebenik" class="authors js-author-name ga-publications-authors">C. Sebenik</a> &middot;     <a href="researcher/28780081_C_Stenson" class="authors js-author-name ga-publications-authors">C. Stenson</a> &middot;     <a href="researcher/6488200_J_D_Doll" class="authors js-author-name ga-publications-authors">J. D. Doll</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Quantum annealing is a new method for finding extrema of multidimensional functions. Based on an extension of classical, simulated annealing, this approach appears robust with respect to avoiding local minima. Further, unlike some of its predecessors, it does not require an approximation to a wavefunction. In this paper, we apply the technique to the problem of finding the lowest energy configurations of Lennard-Jones clusters of up to 19 particles (roughly 10$^5$ local minima). This early success suggests that this method may complement the widely implemented technique of simulated annealing. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Apr 1994  &middot; Chemical Physics Letters  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Maria_Gomez41/publication/1782110_Quantum_Annealing_A_New_Method_for_Minimizing_Multidimensional_Functions/links/53db9d870cf216e4210bfae1.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>    </ul>   <p class="text-gray-lighter">+ 7 more attachments</p>  </div> </div>  </div>    <div class="topics-post-feed-item-actions">  <div class="topics-post-feed-item-actions-followconfirm lf action-confirm details" style="display: none;"><span class="topics-post-feed-item-actions-followconfirm-ico ico-confirm"></span> Following </div>   <div class="lf topics-post-feed-item-info details"> </div> <div class="clear"></div> </div>  <div class="clear"></div> </li>  </ul>  </div> <div id="rgw14_56ab2027499d2" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw15_56ab2027499d2"> <div> <h5> <a href="publication/256721268_An_adaptive_parameter_binary-real_coded_genetic_algorithm_for_constraint_optimization_problems_Performance_analysis_and_estimation_of_optimal_control_parameters" class="color-inherit ga-similar-publication-title"><span class="publication-title">An adaptive parameter binary-real coded genetic algorithm for constraint optimization problems: Performance analysis and estimation of optimal control parameters</span></a>  </h5>  <div class="authors"> <a href="researcher/69748453_Omar_Arif_Abdul-Rahman" class="authors ga-similar-publication-author">Omar Arif Abdul-Rahman</a>, <a href="researcher/7349570_Masaharu_Munetomo" class="authors ga-similar-publication-author">Masaharu Munetomo</a>, <a href="researcher/8679009_Kiyoshi_Akama" class="authors ga-similar-publication-author">Kiyoshi Akama</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw16_56ab2027499d2"> <div> <h5> <a href="publication/221228399_OCD_Online_Convergence_Detection_for_Evolutionary_Multi-Objective_Algorithms_Based_on_Statistical_Testing" class="color-inherit ga-similar-publication-title"><span class="publication-title">OCD: Online Convergence Detection for Evolutionary Multi-Objective Algorithms Based on Statistical Testing</span></a>  </h5>  <div class="authors"> <a href="researcher/22462338_Tobias_Wagner" class="authors ga-similar-publication-author">Tobias Wagner</a>, <a href="researcher/12576571_Heike_Trautmann" class="authors ga-similar-publication-author">Heike Trautmann</a>, <a href="researcher/7008667_Boris_Naujoks" class="authors ga-similar-publication-author">Boris Naujoks</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56ab2027499d2"> <div> <h5> <a href="publication/220741358_Self_organized_multi-agent_entangled_hierarchies_for_network_security" class="color-inherit ga-similar-publication-title"><span class="publication-title">Self organized multi-agent entangled hierarchies for network security.</span></a>  </h5>  <div class="authors"> <a href="researcher/69854795_Eric_M_Holloway" class="authors ga-similar-publication-author">Eric M. Holloway</a>, <a href="researcher/843397_Gary_B_Lamont" class="authors ga-similar-publication-author">Gary B. Lamont</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw45_56ab2027499d2" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw46_56ab2027499d2">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw47_56ab2027499d2" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=ZMWlIXLiT4XS2uju8zPqQuXszS5PiEmviHY11XfctOEmzmopm5bA-B0bcttQ73eT" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="xL4/318ENSaFCDabUHTYQ1LB0ewQdJzj5bQDsG0sA+TYGzruZS686HGDMWOjEfpnITcS3G3s9gj/STl+cL+gzNO3K68RilAEeAgrFJNhHee80d876C0A7EqeeYB/a0anIPPFpEn+yWeD7xhay/QkA2hPTezFEGhPBTrN5iu6NWPZPT9kkm0iY9mr+gsTRagg/JgwynjZxbo3+8s16RCb7JdT7MwjoUTCe8Zk6vLQFMXClvoOEhi0kZooHRJv2Z/DoHGPlsj+o8WXvBmSvk6/7a3M85yH8wUvThY5JyU/XCc="/> <input type="hidden" name="urlAfterLogin" value="publication/45280500_HypE_An_Algorithm_for_Fast_Hypervolume-Based_Many-Objective_Optimization"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vNDUyODA1MDBfSHlwRV9Bbl9BbGdvcml0aG1fZm9yX0Zhc3RfSHlwZXJ2b2x1bWUtQmFzZWRfTWFueS1PYmplY3RpdmVfT3B0aW1pemF0aW9u"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vNDUyODA1MDBfSHlwRV9Bbl9BbGdvcml0aG1fZm9yX0Zhc3RfSHlwZXJ2b2x1bWUtQmFzZWRfTWFueS1PYmplY3RpdmVfT3B0aW1pemF0aW9u"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vNDUyODA1MDBfSHlwRV9Bbl9BbGdvcml0aG1fZm9yX0Zhc3RfSHlwZXJ2b2x1bWUtQmFzZWRfTWFueS1PYmplY3RpdmVfT3B0aW1pemF0aW9u"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw48_56ab2027499d2"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 621;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-modules-publictopics"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Eckart Zitzler","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Eckart_Zitzler","institution":"P\u00e4dagogische Hochschule Bern","institutionUrl":false,"widgetId":"rgw4_56ab2027499d2"},"id":"rgw4_56ab2027499d2","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=1673602","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab2027499d2"},"id":"rgw3_56ab2027499d2","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=45280500","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":45280500,"title":"HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization","journalTitle":"Evolutionary Computation","journalDetailsTooltip":{"data":{"journalTitle":"Evolutionary Computation","journalAbbrev":"Evol Comput","publisher":"Massachusetts Institute of Technology Press (MIT Press)","issn":"1530-9304","impactFactor":"2.37","fiveYearImpactFactor":"3.01","citedHalfLife":">10.0","immediacyIndex":"0.82","eigenFactor":"0.00","articleInfluence":"1.36","widgetId":"rgw6_56ab2027499d2"},"id":"rgw6_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=1530-9304","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":"Computer Engineering and Networks Laboratory, ETH Zurich, 8092 Zurich, Switzerland. ","type":"Article","details":{"doi":"10.1162\/EVCO_a_00009","journalInfos":{"journal":"","publicationDate":"03\/2011;","publicationDateRobot":"2011-03","article":"19(1):45-76.","journalTitle":"Evolutionary Computation","journalUrl":"journal\/1530-9304_Evolutionary_Computation","impactFactor":2.37}},"source":{"sourceUrl":"http:\/\/www.ncbi.nlm.nih.gov\/pubmed\/20649424","sourceName":"PubMed"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1162\/EVCO_a_00009"},{"key":"rft.atitle","value":"HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization"},{"key":"rft.title","value":"Evolutionary computation"},{"key":"rft.jtitle","value":"Evolutionary computation"},{"key":"rft.volume","value":"19"},{"key":"rft.issue","value":"1"},{"key":"rft.date","value":"2011"},{"key":"rft.pages","value":"45-76"},{"key":"rft.issn","value":"1530-9304"},{"key":"rft.au","value":"Johannes Bader,Eckart Zitzler"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56ab2027499d2"},"id":"rgw7_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=45280500","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":45280500,"peopleItems":[{"data":{"authorUrl":"researcher\/55315225_Johannes_Bader","authorNameOnPublication":"Johannes Bader","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Johannes Bader","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/55315225_Johannes_Bader","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw10_56ab2027499d2"},"id":"rgw10_56ab2027499d2","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=55315225&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw9_56ab2027499d2"},"id":"rgw9_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=55315225&authorNameOnPublication=Johannes%20Bader","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Eckart Zitzler","accountUrl":"profile\/Eckart_Zitzler","accountKey":"Eckart_Zitzler","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Eckart Zitzler","profile":{"professionalInstitution":{"professionalInstitutionName":"P\u00e4dagogische Hochschule Bern","professionalInstitutionUrl":"institution\/Paedagogische_Hochschule_Bern"}},"professionalInstitutionName":"P\u00e4dagogische Hochschule Bern","professionalInstitutionUrl":"institution\/Paedagogische_Hochschule_Bern","url":"profile\/Eckart_Zitzler","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Eckart_Zitzler","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw12_56ab2027499d2"},"id":"rgw12_56ab2027499d2","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1673602&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"P\u00e4dagogische Hochschule Bern","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":2,"accountCount":1,"publicationUid":45280500,"widgetId":"rgw11_56ab2027499d2"},"id":"rgw11_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1673602&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=2&accountCount=1&publicationUid=45280500","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab2027499d2"},"id":"rgw8_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=45280500&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":45280500,"abstract":"<noscript><\/noscript><div>In the field of evolutionary multi-criterion optimization, the hypervolume indicator is the only single set quality measure that is known to be strictly monotonic with regard to Pareto dominance: whenever a Pareto set approximation entirely dominates another one, then the indicator value of the dominant set will also be better. This property is of high interest and relevance for problems involving a large number of objective functions. However, the high computational effort required for hypervolume calculation has so far prevented the full exploitation of this indicator's potential; current hypervolume-based search algorithms are limited to problems with only a few objectives. This paper addresses this issue and proposes a fast search algorithm that uses Monte Carlo simulation to approximate the exact hypervolume values. The main idea is not that the actual indicator values are important, but rather that the rankings of solutions induced by the hypervolume indicator. In detail, we present HypE, a hypervolume estimation algorithm for multi-objective optimization, by which the accuracy of the estimates and the available computing resources can be traded off; thereby, not only do many-objective problems become feasible with hypervolume-based search, but also the runtime can be flexibly adapted. Moreover, we show how the same principle can be used to statistically compare the outcomes of different multi-objective optimizers with respect to the hypervolume--so far, statistical testing has been restricted to scenarios with few objectives. The experimental results indicate that HypE is highly effective for many-objective problems in comparison to existing multi-objective evolutionary algorithms. HypE is available for download at http:\/\/www.tik.ee.ethz.ch\/sop\/download\/supplementary\/hype\/.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw13_56ab2027499d2"},"id":"rgw13_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=45280500","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/45280500_HypE_An_Algorithm_for_Fast_Hypervolume-Based_Many-Objective_Optimization\/links\/0e60a57ef0c4cf5df7c57d44\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw5_56ab2027499d2"},"id":"rgw5_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=45280500&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":69748453,"url":"researcher\/69748453_Omar_Arif_Abdul-Rahman","fullname":"Omar Arif Abdul-Rahman","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7349570,"url":"researcher\/7349570_Masaharu_Munetomo","fullname":"Masaharu Munetomo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8679009,"url":"researcher\/8679009_Kiyoshi_Akama","fullname":"Kiyoshi Akama","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jun 2013","journal":"Information Sciences","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/256721268_An_adaptive_parameter_binary-real_coded_genetic_algorithm_for_constraint_optimization_problems_Performance_analysis_and_estimation_of_optimal_control_parameters","usePlainButton":true,"publicationUid":256721268,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"4.04","url":"publication\/256721268_An_adaptive_parameter_binary-real_coded_genetic_algorithm_for_constraint_optimization_problems_Performance_analysis_and_estimation_of_optimal_control_parameters","title":"An adaptive parameter binary-real coded genetic algorithm for constraint optimization problems: Performance analysis and estimation of optimal control parameters","displayTitleAsLink":true,"authors":[{"id":69748453,"url":"researcher\/69748453_Omar_Arif_Abdul-Rahman","fullname":"Omar Arif Abdul-Rahman","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7349570,"url":"researcher\/7349570_Masaharu_Munetomo","fullname":"Masaharu Munetomo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8679009,"url":"researcher\/8679009_Kiyoshi_Akama","fullname":"Kiyoshi Akama","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Information Sciences 06\/2013; 233:54\u201386. DOI:10.1016\/j.ins.2013.01.005"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/256721268_An_adaptive_parameter_binary-real_coded_genetic_algorithm_for_constraint_optimization_problems_Performance_analysis_and_estimation_of_optimal_control_parameters","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/256721268_An_adaptive_parameter_binary-real_coded_genetic_algorithm_for_constraint_optimization_problems_Performance_analysis_and_estimation_of_optimal_control_parameters\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw15_56ab2027499d2"},"id":"rgw15_56ab2027499d2","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=256721268","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":22462338,"url":"researcher\/22462338_Tobias_Wagner","fullname":"Tobias Wagner","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":12576571,"url":"researcher\/12576571_Heike_Trautmann","fullname":"Heike Trautmann","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7008667,"url":"researcher\/7008667_Boris_Naujoks","fullname":"Boris Naujoks","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Conference Paper","publicationDate":"Apr 2009","journal":"Lecture Notes in Computer Science","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/221228399_OCD_Online_Convergence_Detection_for_Evolutionary_Multi-Objective_Algorithms_Based_on_Statistical_Testing","usePlainButton":true,"publicationUid":221228399,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"0.51","url":"publication\/221228399_OCD_Online_Convergence_Detection_for_Evolutionary_Multi-Objective_Algorithms_Based_on_Statistical_Testing","title":"OCD: Online Convergence Detection for Evolutionary Multi-Objective Algorithms Based on Statistical Testing","displayTitleAsLink":true,"authors":[{"id":22462338,"url":"researcher\/22462338_Tobias_Wagner","fullname":"Tobias Wagner","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":12576571,"url":"researcher\/12576571_Heike_Trautmann","fullname":"Heike Trautmann","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7008667,"url":"researcher\/7008667_Boris_Naujoks","fullname":"Boris Naujoks","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Evolutionary Multi-Criterion Optimization, 5th International Conference, EMO 2009, Nantes, France, April 7-10, 2009. Proceedings; 04\/2009"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/221228399_OCD_Online_Convergence_Detection_for_Evolutionary_Multi-Objective_Algorithms_Based_on_Statistical_Testing","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/221228399_OCD_Online_Convergence_Detection_for_Evolutionary_Multi-Objective_Algorithms_Based_on_Statistical_Testing\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56ab2027499d2"},"id":"rgw16_56ab2027499d2","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=221228399","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":69854795,"url":"researcher\/69854795_Eric_M_Holloway","fullname":"Eric M. Holloway","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":843397,"url":"researcher\/843397_Gary_B_Lamont","fullname":"Gary B. Lamont","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Conference Paper","publicationDate":"Jan 2009","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/220741358_Self_organized_multi-agent_entangled_hierarchies_for_network_security","usePlainButton":true,"publicationUid":220741358,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/220741358_Self_organized_multi-agent_entangled_hierarchies_for_network_security","title":"Self organized multi-agent entangled hierarchies for network security.","displayTitleAsLink":true,"authors":[{"id":69854795,"url":"researcher\/69854795_Eric_M_Holloway","fullname":"Eric M. Holloway","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":843397,"url":"researcher\/843397_Gary_B_Lamont","fullname":"Gary B. Lamont","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Genetic and Evolutionary Computation Conference, GECCO 2009, Proceedings, Montreal, Qu\u00e9bec, Canada, July 8-12, 2009, Companion Material; 01\/2009"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/220741358_Self_organized_multi-agent_entangled_hierarchies_for_network_security","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/220741358_Self_organized_multi-agent_entangled_hierarchies_for_network_security\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab2027499d2"},"id":"rgw17_56ab2027499d2","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=220741358","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw14_56ab2027499d2"},"id":"rgw14_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=45280500&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":45280500,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":45280500,"publicationType":"article","linkId":"0e60a57ef0c4cf5df7c57d44","fileName":"HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization","fileUrl":"http:\/\/e-collection.ethbib.ethz.ch\/eserv\/eth:30945\/eth-30945-01.pdf","name":"ethz.ch","nameUrl":"http:\/\/e-collection.ethbib.ethz.ch\/eserv\/eth:30945\/eth-30945-01.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw20_56ab2027499d2"},"id":"rgw20_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=45280500&linkId=0e60a57ef0c4cf5df7c57d44&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw19_56ab2027499d2"},"id":"rgw19_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=45280500&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":35,"valueFormatted":"35","widgetId":"rgw21_56ab2027499d2"},"id":"rgw21_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=45280500","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw18_56ab2027499d2"},"id":"rgw18_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=45280500&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":45280500,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw23_56ab2027499d2"},"id":"rgw23_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=45280500&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":35,"valueFormatted":"35","widgetId":"rgw24_56ab2027499d2"},"id":"rgw24_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=45280500","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw22_56ab2027499d2"},"id":"rgw22_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=45280500&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"1\nHypE: An Algorithm for Fast Hypervolume-Based\nMany-Objective Optimization\nJohannes Bader and Eckart Zitzler\nComputer Engineering and Networks Laboratory, ETH Zurich, 8092 Zurich, Switzerland\n{johannes.bader,eckart.zitzler}@tik.ee.ethz.ch\nTIK-Report No. 286\nNovember 26, 2008\nAbstract\u2014In the field of evolutionary multi-criterion optimiza-\ntion, the hypervolume indicator is the only single set quality\nmeasure that is known to be strictly monotonic with regard to\nPareto dominance: whenever a Pareto set approximation entirely\ndominates another one, then also the indicator value of the former\nwill be better. This property is of high interest and relevance\nfor problems involving a large number of objective functions.\nHowever, the high computational effort required for hypervolume\ncalculation has so far prevented to fully exploit the potential of\nthis indicator; current hypervolume-based search algorithms are\nlimited to problems with only a few objectives.\nThis paper addresses this issue and proposes a fast search\nalgorithm that uses Monte Carlo simulation to approximate the\nexact hypervolume values. The main idea is that not the actual\nindicator values are important, but rather the rankings of solu-\ntions induced by the hypervolume indicator. In detail, we present\nHypE, a hypervolume estimation algorithm for multiobjective\noptimization, by which the accuracy of the estimates and the\navailable computing resources can be traded off; thereby, not\nonly many-objective problems become feasible with hypervolume-\nbased search, but also the runtime can be flexibly adapted.\nMoreover, we show how the same principle can be used to\nstatistically compare the outcomes of different multiobjective\noptimizers with respect to the hypervolume\u2014so far, statistical\ntesting has been restricted to scenarios with few objectives. The\nexperimental results indicate that HypE is highly effective for\nmany-objective problems in comparison to existing multiobjective\nevolutionary algorithms.\nHypE is available for download at http:\/\/www.tik.ee.ethz.ch\/\nsop\/download\/supplementary\/hype\/.\nI. MOTIVATION\nBy far most studies in the field of evolutionary multiobjec-\ntive optimization (EMO) are concerned with the following set\nproblem: find a set of solutions that as a whole represents a\ngood approximation of the Pareto-optimal set. To this end, the\noriginal multiobjective problem consisting of\n\u2022 the decision space X,\n\u2022 the objective space Z = Rn,\n\u2022 a vector function f = (f1,f2,...,fn) comprising n\nobjective functions fi: X \u2192 R, which are without loss\nof generality to be minimized, and\n\u2022 a relation \u2264 on Z, which induces a preference relation\n? on X with a ? b :\u21d4 f(a) \u2264 f(b) for a,b \u2208 X,\nis usually transformed into a single-objective set problem [46].\nThe search space \u03a8 of the resulting set problem includes\nall possible Pareto set approximations1, i.e., \u03a8 contains all\nmultisets over X. The preference relation ? can be used to\ndefine a corresponding set preference relation ? on \u03a8 where\nA ? B :\u21d4 \u2200b \u2208 B \u2203a \u2208 A : a ? b\nfor all Pareto set approximations A,B \u2208 \u03a8. In the following,\nwe will assume that weak Pareto dominance is the underlying\npreference relation, cf. [46].2\nA key question when tackling such a set problem is how\nto define the optimization criterion. Many multiobjective evo-\nlutionary algorithms (MOEAs) implement a combination of\nPareto dominance on sets and a diversity measure based on\nEuclidean distance in the objective space, e.g., NSGA-II [13]\nand SPEA2 [42]. While these methods have been successfully\nemployed in various biobjective optimization scenarios, they\nappear to have difficulties when the number of objectives\nincreases [34]. As a consequence, researchers have tried to\ndevelop alternative concepts, and a recent trend is to use\nset quality measures, also denoted as quality indicators, for\nsearch\u2014so far, they have mainly been used for performance\nassessment. Of particular interest in this context is the hy-\npervolume indicator [43], [45] as it is the only quality in-\ndicator known to be fully sensitive to Pareto dominance\u2014a\nproperty especially desirable when many objective functions\nare involved.\nSeveral hypervolume-based MOEAs have been proposed\nmeanwhile, e.g., [15], [23], [9], but their main drawback is\ntheir extreme computational overhead. Although there have\nbeen recent studies presenting improved algorithms for hy-\npervolume calculation, currently high-dimensional problems\nwith six or more objectives are infeasible for these MOEAs.\nTherefore, the question is whether and how fast hypervolume-\nbased search algorithms can be designed that exploit the\n(1)\n1Here, a Pareto set approximation may also contain dominated solutions as\nwell as duplicates, in contrast to the notation in [47].\n2For reasons of simplicity, we will use the term \u2018u weakly dominates v\u2019\nresp. \u2018u dominates v\u2019 independently of whether u and v are elements of X,\nZ, or \u03a8. For instance, A weakly dominates b with A \u2208 \u03a8 and b \u2208 X means\nA ? {b} and a dominates z with a \u2208 X and z \u2208 Z means f(a) \u2264 z \u2227 z ?\u2264\nf(a)."},{"page":2,"text":"advantages of the hypervolume indicator and at the same time\nare scalable with respect to the number of objectives.\nA first attempt in this direction has been presented in [1].\nThe main idea is to estimate\u2014by means of Monte Carlo\nsimulation\u2014the ranking of the individuals that is induced by\nthe hypervolume indicator and not to determine the exact indi-\ncator values. This paper proposes an advanced method called\nHypE (Hypervolume Estimation Algorithm for Multiobjective\nOptimization) that is based on the same idea, but uses more\neffective fitness assignment and sampling strategies. In detail,\nthe main contributions of this work can be summarized as\nfollows:\n1) A novel method to assign fitness values to individuals\nbased on the hypervolume indicator\u2014for both mating\nand environmental selection;\n2) A hypervolume-based search algorithm (HypE) using\nMonte Carlo simulation that can be applied to problems\nwith arbitrarily many objectives;\n3) A statistical testing procedure that allows to compare\nthe outcomes of different multiobjective optimizers with\nrespect to the hypervolume indicator in many-objective\nscenarios.\nAs we will show in the follwing, the proposed search algo-\nrithm can be easily tuned regarding the available computing\nresources and the number of objectives involved. Thereby,\nit opens a new perspective on how to treat many-objective\nproblems, and the presented concepts may also be helpful\nfor other types of quality indicators to be integrated in the\noptimization process.\nII. A BRIEF REVIEW OF HYPERVOLUME-RELATED\nRESEARCH\nThe hypervolume indicator was originally proposed and\nemployed in [44], [45] to quantitatively compare the outcomes\nof different MOEAs. In these two first publications, the\nindicator was denoted as \u2018size of the space covered\u2019, and later\nalso other terms such as \u2018hyperarea metric\u2019 [33], \u2018S-metric\u2019\n[38], \u2018hypervolume indicator\u2019 [47], and hypervolume measure\n[4] were used. Besides the names, there are also different\ndefinitions available, based on polytopes [45], the Lebesgue\nmeasure [27], [26], [18], or the attainment function [40].\nAs to hypervolume calculation, the first algorithms [39],\n[26] operated recursively and in each recursion step the num-\nber of objectives was decremented; the underlying principle is\nknown as \u2018hypervolume by slicing objectives\u2019 approach [36].\nWhile the method used in [44], [45] was never published\n(only the source code is publicly available [39]), Knowles\nindependently proposed and described a similar method in\n[26]. A few years later, this approach was the first time studied\nsystematically and heuristics to accelerate the computation\nwere proposed in [36]. All these algorithms have a worst-\ncase runtime complexity that is exponential in the number of\nobjecives, more specifically O(Nn\u22121) where N is the number\nof solutions considered [26], [36]. A different approach was\npresented by Fleischer [18] who mistakenly claimed a polyno-\nmial worst-case runtime complexity\u2014While [35] showed that\nit is exponential in n as well. Recently, advanced algorithms\nfor hypervolume calculation have been proposed, a dimension-\nsweep method [19] with a worst-case runtime complexity of\nO(Nn\u22122logN), and a specialized algorithm related to the\nKlee measure problem [5] the runtime of which is in the\nworst case of order O(N logN + Nn\/2). Furthermore, Yang\nand Ding [37] described an algorithm for which they claim a\nworst-case runtime complexity of O((n\/2)N). The fact that\nthere is no exact polynomial algorithm available gave rise\nto the hypothesis that this problem in general is hard to\nsolve, although the tighest known lower bound is of order\n\u03a9(N logN) [3]. New results substantiate this hypothesis:\nBringmann and Friedrich [8] have proven that the problem\nof computing the hypervolume is #P-complete, i.e., it is\nexpected that no polynomial algorithm exists since this would\nimply NP = P.\nThe complexity of the hypervolume calculation in terms\nof programming and computation time may explain why this\nmeasure was seldom used until 2003. However, this changed\nwith the advent of theoretical studies that provided evidence\nfor a unique property of this indicator [24], [47], [18]: it is the\nonly indicator known to be strictly monotonic with respect to\nPareto dominance and thereby guaranteeing that the Pareto-\noptimal front achieves the maximum hypervolume possible,\nwhile any worse set will be assigned a worse indicator value.\nThis property is especially desirable with many-objective\nproblems and since classical MOEAs have been shown to have\ndifficulties in such scenarios [34], a trend can be observed\nin the literature to directly use the hypervolume indicator for\nsearch.\nKnowles and Corne [26], [25] were the first to propose the\nintegration of the hypervolume indicator into the optimization\nprocess. In particular, they described a strategy to maintain\na separate, bounded archive of nondominated solutions based\non the hypervolume indicator. Huband et al. [22] presented\nan MOEA which includes a modified SPEA2 environmental\nselection procedure where a hypervolume-related measure\nreplaces the original density estimation technique. In [41],\nthe binary hypervolume indicator was used to compare in-\ndividuals and to assign corresponding fitness values within a\ngeneral indicator-based evolutionary algorithm (IBEA). The\nfirst MOEA tailored specifically to the hypervolume indicator\nwas described in [15]; it combines nondominated sorting with\nthe hypervolume indicator and considers one offspring per\ngeneration (steady state). Similar fitness assignment strategies\nwere later adopted in [40], [23], and also other search al-\ngorithms were proposed where the hypervolume indicator is\npartially used for search guidance [29], [28]. Moreover, spe-\ncific aspects like hypervolume-based environmental selection\n[7], cf. Section III-B, and explicit gradient determination for\nhypervolume landscapes [16] have been investigated recently.\nTo date, the hypervolume indicator is one of the most\npopular set quality measures. For instance, almost one fourth\nof the papers published in the proceedings of the EMO 2007\nconference [30] report on the use of or are dedicated to the\nhypervolume indicator. However, there are still two major\ndrawbacks that current research acitivities try to tackle: (i)\n2"},{"page":3,"text":"the high computation effort and (ii) the bias of the indicator in\nterms of user preferences. The former issue has been addressed\nin different ways: by automatically reducing the number of\nobjectives [9] and by approximating the indicator values using\nMonte Carlo methods [17], [1], [11]. Everson et al. [17] used\na basic Monte Carlo technique for performance assessment\nin order to estimate the values of the binary hypervolume\nindicator [38]; with their approach the error ratio is not\npolynomially bounded. In contrast, the scheme presented in\n[8] is a fully polynomial randomized approximation scheme\nwhere the error ratio is polynomial in the input size. The\nissue of statistically comparing hypervolume estimates was\nnot addressed in these two papers. Another study [1]\u2014a\nprecursor study for the present paper\u2014employed Monte Carlo\nsimulation for fast hypervolume-based search. As to the bias\nissue, first proof-of-principle results have been presented in\n[40] that demonstrate that and how the hypervolume indicator\ncan be adapted to different user preferences.\nIII. HYPERVOLUME-BASED FITNESS ASSIGNMENT\nWhen considering the hypervolume indicator as the objec-\ntive function of the underlying set problem, the main question\nis how to make use of this measure within a multiobjective\noptimizer to guide the search. In the context of an MOEA,\nthis refers to selection and one can distinguish two situations:\n1) The selection of solutions to be varied (mating selec-\ntion).\n2) The selection of solutions to be kept in memory (envi-\nronmental selection).\nSince the indicator as such operates on (multi)sets of solu-\ntions, while selection considers single solutions, a strategy\nfor assigning fitness values to solutions is required. Most\nhypervolume-based algorithms first perform a nondominated\nsorting and then rank solutions within a particular front accord-\ning to the hypervolume loss that results from the removal of\na specific solution [25], [15], [23], [1]. In the following, we\npropose a generalized fitness assignment strategy that takes\ninto account the entire objective space weakly dominated by\na population. We will first provide a basic scheme for mating\nselection and then present an extension for environmental\nselection. Afterwards, we briefly discuss how the fitness values\ncan be computed exactly using a slightly modified hypervol-\nume calculation algorithm.\nA. Basic Scheme for Mating Selection\nTo begin with, we formally define the hypervolumeindicator\nas a basis for the following discussions. Different definitions\ncan be found in the literature, and we here use the one from\n[46] which draws upon the Lebesgue measure as proposed in\n[27] and considers a reference set of objective vectors.\nDefinition III.1. Let A \u2208 \u03a8 be a Pareto set approximation\nand R \u2282 Z be a reference set of mutually nondominating\nobjective vectors. Then the hypervolume indicator IH can be\ndefined as\nIH(A,R) := \u03bb(H(A,R))\n(2)\nwhere\nH(A,R) := {z \u2208 Z ; \u2203a \u2208 A\u2203r \u2208 R : f(a) \u2264 z \u2264 r}\nand \u03bb is the Lebesgue measure with \u03bb(H(A,R))\n?\nThe set H(A,R) denotes the set of objective vectors that are\nenclosed by the front f(A) given by A and the reference set\nR.\nThe subspace H(A,R) of Z can be further split into\npartitions H(S,A,R), each associated with a specific subset\nS \u2286 A:\nH(S,A,R) := [\nH({s},R)] \\ [\n(3)\n=\nRn1H(A,R)(z)dz and 1H(A,R)being the characteristic func-\ntion of H(A,R).\n?\ns\u2208S\n?\na\u2208A\\S\nH({a},R)]\n(4)\nThe set H(S,A,R) \u2286 Z represents the portion of the objective\nspace that is jointly weakly dominated by the solutions in S\nand not weakly dominated by any other solution in A. It holds\n\u02d9?\nwhich is illustrated in Fig. 1(a). That the partitions are disjoint\ncan be easily shown: Assume that there are two non-identical\nsubsets S1,S2of A for which H(S1,A,R)\u2229H(S2,A,R) ?= \u2205;\nsince the sets are not identical, there exists with loss of gener-\nality an element a \u2208 S1which is not contained in S2; from the\nabove definition follows that H({a},R) \u2287 H(S1,A,R) and\ntherefore H({a},R) \u2229 H(S2,A,R) ?= \u2205; the latter statement\nleads to a contradiction since H({a},R) cannot be part of\nH(S2,A,R) when a ?\u2208 S2.\nIn practice, it is infeasible to determine all distinct\nH(S,A,R) due to combinatorial explosion. Instead, we will\nconsider a more compact splitting of the dominated objective\nspace that refers to single solutions:\nS\u2286A\nH(S,A,R) = H(A,R)\n(5)\nHi(a,A,R) :=\n?\na\u2208S\n|S|=i\nS\u2286A\nH(S,A,R)\n(6)\nAccording\nthe portion of the objective space that is jointly and\nsolely weakly dominated by a and any i \u2212 1 further\nsolutions from A, see Fig. 1(b). Note that the sets\nH1(a,A,R),H2(a,A,R),...,H|A|(a,A,R) are disjoint for\na given a \u2208 A, i.e.,\u02d9?\nfixed i and different solutions a,b \u2208 A. This slightly different\nnotion has reduced the number of subspaces to be considered\nfrom 2|A|for H(S,A,R) to |A|2for Hi(a,A,R).\nNow, given an arbitrary population P\nobtains for each solution a contained in P\n(\u03bb(H1(a,P,R)),\u03bb(H2(a,P,R)),...,\u03bb(H|P|(a,P,R)))\nhypervolume contributions. These vectors can be used to\nassign fitness values to solutions; Subsection III-C describes\nhow the corresponding values \u03bb(Hi(a,A,R))\ncomputed. While most hypervolume-based search algorithms\nonly take the first components, i.e., \u03bb(H1(a,P,R)), into\ntothis definition,\nHi(a,A,R)\nstandsfor\n1\u2264i\u2264|A|Hi(a,A,R) = H({a},R), while\nthe sets Hi(a,A,R) and Hi(b,A,R) may be overlapping for\n\u2208\n\u03a8 one\na vector\nof\ncan be\n3"},{"page":4,"text":"( )\nf a\n( )\nf b\n( )\nf c\n( )\nf d\n({ , }, , }\nH b c A R\n({ , , , }, , }\nH a b c dA R\n( , }\nH A R\n({ }, , }\nH dA R\n{ }\nrR\n=\n(a) The relationship between H(A,R) and H(S,A,R)\n( )\nf a\n( )\nf b\n( )\nf c\n( )\nf d\n4( , , )\nH c A R\nH c A R\n({ , , , }, , )\nH a b c dA R\n=\n3( , , )({ , , }, , )\n({ , , }, , )\nH b c d\n+\nH a b c A R\nA R\n=\n2( , , )\nH c A R\n({ , }, , )\n({ , }, , )\nH c d\n+\nH b c A R\nA R\n=\n1( , , )\nH c A R\n({ }, , )\nH c A R\n=\nr\n(b) The relationship between H(S,A,R) and Hi(a,A,R)\nFigure 1.\nreference set R = {r}.\nIllustration of the notions of H(A,R), H(S,A,R), and Hi(a,A,R) in the objective space for a Pareto set approximation A = {a,b,c,d} and\n( )\nf a\n( )\nf b\n( )\nf c\n( )\nf d\n( , , )\nhI a P R\n( , , )\n1\n2\n1\n2\n1\n1\n1 2\n1 3\n1 3\n1 3\n1 4\n1 4\nhI c P R\n{ }\nrR\n=\nFigure 2. Illustration of the basic fitness assignment scheme where the fitness\nFa of a solution a is set to Fa= Ih(a,P,R).\naccount, we here propose the following scheme to aggregate\nthe hypervolume contributions into a single scalar value.\nDefinition III.2. Let A \u2208 \u03a8 and R \u2282 Z. Then the function\nIhwith\n|A|\n?\ngives for each solution a \u2208 A the hypervolume that can\nbe attributed to a with regard to the overall hypervolume\nIH(A,R).\nIh(a,A,R) :=\ni=1\n1\ni\u03bb(Hi(a,A,R))\n(7)\nThe motivation behind this definition is simple: the hyper-\nvolume contribution of each partition H(S,A,R) is shared\nequally among the dominating solutions s \u2208 S. That means the\nportion of Z solely weakly dominated by a specific solution\na is fully attributed to a, the portion of Z that a weakly\ndominates together with another solution b is attributed half\nto a and so forth\u2014the principle is illustrated in Fig. 2.\nThereby, the overall hypervolume is distributed among the\ndistinct solutions according to their hypervolume contributions\nas the following theorem shows (the proof can be found in\nthe appendix). Note that this scheme does not require that\nthe solutions of the considered Pareto set approximation A\nare mutually non-dominating; it applies to nondominated and\ndominated solutions alike.\n( )\nf a\n( )\nf b\n( )\nf c\n( )\nf d\n{ }\nrR\n=\n( , , )\nhI a A Rconst\n=\n\u2211\nFigure 3.\nthe population members (left). The sizes of the points correlate with the\ncorresponding selection probabilities. As one can see on the right, the overall\nselection probability for the shaded area does not change when dominated\nsolutions are added to the population.\nShows for an example population the selection probabilities for\nTheorem III.3. Let A \u2208 \u03a8 and R \u2282 Z. Then it holds\nIH(A,R) =\n?\na\u2208A\nIh(a,A,R)\n(8)\nThis aggregation method has some desirable properties that\nmake it well suited to mating selection where the fitness Fa\nof a population member a \u2208 P is Fa= Ih(a,P,R) and the\ncorresponding selection probability pa equals Fa\/IH(P,R).\nAs Fig. 3 demonstrates, the accumulated selection probability\nremains the same for any subspace H({a},R) with a \u2208 P,\nindependently of how many individuals b \u2208 P are mapped\nto H({a},R) and how the individuals are located within\nH({a},R). This can be formally stated in the next theorem;\nthe proof can again be found in the appendix.\nTheorem III.4. Let A \u2208 \u03a8 and R \u2282 Z. For every a \u2208 A and\nall multisets B1,B2\u2208 \u03a8 with {a} ? B1and {a} ? B2holds\n?\nb1\u2208{a}\u222aB1\nIh(b1,{a}\u222aB1,R) =\n?\nb2\u2208{a}\u222aB2\nIh(b2,{a}\u222aB2,R)\n(9)\nSince the selection probability per subspace is constant as\nlong as the overall hypervolume value does not change, adding\ndominated solutions to the population leads to a redistribution\nof the selection probabilities and thereby implements a natural\n4"},{"page":5,"text":"Table I\nCOMPARISON OF THREE FITNESS ASSIGNMENT SCHEMES: (1) CONSTANT\nFITNESS, (2) NONDOMINATED SORTING PLUS \u03bb(H1(a,P,R)), AND (3)\nTHE PROPOSED METHOD. EACH VALUE GIVES THE PERCENTAGE OF CASES\nWHERE THE METHOD ASSOCIATED WITH THAT ROW YIELDS A HIGHER\nHYPERVOLUME VALUE THAN THE METHOD ASSOCIATED WITH THE\nCORRESPONDING COLUMN.\nversusconstant (1)standard (2)new (3)\nconstant (1)-44%28%\nstandard (2)56%-37%\nnew (3)72%63%-\nniching mechanism. Another advantage of this fitness assign-\nment scheme is that it takes all hypervolume contributions\nHi(a,P,R) for 1 \u2264 i \u2264 |P| into account. As will be discussed\nin Section IV, this allows to more accurately estimate the\nranking of the individuals according to their fitness values\nwhen using Monte Carlo simulation.\nIn order to study the usefulness of this fitness assignment\nstrategy, we consider the following experiment. A standard\nevolutionary algorithm implementing pure nondominated sort-\ning fitness is applied to a selected test function (biobjective\nWFG1 [21] using the setting as described in Section VI) and\nrun for 100 generations. Then, mating selection is carried out\non the resulting population, i.e., the individuals are reevaluated\nusing the fitness scheme under consideration and offspring is\ngenerated employingbinary tournamentselection with replace-\nment and corresponding variation operators. The hypervolume\nof the (multi)set of offspring is taken as an indicator for the\neffectiveness of the fitness assignment scheme. By compar-\ning the resulting hypervolume values for different strategies\n(constant fitness leading to uniform selection, nondominated\nsorting plus \u03bb(H1(a,P,R)), and the proposed fitness accord-\ning to Def. III.2) and for 100 repetitions of this experiment, we\ncan investigate the influence of the fitness assignment strategy\non the mating selection process.\nThe Quade test, a modification of Friedman\u2019s test which has\nmore power when comparing few treatments [10], reveals that\nthere are significant differences in the quality of the generated\noffspring populations at a signficance level of 0.01 (test statis-\ntics: T3 = 12.2). Performing post-hoc pairwise comparisons\nfollowing [10] using the same significance level as in the\nQuade test provides evidence that the proposed fitness strategy\ncan be advantageous over the other two strategies, cf. Table I;\nin the considered setting, the hypervolume values achieved\nare significantly better. Comparing the standard hypervolume-\nbased fitness with constant fitness, the former outperforms the\nlatter significantly. Nevertheless, also the required computation\nresources need to be taken into account. That means in practice\nthat the advantage over uniform selection may diminish when\nfitness computation becomes expensive. This aspect will be\ninvestigated in Section VI.\nNext, we will extend and generalize the fitness assignment\nscheme with regard to the environmental selection phase.\nB. Extended Scheme for Environmental Selection\nIn the context of hypervolume-based multiobjective search,\nenvironmental selection can be formulated in terms of the\nhypervolume subset selection problem (HSSP).\nDefinition III.5. Let A \u2208 \u03a8, R \u2282 Z, and k \u2208 {0,1,...,|A|}.\nThe hypervolume subset selection problem (HSSP) is defined\nas the problem of finding a subset A?\u2286 A with |A?| = |A|\u2212k\nsuch that the overall hypervolume loss is minimum, i.e.,\nIH(A?,R) =max\nA??\u2286A\n|A??|=|A|\u2212k\nIH(A??,R)\n(10)\nHere, we assume that parents and offspring have been\nmerged into a single population P which then needs to be\ntruncated by removing k solutions. Since dominated solutions\nin the population do not affect the overall hypervolume, they\ncan be deleted first; therefore, we assume in the following\nthat all solutions in P are incomparable3or indifferent4to\neach other.\nIf k = 1, then HSSP can be solved exactly by removing\nthat solution a from the population P with the lowest value\n\u03bb(H1(a,P,R)); this is the principle implemented in most\nhypervolume-based MOEAs which consider one offspring per\ngeneration, e.g., [25], [15], [23]. However, it has been recently\nshown that exchanging only one solution in the population\nlike in steady state MOEAs (k = 1) may lead to premature\nconvergence to a local optimum in the hypervolume landscape\n[46]. This problem can be avoided when generating at least as\nmany offspring as parents are available, i.e., k \u2265 |P|\/2.\nFor arbitrary values of k, dynamic programming can be\nused to solve HSSP in a biobjective setting; in the presence\nof three or more objectives, it is an open problem whether\nHSSP becomes NP-hard. In practice, a greedy heuristic is\nemployed to obtain an approximation [41], [9]: all solutions\nare evaluated with respect to their usefulness and the l least\nimportant solutions are removed where l is a prespecified\nparameter. Most popular are the following two approaches:\n1) Iterative (l = 1): The greedy heuristics is applied k\ntimes in a row; each time, the worst solution is removed\nand afterwards the remaining solutions are re-evaluated.\n2) One shot (l = k): The greedy heuristics is only applied\nonce; the solutions are evaluated and the k worst solu-\ntions are removed in one step.\nBest results are usually obtained using the iterative approach,\nas the re-evaluation increases the quality of the generated\napproximation.In contrast, the one-shot approach substantially\nreduces the computation effort, but the quality of the resulting\nsubset is lower. In the context of density-based MOEAs, the\nfirst approach is for instance used in SPEA2, while the second\nis employed in NSGA-II.\nThe key issue with respect to the above greedy strategy\nis how to evaluate the usefulness of a solution. The scheme\n3Two solutions a,b \u2208 X are called incomparable if and only if neither\nweakly dominates the other one, i.e., a ?? b and b ?? a\n4Two solutions a,b \u2208 X are called indifferent if and only if both weakly\ndominate other one, i.e., a ? b and b ? a\n5"},{"page":6,"text":"r\n( )\nf a\n( )\nf b\n( )\nf c\n( )\nf d\n1\np=\n1\/3\np=\n0\np=\n0\np=\n({ , , , }, )\nH a b c dR\n({ , , }, }\nH a b c R\n({ , , }, )\nH b c dR\nFigure 4.\n{a,b,c,d},R = {r} and shows (i) which portion of the objective space\nremains dominated if any two solutions are removed from A (shaded area),\nand (ii) the probabilities p that a particular area that can be attributed to a \u2208 A\nis lost if a is removed from A together with any other solution in A.\nThe figure is based on the previous example with A =\npresented in Def. III.2 has the drawback that portions of\nthe objective space are taken into account that for sure will\nnot change. Consider, for instance, a population with four\nsolutions as shown in Fig. 4; when two solutions need to\nbe removed (k = 2), then the subspaces H({a,b,c},P,R),\nH({b,c,d},P,R), and H({a,b,c,d},P,R) remain weakly\ndominated independently of which solutions are deleted. This\nobservation led to the idea of considering the expected loss\nin hypervolume that can be attributed to a particular solution\nwhen exactly k solutions are removed. In detail, we consider\nfor each a \u2208 P the average hypervolume loss over all subsets\nS \u2286 P that contain a and k \u2212 1 further solutions; this value\ncan be easily computed by slightly extending the scheme from\nDef. III.2 as follows.\nDefinition III.6. Let A \u2208 \u03a8, R \u2282 Z, and k \u2208 {0,1,...,|A|}.\nThen the function Ik\n\u23a1\n\u23a3\nwhere S = {S \u2286 A; a \u2208 S \u2227 |S| = k} contains all subsets of\nA that include a and have cardinality k gives for each solution\na \u2208 A the expected hypervolume loss that can be attributed\nto a when a and k \u2212 1 uniformly randomly chosen solutions\nfrom A are removed from A.\nhwith\nIk\nh(a,A,R) :=\n1\n|S|\n?\nS\u2208S\n\u23a2\n?\na\u2208T\nT\u2286S\n1\n|T|\u03bb?H(T,A,R)?\n\u23a4\n\u23a6\n\u23a5\n(11)\nNotice\nI|A|\ncan be regarded as a generalization of the scheme presented\nin Def. III.2 and the commonly used fitness assignment\nstrategy for hypervolume-based search [25], [15], [23], [1].\nThe next theorem shows how to calculate Ik\naveraging over all subsets S \u2208 S; the proof can be found in\nthe appendix.\nthat\nI1\nh(a,A,R)\nIh(a,A,R), i.e., this modified scheme\n=\n\u03bb(H1(a,A,R))\nand\nh(a,A,R)=\nh(a,A,R) without\nTheorem III.7. Let A \u2208 \u03a8, R \u2282 Z, and k \u2208 {0,1,...,|A|}.\nThen it holds\nIk\nh(a,A,R) =\nk\n?\ni=1\n\u03b1i\ni\u03bb(Hi(a,A,R))\n(12)\nTable II\nCOMPARISON OF GREEDY STRATEGIES FOR THE HSSP (ITERATIVE VS.\nONE SHOT) USING THE NEW (Ik\nFITNESS (I1\nCONSIDERED AS WELL. THE FIRST COLUMN GIVES THE PORTION OF\nCASES AN OPTIMAL SUBSET WAS GENERATED; THE SECOND COLUMN\nPROVIDES THE AVERAGE DIFFERENCE IN HYPERVOLUME BETWEEN\nOPTIMAL AND GENERATED SUBSET. THE LAST TWO COLUMNS REFLECT\nTHE DIRECT COMPARISONS BETWEEN THE TWO FITNESS SCHEMES FOR\nEACH GREEDY APPROACH (ITERATIVE, ONE SHOT) SEPARATELY; THEY\nGIVE THE PERCENTAGES OF CASES WHERE THE CORRESPONDING\nMETHOD WAS BETTER THAN OR EQUAL TO THE OTHER ONE.\nh) AND THE STANDARD HYPERVOLUME\nh); AS A REFERENCE, PURELY RANDOM DELETIONS ARE\ngreedy strategyoptimum founddistancebetterequal\niterative with Ik\niterative with I1\nh\n59.8 %\n44.5 %\n1.09 10\u22123\n2.59 10\u22123\n30.3 %\n3.17 %\n66.5 %\n66.5 %\nh\none shot with Ik\none shot with I1\nh\n16.9 %\n3.4 %\n39.3 10\u22123\n69.6 10\u22123\n65.2 %\n11.1 %\n23.7 %\n23.7 %\nh\nuniformly random0.381 %257 10\u22123\nwhere\n\u03b1i:=\ni\u22121\n?\nj=1\nk \u2212 j\n|A| \u2212 j\n(13)\nNext, we will study the effectiveness of Ik\napproximating the optimal HSSP solution. To this end, we\nassume that for the iterative greedy strategy (l = 1) in the first\nround the values Ik\nround the values Ik\u22121\nh\n(a,A,R), and so forth; each time an\nindividual assigned the lowest value is selected for removal.\nFor the one-step greedy method (l = k), only the Ik\nvalues are considered.\nTable II provides a comparison of the different techniques\nfor 100,000 randomly chosen Pareto set approximations A \u2208\n\u03a8 containing ten incomparable solutions, where the ten points\nare randomly distributed on a three dimensional unit simplex,\ni.e., we consider a three objective scenario. The parameter k\nwas set to 5, so that half of the solutions needed to be removed.\nThe relatively small numbers were chosen to allow to compute\nthe optimal subsets by enumeration. Thereby, the maximum\nhypervolume values achievable could be determined.\nThe comparison reveals that the new fitness assignment\nscheme is in the considered scenario more effective in approx-\nimating HSSP than the standard scheme. The mean relative\ndistance (see Table II) to the optimal solution is about 60%\nsmaller than the distance achieved using I1\nand about 44% smaller in the one shot case. Furthermore, the\noptimum was found much more often in comparison to the\nstandard fitness: 34% more often for the iterative approach\nand 497% in the one shot scenario.\nFinally, note that the proposed evaluation function Ik\nbe combined with nondominated sorting for environmental\nselection, cf. Section V, similarly to [15], [23], [9], [40],\n[1]. One reason is computation time: with nondominated\nsorting the worst dominated solutions can be removed quickly\nwithout invoking the hypervolume calculation algorithm; this\nadvantage mainly applies to low-dimensional problems and to\nthe early stage of the search process. Another reason is that\nthe full benefits of the scheme proposed in Def. III.6 can be\nh(a,A,R) for\nh(a,A,R) are considered, in the second\nh(a,A,R)\nhin the iterative case\nhwill\n6"},{"page":7,"text":"exploited when the Pareto set approximation A under consid-\neration only contains incomparable and indifferent solutions;\notherwise, it cannot be guaranteed that nondominatedsolutions\nare preferred over dominated ones.\nC. Exact Calculation of Ik\nh\nIn this subsection, we tackle the question of how to calculate\nthe fitness values for a given population P \u2208 \u03a8. We present\nan algorithm that determines the values Ik\nelements a \u2208 P and a fixed k\u2014in the case of mating\nselection k equals |P|, in the case of environmental selection\nk gives the number of solutions to be removed from P. It\noperates according to the \u2019hypervolume by slicing objectives\u2019\nprinciple [39], [26], [36], but differs from existing methods\nin that it allows (i) to consider a set R of reference points\nand (ii) to compute all fitness values, e.g., the I1\nvalues for k = 1, in parallel for any number of objectives\ninstead of subsequently as in [4]. Although it looks at all\npartitions H(S,P,R) with S \u2286 P explicitly, the worst-case\nruntime complexity is not affected by this; it is of order\nO(|P|n+n|P|log|P|) assuming that sorting of the solutions\nin all dimensions is carried out as a preprocessing step. Clearly,\nthis is only feasible for a low number of objectives, and the\nnext section discusses how the fitness values can be estimated\nusing Monte Carlo methods.\nDetails of the procedure are given by Algorithms 1 and 2.\nAlgorithm 1 just provides the top level call to the recursive\nfunction doSlicing and returns a fitness assignment F, a\nmultiset containing for each a \u2208 P a corresponding pair\n(a,v) where v is the fitness value. Note that n at Line 3\ndenotes the number of objectives. Algorithm 2 recursively\ncuts the dominated space into hyperrectangles and returns\na (partial) fitness assignment F?. At each recursion level, a\nscan is performed along a specific objective\u2014given by i\u2014\nwith u\u2217representing the current scan position. The vector\n(z1,...,zn) contains for all dimensions the scan positions,\nand at each invocation of doSlicing solutions (more precisely:\ntheir objective vectors) and reference points are filtered out\naccording to these scan positions (Lines 3 and 4) where also\ndominated solutions may be selected in contrast to [39], [26],\n[36]. Furthermore, the partial volume V is updated before\nrecursively invoking Algorithm 2 based on the distance to the\nnext scan position. At the lowest recursion level (i = 0), the\nvariable V gives the hypervolume of the partition H(A,P,R),\ni.e., V = \u03bb(H(A,P,R)) where A stands for the remaining so-\nlutions fulfilling the bounds given by the vector (z1,...,zn)\u2014\nUP contains the objective vectors corresponding to A, cf.\nLine 3. Since the fitness according to Def. III.6 is additive with\nrespect to the partitions, for each a \u2208 A the partial fitness value\nv can be updated by adding\nis a multiset, i.e., it may contain indifferent solutions or even\nduplicates; therefore, all the other sets in the algorithms are\nmultisets.\nThe following example illustrates the working principle of\nthe hypervolume computation.\nh(a,P,R) for all\nh(a,P,R)\n\u03b1|UP|\n|UP|V . Note that the population\nExample III.8.\nConsider thethree-objectivescenario\nAlgorithm 1 Hypervolume-based Fitness Value Computation\nRequire: population P \u2208 \u03a8, reference set R \u2286 Z, fitness\nparameter k \u2208 N\n1: procedure computeHypervolume(P, R, k)\n2:\n3:\nreturn doSlicing(F,R,k,n,1,(\u221e,\u221e,...,\u221e));\n4: end procedure\nF \u2190?\na\u2208P{(a,0)}\nAlgorithm 2 Recursive Objective Space Partitioning\nRequire: current fitness assignment F, reference set R \u2286 Z,\nfitness parameter k \u2208 N, recursion level i, partial volume\nV \u2208 R, scan positions (z1,...,zn) \u2208 Rn\n1: procedure doSlicing(F, R, k, i, V , (z1,...,zn))\n2:\n\/\u2217 filter out relevant solutions and reference points \u2217\/\n3:\n4:\n5:\nif i = 0 \u2227 UR ?= \u2205 then\n6:\n\/\u2217 end of recursion reached \u2217\/\n7:\nj=1\n(k \u2212 j)\/(|F| \u2212 j)\n8:\n\/\u2217 update hypervolumes of filtered solutions \u2217\/\n9:\nF?\u2190 \u2205\n10:\nfor all (a,v) \u2208 F do\n11:\nif \u22001 \u2264 j \u2264 n : fj(a) \u2264 zj then\n12:\nF?\u2190 F?\u222a {(a,v +\n13:\nelse\n14:\nF?\u2190 F?\u222a {(a,v)}\n15:\nend if\n16:\nend for\n17:\nelse if i > 0 then\n18:\n\/\u2217 recursion continues \u2217\/\n19:\nF?\u2190 F\n20:\nU \u2190 UP \u222a UR\n21:\n\/\u2217 scan current dimension in ascending order \u2217\/\n22:\nwhile U ?= \u2205 do\n23:\nu\u2217\u2190 min(u1,...,un)\u2208Uui\n24:\nU?\u2190 {(u1,...,un) \u2208 U |ui> u\u2217}\n25:\nif U??= \u2205 then\n26:\n27:\nF?\u2190 doSlicing(F?, R, k, i \u2212 1, V?,\n28:\n(z1,...,zi\u22121,u\u2217,zi+1,...,zn) )\n29:\nend if\n30:\nU = U?\n31:\nend while\n32:\nend if\n33:\nreturn F?\n34: end procedure\nUP \u2190?\n(a,v)\u2208F, \u2200i<j\u2264n: fj(a)\u2264zj{f(a)}\n(r1,...,rn)\u2208R, \u2200i<j\u2264n: rj\u2265zj{(r1,...,rn)}\nUR \u2190?\n\u03b1 \u2190?|UP|\u22121\n\u03b1\n|UP|V )}\nV?= V \u00b7?(min(u?\n1,...,u?\nn)\u2208U? u?\ni) \u2212 u\u2217?\ndepicted in Fig. 5 where the population contains four\nsolutions a,b,c,d the objective vectors of which are\nf(a)=(\u221210,\u22123,\u22122),f(b)\n(\u22126,\u22128,\u221210),f(d) = (\u22124,\u22125,\u221211) and the reference\nset includes two points r = (\u22122,0,0),s = (0,\u22123,\u22124).\nFurthermore, let the parameter k be 2.\nIn the first call of doSlicing, it holds i = 3 and U contains\nall objective vectors associated with the population and all\nreference points. The following representation shows U with\n=(\u22128,\u22121,\u22128),f(c)=\n7"},{"page":8,"text":"r\ns\nr\ns\n0\n?\ns\nr\n3f\n1f\n2f\ns\ns\nr\n( )\nf a\n( )\nf a\n( )\nf b\n( )\nf b\n( )\nf b\n( )\nf b\n( )\nf c\n( )\nf c\n( )\nf c\n( )\nf c\n( )\nf d\n( )\nf d\n( )\nf d\n( )\nf d\nFigure 5.\none looks from (\u221e,\u2212\u221e,\u221e) to the origin. First, the dominated polytope is cut along the third dimension leading to five slices, which are again cut along\nthe second dimension and finally along the first dimension. In contrast to existing \u2019Hypervolume by Slicing Objectives\u2019 algorithms, also dominated points are\ncarried along.\nIllustration of the principle underlying Algorithm 2 where one looks from (\u2212\u221e,\u2212\u221e,\u2212\u221e) on the front except for the lower left picture where\nits elements sorted in ascending order according to their third\nvector components:\nU\n=\nf(d) :\nf(c) :\nf(b) :\n(\u22124,\u22125,\u221211) \u2193\n(\u22126,\u22128,\u221210)\n(\u22128,\u22121,\u22128)\n(\u22120,\u22123,\u22124)\n(\u221210,\u22123,\u22122)\n(\u22122,0,0)\ns :\nf(a) :\nr :\n(14)\nHence, in the first two iterations of the loop beginning at\nLine 22 the variable u\u2217is assigned to f3(d) = \u221211 resp.\nu\u2217= f3(c) = \u221210. Within the third iteration, U is reduced to\n{f(a),f(b),r,s} which yields u\u2217= f3(b) = \u22128 and in turn\nV?= 1 \u00b7 (\u22124 \u2212 (\u22128)) = 4 with the current vector of scan\npositions being (z1,z2,z3) = (\u221e,\u221e,\u22128); these values are\npassed to the next recursion level i = 2 where U is initialized\nat Line 20 as follows (this time sorted according to the second\ndimension):\nU\n=\nf(c) :\nf(d) :\n(\u22126,\u22128,\u221210) \u2193\n(\u22124,\u22125,\u221211)\n(0,\u22123,\u22124)\n(\u22128,\u22121,\u22128)\n(\u22122,0,0)\ns :\nf(b) :\nr :\n(15)\nNow, after three iterations of the loop at Line 22 with\nu\u2217= f2(c) = \u22128, u\u2217= f2(d) = \u22125, and u\u2217= s2= \u22123, re-\nspectively, U is reduced in the fourth iteration to {f(b),r} and\nu\u2217is set to f2(b) = \u22121. As a result, V?= 1\u00b74\u00b7(0\u2212(\u22121)) = 4\nand (z1,z2,z3) = (\u221e,\u22121,\u22128) which are the parameters for\nthe next recursive invocation of doSlicing where U is set to:\nU\n=\nf(b) :\nf(c) :\nf(d) :\n(\u22128,\u22121,\u22128) \u2193\n(\u22126,\u22128,\u221210)\n(\u22124,\u22125,\u221211)\n(\u22122,0,0)\nr :\n(16)\nAt this recursion level with i = 1, in the second iteration it\nholds u\u2217= f1(c) = \u22126 and V?= 1\u00b7 4\u00b7 1\u00b7 (\u22124\u2212 (\u22126)) = 8.\nWhen calling doSlicing at this stage, the last recursion level\nis reached (i = 0): First, \u03b1 is computed based on the\npopulation size N = 4, the number of individuals dominating\nthe hyperrectangle (|UP| = 2), and the fitness parameter\nk = 2, which yields \u03b1 = 1\/3; then for b and c, the fitness\nvalues are increased by adding \u03b1 \u00b7 V\/|UP| = 4\/3.\nApplying this procedure to all slices at a particular recur-\nsion level identifies all hyperrectangles which constitute the\nportion of the objective space enclosed by the population and\nthe reference set.\nIV. ESTIMATING HYPERVOLUME CONTRIBUTIONS USING\nCARLO SIMULATION\nAs outlined above, the computation of the proposed\nhypervolume-based fitness scheme is that expensive that only\nproblems with at maximum four or five objectives are tractable\nwithin reasonable time limits. However, in the context of\nrandomized search heuristics one may argue that the exact\nfitness values are not crucial and approximated values may\nbe sufficient; furthermore, if using pure rank-based selection\nschemes, then only the resulting order of the individuals\nmatters. These considerations lead to the idea of estimating\nthe hypervolume contributions by means of Monte Carlo\nsimulation.\nThe basic principle is described and investigated in the\nfollowing subsection, while more advanced sampling strategies\n8"},{"page":9,"text":"that automatically adjust the number of samples to be drawn\nare discussed in the second part of this section.\nA. Basic Concept\nTo approximate the fitness values according to Defini-\ntion III.6, we need to estimate the Lebesgue measures of the\ndomains Hi(a,P,R) where P \u2208 \u03a8 is the population. Since\nthese domains are all integrable, their Lebesgue measure can\nbe approximated by means of Monte Carlo simulation.\nFor this purpose, a sampling space S \u2286 Z has to be\ndefined with the following properties: (i) the hypervolume\nof S can easily be computed, (ii) samples from the space\nS can be generated fast, and (iii) S is a superset of the\ndomains Hi(a,P,R) the hypervolumes of which one would\nlike to approximate. The latter condition is met by setting\nS = H(P,R), but since it is hard both to calculate the\nLebesgue measure of this sampling space and to draw samples\nfrom it, we propose using the axis-aligned minimum bounding\nbox containing the Hi(a,P,R) subspaces instead, i.e.:\nS := {(z1,...,zn) \u2208 Z |\u22001 \u2264 i \u2264 n : li\u2264 zi\u2264 ui}\nwhere\nli\n:=mina\u2208Pfi(a)\nui\n:=max(r1,...,rn)\u2208Rri\nfor 1 \u2264 i \u2264 n. Hence, the volume V of the sampling space S\nis given by V =?n\nobjective vectors s1,...,sM from S uniformly at random.\nFor each sj it is checked whether it lies in any partition\nHi(a,P,R) for 1 \u2264 i \u2264 k and a \u2208 P. This can be determined\nin two steps: first, it is verified that sjis \u2019below\u2019 the reference\nset R, i.e., it exists r \u2208 R that is dominated by sj; second,\nit is verified that the multiset A of those population members\ndominating sj is not empty. If both conditions are fulfilled,\nthen we know that\u2014given A\u2014the sampling point sj lies in\nall partitions Hi(a,P,R) where i = |A| and a \u2208 A. This\nsituation will be denoted as a hit regarding the ith partition of\na. If any of the above two conditions is not fulfilled, then we\ncall sj a miss. Let X(i,a)\nj\ndenote the corresponding random\nvariable that is equal to 1 in case of a hit of sj regarding the\nith partition of a and 0 otherwise.\nBased on the M sampling points, we obtain an estimate\nfor \u03bb(Hi(a,P,R)) by simply counting the number of hits and\nmultiplying the hit ratio with the volume of the sampling box:\n(17)\n(18)\ni=1max{0,ui\u2212 li}.\nNow given S, sampling is carried out by selecting M\n\u02c6\u03bb?Hi(a,P,R)?=\n?M\nj=1X(i,a))\nM\nj\n\u00b7 V\n(19)\nThis value approaches the exact value \u03bb(Hi(a,P,R)) with\nincreasing M by the law of large numbers. Due to the linearity\nof the expectation operator, the fitness scheme according to\nEq. (11) can be approximated by replacing the Lebesgue\nmeasure with the respective estimates given by Eq. (19):\n\u02c6Ik\nh(a,P,R) =\nk\n?\ni=1\n\u03b1i\ni\n\u00b7\n??M\nj=1X(i,a))\nM\nj\nV\n?\n(20)\nAlgorithm 3 Hypervolume-based Fitness Value Estimation\nRequire: population P \u2208 \u03a8, reference set R \u2286 Z, fitness\nparameter k \u2208 N, number of sampling points M \u2208 N\n1: procedure estimateHypervolume(P, R, k, M)\n2:\n\/\u2217 determine sampling box S \u2217\/\n3:\nfor i \u2190 1,n do\n4:\nli= mina\u2208Pfi(a)\n5:\nui= max(r1,...,rn)\u2208Rri\n6:\nend for\n7:\nS \u2190 [l1,u1] \u00d7 \u00b7\u00b7\u00b7 \u00d7 [ln,un]\n8:\n9:\n\/\u2217 reset fitness assignment \u2217\/\n10:\n11:\n\/\u2217 perform sampling \u2217\/\n12:\nfor j \u2190 1,M do\n13:\nchoose s \u2208 S uniformly at random\n14:\nif \u2203r \u2208 R : s \u2264 r then\n15:\n16:\nif |UP| \u2264 k then\n17:\n\/\u2217 hit in a relevant partition \u2217\/\n18:\nl=1\n19:\n\/\u2217 update hypervolume estimates \u2217\/\n20:\nF?\u2190 \u2205\n21:\nfor all (a,v) \u2208 F do\n22:\nif f(a) \u2264 s then\n23:\nF?\u2190 F?\u222a {(a,v +\n24:\nelse\n25:\nF?\u2190 F?\u222a {(a,v)}\n26:\nend if\n27:\nend for\n28:\nF \u2190 F?\n29:\nend if\n30:\nend if\n31:\nend for\n32:\nreturn F\n33: end procedure\nV \u2190?n\nF \u2190?\ni=1max{0,(ui\u2212 li)}\na\u2208P{(a,0)}\nUP \u2190?\n\u03b1 \u2190?|UP|\u22121\na\u2208P, f(a)\u2264s{f(a)}\nk\u2212l\n|P|\u2212l\n\u03b1\n|UP|\u00b7V\nM)}\nThe details of estimation procedure are described by Algo-\nrithm 3 which returns a fitness assignment, i.e., for each\na \u2208 P the corresponding hypervolume estimate\u02c6Ik\nwill be later used by the evolutionary algorithm presented in\nSection V. Note that the partitions Hi(a,P,R) with i > k do\nnot need to be considered for the fitness calculation as they do\nnot contribute to the Ik\ncf. Def. III.6.\nIn order to study how closely the sample size M and the\naccuracy of the estimates is related, a simple experiment was\ncarried out: ten imaginary individuals a \u2208 A were generated,\nthe objective vectors f(a) of which are uniformly distributed\nat random on a three dimensional unit simplex, similarly to\nthe experiments presented in Table II. These individuals were\nthen ranked on the one hand according to the estimates\u02c6I|A|\nand on the other hand with respect to the exact values I|A|\nThe closer the former ranking is to the latter ranking, the\nhigher is the accuracy of the estimation procedure given by\nAlgorithm 3. To quantify the differences between the two\nh(a,P,R). It\nhvalues that we would like to estimate,\nh\nh.\n9"},{"page":10,"text":"Table III\nACCURACY OF THE RANKING OF 10 INDIVIDUALS ACCORDING TO\u02c6I10\n(20) IN COMPARISON TO I10\nh\nPERCENTAGES REPRESENT THE NUMBER OF PAIRS OF INDIVIDUALS\nRANKED CORRECTLY.\nh\nFOR DIFFERENT SAMPLE SIZES. THE\nnumber of samples M\nranking accuracy\n101\n102\n103\n104\n105\n106\n107\n56.0%\n74.1%\n89.9%\n96.9%\n99.2%\n99.8%\n100.0 %\nrankings, we calculated the percentage of all pairs (i,j) with\n1 \u2264 i < j \u2264 |A| where the individuals at the ith position\nand the jth position in the ranking according to I|A|\nsame order in the ranking according to\u02c6I|A|\nexperiment was repeated for different numbers of sampling\npoints as shown in Table III. The experimental results indicate\nthat 10.000 samples are necessary to achieve an error below\n5% and that 10.000.000 sampling point are sufficient in this\nsetting to obtain the exact ranking.\nh\nhave the\nh, see [31]. The\nB. Adaptive Sampling\nSeeing the close relationship between sample size and ac-\ncuracy, one may ask whether M can be adjusted automatically\non the basis of confidence intervals. That means sampling is\nstopped as soon as the statistical confidence in the estimated\nfitness values reaches a prespecified level. The hope is that\nthereby the estimation is less sensitive to the choice of M and\nthat the number of drawn samples can be reduced.\nUsing the normal approximation of the distribution of an\nestimate\u02c6Ik\nthat the true value Ik\nh(a,A,R), we can state there is a probability of L,\nh(a,A,R) is in\nIk\nh(a,A,R) \u2208\u02c6Ik\nh(a,A,R) \u00b1 z1\/2+L\/2\n?\n?\nVar?\u02c6Ik\nh(a,A,R)?\n(21)\nwhere z\u03b2denotes the \u03b2-percentile of a standard normal distri-\nbution and?\nBased on this confidence interval, we can derive a lower\nbound for the probability Cw, that the individual with the\nsmallest estimated contribution\u02c6Ik\nthe hypervolume (see Appendix B1a). Let A = {a1,...,a|A|}\nwith a1\u2264 ai\u2200i \u2208 {1,...,|A|}, then\nVar?\u02c6Ik\nh(a,A,R)?\ndenotes the estimated variance\nof the estimate according to (30), see Appendix B for details.\nh(a,A,R) contributes least to\nCw\u2265 1 \u2212 (1 \u2212 L)\/2 \u2212\n|A|\n?\ni=2\nP?Ik\nh(ai,A,R) > B?\n(22)\nwhere B denotes the upper end of confidence interval (21)\nfor confidence level (L + 1)\/2. Similiarly, a lower bound for\nthe confidence Cr, that the ranking of individuals is correct\nfollows as\nCr\u2265 1 \u2212\n|A|\u22121\n?\n|A|\n?\ni=1\nP(Ik\nh(ai,A,R) < Bi)\n\u2212\ni=2\nP(Ik\nh(ai,A,R) > Bi\u22121)\n(23)\n(see Appendix B1c for details, including the meaning of Bi).\nThe lower bound (22) can be used to limit the number of\nsamples used to estimate Ik\nselection step, and (23) to limit the number of samples in\nthe mating selection. Algorithm 4 gives an adaptive sam-\npling procedure\u2014based on the confidence levels\u2014that re-\nsembles Algorithm 3. In contrast to the latter, the elements\n(a,v,(h1,...,h|P|)) of F take a third component that records\nthe number of hits hi per partition Hi(a,A,R), which are\nneeded to calculate the confidence level.\nIn the following we investigate to what extend the adaptive\nadjustment of sampling size reduces the total number of\nsamples needed. For this purpose, assume from a Pareto-front\napproximation A, some individuals are removed one by one.\nIn each removal step, the confidence level Cw is calculated\nafter a certain number of samples \u0398 are drawn, here set to\nt = 100. If Cw exceeds a user defined level L, then the\nadaptive sampling stops. Otherwise it continues, periodically\nrecalculating the confidence level after \u0398 samples until either\nthe confidence exceeds L or the maximum number of samples\nMmax is reached.\nWe consider the following scenario: 20 individuals a \u2208 A\nare generated whose objective values are set uniformly at\nrandom on a 3 dimensional unit simplex. From these 20\nindividuals, 10 are removed one by one based on the smallest\nindicator value Ik\n|A| \u2212 10. The resultant set of 10 individuals Aref acts as\nreference to assess the quality of the two sampling strategies.\nThe same procedure is then carried out for the estimated\nindicator\u02c6Ik\nas well as using an adaptive number of samples according\nto Algorithm 4. In the latter case, the maximum number of\nsamples Mmax is set to M and the desired confidence level\nL is set to 0.99. The quality of the resulting set Aadaptiveis\nassessed by counting the percentage of individuals which are\nin Aadaptivebut not in Aref, i.e., |Aref\u2229Aadaptive|\/|Aref|. In\nthe same way the percentage is calculated for the set Aconstant\nthat results from applying the constant sampling strategy.\nFigure 6 shows the percentages obtained for both the\nconstant and adaptive sampling approach. For small maximum\nnumber of samples Mmax, the adaptive sampling algorithm\nhardly ever reaches the desired confidence level L. Hence,\nboth the number of samples and ranking accuracy is similar\nto the constant approach. However, as the maximum sample\nsize increases, the more often L is reached. In this case, the\nnumber of samples is only half the number needed by the\nalgorithm based on the constant sampling strategy while the\naccuracy of the estimate is only marginaly affected. Since the\nconfidence level is set to a relatively high value, the accuracy\nh(a,A,R) in the environmental\nh(a,A,R) with R = (2,2,2) and k =\nh(a,A,R) using a constant number of samples M\n10"},{"page":11,"text":"is only affected very slightly. This indicates that using adaptive\nsampling might be beneficial to speed up sampling when the\nnumber of samples is large.\n102103104 105 106107\n0.93\n0.95\n0.97\n0.99\ntotal number of samples\ncorrect decisions\nadaptive sampling\nconstant sampling\nrelated sample sizes\nFigure 6.\nfor the constant and adaptive sampling strategy. As the grey arrows indicate,\nthe latter uses less samples than the constant approach if the sample size is\nlarge enough. On the other hand, the two approaches differ only slightly when\nfew samples are used.\nRanking accuracy with increasing (maximum) number of samples\nV. HYPE: HYPERVOLUME ESTIMATION ALGORITHM FOR\nMULTIOBJECTIVE OPTIMIZATION\nIn this section, we describe an evolutionary algorithm\nnamed HypE (Hypervolume Estimation Algorithm for Mul-\ntiobjective Optimization) which is based on the fitness assign-\nment schemes presented in the previous sections. When the\nnumber of objectives is small (\u2264 3), the hypervolume values\nIk\nare estimated based on Algorithm 3.\nThe main loop of HypE is given by Algorithm 5. It\nreflects a standard evolutionary algorithm and consists of\nthe successive application of mating selection (Algorithm 6),\nvariation, and environmental selection (Algorithm 7). As to\nmating selection, binary tournament selection is proposed here,\nalthough any other selection scheme could be used as well. The\nprocedure variation encapsulates the application of mutation\nand recombination operators to generate N offspring. Finally,\nenvironmental selection aims at selecting the most promising\nN solutions from the multiset-union of parent population\nand offspring; more precisely, it creates a new population by\ncarrying out the following two steps:\nhare computed exactly using Algorithm 1, otherwise they\n1) First, the union of parents and offspring is divided into\ndisjoint partitions using the principle of nondominated\nsorting [20], [13], also known as dominance depth. Start-\ning with the lowest dominance depth level, the partitions\nare moved one by one to the new population as long\nas the first partition is reached that cannot be transfered\ncompletely. This corresponds to the scheme used in most\nhypervolume-based multiobjective optimizers [15], [23],\n[9].\nAlgorithm 4 Hypervolume-based Fitness Value Estimation\nWith Adaptive Sampling\nRequire: population P \u2208 \u03a8, reference set R \u2286 Z, fitness\nparameter k \u2208 N, maximum number of sampling points\nMmax\u2208 N, desired confidence L, sampling interval \u0398\n1: procedure estimateHypervolumeAS(P, R, k, Mmax, L)\n2:\n\/\u2217 determine sampling box S \u2217\/\n3:\nfor i \u2190 1,n do\n4:\nli= mina\u2208Pfi(a)\n5:\nui= max(r1,...,rn)\u2208R ri\n6:\nend for\n7:\nS \u2190 [l1,u1] \u00d7 \u00b7\u00b7\u00b7 \u00d7 [ln,un]\n8:\n9:\n\/\u2217 reset fitness assignement\n10:\nthird component: number of hits per Hi(a,P,R) \u2217\/\n11:\n12:\n\/\u2217 compute \u03b1-vector according to Equation (13) \u2217\/\n13:\n\u03b1 \u2190 (0,...,0)\n14:\nfor i \u2190 1,k do\n15:\nl=1\n|A|\u2212l\n16:\nend for\n17:\n\/\u2217 perform sampling \u2217\/\n18:\nC \u2190 0 \/\u2217 either Cw(env. sel.) or Cr(mat. sel) \u2217\/\n19:\nj \u2190 1\n20:\nwhile j \u2264 Mmaxand C < L do\n21:\nchoose s \u2208 S uniformly at random\n22:\nif \u2203r \u2208 R : s \u2264 r then\n23:\nF?\u2190 \u2205\n24:\n\/\u2217 update estimates and hit counts \u2217\/\n25:\nfor all (a,v,(h1,...,h|P|)) \u2208 F do\n26:\nif \u22001 \u2264 j \u2264 n : fj(a) \u2264 sj then\n27:\n(h?\n28:\nh?\n29:\nF?\u2190 F?\u222a {(a,v +\u03b1A\n30:\n31:\nelse\n32:\nF?\u2190 F?\u222a {(a,v,(h1,...,h|P|))}\n33:\nend if\n34:\nend for\n35:\nF \u2190 F?\n36:\nend if\n37:\n\/\u2217 Recalculate confidence level reached \u2217\/\n38:\nif\nmod (j,\u0398) = 0 then\n39:\nC \u2190 confidence according to (22) or (23)\n40:\nend if\n41:\nend while\n42:\nreturn F\n43: end procedure\nV \u2190?n\nF \u2190?\ni=1max{0,(ui\u2212 li)}\na\u2208P{(a,0,(0,...,0))}\n\u03b1i\u2190?k\u22121\nk\u2212l\n1,...,h?\n|A|\u2190 h?\n|P|) \u2190 (h1,...,h|P|)\n|A|+ 1\n|A|\u00b7V\n(h?\nM,\n1,...,h?\n|P|))}\n2) The partition that only fits partially into the new pop-\nulation is then processed using the method presented\nin Section III-B. In each step, the fitness values for\nthe partition under consideration are computed and the\nindividual with the worst fitness is removed\u2014if multiple\nindividuals share the same minimal fitness, then one of\nthem is selected uniformly at random. This procedure\n11"},{"page":12,"text":"Algorithm 5 HypE Main Loop\nRequire: reference set R \u2286 Z, population size N \u2208 N,\nnumber of generations gmax, number of sampling points\nM \u2208 N\n1: initialize population P by selecting N solutions from X\nuniformly at random\n2: g \u2190 0\n3: while g \u2264 gmaxdo\n4:\nP?\u2190 matingSelection(P,R,N,M)\n5:\nP??\u2190 variation(P?,N)\n6:\nP \u2190 environmentalSelection(P \u222a P??,R,N,M)\n7:\ng \u2190 g + 1\n8: end while\nis repeated until the partition has been reduced to the\ndesired size, i.e., until it fits into the remaining slots left\nin the new population.\nConcerning the fitness assignment, the number of objectives\ndetermines whether the exact or the estimated Ik\nare considered. If less than four objectives are involved,\nwe recommend to employ Algorithm 1, otherwise to use\nAlgorithm 3. The latter works with a fixed number of sampling\npoints to estimate the hypervolume values Ik\nconfidence of the decision to be made; hence, the variance of\nthe estimates does not need to be calculated and it is sufficient\nto update for each sample drawn an array storing the fitness\nvalues of the population members.\nInstead of Algorithm 3, one may also apply the adaptive\nsampling routine described in Algorithm 4 when estimating\nthe hypervolume contributions. To this end, the variance of the\nestimates is calculated after a certain number of initial samples\nand from this, the confidence level is determined. If this lies\nbelow a user-defined level, then the sampling process contin-\nues. Since this process can last arbitrarily long (the difference\nbetween the hypervolume contributions of two solutions can\nbe arbitrarily small), an upper bound for the maximal number\nof samples Mmaxhas to be defined. If this number is reached,\na decision is made based on the current estimates regardless\nof the confidence level. The main advantage of this adaptive\nprocedure is that it is robust with respect to the choice of the\nsample size M. Its main disadvantage is the need to store for\neach population member the number of hits in all domains\nHi(a,P,R), which slows down the sampling considerably, as\nwill be shown in Section VI-C.\nhvalues\nh, regardless of the\nVI. EXPERIMENTS\nThis section serves two goals: (i) to investigate the influence\nof specific algorithmic concepts (fitness, sample size, adaptive\nsampling) on the performance of HypE, and (ii) to study the\neffectiveness of HypE in comparison to existing MOEAs. A\ndifficulty that arises in this context is how to statistically com-\npare the quality of Pareto-set approximations with respect to\nthe hypervolume indicator when a large number of objectives\n(n \u2265 5) is considered. In this case, exact computation of\nthe hypervolume becomes infeasible; to this end, we propose\nAlgorithm 6 HypEMating Selection\nRequire: population P \u2208 \u03a8, reference set R \u2286 Z, number of\noffspring N \u2208 N, number of sampling points M \u2208 N\n1: procedure matingSelection(P,R,N,M)\n2:\nif n \u2264 3 then\n3:\nF \u2190 computeHypervolume(P,R,N)\n4:\nelse\n5:\nF \u2190 estimateHypervolume(P,R,N,M)\n6:\nend if\n7:\nQ \u2190 \u2205\n8:\nwhile |Q| < N do\n9:\nchoose (a,va),(b,vb) \u2208 F uniformly at random\n10:\nif va> vbthen\n11:\nQ \u2190 Q \u222a {a}\n12:\nelse\n13:\nQ \u2190 Q \u222a {b}\n14:\nend if\n15:\nend while\n16:\nreturn Q\n17: end procedure\nMonte Carlo sampling using appropriate statistical tools as\ndetailed below.\nA. Experimental Setup\nHypE is implemented within the PISA framework [6] and\ntested in two versions: the first (HypE) uses fitness-based\nmating selection as described in Algorithm 6, while the second\n(HypE*) employs a uniform mating selection scheme where\nall individuals have the same probability of being chosen\nfor reproduction. Unless stated otherwise, for sampling the\nnumber of sampling points is fixed to M = 10,000 and\nMmax = 20,000 respectively, both kept constant during a\nrun.\nHypE and HypE* are compared to three popular MOEAs,\nnamely NSGA-II [13], SPEA2 [42], and IBEA (in combination\nwith the \u03b5-indicator) [41]. Since these algorithms are not\ndesigned to optimize the hypervolume, it cannot be expected\nthat they perform particularly well when measuring the quality\nof the approximation in terms of the hypervolume indicator.\nNevertheless, they serve as an important reference as they are\nconsiderably faster than hypervolume-based search algorithms\nand therefore can execute a substantially larger number of gen-\nerations when keeping the available computation time fixed.\nOn the other hand, dedicated hypervolume-based methods are\nincluded in the comparisons. The algorithms proposed in [15],\n[23], [9] use the same fitness assignment scheme which can be\nmimicked by means of a HypE variant that only uses the I1\nvalues for fitness assignment, i.e., k is set to 1, and employs the\nroutine for exact hypervolume calculation (Algorithm 1). We\nwill refer to this approach as RHV (regular hypervolume-based\nalgorithm)\u2014the acronym RHV* stands for the variant that\nuses uniform selection for mating. However, we do not provide\ncomparisons to the original implementations of [15], [23], [9]\nbecause the focus is on the fitness assignment principles and\nh\n12"},{"page":13,"text":"Algorithm 7 HypE Environmental Selection\nRequire: population P \u2208 \u03a8, reference set R \u2286 Z, number of\noffspring N \u2208 N, number of sampling points M \u2208 N\n1: procedure environmentalSelection(P,R,N,M)\n2:\nP?\u2190 P \/\u2217 remaining population members \u2217\/\n3:\nQ \u2190 \u2205 \/\u2217 new population \u2217\/\n4:\nQ?\u2190 \u2205 \/\u2217 current nondominated set \u2217\/\n5:\n\/\u2217 iteratively copy nondominated sets to Q \u2217\/\n6:\nrepeat\n7:\nQ \u2190 Q \u222a Q?\n8:\n\/\u2217 determine current nondominated set in P?\u2217\/\n9:\nQ?,P??\u2190 \u2205\n10:\nfor all a \u2208 P?do\n11:\nif \u2200b \u2208 P?: b ? a \u21d2 a ? b then\n12:\nQ?\u2190 Q?\u222a {a}\n13:\nelse\n14:\nP??\u2190 P??\u222a {a}\n15:\nend if\n16:\nend for\n17:\nP?\u2190 P??\n18:\nuntil |Q| + |Q?| \u2265 N \u2228 P?= \u2205\n19:\n\/\u2217 truncate last non-fitting nondominated set Q?\u2217\/\n20:\nk = |Q| + |Q?| \u2212 N\n21:\nwhile k > 0 do\n22:\nif n \u2264 3 then\n23:\nF \u2190 computeHypervolume(Q?,R,k)\n24:\nelse\n25:\nF \u2190 estimateHypervolume(Q?,R,k,M)\n26:\nend if\n27:\n\/\u2217 remove worst solution from Q?\u2217\/\n28:\nQ?\u2190 \u2205\n29:\nremoved \u2190 false\n30:\nfor all (a,v) \u2208 F do\n31:\nif removed = true \u2228 v ?= min(a,v)\u2208F{v} then\n32:\nQ?\u2190 Q?\u222a {a}\n33:\nelse\n34:\nremoved \u2190 true\n35:\nend if\n36:\nend for\n37:\nk \u2190 k \u2212 1\n38:\nend while\n39:\nQ \u2190 Q \u222a Q?\n40:\nreturn Q\n41: end procedure\nnot on specific data structures for fast hypervolume calculation\nas in [15] or specific variation operators as in [23]. Further-\nmore, we consider the sampling-based optimizer proposed\nin [1], here denoted as SHV (sampling-based hypervolume-\noriented algorithm); it more or less corresponds to RHV\nwith adaptive sampling. Finally, to study the influence of\nthe nondominated sorting we also include a simple HypE\nvariant named RS (random selection) where all individuals\nare assigned the same constant fitness value. Thereby, the\nselection pressure is only maintained by the nondominated\nsorting carried out during the environmental selection phase.\nAs basis for the comparisons, the DTLZ [14], the WFG [21],\nand the knapsack [45] test problem suites are considered since\nthey allow the number of objectives to be scaled arbitrarily\u2014\nhere, ranging from 2 to 50 objectives. For the DTLZ problem,\nthe number of decision variables is set to 300, while for the\nWFG problems individual values are used, see Table IV. As\nto the knapsack problem, we used 400 items which where\nmodified with mutation probability 1 by one-bit mutation\nand by one-point crossover with probability 0.5. For each\nbenchmark function, 30 runs are carried out per algorithm\nusing a population size of N = 50 and a maximum number\ngmax = 200 of generations (unless the computation time is\nfixed). The individuals are represented by real vectors, where\na polynomial distribution is used for mutation and the SBX-\n20 operator for recombination [12]. The recombination and\nmutation probabilities are set according to [14].\nB. Statistical Comparison Methodology\nThe quality of the Pareto-set approximations are assessed\nusing the hypervolume indicator, where for less than 6 objec-\ntives the indicator values are calculated exactly and otherwise\napproximated by Monte Carlo sampling as described in [2].\nWhen sampling is used, uncertainty of measurement is in-\ntroduced which can be expressed by the standard deviation\nof the sampled value u(\u02c6IH(A,R)) = IH(A,R)?p(1 \u2212 p)\/n,\nand\u02c6IH the hypervolume estimate. Unless otherwise noted,\n1,000,000 samples are used per Pareto-set approximation.\nFor a typical hit probability between 10% to 90% observed,\nthis leads to a very small uncertainty below 10\u22123in relation\nto IH. Therefore, it is highly unlikely that the uncertainty\nwill influence the statistical test applyied to the hypervolume\nestimates and if it does nonetheless, the statistical tests become\nover-conservative. Hence, we do not consider uncertainty in\nthe following tests.\nLet Aiwith 1 \u2264 i \u2264 l denote the algorithms to be compared.\nFor each algorithm Ai, the same number r of independent\nruns are carried out for 200 generations. For formal reason,\nthe null hypothesis that all algorithms are equally well suited\nto approximate the Pareto-opimal set is investigated first, using\nthe Kruskal-Wallis test at a significance level of \u03b1 = 0.01 [10].\nThis hypothesis could be rejected in all test cases described\nbelow. Thereafter, for all pairs of algorithms the difference in\nmedian of the hypervolume is compared.\nTo test the difference for significance, the Conover-Inman\nprocedure is applied with the same \u03b1 level as in the Kruskal-\nWallis test [10]. Let \u03b4i,jbe 1, if Aiturns out to be significantly\nbetter than Aj and 0 otherwise. Based on \u03b4i,j, for each\nalgorithm Ai the performance index P(Ai) is determined as\nfollows:\nwhere p denotes the hit probability of the sampling process\nP(Ai) =\nl\n?\nj=1\nj=i\n\u03b4i,j\n(24)\nThis value reveals how many other algorithms are better\nthan the corresponding algorithm on the specific test case.\nThe smaller the index, the better the algorithm; an index of\n13"},{"page":14,"text":"Table IV\nNUMBER OF DECISION VARIABLES AND THEIR DECOMPOSITION INTO POSITION AND DISTANCE VARIABLES AS USED FOR THE WFG TEST FUNCTIONS\nDEPENDING ON THE NUMBER OF OBJECTIVES.\nObjective Space Dimensions (n)\n2d3d 5d7d10d 25d50d\ndistance parameters\nposition parameters\n20204258\n12\n5076\n24\n150\n494489\ndecision variables242450 7059 100199\nzero means that no other algorithm generated significantly\nbetter Pareto-set approximations in terms of the hypervolume\nindicator.\nC. Results\nIn the following, we discuss the experimental results\ngrouped according to the foci of the investigations.\n1) Constant Versus Adaptive Sampling: First, the issue of\nadaptive sampling is examined. We compare HypE with its\ncounterpart that uses the adaptive sampling strategy described\nin Algorithm 4 in both environmental selection (employ-\ning Equation (22)) and mating selection (emplyoing Equa-\ntion (23)) with confidence level 0.1 and maximal number of\nsamples of M = 20,000. As mentioned above, standard HypE\nis run with a sample size of M = 10,000 and a constant\nnumber of 200 generations.\nThe experiments indicate that for the 17 test problems each\ninstantiated with 5, 7, 10, 25, and 50 objectives, the adaptive\nstrategy beats the constant sampling method in 12 cases, which\nis vice versa in 10 cases better than adaptive sampling. In the\nremaining 63 instances, the two versions of HypE do not differ\nsignificantly in terms of the hypervolumes achieved.\nWhile both strategy show about the same performance, it is\nexpected that adaptive sampling needs less computation time\nbecause fewer samples are used. However, this is not the case:\nthe adaptive sampling takes between 2.4 and 5.2 times longer\nthan its counterpart, the difference increasing with the number\nof objectives. Two main reasons can be mentioned to explain\nthe additional computation time:\n\u2022 The confidence levels are seldom reached. This means\nthe maximum number of samples are drawn with the\noverhead of calculating the variance of the points from\ntime to time. Figure 7 shows the average number of\nsamples drawn for the DTLZ 2 test problem with 5\nobjectives. While at the initial stage of the run about\n50% of the maximum number of samples are used,\nthe percentage steadily increases to over 95% after 100\ngenerations.\n\u2022 Calculating the variances requires to take track of the\nhits in each partition Hi(a,A,R) for every population\nmember a. This slows down the sampling process con-\nsiderably.\nThese observations lead to the conclusion that the benefits of\nadaptive sampling are mainly compensated by the additional\ncomputational overhead. For this reason, only HypE with a\n0 20 40 6080 100120140 160 180200\n8\n10\n12\n14\n16\n18\n20x 103\ngeneration\nnumber of samples per removal\nFigure 7.\nHypE with significance level 0.05 and a maximum of 20,000 samples. The\ntestproblem is DTLZ 2 with 5 objectives and the average is based on 30 runs.\nAverage number of samples used per removal for adaptive\nconstant sample size is considered in the following compar-\nisons. Future research may be necessary to investigate how\nadaptivity could be integrated more efficiently. For instance,\nas Figure 7 indicates, one could use adaptive sampling in an\nearly stage of the evolutionary run, say in the first twenty\ngenerations, and then switch to constant sampling. Further-\nmore, adaptive sampling might pay off when the user defined\nconfidence L is close to 1 and large number of samples need\nto be drawn.\n2) Exact Hypervolume Computation Versus Sampling:\nNext, we compare HypE with RHV\u2014due to the large com-\nputation effort caused by the exact hypervolume calculation\nonly on a single test problem, namely DTLZ2 with 2, 3, 4,\nand 5 objectives. Both HypE and HypE* are run with exact\nfitness calculation (Algorithm 1) as well as with the estimation\nprocedure (Algorithm 3); the former variants are marked with\na trailing \u2019e\u2019, while the latter variants are marked with a trailing\n\u2019-s\u2019. All algorithms run for 200 generations, per algorithm 30\nruns were performed.\nFigure 8 shows the hypervolume values normalized for\neach test problem instance separately. As one may expect,\nHypE beats HypE*. Moreover, fitness-based mating selection\nis beneficial to both HypE and RHV. The two best variants,\nHypE-e and RHV, reach about the same hypervolume values,\nindependently of the number of objectives. Although HypE\nreaches a better hypervolume median for all four number\nof objectives, the difference is never significant\n5. . Hence,\n5According to the Kruskal-Wallis test described in Section VI-B with\nconfidence level 0.01.\n14"},{"page":15,"text":"235710 2550\n0\n1\n2\n3\n4\n5\n6\ndimension\nmean performance\nHypE\nHypE*\nIBEA\nRHVNSGA-II\nSPEA2\nRS\nFigure 9. Mean performance score over all test problems for different number\nof objectives. The smaller the score, the better the Pareto-set approximation\nin terms of hypervolume.\nHypE can be considered an adequate alternative to the regular\nhypervolume algorithms; the main advantage though becomes\nevident when the respective fitness measures need to be\nestimated, see below.\n3) HypE Versus Other MOEAs: Now we compare HypE\nand HypE*, both using a constant number of samples, to other\nmultiobjective evolutionary algorithms. Table V on pages 23\u2013\n25 shows the performance score and mean hypervolume on the\n17 test problems mentioned in the Experimental Setup Section.\nExcept on few testproblems HypE is better than HypE*. HypE\nreaches the best performance score overall. Summing up all\nperformance scores, HypE yields the best total (76), followed\nby HypE* (143), IBEA (171) and the method proposed in\n[1] (295). SPEA2 and NSGA-II reach almost the same score\n(413 and 421 respectively), clearly outperforming the random\nselection (626).\nIn order to better visualize the performance index, we\nshow to Figures where the index is summarized for dif-\nferent test-problems and number of objectives respectively.\nFigure 9 shows the average performance over all testproblems\nfor different number of objectives. Except for two objective\nproblems, HypE yields the best score, increasing its lead\nin higher dimensions. The version using uniform mating\nselection, HypE*, is outperformed by IBEA for two to seven\nobjectives and only thereafter reaches a similar score as HypE.\nThis indicates, that using non-uniform mating selection is\nparticularly advantageous for small number of objectives.\nNext we look at the performance score for the individual\ntest-problems. Figure 10 shows the average index over all\nnumber of objectives. For DTLZ2, 4, 5 and 7, knapsack and\nWFG8, IBEA outperforms HypE, for DTLZ7 and knapsack,\nSHV as well is better than HypE. On the remaining 11\ntestproblems, HypE reaches the best mean performance.\nNote that the above comparison is carried out for the case\nall algorithms run for the same number of generations and\nHypE needs longer execution time, e.g., in comparison to\nSPEA2 or NSGA-II. We therefore investigate in the following,\nwhether NSGA-II and SPEA2 will not overtake HypE given\nD1 D2 D3 D4 D5 D6 D7 K1 W1W2W3W4W5W6W7W8W9\ntestproblem\n0\n1\n2\n3\n4\n5\n6\nmean performance\n \n \nRHV\nIBEA\nNSGA\u2212II\nRS\nSPEA HypE HypE*\nFigure 10.\nproblems, namely DTLZ (Dx), WFG (Wx) and knapsack (K1). The values of\nHypE+ are connected by a solid line to easier assess the score.\nMean performance score over all dimensions for different test\na constant amount of time. Figure 11 shows the hypervolume\nof the Pareto-set approximations over time for HypE using\nthe exact fitness values as well as the estimated values for\ndifferent samples sizes M. Although only the results on WFG9\nare shown, the same experiments were repeated on DTLZ2,\nDTLZ7, WFG3 and WFG6 and provided similar outcomes.\nEven though SPEA2, NSGA-II and even IBEA are able to\nprocess twice as many generations as the exact HypE, they do\nnot reach its hypervolume. In the three dimensional example\nused, HypE can be run sufficiently fast without approximating\nthe fitness values. Nevertheless, the sampled version is used as\nwell to show the dependency of the execution time and quality\non the number of samples M. Via M, the execution time\nof HypE can be traded off against the quality of the Pareto-\nset approximation. The fewer samples are used, the more the\nbehavior of HypE resembles random selection. On the other\nhand by increasing M, the quality of exact calculation can be\nachieved, increasing the execution time, though. For example,\nwith M = 1,000, HypE is able to carry out nearly the\nsame number of generations as SPEA2 or NSGA-II, but the\nPareto-set is just as good as when 100,000 samples are used,\nproducing only a fifteenth the number of generations. In the\nexample given, M = 10,000 represents the best compromise,\nbut the number of samples should be increased in two cases:\n(i) the fitness evaluation takes more time. This will affect\nthe faster algorithm much more and increasing the number of\nsamples will influence the execution time much less. (ii) More\ngenerations are used. In this case, HypE using more samples\nmight overtake the faster versions with fewer samples, since\nthose are more vulnerable to stagnation.\nVII. CONCLUSIONS\nThis paper proposes HypE (Hypervolume Estimation Algo-\nrithm for Multiobjective Optimization), a novel hypervolume-\nbased multiobjective evolutionary algorithm that can be ap-\nplied to problems with arbitrary numbers of objective func-\ntions. It incorporates a new fitness assignment scheme based\n15"},{"page":16,"text":"HypE*-s\nHypE*-e\nRHV*\nHypE-s\nHypE-e\nRHV\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nValues\nHypE*-s\nHypE*-e\nRHV*\nHypE-s\nHypE-e\nRHV\nHypE*-s\nHypE*-e\nRHV*\nHypE-s\nHypE-e\nRHV\nHypE*-s\nHypE*-e\nRHV*\nHypE-s\nHypE-e\nRHV\n2 objectives 3 objectives4 objectives 5 objectives\nFigure 8.\n3, 4, and 5 objectives. For presentation reasons, the hypervolume values are normalized to the minimal and maximal values observed per problem instance.\nComparison of the hypervolume indicator values for different variants of HypE and the regular hypervolume algorithm (RHV) on DTLZ2 with 2,\nminutes\nhypervolume\n \n0\n \n1658\n135\n522\n2353\n2434\n2125\n12345678910\n  .475\n.500\n  .525\n  .550\n522\n2024\n881\n134\n1123\nminutes\n0123456789 10\nSHV-1k\nSHV-10k\nSHV-100k\nHypE-1k\nHypE-100k\nHypE-10k\nIBEA\nNSGA-II\nSPEA2\nSHV-10k\nHypE-e\nFigure 11.\nThe test problem is WFG9 for three objectives. HypE is compared to the algorithms presented in Section VI, where the results are split in two figures with\nidentical axis for the sake of clarity. The numbers at the left border of the figures indicate the total number of generations.\nHypervolume process over ten minutes of HypE+ for different samples sizes x in thousands (Hy-xk) as well as using the exact values (Hy-e).\non the Lebesgue measure, where this measure can be both ex-\nactly calculated and estimated by means of Monte Carlo sam-\npling. The latter allows to trade-off fitness accuracy versus the\noverall computing time budget which renders hypervolume-\nbased search possible also for many-objective problems, in\ncontrast to [15], [23], [9]. HypE is available for download at\nhttp:\/\/www.tik.ee.ethz.ch\/sop\/download\/supplementary\/hype\/.\nHypE was compared to various state-of-the-art MOEAs\nwith regard to the hypervolume indicator values of the gen-\nerated Pareto-set approximations\u2014on the DTLZ [14], the\nWFG [21], and the knapsack [45] test problem suites. The\nsimulations results indicate that HypE is a highly competitive\nmultiobjective search algorithm; in the considered setting the\nPareto front approximations obtained by HypE reached the\nbest hypervolume value in 6 out of 7 cases averaged over all\ntestproblems.\nA promising direction of future research is the development\nof advanced adaptive sampling strategies that exploit the avail-\nable computing resources most effectively, such as increasing\nthe number of samples towards the end of the evolutionary\nrun.\nACKNOWLEDGEMENTS\nJohannes Bader has been supported by the Indo-Swiss Joint\nResearch Program under grant IT14.\nREFERENCES\n[1] J. Bader, K. Deb, and E. Zitzler. Faster Hypervolume-based Search using\nMonte Carlo Sampling. In Conference on Multiple Criteria Decision\nMaking (MCDM 2008). Springer, 2008. to appear.\n16"},{"page":17,"text":"[2] J. Bader and E. Zitzler. HypE: Fast Hypervolume-Based Multiobjective\nSearch Using Monte Carlo Sampling.\nTechnische Informatik und Kommunikationsnetze, ETH Z\u00fcrich, April\n2006.\n[3] N. Beume, C. M. Fonseca, M. Lopez-Ibanez, L. Paquete, and J. Vahren-\nhold. On the Complexity of Computing the Hypervolume Indicator.\nTechnical Report CI-235\/07, University of Dortmund, December 2007.\n[4] N. Beume, B. Naujoks, and M. Emmerich. SMS-EMOA: Multiobjective\nselection based on dominated hypervolume.\nOperational Research, 181:1653\u20131669, 2007.\n[5] N. Beume and G. Rudolph. Faster S-Metric Calculation by Considering\nDominated Hypervolume as Klee\u2019s Measure Problem. Technical Report\nCI-216\/06, Sonderforschungsbereich 531 Computational Intelligence,\nUniversit\u00e4t Dortmund, 2006. shorter version published at IASTED\nInternational Conference on Computational Intelligence (CI 2006).\n[6] S. Bleuler, M. Laumanns, L. Thiele, and E. Zitzler. PISA\u2014A Plat-\nform and Programming Language Independent Interface for Search\nAlgorithms. In C. M. Fonseca, P. J. Fleming, E. Zitzler, K. Deb,\nand L. Thiele, editors, Conference on Evolutionary Multi-Criterion\nOptimization (EMO 2003), volume 2632 of LNCS, pages 494\u2013508,\nBerlin, 2003. Springer.\n[7] L. Bradstreet, L. Barone, and L. While. Maximising Hypervolume for\nSelection in Multi-objective Evolutionary Algorithms. In 2006 IEEE\nCongress on Evolutionary Computation (CEC 2006), pages 6208\u20136215,\nVancouver, BC, Canada, 2006. IEEE.\n[8] K. Bringmann and T. Friedrich. Approximating the volume of unions\nand intersections of high-dimensional geometric objects. In Proc. of the\n19th International Symposium on Algorithms and Computation (ISAAC\n2008), Berlin, Germany, 2008. Springer.\n[9] D. Brockhoff and E. Zitzler. Improving Hypervolume-based Multiobjec-\ntive Evolutionary Algorithms by Using Objective Reduction Methods. In\nCongress on Evolutionary Computation (CEC 2007), pages 2086\u20132093.\nIEEE Press, 2007.\n[10] W. J. Conover. Practical Nonparametric Statistics. John Wiley, 3 edition,\n1999.\n[11] J. Cooper and T. Friedrich. The Hypervolume Indicator and Klee\u2019s\nMeasure Problem. Submitted to ICALP 2008.\n[12] K. Deb.\nMulti-objective optimization using evolutionary algorithms.\nWiley, Chichester, UK, 2001.\n[13] K. Deb, S. Agrawal, A. Pratap, and T. Meyarivan. A Fast Elitist Non-\nDominated Sorting Genetic Algorithm for Multi-Objective Optimization:\nNSGA-II.In M. Schoenauer et al., editors, Conference on Parallel\nProblem Solving from Nature (PPSN VI), volume 1917 of LNCS, pages\n849\u2013858. Springer, 2000.\n[14] K. Deb, L. Thiele, M. Laumanns, and E. Zitzler. Scalable Test Problems\nfor Evolutionary Multi-Objective Optimization. In A. Abraham, R. Jain,\nand R. Goldberg, editors, Evolutionary Multiobjective Optimization:\nTheoretical Advances and Applications, chapter 6, pages 105\u2013145.\nSpringer, 2005.\n[15] M. Emmerich, N. Beume, and B. Naujoks. An EMO Algorithm Using\nthe Hypervolume Measure as Selection Criterion.\nEvolutionary Multi-Criterion Optimization (EMO 2005), volume 3410\nof LNCS, pages 62\u201376. Springer, 2005.\n[16] M. Emmerich, A. Deutz, and N. Beume. Gradient-Based\/Evolutionary\nRelay Hybrid for Computing Pareto Front Approximations Maximizing\nthe S-Metric. In Hybrid Metaheuristics, pages 140\u2013156. Springer, 2007.\n[17] R. Everson, J. Fieldsend, and S. Singh. Full Elite-Sets for Multiobjective\nOptimisation. In I.C. Parmee, editor, Proceedings of the fifth interna-\ntional conference on adaptive computing in design and manufacture\n(ADCM 2002), pages 343\u2013354, London, UK, 2002. Springer.\n[18] M. Fleischer. The measure of Pareto optima. Applications to multi-\nobjective metaheuristics. In C. M. Fonseca et al., editors, Conference on\nEvolutionary Multi-Criterion Optimization (EMO 2003), volume 2632\nof LNCS, pages 519\u2013533, Faro, Portugal, 2003. Springer.\n[19] C. M. Fonseca, L. Paquete, and M. L\u00f3pez-Ib\u00e1\u00f1ez.\nDimension-Sweep Algorithm for the Hypervolume Indicator.\nCongress on Evolutionary Computation (CEC 2006), pages 1157\u20131163,\nSheraton Vancouver Wall Centre Hotel, Vancouver, BC Canada, 2006.\nIEEE Press.\n[20] D. E. Goldberg.\nGenetic Algorithms in Search, Optimization, and\nMachine Learning. Addison-Wesley, Reading, Massachusetts, 1989.\nTIK Report 286, Institut f\u00fcr\nEuropean Journal on\nIn Conference on\nAn Improved\nIn\n[21] S. Huband, P. Hingston, L. Barone, and L. While.\nMultiobjective Test Problems and a Scalable Test Problem Toolkit. IEEE\nTransactions on Evolutionary Computation, 10(5):477\u2013506, 2006.\n[22] S. Huband, P. Hingston, L. White, and L. Barone.\nStrategy with Probabilistic Mutation for Multi-Objective Optimisation.\nIn Proceedings of the 2003 Congress on Evolutionary Computation\n(CEC 2003), volume 3, pages 2284\u20132291, Canberra, Australia, 2003.\nIEEE Press.\n[23] C. Igel, N. Hansen, and S. Roth. Covariance Matrix Adaptation for\nMulti-objective Optimization. Evolutionary Computation, 15(1):1\u201328,\n2007.\n[24] J. Knowles and D. Corne. On Metrics for Comparing Non-Dominated\nSets. In Congress on Evolutionary Computation (CEC 2002), pages\n711\u2013716, Piscataway, NJ, 2002. IEEE Press.\n[25] J. Knowles and D. Corne. Properties of an Adaptive Archiving\nAlgorithm for Storing Nondominated Vectors. IEEE Transactions on\nEvolutionary Computation, 7(2):100\u2013116, 2003.\n[26] J. D. Knowles. Local-Search and Hybrid Evolutionary Algorithms for\nPareto Optimization. PhD thesis, University of Reading, 2002.\n[27] M. Laumanns, G. Rudolph, and H.-P. Schwefel.\nPareto Set: Concepts, Diversity Issues, and Performance Assessment.\nTechnical Report CI-7299, University of Dortmund, 1999.\n[28] S. Mostaghim, J. Branke, and H. Schmeck.\nSwarm Optimization on Computer Grids. In Proceedings of the 9th\nannual conference on Genetic and evolutionary computation (GECCO\n2007), pages 869\u2013875, New York, NY, USA, 2007. ACM.\n[29] M. Nicolini. A Two-Level Evolutionary Approach to Multi-criterion\nOptimization of Water Supply Systems. In Conference on Evolutionary\nMulti-Criterion Optimization (EMO 2005), volume 3410 of LNCS, pages\n736\u2013751. Springer, 2005.\n[30] S. Obayashi, K. K. Deb, C. Poloni, T. Hiroyasu, and T. Murata, editors.\nConference on Evolutionary Multi-Criterion Optimization (EMO 2007),\nvolume 4403 of LNCS, Berlin, Germany, 2007. Springer.\n[31] J. Scharnow, K. Tinnefeld, and I. Wegener. The Analysis of Evolu-\ntionary Algorithms on Sorting and Shortest Paths Problems. Journal of\nMathematical Modelling and Algorithms, 3(4):349\u2013366, 2004. Online\nDate Tuesday, December 28, 2004.\n[32] J. R. Swisher and S. H. Jacobson. A survey of ranking, selection, and\nmultiple comparison procedures for discrete-event simulation. In WSC\n\u201999: Proceedings of the 31st conference on Winter simulation, pages\n492\u2013501, New York, NY, USA, 1999. ACM.\n[33] D. A. Van Veldhuizen. Multiobjective Evolutionary Algorithms: Classi-\nfications, Analyses, and New Innovations. PhD thesis, Graduate School\nof Engineering, Air Force Institute of Technology, Air University, 1999.\n[34] T. Wagner, N. Beume, and B. Naujoks.\nand Indicator-based Methods in Many-objective Optimization.\nS. Obayashi et al., editors, Conference on Evolutionary Multi-Criterion\nOptimization (EMO 2007), volume 4403 of LNCS, pages 742\u2013756,\nBerlin Heidelberg, Germany, 2007. Springer.\nlished as internal report of Sonderforschungsbereich 531 Computational\nIntelligence CI-217\/06, Universit\u00e4t Dortmund, September 2006.\n[35] L. While. A New Analysis of the LebMeasure Algorithm for Calculating\nHypervolume. In Conference on Evolutionary Multi-Criterion Optimiza-\ntion (EMO 2005), volume 3410 of LNCS, pages 326\u2013340, Guanajuato,\nM\u00e9xico, 2005. Springer.\n[36] L. While, P. Hingston, L. Barone, and S. Huband. A Faster Algorithm\nfor Calculating Hypervolume.\nComputation, 10(1):29\u201338, 2006.\n[37] Q. Yang and S. Ding.Novel Algorithm to Calculate Hypervolume\nIndicator of Pareto Approximation Set. In Advanced Intelligent Com-\nputing Theories and Applications. With Aspects of Theoretical and\nMethodological Issues, Third International Conference on Intelligent\nComputing (ICIC 2007), volume 2, pages 235\u2013244, 2007.\n[38] E. Zitzler.\nEvolutionary Algorithms for Multiobjective Optimization:\nMethods and Applications. PhD thesis, ETH Zurich, Switzerland, 1999.\n[39] E. Zitzler. Hypervolume metric calculation. ftp:\/\/ftp.tik.ee.ethz.ch\/pub\/\npeople\/zitzler\/hypervol.c, 2001.\n[40] E. Zitzler, D. Brockhoff, and L. Thiele. The Hypervolume Indicator\nRevisited: On the Design of Pareto-compliant Indicators Via Weighted\nIntegration. In S. Obayashi et al., editors, Conference on Evolutionary\nA Review of\nAn Evolution\nApproximating the\nMulti-objective Particle\nPareto-, Aggregation-,\nIn\nextended version pub-\nIEEE Transactions on Evolutionary\n17"},{"page":18,"text":"Multi-Criterion Optimization (EMO 2007), volume 4403 of LNCS, pages\n862\u2013876, Berlin, 2007. Springer.\n[41] E. Zitzler and S. K\u00fcnzli. Indicator-Based Selection in Multiobjective\nSearch.In Conference on Parallel Problem Solving from Nature\n(PPSN VIII), volume 3242 of LNCS, pages 832\u2013842. Springer, 2004.\n[42] E. Zitzler, M. Laumanns, and L. Thiele. SPEA2: Improving the Strength\nPareto Evolutionary Algorithm for Multiobjective Optimization. In K.C.\nGiannakoglou et al., editors, Evolutionary Methods for Design, Optimi-\nsation and Control with Application to Industrial Problems (EUROGEN\n2001), pages 95\u2013100. International Center for Numerical Methods in\nEngineering (CIMNE), 2002.\n[43] E. Zitzler and L. Thiele. An Evolutionary Approach for Multiobjective\nOptimization: The Strength Pareto Approach.\nComputer Engineering and Networks Laboratory, ETH Zurich, May\n1998.\n[44] E. Zitzler and L. Thiele. Multiobjective Optimization Using Evolution-\nary Algorithms - A Comparative Case Study. In Conference on Parallel\nProblem Solving from Nature (PPSN V), pages 292\u2013301, Amsterdam,\n1998.\n[45] E. Zitzler and L. Thiele. Multiobjective Evolutionary Algorithms: A\nComparative Case Study and the Strength Pareto Approach.\nTransactions on Evolutionary Computation, 3(4):257\u2013271, 1999.\n[46] E. Zitzler, L. Thiele, and J. Bader. On Set-Based Multiobjective Opti-\nmization. Technical Report 300, Computer Engineering and Networks\nLaboratory, ETH Zurich, February 2008.\n[47] E. Zitzler, L. Thiele, M. Laumanns, C. M. Fonseca, and V. Grunert\nda Fonseca. Performance Assessment of Multiobjective Optimizers: An\nAnalysis and Review. IEEE Transactions on Evolutionary Computation,\n7(2):117\u2013132, 2003.\nTechnical Report 43,\nIEEE\nAPPENDIX\nA. Proofs for Section III\nProof of Theorem III.3, page 4: According to 5 it holds:\nIH(A,R) = \u03bb?H(A,R))\n= \u03bb\nH(S,A,R)\n?\u02d9?\nS\u2286A\n?\n.\nBy dividing the subsets into groups of equal size, one obtains\n?\n= \u03bb\n\u02d9?\n1\u2264i\u2264|A|\n\u02d9?\n|S|=i\nS\u2286A\nH(S,A,R)\n?\nwhich can be rewritten as\n=\n|A|\n?\ni=1\n\u03bb\n? ?\n|S|=i\nS\u2286A\nH(S,A,R)\n?\nbecause the inner unions are all disjoint. Now, for each subset\nof size i we count the Lebesque measure once for each element\nand then divide by 1\/i:\n=\n|A|\n?\ni=1\n1\ni\n?\na\u2208A\n\u03bb\n? ?\n|S|=i\na\u2208S\nS\u2286A\nH(S,A,R)\n?\n.\nChanging the order of the sums results in\n=\n?\na\u2208A\n|A|\n?\ni=1\n1\ni\u03bb\n? ?\n|S|=i\na\u2208S\nS\u2286A\nH(S,A,R)\n?\nand using Definition III.2 we obtain\n=\n?\na\u2208A\nIh(a,A,R)\nwhich concludes the proof.\nProof of Theorem III.4, page 4: From Theorem III.3 we\nknow that\n?\nwhich\u2014following Definition III.1\u2014equals\nb1\u2208{a}\u222aB1\nIh(b1,{a} \u222a B1,R) = IH({a} \u222a B1,R)\n= \u03bb?H({a} \u222a B1,R)?.\nSince {a} ? B1, it holds H(b,R) \u2286 H(A,R) for all b \u2208 B1\nand therefore the above formula can be simplified to\n= \u03bb?H({a},R)?\nThe same holds analogically for the right-hand side of the\nequation in Theorem III.4 which proves the claim.\nProof of Theorem III.7, page 4: Definition III.6 states\nthat\n\u23a1\n\u23a3\nwhere S denotes the set of subsets of A, that contain k\nelements, one of which is individual a, i.e., S = {S \u2286 A;a \u2208\nS \u2227 |S| = k}. Inserting the definition of S leads to\n1\n|S|\nS\u2208A\n|S|=k\na\u2208S\nTo combine the two summations of the previous equa-\ntion, let o(T) denote the number of times the summand\n1\nIk\nh(a,A,R) =\n1\n|S|\n?\nS\u2208S\n\u23a2\n?\na\u2208T\nT\u2286S\n1\n|T|\u03bb?H(T,A,R)?\n\u23a4\n\u23a6\n\u23a5\n=\n?\n?\na\u2208T\nT\u2286S\n1\n|T|\u03bb?H(T,A,R)?\n(25)\n|T|\u03bb?H(T,A,R)?is added for the same set T\n=\n|S|\n1\n?\na\u2208T\nT\u2286A\no(T)1\n|T|\u03bb?H(T,A,R)?\nsplitting up into summation over subsets of equal size gives\n=\n1\n|S|\nk\n?\ni=1\n1\ni\n?\n|T|=i\na\u2208T\nT\u2286A\no(T)\u03bb?H(T,A,R)?\nFor symmetry reasons, each subset T with cardinality |T| = i\nhas the same number of occurences o(T) =: oi\n=\n1\n|S|\nk\n?\ni=1\noi\ni\n?\n|T|=i\na\u2208T\nT\u2286A\n\u03bb?H(T,A,R)?\n18"},{"page":19,"text":"A\nS\nT\nk=6\n|A|=10\ni=4\na\na\na\nFigure 12.\nsets contain a. Given one particular T of size i there exist?|A|\u2212i\nT is a subset of S, which in turn is a subset of A; all three\nk\u2212i\n?\nsubsets\nS \u2286 A of size k which are a superset of T.\nsince all H(T,A,R) in the sum are disjoint according to (5)\n=\n1\n|S|\nk\n?\ni=1\noi\ni\u03bb\n? ?\n|T|=i\na\u2208T\nT\u2286A\nH(T,A,R)\n?\nand according to Equation (6)\n=\nk\n?\ni=1\noi\ni \u00b7 |S|\u03bb?Hi(a,A,R)?\nAfter having transformed the original equation, we deter-\nmine the number oi, i.e., the number of times the term\n1\nappears in (25). The term is added once\nevery time the corresponding set T is a subset of S. Hence,\no(T) with |T| = i corresponds to the number of sets S that are\na superset of T. As depicted in Figure A, T defines i elements\nof S and the remaining k \u2212 i elements can be chosen freely\nfrom the |A| \u2212 i elements in A that are not yet in S.\nTherefore, there exist?|A|\u2212i\n?|A|\u2212i\noi\n|S|=\nk\u22121\n=(|A| \u2212 i)!(k \u2212 1)!((|A| \u2212 1) \u2212 (k \u2212 1))!\n(k \u2212 i)!((|A| \u2212 i) \u2212 (k \u2212 i))!(|A| \u2212 1)!\n=(|A| \u2212 i)!(k \u2212 1)!\n(|A| \u2212 1)!(k \u2212 i)!\n=\n(|A| \u2212 1)(|A| \u2212 2)\u00b7\u00b7\u00b7(|A| \u2212 (i \u2212 1)\n=\n|A| \u2212 j\n= \u03b1i\n|T|\u03bb?H(T,A,R)?\nk\u2212i\n?subsets S \u2208 S that contain one\nparticular T with |T| = i and a \u2208 T. Therefore, o(T) = oi=\nk\u2212i\nHence\n?|A|\u2212i\n?. Likewise, the total number of sets S is |S| =?|A|\u22121\n?\nk\u22121\n?.\nk\u2212i\n?|A|\u22121\n?\n(k \u2212 1)(k \u2212 2)\u00b7\u00b7\u00b7(k \u2212 (i \u2212 1))\ni\u22121\n?\nj=1\nk \u2212 j\nTherefore\nIk\nh(a,A,R) =\nk\n?\ni=1\n\u03b1i\ni\u03bb?Hi(a,A,R)?\nwhich concludes the proof.\nB. Derivation of Confidence Intervals used in IV-B\nHere, we consider confidence intervals for\u02c6Ik\norder to adaptively determine the number of sampling points\nh(a,A,R) in\nto be drawn in order to reach a user predefined confidence\nlevel. However, there are also other uses: for instance, to\nindicate the quality when reporting the values to a decision\nmaker or carrying out statistical tests; or to adapt the selection\nprobabilites for fitness proportionate selection meaning that\nhigh-fitness individuals are penalized whenever the fitness\nvalue comes along with a high uncertainty.\nThe confidence intervals can be derived by determining the\ndistribution of a sampled value. To this end, a new discrete\nrandom variable Z is introduced with the property Pr(Z =\n\u03b1i\/i) = piwhere pidenotes the probability of a hit in the ith\npartition, i.e., a hit in Hi(a,A,R); picorresponds to the ratio\nbetween the volume of the particular partition and the volume\nof the sampling space. Formula (20) can now be rewritten as\na sum of M random variables Zi distributed independently\nidentically as Z\n\u02c6Ik\nh(a,A,R) =V\nM\nM\n?\ns=1\nZi\n(26)\nAccording to the central limit theorem, this estimate is ap-\nproximately normally distributed with mean Ik\nthe following variance:\nh(a,A,R and\nVar?\u02c6Ik\nh(a,A,R)?=V2\nM2\n?\nk\n?\nk\n?\ni=1\n\u03b12\ni2Var(Pi)+\ni\n(27)\n2\nk\n?\ni=1j=i+1\n\u03b1i\u03b1j\nij\nCov(Pi,Pj)\n?\n(28)\nwhere Pi denotes the number of hits in the ith partition.\nThe variance of Pifollows from the binomial distribution as\nVar(Pi) = Mpi(1 \u2212 pi) and the covariance of Piand Pj is\nM\n?\nbecause Cov(X(i)\nSince the probabilities pi are unknown, the estimates \u02c6 pi =\nPi\/M are used instead. This of course presupposes counting\nthe number of hits in a particular partition. The estimated\nvariance according to Eq. (28) becomes\nCov(Pi,Pj) =\nf=1\nM\n?\ng ) = \u2212pipj if i = j and 0 otherwise.\ng=1\nCov?X(i)\nf,X(j)\ng\n?= \u2212Mpipj\n(29)\nf,X(j)\n?\nVar?\u02c6Ik\nh(a,A,R)?=V2\nM\n?\nk\n?\ni=1\n\u03b12\ni2Pi(1 \u2212 Pi) \u2212 2\ni\nk\n?\ni=1\nk\n?\nj=i+1\n\u03b1i\u03b1j\nij\nPiPj\n?\n(30)\nThe normal approximation with the mean and variance just\nderived can now be used to give the confidence interval for a\nparticular estimate, i.e.,\nIk\nh(a,A,R) \u2208\u02c6Ik\nh(a,A,R) \u00b1 z1\u2212perr\/2\n?\n?\nVar?\u02c6Ik\nh(a,A,R)?\n(31)\nwhere z\u03b2 denotes the \u03b2-percentile of a standard normal\ndistribution and perr the probability of error.\n19"},{"page":20,"text":"1) Ranking and Selection: In order to use the confidence\ninterval of Equation 31 for adaptive sampling, we will distin-\nguish three situations\u2014depending on how the fitness estimates\n\u02c6Ik\nh(a,A,R) of a population A are used:\n\u2022 The individual of A with the smallest value needs to be\nidentified for removal: This occurs when applying the\niterative greedy strategy for environmental selection.\n\u2022 The subset A?of A compromising the k individuals\nwith the smallest fitness values should be determined for\nremoval: This occurs when applying the one-shot greedy\nstrategy for environmental selection.\n\u2022 The population members a \u2208 A should be ranked\naccording to their Ik\nwhen using rank-based mating selection, e.g., tournament\nselection.\nh(a,A,R) values: This is the case\nIn all three scenarios, one is interested in the probability that\nthe selection and ranking respectively is done correctly, i.e.,\ncorresponds to the one that would result when using the exact\nvalues Ik\nwhich then can be used as a stopping criterion for the sampling\nprocess.\nConsider first the simple case of deciding between the\nestimate of two individuals a,b \u2208 A whether either is\nsmaller. Let the indicator values be v1 := Ik\nv2 := Ik\nand \u02c6 v2 :=\u02c6Ik\nis selected. Furthermore, let \u03b4 := v1\u2212 v2 denote difference\nbetween the indicator values and\u02c6\u03b4 := \u02c6 v1\u2212 \u02c6 v2the difference\nbetween the estimates. By examining the distribution of \u03b4\ngiven\u02c6\u03b4, the probability of correct selection Cp corresponds\nto the probability P(\u03b4 < 0|\u02c6\u03b4).\nThe difference \u03b4 is approximately normally distributed\naround \u02c6 v1 \u2212 \u02c6 v2 due to the same arguments used above.\nSince the same samples are used to estimate v1 and v2, the\ncorresponding estimates are not independent\u2014the variance\nof the difference is Var(\u03b4) \u2248 \u03c32\nThe first two summands can be determined according to\n(30), the correlation \u03c1 is more difficult to determine as it\ncan vary greatly as Fig. 13 reveals. The correlation can be\nestimated iteratively, but this implies updating \u03c1 for all pairs\nof individuals after each sample drawn. For two reasons this is\nimpractical: Firstly, keeping track of all correlations requires a\nlot of memory and processing time compared to the sampling\nprocess itself and will significantly slow down the estimation\nprocess; secondly, the exact error probability is not crucial,\nand an approximation of it or a lower bound is adequate for\nthe intended applications.\nTherefore, we propose using an upper bound for the correct\nprobability, based solely on the variance of the estimates. If\nthe exact value v1is smaller than a value B and v2is bigger\nthan B, then clearly v1< v2. We can therefore give a lower\nh(a,A,R). This probability is derived in the following\nh(a,A,R) and\nh(a,A,R)\nh(b,A,R) and their estimates be \u02c6 v1 :=\u02c6Ik\nh(b,A,R) with \u02c6 v1 < \u02c6 v2, hence individual a\n\u02c6 v1+ \u03c32\n\u02c6 v2\u2212 2\u03c3\u02c6 v1\u03c3\u02c6 v2\u03c1\u02c6 v1,\u02c6 v2.\n\u22120.4\u22120.20 0.2\ncorrelation\n0.40.6 0.81\n0\n50\n100\n150\n200\n250\n300\n350\n400\nfrequency\nFigure 13.\na given Pareto-set approximation introduced in Table II. The closer to points\nare, the higher the correlation between their estimates becomes. Contrariwise,\nthe estimates can be negatively correlated for points which lie far apart.\nCorrelation between fitness estimates of different individuals of\n101\n102\n103\n104\n105\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nSamples\nPairwise Confidence\n \n \nExact\nLower Bound\nFigure 14.\nrandomly selected individuals of a Pareto set approximation as in Table II.\nThe solid line shows the exact confidence interval calculated based on\nthe correlation information. The dashed course represents the lower bound\naccording to (33).\nShows the confidence interval for a comparison between two\nbound for ordering the two individuals correctly by\nP(v1< v2|\u02c6 v1< \u02c6 v2) \u2265 P(v1< B|\u02c6 v1) \u00b7 P(v2> B|v1< B, \u02c6 v2)\n(32)\nwhere B =?\u02c6 v1+ \u02c6 v2\nP(v1< v2|\u02c6 v1< \u02c6 v2) \u2265 1 \u2212 P(v1> B|\u02c6 v1) \u2212 P(v2< B|\u02c6 v2)\n?\/2. (32) in turn can be lower bounded,\nwhich finally leads to\n(33)\nIn (33) all probabilities involved are pairwise independent\nand hence can be determined using the normal distribution\nwith the variance given in Eq. (30). Figure B1 shows the\nestimated confidence according to (33) in comparison to the\nexact confidence level.\nAfter these general considerations, we can now provide\nspecific lower bounds for the three situations mentioned above.\na) Selecting the Worst Individual: There exist many\nprocedures in the field of ranking and selection [32]. However,\nthey often assume that the estimates are independent or at\n20"},{"page":21,"text":"selecting the worstselecting the s worst ranking\n?\n?\n?\nincreasing value\nB\n1\u02c6 v\n2\u02c6 v\n2\u02c6 v\n|A|\n\u02c6 v\n1\u02c6 v\n1\u02c6 v\ns\u02c6 v\ns+1\n\u02c6 v\n|A|\n\u02c6 v\n?\n|A|\n\u02c6 v\n1\n\u02c6\nv\n\u02c6\nv\n2\nss\nB\n++\n=\n1\nB\n2\nB\n| |-1\nA\nB\nFigure 15.\nSection B1: When selecting the worst point, the threshold is determined by\nits estimated variance (left illustration); when the s worst points need to be\nselected (middle) or the points have to be ranked, then the threshold value(s)\nlie halfway between two estimates.\nShows the threshold for values for the three cases discussed in\nleast that their correlation matrix has special properties like\nsphericity. Unfortunately, these assumptions are not met in\nour case. Moreover, determining the necesarry correlations\nbetween estimates slows down the sampling process consid-\nerably.\nWe therefore propose to use a rough lower bound for the\nconfidence Cw. Though it is a quite conservative bound, the\nsampling speed is in a much minor degree affected compared\nto tighter approximations which consider the correlation. The\nidea is again to determine a threshold to which the estimates\nare compared.\nLet vi = Ik\ndifferent individuals ai \u2208 A with 1 \u2264 i \u2264 |A| and \u02c6 vi\nthe sampled estimates thereof. Let \u02c6 v1 < \u02c6 v2\u00b7\u00b7\u00b7 <\u02c6I|A|. The\nconfidence Cw of correctly selecting the worst individuals is\nP(?|A|\ndetermine a lower bound for Cw. Firstly, we calculate the\nconfidence interval of the worst estimate \u02c6 v1according to (31)\nat the level (L + 1)\/2, where L denotes the user defined\nconfidence level for the selection problem. Let the threshold\nB then be the upper endpoint of this interval. Hence, v1will\nbe smaller than B approximately with probability (L + 1)\/2\ngiven \u02c6 v1.\nFor the remaining estimates \u02c6 viwith i > 1 we then compute\nthe probability P(vi \u2265 B|\u02c6 vi), that their exact value vi is\nbigger than the threshold B by using once again the normal\napproximation (shown as arrows in Fig. 15). Clearly, the\nbigger \u02c6 vi, the bigger the probability becomes.\nBased on L and P(vi \u2265 B|\u02c6 vi,i \u2265 2), the probability\nof obtaining a correct selection outcome is then calculated\nanalogically to (32) (all following probabilities are under\nthe condition of known \u02c6 vi, which is omitted to facilitate\nreadability):\nh(ai,A,R) denote the indicator values for\ni=2vi> v1|\u02c6 v1,..., \u02c6 v|A|).\nLeft hand side of Figure 15 illustrates the procedure to\nCw= P\n\u239b\n\u239d\n\u239b\n|A|\n?\ni=2\nvi> v1\n\u239e\n\u23a0\n(34)\n\u2265 P(v1< B) \u00b7 P(v2\u2265 B|v1< B)\u00b7\u00b7\u00b7\n(35)\nP\n\u239dv|A|\u2265 B|v1< B,\n|A|\u22121\n?\ni=2\nvi\u2265 B\n\u239e\n\u23a0\n(36)\nwhich in turn can be lower bounded by\nCw\u2265 1 \u2212 (1 \u2212 L)\/2 \u2212\n|A|\n?\ni=2\nP(vi> B)\n(37)\nIn the latter approximation, all factors P(vi > B) can be\ndetermined using the normal approximation and the variance\nderived in (28), not involving any correlation between different\nestimates.\nb) Selecting the s Worst Individuals: Let again \u02c6 v1 <\n\u02c6 v2 \u2264 \u00b7\u00b7\u00b7 \u2264 \u02c6 v|A|denote the estimates of vi = Ik\nwith ai\u2208 A. In this section we consider the problem of not\nonly removing the worst point, but the set\u02c6Aw= {a1,...,as}\nof those s individuals with the worst estimated indicator\nvalues. On this problem, the confidence Csshall be determined\nthat the set \u02c6Aw corresponds to the set Aw resulting form\nranking the individuals according to the exact indicator values\nvi. Hence, Csis equal to the probability P(\u02c6Aw= Aw).\nThe middle part of Figure 15 illustrates how a lower bound\nfor the probability can be obtained. This time, the threshold\nB is set to lie between the best estimate still to be removed \u02c6 vs\nand the worst estimate which remains in the set \u02c6 vs+1. Hence,\nB = (\u02c6 vs+1+ \u02c6 vs)\/2. A lower bound for the confidence of\ncorrectly selecting the set Aw is obtained by multiplying the\nprobabilities, that vi is smaller than B for all individuals of\n\u02c6 Awand bigger than B for all ai\u2208 A\\\u02c6 Aw:\nCs= Pc(\u02c6Aw= Aw)\n?\n|A|\n?\nwhich can be further simplified by the lower bounded\nh(ai,A,R),\n(38)\n\u2265\ns?\ni=1\nPvi< B|\ni\u22121\n?\nj=1\nvj< B\n?\n\u00b7\n(39)\ni=s+1\nP\n?\nvi> B|\ns?\nj=1\nvj< B,\ni\u22121\n?\nk=1\nvk> B\n?\n(40)\n\u2265 1 \u2212\ns\n?\ni=1\nP(vi> B) \u2212\n|A|\n?\ni=s+1\nP(vi< B)\n(41)\nc) Ranking Individuals: Given a ranking of estimated in-\ndicator values \u02c6 v1< \u02c6 v2\u00b7\u00b7\u00b7 < \u02c6 v|A|, where \u02c6 vi:=\u02c6Ik\nbefore. We are interested in finding the confidence Cr, that this\nranking represents the ranking of the exact values vi. To this\nend, we introduce multiple thresholds Biwith 1 \u2264 i \u2264 |A|\u22121\nbetween each pair \u02c6 vi and \u02c6 vi+1 of consecutive estimates (see\nright hand side of Figure 15). Hence, Bi= (\u02c6 vi+ \u02c6 vi+1)\/2.\nh(ai,A,R) as\n21"},{"page":22,"text":"Using Bi, a lower bound is\nCr= P(\u2200i \u2264 j : vi< vj)\n(42)\n\u2265 P(v1< B1)\n\u00b7 P(v|A|> B|A|\u22121)\nwhich is again lower bounded by\n|A|\u22121\n?\ni=2\n(P(vi> Bi) \u00b7 P(vi< Bi+1)) (43)\n(44)\n\u2265 1 \u2212\n|A|\u22121\n?\ni=1\nP(vi< Bi) \u2212\n|A|\n?\ni=2\nP(vi> Bi\u22121)\n(45)\n22"},{"page":23,"text":"Table V: Comparison of HypE to different MOEAs with respect to the\nhypervolume indicator. The first number represents the performance\nscore P, which stands for the number of participants significantly\ndominating the selected algorithm. The number in brackets denote the\nhypervolume value, normalized to the minimum and maximum value\nobserved on the test problem.\nProblem SHVIBEANSGA-IIRSSPEA2HypEHypE*\n2 objectives\nDTLZ 1\nDTLZ 2\nDTLZ 3\nDTLZ 4\nDTLZ 5\nDTLZ 6\nDTLZ 7\nKnapsack\nWFG 1\nWFG 2\nWFG 3\nWFG 4\nWFG 5\nWFG 6\nWFG 7\nWFG 8\nWFG 9\n3 (0.286)\n2 (0.438)\n6 (0.265)\n1 (0.848)\n2 (0.489)\n2 (0.670)\n0 (0.945)\n2 (0.523)\n4 (0.567)\n1 (0.987)\n2 (0.994)\n0 (0.964)\n3 (0.994)\n2 (0.945)\n3 (0.929)\n3 (0.431)\n1 (0.920)\n0 (0.667)\n0 (0.871)\n0 (0.759)\n0 (0.928)\n0 (0.931)\n0 (0.914)\n1 (0.898)\n0 (0.631)\n0 (0.949)\n4 (0.962)\n0 (0.997)\n0 (0.969)\n0 (0.997)\n0 (0.975)\n0 (0.988)\n0 (0.675)\n0 (0.939)\n2 (0.441)\n5 (0.306)\n1 (0.596)\n3 (0.732)\n5 (0.361)\n5 (0.326)\n6 (0.739)\n0 (0.603)\n1 (0.792)\n3 (0.974)\n4 (0.991)\n4 (0.891)\n5 (0.992)\n4 (0.932)\n1 (0.946)\n1 (0.536)\n4 (0.891)\n3 (0.306)\n5 (0.278)\n3 (0.452)\n3 (0.834)\n6 (0.279)\n4 (0.388)\n2 (0.818)\n3 (0.493)\n6 (0.160)\n6 (0.702)\n6 (0.559)\n6 (0.314)\n6 (0.402)\n6 (0.418)\n6 (0.294)\n3 (0.367)\n6 (0.313)\n3 (0.343)\n2 (0.431)\n1 (0.578)\n2 (0.769)\n2 (0.463)\n6 (0.229)\n4 (0.817)\n0 (0.574)\n1 (0.776)\n4 (0.969)\n4 (0.990)\n4 (0.898)\n2 (0.995)\n4 (0.930)\n2 (0.939)\n1 (0.514)\n4 (0.878)\n1 (0.545)\n1 (0.682)\n3 (0.454)\n1 (0.779)\n1 (0.724)\n1 (0.856)\n2 (0.853)\n0 (0.633)\n2 (0.744)\n0 (0.990)\n0 (0.997)\n0 (0.968)\n0 (0.998)\n1 (0.955)\n1 (0.947)\n0 (0.683)\n1 (0.924)\n3 (0.279)\n4 (0.362)\n2 (0.483)\n3 (0.711)\n4 (0.428)\n2 (0.659)\n1 (0.876)\n0 (0.630)\n4 (0.557)\n0 (0.989)\n2 (0.994)\n0 (0.963)\n2 (0.995)\n2 (0.942)\n4 (0.920)\n1 (0.549)\n0 (0.931)\n3 objectives\nDTLZ 1\nDTLZ 2\nDTLZ 3\nDTLZ 4\nDTLZ 5\nDTLZ 6\nDTLZ 7\nKnapsack\nWFG 1\nWFG 2\nWFG 3\nWFG 4\nWFG 5\nWFG 6\nWFG 7\nWFG 8\nWFG 9\n3 (0.313)\n2 (0.995)\n3 (0.210)\n1 (0.945)\n1 (0.991)\n2 (0.971)\n0 (0.993)\n2 (0.441)\n4 (0.792)\n0 (0.556)\n2 (0.995)\n0 (0.978)\n2 (0.988)\n2 (0.959)\n1 (0.965)\n2 (0.887)\n1 (0.954)\n1 (0.505)\n0 (0.998)\n1 (0.495)\n0 (0.989)\n0 (0.994)\n0 (0.990)\n1 (0.987)\n0 (0.544)\n3 (0.811)\n3 (0.475)\n3 (0.981)\n3 (0.955)\n3 (0.952)\n2 (0.955)\n3 (0.950)\n0 (0.922)\n3 (0.914)\n6 (0.168)\n5 (0.683)\n3 (0.179)\n3 (0.777)\n5 (0.696)\n6 (0.151)\n6 (0.633)\n1 (0.462)\n3 (0.827)\n3 (0.406)\n4 (0.966)\n5 (0.708)\n4 (0.884)\n4 (0.914)\n5 (0.770)\n4 (0.842)\n5 (0.735)\n0 (0.607)\n6 (0.491)\n0 (0.679)\n3 (0.774)\n6 (0.374)\n5 (0.237)\n4 (0.794)\n6 (0.322)\n6 (0.207)\n6 (0.261)\n6 (0.689)\n6 (0.220)\n6 (0.343)\n6 (0.415)\n6 (0.183)\n6 (0.301)\n6 (0.283)\n5 (0.275)\n4 (0.888)\n3 (0.216)\n2 (0.860)\n4 (0.882)\n4 (0.266)\n5 (0.722)\n1 (0.441)\n1 (0.881)\n2 (0.441)\n4 (0.966)\n4 (0.740)\n5 (0.877)\n5 (0.879)\n4 (0.858)\n5 (0.780)\n4 (0.766)\n1 (0.395)\n1 (0.996)\n2 (0.398)\n0 (0.987)\n2 (0.990)\n0 (0.991)\n3 (0.970)\n0 (0.550)\n0 (0.985)\n0 (0.446)\n0 (0.999)\n1 (0.975)\n0 (0.991)\n0 (0.987)\n0 (0.988)\n0 (0.906)\n0 (0.972)\ncontinued on next page\n3 (0.336)\n3 (0.994)\n3 (0.196)\n2 (0.922)\n3 (0.989)\n3 (0.967)\n2 (0.980)\n0 (0.473)\n1 (0.894)\n0 (0.372)\n1 (0.998)\n0 (0.979)\n0 (0.991)\n1 (0.981)\n2 (0.958)\n3 (0.870)\n1 (0.956)\n23"},{"page":24,"text":"continued from previous page\nProblem SHVIBEA NSGA-IIRS SPEA2HypEHypE*\n5 objectives\nDTLZ 1\nDTLZ 2\nDTLZ 3\nDTLZ 4\nDTLZ 5\nDTLZ 6\nDTLZ 7\nKnapsack\nWFG 1\nWFG 2\nWFG 3\nWFG 4\nWFG 5\nWFG 6\nWFG 7\nWFG 8\nWFG 9\n2 (0.927)\n1 (0.998)\n2 (0.754)\n1 (0.997)\n0 (0.997)\n3 (0.964)\n0 (0.988)\n0 (0.676)\n4 (0.766)\n0 (0.671)\n6 (0.339)\n0 (0.965)\n5 (0.754)\n0 (0.953)\n0 (0.921)\n0 (0.847)\n5 (0.496)\n3 (0.905)\n0 (0.999)\n1 (0.786)\n0 (0.998)\n0 (0.998)\n1 (0.979)\n0 (0.986)\n0 (0.862)\n5 (0.703)\n0 (0.533)\n0 (0.974)\n3 (0.894)\n1 (0.971)\n0 (0.949)\n1 (0.822)\n0 (0.856)\n2 (0.720)\n5 (0.831)\n4 (0.808)\n6 (0.365)\n4 (0.749)\n4 (0.854)\n5 (0.428)\n6 (0.478)\n2 (0.163)\n2 (0.832)\n0 (0.644)\n3 (0.946)\n5 (0.711)\n4 (0.892)\n4 (0.913)\n2 (0.774)\n4 (0.685)\n4 (0.645)\n6 (0.548)\n6 (0.324)\n4 (0.529)\n5 (0.558)\n6 (0.403)\n6 (0.311)\n4 (0.672)\n2 (0.235)\n6 (0.291)\n6 (0.351)\n5 (0.760)\n6 (0.241)\n6 (0.303)\n6 (0.392)\n6 (0.157)\n6 (0.309)\n6 (0.138)\n4 (0.869)\n5 (0.795)\n4 (0.520)\n6 (0.537)\n5 (0.841)\n4 (0.597)\n5 (0.569)\n1 (0.369)\n2 (0.820)\n0 (0.624)\n4 (0.932)\n4 (0.741)\n3 (0.911)\n5 (0.872)\n4 (0.745)\n5 (0.588)\n3 (0.667)\n0 (0.968)\n2 (0.998)\n0 (0.824)\n2 (0.992)\n2 (0.996)\n0 (0.988)\n2 (0.868)\n2 (0.242)\n0 (0.973)\n0 (0.557)\n0 (0.977)\n1 (0.948)\n0 (0.978)\n1 (0.948)\n2 (0.784)\n2 (0.825)\n0 (0.937)\n1 (0.961)\n3 (0.998)\n1 (0.768)\n2 (0.992)\n2 (0.995)\n1 (0.977)\n2 (0.862)\n2 (0.256)\n1 (0.951)\n3 (0.503)\n0 (0.971)\n1 (0.949)\n1 (0.975)\n2 (0.940)\n5 (0.700)\n3 (0.809)\n0 (0.956)\n7 objectives\nDTLZ 1\nDTLZ 2\nDTLZ 3\nDTLZ 4\nDTLZ 5\nDTLZ 6\nDTLZ 7\nKnapsack\nWFG 1\nWFG 2\nWFG 3\nWFG 4\nWFG 5\nWFG 6\nWFG 7\nWFG 8\nWFG 9\n2 (0.962)\n3 (0.998)\n1 (0.951)\n1 (0.999)\n1 (0.997)\n3 (0.954)\n0 (0.981)\n0 (0.745)\n4 (0.647)\n0 (0.632)\n6 (0.105)\n3 (0.888)\n6 (0.042)\n0 (0.978)\n1 (0.688)\n0 (0.933)\n5 (0.385)\n2 (0.960)\n0 (1.000)\n1 (0.958)\n0 (1.000)\n0 (0.997)\n2 (0.983)\n1 (0.958)\n0 (0.768)\n5 (0.649)\n0 (0.747)\n2 (0.975)\n2 (0.919)\n2 (0.982)\n0 (0.967)\n3 (0.657)\n1 (0.905)\n2 (0.681)\n5 (0.950)\n5 (0.808)\n5 (0.589)\n4 (0.902)\n4 (0.888)\n5 (0.635)\n5 (0.348)\n2 (0.235)\n2 (0.814)\n1 (0.409)\n3 (0.961)\n4 (0.688)\n4 (0.905)\n4 (0.940)\n0 (0.813)\n4 (0.709)\n3 (0.679)\n6 (0.563)\n6 (0.340)\n6 (0.438)\n6 (0.569)\n6 (0.502)\n6 (0.397)\n4 (0.559)\n2 (0.226)\n6 (0.189)\n5 (0.155)\n5 (0.709)\n6 (0.200)\n5 (0.406)\n6 (0.453)\n6 (0.207)\n6 (0.366)\n6 (0.119)\n2 (0.961)\n4 (0.850)\n4 (0.723)\n5 (0.814)\n4 (0.899)\n4 (0.756)\n5 (0.352)\n2 (0.272)\n2 (0.812)\n0 (0.837)\n4 (0.958)\n4 (0.694)\n3 (0.938)\n5 (0.921)\n3 (0.658)\n5 (0.537)\n3 (0.683)\n0 (0.995)\n1 (0.999)\n0 (0.973)\n2 (0.999)\n0 (0.997)\n0 (0.993)\n2 (0.877)\n2 (0.276)\n0 (0.956)\n0 (0.528)\n0 (0.983)\n0 (0.956)\n0 (0.986)\n0 (0.974)\n1 (0.713)\n2 (0.863)\n0 (0.928)\n0 (0.995)\n1 (0.999)\n1 (0.952)\n2 (0.999)\n1 (0.997)\n1 (0.988)\n2 (0.870)\n4 (0.212)\n1 (0.937)\n0 (0.630)\n0 (0.982)\n0 (0.952)\n0 (0.987)\n3 (0.967)\n5 (0.606)\n2 (0.874)\n0 (0.943)\n10 objectives\nDTLZ 1\nDTLZ 2\nDTLZ 3\nDTLZ 4\nDTLZ 5\nDTLZ 6\nDTLZ 7\nKnapsack\nWFG 1\nWFG 2\nWFG 3\nWFG 4\nWFG 5\nWFG 6\nWFG 7\nWFG 8\nWFG 9\n3 (0.981)\n3 (0.999)\n3 (0.951)\n2 (1.000)\n3 (0.951)\n4 (0.497)\n0 (0.986)\n0 (0.568)\n6 (0.402)\n0 (0.971)\n6 (0.088)\n3 (0.698)\n6 (0.014)\n3 (0.934)\n1 (0.686)\n0 (0.956)\n5 (0.222)\n5 (0.971)\n2 (1.000)\n1 (0.990)\n0 (1.000)\n0 (0.998)\n2 (0.987)\n1 (0.831)\n0 (0.529)\n4 (0.843)\n0 (0.988)\n1 (0.973)\n2 (0.896)\n2 (0.979)\n1 (0.949)\n4 (0.464)\n1 (0.903)\n3 (0.584)\n4 (0.986)\n5 (0.825)\n5 (0.676)\n4 (0.988)\n4 (0.899)\n4 (0.706)\n4 (0.137)\n2 (0.149)\n2 (0.932)\n0 (0.978)\n3 (0.947)\n3 (0.708)\n4 (0.832)\n4 (0.896)\n1 (0.604)\n4 (0.689)\n3 (0.644)\n6 (0.590)\n6 (0.290)\n6 (0.358)\n6 (0.560)\n6 (0.471)\n6 (0.276)\n6 (0.057)\n4 (0.119)\n5 (0.562)\n5 (0.020)\n5 (0.792)\n6 (0.207)\n5 (0.365)\n6 (0.449)\n6 (0.077)\n6 (0.221)\n6 (0.109)\n2 (0.990)\n4 (0.868)\n4 (0.750)\n5 (0.960)\n4 (0.892)\n3 (0.769)\n4 (0.166)\n2 (0.173)\n2 (0.937)\n2 (0.962)\n4 (0.933)\n5 (0.669)\n3 (0.913)\n5 (0.865)\n4 (0.473)\n5 (0.438)\n2 (0.676)\n0 (0.999)\n0 (1.000)\n0 (0.994)\n1 (1.000)\n0 (0.998)\n0 (0.994)\n2 (0.744)\n5 (0.068)\n0 (0.977)\n0 (0.981)\n0 (0.980)\n0 (0.950)\n0 (0.987)\n0 (0.959)\n0 (0.683)\n2 (0.883)\n1 (0.893)\ncontinued on next page\n0 (0.999)\n0 (1.000)\n1 (0.990)\n0 (1.000)\n1 (0.997)\n1 (0.992)\n1 (0.781)\n5 (0.060)\n0 (0.975)\n1 (0.966)\n1 (0.976)\n0 (0.955)\n0 (0.989)\n1 (0.949)\n3 (0.548)\n2 (0.875)\n0 (0.925)\n24"},{"page":25,"text":"continued from previous page\nProblemSHVIBEANSGA-IIRSSPEA2HypEHypE*\n25 objectives\nDTLZ 1\nDTLZ 2\nDTLZ 3\nDTLZ 4\nDTLZ 5\nDTLZ 6\nDTLZ 7\nKnapsack\nWFG 1\nWFG 2\nWFG 3\nWFG 4\nWFG 5\nWFG 6\nWFG 7\nWFG 8\nWFG 9\n4 (0.994)\n3 (0.999)\n3 (0.967)\n3 (1.000)\n5 (0.781)\n6 (0.286)\n0 (0.973)\n0 (0.000)\n6 (0.183)\n0 (0.951)\n6 (0.037)\n6 (0.063)\n6 (0.003)\n3 (0.932)\n3 (0.286)\n0 (0.924)\n5 (0.118)\n5 (0.987)\n2 (1.000)\n2 (0.999)\n2 (1.000)\n2 (0.996)\n2 (0.993)\n0 (0.966)\n4 (0.000)\n4 (0.930)\n0 (0.951)\n0 (0.983)\n2 (0.890)\n3 (0.832)\n0 (0.959)\n4 (0.183)\n0 (0.909)\n3 (0.531)\n2 (1.000)\n4 (0.965)\n4 (0.930)\n4 (1.000)\n3 (0.949)\n3 (0.957)\n3 (0.856)\n5 (0.000)\n0 (0.971)\n2 (0.935)\n3 (0.965)\n3 (0.541)\n4 (0.796)\n5 (0.913)\n2 (0.386)\n4 (0.517)\n3 (0.580)\n6 (0.657)\n6 (0.301)\n6 (0.455)\n6 (0.546)\n6 (0.457)\n5 (0.412)\n2 (0.893)\n3 (0.000)\n5 (0.815)\n6 (0.072)\n5 (0.758)\n5 (0.170)\n5 (0.227)\n6 (0.579)\n6 (0.081)\n6 (0.189)\n5 (0.133)\n3 (0.998)\n5 (0.882)\n5 (0.827)\n5 (0.991)\n4 (0.808)\n4 (0.830)\n4 (0.671)\n6 (0.000)\n3 (0.965)\n2 (0.933)\n3 (0.963)\n4 (0.432)\n2 (0.915)\n3 (0.926)\n4 (0.185)\n5 (0.305)\n2 (0.681)\n0 (1.000)\n0 (1.000)\n0 (0.999)\n0 (1.000)\n0 (0.999)\n0 (0.999)\n2 (0.889)\n1 (0.000)\n0 (0.972)\n2 (0.934)\n1 (0.974)\n0 (0.941)\n0 (0.989)\n0 (0.961)\n0 (0.707)\n2 (0.817)\n0 (0.893)\n0 (1.000)\n0 (1.000)\n0 (1.000)\n0 (1.000)\n1 (0.999)\n0 (0.998)\n3 (0.825)\n2 (0.000)\n0 (0.973)\n2 (0.928)\n1 (0.977)\n0 (0.945)\n0 (0.989)\n0 (0.962)\n1 (0.479)\n3 (0.792)\n1 (0.848)\n50 objectives\nDTLZ 1\nDTLZ 2\nDTLZ 3\nDTLZ 4\nDTLZ 5\nDTLZ 6\nDTLZ 7\nKnapsack\nWFG 1\nWFG 2\nWFG 3\nWFG 4\nWFG 5\nWFG 6\nWFG 7\nWFG 8\nWFG 9\n4 (0.992)\n3 (1.000)\n3 (0.984)\n2 (1.000)\n5 (0.477)\n6 (0.112)\n1 (0.767)\n0 (0.000)\n6 (0.210)\n3 (0.538)\n6 (0.059)\n6 (0.011)\n6 (0.003)\n4 (0.933)\n1 (0.312)\n1 (0.669)\n5 (0.250)\n5 (0.985)\n2 (1.000)\n2 (1.000)\n2 (1.000)\n2 (0.996)\n2 (0.995)\n0 (0.966)\n4 (0.000)\n4 (0.869)\n0 (0.962)\n0 (0.981)\n2 (0.783)\n2 (0.940)\n2 (0.963)\n5 (0.026)\n0 (0.913)\n3 (0.597)\n2 (1.000)\n4 (0.998)\n3 (0.988)\n4 (1.000)\n3 (0.954)\n3 (0.979)\n5 (0.233)\n5 (0.000)\n2 (0.962)\n0 (0.959)\n2 (0.972)\n3 (0.268)\n4 (0.789)\n4 (0.941)\n3 (0.208)\n4 (0.341)\n3 (0.559)\n6 (0.566)\n6 (0.375)\n6 (0.518)\n6 (0.517)\n5 (0.425)\n5 (0.399)\n4 (0.254)\n3 (0.000)\n4 (0.823)\n6 (0.076)\n5 (0.731)\n5 (0.118)\n5 (0.416)\n6 (0.663)\n5 (0.022)\n6 (0.147)\n6 (0.166)\n3 (0.999)\n5 (0.917)\n5 (0.891)\n5 (0.999)\n4 (0.752)\n4 (0.839)\n6 (0.020)\n6 (0.000)\n2 (0.961)\n0 (0.952)\n2 (0.973)\n3 (0.258)\n3 (0.913)\n2 (0.961)\n4 (0.034)\n5 (0.233)\n2 (0.727)\n1 (1.000)\n0 (1.000)\n0 (1.000)\n0 (1.000)\n0 (0.999)\n0 (0.998)\n2 (0.684)\n1 (0.000)\n0 (0.971)\n2 (0.945)\n0 (0.976)\n0 (0.944)\n1 (0.987)\n0 (0.974)\n0 (0.581)\n1 (0.602)\n0 (0.907)\n0 (1.000)\n0 (1.000)\n0 (1.000)\n0 (1.000)\n0 (0.999)\n1 (0.998)\n3 (0.675)\n2 (0.000)\n0 (0.970)\n3 (0.943)\n0 (0.979)\n1 (0.908)\n0 (0.989)\n0 (0.976)\n1 (0.378)\n2 (0.579)\n0 (0.903)\n25"}],"widgetId":"rgw25_56ab2027499d2"},"id":"rgw25_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=45280500&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw26_56ab2027499d2"},"id":"rgw26_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=45280500&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":45280500,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":{"data":{"hasItems":true,"items":[{"data":{"now":"Fri, 29 Jan 2016 08:17:43 +0000","isLoggedIn":false,"isHidden":false,"postType":"question","attachmentsList":null,"postId":"547b48e6d685cc63588b46b7","type":"question","title":"What are the most interesting non-metaphor-based metaheuristics out there?","text":"<noscript><\/noscript><p>We are all familiar with the proliferation of metaphor-based metaheuristics.\u00a0 From the stalwarts of Simulated Annealing, Genetic Algorithms, Ant Colony Optimization, and Particle Swarm Optimization, we have many others such as Harmony Search, Imperialist Competitive Algorithm, and Fireworks Algorithms.\u00a0 It is not even certain that some of these metaphors (e.g. fireworks) perform optimization in the real-world.<\/p>\n<p>More important than mimicking a metaphor, our goal is to perform optimization.\u00a0 Focusing on this task, non-metaphor-based techniques such as Tabu Search and Estimation of Distribution Algorithms have been developed to address specific aspects of search spaces.<\/p>\n<p>Can anyone tell me more about other interesting non-metaphor-based metaheuristics, and the specific optimization task that they are designed to address?<\/p>","creationDate":"Sun, 30 Nov 2014 16:42:14 +0000","modificationDate":"Sun, 30 Nov 2014 16:42:14 +0000","hasAuthor":true,"authorName":"Stephen Chen","authorUrl":"profile\/Stephen_Chen4","authorImage":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272370405474311%401441949573624_m\/Stephen_Chen4.png","postUrl":"post\/What_are_the_most_interesting_non-metaphor-based_metaheuristics_out_there","commentCount":9,"commentCountInt":9,"hasComments":true,"isNew":false,"textShortened":"<noscript><\/noscript><p>We are all familiar with the proliferation of metaphor-based metaheuristics....","isTextShortened":true,"showProfileImage":false,"showItemLabel":false,"hasComment":true,"comments":[{"data":{"now":"Fri, 29 Jan 2016 08:17:43 +0000","isLoggedIn":false,"isHidden":false,"postType":"comment","attachmentsList":{"data":{"publications":[{"type":"publication","id":1782110,"url":"publication\/1782110_Quantum_Annealing_A_New_Method_for_Minimizing_Multidimensional_Functions","html":{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":6167779,"url":"researcher\/6167779_A_B_Finnila","fullname":"A. B. Finnila","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":12219872,"url":"researcher\/12219872_M_A_Gomez","fullname":"M. A. Gomez","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":28779477,"url":"researcher\/28779477_C_Sebenik","fullname":"C. Sebenik","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":28780081,"url":"researcher\/28780081_C_Stenson","fullname":"C. Stenson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":[],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Apr 1994","journal":"Chemical Physics Letters","showEnrichedPublicationItem":false,"citationCount":159,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/1782110_Quantum_Annealing_A_New_Method_for_Minimizing_Multidimensional_Functions","usePlainButton":true,"publicationUid":1782110,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/1782110_Quantum_Annealing_A_New_Method_for_Minimizing_Multidimensional_Functions","title":"Quantum Annealing: A New Method for Minimizing Multidimensional Functions","displayTitleAsLink":true,"authors":[{"id":6167779,"url":"researcher\/6167779_A_B_Finnila","fullname":"A. B. Finnila","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":12219872,"url":"researcher\/12219872_M_A_Gomez","fullname":"M. A. Gomez","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":28779477,"url":"researcher\/28779477_C_Sebenik","fullname":"C. Sebenik","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":28780081,"url":"researcher\/28780081_C_Stenson","fullname":"C. Stenson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":6488200,"url":"researcher\/6488200_J_D_Doll","fullname":"J. D. Doll","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Chemical Physics Letters 04\/1994; 219(5-6). DOI:10.1016\/0009-2614(94)00117-0"],"abstract":"Quantum annealing is a new method for finding extrema of multidimensional functions. Based on an extension of classical, simulated annealing, this approach appears robust with respect to avoiding local minima. Further, unlike some of its predecessors, it does not require an approximation to a wavefunction. In this paper, we apply the technique to the problem of finding the lowest energy configurations of Lennard-Jones clusters of up to 19 particles (roughly 10$^5$ local minima). This early success suggests that this method may complement the widely implemented technique of simulated annealing.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/1782110_Quantum_Annealing_A_New_Method_for_Minimizing_Multidimensional_Functions","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Maria_Gomez41\/publication\/1782110_Quantum_Annealing_A_New_Method_for_Minimizing_Multidimensional_Functions\/links\/53db9d870cf216e4210bfae1.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Maria_Gomez41","sourceName":"Maria A. Gomez","hasSourceUrl":true},"publicationUid":1782110,"publicationUrl":"publication\/1782110_Quantum_Annealing_A_New_Method_for_Minimizing_Multidimensional_Functions","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/1782110_Quantum_Annealing_A_New_Method_for_Minimizing_Multidimensional_Functions\/links\/53db9d870cf216e4210bfae1\/smallpreview.png","linkId":"53db9d870cf216e4210bfae1","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=1782110&reference=53db9d870cf216e4210bfae1&eventCode=&origin=publication_list","widgetId":"rgw32_56ab2027499d2"},"id":"rgw32_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=1782110&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"53db9d870cf216e4210bfae1","context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/1782110_Quantum_Annealing_A_New_Method_for_Minimizing_Multidimensional_Functions\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw31_56ab2027499d2"},"id":"rgw31_56ab2027499d2","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?publicationUid=1782110","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}}],"hasLinks":false,"hasFiles":false,"hasPublications":true,"hasImages":false,"hasMoreAttachments":true,"amountMoreAttachments":7,"widgetId":"rgw30_56ab2027499d2"},"id":"rgw30_56ab2027499d2","partials":[],"templateName":"publictopics\/stubs\/TopicPostAttachmentsList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publictopics.TopicPostAttachmentsList.html?limit=1","viewClass":"views.publictopics.TopicPostAttachmentsListView","yuiModules":["rg.views.publictopics.TopicPostAttachmentsListView","css-modules-publictopics"],"stylesheets":["modules\/publictopics.css"],"_isYUI":true},"postId":"547ba2e7cf57d7c6078b4674","type":"comment","title":"Dear Stephen,\nThis is a very good question because it has been increasingly difficult to find novel research in metaheuristics without recurring to in...","text":"<noscript><\/noscript><p>Dear Stephen,<\/p>\n<p>This is a very good question because it has been increasingly difficult to find novel research in metaheuristics without recurring to inspirations of any kind. Attempts of unifying the various flavors of metaphor-based metaheuristics into an unique framework have\u00a0appeared e.g. in\u00a0Taillard et al. (2001).<br \/><br \/>On the other hand, I think many researchers have turned on improving metaheuristics for a vast range of domains, by integrating concepts that do not rely on metaphors.<\/p>\n<p>One example of this are indicator-based metaheuristics for Multi-Objective Optimization (MOO), whose main challenge is on designing efficient ways of measuring the quality of sets of mutually non-inferior solutions in order to approximate the Pareto Frontier. Although many indicator-based MOO algorithms are still grounded in evolutionary-inspired frameworks, the focus is entirely on how to design heuristic operators that take advantage of the information provided by such indicators. See e.g. Bader and Zitzler (2008).<\/p>\n<p>Now, to answer your question, I can point out a few interesting modern metaheuristics not entirely focused on metaphors:<\/p>\n<p>- <strong>Natural evolution strategies<\/strong>: these are methods that update candidate solutions in continous search spaces in the direction of the natural gradient (a more general gradient estimation based on differential geometry, see Amari and Douglas (1998)). A recent example can be found in Sun et al. (2009) for numerical optimization problems and a non-markovian double pole balancing application.<\/p>\n<p>- <strong>Cross entropy methods<\/strong>: given a parametric family of distributions and an adaptive threshold on the objective function values sampled so far, these methods can be viewed as the higher-level strategy of finding the distribution representing the elite solutions by minimizing the relative entropy. Rubinstein (1999) applied CE both for continuous and combinatorial optimization (TSP, shortest path). In my opinion, CE could\u00a0also be viewed as an Estimation of Distribution Algorithm (EDA).<\/p>\n<p>-\u00a0<strong>Particle Filter methods<\/strong>: these were presented as a \"reformulation of stochastic global optimization as a filtering problem\" and have been applied for\u00a0parametric density estimation. See Stinis (2009). Again, IMO, these are closely related to EDAs, although having a much more formal flavor and convergence guarantees.<\/p>\n<p>- <strong>Quantum annealing<\/strong>: these methods\u00a0can\u00a0be considered as the quantum\u00a0counterpart to simulated annealing, one of the\u00a0most popular and well-succeeded metaheuristics. Now, these are\u00a0interesting because they can be implemented in a quantum\u00a0device for\u00a0actually physically performing the steps required to achieve\u00a0a configuration of\u00a0minimal potential energy. That's the approach the canadian company D-Wave claims to have implemented by using an adiabatic quantum computation processor. See e.g.\u00a0Finnila et al. (1994).<\/p>\n<p>A very good recent survey on metaheuristics for optimization appeared in Boussa\u00efd, Lepagnot, and Siarry (2013), where\u00a0you can find several examples not mentioned in your question but that came out long ago (the following\u00a0references can be found\u00a0in the original paper):<\/p>\n<p>- <strong>Greedy Randomized Adaptive Search Procedure<\/strong> (GRASP) after Feo and Resende (1989) who designed such methods mainly for \"set covering problems that arise from Steiner triple systems\"<\/p>\n<p>- <strong>Noising Method<\/strong> (NM) after Charon and Hudry (1993) that applied it for \"the clique partitioning of a graph\"<\/p>\n<p>- <strong>Variable Neighborhood Search<\/strong> (VNS) after Hansen and Mladenovic (1997) for the traveling salesman problem<\/p>\n<p>- <strong>Guided local search<\/strong> (GLS) after C. Voudouris, E. Tsang (1999) also for TSP<\/p>\n<p>- <strong>Iterated local search<\/strong> (ITL) after St\u00fctzle (1998) for TSP, quadratic assignment problems, and flow shop problems<\/p>\n<p>- <strong>Scatter Search<\/strong> (SS) and <strong>Path Relinking<\/strong> (PR) after Glover (1997) are methods closely-related with evolutionary-inspired algorithms, the difference being that principled heuristics are provided for\u00a0combining multiple candidate solutions to produce novel ones.<br \/><br \/>Hope that helps.<\/p>","creationDate":"Sun, 30 Nov 2014 23:06:15 +0000","modificationDate":"Sun, 30 Nov 2014 23:06:15 +0000","hasAuthor":true,"authorName":"Carlos R. B. Azevedo","authorUrl":"profile\/Carlos_Azevedo2","authorImage":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272366458634266%401441948632477_m\/Carlos_Azevedo2.png","postUrl":"post\/What_are_the_most_interesting_non-metaphor-based_metaheuristics_out_there#view=547ba2e7cf57d7c6078b4674","commentCount":0,"commentCountInt":0,"hasComments":false,"isNew":false,"textShortened":"<noscript><\/noscript><p>Dear Stephen,<\/p>\n<p>This is a very good question because it has been increa...","isTextShortened":true,"widgetId":"rgw29_56ab2027499d2"},"id":"rgw29_56ab2027499d2","partials":[],"templateName":"publictopics\/stubs\/PostFeedItemComment.html","templateExtensions":["generalHelpers","datetimeHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publictopics.PostFeedItemComment.html?widgetIdAddOn=&shortenText=1","viewClass":"views.publictopics.PostFeedItemCommentView","yuiModules":["rg.views.publictopics.PostFeedItemCommentView","css-modules-publictopics"],"stylesheets":["modules\/publictopics.css"],"_isYUI":true}],"profileName":"Carlos R. B. Azevedo","profileUrl":"profile\/Carlos_Azevedo2","profileImage":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272366458634266%401441948632477_m\/Carlos_Azevedo2.png","canFollow":true,"signUpUrl":"signup.SignUp.html","voteCount":5,"hasOneVote":false,"showKeyword":true,"keyword":"Ant Colony Optimization","isApproved":true,"keywordUrl":"topic\/ant_colony_optimization","titleHighlighted":"<noscript><\/noscript>What are the most interesting non-metaphor-based metaheuristics out there?","isTitleHighlighted":true,"textHighlighted":"<noscript><\/noscript><p>We are all familiar with the proliferation of metaphor-based metaheuristics.\u00a0 From the stalwarts of Simulated Annealing, Genetic Algorithms, Ant Colony Optimization, and Particle Swarm Optimization, we have many others such as Harmony Search, Imperialist Competitive Algorithm, and Fireworks Algorithms.\u00a0 It is not even certain that some of these metaphors (e.g. fireworks) perform optimization in the real-world.<\/p>\n<p>More important than mimicking a metaphor, our goal is to perform optimization.\u00a0 Focusing on this task, non-metaphor-based techniques such as Tabu Search and Estimation of Distribution Algorithms have been developed to address specific aspects of search spaces.<\/p>\n<p>Can anyone tell me more about other interesting non-metaphor-based metaheuristics, and the specific optimization task that they are designed to address?<\/p>","isTextHighlighted":false,"followUrl":"publictopics.PostFeedItem.follow.html?ev=tp_feed_post_xflw","unfollowUrl":"publictopics.PostFeedItem.unfollow.html?ev=tp_feed_post_xflw","showComments":true,"showActions":true,"showReason":true,"displayUpdatedExpanderPlugin":false,"widgetId":"rgw28_56ab2027499d2"},"id":"rgw28_56ab2027499d2","partials":[],"templateName":"publictopics\/stubs\/PostFeedItem.html","templateExtensions":["generalHelpers","datetimeHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publictopics.PostFeedItem.html?showComments=1&showKeyword=1&specificCommentIds%5B0%5D=547ba2e7cf57d7c6078b4674&showItemLabel=0","viewClass":"views.publictopics.PostFeedItemView","yuiModules":["rg.views.publictopics.PostFeedItemView","css-modules-publictopics"],"stylesheets":["modules\/publictopics.css"],"_isYUI":true}],"hasMore":true,"totalItemsCount":5,"page":1,"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureQuestions.html?baseUrl=default&publicationUid=45280500","widgetParams":{"page":1,"baseUrl":"default"},"pager":false,"showLink":false,"publicationUid":45280500,"totalCount":0,"isLoggedIn":false,"askQuestionUrl":"topics?publicationUid=45280500&_tpectx=publication_detail&askQuestion=1","widgetId":"rgw27_56ab2027499d2"},"id":"rgw27_56ab2027499d2","partials":{"pager":"application\/stubs\/partials\/pager.html"},"templateName":"publicliterature\/stubs\/PublicLiteratureQuestions.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureQuestions.html?baseUrl=default&publicationUid=45280500","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationCitations":{"data":{"publicationUid":45280500,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2001967256,"url":"researcher\/2001967256_Hu_Zhang","fullname":"Hu Zhang","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278552525197316%401443423505819_m\/Hu_Zhang12.png"},{"id":12244323,"url":"researcher\/12244323_Aimin_Zhou","fullname":"Aimin Zhou","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":32568397,"url":"researcher\/32568397_Shenmin_Song","fullname":"Shenmin Song","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2095136196,"url":"researcher\/2095136196_Qingfu_Zhang","fullname":"Qingfu Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":2,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Jan 2016","journal":"IEEE Transactions on Evolutionary Computation","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291337172_A_Self-Organizing_Multiobjective_Evolutionary_Algorithm","usePlainButton":true,"publicationUid":291337172,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/291337172_A_Self-Organizing_Multiobjective_Evolutionary_Algorithm","title":"A Self-Organizing Multiobjective Evolutionary Algorithm","displayTitleAsLink":true,"authors":[{"id":2001967256,"url":"researcher\/2001967256_Hu_Zhang","fullname":"Hu Zhang","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278552525197316%401443423505819_m\/Hu_Zhang12.png"},{"id":12244323,"url":"researcher\/12244323_Aimin_Zhou","fullname":"Aimin Zhou","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":32568397,"url":"researcher\/32568397_Shenmin_Song","fullname":"Shenmin Song","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095136196,"url":"researcher\/2095136196_Qingfu_Zhang","fullname":"Qingfu Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8941014,"url":"researcher\/8941014_Xiao-Zhi_Gao","fullname":"Xiao-Zhi Gao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095138924,"url":"researcher\/2095138924_Jun_Zhang","fullname":"Jun Zhang","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["IEEE Transactions on Evolutionary Computation 01\/2016;  DOI:10.1109\/TEVC.2016.2521868"],"abstract":"Under mild conditions, the Pareto front (Pareto set) of a continuous m-objective optimization problem forms an (m\u22121)-dimensional piecewise continuous manifold. Based on this property, this paper proposes a self-organizing multiobjective evolutionary algorithm. At each generation, a self-organizing mapping method with (m \u2212 1) latent variables is applied to establish the neighborhood relationship among current solutions. A solution is only allowed to mate with its neighboring solutions to generate a new solution. To reduce the computational overhead, the self-organizing training step and the evolution step are conducted in an alternative manner. In other words, the self-organizing training is performed only one single step at each generation. The proposed algorithm has been applied to a number of test instances, and compared with some state-of the-art multiobjective evolutionary methods. The results have demonstrated its advantages over other approaches.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291337172_A_Self-Organizing_Multiobjective_Evolutionary_Algorithm","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Hu_Zhang12\/publication\/291337172_A_Self-Organizing_Multiobjective_Evolutionary_Algorithm\/links\/56a5ce6708ae232fb209759a.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Hu_Zhang12","sourceName":"Hu Zhang","hasSourceUrl":true},"publicationUid":291337172,"publicationUrl":"publication\/291337172_A_Self-Organizing_Multiobjective_Evolutionary_Algorithm","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/291337172_A_Self-Organizing_Multiobjective_Evolutionary_Algorithm\/links\/56a5ce6708ae232fb209759a\/smallpreview.png","linkId":"56a5ce6708ae232fb209759a","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=291337172&reference=56a5ce6708ae232fb209759a&eventCode=&origin=publication_list","widgetId":"rgw36_56ab2027499d2"},"id":"rgw36_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=291337172&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"56a5ce6708ae232fb209759a","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":45280500,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291337172_A_Self-Organizing_Multiobjective_Evolutionary_Algorithm\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Multiobjective evolutionary algorithms (MOEAs) have been accepted as a major approach for approximating the PF[2],[3], and various MOEAs are proposed[4]\u2013[6]. Three classes of widely investigated and used MOEAs are the Pareto dominance-based[7]\u2013[9], performance indicator- based[10],[11]and decomposition-based approaches[12]\u2013[15]. An efficient MOEA should make use of problem specific knowledge to guide its search. "],"widgetId":"rgw37_56ab2027499d2"},"id":"rgw37_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw35_56ab2027499d2"},"id":"rgw35_56ab2027499d2","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=291337172&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2051767641,"url":"researcher\/2051767641_Yuning_Chen","fullname":"Yuning Chen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":6546247,"url":"researcher\/6546247_Jin-Kao_Hao","fullname":"Jin-Kao Hao","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291186153_The_Bi-Objective_Quadratic_Multiple_Knapsack_Problem_Model_and_Heuristics","usePlainButton":true,"publicationUid":291186153,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/291186153_The_Bi-Objective_Quadratic_Multiple_Knapsack_Problem_Model_and_Heuristics","title":"The Bi-Objective Quadratic Multiple Knapsack Problem: Model and Heuristics","displayTitleAsLink":true,"authors":[{"id":2051767641,"url":"researcher\/2051767641_Yuning_Chen","fullname":"Yuning Chen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":6546247,"url":"researcher\/6546247_Jin-Kao_Hao","fullname":"Jin-Kao Hao","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"The single objective quadratic multiple knapsack problem (QMKP) is a useful model to formulate a number of practical problems. However, it is not suitable for situations where more than one objective needs to be considered. In this paper, we extend the single objective QMKP to the bi-objective case such that we simultaneously maximize the total profit of the items packed into the knapsacks and the \u2019makespan\u2019 (the gain of the least profit knapsack). Given the imposing computational challenge, we propose a hybrid two-stage (HTS) algorithm to approximate the Pareto front of the bi-objective QMKP. HTS combines two different and complementary search methods - scalarizing memetic search (first stage) and Pareto local search (second stage). Experimental assessments on a set of 60 problem instances show that HTS dominates a standard multi-objective evolutionary algorithm (NSGA II), and two simplified variants of HTS. We also present a comparison with two state-of-the-art algorithms for the single objective QMKP to assess the quality of the extreme solutions of the approximated Pareto front.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291186153_The_Bi-Objective_Quadratic_Multiple_Knapsack_Problem_Model_and_Heuristics","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Jin-Kao_Hao\/publication\/291186153_The_Bi-Objective_Quadratic_Multiple_Knapsack_Problem_Model_and_Heuristics\/links\/56a230a608ae2afab8867dfb.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Jin-Kao_Hao","sourceName":"Jin-Kao Hao","hasSourceUrl":true},"publicationUid":291186153,"publicationUrl":"publication\/291186153_The_Bi-Objective_Quadratic_Multiple_Knapsack_Problem_Model_and_Heuristics","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/291186153_The_Bi-Objective_Quadratic_Multiple_Knapsack_Problem_Model_and_Heuristics\/links\/56a230a608ae2afab8867dfb\/smallpreview.png","linkId":"56a230a608ae2afab8867dfb","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=291186153&reference=56a230a608ae2afab8867dfb&eventCode=&origin=publication_list","widgetId":"rgw39_56ab2027499d2"},"id":"rgw39_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=291186153&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"56a230a608ae2afab8867dfb","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":45280500,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291186153_The_Bi-Objective_Quadratic_Multiple_Knapsack_Problem_Model_and_Heuristics\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":[", where R is a reference set which is the best-identified approximation complied from the approximation sets of all tested configurations. Intuitively, this indicator measures the portion of the objective space that is dominated by R but not by A. The hypervolume indicator is one of the most commonly used measures for evaluating the multi-objective optimization algorithms, since it is the only unary measure which is consistent with the Pareto dominance relation[5], i.e., it allows to obtain a total order between approximation sets. In our experiments, these two indicators were computed based on the normalized objective vectors of the non-dominated solutions. "],"widgetId":"rgw40_56ab2027499d2"},"id":"rgw40_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw38_56ab2027499d2"},"id":"rgw38_56ab2027499d2","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=291186153&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2034216062,"url":"researcher\/2034216062_Wali_Khan_Mashwani","fullname":"Wali Khan Mashwani","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278684876460045%401443455060998_m"},{"id":2077832430,"url":"researcher\/2077832430_Abdel_Salhi","fullname":"Abdel Salhi","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Applied Soft Computing","showEnrichedPublicationItem":false,"citationCount":1,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/288827687_Multiobjective_evolutionary_algorithm_based_on_multimethod_with_dynamic_resources_allocation","usePlainButton":true,"publicationUid":288827687,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/288827687_Multiobjective_evolutionary_algorithm_based_on_multimethod_with_dynamic_resources_allocation","title":"Multiobjective evolutionary algorithm based on multimethod with dynamic resources allocation","displayTitleAsLink":true,"authors":[{"id":2034216062,"url":"researcher\/2034216062_Wali_Khan_Mashwani","fullname":"Wali Khan Mashwani","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278684876460045%401443455060998_m"},{"id":2077832430,"url":"researcher\/2077832430_Abdel_Salhi","fullname":"Abdel Salhi","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Applied Soft Computing 01\/2016; 39:292-309."],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/288827687_Multiobjective_evolutionary_algorithm_based_on_multimethod_with_dynamic_resources_allocation","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Wali_Mashwani2\/publication\/288827687_Multiobjective_evolutionary_algorithm_based_on_multimethod_with_dynamic_resources_allocation\/links\/5684c29008ae1975839389fa.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Wali_Mashwani2","sourceName":"Wali Khan Mashwani","hasSourceUrl":true},"publicationUid":288827687,"publicationUrl":"publication\/288827687_Multiobjective_evolutionary_algorithm_based_on_multimethod_with_dynamic_resources_allocation","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/288827687_Multiobjective_evolutionary_algorithm_based_on_multimethod_with_dynamic_resources_allocation\/links\/5684c29008ae1975839389fa\/smallpreview.png","linkId":"5684c29008ae1975839389fa","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=288827687&reference=5684c29008ae1975839389fa&eventCode=&origin=publication_list","widgetId":"rgw42_56ab2027499d2"},"id":"rgw42_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=288827687&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"5684c29008ae1975839389fa","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":45280500,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/288827687_Multiobjective_evolutionary_algorithm_based_on_multimethod_with_dynamic_resources_allocation\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Dur to population based nature, they can find a diversified set of solutions for the given MOPs in single simulation run unlike traditional techniques [18] [9] [19] [11] [20]. In general, classical MOEAs can be divided into three main different classes, namely, the Pareto dominance based MOEAs (e.g., [21] [22] [23] [24] [25] [26] [27] [28] [29]), the decomposition based MOEAs (e.g., [30\u201340,19,41,42]), and Indicator Based algorithms (e.g., [43] [44] [45] [46] [47] [48] [49]). Among above mentioned three classes, Pareto dominance based MOEAs are very commonly used in the existing specialized literature of EC. "],"widgetId":"rgw43_56ab2027499d2"},"id":"rgw43_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw41_56ab2027499d2"},"id":"rgw41_56ab2027499d2","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=288827687&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":45280500,"publicationLink":"publication\/45280500_HypE_An_Algorithm_for_Fast_Hypervolume-Based_Many-Objective_Optimization","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw34_56ab2027499d2"},"id":"rgw34_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=45280500&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=244","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":244,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw33_56ab2027499d2"},"id":"rgw33_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=45280500&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/45280500_HypE_An_Algorithm_for_Fast_Hypervolume-Based_Many-Objective_Optimization","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab2027499d2"},"id":"rgw2_56ab2027499d2","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":45280500},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=45280500&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab2027499d2"},"id":"rgw1_56ab2027499d2","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"O6k4fS3u6bbAwQ0iF3BlsV1PLI3m3zVG1q2IzMc6JryuMN7JyY4p\/MeN1\/o4UkULp0PM+vrdOkdF9ZPpTLLFTUHHhyHyl6L2nWwR8H62ZJ0mJsdWn6OLXkwBamnTOHPSdKXFmrJKHL\/q\/DnTJtsp4WltLY8JBK+kNGXbte7r\/hQo7U6+Xnv5GnnKOXmTxc0RX5\/sOTyh4GmfQY6S94BqOQpafXGzl4O+tmTJEqv+D5jeEM1y+syCW7\/oYF2LBiTQJRhuUn5ruqNCi9idLRes5DCc4pAht4pF8U489zMzvYs=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/45280500_HypE_An_Algorithm_for_Fast_Hypervolume-Based_Many-Objective_Optimization\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization\" \/>\n<meta property=\"og:description\" content=\"In the field of evolutionary multi-criterion optimization, the hypervolume indicator is the only single set quality measure that is known to be strictly monotonic with regard to Pareto dominance:...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/45280500_HypE_An_Algorithm_for_Fast_Hypervolume-Based_Many-Objective_Optimization\/links\/0e60a57ef0c4cf5df7c57d44\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/45280500_HypE_An_Algorithm_for_Fast_Hypervolume-Based_Many-Objective_Optimization\" \/>\n<meta property=\"rg:id\" content=\"PB:45280500\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1162\/EVCO_a_00009\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"HypE: An Algorithm for Fast Hypervolume-Based Many-Objective Optimization\" \/>\n<meta name=\"citation_author\" content=\"Johannes Bader\" \/>\n<meta name=\"citation_author\" content=\"Eckart Zitzler\" \/>\n<meta name=\"citation_pmid\" content=\"20649424\" \/>\n<meta name=\"citation_publication_date\" content=\"2011\/03\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"Evolutionary Computation\" \/>\n<meta name=\"citation_issn\" content=\"1530-9304\" \/>\n<meta name=\"citation_volume\" content=\"19\" \/>\n<meta name=\"citation_issue\" content=\"1\" \/>\n<meta name=\"citation_firstpage\" content=\"45\" \/>\n<meta name=\"citation_lastpage\" content=\"76\" \/>\n<meta name=\"citation_doi\" content=\"10.1162\/EVCO_a_00009\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/45280500_HypE_An_Algorithm_for_Fast_Hypervolume-Based_Many-Objective_Optimization\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/45280500_HypE_An_Algorithm_for_Fast_Hypervolume-Based_Many-Objective_Optimization\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/212307534887204\/styles\/modules\/publictopics.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-b4d1d89e-970d-4b58-a442-f95cb0c7dd9b","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":602,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw44_56ab2027499d2"},"id":"rgw44_56ab2027499d2","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-b4d1d89e-970d-4b58-a442-f95cb0c7dd9b", "63788811b8917f814cef0fc6d40a65ef83b40226");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-b4d1d89e-970d-4b58-a442-f95cb0c7dd9b", "63788811b8917f814cef0fc6d40a65ef83b40226");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw45_56ab2027499d2"},"id":"rgw45_56ab2027499d2","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/45280500_HypE_An_Algorithm_for_Fast_Hypervolume-Based_Many-Objective_Optimization","requestToken":"xL4\/318ENSaFCDabUHTYQ1LB0ewQdJzj5bQDsG0sA+TYGzruZS686HGDMWOjEfpnITcS3G3s9gj\/STl+cL+gzNO3K68RilAEeAgrFJNhHee80d876C0A7EqeeYB\/a0anIPPFpEn+yWeD7xhay\/QkA2hPTezFEGhPBTrN5iu6NWPZPT9kkm0iY9mr+gsTRagg\/JgwynjZxbo3+8s16RCb7JdT7MwjoUTCe8Zk6vLQFMXClvoOEhi0kZooHRJv2Z\/DoHGPlsj+o8WXvBmSvk6\/7a3M85yH8wUvThY5JyU\/XCc=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=ZMWlIXLiT4XS2uju8zPqQuXszS5PiEmviHY11XfctOEmzmopm5bA-B0bcttQ73eT","encodedUrlAfterLogin":"cHVibGljYXRpb24vNDUyODA1MDBfSHlwRV9Bbl9BbGdvcml0aG1fZm9yX0Zhc3RfSHlwZXJ2b2x1bWUtQmFzZWRfTWFueS1PYmplY3RpdmVfT3B0aW1pemF0aW9u","signupCallToAction":"Join for free","widgetId":"rgw47_56ab2027499d2"},"id":"rgw47_56ab2027499d2","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw46_56ab2027499d2"},"id":"rgw46_56ab2027499d2","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw48_56ab2027499d2"},"id":"rgw48_56ab2027499d2","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
