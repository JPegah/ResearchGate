<!DOCTYPE html> <html lang="en" class="" id="rgw38_56ab1e628809b"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="nc1v+q2WXctKFFkT2MhnCsJ2zebIoMD2vjmFPSpl06EDttzYzfz2pNOrro/c2YEtr9zRw95uFeHKqfErB3dQjHtBCjpQnQAIekDzU20+vpjP5btaCwlIGM0XlpnH2rQqIEWJeCIqxRNvbLgwBDaYuLK/u2tiPv/RMmyPnUxzvd3SO3aINiY4ld7SJwXDAhWiTjlcFRmBDeGZC+DdKlSjxhiKGMqbYXU4Wedm9I8tcbZaIyBXF4zGiLoyraNpkHYqKTvnwx0bkRodLXutwQsL4bVu1xePZVOGuYRya8G2PQ8="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-c3c669c2-24fa-4bb8-b98e-c8d71a067f3c",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/3192933_Bayesian_classification_with_Gaussian_processes" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Bayesian classification with Gaussian processes" />
<meta property="og:description" content="We consider the problem of assigning an input vector to one of m
classes by predicting P(c|x) for c=1,...,m. For a two-class problem, the
probability of class one given x is estimated by σ(y(x)),..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/3192933_Bayesian_classification_with_Gaussian_processes/links/0e5fab2ef0c41c4932e30cc9/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/3192933_Bayesian_classification_with_Gaussian_processes" />
<meta property="rg:id" content="PB:3192933" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1109/34.735807" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Bayesian classification with Gaussian processes" />
<meta name="citation_author" content="C.K.I. Williams" />
<meta name="citation_author" content="David Barber" />
<meta name="citation_publication_date" content="1999/01/01" />
<meta name="citation_journal_title" content="IEEE Transactions on Pattern Analysis and Machine Intelligence" />
<meta name="citation_issn" content="0162-8828" />
<meta name="citation_volume" content="20" />
<meta name="citation_issue" content="12" />
<meta name="citation_firstpage" content="1342" />
<meta name="citation_lastpage" content="1351" />
<meta name="citation_doi" content="10.1109/34.735807" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/3192933_Bayesian_classification_with_Gaussian_processes" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/3192933_Bayesian_classification_with_Gaussian_processes" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Bayesian classification with Gaussian processes</title>
<meta name="description" content="Bayesian classification with Gaussian processes on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1e628809b" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1e628809b" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab1e628809b">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1109%2F34.735807&rft.atitle=Bayesian%20classification%20with%20Gaussian%20processes&rft.title=Pattern%20Analysis%20and%20Machine%20Intelligence%2C%20IEEE%20Transactions%20on&rft.jtitle=Pattern%20Analysis%20and%20Machine%20Intelligence%2C%20IEEE%20Transactions%20on&rft.volume=20&rft.issue=12&rft.date=1999&rft.pages=1342%20-%201351&rft.issn=0162-8828&rft.au=C.K.I.%20Williams%2CDavid%20Barber&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Bayesian classification with Gaussian processes</h1> <meta itemprop="headline" content="Bayesian classification with Gaussian processes">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/3192933_Bayesian_classification_with_Gaussian_processes/links/0e5fab2ef0c41c4932e30cc9/smallpreview.png">  <div id="rgw8_56ab1e628809b" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56ab1e628809b"> <a href="researcher/6808508_CKI_Williams" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="C.K.I. Williams" alt="C.K.I. Williams" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">C.K.I. Williams</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw10_56ab1e628809b">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/6808508_CKI_Williams"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="C.K.I. Williams" alt="C.K.I. Williams" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/6808508_CKI_Williams" class="display-name">C.K.I. Williams</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56ab1e628809b" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/David_Barber5" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272799684100117%401442051921461_m" title="David Barber" alt="David Barber" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">David Barber</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw12_56ab1e628809b" data-account-key="David_Barber5">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/David_Barber5"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272799684100117%401442051921461_l" title="David Barber" alt="David Barber" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/David_Barber5" class="display-name">David Barber</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_College_London" title="University College London">University College London</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">  <div> Dept. of Artificial Intelligence, Edinburgh Univ. </div>      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/0162-8828_IEEE_Transactions_on_Pattern_Analysis_and_Machine_Intelligence"><span itemprop="name">IEEE Transactions on Pattern Analysis and Machine Intelligence</span></a> </span>    (Impact Factor: 5.78).     <meta itemprop="datePublished" content="1999-01">  01/1999;  20(12):1342 - 1351.    DOI:&nbsp;10.1109/34.735807           <div class="pub-source"> Source: <a href="http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=735807" rel="nofollow">IEEE Xplore</a> </div>  </div> <div id="rgw13_56ab1e628809b" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>We consider the problem of assigning an input vector to one of m<br />
classes by predicting P(c|x) for c=1,...,m. For a two-class problem, the<br />
probability of class one given x is estimated by &sigma;(y(x)), where<br />
&sigma;(y)=1/(1+e<sup>-y</sup>). A Gaussian process prior is placed on<br />
y(x), and is combined with the training data to obtain predictions for<br />
new x points. We provide a Bayesian treatment, integrating over<br />
uncertainty in y and in the parameters that control the Gaussian process<br />
prior the necessary integration over y is carried out using Laplace's<br />
approximation. The method is generalized to multiclass problems (m&gt;2)<br />
using the softmax function. We demonstrate the effectiveness of the<br />
method on a number of datasets</div> </p>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw26_56ab1e628809b">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw25_56ab1e628809b"  itemprop="articleBody">  <p>Page 1</p> <p>1342IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  20,  NO.  12,  DECEMBER  1998<br />Bayesian Classification<br />With Gaussian Processes<br />Christopher K.I. Williams, Member, IEEE Computer Society, and David Barber<br />Abstract—We consider the problem of assigning an input vector to one of m classes by predicting P(c|x) for c = 1, º, m. For a two-<br />class problem, the probability of class one given x is estimated by s(y(x)), where s(y) = 1/(1 + e-y). A Gaussian process prior is<br />placed on y(x), and is combined with the training data to obtain predictions for new x points. We provide a Bayesian treatment,<br />integrating over uncertainty in y and in the parameters that control the Gaussian process prior; the necessary integration over y is<br />carried out using Laplace’s approximation. The method is generalized to multiclass problems (m &gt; 2) using the softmax function. We<br />demonstrate the effectiveness of the method on a number of datasets.<br />Index Terms—Gaussian processes, classification problems, parameter uncertainty, Markov chain Monte Carlo, hybrid Monte Carlo,<br />Bayesian classification.<br />——————————???????——————————<br />1 INTRODUCTION<br />W<br />E consider the problem of assigning an input vector x<br />to one out of m classes by predicting P(c|x) for c = 1,<br />º, m. A classic example of this method is logistic regres-<br />sion. For a two-class problem, the probability of class 1<br />given x is estimated by s(wTx + b), where s(y) = 1/ (1 + e-y).<br />However, this method is not at all “flexible,” i.e., the dis-<br />criminant surface is simply a hyperplane in x-space. This<br />problem can be overcome, to some extent, by expanding the<br />input x into a set of basis functions {f(x)}, for example<br />quadratic functions of the components of x. For a high-<br />dimensional input space, there will be a large number of<br />basis functions, each one with an associated parameter, and<br />one risks “overfitting” the training data. This motivates a<br />Bayesian treatment of the problem, where the priors on the<br />parameters encourage smoothness in the model.<br />Putting priors on the parameters of the basis functions<br />indirectly induces priors over the functions that can be<br />produced by the model. However, it is possible (and we<br />would argue, perhaps more natural) to put priors directly<br />over the functions themselves. One advantage of function-<br />space priors is that they can impose a general smoothness<br />constraint without being tied to a limited number of basis<br />functions. In the regression case where the task is to predict<br />a real-valued output, it is possible to carry out nonparametric<br />regression using Gaussian Processes (GPs); see, e.g., [25],<br />[28]. The solution for the regression problem under a GP<br />prior (and Gaussian noise model) is to place a kernel func-<br />tion on each training data point, with coefficients deter-<br />mined by solving a linear system. If the parameters ? that<br />describe the Gaussian process are unknown, Bayesian in-<br />ference can be carried out for them, as described in [28].<br />The Gaussian Process method can be extended to classi-<br />fication problems by defining a GP over y, the input to the<br />sigmoid function. This idea has been used by a number of<br />authors, although previous treatments typically do not take<br />a fully Bayesian approach, ignoring uncertainty in both the<br />posterior distribution of y given the data, and uncertainty<br />in the parameters ?. This paper attempts a fully Bayesian<br />treatment of the problem, and also introduces a particular<br />form of covariance function for the Gaussian process prior<br />which, we believe, is useful from a modeling point of view.<br />The structure of the remainder of the paper is as follows:<br />Section 2, discusses the use of Gaussian processes for re-<br />gression problems, as this is essential background for the<br />classification case. In Section 3, we describe the application<br />of Gaussian processes to two-class classification problems,<br />and extend this to multiple-class problems in Section 4. Ex-<br />perimental results are presented in Section 5, followed by a<br />discussion in Section 6. This paper is a revised and ex-<br />panded version of [1].<br />2 GAUSSIAN PROCESSES FOR REGRESSION<br />It will be useful to first consider the regression problem, i.e.,<br />the prediction of a real valued output y* = y(x*) for a new<br />input value x*, given a set of training data ? = {(xi, ti), i = 1<br />º n}. This is of relevance because our strategy will be to<br />transform the classification problem into a regression<br />problem by dealing with the input values to the logistic<br />transfer function.<br />A stochastic process prior over functions allows us to<br />specify, given a set of inputs, x1, º xn, the distribution over<br />their corresponding outputs yxxx<br />=<br />def<br />n<br />yyy<br />12<br />? ? ? ?<br />,<br />? ?<br />??<br />,,<br />K<br />. We<br />denote this prior over functions as P(y), and similarly, P(y*, y)<br />for the joint distribution including y*. If we also specify<br />P(t|y), the probability of observing the particular values<br />0162-8828/98/$10.00 © 1998 IEEE<br />????????????????<br />•? C.K.I. Williams is with the Department of Artificial Intelligence, Univer-<br />sity of Edinburgh, Edinburgh EH1 2QL, Scotland, UK.<br />E-mail: ckiw@dai.ed.ac.uk.<br />•? D. Barber is with RWCP, Theoretical Foundation SNN, University of<br />Nijmegen, 6525 EZ Nijmegen, The Netherlands.<br />E-mail: davidb@mbfys.kun.nl.<br />Manuscript received 1 Dec. 1997; revised 5 Oct. 1998. Recommended for accep-<br />tance by A. Webb.<br />For information on obtaining reprints of this article, please send e-mail to:<br />tpami@computer.org, and reference IEEECS Log Number 108019.</p>  <p>Page 2</p> <p>WILLIAMS AND BARBER:  BAYESIAN CLASSIFICATION WITH GAUSSIAN PROCESSES1343<br />t = (t1, º tn)T given actual values y (i.e., a noise model),<br />then we have that<br />? ??<br />,<br />? ?? ? ? ? ? ?<br />? ? ? ?<br />P yP yd<br />?<br />**<br />=<br />ty t y                                          (1)<br />=<br />*<br />1<br />P<br />P yPPd<br />t<br />yy t yy<br />                    (2)<br />=<br />*<br />P yPdyy ty.                                   (3)<br />Hence, the predictive distribution for y* is found from the<br />marginalization of the product of the prior and the noise<br />model. Note that in order to make predictions, it is not nec-<br />essary to deal directly with priors over function space, only<br />n- or n + 1-dimensional joint densities. However, it is still<br />not easy to carry out these calculations unless the densities<br />involved have a special form.<br />If P(t|y) and P(y*, y) are Gaussian, then P(y*|t) is a<br />Gaussian whose mean and variance can be calculated using<br />matrix computations involving matrices of size n ¥ n. Speci-<br />fying P(y*, y) to be a multidimensional Gaussian (for all<br />values of n and placements of the points x*, x1, º xn) means<br />that the prior over functions is a Gaussian process. More<br />formally, a stochastic process is a collection of random vari-<br />ables {Y(x)|x Œ X} indexed by a set X. In our case, X will be<br />the input space with dimension d, the number of inputs. A<br />GP is a stochastic process which can be fully specified by its<br />mean function m(x) = E[Y(x)] and its covariance function<br />C(x, x&#39;) = E[(Y(x) - m(x))(Y(x&#39;) - m(x&#39;))]; any finite set of Y-<br />variables will have a joint multivariate Gaussian distribu-<br />tion. Below we consider GPs which have m(x) ∫ 0.<br />If we further assume that the noise model P(t|y) is<br />Gaussian with mean zero and variance s2I, then the pre-<br />dicted mean and variance at x* are given by<br />? ? ? ??<br />? ???<br />where [K]ij = C(xi, xj) and k(x*) = (C(x*, x1), º, C(x*, xn))T (see,<br />e.g., [25]).<br />                 $ yKI<br />T<br />xkxt<br />**<br />-<br />=+<br />?<br />s2<br />1<br />ss<br />$<br />y<br />,<br />T<br />CKI<br />22<br />1<br />xxxkxk x<br />****<br />-<br />*<br />=-+<br />? ??<br />? ? ?,<br />2.1 Parameterizing the Covariance Function<br />There are many reasonable choices for the covariance func-<br />tion. Formally, we are required to specify functions which<br />will generate a non-negative definite covariance matrix for<br />any set of points (x1, º, xk). From a modeling point of view,<br />we wish to specify covariances so that points with nearby<br />inputs will give rise to similar predictions. We find that the<br />following covariance function works well:<br />???<br />??<br />where xl is the lth component of x and ??= (logv0, logv1,<br />logw1, º, logwd) is the vector of parameters that are<br />needed to define the covariance function. Note that ? is<br />analogous to the hyperparameters in a neural network. We<br />define the parameters to be the log of the variables in (4)<br />since these are positive scale-parameters. This covariance<br />function can be obtained from a network of Gaussian radial<br />Cvw x<br />l<br />xv<br />ll<br />l<br />d<br />x x,exp<br />¢ =<br />?<br />-- ¢<br />???<br />??<br />+<br />=Â<br />?<br />??<br />0<br />2<br />1<br />1<br />1<br />2<br />,            (4)<br />basis functions in the limit of an infinite number of hidden<br />units [27].<br />The wl parameters in (4) allow a different length scale on<br />each input dimension. For irrelevant inputs, the corre-<br />sponding wl will become small, and the model will ignore<br />that input. This is closely related to the Automatic Rele-<br />vance Determination (ARD) idea of MacKay [10] and Neal<br />[15]. The v0 variable specifies the overall scale of the prior.<br />v1 specifies the variance of a zero-mean offset which has a<br />Gaussian distribution.<br />The Gaussian process framework allows quite a wide<br />variety of priors over functions. For example, the Ornstein-<br />Uhlenbeck process (with covariance function C(x, x’) =<br />e-|x-x’|) has very rough sample paths which are not mean-<br />square differentiable. On the other hand, the squared expo-<br />nential covariance function of (4) gives rise to an infinitely<br />m.s. differentiable process. In general, we believe that the<br />GP method is a quite general-purpose route for imposing<br />prior beliefs about the desired amount of smoothness. For<br />reasonably high-dimensional problems, this needs to be<br />combined with other modeling assumptions such as ARD.<br />Another modeling assumption that may be used is to build<br />up the covariance function as a sum of covariance func-<br />tions, each one of which may depend on only some of the<br />input variables (see Section 3.3 for further details).<br />2.2 Dealing With Parameters<br />Given a covariance function, it is straightforward to make<br />predictions for new test points. However, in practical situa-<br />tions we are unlikely to know which covariance function to<br />use. One option is to choose a parametric family of covari-<br />ance functions (with a parameter vector ?), and then either to<br />estimate the parameters (for example, using the method of<br />maximum likelihood) or to use a Bayesian approach where a<br />posterior distribution over the parameters is obtained.<br />These calculations are facilitated by the fact that the log<br />likelihood l = log P(?|?) can be calculated analytically as<br />1<br />22<br />where ~KKI<br />=<br />It is also possible to express analytically the partial deriva-<br />tives of the log likelihood with respect to the parameters<br />?<br />?? ??+<br />(see, e.g., [11]).<br />Given l and its derivatives with respect to ?, it is straight-<br />forward to feed this information to an optimization package<br />in order to obtain a local maximum of the likelihood.<br />In general one may be concerned about making point<br />estimates when the number of parameters is large relative<br />to the number of data points, or if some of the parameters<br />may be poorly determined, or if there may be local maxima<br />in the likelihood surface. For these reasons, the Bayesian<br />approach of defining a prior distribution over the parame-<br />ters and then obtaining a posterior distribution once the<br />data ? has been seen is attractive. To make a prediction for<br />a new test point x* one simply averages over the posterior<br />distribution P(?|?), i.e.,<br />lKK<br />n<br />2<br />T<br />= ---<br />-<br />1<br />2<br />1<br />log~<br />~<br />logtt<br />p ,                  (5)<br />+ s2 and ~<br />K  denotes the determinant of ~K .<br />∂<br />q<br />∂<br />= -<br />∂<br />∂<br />?<br />∂<br />∂<br />---<br />l<br />tr K<br />K<br />q<br />K<br />K<br />q<br />K<br />ii<br />T<br />i<br />1<br />2<br />1<br />2<br />111<br />~<br />~<br />~<br />~<br />~<br />tt,            (6)</p>  <p>Page 3</p> <p>1344IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  20,  NO.  12,  DECEMBER  1998<br />P y<br />?<br />P y<br />?<br />P<br />? ? ?<br />?<br />d<br />**<br />=<br />???<br />?<br />??<br />, .                     (7)<br />For GPs, it is not possible to do this integration analytically<br />in general, but numerical methods may be used. If ? is of<br />sufficiently low dimension, then techniques involving grids<br />in ?-space can be used.<br />If ? is high dimensional, it is very difficult to locate the<br />regions of parameter space which have high-posterior den-<br />sity by gridding techniques or importance sampling. In this<br />case, Markov chain Monte Carlo (MCMC) methods may be<br />used. These work by constructing a Markov chain whose<br />equilibrium distribution is the desired distribution P(?|?);<br />the integral in (7) is then approximated using samples from<br />the Markov chain.<br />Two standard methods for constructing MCMC methods<br />are the Gibbs sampler and Metropolis-Hastings algorithms<br />(see, e.g., [5]). However, the conditional parameter distri-<br />butions are not amenable to Gibbs sampling if the covari-<br />ance function has the form given by (4), and the Metropolis-<br />Hastings algorithm does not utilize the derivative informa-<br />tion that is available, which means that it tends to have an<br />inefficient random-walk behavior in parameter-space. Fol-<br />lowing the work of Neal [15] on Bayesian treatment of neu-<br />ral networks, Williams and Rasmussen [28] and Rasmussen<br />[17] have used the Hybrid Monte Carlo (HMC) method of<br />Duane et al. [4] to obtain samples from P(?|?). The HMC<br />algorithm is described in more detail in Appendix D.<br />3 GAUSSIAN PROCESSES FOR TWO-CLASS<br />CLASSIFICATION<br />For simplicity of exposition, we will first present our<br />method as applied to two-class problems; the extension to<br />multiple classes is covered in Section 4.<br />By using the logistic transfer function to produce an out-<br />put which can be interpreted as p(x), the probability of the<br />input x belonging to class 1, the job of specifying a prior<br />over functions p can be transformed into that of specifying<br />a prior over the input to the transfer function, which we<br />shall call the activation, and denote by y (see Fig. 1). For the<br />two-class problem we can use the logistic function p(x) =<br />s(y(x)), where s(y) = 1/ (1 + e-y). We will denote the prob-<br />ability and activation corresponding to input xi by pi and yi,<br />respectively. Fundamentally, the GP approaches to classifi-<br />cation and regression problems are similar, except that the<br />error model which is t ? N(y, s2) in the regression case is<br />replaced by t ? Bern(s(y)). The choice of v0 in (4) will affect<br />how “hard” the classification is; i.e., if p(x) hovers around<br />0.5 or takes on the extreme values of 0 and 1.<br />Previous and related work to this approach is discussed<br />in Section 3.3.<br />As in the regression case, there are now two problems to<br />address<br />1)?making predictions with fixed parameters and<br />2)?dealing with parameters.<br />We shall discuss these issues in turn.<br />3.1 Making Predictions With Fixed Parameters<br />To make predictions when using fixed parameters, we<br />would like to compute $ p<br />*<br />us to find P(p*|t) = P(p(x*)|t) for a new input x*. This can be<br />done by finding the distribution P(y*|t) (y* is the activation<br />of ?*), and then using the appropriate Jacobian to transform<br />the distribution. Formally, the equations for obtaining<br />P(y*|t) are identical to (1), (2), and (3). However, even if we<br />use a GP prior so that P(y*, y) is Gaussian, the usual expres-<br />sion for P<br />i<br />i<br />?<br />’p<br />(where the ts take on values of zero or one) means that the<br />marginalization to obtain P(y*|t) is no longer analytically<br />tractable.<br />Faced with this problem, there are two routes that we<br />can follow:<br />1)?to use an analytic approximation to the integral in (1),<br />(2), and (3) or<br />2)?to use Monte Carlo methods, specifically MCMC<br />methods, to approximate it.<br />Below, we consider an analytic approximation based on<br />Laplace’s approximation; some other approximations are<br />discussed in Section 3.3.<br />In Laplace’s approximation, the integrand P(y*, y|t, ?) is<br />approximated by a Gaussian distribution centered at a<br />maximum of this function with respect to y*, y with an in-<br />verse covariance matrix given by -——logP(y*, y|t, ?).<br />Finding a maximum can be carried out using the Newton-<br />Raphson iterative method on y, which then allows the ap-<br />proximate distribution of y* to be calculated. Details of the<br />maximization procedure can be found in Appendix A.<br />ppp<br />***<br />=<br />Pdt<br />? ?<br />, which requires<br />t<br />i<br />t<br />i<br />i<br />t y ? ?<br />?<br />=-<br />-<br />p<br />1<br />1<br /> for classification data<br />fi<br />  <br />fi<br />  <br />Fig. 1. p(x) is obtained from y(x) by “squashing” it through the sigmoid function s.</p>  <p>Page 4</p> <p>WILLIAMS AND BARBER:  BAYESIAN CLASSIFICATION WITH GAUSSIAN PROCESSES1345<br />3.2 Integration Over the Parameters<br />To make predictions we integrate the predicted probabili-<br />ties over the posterior P(?|t) µ P(t|?)P(q), as we saw in<br />Section 2.2. For the regression problem P(t|?) can be cal-<br />culated exactly using Pt<br />?<br />? ?<br />tegral is not analytically tractable for the classification<br />problem. Let Y = log P(t|y) + log P(y). Using P tiyi<br />yi<br />-+<br />?, we obtain<br />?<br />PPd t y<br />? ? ? ?<br />yy<br />?<br />=<br />, but this in-<br />? ?=<br />t y<br />i<br />e<br />i<br />log 1 ?<br />                    Y =-+<br />=Â<br />i<br />t y<br />T<br />y<br />n<br />ei<br />log 1<br />1<br />?<br />---<br />-<br />1<br />2<br />1<br />22<br />2<br />1<br />yy<br />TKK<br />n<br />loglog p .              (8)<br />By using Laplace’s approximation about the maximum ~y<br />we find that<br />? ? ? ? ?<br />2<br />We denote the right-hand side of this equation by log<br />Pa(t|?) (where a stands for approximate).<br />The integration over ?-space also cannot be done ana-<br />lytically, and we employ a Markov Chain Monte Carlo<br />method. Following Neal [15] and Williams and Rasmus-<br />sen [28] we have used the Hybrid Monte Carlo (HMC)<br />method of Duane et al [4] as described in Appendix D. We<br />use log Pa(t|?) as an approximation for log P(t|?), and use<br />broad Gaussian priors on the parameters.<br />log<br />~<br />y log logPKW<br />n<br />2<br />t<br />?Y-++<br />-<br />1<br />2<br />1<br />p.         (9)<br />3.3 Previous and Related Work<br />Our work on Gaussian processes for regression and classifi-<br />cation developed from the observation in [15] that a large<br />class of neural network models converge to GPs in the limit<br />of an infinite number of hidden units. The computational<br />Bayesian treatment of GPs can be easier than for neural<br />networks. In the regression case, an infinite number of<br />weights are effectively integrated out, and one ends up<br />dealing only with the (hyper)parameters. Results from [17]<br />show that Gaussian processes for regression are comparable<br />in performance to other state-of-the-art methods.<br />Nonparametric methods for classification problems can<br />be seen to arise from the combination of two different<br />strands of work. Starting from linear regression, McCul-<br />lagh and Nelder [12] developed generalized linear models<br />(GLMs). In the two-class classification context, this gives<br />rise to logistic regression. The other strand of work was<br />the development of nonparametric smoothing for the re-<br />gression problem. Viewed as a Gaussian process prior<br />over functions this can be traced back at least as far as the<br />work of Kolmogorov and Wiener in the 1940s. Gaussian<br />process prediction is well known in the geostatistics field<br />(see, e.g., [3]) where it is known as “kriging”. Alterna-<br />tively, by considering “roughness penalties” on functions,<br />one can obtain spline methods; for recent overviews, see<br />[25] and [8]. There is a close connection between the GP<br />and roughness penalty views, as explored in [9]. By com-<br />bining GLMs with nonparametric regression one obtains<br />what we shall call a nonparametric GLM method for clas-<br />sification. Early references to this method include [21] and<br />[16], and discussions can also be found in texts such as [8]<br />and [25].<br />There are two differences between the nonparametric<br />GLM method as it is usually described and a Bayesian<br />treatment. Firstly, for fixed parameters the nonparametric<br />GLM method ignores the uncertainty in y* and, hence, the<br />need to integrate over this (as described in Section 3.1).<br />The second difference relates to the treatment of the pa-<br />rameters ?. As discussed in Section 2.2, given parameters ?,<br />one can either attempt to obtain a point estimate for the<br />parameters or to carry out an integration over the posterior.<br />Point estimates may be obtained by maximum likelihood<br />estimation of ?, or by cross-validation or generalized cross-<br />validation (GCV) methods, see e.g., [25], [8]. One problem<br />with CV-type methods is that if the dimension of ? is large,<br />then it can be computationally intensive to search over a<br />region/ grid in parameter-space looking for the parameters<br />that maximize the criterion. In a sense, the HMC method<br />described above is doing a similar search, but using gradi-<br />ent information,1 and carrying out averaging over the pos-<br />terior distribution of parameters. In defense of (G)CV<br />methods, we note Wahba’s comments (e.g., in [26], referring<br />back to [24]) that these methods may be more robust<br />against an unrealistic prior.<br />One other difference between the kinds of non-<br />parametric GLM models usually considered and our<br />method is the exact nature of the prior that is used. Often,<br />the roughness penalties used are expressed in terms of a<br />penalty on the kth derivative of y(x), which gives rise to a<br />power law power spectrum for the prior on y(x). There can<br />also be differences over parameterization of the covariance<br />function; for example it is unusual to find parameters like<br />those for ARD introduced in (4) in nonparametric GLM<br />models. On the other hand, Wahba et al [26] have consid-<br />ered a smoothing spline analysis of variance (SS-ANOVA)<br />decomposition. In Gaussian process terms, this builds up a<br />prior on y as a sum of priors on each of the functions in the<br />decomposition<br />x ? ?<br />? ?<br />Â<br />a<br />yyxyxx<br />??<br />=+++<br />Â<br />a b<br />,<br />m<br />aa<br />ab<br />a<br />b<br />,<br />K .         (10)<br />The important point is that functions involving all orders of<br />interaction (from univariate functions, which on their own<br />give rise to an additive model) are included in this sum, up<br />to the full interaction term which is the only one that we are<br />using. From a Bayesian point of view, questions as to the<br />kinds of priors that are appropriate is an interesting mod-<br />eling issue.<br />There has also been some recent work which is related<br />to the method presented in this paper. In Section 3.1, we<br />mentioned that it is necessary to approximate the integral<br />in (1), (2), and (3) and described the use of Laplace’s<br />approximation.<br />Following the preliminary version of this paper pre-<br />sented in [1], Gibbs and MacKay [7] developed an alterna-<br />tive analytic approximation, by using variational methods<br />to find approximating Gaussian distributions that bound<br />the marginal likelihood P(t|?) above and below. These<br />1. It would be possible to obtain derivatives of the CV-score with respect<br />to ?, but this has not, to our knowledge, been used in practice.</p>  <p>Page 5</p> <p>1346 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  20,  NO.  12,  DECEMBER  1998<br />approximate distributions are then used to predict P(y*|t, ?)<br />and thus $ p x* ? ?. For the parameters, Gibbs and MacKay esti-<br />mated ? by maximizing their lower bound on P(t|?).<br />It is also possible to use a fully MCMC treatment of the<br />classification problem, as discussed in the recent paper of<br />Neal [14]. His method carries out the integrations over the<br />posterior distributions of y and ? simultaneously. It works<br />by generating samples from P(y, ?|?) in a two-stage proc-<br />ess. Firstly, for fixed ?, each of the n individual yis are up-<br />dated sequentially using Gibbs sampling. This “sweep”<br />takes time O(n2) once the matrix K-1 has been computed (in<br />time O(n3)), so it actually makes sense to perform quite a<br />few Gibbs sampling scans between each update of the pa-<br />rameters, as this probably makes the Markov chain mix<br />faster. Secondly, the parameters are updated using the Hy-<br />brid Monte Carlo method. To make predictions, one aver-<br />ages over the predictions made by each y, ? sample.<br />4 GPS FOR MULTIPLE-CLASS CLASSIFICATION<br />The extension of the preceding framework to multiple<br />classes is essentially straightforward, although notationally<br />more complex.<br />Throughout we employ a one-of-m class coding scheme,2<br />and use the multiclass analogue of the logistic function—<br />the softmax function—to describe the class probabilities.<br />The probability that an instance labeled by i is in class c is<br />denoted by pc<br />example number, and a lower index the class label.<br />Similarly, the activations associated with the probabilities<br />are denoted by yc<br />relates the activations and probabilities through<br />i, so that an upper index to denotes the<br />i. Formally, the softmax link function<br />pc<br />i<br />c<br />y<br />i<br />cc<br />i<br />y<br />=<br />Â¢¢<br />exp<br />exp<br />which automatically enforces the constraint Â<br />targets are similarly represented by tc<br />using a one-of-m coding.<br />The log likelihood takes the form ? ? Âi c c<br />for the softmax link function gives<br />=<br />cc<br />i<br />p<br />1. The<br />i, and are specified<br />i<br />c<br />i<br />t<br />,<br />lnp , which<br />? ?<br />ty<br />c<br />i<br />c<br />i<br />c<br />i<br />ci c<br />,<br />-<br />?<br />??<br />?<br />??<br />¢<br />¢ÂÂ<br />ln exp<br />p<br />.                   (11)<br />As for the two class case, we shall assume that the GP prior<br />operates in activation space; that is we specify the correla-<br />tions between the activations yc<br />One important assumption we make is that our prior<br />knowledge is restricted to correlations between the activa-<br />tions of a particular class. Whilst there is no difficulty in<br />extending the framework to include interclass correlations,<br />we have not yet encountered a situation where we felt able<br />to specify such correlations. Formally, the activation corre-<br />lations take the form,<br />i.<br />2. That is, the class is represented by a vector of length m with zero en-<br />tries everywhere except for the correct component which contains 1.<br />y y<br />c<br />K<br />i<br />c<br />i<br />c c<br />c<br />i i<br />,,                           (12)<br />¢<br />¢<br />¢<br />¢<br />= d,<br />where Kc<br />the cth class. Each individual correlation matrix Kc has the<br />form given by (4) for the two-class case. We shall use a<br />separate set of parameters for each class. The use of m in-<br />dependent processes to perform the classification is redun-<br />dant, but forcing the activations of one process to be, say,<br />zero would introduce an arbitrary asymmetry into the<br />prior.<br />For simplicity, we introduce the augmented vector notation,<br />?<br />i i , ¢ is the i, i’ element of the covariance matrix for<br />y+<br />*<br />1<br />*<br />2<br />*<br />m<br />= yyyyyyyyy<br />n<br />1<br />n<br />2mc1<br />1<br />2<br />111<br />,,,,,,,,,,,,,<br />KKKKK<br />?,<br />where, as in the two-class case, yc<br />corresponding to input x* for class c; this notation is also<br />used to define t+ and ?+. In a similar manner, we define y, t,<br />and ? by excluding the values corresponding to the test<br />point x*. Let y*<br />= yyym12<br />,,,<br />K<br />?<br />With this definition of the augmented vectors, the GP<br />prior takes the form<br />-???<br />where, from (12), the covariance matrix K+ is block diago-<br />nal in the matrices, KKm1<br />,,<br />K<br />expresses the correlations of activations within class c.<br />As in the two-class case, to use Laplace’s approximation<br />we need to find the mode of P(y+|t). The procedure is de-<br />scribed in Appendix C. As for the two-class case, we make<br />predictions for ?(x*) by averaging the softmax function over<br />the Gaussian approximation to the posterior distribution of<br />y*. At present, we simply estimate this integral using 1,000<br />draws from a Gaussian random vector generator.<br />* denotes the activation<br />***<br />?.<br />PK<br />T<br />+<br />yyy<br />+<br />+-<br />+<br />µ<br />???<br />? ?<br />? ?<br />exp<br />1<br />2<br />1<br />,                    (13)<br />++<br />. Each individual matrix Kc<br />+<br />5 EXPERIMENTAL RESULTS<br />When using the Newton-Raphson algorithm, ? was ini-<br />tialized each time with entries 1/ m, and iterated until the<br />mean relative difference of the elements of W between con-<br />secutive iterations was less than 10-4.<br />For the HMC algorithm, the same step size e is used for<br />all parameters, and should be as large as possible while<br />keeping the rejection rate low. We have used a trajectory<br />made up of L = 20 leapfrog steps, which gave a low cor-<br />relation between successive states. The priors over pa-<br />rameters were set to be Gaussian with a mean of -3 and a<br />standard deviation of 3. In all our simulations, a step size<br />e = 0.1 produced a low rejection rate (&lt; 5 percent). The<br />parameters corresponding to the wls were initialized to -2<br />and that for v0 to 0. The sampling procedure was run for<br />200 iterations, and the first third of the run was discarded;<br />this “burn-in” is intended to give the parameters time to<br />come close to their equilibrium distribution. Tests carried<br />out using the R-CODA package3 on the examples in Section<br />5.1 suggested that this was indeed effective in removing<br />3. Available from the Comprehensive R Archive Network at<br />http://www.ci.tuwien.ac.at.</p>  <p>Page 6</p> <p>WILLIAMS AND BARBER:  BAYESIAN CLASSIFICATION WITH GAUSSIAN PROCESSES 1347<br />the transients, although we note that it is widely recog-<br />nized (see, e.g., [2]) that determining when the equilib-<br />rium distribution has been reached is a difficult problem.<br />Although the number of iterations used is much less than<br />typically used for MCMC methods, it should be remem-<br />bered that<br />1)?each iteration involves L = 20 leapfrog steps and<br />2)?that by using HMC we aim to reduce the “random<br />walk” behavior seen in methods such as the Me-<br />tropolis algorithm.<br />Autocorrelation analysis for each parameter indicated, in<br />general, that low correlation was obtained after a lag of a<br />few iterations.<br />The MATLAB code which we used to run our experi-<br />ments is available from ftp://cs.aston.ac.uk/neural/willicki/gpclass/.<br />5.1 Two Classes<br />We have tried out our method on two well-known two-<br />class classification problems, the Leptograpsus crabs and<br />Pima Indian diabetes datasets.4 We first rescale the inputs<br />so that they have mean of zero and unit variance on the<br />training set. Our Matlab implementations for the HMC<br />simulations for both tasks each take several hours on a<br />SGI Challenge machine (200MHz R10000), although good<br />results can be obtained in much less time. We also tried a<br />standard Metropolis MCMC algorithm for the Crabs<br />problem, and found similar results, although the sampling<br />by this method is somewhat slower than that for HMC.<br />The results for the Crabs and Pima tasks, together with<br />comparisons with other methods (from [20] and [18]) are<br />given in Tables 1 and 2, respectively. The tables also include<br />results obtained for Gaussian processes using<br />1)?estimation of the parameters by maximizing the pe-<br />nalized likelihood (found using 20 iterations of a<br />scaled conjugate gradient optimizer) and<br />2)?Neal’s MCMC method.<br />Details of the set-up used for Neal’s method are given in<br />Appendix E.<br />In the Leptograpsus crabs problem, we attempt to clas-<br />sify the sex of crabs on the basis of five anatomical attrib-<br />utes, with an optional additional color attribute. There are<br />50 examples available for crabs of each sex and color, mak-<br />ing a total of 200 labeled examples. These are split into a<br />training set of 20 crabs of each sex and color, making 80<br />training examples, with the other 120 examples used as the<br />test set. The performance of our GP method is equal to the<br />best of the other methods reported in [20], namely a two<br />hidden unit neural network with direct input to output<br />connections, a logistic output unit, and trained with maxi-<br />mum likelihood (Network(1) in Table 1). Neal’s method<br />gave a very similar level of performance. We also found<br />that estimating the parameters using maximum penalized<br />likelihood (MPL) gave similar performance with less than a<br />minute of computing time.<br />For the Pima Indians, diabetes problem we have used<br />the data as made available by Prof. Ripley, with his train-<br />ing/ test split of 200 and 332 examples, respectively [18].<br />4. Available from http://markov.stats.ox.ac.uk/pub/PRNN.<br />The baseline error obtained by simply classifying each record<br />as coming from a diabetic gives rise to an error of 33 per-<br />cent. Again, ours and Neal’s GP methods are comparable<br />with the best alternative performance, with an error of<br />around 20 percent, as shown in Table 2. It is encouraging<br />that the results obtained using Laplace’s approximation and<br />Neal’s method are similar.5 We also estimated the parame-<br />ters using maximum penalized likelihood, rather than<br />Monte Carlo integration. The performance in this case was<br />a little worse, with 21.7 percent error, but for only two min-<br />utes computing time.<br />Analysis of the posterior distribution of the w parame-<br />ters in the covariance function (4) can be informative. Fig. 2<br />plots the posterior marginal mean and one standard devia-<br />tion error bars for each of the seven input dimensions. Re-<br />calling that the variables are scaled to have zero mean and<br />unit variance, it would appear that variables 1 and 3 have<br />the shortest length scales (and therefore the most variabil-<br />ity) associated with them.<br />5.2 Multiple Classes<br />Due to the rather long time taken to run our code, we were<br />only able to test it on relatively small problems, by which<br />we mean only a few hundred data points and several<br />classes. Furthermore, we found that a full Bayesian inte-<br />gration over possible parameter settings was beyond our<br />computational means, and we therefore had to be satisfied<br />with a maximum penalized likelihood approach. Rather<br />than using the potential and its gradient in a HMC routine,<br />we now simply used them as inputs to a scaled conjugate<br />gradient optimizer (based on [13]) instead, attempting to<br />find a mode of the class posterior, rather than to average<br />over the posterior distribution.<br />We tested the multiple-class method on the Forensic<br />Glass dataset described in [18]. This is a dataset of 214 ex-<br />amples with nine inputs and six output classes. Because the<br />5. The performance obtained by Gibbs and MacKay in [7] was similar.<br />Their method made four errors in the crab task (with color given), and 70<br />errors on the Pima dataset.<br />TABLE 1<br />NUMBER OF TEST ERRORS<br />FOR THE LEPTOGRAPSUS CRABS TASK<br />MethodColor given<br />3<br />5<br />8<br />4<br />8<br />3<br />Color not given<br />3<br />3<br />8<br />4<br />4<br />6<br />Neural Network(1)<br />Neural Network(2)<br />Linear Discriminant<br />Logistic regression<br />MARS (degree = 1)<br />PP regression (4 ridge<br />functions)<br />Gaussian Process (Laplace<br />Approximation, HMC)<br />33<br />Gaussian Process (Laplace<br />Approximation, MPL)<br />43<br />Gaussian Process (Neal’s<br />method)<br />Comparisons are taken from from Ripley [18] and Ripley [20], respectively.<br />Network(2) used two hidden units and the predictive approach (Ripley, [19])<br />which uses Laplace’s approximation to weight each network local minimum.<br />43</p>  <p>Page 7</p> <p>1348IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  20,  NO.  12,  DECEMBER  1998<br />dataset is so small, the performance is estimated from using<br />10-fold cross validation. Computing the penalized maxi-<br />mum likelihood estimate of our multiple GP method took<br />approximately 24 hours on our SGI Challenge and gave a<br />classification error rate of 23.3 percent. As we see from<br />Table 3, this is comparable to the best of the other methods.<br />The performance of Neal’s method is surprisingly poor; this<br />may be due to the fact that we allow separate parameters<br />for each of the y processes, while these are constrained to be<br />equal in Neal’s code. There are also small but perhaps sig-<br />nificant differences in the specification of the prior (see<br />Appendix E for details).<br />6 DISCUSSION<br />In this paper, we have extended the work of Williams and<br />Rasmussen [28] to classification problems, and have dem-<br />onstrated that it performs well on the datasets we have<br />tried. We believe that the kinds of Gaussian process prior<br />we have used are more easily interpretable than models<br />(such as neural networks) in which the priors are on the<br />parameterization of the function space. For example, the<br />posterior distribution of the ARD parameters (as illustrated<br />in Fig. 2 for the Pima Indians diabetes problem) indicates<br />the relative importance of various inputs. This interpret-<br />ability should also facilitate the incorporation of prior<br />knowledge into new problems.<br />There are quite strong similarities between GP classifiers<br />and support-vector machines (SVMs) [23]. The SVM uses a<br />covariance kernel, but differs from the GP approach by us-<br />ing a different data fit term (the maximum margin), so that<br />the optimal y is found using quadratic programming. The<br />comparison of these two algorithms is an interesting direc-<br />tion for future research.<br />A problem with methods based on GPs is that they re-<br />quire computations (trace, determinants and linear solu-<br />tions) involving n ¥ n matrices, where n is the number of<br />training examples, and hence run into problems on large<br />datasets. We have looked into methods using Bayesian nu-<br />merical techniques to calculate the trace and determinant<br />[22], [6], although we found that these techniques did not<br />work well for the (relatively) small size problems on which<br />we tested our methods. Computational methods used to<br />speed up the quadratic programming problem for SVMs<br />may also be useful for the GP classifier problem. We are<br />also investigating the use of different covariance functions<br />and improvements on the approximations employed.<br />APPENDIX A<br />MAXIMIZING P(y+|t): TWO-CLASS CASE<br />We describe how to find iteratively the vector y+ so that<br />P(y+|t) is maximized. This material is also covered in<br />[8, Section 5.3.3] and [25, Section 9.2]. We provide it here for<br />completeness and so that the terms in (9) are well-defined.<br />Let y+ denote (y*, y), the complete set of activations. By<br />Bayes’ theorem log P(y+|t) = log P(t|y) + log P(y+) - log P(t),<br />and let Y+ = log P(t|y) + log P(y+). As P(t) does not depend<br />on y+ (it is just a normalizing factor), the maximum of<br />TABLE 2<br />NUMBER OF TEST ERRORS ON THE PIMA INDIAN DIABETES TASK<br />Method<br />Neural Network<br />Linear Discriminant<br />Logistic Regression<br />MARS (degree = 1)<br />PP regression (4 ridge functions)<br />Gaussian Mixture<br />Gaussian Process (Laplace<br />Approximation, HMC)<br />Gaussian Process (Laplace<br />Approximation, MPL)<br />Gaussian Process (Neal’s method)<br />Comparisons are taken from from Ripley [18] and Ripley [20], respectively. The<br />neural network had one hidden unit and was trained with maximum likelihood;<br />the results were worse for nets with two or more hidden units (Ripley, [18]).<br />Pima Indian diabetes<br />75+<br />67<br />66<br />75<br />75<br />64<br />68<br />69<br />68<br />TABLE 3<br />PERCENTAGE OF TEST ERROR<br />FOR THE FORENSIC GLASS PROBLEM<br />Method Forensic Glass<br />23.8%<br />36%<br />32.2%<br />35%<br />30.8%<br />32.2%<br />23.3%<br />31.8%<br />Neural Network (4HU)<br />Linear Discriminant<br />MARS (degree = 1)<br />PP regression (5 ridge functions)<br />Gaussian Mixture<br />Decision Tree<br />Gaussian Process (LA, MPL)<br /> Gaussian Process (Neal’s method)<br />See Ripley [18] for details of the methods.<br />Fig. 2. Plot of the log w parameters for the Pima dataset. The circle<br />indicates the posterior marginal mean obtained from the HMC run<br />(after burn-in), with one standard deviation error bars. The square<br />symbol shows the log w-parameter values found by maximizing the<br />penalized likelihood. The variables are: 1) the number of pregnan-<br />cies, 2) plasma glucose concentration, 3) diastolic blood pressure,<br />4) triceps skin fold thickness, 5) body mass index, 6) diabetes pedi-<br />gree function, 7) age. For comparison, Wahba et al. [26] using gen-<br />eralized linear regression, found that variables 1, 2, 5, and 6 were<br />the most important.</p>  <p>Page 8</p> <p>WILLIAMS AND BARBER:  BAYESIAN CLASSIFICATION WITH GAUSSIAN PROCESSES1349<br />P(y+|t) is found by maximizing Y+ with respect to y+.<br />Using log<br />logP t yt y<br />iiii<br />? ?<br />?<br />?<br />e<br />yi<br />?<br />?<br />=-+<br />1, we obtain<br />Y+<br />=<br />++<br />-<br />+<br />=-+-<br />Â<br />i<br />t yyy<br />T<br />y<br />n<br />T<br />eK<br />i<br />log 1<br />1<br />2<br />1<br />1<br />--<br />+<br />2<br />+<br />1<br />2<br />1<br />2 log logK<br />n<br />p ,                        (14)<br />where K+ is the covariance matrix of the GP evaluated at<br />x1, º xn, x*. Y is defined similarly in (8). K+ can be parti-<br />tioned in terms of an n ¥ n matrix K, a n ¥ 1 vector k and a<br />scalar k*, viz.<br />=???<br />k<br />As y* only enters into (14) in the quadratic prior term and<br />has no data point associated with it, maximizing Y+ with<br />respect to y+ can be achieved by first maximizing Y with<br />respect to y and then doing the further quadratic optimiza-<br />tion to determine y*. To find a maximum of Y, we use the<br />Newton-Raphson iteration ynew = y - (——Y)-1—Y. Differen-<br />tiating (8) with respect to y we find<br />—Y = (t - ?) - K-1y                           (16)<br />K<br />K<br />k<br />T<br />+<br />*<br />???<br />k<br />.                               (15)<br />——Y = - K-1 - W,                                 (17)<br />where the “noise” matrix is given by W = diag(p1(1 - p1), ..,<br />pn(1 - pn)). This results in the iterative equation,<br />ynew = (K-1 + W)-1W(y + W-1(t - ?)).                (18)<br />To avoid unnecessary inversions, it is usually more con-<br />venient to rewrite this in the form<br />ynew = K(I + W K)-1(Wy + (t - ?)).                  (19)<br />Note that -——Y is always positive definite, so that the op-<br />timization problem is convex.<br />Given a converged solution ~y  for y, y* can easily be<br />found using yK<br />*<br />kyk<br />from (16). var(y*) is given by K<br />?<br />is the W with a zero appended in the (n + 1)th diagonal po-<br />sition. Given the mean and variance of y* it is then easy to<br />find $ pppp<br />****<br />Pdt<br />? ?<br />, the mean of the distribution of<br />P(p*|t). In order to calculate the Gaussian integral over the<br />logistic sigmoid function, we employ an approximation<br />based on the expansion of the sigmoid function in terms of<br />the error function. As the Gaussian integral of an error<br />function is another error function, this approximation is fast<br />to compute. Specifically, we use a basis set of five scaled<br />error functions to interpolate the logistic sigmoid at chosen<br />points.6 This gives an accurate approximation (to 10-4) to<br />the desired integral with a small computational cost.<br />The justification of Laplace’s approximation in our case<br />is somewhat different from the argument usually put for-<br />ward, e.g., for asymptotic normality of the maximum<br />TT<br />-<br />==-<br />t<br />1~<br />~?<br />?<br />-<br />?, as K-<br />-<br />1<br />?? ?? ?, where W+<br />=-<br />1~<br />y<br />~<br />?<br />t<br />??<br />W<br />nn<br />++<br />++<br />+<br />1<br />11<br />=<br />6. In detail, we used the basis functions erf(lx)) for l = [0.41, 0.4, 0.37,<br />0.44, 0.39]. These were used to interpolate s(x) at x = [0, 0.6, 2, 3.5, 4.5, •].<br />likelihood estimator for a model with a finite number of<br />parameters. This is because the dimension of the problem<br />grows with the number of data points. However, if we con-<br />sider the “infill asymptotics” (see, e.g., [3]), where the num-<br />ber of data points in a bounded region increases, then a local<br />average of the training data at any point x will provide a<br />tightly localized estimate for p(x) and hence y(x) (this rea-<br />soning parallels more formal arguments found in [29]).<br />Thus, we would expect the distribution P(y) to become<br />more Gaussian with increasing data.<br />APPENDIX B<br />DERIVATIVES OF logPa(t|?)wrt?<br />For both the HMC and MPL methods, we require the de-<br />rivative of la = logPa(t|?) with respect to components of ?,<br />for example qk. This derivative will involve two terms, one<br />due to explicit dependencies of<br />1<br />2<br /> lKW<br />n<br />2<br />a=-++<br />-<br />Y~<br />loglogy ? ?<br />2<br />1<br />p<br />on qk, and also because a change in ? will cause a change in<br />~y . However, as ~y  is chosen so that?—Y =<br />=<br />y<br />y y<br />? ?<br />~<br />0, we obtain<br />∂<br />∂<br />=<br />∂<br />∂<br />-<br />∂+<br />∂<br />∂<br />∂<br />-<br />=Â<br />i<br />ll<br />KW<br />y<br />y<br />q<br />a<br />k<br />a<br />k<br />i<br />n<br />i<br />k<br />qq<br />explicit<br />1<br />2<br />1<br />1<br />log<br />~<br />~<br />.        (20)<br />The dependence of |K-1 + W| on ~y  arises through the de-<br />pendence of W on ~? , and, hence, ~y . By differentiating<br />~<br />~<br />y<br />t<br />=-<br />K<br />?<br />??, one obtains<br />∂<br />∂<br />q<br />k<br />??<br />and, hence, the required derivative can be calculated.<br />=+<br />∂<br />∂<br />-<br />-<br />~<br />y<br />~<br />? ,                    (21)<br />t<br />q<br />k<br />IKW<br />K<br />??<br />1<br />APPENDIX C<br />MAXIMIZING P(y+|t): MULTIPLE-CLASS CASE<br />The GP prior and likelihood, defined by (13) and (11), de-<br />fine the posterior distribution of activations, P(y+|t). As in<br />Appendix A we are interested in a Laplace approximation<br />to this posterior, and therefore need to find the mode with<br />respect to y+. Dropping unnecessary constants, the multi-<br />class analogue of (14) is<br />1<br />22<br />Y+++<br />-<br />++<br />= --+-Â Â<br />i<br />1<br />1<br />yyt y<br />TT<br />c<br />i<br />c<br />KKyloglnexp<br />.<br />By the same principle as in Appendix A, we define Y by<br />analogy with (8), and first optimize Y with respect to y,<br />afterwards performing the quadratic optimization of Y+<br />with respect to y*.<br />In order to optimize ? with respect to y, we make use of<br />the Hessian given by<br />——Y = -K-1 - W,                               (22)<br />where K is the mn ¥ mn block-diagonal matrix with blocks<br />Kc, c = 1, º, m. Although this is in the same form as for the<br />two-class case, (17), there is a slight change in the definition<br />of the “noise” matrix, W. A convenient way to define W is</p>  <p>Page 9</p> <p>1350IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  20,  NO.  12,  DECEMBER  1998<br />by introducing the matrix P which is a mn ¥ n matrix of the<br />form ’ = diagdiag<br />pp<br />11<br />..,,<br />???<br />K<br />tion, we can write the noise matrix in the form of a diagonal<br />matrix and an outer product,<br />?<br />n<br />mm<br />n<br />pp<br />11<br />..<br />???<br />. Using this nota-<br />W diag<br />n<br />1<br />mm<br />nT<br />= - + ’’pppp<br />1<br />11<br />..,..,..<br />?<br />.             (23)<br />As in the two-class case, we note that -——Y is again posi-<br />tive definite, so that the optimization problem is convex.<br />The update equation for iterative optimization of Y<br />with respect to the activations y then follows the same<br />form as that given by (18). The advantage of the repre-<br />sentation of the noise matrix in (23) is that we can then<br />invert matrices and find their determinants using the<br />identities,<br />(A + ’’ T)-1 = A-1 - A-1’(In + ’TA-1’)-1’TA-1       (24)<br />and<br />det(A + ’’T) = det(A)det(In + ’TA-1’),            (25)<br />??. As A is block-diagonal, it<br />can be inverted blockwise. Thus, rather than requiring<br />determinants and inverses of a mn ¥ mn matrix, we only<br />need to carry out expensive matrix computations on n ¥<br />n matrices. The resulting update equations for y are then<br />of the same form as given in (18), where the noise matrix<br />and covariance matrices are now in their multiple class<br />form.<br />Essentially, these are all the results needed to generalize<br />the method to the multiple-class problem. Although, as we<br />mentioned above, the time complexity of the problem does<br />not scale with the m3, but rather m (due to the identities in<br />(24), (25)), calculating the function and its gradient is still<br />rather expensive. We have experimented with several<br />methods of mode finding for the Laplace approximation.<br />The advantage of the Newton iteration method is its fast<br />quadratic convergence. An integral part of each Newton<br />step is the calculation of the inverse of a matrix M acting<br />upon a vector, i.e., M-1b. In order to speed up this particu-<br />lar step, we used a conjugate gradient (CG) method to solve<br />iteratively the corresponding linear system Mz = b. As we<br />repeatedly need to solve the system (because W changes as<br />y is updated), it saves time not to run the CG method to<br />convergence each time it is called. In our experiments, the<br />CG algorithm was terminated when 1<br />where r = Mz - b.<br />The calculation of the derivative of logPa(t|?) wrt ? in<br />the multiple-class case is analogous to the two-class case<br />described in Appendix B.<br />where AK diag<br />m<br />n<br />=+<br />-1<br />1<br />1<br />pp<br />..<br />10<br />1<br />9<br />nr<br />i<br />n<br />i<br />Â&lt;<br />=<br />-<br />,<br />APPENDIX D<br />HYBRID MONTE CARLO<br />HMC works by creating a fictitious dynamical system in<br />which the parameters are regarded as position variables,<br />and augmenting these with momentum variables p. The<br />purpose of the dynamical system is to give the parameters<br />“inertia” so that random-walk behavior in ?-space can be<br />avoided. The total energy, H, of the system is the sum of the<br />kinetic energy, K = pTp/ 2 and the potential energy, E. The<br />potential energy is defined such that p(?|D) µ exp(-E),<br />i.e., E = - logP(t|?) - log P(?). We sample from the joint<br />distribution for ? and p given by P(?, p) µ exp(-E - K); the<br />marginal of this distribution for ? is the required posterior.<br />A sample of parameters from the posterior can therefore<br />be obtained by simply ignoring the momenta.<br />Sampling from the joint distribution is achieved by two steps:<br />1)?finding new points in phase space with near-identical<br />energies H by simulating the dynamical system using<br />a discretised approximation to Hamiltonian dynamics<br />and<br />2)?changing the energy H by Gibbs sampling the mo-<br />mentum variables.<br />Hamilton’s first-order differential equations for H are<br />approximated using the leapfrog method which requires<br />the derivatives of E with respect to ?. Given a Gaussian<br />prior on ?, logP(?) is straightforward to differentiate. The<br />derivative of logPa(t|?) is also straightforward, although<br />implicit dependencies of ~y  (and, hence, ~p ) on ? must be<br />taken into account as described in Appendix B. The calcu-<br />lation of the energy can be quite expensive as for each new<br />?, we need to perform the maximization required for<br />Laplace’s approximation, (9). This proposed state is then<br />accepted or rejected using the Metropolis rule depending<br />on the final energy H* (which is not necessarily equal to the<br />initial energy H because of the discretization).<br />APPENDIX E<br />SIMULATION SET-UP FOR NEAL’S CODE<br />We used the fbm software available from http://www.cs.utoronto.<br />ca/radford/fbm.software.html. For example, the commands used<br />to run the Pima example are<br />gp-spec pima1.log 7 1 - - 0.1 / 0.05:0.5<br />x0.2:0.5:1<br />model-spec pima1.log binary<br />pima1.log 7 1 2 <br />mypima.te@1:332.<br />gp-gen pima1.log fix 0.5 1<br />mc-spec pima1.log repeat 4 scan-values<br />200 heatbath hybrid 6 0.5<br />gp-mc pima1.log 500<br />/  pima1.tr@1:200.<br />which follow closely the example given in Neal’s docu-<br />mentation.<br />The gp-spec command specifies the form of the Gaus-<br />sian process, and in particular the priors on the parameters<br />v0 and the ws (see (4)). The expression 0.05:0.5 specifies a<br />Gamma-distribution prior on v0, and x0.2:0.5:1 specifies<br />a hierarchical Gamma prior on the ws. Note that a “jitter”<br />of 0.1 is also specified on the prior covariance function;<br />this improves conditioning of the covariance matrix.<br />The mc-spec command gives details of the MCMC up-<br />dating procedure. It specifies four repetitions of 200 scans of<br />the y values followed by six HMC updates of the parameters<br />(using a step-size adjustment factor of 0.5). gp-mc specifies<br />that this is sequence is carried out 500 times.</p>  <p>Page 10</p> <p>WILLIAMS AND BARBER:  BAYESIAN CLASSIFICATION WITH GAUSSIAN PROCESSES1351<br />We aimed for a rejection rate of around 5 percent. If this<br />was exceeded, the stepsize reduction factor was reduced<br />and the simulation run again.<br />ACKNOWLEDGMENTS<br />We thank Prof. B. Ripley for making available the Lepto-<br />grapsus crabs, Pima Indian diabetes, and Forensic Glass<br />datasets. This work was partially supported by EPSRC<br />grant GR/ J75425, Novel Developments in Learning Theory for<br />Neural Networks, and much of the work was carried out at<br />Aston University. The authors gratefully acknowledge the<br />hospitality provided by the Isaac Newton Institute for<br />Mathematical Sciences (Cambridge, UK) where this paper<br />was written. We thank Mark Gibbs, David MacKay, and<br />Radford Neal for helpful discussions, and the anonymous<br />referees for their comments which helped improve the<br />paper.<br />REFERENCES<br />[1]? D. Barber and C.K.I Williams, “Gaussian Processes for Bayesian<br />Classification via Hybrid Monte Carlo,” M.C. Mozer, M.I. Jordan,<br />and T. Petsche, eds., Advances in Neural Information Processing Sys-<br />tems 9. MIT Press, 1997.<br />[2]? M.K. Cowles and B.P. Carlin, ”Markov-Chain Monte-Carlo Con-<br />vergence Diagnostics—A Comparative Review,” J. Am. Statistics<br />Assoc., vol. 91, pp. 883-904, 1996.<br />[3]? N.A.C. Cressie, Statistics for Spatial Data. New York, NY: Wiley,<br />1993.<br />[4]? S. Duane, A.D. Kennedy, B.J. Pendleton, and D. Roweth, “Hybrid<br />Monte Carlo,” Physics Letters B, vol. 195, pp. 216-222, 1987.<br />[5]? A. Gelman, J.B. Carlin, H.S. Stern, and D.B. Rubin, Bayesian Data<br />Analysis. London: Chapman and Hall, 1995.<br />[6]? M. Gibbs and D.J.C. MacKay, “Efficient Implementation of Gaus-<br />sian Processes,” Draft manuscript, available from http://wol.ra.phy.<br />cam.ac.uk/mackay/homepage.html., 1997.<br />[7]? M. Gibbs and D.J.C. MacKay, “Variational Gaussian Process Clas-<br />sifiers,” Draft manuscript, available via http://wol.ra.phy.cam.ac.uk/<br />mackay/homepage.html., 1997.<br />[8]? P. J. Green and B. W. Silverman, Nonparametric Regression and Gen-<br />eralized Linear Models. London: Chapman and Hall, 1994.<br />[9]? G. Kimeldorf and G. Wahba, “A Correspondence Between Baye-<br />sian Estimation of Stochastic Processes and Smoothing by<br />Splines,” Annals of Math. Statistics, vol. 41, pp. 495-502, 1970.<br />[10]? D.J.C. MacKay, “Bayesian Methods for Backpropagation Net-<br />works,” J.L. van Hemmen, E. Domany, and K. Schulten, eds.,<br />Models of Neural Networks II. Springer, 1993.<br />[11]? K.V. Mardia and R.J. Marshall, “Maximum Likelihood Estimation<br />for Models of Residual Covariance in Spatial Regression,” Biome-<br />trika, vol. 71, no. 1, pp. 135-146, 1984.<br />[12]? P. McCullagh, and J. Nelder, Generalized Linear Models. Chapman<br />and Hall, 1983.<br />[13]? M. Møller, “A Scaled Conjugate Gradient Algorithm for Fast Su-<br />pervised Learning,” Neural Networks, vol. 6, no. 4, pp. 525-533,<br />1993.<br />[14]? R.M. Neal, “Monte Carlo Implementation of Gaussian Process<br />Models for Bayesian Regression and Classification,” Technical<br />Report 9702, Dept. of Statistics, Univ. of Toronto, 1997. Available<br />from http://www.cs.toronto.edu/~radford/.<br />[15]? R.M. Neal, Bayesian Learning for Neural Networks. New York,<br />Springer, 1996. Lecture Notes in Statistics 118.<br />[16]? F. O’Sullivan, B.S. Yandell, and W.J. Raynor, “Automatic Smooth-<br />ing of Regression Functions in Generalized Linear Models,” J. Am.<br />Statistical Assoc., vol. 81, pp. 96-103, 1986.<br />[17]? C.E. Rasmussen, Evaluation of Gaussian Processes and Other Methods<br />for Non-Linear Regression. PhD thesis, Dept. of Computer Science,<br />Univ. of Toronto, 1996. Available from http://www.cs.utoronto.ca/~carl/.<br />[18]? B. Ripley, Pattern Recognition and Neural Networks. Cambridge, UK:<br />Cambridge Univ. Press, 1996.<br />[19]? B.D. Ripley, “Statistical Aspects of Neural Networks,” O.E. Barn-<br />dorff-Nielsen, J.L. Jensen, and W.S. Kendall, eds., Networks and<br />Chaos—Statistical and Probabilistic Aspects, pp. 40-123. Chapman<br />and Hall, 1993.<br />[20]? B.D. Ripley, “Flexible Non-Linear Approaches to Classification,”<br />V. Cherkassy, J.H. Friedman, and H. Wechsler, eds., From Statistics<br />to Neural Networks, pp. 105-126. Springer, 1994.<br />[21]? B.W. Silverman, “Density Ratios, Empirical Likelihood and Cot<br />Death,” Applied Statistics, vol. 27, no. 1, pp. 26-33, 1978.<br />[22]? J. Skilling, “Bayesian Numerical Analysis,” W.T. Grandy, Jr. and P.<br />Milonni, eds., Physics and Probability. Cambridge Univ. Press, 1993.<br />[23]? V.N. Vapnik, The Nature of Statistical Learning Theory. New York,<br />NY: Springer Verlag, 1995.<br />[24]? G. Wahba, “A Comparison of GCV and GML for Choosing the<br />Smoothing Parameter in the Generalized Spline Smoothing Prob-<br />lem,” Annals of Statistics, vol. 13, pp. 1,378-1,402, 1985.<br />[25]? G. Wahba, Spline Models for Observational Data. Soc. Industrial and<br />Applied Mathematics, 1990. CBMS-NSF Regional Conf. Series in<br />Applied Mathematics.<br />[26]? G. Wahba, C. Gu, Y. Wang, and R. Chappell, “Soft Classification,<br />a.k.a., Risk Estimation, via Penalized Log Likelihood and<br />Smoothing Spline Analysis of Variance,” D.H. Wolpert, ed., The<br />Mathematics of Generalization. Addison-Wesley, 1995. Proc. vol. XX<br />in the Santa Fe Institute Studies in the Sciences of Complexity.<br />[27]? C.K.I. Williams, “Computing With Infinite Networks,” M.C.<br />Mozer, M.I. Jordan, and T. Petsche, eds., Advances in Neural Infor-<br />mation Processing Systems 9. MIT Press, 1997.<br />[28]? C.K.I. Williams and C.E. Rasmussen, “Gaussian Processes for<br />Regression,” D.S. Touretzky, M.C. Mozer, and M.E. Hasselmo,<br />eds., Advances in Neural Information Processing Systems 8, pp. 514-<br />520. MIT Press, 1996.<br />[29]? S.J. Yakowitz and F. Szidarovszky, “A Comparison of Kriging<br />With Nonparametric Regression Methods,” J. Multivariate Analy-<br />sis, vol. 16, pp. 21-53, 1985.<br />Chris Williams studied physics at Cambridge,<br />graduating in 1982 and continued on to do Part<br />III Maths (1983). He then received an MSc in<br />water resources at the University of Newcastle<br />upon Tyne before going to work in Lesotho,<br />Southern Africa in low-cost sanitation. In 1988,<br />he returned to academia, studying neural net-<br />works/artificial intelligence with Geoff Hinton at<br />the University of Toronto (MSc 1990, PhD 1994).<br />He was a member of the Neural Computing<br />Research Group at Aston University from 1994<br />to 1998, and is currently a lecturer in the Division of Informatics at the<br />University of Edinburgh.<br />His research interests cover a wide range of theoretical and practi-<br />cal issues in neural networks, statistical pattern recognition, computer<br />vision, and artificial intelligence.<br />David Barber completed the University of Cam-<br />bridge mathematics tripos in 1990. Following a<br />year at Heidelberg University, Germany, he re-<br />ceived an MSc in neural networks at King’s Col-<br />lege, London. Subsequently, he went to the Uni-<br />versity of Edinburgh, completing a PhD in the<br />theoretical physics department with a study of<br />the statistical mechanics of machine learning.<br />After a two-year research position at Aston Uni-<br />versity, he took up his current research position<br />at the University of Nijmegen, Holland. His main<br />interests include machine learning, Bayesian techniques, and statisti-<br />cal mechanics.</p>   </div> <div id="rgw18_56ab1e628809b" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw19_56ab1e628809b">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw20_56ab1e628809b"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://eprints.aston.ac.uk/4491/1/IEEE_transactions_on_pattern_analysis_20(12).pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Bayesian classification with Gaussian processes">Bayesian classification with Gaussian processes</a> </div>  <div class="details">   Available from <a href="http://eprints.aston.ac.uk/4491/1/IEEE_transactions_on_pattern_analysis_20(12).pdf" target="_blank" rel="nofollow">aston.ac.uk</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw27_56ab1e628809b" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (312) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw28_56ab1e628809b" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw29_56ab1e628809b" >  <div class="indent-left">  <div id="rgw30_56ab1e628809b" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Kostas_Stathis" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Kostas Stathis </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw31_56ab1e628809b">  <li class="citation-context-item"> "Gaussian Processes (GPs) are widely used tools in statistics (Barry, 1986), machine learning (Neal, 1995; Williams and Barber, 1998; Kuss and Rasmussen, 2005; Rasmussen and Williams, 2006; Damianou and Lawrence, 2013), robotics (Ferris et al., 2006), computer vision (Kemmler et al., 2013), and scientific computation (Kennedy and O&amp;apos;Hagan, 2001; Schneider et al., 2008; Kwan et al., 2013). They are also central to probabilistic numerics, an emerging effort to develop more computationally efficient numerical procedures, and to Bayesian optimization, a family of meta-optimization techniques that are widely used to tune parameters for deep learning algorithms (Snoek et al., 2012; Gelbart et al., 2014). " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization"> <span class="publication-title js-publication-title">Probabilistic Programming with Gaussian Process Memoization</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2034167984_Ulrich_Schaechtle" class="authors js-author-name ga-publications-authors">Ulrich Schaechtle</a> &middot;     <a href="researcher/2089456342_Ben_Zinberg" class="authors js-author-name ga-publications-authors">Ben Zinberg</a> &middot;     <a href="researcher/2089399536_Alexey_Radul" class="authors js-author-name ga-publications-authors">Alexey Radul</a> &middot;     <a href="researcher/8074903_Kostas_Stathis" class="authors js-author-name ga-publications-authors">Kostas Stathis</a> &middot;     <a href="researcher/2089392273_Vikash_K_Mansinghka" class="authors js-author-name ga-publications-authors">Vikash K. Mansinghka</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Gaussian Processes (GPs) are widely used tools in statistics, machine
learning, robotics, computer vision, and scientific computation. However,
despite their popularity, they can be difficult to apply; all but the simplest
classification or regression applications require specification and inference
over complex covariance functions that do not admit simple analytical
posteriors. This paper shows how to embed Gaussian processes in any
higher-order probabilistic programming language, using an idiom based on
memoization, and demonstrates its utility by implementing and extending classic
and state-of-the-art GP applications. The interface to Gaussian processes,
called gpmem, takes an arbitrary real-valued computational process as input and
returns a statistical emulator that automatically improve as the original
process is invoked and its input-output behavior is recorded. The flexibility
of gpmem is illustrated via three applications: (i) robust GP regression with
hierarchical hyper-parameter learning, (ii) discovering symbolic expressions
from time-series data by fully Bayesian structure learning over kernels
generated by a stochastic grammar, and (iii) a bandit formulation of Bayesian
optimization with automatic inference and action selection. All applications
share a single 50-line Python library and require fewer than 20 lines of
probabilistic code each. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Dec 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Kostas_Stathis/publication/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization/links/5687eb7108ae051f9af5a28a.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw32_56ab1e628809b" >  <div class="indent-left">  <div id="rgw33_56ab1e628809b" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/281451121_GPU-Accelerated_Gaussian_Processes_for_Object_Detection">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Calum_Blair" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Calum Blair </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw34_56ab1e628809b">  <li class="citation-context-item"> "Evaluating this directly as in (3) is intractable [15], so we use a Laplacian approximation (from [13, Ch.3 §4]), which allows the posterior over the training data and labels in (3) to be approximated as a Gaussian: " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Conference Paper:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/281451121_GPU-Accelerated_Gaussian_Processes_for_Object_Detection"> <span class="publication-title js-publication-title">GPU-Accelerated Gaussian Processes for Object Detection</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2054065797_Calum_Blair" class="authors js-author-name ga-publications-authors">Calum Blair</a> &middot;     <a href="researcher/2054134769_John_Thompson" class="authors js-author-name ga-publications-authors">John Thompson</a> &middot;     <a href="researcher/2048219205_Neil_M_Robertson" class="authors js-author-name ga-publications-authors">Neil M Robertson</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Gaussian Process classification (GPC) allows accurate and reliable detection of objects. The high computational load of squared-error or radial basis function kernels limits the applications that GPC can be used in, as memory requirements and computation time are both limiting factors. We describe our version of accelerated GPC on GPU (Graphics Processing Unit). GPUs have limited memory so any GPC implementation must be memory-efficient as well as computationally efficient. Using a high-performance pedestrian detector as a starting point, we use its packed or block-based feature descriptor and demonstrate a fast matrix multiplication implementation of GPC which is also extremely memory efficient. We demonstrate a speed up of 3.7 times over a multicore, BLAS-optimised CPU implementation. Results show that this is more accurate and reliable than results obtained from a comparable support vector machine algorithm. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Conference Paper &middot; Sep 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Calum_Blair/publication/281451121_GPU-Accelerated_Gaussian_Processes_for_Object_Detection/links/55e851be08ae3e1218422e18.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw35_56ab1e628809b" >  <div class="indent-left">  <div id="rgw36_56ab1e628809b" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Vasek_Smidl" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Vasek Smidl </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw37_56ab1e628809b">  <li class="citation-context-item"> "Inference of GP covariance parameters is analytically intractable, and standard inference methods require repeatedly calculating the so called marginal likelihood . When the likelihood function is not Gaussian, e.g., in classification, in ordinal regression, in modeling of stochastic volatility, in Cox-processes, the marginal likelihood cannot be computed analytically, and this has motivated a large body of the literature to develop approximate inference methods [56] [43] [31] [47] [41] [24], reparameterization techniques [36] [34] [54] [14], and exact inference with unbiased computations of the marginal likelihood [12] [10]. Even in the case of a Gaussian likelihood, which makes the marginal likelihood computable, inference is generally costly because the computation of the marginal likelihood has time complexity scaling with the cube of the number of input vectors [11]. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes"> <span class="publication-title js-publication-title">Adaptive Multiple Importance Sampling for Gaussian Processes</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2079162962_Xiaoyu_Xiong" class="authors js-author-name ga-publications-authors">Xiaoyu Xiong</a> &middot;     <a href="researcher/82161005_Vaclav_Smidl" class="authors js-author-name ga-publications-authors">Václav Šmídl</a> &middot;     <a href="researcher/70871340_Maurizio_Filippone" class="authors js-author-name ga-publications-authors">Maurizio Filippone</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> In applications of Gaussian processes where quantification of uncertainty is
a strict requirement, it is necessary to accurately characterize the posterior
distribution over Gaussian process covariance parameters. Normally, this is
done by means of Markov chain Monte Carlo (MCMC) algorithms. Focusing on
Gaussian process regression where the marginal likelihood is computable but
expensive to evaluate, this paper studies algorithms based on importance
sampling to carry out expectations under the posterior distribution over
covariance parameters. The results indicate that expectations computed using
Adaptive Multiple Importance Sampling converge faster per unit of computation
than those computed with MCMC algorithms for models with few covariance
parameters, and converge as fast as MCMC for models with up to around twenty
covariance parameters. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Aug 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Vasek_Smidl/publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes/links/55e94dff08ae65b6389aee89.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw22_56ab1e628809b" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw23_56ab1e628809b">  </ul> </div> </div>   <div id="rgw14_56ab1e628809b" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw15_56ab1e628809b"> <div> <h5> <a href="publication/2338748_Gaussian_Processes_for_Bayesian_Classification_via_Hybrid_Monte_Carlo" class="color-inherit ga-similar-publication-title"><span class="publication-title">Gaussian Processes for Bayesian Classification via Hybrid Monte Carlo</span></a>  </h5>  <div class="authors"> <a href="researcher/7304106_David_Barber" class="authors ga-similar-publication-author">David Barber</a>, <a href="researcher/2048918802_Christopher_K_I_Williams" class="authors ga-similar-publication-author">Christopher K. I. Williams</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw16_56ab1e628809b"> <div> <h5> <a href="publication/243766498_Bayesian_Classification_with_Gaussian_Priors" class="color-inherit ga-similar-publication-title"><span class="publication-title">Bayesian Classification with Gaussian Priors</span></a>  </h5>  <div class="authors"> <a href="researcher/2016493079_C_Williams" class="authors ga-similar-publication-author">C. Williams</a>, <a href="researcher/2016789686_David_Barber" class="authors ga-similar-publication-author">David Barber</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56ab1e628809b"> <div> <h5> <a href="publication/239597440_CLASIFICACION_DE_EVENTOS_SISMICOS_EMPLEANDO_PROCESOS_GAUSSIANOS_Event_Seismic_Classification_using_Gaussian_Processes" class="color-inherit ga-similar-publication-title"><span class="publication-title">CLASIFICACIÓN DE EVENTOS SÍSMICOS EMPLEANDO PROCESOS GAUSSIANOS Event Seismic Classification using Gaussian Processes</span></a>  </h5>  <div class="authors"> <a href="researcher/2012227962_Nevado_del_Ruiz" class="authors ga-similar-publication-author">Nevado del Ruíz</a>, <a href="researcher/2006096532_MAURICIO_ALVAREZ" class="authors ga-similar-publication-author">MAURICIO ALVAREZ</a>, <a href="researcher/49545322_RICARDO_HENAO" class="authors ga-similar-publication-author">RICARDO HENAO</a>, <a href="researcher/2012205903_EDISON_DUQUE" class="authors ga-similar-publication-author">EDISON DUQUE</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw39_56ab1e628809b" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw40_56ab1e628809b">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw41_56ab1e628809b" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=ekMTROhX1valLXvDasiDaVe6hyZTIedNIwp2aJZayQhzNYsSuMOz2bsQbPB5lHNI" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="DgGg2b0DC04HXbepkG2UNlm7bPGwqHvhwbFAAbmBxzQVlbMgf1xREGvVwhahRAnIn8mdoJ1NsT60esd0w0NmZIg5Wn4inxO316tRE9Yx48mu5B73ghUJKAjf1PXCDgsC0oVvdq4IbFrneMhA6t9bf5efS8PmO+c2BofkG8rzLJjYXTDx9NswJu65ImU/t6gT9qwV5oUoLCRjl1KDfgZ7pdQgZrniLgjhahapINYnzWhSzmf3BR5EVZP4jVEsLyOiw9asiE5voDGk5bBcnVs53zaJanWnzXB5jsWUs6bkMkc="/> <input type="hidden" name="urlAfterLogin" value="publication/3192933_Bayesian_classification_with_Gaussian_processes"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMzE5MjkzM19CYXllc2lhbl9jbGFzc2lmaWNhdGlvbl93aXRoX0dhdXNzaWFuX3Byb2Nlc3Nlcw%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMzE5MjkzM19CYXllc2lhbl9jbGFzc2lmaWNhdGlvbl93aXRoX0dhdXNzaWFuX3Byb2Nlc3Nlcw%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMzE5MjkzM19CYXllc2lhbl9jbGFzc2lmaWNhdGlvbl93aXRoX0dhdXNzaWFuX3Byb2Nlc3Nlcw%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw42_56ab1e628809b"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 1426;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2198378204065/javascript/min/lib/error_logging.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"David Barber","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272799684100117%401442051921461_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/David_Barber5","institution":"University College London","institutionUrl":false,"widgetId":"rgw4_56ab1e628809b"},"id":"rgw4_56ab1e628809b","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=3244235","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab1e628809b"},"id":"rgw3_56ab1e628809b","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=3192933","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":3192933,"title":"Bayesian classification with Gaussian processes","journalTitle":"IEEE Transactions on Pattern Analysis and Machine Intelligence","journalDetailsTooltip":{"data":{"journalTitle":"IEEE Transactions on Pattern Analysis and Machine Intelligence","journalAbbrev":"IEEE T PATTERN ANAL","publisher":"IEEE Computer Society; Institute of Electrical and Electronics Engineers, Institute of Electrical and Electronics Engineers","issn":"0162-8828","impactFactor":"5.78","fiveYearImpactFactor":"7.76","citedHalfLife":">10.0","immediacyIndex":"0.71","eigenFactor":"0.05","articleInfluence":"3.31","widgetId":"rgw6_56ab1e628809b"},"id":"rgw6_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=0162-8828","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":"Dept. of Artificial Intelligence, Edinburgh Univ.","type":"Article","details":{"doi":"10.1109\/34.735807","journalInfos":{"journal":"","publicationDate":"01\/1999;","publicationDateRobot":"1999-01","article":"20(12):1342 - 1351.","journalTitle":"IEEE Transactions on Pattern Analysis and Machine Intelligence","journalUrl":"journal\/0162-8828_IEEE_Transactions_on_Pattern_Analysis_and_Machine_Intelligence","impactFactor":5.78}},"source":{"sourceUrl":"http:\/\/ieeexplore.ieee.org\/xpl\/freeabs_all.jsp?arnumber=735807","sourceName":"IEEE Xplore"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1109\/34.735807"},{"key":"rft.atitle","value":"Bayesian classification with Gaussian processes"},{"key":"rft.title","value":"Pattern Analysis and Machine Intelligence, IEEE Transactions on"},{"key":"rft.jtitle","value":"Pattern Analysis and Machine Intelligence, IEEE Transactions on"},{"key":"rft.volume","value":"20"},{"key":"rft.issue","value":"12"},{"key":"rft.date","value":"1999"},{"key":"rft.pages","value":"1342 - 1351"},{"key":"rft.issn","value":"0162-8828"},{"key":"rft.au","value":"C.K.I. Williams,David Barber"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56ab1e628809b"},"id":"rgw7_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=3192933","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":3192933,"peopleItems":[{"data":{"authorUrl":"researcher\/6808508_CKI_Williams","authorNameOnPublication":"C.K.I. Williams","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"C.K.I. Williams","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/6808508_CKI_Williams","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw10_56ab1e628809b"},"id":"rgw10_56ab1e628809b","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=6808508&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw9_56ab1e628809b"},"id":"rgw9_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=6808508&authorNameOnPublication=C.K.I.%20Williams","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"David Barber","accountUrl":"profile\/David_Barber5","accountKey":"David_Barber5","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272799684100117%401442051921461_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"David Barber","profile":{"professionalInstitution":{"professionalInstitutionName":"University College London","professionalInstitutionUrl":"institution\/University_College_London"}},"professionalInstitutionName":"University College London","professionalInstitutionUrl":"institution\/University_College_London","url":"profile\/David_Barber5","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272799684100117%401442051921461_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"David_Barber5","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw12_56ab1e628809b"},"id":"rgw12_56ab1e628809b","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=3244235&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University College London","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":2,"accountCount":1,"publicationUid":3192933,"widgetId":"rgw11_56ab1e628809b"},"id":"rgw11_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=3244235&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=2&accountCount=1&publicationUid=3192933","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab1e628809b"},"id":"rgw8_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=3192933&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":3192933,"abstract":"<noscript><\/noscript><div>We consider the problem of assigning an input vector to one of m<br \/>\nclasses by predicting P(c|x) for c=1,...,m. For a two-class problem, the<br \/>\nprobability of class one given x is estimated by &sigma;(y(x)), where<br \/>\n&sigma;(y)=1\/(1+e<sup>-y<\/sup>). A Gaussian process prior is placed on<br \/>\ny(x), and is combined with the training data to obtain predictions for<br \/>\nnew x points. We provide a Bayesian treatment, integrating over<br \/>\nuncertainty in y and in the parameters that control the Gaussian process<br \/>\nprior the necessary integration over y is carried out using Laplace's<br \/>\napproximation. The method is generalized to multiclass problems (m&gt;2)<br \/>\nusing the softmax function. We demonstrate the effectiveness of the<br \/>\nmethod on a number of datasets<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw13_56ab1e628809b"},"id":"rgw13_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=3192933","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/3192933_Bayesian_classification_with_Gaussian_processes\/links\/0e5fab2ef0c41c4932e30cc9\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw5_56ab1e628809b"},"id":"rgw5_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=3192933&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":7304106,"url":"researcher\/7304106_David_Barber","fullname":"David Barber","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2048918802,"url":"researcher\/2048918802_Christopher_K_I_Williams","fullname":"Christopher K. I. Williams","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Aug 1998","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/2338748_Gaussian_Processes_for_Bayesian_Classification_via_Hybrid_Monte_Carlo","usePlainButton":true,"publicationUid":2338748,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/2338748_Gaussian_Processes_for_Bayesian_Classification_via_Hybrid_Monte_Carlo","title":"Gaussian Processes for Bayesian Classification via Hybrid Monte Carlo","displayTitleAsLink":true,"authors":[{"id":7304106,"url":"researcher\/7304106_David_Barber","fullname":"David Barber","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2048918802,"url":"researcher\/2048918802_Christopher_K_I_Williams","fullname":"Christopher K. I. Williams","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/2338748_Gaussian_Processes_for_Bayesian_Classification_via_Hybrid_Monte_Carlo","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/2338748_Gaussian_Processes_for_Bayesian_Classification_via_Hybrid_Monte_Carlo\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw15_56ab1e628809b"},"id":"rgw15_56ab1e628809b","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=2338748","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2016493079,"url":"researcher\/2016493079_C_Williams","fullname":"C. Williams","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2016789686,"url":"researcher\/2016789686_David_Barber","fullname":"David Barber","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 1998","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/243766498_Bayesian_Classification_with_Gaussian_Priors","usePlainButton":true,"publicationUid":243766498,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/243766498_Bayesian_Classification_with_Gaussian_Priors","title":"Bayesian Classification with Gaussian Priors","displayTitleAsLink":true,"authors":[{"id":2016493079,"url":"researcher\/2016493079_C_Williams","fullname":"C. Williams","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2016789686,"url":"researcher\/2016789686_David_Barber","fullname":"David Barber","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/243766498_Bayesian_Classification_with_Gaussian_Priors","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/243766498_Bayesian_Classification_with_Gaussian_Priors\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56ab1e628809b"},"id":"rgw16_56ab1e628809b","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=243766498","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2012227962,"url":"researcher\/2012227962_Nevado_del_Ruiz","fullname":"Nevado del Ru\u00edz","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2006096532,"url":"researcher\/2006096532_MAURICIO_ALVAREZ","fullname":"MAURICIO ALVAREZ","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":49545322,"url":"researcher\/49545322_RICARDO_HENAO","fullname":"RICARDO HENAO","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2012205903,"url":"researcher\/2012205903_EDISON_DUQUE","fullname":"EDISON DUQUE","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/239597440_CLASIFICACION_DE_EVENTOS_SISMICOS_EMPLEANDO_PROCESOS_GAUSSIANOS_Event_Seismic_Classification_using_Gaussian_Processes","usePlainButton":true,"publicationUid":239597440,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/239597440_CLASIFICACION_DE_EVENTOS_SISMICOS_EMPLEANDO_PROCESOS_GAUSSIANOS_Event_Seismic_Classification_using_Gaussian_Processes","title":"CLASIFICACI\u00d3N DE EVENTOS S\u00cdSMICOS EMPLEANDO PROCESOS GAUSSIANOS Event Seismic Classification using Gaussian Processes","displayTitleAsLink":true,"authors":[{"id":2012227962,"url":"researcher\/2012227962_Nevado_del_Ruiz","fullname":"Nevado del Ru\u00edz","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2006096532,"url":"researcher\/2006096532_MAURICIO_ALVAREZ","fullname":"MAURICIO ALVAREZ","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":49545322,"url":"researcher\/49545322_RICARDO_HENAO","fullname":"RICARDO HENAO","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2012205903,"url":"researcher\/2012205903_EDISON_DUQUE","fullname":"EDISON DUQUE","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/239597440_CLASIFICACION_DE_EVENTOS_SISMICOS_EMPLEANDO_PROCESOS_GAUSSIANOS_Event_Seismic_Classification_using_Gaussian_Processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/239597440_CLASIFICACION_DE_EVENTOS_SISMICOS_EMPLEANDO_PROCESOS_GAUSSIANOS_Event_Seismic_Classification_using_Gaussian_Processes\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab1e628809b"},"id":"rgw17_56ab1e628809b","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=239597440","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw14_56ab1e628809b"},"id":"rgw14_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=3192933&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":3192933,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":3192933,"publicationType":"article","linkId":"0e5fab2ef0c41c4932e30cc9","fileName":"Bayesian classification with Gaussian processes","fileUrl":"http:\/\/eprints.aston.ac.uk\/4491\/1\/IEEE_transactions_on_pattern_analysis_20(12).pdf","name":"aston.ac.uk","nameUrl":"http:\/\/eprints.aston.ac.uk\/4491\/1\/IEEE_transactions_on_pattern_analysis_20(12).pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw20_56ab1e628809b"},"id":"rgw20_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=3192933&linkId=0e5fab2ef0c41c4932e30cc9&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw19_56ab1e628809b"},"id":"rgw19_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=3192933&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":5,"valueFormatted":"5","widgetId":"rgw21_56ab1e628809b"},"id":"rgw21_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=3192933","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw18_56ab1e628809b"},"id":"rgw18_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=3192933&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":3192933,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw23_56ab1e628809b"},"id":"rgw23_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=3192933&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":5,"valueFormatted":"5","widgetId":"rgw24_56ab1e628809b"},"id":"rgw24_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=3192933","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw22_56ab1e628809b"},"id":"rgw22_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=3192933&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"1342IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  20,  NO.  12,  DECEMBER  1998\nBayesian Classification\nWith Gaussian Processes\nChristopher K.I. Williams, Member, IEEE Computer Society, and David Barber\nAbstract\u2014We consider the problem of assigning an input vector to one of m classes by predicting P(c|x) for c = 1, \u00ba, m. For a two-\nclass problem, the probability of class one given x is estimated by s(y(x)), where s(y) = 1\/(1 + e-y). A Gaussian process prior is\nplaced on y(x), and is combined with the training data to obtain predictions for new x points. We provide a Bayesian treatment,\nintegrating over uncertainty in y and in the parameters that control the Gaussian process prior; the necessary integration over y is\ncarried out using Laplace\u2019s approximation. The method is generalized to multiclass problems (m > 2) using the softmax function. We\ndemonstrate the effectiveness of the method on a number of datasets.\nIndex Terms\u2014Gaussian processes, classification problems, parameter uncertainty, Markov chain Monte Carlo, hybrid Monte Carlo,\nBayesian classification.\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014???????\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n1 INTRODUCTION\nW\nE consider the problem of assigning an input vector x\nto one out of m classes by predicting P(c|x) for c = 1,\n\u00ba, m. A classic example of this method is logistic regres-\nsion. For a two-class problem, the probability of class 1\ngiven x is estimated by s(wTx + b), where s(y) = 1\/ (1 + e-y).\nHowever, this method is not at all \u201cflexible,\u201d i.e., the dis-\ncriminant surface is simply a hyperplane in x-space. This\nproblem can be overcome, to some extent, by expanding the\ninput x into a set of basis functions {f(x)}, for example\nquadratic functions of the components of x. For a high-\ndimensional input space, there will be a large number of\nbasis functions, each one with an associated parameter, and\none risks \u201coverfitting\u201d the training data. This motivates a\nBayesian treatment of the problem, where the priors on the\nparameters encourage smoothness in the model.\nPutting priors on the parameters of the basis functions\nindirectly induces priors over the functions that can be\nproduced by the model. However, it is possible (and we\nwould argue, perhaps more natural) to put priors directly\nover the functions themselves. One advantage of function-\nspace priors is that they can impose a general smoothness\nconstraint without being tied to a limited number of basis\nfunctions. In the regression case where the task is to predict\na real-valued output, it is possible to carry out nonparametric\nregression using Gaussian Processes (GPs); see, e.g., [25],\n[28]. The solution for the regression problem under a GP\nprior (and Gaussian noise model) is to place a kernel func-\ntion on each training data point, with coefficients deter-\nmined by solving a linear system. If the parameters ? that\ndescribe the Gaussian process are unknown, Bayesian in-\nference can be carried out for them, as described in [28].\nThe Gaussian Process method can be extended to classi-\nfication problems by defining a GP over y, the input to the\nsigmoid function. This idea has been used by a number of\nauthors, although previous treatments typically do not take\na fully Bayesian approach, ignoring uncertainty in both the\nposterior distribution of y given the data, and uncertainty\nin the parameters ?. This paper attempts a fully Bayesian\ntreatment of the problem, and also introduces a particular\nform of covariance function for the Gaussian process prior\nwhich, we believe, is useful from a modeling point of view.\nThe structure of the remainder of the paper is as follows:\nSection 2, discusses the use of Gaussian processes for re-\ngression problems, as this is essential background for the\nclassification case. In Section 3, we describe the application\nof Gaussian processes to two-class classification problems,\nand extend this to multiple-class problems in Section 4. Ex-\nperimental results are presented in Section 5, followed by a\ndiscussion in Section 6. This paper is a revised and ex-\npanded version of [1].\n2 GAUSSIAN PROCESSES FOR REGRESSION\nIt will be useful to first consider the regression problem, i.e.,\nthe prediction of a real valued output y* = y(x*) for a new\ninput value x*, given a set of training data ? = {(xi, ti), i = 1\n\u00ba n}. This is of relevance because our strategy will be to\ntransform the classification problem into a regression\nproblem by dealing with the input values to the logistic\ntransfer function.\nA stochastic process prior over functions allows us to\nspecify, given a set of inputs, x1, \u00ba xn, the distribution over\ntheir corresponding outputs yxxx\n=\ndef\nn\nyyy\n12\n? ? ? ?\n,\n? ?\n??\n,,\nK\n. We\ndenote this prior over functions as P(y), and similarly, P(y*, y)\nfor the joint distribution including y*. If we also specify\nP(t|y), the probability of observing the particular values\n0162-8828\/98\/$10.00 \u00a9 1998 IEEE\n????????????????\n\u2022? C.K.I. Williams is with the Department of Artificial Intelligence, Univer-\nsity of Edinburgh, Edinburgh EH1 2QL, Scotland, UK.\nE-mail: ckiw@dai.ed.ac.uk.\n\u2022? D. Barber is with RWCP, Theoretical Foundation SNN, University of\nNijmegen, 6525 EZ Nijmegen, The Netherlands.\nE-mail: davidb@mbfys.kun.nl.\nManuscript received 1 Dec. 1997; revised 5 Oct. 1998. Recommended for accep-\ntance by A. Webb.\nFor information on obtaining reprints of this article, please send e-mail to:\ntpami@computer.org, and reference IEEECS Log Number 108019."},{"page":2,"text":"WILLIAMS AND BARBER:  BAYESIAN CLASSIFICATION WITH GAUSSIAN PROCESSES1343\nt = (t1, \u00ba tn)T given actual values y (i.e., a noise model),\nthen we have that\n? ??\n,\n? ?? ? ? ? ? ?\n? ? ? ?\nP yP yd\n?\n**\n=\nty t y                                          (1)\n=\n*\n1\nP\nP yPPd\nt\nyy t yy\n                    (2)\n=\n*\nP yPdyy ty.                                   (3)\nHence, the predictive distribution for y* is found from the\nmarginalization of the product of the prior and the noise\nmodel. Note that in order to make predictions, it is not nec-\nessary to deal directly with priors over function space, only\nn- or n + 1-dimensional joint densities. However, it is still\nnot easy to carry out these calculations unless the densities\ninvolved have a special form.\nIf P(t|y) and P(y*, y) are Gaussian, then P(y*|t) is a\nGaussian whose mean and variance can be calculated using\nmatrix computations involving matrices of size n \u00a5 n. Speci-\nfying P(y*, y) to be a multidimensional Gaussian (for all\nvalues of n and placements of the points x*, x1, \u00ba xn) means\nthat the prior over functions is a Gaussian process. More\nformally, a stochastic process is a collection of random vari-\nables {Y(x)|x \u0152 X} indexed by a set X. In our case, X will be\nthe input space with dimension d, the number of inputs. A\nGP is a stochastic process which can be fully specified by its\nmean function m(x) = E[Y(x)] and its covariance function\nC(x, x') = E[(Y(x) - m(x))(Y(x') - m(x'))]; any finite set of Y-\nvariables will have a joint multivariate Gaussian distribu-\ntion. Below we consider GPs which have m(x) \u222b 0.\nIf we further assume that the noise model P(t|y) is\nGaussian with mean zero and variance s2I, then the pre-\ndicted mean and variance at x* are given by\n? ? ? ??\n? ???\nwhere [K]ij = C(xi, xj) and k(x*) = (C(x*, x1), \u00ba, C(x*, xn))T (see,\ne.g., [25]).\n                 $ yKI\nT\nxkxt\n**\n-\n=+\n?\ns2\n1\nss\n$\ny\n,\nT\nCKI\n22\n1\nxxxkxk x\n****\n-\n*\n=-+\n? ??\n? ? ?,\n2.1 Parameterizing the Covariance Function\nThere are many reasonable choices for the covariance func-\ntion. Formally, we are required to specify functions which\nwill generate a non-negative definite covariance matrix for\nany set of points (x1, \u00ba, xk). From a modeling point of view,\nwe wish to specify covariances so that points with nearby\ninputs will give rise to similar predictions. We find that the\nfollowing covariance function works well:\n???\n??\nwhere xl is the lth component of x and ??= (logv0, logv1,\nlogw1, \u00ba, logwd) is the vector of parameters that are\nneeded to define the covariance function. Note that ? is\nanalogous to the hyperparameters in a neural network. We\ndefine the parameters to be the log of the variables in (4)\nsince these are positive scale-parameters. This covariance\nfunction can be obtained from a network of Gaussian radial\nCvw x\nl\nxv\nll\nl\nd\nx x,exp\n\u00a2 =\n?\n-- \u00a2\n???\n??\n+\n=\u00c2\n?\n??\n0\n2\n1\n1\n1\n2\n,            (4)\nbasis functions in the limit of an infinite number of hidden\nunits [27].\nThe wl parameters in (4) allow a different length scale on\neach input dimension. For irrelevant inputs, the corre-\nsponding wl will become small, and the model will ignore\nthat input. This is closely related to the Automatic Rele-\nvance Determination (ARD) idea of MacKay [10] and Neal\n[15]. The v0 variable specifies the overall scale of the prior.\nv1 specifies the variance of a zero-mean offset which has a\nGaussian distribution.\nThe Gaussian process framework allows quite a wide\nvariety of priors over functions. For example, the Ornstein-\nUhlenbeck process (with covariance function C(x, x\u2019) =\ne-|x-x\u2019|) has very rough sample paths which are not mean-\nsquare differentiable. On the other hand, the squared expo-\nnential covariance function of (4) gives rise to an infinitely\nm.s. differentiable process. In general, we believe that the\nGP method is a quite general-purpose route for imposing\nprior beliefs about the desired amount of smoothness. For\nreasonably high-dimensional problems, this needs to be\ncombined with other modeling assumptions such as ARD.\nAnother modeling assumption that may be used is to build\nup the covariance function as a sum of covariance func-\ntions, each one of which may depend on only some of the\ninput variables (see Section 3.3 for further details).\n2.2 Dealing With Parameters\nGiven a covariance function, it is straightforward to make\npredictions for new test points. However, in practical situa-\ntions we are unlikely to know which covariance function to\nuse. One option is to choose a parametric family of covari-\nance functions (with a parameter vector ?), and then either to\nestimate the parameters (for example, using the method of\nmaximum likelihood) or to use a Bayesian approach where a\nposterior distribution over the parameters is obtained.\nThese calculations are facilitated by the fact that the log\nlikelihood l = log P(?|?) can be calculated analytically as\n1\n22\nwhere ~KKI\n=\nIt is also possible to express analytically the partial deriva-\ntives of the log likelihood with respect to the parameters\n?\n?? ??+\n(see, e.g., [11]).\nGiven l and its derivatives with respect to ?, it is straight-\nforward to feed this information to an optimization package\nin order to obtain a local maximum of the likelihood.\nIn general one may be concerned about making point\nestimates when the number of parameters is large relative\nto the number of data points, or if some of the parameters\nmay be poorly determined, or if there may be local maxima\nin the likelihood surface. For these reasons, the Bayesian\napproach of defining a prior distribution over the parame-\nters and then obtaining a posterior distribution once the\ndata ? has been seen is attractive. To make a prediction for\na new test point x* one simply averages over the posterior\ndistribution P(?|?), i.e.,\nlKK\nn\n2\nT\n= ---\n-\n1\n2\n1\nlog~\n~\nlogtt\np ,                  (5)\n+ s2 and ~\nK  denotes the determinant of ~K .\n\u2202\nq\n\u2202\n= -\n\u2202\n\u2202\n?\n\u2202\n\u2202\n---\nl\ntr K\nK\nq\nK\nK\nq\nK\nii\nT\ni\n1\n2\n1\n2\n111\n~\n~\n~\n~\n~\ntt,            (6)"},{"page":3,"text":"1344IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  20,  NO.  12,  DECEMBER  1998\nP y\n?\nP y\n?\nP\n? ? ?\n?\nd\n**\n=\n???\n?\n??\n, .                     (7)\nFor GPs, it is not possible to do this integration analytically\nin general, but numerical methods may be used. If ? is of\nsufficiently low dimension, then techniques involving grids\nin ?-space can be used.\nIf ? is high dimensional, it is very difficult to locate the\nregions of parameter space which have high-posterior den-\nsity by gridding techniques or importance sampling. In this\ncase, Markov chain Monte Carlo (MCMC) methods may be\nused. These work by constructing a Markov chain whose\nequilibrium distribution is the desired distribution P(?|?);\nthe integral in (7) is then approximated using samples from\nthe Markov chain.\nTwo standard methods for constructing MCMC methods\nare the Gibbs sampler and Metropolis-Hastings algorithms\n(see, e.g., [5]). However, the conditional parameter distri-\nbutions are not amenable to Gibbs sampling if the covari-\nance function has the form given by (4), and the Metropolis-\nHastings algorithm does not utilize the derivative informa-\ntion that is available, which means that it tends to have an\ninefficient random-walk behavior in parameter-space. Fol-\nlowing the work of Neal [15] on Bayesian treatment of neu-\nral networks, Williams and Rasmussen [28] and Rasmussen\n[17] have used the Hybrid Monte Carlo (HMC) method of\nDuane et al. [4] to obtain samples from P(?|?). The HMC\nalgorithm is described in more detail in Appendix D.\n3 GAUSSIAN PROCESSES FOR TWO-CLASS\nCLASSIFICATION\nFor simplicity of exposition, we will first present our\nmethod as applied to two-class problems; the extension to\nmultiple classes is covered in Section 4.\nBy using the logistic transfer function to produce an out-\nput which can be interpreted as p(x), the probability of the\ninput x belonging to class 1, the job of specifying a prior\nover functions p can be transformed into that of specifying\na prior over the input to the transfer function, which we\nshall call the activation, and denote by y (see Fig. 1). For the\ntwo-class problem we can use the logistic function p(x) =\ns(y(x)), where s(y) = 1\/ (1 + e-y). We will denote the prob-\nability and activation corresponding to input xi by pi and yi,\nrespectively. Fundamentally, the GP approaches to classifi-\ncation and regression problems are similar, except that the\nerror model which is t ? N(y, s2) in the regression case is\nreplaced by t ? Bern(s(y)). The choice of v0 in (4) will affect\nhow \u201chard\u201d the classification is; i.e., if p(x) hovers around\n0.5 or takes on the extreme values of 0 and 1.\nPrevious and related work to this approach is discussed\nin Section 3.3.\nAs in the regression case, there are now two problems to\naddress\n1)?making predictions with fixed parameters and\n2)?dealing with parameters.\nWe shall discuss these issues in turn.\n3.1 Making Predictions With Fixed Parameters\nTo make predictions when using fixed parameters, we\nwould like to compute $ p\n*\nus to find P(p*|t) = P(p(x*)|t) for a new input x*. This can be\ndone by finding the distribution P(y*|t) (y* is the activation\nof ?*), and then using the appropriate Jacobian to transform\nthe distribution. Formally, the equations for obtaining\nP(y*|t) are identical to (1), (2), and (3). However, even if we\nuse a GP prior so that P(y*, y) is Gaussian, the usual expres-\nsion for P\ni\ni\n?\n\u2019p\n(where the ts take on values of zero or one) means that the\nmarginalization to obtain P(y*|t) is no longer analytically\ntractable.\nFaced with this problem, there are two routes that we\ncan follow:\n1)?to use an analytic approximation to the integral in (1),\n(2), and (3) or\n2)?to use Monte Carlo methods, specifically MCMC\nmethods, to approximate it.\nBelow, we consider an analytic approximation based on\nLaplace\u2019s approximation; some other approximations are\ndiscussed in Section 3.3.\nIn Laplace\u2019s approximation, the integrand P(y*, y|t, ?) is\napproximated by a Gaussian distribution centered at a\nmaximum of this function with respect to y*, y with an in-\nverse covariance matrix given by -\u2014\u2014logP(y*, y|t, ?).\nFinding a maximum can be carried out using the Newton-\nRaphson iterative method on y, which then allows the ap-\nproximate distribution of y* to be calculated. Details of the\nmaximization procedure can be found in Appendix A.\nppp\n***\n=\nPdt\n? ?\n, which requires\nt\ni\nt\ni\ni\nt y ? ?\n?\n=-\n-\np\n1\n1\n for classification data\nfi\n  \nfi\n  \nFig. 1. p(x) is obtained from y(x) by \u201csquashing\u201d it through the sigmoid function s."},{"page":4,"text":"WILLIAMS AND BARBER:  BAYESIAN CLASSIFICATION WITH GAUSSIAN PROCESSES1345\n3.2 Integration Over the Parameters\nTo make predictions we integrate the predicted probabili-\nties over the posterior P(?|t) \u00b5 P(t|?)P(q), as we saw in\nSection 2.2. For the regression problem P(t|?) can be cal-\nculated exactly using Pt\n?\n? ?\ntegral is not analytically tractable for the classification\nproblem. Let Y = log P(t|y) + log P(y). Using P tiyi\nyi\n-+\n?, we obtain\n?\nPPd t y\n? ? ? ?\nyy\n?\n=\n, but this in-\n? ?=\nt y\ni\ne\ni\nlog 1 ?\n                    Y =-+\n=\u00c2\ni\nt y\nT\ny\nn\nei\nlog 1\n1\n?\n---\n-\n1\n2\n1\n22\n2\n1\nyy\nTKK\nn\nloglog p .              (8)\nBy using Laplace\u2019s approximation about the maximum ~y\nwe find that\n? ? ? ? ?\n2\nWe denote the right-hand side of this equation by log\nPa(t|?) (where a stands for approximate).\nThe integration over ?-space also cannot be done ana-\nlytically, and we employ a Markov Chain Monte Carlo\nmethod. Following Neal [15] and Williams and Rasmus-\nsen [28] we have used the Hybrid Monte Carlo (HMC)\nmethod of Duane et al [4] as described in Appendix D. We\nuse log Pa(t|?) as an approximation for log P(t|?), and use\nbroad Gaussian priors on the parameters.\nlog\n~\ny log logPKW\nn\n2\nt\n?Y-++\n-\n1\n2\n1\np.         (9)\n3.3 Previous and Related Work\nOur work on Gaussian processes for regression and classifi-\ncation developed from the observation in [15] that a large\nclass of neural network models converge to GPs in the limit\nof an infinite number of hidden units. The computational\nBayesian treatment of GPs can be easier than for neural\nnetworks. In the regression case, an infinite number of\nweights are effectively integrated out, and one ends up\ndealing only with the (hyper)parameters. Results from [17]\nshow that Gaussian processes for regression are comparable\nin performance to other state-of-the-art methods.\nNonparametric methods for classification problems can\nbe seen to arise from the combination of two different\nstrands of work. Starting from linear regression, McCul-\nlagh and Nelder [12] developed generalized linear models\n(GLMs). In the two-class classification context, this gives\nrise to logistic regression. The other strand of work was\nthe development of nonparametric smoothing for the re-\ngression problem. Viewed as a Gaussian process prior\nover functions this can be traced back at least as far as the\nwork of Kolmogorov and Wiener in the 1940s. Gaussian\nprocess prediction is well known in the geostatistics field\n(see, e.g., [3]) where it is known as \u201ckriging\u201d. Alterna-\ntively, by considering \u201croughness penalties\u201d on functions,\none can obtain spline methods; for recent overviews, see\n[25] and [8]. There is a close connection between the GP\nand roughness penalty views, as explored in [9]. By com-\nbining GLMs with nonparametric regression one obtains\nwhat we shall call a nonparametric GLM method for clas-\nsification. Early references to this method include [21] and\n[16], and discussions can also be found in texts such as [8]\nand [25].\nThere are two differences between the nonparametric\nGLM method as it is usually described and a Bayesian\ntreatment. Firstly, for fixed parameters the nonparametric\nGLM method ignores the uncertainty in y* and, hence, the\nneed to integrate over this (as described in Section 3.1).\nThe second difference relates to the treatment of the pa-\nrameters ?. As discussed in Section 2.2, given parameters ?,\none can either attempt to obtain a point estimate for the\nparameters or to carry out an integration over the posterior.\nPoint estimates may be obtained by maximum likelihood\nestimation of ?, or by cross-validation or generalized cross-\nvalidation (GCV) methods, see e.g., [25], [8]. One problem\nwith CV-type methods is that if the dimension of ? is large,\nthen it can be computationally intensive to search over a\nregion\/ grid in parameter-space looking for the parameters\nthat maximize the criterion. In a sense, the HMC method\ndescribed above is doing a similar search, but using gradi-\nent information,1 and carrying out averaging over the pos-\nterior distribution of parameters. In defense of (G)CV\nmethods, we note Wahba\u2019s comments (e.g., in [26], referring\nback to [24]) that these methods may be more robust\nagainst an unrealistic prior.\nOne other difference between the kinds of non-\nparametric GLM models usually considered and our\nmethod is the exact nature of the prior that is used. Often,\nthe roughness penalties used are expressed in terms of a\npenalty on the kth derivative of y(x), which gives rise to a\npower law power spectrum for the prior on y(x). There can\nalso be differences over parameterization of the covariance\nfunction; for example it is unusual to find parameters like\nthose for ARD introduced in (4) in nonparametric GLM\nmodels. On the other hand, Wahba et al [26] have consid-\nered a smoothing spline analysis of variance (SS-ANOVA)\ndecomposition. In Gaussian process terms, this builds up a\nprior on y as a sum of priors on each of the functions in the\ndecomposition\nx ? ?\n? ?\n\u00c2\na\nyyxyxx\n??\n=+++\n\u00c2\na b\n,\nm\naa\nab\na\nb\n,\nK .         (10)\nThe important point is that functions involving all orders of\ninteraction (from univariate functions, which on their own\ngive rise to an additive model) are included in this sum, up\nto the full interaction term which is the only one that we are\nusing. From a Bayesian point of view, questions as to the\nkinds of priors that are appropriate is an interesting mod-\neling issue.\nThere has also been some recent work which is related\nto the method presented in this paper. In Section 3.1, we\nmentioned that it is necessary to approximate the integral\nin (1), (2), and (3) and described the use of Laplace\u2019s\napproximation.\nFollowing the preliminary version of this paper pre-\nsented in [1], Gibbs and MacKay [7] developed an alterna-\ntive analytic approximation, by using variational methods\nto find approximating Gaussian distributions that bound\nthe marginal likelihood P(t|?) above and below. These\n1. It would be possible to obtain derivatives of the CV-score with respect\nto ?, but this has not, to our knowledge, been used in practice."},{"page":5,"text":"1346 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  20,  NO.  12,  DECEMBER  1998\napproximate distributions are then used to predict P(y*|t, ?)\nand thus $ p x* ? ?. For the parameters, Gibbs and MacKay esti-\nmated ? by maximizing their lower bound on P(t|?).\nIt is also possible to use a fully MCMC treatment of the\nclassification problem, as discussed in the recent paper of\nNeal [14]. His method carries out the integrations over the\nposterior distributions of y and ? simultaneously. It works\nby generating samples from P(y, ?|?) in a two-stage proc-\ness. Firstly, for fixed ?, each of the n individual yis are up-\ndated sequentially using Gibbs sampling. This \u201csweep\u201d\ntakes time O(n2) once the matrix K-1 has been computed (in\ntime O(n3)), so it actually makes sense to perform quite a\nfew Gibbs sampling scans between each update of the pa-\nrameters, as this probably makes the Markov chain mix\nfaster. Secondly, the parameters are updated using the Hy-\nbrid Monte Carlo method. To make predictions, one aver-\nages over the predictions made by each y, ? sample.\n4 GPS FOR MULTIPLE-CLASS CLASSIFICATION\nThe extension of the preceding framework to multiple\nclasses is essentially straightforward, although notationally\nmore complex.\nThroughout we employ a one-of-m class coding scheme,2\nand use the multiclass analogue of the logistic function\u2014\nthe softmax function\u2014to describe the class probabilities.\nThe probability that an instance labeled by i is in class c is\ndenoted by pc\nexample number, and a lower index the class label.\nSimilarly, the activations associated with the probabilities\nare denoted by yc\nrelates the activations and probabilities through\ni, so that an upper index to denotes the\ni. Formally, the softmax link function\npc\ni\nc\ny\ni\ncc\ni\ny\n=\n\u00c2\u00a2\u00a2\nexp\nexp\nwhich automatically enforces the constraint \u00c2\ntargets are similarly represented by tc\nusing a one-of-m coding.\nThe log likelihood takes the form ? ? \u00c2i c c\nfor the softmax link function gives\n=\ncc\ni\np\n1. The\ni, and are specified\ni\nc\ni\nt\n,\nlnp , which\n? ?\nty\nc\ni\nc\ni\nc\ni\nci c\n,\n-\n?\n??\n?\n??\n\u00a2\n\u00a2\u00c2\u00c2\nln exp\np\n.                   (11)\nAs for the two class case, we shall assume that the GP prior\noperates in activation space; that is we specify the correla-\ntions between the activations yc\nOne important assumption we make is that our prior\nknowledge is restricted to correlations between the activa-\ntions of a particular class. Whilst there is no difficulty in\nextending the framework to include interclass correlations,\nwe have not yet encountered a situation where we felt able\nto specify such correlations. Formally, the activation corre-\nlations take the form,\ni.\n2. That is, the class is represented by a vector of length m with zero en-\ntries everywhere except for the correct component which contains 1.\ny y\nc\nK\ni\nc\ni\nc c\nc\ni i\n,,                           (12)\n\u00a2\n\u00a2\n\u00a2\n\u00a2\n= d,\nwhere Kc\nthe cth class. Each individual correlation matrix Kc has the\nform given by (4) for the two-class case. We shall use a\nseparate set of parameters for each class. The use of m in-\ndependent processes to perform the classification is redun-\ndant, but forcing the activations of one process to be, say,\nzero would introduce an arbitrary asymmetry into the\nprior.\nFor simplicity, we introduce the augmented vector notation,\n?\ni i , \u00a2 is the i, i\u2019 element of the covariance matrix for\ny+\n*\n1\n*\n2\n*\nm\n= yyyyyyyyy\nn\n1\nn\n2mc1\n1\n2\n111\n,,,,,,,,,,,,,\nKKKKK\n?,\nwhere, as in the two-class case, yc\ncorresponding to input x* for class c; this notation is also\nused to define t+ and ?+. In a similar manner, we define y, t,\nand ? by excluding the values corresponding to the test\npoint x*. Let y*\n= yyym12\n,,,\nK\n?\nWith this definition of the augmented vectors, the GP\nprior takes the form\n-???\nwhere, from (12), the covariance matrix K+ is block diago-\nnal in the matrices, KKm1\n,,\nK\nexpresses the correlations of activations within class c.\nAs in the two-class case, to use Laplace\u2019s approximation\nwe need to find the mode of P(y+|t). The procedure is de-\nscribed in Appendix C. As for the two-class case, we make\npredictions for ?(x*) by averaging the softmax function over\nthe Gaussian approximation to the posterior distribution of\ny*. At present, we simply estimate this integral using 1,000\ndraws from a Gaussian random vector generator.\n* denotes the activation\n***\n?.\nPK\nT\n+\nyyy\n+\n+-\n+\n\u00b5\n???\n? ?\n? ?\nexp\n1\n2\n1\n,                    (13)\n++\n. Each individual matrix Kc\n+\n5 EXPERIMENTAL RESULTS\nWhen using the Newton-Raphson algorithm, ? was ini-\ntialized each time with entries 1\/ m, and iterated until the\nmean relative difference of the elements of W between con-\nsecutive iterations was less than 10-4.\nFor the HMC algorithm, the same step size e is used for\nall parameters, and should be as large as possible while\nkeeping the rejection rate low. We have used a trajectory\nmade up of L = 20 leapfrog steps, which gave a low cor-\nrelation between successive states. The priors over pa-\nrameters were set to be Gaussian with a mean of -3 and a\nstandard deviation of 3. In all our simulations, a step size\ne = 0.1 produced a low rejection rate (< 5 percent). The\nparameters corresponding to the wls were initialized to -2\nand that for v0 to 0. The sampling procedure was run for\n200 iterations, and the first third of the run was discarded;\nthis \u201cburn-in\u201d is intended to give the parameters time to\ncome close to their equilibrium distribution. Tests carried\nout using the R-CODA package3 on the examples in Section\n5.1 suggested that this was indeed effective in removing\n3. Available from the Comprehensive R Archive Network at\nhttp:\/\/www.ci.tuwien.ac.at."},{"page":6,"text":"WILLIAMS AND BARBER:  BAYESIAN CLASSIFICATION WITH GAUSSIAN PROCESSES 1347\nthe transients, although we note that it is widely recog-\nnized (see, e.g., [2]) that determining when the equilib-\nrium distribution has been reached is a difficult problem.\nAlthough the number of iterations used is much less than\ntypically used for MCMC methods, it should be remem-\nbered that\n1)?each iteration involves L = 20 leapfrog steps and\n2)?that by using HMC we aim to reduce the \u201crandom\nwalk\u201d behavior seen in methods such as the Me-\ntropolis algorithm.\nAutocorrelation analysis for each parameter indicated, in\ngeneral, that low correlation was obtained after a lag of a\nfew iterations.\nThe MATLAB code which we used to run our experi-\nments is available from ftp:\/\/cs.aston.ac.uk\/neural\/willicki\/gpclass\/.\n5.1 Two Classes\nWe have tried out our method on two well-known two-\nclass classification problems, the Leptograpsus crabs and\nPima Indian diabetes datasets.4 We first rescale the inputs\nso that they have mean of zero and unit variance on the\ntraining set. Our Matlab implementations for the HMC\nsimulations for both tasks each take several hours on a\nSGI Challenge machine (200MHz R10000), although good\nresults can be obtained in much less time. We also tried a\nstandard Metropolis MCMC algorithm for the Crabs\nproblem, and found similar results, although the sampling\nby this method is somewhat slower than that for HMC.\nThe results for the Crabs and Pima tasks, together with\ncomparisons with other methods (from [20] and [18]) are\ngiven in Tables 1 and 2, respectively. The tables also include\nresults obtained for Gaussian processes using\n1)?estimation of the parameters by maximizing the pe-\nnalized likelihood (found using 20 iterations of a\nscaled conjugate gradient optimizer) and\n2)?Neal\u2019s MCMC method.\nDetails of the set-up used for Neal\u2019s method are given in\nAppendix E.\nIn the Leptograpsus crabs problem, we attempt to clas-\nsify the sex of crabs on the basis of five anatomical attrib-\nutes, with an optional additional color attribute. There are\n50 examples available for crabs of each sex and color, mak-\ning a total of 200 labeled examples. These are split into a\ntraining set of 20 crabs of each sex and color, making 80\ntraining examples, with the other 120 examples used as the\ntest set. The performance of our GP method is equal to the\nbest of the other methods reported in [20], namely a two\nhidden unit neural network with direct input to output\nconnections, a logistic output unit, and trained with maxi-\nmum likelihood (Network(1) in Table 1). Neal\u2019s method\ngave a very similar level of performance. We also found\nthat estimating the parameters using maximum penalized\nlikelihood (MPL) gave similar performance with less than a\nminute of computing time.\nFor the Pima Indians, diabetes problem we have used\nthe data as made available by Prof. Ripley, with his train-\ning\/ test split of 200 and 332 examples, respectively [18].\n4. Available from http:\/\/markov.stats.ox.ac.uk\/pub\/PRNN.\nThe baseline error obtained by simply classifying each record\nas coming from a diabetic gives rise to an error of 33 per-\ncent. Again, ours and Neal\u2019s GP methods are comparable\nwith the best alternative performance, with an error of\naround 20 percent, as shown in Table 2. It is encouraging\nthat the results obtained using Laplace\u2019s approximation and\nNeal\u2019s method are similar.5 We also estimated the parame-\nters using maximum penalized likelihood, rather than\nMonte Carlo integration. The performance in this case was\na little worse, with 21.7 percent error, but for only two min-\nutes computing time.\nAnalysis of the posterior distribution of the w parame-\nters in the covariance function (4) can be informative. Fig. 2\nplots the posterior marginal mean and one standard devia-\ntion error bars for each of the seven input dimensions. Re-\ncalling that the variables are scaled to have zero mean and\nunit variance, it would appear that variables 1 and 3 have\nthe shortest length scales (and therefore the most variabil-\nity) associated with them.\n5.2 Multiple Classes\nDue to the rather long time taken to run our code, we were\nonly able to test it on relatively small problems, by which\nwe mean only a few hundred data points and several\nclasses. Furthermore, we found that a full Bayesian inte-\ngration over possible parameter settings was beyond our\ncomputational means, and we therefore had to be satisfied\nwith a maximum penalized likelihood approach. Rather\nthan using the potential and its gradient in a HMC routine,\nwe now simply used them as inputs to a scaled conjugate\ngradient optimizer (based on [13]) instead, attempting to\nfind a mode of the class posterior, rather than to average\nover the posterior distribution.\nWe tested the multiple-class method on the Forensic\nGlass dataset described in [18]. This is a dataset of 214 ex-\namples with nine inputs and six output classes. Because the\n5. The performance obtained by Gibbs and MacKay in [7] was similar.\nTheir method made four errors in the crab task (with color given), and 70\nerrors on the Pima dataset.\nTABLE 1\nNUMBER OF TEST ERRORS\nFOR THE LEPTOGRAPSUS CRABS TASK\nMethodColor given\n3\n5\n8\n4\n8\n3\nColor not given\n3\n3\n8\n4\n4\n6\nNeural Network(1)\nNeural Network(2)\nLinear Discriminant\nLogistic regression\nMARS (degree = 1)\nPP regression (4 ridge\nfunctions)\nGaussian Process (Laplace\nApproximation, HMC)\n33\nGaussian Process (Laplace\nApproximation, MPL)\n43\nGaussian Process (Neal\u2019s\nmethod)\nComparisons are taken from from Ripley [18] and Ripley [20], respectively.\nNetwork(2) used two hidden units and the predictive approach (Ripley, [19])\nwhich uses Laplace\u2019s approximation to weight each network local minimum.\n43"},{"page":7,"text":"1348IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  20,  NO.  12,  DECEMBER  1998\ndataset is so small, the performance is estimated from using\n10-fold cross validation. Computing the penalized maxi-\nmum likelihood estimate of our multiple GP method took\napproximately 24 hours on our SGI Challenge and gave a\nclassification error rate of 23.3 percent. As we see from\nTable 3, this is comparable to the best of the other methods.\nThe performance of Neal\u2019s method is surprisingly poor; this\nmay be due to the fact that we allow separate parameters\nfor each of the y processes, while these are constrained to be\nequal in Neal\u2019s code. There are also small but perhaps sig-\nnificant differences in the specification of the prior (see\nAppendix E for details).\n6 DISCUSSION\nIn this paper, we have extended the work of Williams and\nRasmussen [28] to classification problems, and have dem-\nonstrated that it performs well on the datasets we have\ntried. We believe that the kinds of Gaussian process prior\nwe have used are more easily interpretable than models\n(such as neural networks) in which the priors are on the\nparameterization of the function space. For example, the\nposterior distribution of the ARD parameters (as illustrated\nin Fig. 2 for the Pima Indians diabetes problem) indicates\nthe relative importance of various inputs. This interpret-\nability should also facilitate the incorporation of prior\nknowledge into new problems.\nThere are quite strong similarities between GP classifiers\nand support-vector machines (SVMs) [23]. The SVM uses a\ncovariance kernel, but differs from the GP approach by us-\ning a different data fit term (the maximum margin), so that\nthe optimal y is found using quadratic programming. The\ncomparison of these two algorithms is an interesting direc-\ntion for future research.\nA problem with methods based on GPs is that they re-\nquire computations (trace, determinants and linear solu-\ntions) involving n \u00a5 n matrices, where n is the number of\ntraining examples, and hence run into problems on large\ndatasets. We have looked into methods using Bayesian nu-\nmerical techniques to calculate the trace and determinant\n[22], [6], although we found that these techniques did not\nwork well for the (relatively) small size problems on which\nwe tested our methods. Computational methods used to\nspeed up the quadratic programming problem for SVMs\nmay also be useful for the GP classifier problem. We are\nalso investigating the use of different covariance functions\nand improvements on the approximations employed.\nAPPENDIX A\nMAXIMIZING P(y+|t): TWO-CLASS CASE\nWe describe how to find iteratively the vector y+ so that\nP(y+|t) is maximized. This material is also covered in\n[8, Section 5.3.3] and [25, Section 9.2]. We provide it here for\ncompleteness and so that the terms in (9) are well-defined.\nLet y+ denote (y*, y), the complete set of activations. By\nBayes\u2019 theorem log P(y+|t) = log P(t|y) + log P(y+) - log P(t),\nand let Y+ = log P(t|y) + log P(y+). As P(t) does not depend\non y+ (it is just a normalizing factor), the maximum of\nTABLE 2\nNUMBER OF TEST ERRORS ON THE PIMA INDIAN DIABETES TASK\nMethod\nNeural Network\nLinear Discriminant\nLogistic Regression\nMARS (degree = 1)\nPP regression (4 ridge functions)\nGaussian Mixture\nGaussian Process (Laplace\nApproximation, HMC)\nGaussian Process (Laplace\nApproximation, MPL)\nGaussian Process (Neal\u2019s method)\nComparisons are taken from from Ripley [18] and Ripley [20], respectively. The\nneural network had one hidden unit and was trained with maximum likelihood;\nthe results were worse for nets with two or more hidden units (Ripley, [18]).\nPima Indian diabetes\n75+\n67\n66\n75\n75\n64\n68\n69\n68\nTABLE 3\nPERCENTAGE OF TEST ERROR\nFOR THE FORENSIC GLASS PROBLEM\nMethod Forensic Glass\n23.8%\n36%\n32.2%\n35%\n30.8%\n32.2%\n23.3%\n31.8%\nNeural Network (4HU)\nLinear Discriminant\nMARS (degree = 1)\nPP regression (5 ridge functions)\nGaussian Mixture\nDecision Tree\nGaussian Process (LA, MPL)\n Gaussian Process (Neal\u2019s method)\nSee Ripley [18] for details of the methods.\nFig. 2. Plot of the log w parameters for the Pima dataset. The circle\nindicates the posterior marginal mean obtained from the HMC run\n(after burn-in), with one standard deviation error bars. The square\nsymbol shows the log w-parameter values found by maximizing the\npenalized likelihood. The variables are: 1) the number of pregnan-\ncies, 2) plasma glucose concentration, 3) diastolic blood pressure,\n4) triceps skin fold thickness, 5) body mass index, 6) diabetes pedi-\ngree function, 7) age. For comparison, Wahba et al. [26] using gen-\neralized linear regression, found that variables 1, 2, 5, and 6 were\nthe most important."},{"page":8,"text":"WILLIAMS AND BARBER:  BAYESIAN CLASSIFICATION WITH GAUSSIAN PROCESSES1349\nP(y+|t) is found by maximizing Y+ with respect to y+.\nUsing log\nlogP t yt y\niiii\n? ?\n?\n?\ne\nyi\n?\n?\n=-+\n1, we obtain\nY+\n=\n++\n-\n+\n=-+-\n\u00c2\ni\nt yyy\nT\ny\nn\nT\neK\ni\nlog 1\n1\n2\n1\n1\n--\n+\n2\n+\n1\n2\n1\n2 log logK\nn\np ,                        (14)\nwhere K+ is the covariance matrix of the GP evaluated at\nx1, \u00ba xn, x*. Y is defined similarly in (8). K+ can be parti-\ntioned in terms of an n \u00a5 n matrix K, a n \u00a5 1 vector k and a\nscalar k*, viz.\n=???\nk\nAs y* only enters into (14) in the quadratic prior term and\nhas no data point associated with it, maximizing Y+ with\nrespect to y+ can be achieved by first maximizing Y with\nrespect to y and then doing the further quadratic optimiza-\ntion to determine y*. To find a maximum of Y, we use the\nNewton-Raphson iteration ynew = y - (\u2014\u2014Y)-1\u2014Y. Differen-\ntiating (8) with respect to y we find\n\u2014Y = (t - ?) - K-1y                           (16)\nK\nK\nk\nT\n+\n*\n???\nk\n.                               (15)\n\u2014\u2014Y = - K-1 - W,                                 (17)\nwhere the \u201cnoise\u201d matrix is given by W = diag(p1(1 - p1), ..,\npn(1 - pn)). This results in the iterative equation,\nynew = (K-1 + W)-1W(y + W-1(t - ?)).                (18)\nTo avoid unnecessary inversions, it is usually more con-\nvenient to rewrite this in the form\nynew = K(I + W K)-1(Wy + (t - ?)).                  (19)\nNote that -\u2014\u2014Y is always positive definite, so that the op-\ntimization problem is convex.\nGiven a converged solution ~y  for y, y* can easily be\nfound using yK\n*\nkyk\nfrom (16). var(y*) is given by K\n?\nis the W with a zero appended in the (n + 1)th diagonal po-\nsition. Given the mean and variance of y* it is then easy to\nfind $ pppp\n****\nPdt\n? ?\n, the mean of the distribution of\nP(p*|t). In order to calculate the Gaussian integral over the\nlogistic sigmoid function, we employ an approximation\nbased on the expansion of the sigmoid function in terms of\nthe error function. As the Gaussian integral of an error\nfunction is another error function, this approximation is fast\nto compute. Specifically, we use a basis set of five scaled\nerror functions to interpolate the logistic sigmoid at chosen\npoints.6 This gives an accurate approximation (to 10-4) to\nthe desired integral with a small computational cost.\nThe justification of Laplace\u2019s approximation in our case\nis somewhat different from the argument usually put for-\nward, e.g., for asymptotic normality of the maximum\nTT\n-\n==-\nt\n1~\n~?\n?\n-\n?, as K-\n-\n1\n?? ?? ?, where W+\n=-\n1~\ny\n~\n?\nt\n??\nW\nnn\n++\n++\n+\n1\n11\n=\n6. In detail, we used the basis functions erf(lx)) for l = [0.41, 0.4, 0.37,\n0.44, 0.39]. These were used to interpolate s(x) at x = [0, 0.6, 2, 3.5, 4.5, \u2022].\nlikelihood estimator for a model with a finite number of\nparameters. This is because the dimension of the problem\ngrows with the number of data points. However, if we con-\nsider the \u201cinfill asymptotics\u201d (see, e.g., [3]), where the num-\nber of data points in a bounded region increases, then a local\naverage of the training data at any point x will provide a\ntightly localized estimate for p(x) and hence y(x) (this rea-\nsoning parallels more formal arguments found in [29]).\nThus, we would expect the distribution P(y) to become\nmore Gaussian with increasing data.\nAPPENDIX B\nDERIVATIVES OF logPa(t|?)wrt?\nFor both the HMC and MPL methods, we require the de-\nrivative of la = logPa(t|?) with respect to components of ?,\nfor example qk. This derivative will involve two terms, one\ndue to explicit dependencies of\n1\n2\n lKW\nn\n2\na=-++\n-\nY~\nloglogy ? ?\n2\n1\np\non qk, and also because a change in ? will cause a change in\n~y . However, as ~y  is chosen so that?\u2014Y =\n=\ny\ny y\n? ?\n~\n0, we obtain\n\u2202\n\u2202\n=\n\u2202\n\u2202\n-\n\u2202+\n\u2202\n\u2202\n\u2202\n-\n=\u00c2\ni\nll\nKW\ny\ny\nq\na\nk\na\nk\ni\nn\ni\nk\nqq\nexplicit\n1\n2\n1\n1\nlog\n~\n~\n.        (20)\nThe dependence of |K-1 + W| on ~y  arises through the de-\npendence of W on ~? , and, hence, ~y . By differentiating\n~\n~\ny\nt\n=-\nK\n?\n??, one obtains\n\u2202\n\u2202\nq\nk\n??\nand, hence, the required derivative can be calculated.\n=+\n\u2202\n\u2202\n-\n-\n~\ny\n~\n? ,                    (21)\nt\nq\nk\nIKW\nK\n??\n1\nAPPENDIX C\nMAXIMIZING P(y+|t): MULTIPLE-CLASS CASE\nThe GP prior and likelihood, defined by (13) and (11), de-\nfine the posterior distribution of activations, P(y+|t). As in\nAppendix A we are interested in a Laplace approximation\nto this posterior, and therefore need to find the mode with\nrespect to y+. Dropping unnecessary constants, the multi-\nclass analogue of (14) is\n1\n22\nY+++\n-\n++\n= --+-\u00c2 \u00c2\ni\n1\n1\nyyt y\nTT\nc\ni\nc\nKKyloglnexp\n.\nBy the same principle as in Appendix A, we define Y by\nanalogy with (8), and first optimize Y with respect to y,\nafterwards performing the quadratic optimization of Y+\nwith respect to y*.\nIn order to optimize ? with respect to y, we make use of\nthe Hessian given by\n\u2014\u2014Y = -K-1 - W,                               (22)\nwhere K is the mn \u00a5 mn block-diagonal matrix with blocks\nKc, c = 1, \u00ba, m. Although this is in the same form as for the\ntwo-class case, (17), there is a slight change in the definition\nof the \u201cnoise\u201d matrix, W. A convenient way to define W is"},{"page":9,"text":"1350IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,  VOL.  20,  NO.  12,  DECEMBER  1998\nby introducing the matrix P which is a mn \u00a5 n matrix of the\nform \u2019 = diagdiag\npp\n11\n..,,\n???\nK\ntion, we can write the noise matrix in the form of a diagonal\nmatrix and an outer product,\n?\nn\nmm\nn\npp\n11\n..\n???\n. Using this nota-\nW diag\nn\n1\nmm\nnT\n= - + \u2019\u2019pppp\n1\n11\n..,..,..\n?\n.             (23)\nAs in the two-class case, we note that -\u2014\u2014Y is again posi-\ntive definite, so that the optimization problem is convex.\nThe update equation for iterative optimization of Y\nwith respect to the activations y then follows the same\nform as that given by (18). The advantage of the repre-\nsentation of the noise matrix in (23) is that we can then\ninvert matrices and find their determinants using the\nidentities,\n(A + \u2019\u2019 T)-1 = A-1 - A-1\u2019(In + \u2019TA-1\u2019)-1\u2019TA-1       (24)\nand\ndet(A + \u2019\u2019T) = det(A)det(In + \u2019TA-1\u2019),            (25)\n??. As A is block-diagonal, it\ncan be inverted blockwise. Thus, rather than requiring\ndeterminants and inverses of a mn \u00a5 mn matrix, we only\nneed to carry out expensive matrix computations on n \u00a5\nn matrices. The resulting update equations for y are then\nof the same form as given in (18), where the noise matrix\nand covariance matrices are now in their multiple class\nform.\nEssentially, these are all the results needed to generalize\nthe method to the multiple-class problem. Although, as we\nmentioned above, the time complexity of the problem does\nnot scale with the m3, but rather m (due to the identities in\n(24), (25)), calculating the function and its gradient is still\nrather expensive. We have experimented with several\nmethods of mode finding for the Laplace approximation.\nThe advantage of the Newton iteration method is its fast\nquadratic convergence. An integral part of each Newton\nstep is the calculation of the inverse of a matrix M acting\nupon a vector, i.e., M-1b. In order to speed up this particu-\nlar step, we used a conjugate gradient (CG) method to solve\niteratively the corresponding linear system Mz = b. As we\nrepeatedly need to solve the system (because W changes as\ny is updated), it saves time not to run the CG method to\nconvergence each time it is called. In our experiments, the\nCG algorithm was terminated when 1\nwhere r = Mz - b.\nThe calculation of the derivative of logPa(t|?) wrt ? in\nthe multiple-class case is analogous to the two-class case\ndescribed in Appendix B.\nwhere AK diag\nm\nn\n=+\n-1\n1\n1\npp\n..\n10\n1\n9\nnr\ni\nn\ni\n\u00c2<\n=\n-\n,\nAPPENDIX D\nHYBRID MONTE CARLO\nHMC works by creating a fictitious dynamical system in\nwhich the parameters are regarded as position variables,\nand augmenting these with momentum variables p. The\npurpose of the dynamical system is to give the parameters\n\u201cinertia\u201d so that random-walk behavior in ?-space can be\navoided. The total energy, H, of the system is the sum of the\nkinetic energy, K = pTp\/ 2 and the potential energy, E. The\npotential energy is defined such that p(?|D) \u00b5 exp(-E),\ni.e., E = - logP(t|?) - log P(?). We sample from the joint\ndistribution for ? and p given by P(?, p) \u00b5 exp(-E - K); the\nmarginal of this distribution for ? is the required posterior.\nA sample of parameters from the posterior can therefore\nbe obtained by simply ignoring the momenta.\nSampling from the joint distribution is achieved by two steps:\n1)?finding new points in phase space with near-identical\nenergies H by simulating the dynamical system using\na discretised approximation to Hamiltonian dynamics\nand\n2)?changing the energy H by Gibbs sampling the mo-\nmentum variables.\nHamilton\u2019s first-order differential equations for H are\napproximated using the leapfrog method which requires\nthe derivatives of E with respect to ?. Given a Gaussian\nprior on ?, logP(?) is straightforward to differentiate. The\nderivative of logPa(t|?) is also straightforward, although\nimplicit dependencies of ~y  (and, hence, ~p ) on ? must be\ntaken into account as described in Appendix B. The calcu-\nlation of the energy can be quite expensive as for each new\n?, we need to perform the maximization required for\nLaplace\u2019s approximation, (9). This proposed state is then\naccepted or rejected using the Metropolis rule depending\non the final energy H* (which is not necessarily equal to the\ninitial energy H because of the discretization).\nAPPENDIX E\nSIMULATION SET-UP FOR NEAL\u2019S CODE\nWe used the fbm software available from http:\/\/www.cs.utoronto.\nca\/radford\/fbm.software.html. For example, the commands used\nto run the Pima example are\ngp-spec pima1.log 7 1 - - 0.1 \/ 0.05:0.5\nx0.2:0.5:1\nmodel-spec pima1.log binary\npima1.log 7 1 2 \nmypima.te@1:332.\ngp-gen pima1.log fix 0.5 1\nmc-spec pima1.log repeat 4 scan-values\n200 heatbath hybrid 6 0.5\ngp-mc pima1.log 500\n\/  pima1.tr@1:200.\nwhich follow closely the example given in Neal\u2019s docu-\nmentation.\nThe gp-spec command specifies the form of the Gaus-\nsian process, and in particular the priors on the parameters\nv0 and the ws (see (4)). The expression 0.05:0.5 specifies a\nGamma-distribution prior on v0, and x0.2:0.5:1 specifies\na hierarchical Gamma prior on the ws. Note that a \u201cjitter\u201d\nof 0.1 is also specified on the prior covariance function;\nthis improves conditioning of the covariance matrix.\nThe mc-spec command gives details of the MCMC up-\ndating procedure. It specifies four repetitions of 200 scans of\nthe y values followed by six HMC updates of the parameters\n(using a step-size adjustment factor of 0.5). gp-mc specifies\nthat this is sequence is carried out 500 times."},{"page":10,"text":"WILLIAMS AND BARBER:  BAYESIAN CLASSIFICATION WITH GAUSSIAN PROCESSES1351\nWe aimed for a rejection rate of around 5 percent. If this\nwas exceeded, the stepsize reduction factor was reduced\nand the simulation run again.\nACKNOWLEDGMENTS\nWe thank Prof. B. Ripley for making available the Lepto-\ngrapsus crabs, Pima Indian diabetes, and Forensic Glass\ndatasets. This work was partially supported by EPSRC\ngrant GR\/ J75425, Novel Developments in Learning Theory for\nNeural Networks, and much of the work was carried out at\nAston University. The authors gratefully acknowledge the\nhospitality provided by the Isaac Newton Institute for\nMathematical Sciences (Cambridge, UK) where this paper\nwas written. We thank Mark Gibbs, David MacKay, and\nRadford Neal for helpful discussions, and the anonymous\nreferees for their comments which helped improve the\npaper.\nREFERENCES\n[1]? D. Barber and C.K.I Williams, \u201cGaussian Processes for Bayesian\nClassification via Hybrid Monte Carlo,\u201d M.C. Mozer, M.I. Jordan,\nand T. Petsche, eds., Advances in Neural Information Processing Sys-\ntems 9. MIT Press, 1997.\n[2]? M.K. Cowles and B.P. Carlin, \u201dMarkov-Chain Monte-Carlo Con-\nvergence Diagnostics\u2014A Comparative Review,\u201d J. Am. Statistics\nAssoc., vol. 91, pp. 883-904, 1996.\n[3]? N.A.C. Cressie, Statistics for Spatial Data. New York, NY: Wiley,\n1993.\n[4]? S. Duane, A.D. Kennedy, B.J. Pendleton, and D. Roweth, \u201cHybrid\nMonte Carlo,\u201d Physics Letters B, vol. 195, pp. 216-222, 1987.\n[5]? A. Gelman, J.B. Carlin, H.S. Stern, and D.B. Rubin, Bayesian Data\nAnalysis. London: Chapman and Hall, 1995.\n[6]? M. Gibbs and D.J.C. MacKay, \u201cEfficient Implementation of Gaus-\nsian Processes,\u201d Draft manuscript, available from http:\/\/wol.ra.phy.\ncam.ac.uk\/mackay\/homepage.html., 1997.\n[7]? M. Gibbs and D.J.C. MacKay, \u201cVariational Gaussian Process Clas-\nsifiers,\u201d Draft manuscript, available via http:\/\/wol.ra.phy.cam.ac.uk\/\nmackay\/homepage.html., 1997.\n[8]? P. J. Green and B. W. Silverman, Nonparametric Regression and Gen-\neralized Linear Models. London: Chapman and Hall, 1994.\n[9]? G. Kimeldorf and G. Wahba, \u201cA Correspondence Between Baye-\nsian Estimation of Stochastic Processes and Smoothing by\nSplines,\u201d Annals of Math. Statistics, vol. 41, pp. 495-502, 1970.\n[10]? D.J.C. MacKay, \u201cBayesian Methods for Backpropagation Net-\nworks,\u201d J.L. van Hemmen, E. Domany, and K. Schulten, eds.,\nModels of Neural Networks II. Springer, 1993.\n[11]? K.V. Mardia and R.J. Marshall, \u201cMaximum Likelihood Estimation\nfor Models of Residual Covariance in Spatial Regression,\u201d Biome-\ntrika, vol. 71, no. 1, pp. 135-146, 1984.\n[12]? P. McCullagh, and J. Nelder, Generalized Linear Models. Chapman\nand Hall, 1983.\n[13]? M. M\u00f8ller, \u201cA Scaled Conjugate Gradient Algorithm for Fast Su-\npervised Learning,\u201d Neural Networks, vol. 6, no. 4, pp. 525-533,\n1993.\n[14]? R.M. Neal, \u201cMonte Carlo Implementation of Gaussian Process\nModels for Bayesian Regression and Classification,\u201d Technical\nReport 9702, Dept. of Statistics, Univ. of Toronto, 1997. Available\nfrom http:\/\/www.cs.toronto.edu\/~radford\/.\n[15]? R.M. Neal, Bayesian Learning for Neural Networks. New York,\nSpringer, 1996. Lecture Notes in Statistics 118.\n[16]? F. O\u2019Sullivan, B.S. Yandell, and W.J. Raynor, \u201cAutomatic Smooth-\ning of Regression Functions in Generalized Linear Models,\u201d J. Am.\nStatistical Assoc., vol. 81, pp. 96-103, 1986.\n[17]? C.E. Rasmussen, Evaluation of Gaussian Processes and Other Methods\nfor Non-Linear Regression. PhD thesis, Dept. of Computer Science,\nUniv. of Toronto, 1996. Available from http:\/\/www.cs.utoronto.ca\/~carl\/.\n[18]? B. Ripley, Pattern Recognition and Neural Networks. Cambridge, UK:\nCambridge Univ. Press, 1996.\n[19]? B.D. Ripley, \u201cStatistical Aspects of Neural Networks,\u201d O.E. Barn-\ndorff-Nielsen, J.L. Jensen, and W.S. Kendall, eds., Networks and\nChaos\u2014Statistical and Probabilistic Aspects, pp. 40-123. Chapman\nand Hall, 1993.\n[20]? B.D. Ripley, \u201cFlexible Non-Linear Approaches to Classification,\u201d\nV. Cherkassy, J.H. Friedman, and H. Wechsler, eds., From Statistics\nto Neural Networks, pp. 105-126. Springer, 1994.\n[21]? B.W. Silverman, \u201cDensity Ratios, Empirical Likelihood and Cot\nDeath,\u201d Applied Statistics, vol. 27, no. 1, pp. 26-33, 1978.\n[22]? J. Skilling, \u201cBayesian Numerical Analysis,\u201d W.T. Grandy, Jr. and P.\nMilonni, eds., Physics and Probability. Cambridge Univ. Press, 1993.\n[23]? V.N. Vapnik, The Nature of Statistical Learning Theory. New York,\nNY: Springer Verlag, 1995.\n[24]? G. Wahba, \u201cA Comparison of GCV and GML for Choosing the\nSmoothing Parameter in the Generalized Spline Smoothing Prob-\nlem,\u201d Annals of Statistics, vol. 13, pp. 1,378-1,402, 1985.\n[25]? G. Wahba, Spline Models for Observational Data. Soc. Industrial and\nApplied Mathematics, 1990. CBMS-NSF Regional Conf. Series in\nApplied Mathematics.\n[26]? G. Wahba, C. Gu, Y. Wang, and R. Chappell, \u201cSoft Classification,\na.k.a., Risk Estimation, via Penalized Log Likelihood and\nSmoothing Spline Analysis of Variance,\u201d D.H. Wolpert, ed., The\nMathematics of Generalization. Addison-Wesley, 1995. Proc. vol. XX\nin the Santa Fe Institute Studies in the Sciences of Complexity.\n[27]? C.K.I. Williams, \u201cComputing With Infinite Networks,\u201d M.C.\nMozer, M.I. Jordan, and T. Petsche, eds., Advances in Neural Infor-\nmation Processing Systems 9. MIT Press, 1997.\n[28]? C.K.I. Williams and C.E. Rasmussen, \u201cGaussian Processes for\nRegression,\u201d D.S. Touretzky, M.C. Mozer, and M.E. Hasselmo,\neds., Advances in Neural Information Processing Systems 8, pp. 514-\n520. MIT Press, 1996.\n[29]? S.J. Yakowitz and F. Szidarovszky, \u201cA Comparison of Kriging\nWith Nonparametric Regression Methods,\u201d J. Multivariate Analy-\nsis, vol. 16, pp. 21-53, 1985.\nChris Williams studied physics at Cambridge,\ngraduating in 1982 and continued on to do Part\nIII Maths (1983). He then received an MSc in\nwater resources at the University of Newcastle\nupon Tyne before going to work in Lesotho,\nSouthern Africa in low-cost sanitation. In 1988,\nhe returned to academia, studying neural net-\nworks\/artificial intelligence with Geoff Hinton at\nthe University of Toronto (MSc 1990, PhD 1994).\nHe was a member of the Neural Computing\nResearch Group at Aston University from 1994\nto 1998, and is currently a lecturer in the Division of Informatics at the\nUniversity of Edinburgh.\nHis research interests cover a wide range of theoretical and practi-\ncal issues in neural networks, statistical pattern recognition, computer\nvision, and artificial intelligence.\nDavid Barber completed the University of Cam-\nbridge mathematics tripos in 1990. Following a\nyear at Heidelberg University, Germany, he re-\nceived an MSc in neural networks at King\u2019s Col-\nlege, London. Subsequently, he went to the Uni-\nversity of Edinburgh, completing a PhD in the\ntheoretical physics department with a study of\nthe statistical mechanics of machine learning.\nAfter a two-year research position at Aston Uni-\nversity, he took up his current research position\nat the University of Nijmegen, Holland. His main\ninterests include machine learning, Bayesian techniques, and statisti-\ncal mechanics."}],"widgetId":"rgw25_56ab1e628809b"},"id":"rgw25_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=3192933&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw26_56ab1e628809b"},"id":"rgw26_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=3192933&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":3192933,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":3192933,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2034167984,"url":"researcher\/2034167984_Ulrich_Schaechtle","fullname":"Ulrich Schaechtle","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277112939401216%401443080281088_m"},{"id":2089456342,"url":"researcher\/2089456342_Ben_Zinberg","fullname":"Ben Zinberg","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089399536,"url":"researcher\/2089399536_Alexey_Radul","fullname":"Alexey Radul","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":8074903,"url":"researcher\/8074903_Kostas_Stathis","fullname":"Kostas Stathis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Dec 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization","usePlainButton":true,"publicationUid":287249271,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization","title":"Probabilistic Programming with Gaussian Process Memoization","displayTitleAsLink":true,"authors":[{"id":2034167984,"url":"researcher\/2034167984_Ulrich_Schaechtle","fullname":"Ulrich Schaechtle","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277112939401216%401443080281088_m"},{"id":2089456342,"url":"researcher\/2089456342_Ben_Zinberg","fullname":"Ben Zinberg","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089399536,"url":"researcher\/2089399536_Alexey_Radul","fullname":"Alexey Radul","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8074903,"url":"researcher\/8074903_Kostas_Stathis","fullname":"Kostas Stathis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089392273,"url":"researcher\/2089392273_Vikash_K_Mansinghka","fullname":"Vikash K. Mansinghka","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Gaussian Processes (GPs) are widely used tools in statistics, machine\nlearning, robotics, computer vision, and scientific computation. However,\ndespite their popularity, they can be difficult to apply; all but the simplest\nclassification or regression applications require specification and inference\nover complex covariance functions that do not admit simple analytical\nposteriors. This paper shows how to embed Gaussian processes in any\nhigher-order probabilistic programming language, using an idiom based on\nmemoization, and demonstrates its utility by implementing and extending classic\nand state-of-the-art GP applications. The interface to Gaussian processes,\ncalled gpmem, takes an arbitrary real-valued computational process as input and\nreturns a statistical emulator that automatically improve as the original\nprocess is invoked and its input-output behavior is recorded. The flexibility\nof gpmem is illustrated via three applications: (i) robust GP regression with\nhierarchical hyper-parameter learning, (ii) discovering symbolic expressions\nfrom time-series data by fully Bayesian structure learning over kernels\ngenerated by a stochastic grammar, and (iii) a bandit formulation of Bayesian\noptimization with automatic inference and action selection. All applications\nshare a single 50-line Python library and require fewer than 20 lines of\nprobabilistic code each.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Kostas_Stathis\/publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization\/links\/5687eb7108ae051f9af5a28a.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Kostas_Stathis","sourceName":"Kostas Stathis","hasSourceUrl":true},"publicationUid":287249271,"publicationUrl":"publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization\/links\/5687eb7108ae051f9af5a28a\/smallpreview.png","linkId":"5687eb7108ae051f9af5a28a","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=287249271&reference=5687eb7108ae051f9af5a28a&eventCode=&origin=publication_list","widgetId":"rgw30_56ab1e628809b"},"id":"rgw30_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=287249271&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"5687eb7108ae051f9af5a28a","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":3192933,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Gaussian Processes (GPs) are widely used tools in statistics (Barry, 1986), machine learning (Neal, 1995; Williams and Barber, 1998; Kuss and Rasmussen, 2005; Rasmussen and Williams, 2006; Damianou and Lawrence, 2013), robotics (Ferris et al., 2006), computer vision (Kemmler et al., 2013), and scientific computation (Kennedy and O&apos;Hagan, 2001; Schneider et al., 2008; Kwan et al., 2013). They are also central to probabilistic numerics, an emerging effort to develop more computationally efficient numerical procedures, and to Bayesian optimization, a family of meta-optimization techniques that are widely used to tune parameters for deep learning algorithms (Snoek et al., 2012; Gelbart et al., 2014). "],"widgetId":"rgw31_56ab1e628809b"},"id":"rgw31_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw29_56ab1e628809b"},"id":"rgw29_56ab1e628809b","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=287249271&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2054065797,"url":"researcher\/2054065797_Calum_Blair","fullname":"Calum Blair","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272752682729486%401442040715308_m\/Calum_Blair.png"},{"id":2054134769,"url":"researcher\/2054134769_John_Thompson","fullname":"John Thompson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2048219205,"url":"researcher\/2048219205_Neil_M_Robertson","fullname":"Neil M Robertson","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Conference Paper","publicationDate":"Sep 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/281451121_GPU-Accelerated_Gaussian_Processes_for_Object_Detection","usePlainButton":true,"publicationUid":281451121,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/281451121_GPU-Accelerated_Gaussian_Processes_for_Object_Detection","title":"GPU-Accelerated Gaussian Processes for Object Detection","displayTitleAsLink":true,"authors":[{"id":2054065797,"url":"researcher\/2054065797_Calum_Blair","fullname":"Calum Blair","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272752682729486%401442040715308_m\/Calum_Blair.png"},{"id":2054134769,"url":"researcher\/2054134769_John_Thompson","fullname":"John Thompson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2048219205,"url":"researcher\/2048219205_Neil_M_Robertson","fullname":"Neil M Robertson","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Sensor Signal Processing for Defence (SSPD 2015); 09\/2015"],"abstract":"Gaussian Process classification (GPC) allows accurate and reliable detection of objects. The high computational load of squared-error or radial basis function kernels limits the applications that GPC can be used in, as memory requirements and computation time are both limiting factors. We describe our version of accelerated GPC on GPU (Graphics Processing Unit). GPUs have limited memory so any GPC implementation must be memory-efficient as well as computationally efficient. Using a high-performance pedestrian detector as a starting point, we use its packed or block-based feature descriptor and demonstrate a fast matrix multiplication implementation of GPC which is also extremely memory efficient. We demonstrate a speed up of 3.7 times over a multicore, BLAS-optimised CPU implementation. Results show that this is more accurate and reliable than results obtained from a comparable support vector machine algorithm.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/281451121_GPU-Accelerated_Gaussian_Processes_for_Object_Detection","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Calum_Blair\/publication\/281451121_GPU-Accelerated_Gaussian_Processes_for_Object_Detection\/links\/55e851be08ae3e1218422e18.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Calum_Blair","sourceName":"Calum Blair","hasSourceUrl":true},"publicationUid":281451121,"publicationUrl":"publication\/281451121_GPU-Accelerated_Gaussian_Processes_for_Object_Detection","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/281451121_GPU-Accelerated_Gaussian_Processes_for_Object_Detection\/links\/55e851be08ae3e1218422e18\/smallpreview.png","linkId":"55e851be08ae3e1218422e18","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=281451121&reference=55e851be08ae3e1218422e18&eventCode=&origin=publication_list","widgetId":"rgw33_56ab1e628809b"},"id":"rgw33_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=281451121&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"55e851be08ae3e1218422e18","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":3192933,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/281451121_GPU-Accelerated_Gaussian_Processes_for_Object_Detection\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Evaluating this directly as in (3) is intractable [15], so we use a Laplacian approximation (from [13, Ch.3 \u00a74]), which allows the posterior over the training data and labels in (3) to be approximated as a Gaussian: "],"widgetId":"rgw34_56ab1e628809b"},"id":"rgw34_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw32_56ab1e628809b"},"id":"rgw32_56ab1e628809b","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=281451121&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2079162962,"url":"researcher\/2079162962_Xiaoyu_Xiong","fullname":"Xiaoyu Xiong","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A309409379487746%401450780352548_m\/Xiaoyu_Xiong.png"},{"id":82161005,"url":"researcher\/82161005_Vaclav_Smidl","fullname":"V\u00e1clav \u0160m\u00eddl","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70871340,"url":"researcher\/70871340_Maurizio_Filippone","fullname":"Maurizio Filippone","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Aug 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes","usePlainButton":true,"publicationUid":280773011,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes","title":"Adaptive Multiple Importance Sampling for Gaussian Processes","displayTitleAsLink":true,"authors":[{"id":2079162962,"url":"researcher\/2079162962_Xiaoyu_Xiong","fullname":"Xiaoyu Xiong","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A309409379487746%401450780352548_m\/Xiaoyu_Xiong.png"},{"id":82161005,"url":"researcher\/82161005_Vaclav_Smidl","fullname":"V\u00e1clav \u0160m\u00eddl","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70871340,"url":"researcher\/70871340_Maurizio_Filippone","fullname":"Maurizio Filippone","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"In applications of Gaussian processes where quantification of uncertainty is\na strict requirement, it is necessary to accurately characterize the posterior\ndistribution over Gaussian process covariance parameters. Normally, this is\ndone by means of Markov chain Monte Carlo (MCMC) algorithms. Focusing on\nGaussian process regression where the marginal likelihood is computable but\nexpensive to evaluate, this paper studies algorithms based on importance\nsampling to carry out expectations under the posterior distribution over\ncovariance parameters. The results indicate that expectations computed using\nAdaptive Multiple Importance Sampling converge faster per unit of computation\nthan those computed with MCMC algorithms for models with few covariance\nparameters, and converge as fast as MCMC for models with up to around twenty\ncovariance parameters.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Vasek_Smidl\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/links\/55e94dff08ae65b6389aee89.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Vasek_Smidl","sourceName":"Vasek Smidl","hasSourceUrl":true},"publicationUid":280773011,"publicationUrl":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/links\/55e94dff08ae65b6389aee89\/smallpreview.png","linkId":"55e94dff08ae65b6389aee89","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=280773011&reference=55e94dff08ae65b6389aee89&eventCode=&origin=publication_list","widgetId":"rgw36_56ab1e628809b"},"id":"rgw36_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=280773011&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"55e94dff08ae65b6389aee89","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":3192933,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Inference of GP covariance parameters is analytically intractable, and standard inference methods require repeatedly calculating the so called marginal likelihood . When the likelihood function is not Gaussian, e.g., in classification, in ordinal regression, in modeling of stochastic volatility, in Cox-processes, the marginal likelihood cannot be computed analytically, and this has motivated a large body of the literature to develop approximate inference methods [56] [43] [31] [47] [41] [24], reparameterization techniques [36] [34] [54] [14], and exact inference with unbiased computations of the marginal likelihood [12] [10]. Even in the case of a Gaussian likelihood, which makes the marginal likelihood computable, inference is generally costly because the computation of the marginal likelihood has time complexity scaling with the cube of the number of input vectors [11]. "],"widgetId":"rgw37_56ab1e628809b"},"id":"rgw37_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw35_56ab1e628809b"},"id":"rgw35_56ab1e628809b","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=280773011&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":3192933,"publicationLink":"publication\/3192933_Bayesian_classification_with_Gaussian_processes","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw28_56ab1e628809b"},"id":"rgw28_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=3192933&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=312","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":312,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw27_56ab1e628809b"},"id":"rgw27_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=3192933&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/3192933_Bayesian_classification_with_Gaussian_processes","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1e628809b"},"id":"rgw2_56ab1e628809b","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":3192933},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=3192933&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1e628809b"},"id":"rgw1_56ab1e628809b","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"nc1v+q2WXctKFFkT2MhnCsJ2zebIoMD2vjmFPSpl06EDttzYzfz2pNOrro\/c2YEtr9zRw95uFeHKqfErB3dQjHtBCjpQnQAIekDzU20+vpjP5btaCwlIGM0XlpnH2rQqIEWJeCIqxRNvbLgwBDaYuLK\/u2tiPv\/RMmyPnUxzvd3SO3aINiY4ld7SJwXDAhWiTjlcFRmBDeGZC+DdKlSjxhiKGMqbYXU4Wedm9I8tcbZaIyBXF4zGiLoyraNpkHYqKTvnwx0bkRodLXutwQsL4bVu1xePZVOGuYRya8G2PQ8=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/3192933_Bayesian_classification_with_Gaussian_processes\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Bayesian classification with Gaussian processes\" \/>\n<meta property=\"og:description\" content=\"We consider the problem of assigning an input vector to one of m\nclasses by predicting P(c|x) for c=1,...,m. For a two-class problem, the\nprobability of class one given x is estimated by \u03c3(y(x)),...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/3192933_Bayesian_classification_with_Gaussian_processes\/links\/0e5fab2ef0c41c4932e30cc9\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/3192933_Bayesian_classification_with_Gaussian_processes\" \/>\n<meta property=\"rg:id\" content=\"PB:3192933\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1109\/34.735807\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Bayesian classification with Gaussian processes\" \/>\n<meta name=\"citation_author\" content=\"C.K.I. Williams\" \/>\n<meta name=\"citation_author\" content=\"David Barber\" \/>\n<meta name=\"citation_publication_date\" content=\"1999\/01\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"IEEE Transactions on Pattern Analysis and Machine Intelligence\" \/>\n<meta name=\"citation_issn\" content=\"0162-8828\" \/>\n<meta name=\"citation_volume\" content=\"20\" \/>\n<meta name=\"citation_issue\" content=\"12\" \/>\n<meta name=\"citation_firstpage\" content=\"1342\" \/>\n<meta name=\"citation_lastpage\" content=\"1351\" \/>\n<meta name=\"citation_doi\" content=\"10.1109\/34.735807\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/3192933_Bayesian_classification_with_Gaussian_processes\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/3192933_Bayesian_classification_with_Gaussian_processes\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-c3c669c2-24fa-4bb8-b98e-c8d71a067f3c","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":1409,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw38_56ab1e628809b"},"id":"rgw38_56ab1e628809b","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-c3c669c2-24fa-4bb8-b98e-c8d71a067f3c", "c215ed7a6d1b4f4a31c974e2b8179a63279047aa");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-c3c669c2-24fa-4bb8-b98e-c8d71a067f3c", "c215ed7a6d1b4f4a31c974e2b8179a63279047aa");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw39_56ab1e628809b"},"id":"rgw39_56ab1e628809b","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/3192933_Bayesian_classification_with_Gaussian_processes","requestToken":"DgGg2b0DC04HXbepkG2UNlm7bPGwqHvhwbFAAbmBxzQVlbMgf1xREGvVwhahRAnIn8mdoJ1NsT60esd0w0NmZIg5Wn4inxO316tRE9Yx48mu5B73ghUJKAjf1PXCDgsC0oVvdq4IbFrneMhA6t9bf5efS8PmO+c2BofkG8rzLJjYXTDx9NswJu65ImU\/t6gT9qwV5oUoLCRjl1KDfgZ7pdQgZrniLgjhahapINYnzWhSzmf3BR5EVZP4jVEsLyOiw9asiE5voDGk5bBcnVs53zaJanWnzXB5jsWUs6bkMkc=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=ekMTROhX1valLXvDasiDaVe6hyZTIedNIwp2aJZayQhzNYsSuMOz2bsQbPB5lHNI","encodedUrlAfterLogin":"cHVibGljYXRpb24vMzE5MjkzM19CYXllc2lhbl9jbGFzc2lmaWNhdGlvbl93aXRoX0dhdXNzaWFuX3Byb2Nlc3Nlcw%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw41_56ab1e628809b"},"id":"rgw41_56ab1e628809b","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw40_56ab1e628809b"},"id":"rgw40_56ab1e628809b","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw42_56ab1e628809b"},"id":"rgw42_56ab1e628809b","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
