<!DOCTYPE html> <html lang="en" class="" id="rgw38_56ab1d5f02afa"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="YQrQ9ZNbAEGZpG8qB1ZIV/j3s2cRiDMmoVaEaib1smuLfJpS3+F2mcTR0eNLVTsKWED7EY4amkXZJYzOQns02nySUpzQsEkhN43hi81Rz5JW1+y7vCxyCfWBE4VGQ9Szdj9s/8XNsI/lUNfhQfCV7L+v7nxxFlGaaumWk9tQrhohApHqKgRsvnCkIDhCZA0H1xiCA4j9LJNl7s95veX3VXhL8N07Y6rudcVxg04+AHvJEaRJa9f9jOS9SXPMEwU07UL+A7EHNrKQ+zBttT8QqOYfrho/eNM6vcRcM1prM8k="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-c7a10189-e627-4c50-b5a6-278be91fa2c7",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Adaptive Multiple Importance Sampling for Gaussian Processes" />
<meta property="og:description" content="In applications of Gaussian processes where quantification of uncertainty is
a strict requirement, it is necessary to accurately characterize the posterior
distribution over Gaussian process..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes/links/55e94dff08ae65b6389aee89/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes" />
<meta property="rg:id" content="PB:280773011" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Adaptive Multiple Importance Sampling for Gaussian Processes" />
<meta name="citation_author" content="Xiaoyu Xiong" />
<meta name="citation_author" content="Václav Šmídl" />
<meta name="citation_author" content="Maurizio Filippone" />
<meta name="citation_publication_date" content="2015/08/05" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Vasek_Smidl/publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes/links/55e94dff08ae65b6389aee89.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Adaptive Multiple Importance Sampling for Gaussian Processes (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Adaptive Multiple Importance Sampling for Gaussian Processes on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1d5f02afa" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1d5f02afa" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab1d5f02afa">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Adaptive%20Multiple%20Importance%20Sampling%20for%20Gaussian%20Processes&rft.date=2015&rft.au=Xiaoyu%20Xiong%2CV%C3%A1clav%20%C5%A0m%C3%ADdl%2CMaurizio%20Filippone&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Adaptive Multiple Importance Sampling for Gaussian Processes</h1> <meta itemprop="headline" content="Adaptive Multiple Importance Sampling for Gaussian Processes">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes/links/55e94dff08ae65b6389aee89/smallpreview.png">  <div id="rgw7_56ab1d5f02afa" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab1d5f02afa" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Xiaoyu_Xiong" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A309409379487746%401450780352548_m/Xiaoyu_Xiong.png" title="Xiaoyu Xiong" alt="Xiaoyu Xiong" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Xiaoyu Xiong</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw9_56ab1d5f02afa" data-account-key="Xiaoyu_Xiong">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Xiaoyu_Xiong"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A309409379487746%401450780352548_l/Xiaoyu_Xiong.png" title="Xiaoyu Xiong" alt="Xiaoyu Xiong" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Xiaoyu_Xiong" class="display-name">Xiaoyu Xiong</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_of_Glasgow" title="University of Glasgow">University of Glasgow</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56ab1d5f02afa" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Vasek_Smidl" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272548118134785%401441991943162_m/Vasek_Smidl.png" title="Vasek Smidl" alt="Vasek Smidl" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Vasek Smidl</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw11_56ab1d5f02afa" data-account-key="Vasek_Smidl">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Vasek_Smidl"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272548118134785%401441991943162_l/Vasek_Smidl.png" title="Vasek Smidl" alt="Vasek Smidl" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Vasek_Smidl" class="display-name">Vasek Smidl</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Academy_of_Sciences_of_the_Czech_Republic" title="Academy of Sciences of the Czech Republic">Academy of Sciences of the Czech Republic</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56ab1d5f02afa" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Maurizio_Filippone" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A273666076311560%401442258485613_m" title="Maurizio Filippone" alt="Maurizio Filippone" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Maurizio Filippone</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw13_56ab1d5f02afa" data-account-key="Maurizio_Filippone">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Maurizio_Filippone"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A273666076311560%401442258485613_l" title="Maurizio Filippone" alt="Maurizio Filippone" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Maurizio_Filippone" class="display-name">Maurizio Filippone</a>    </h5> <div class="truncate-single-line meta">   </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">        <meta itemprop="datePublished" content="2015-08">  08/2015;               <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1508.01050" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw14_56ab1d5f02afa" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>In applications of Gaussian processes where quantification of uncertainty is<br />
a strict requirement, it is necessary to accurately characterize the posterior<br />
distribution over Gaussian process covariance parameters. Normally, this is<br />
done by means of Markov chain Monte Carlo (MCMC) algorithms. Focusing on<br />
Gaussian process regression where the marginal likelihood is computable but<br />
expensive to evaluate, this paper studies algorithms based on importance<br />
sampling to carry out expectations under the posterior distribution over<br />
covariance parameters. The results indicate that expectations computed using<br />
Adaptive Multiple Importance Sampling converge faster per unit of computation<br />
than those computed with MCMC algorithms for models with few covariance<br />
parameters, and converge as fast as MCMC for models with up to around twenty<br />
covariance parameters.</div> </p>  </div>   </div>      <div class="action-container"> <div id="rgw15_56ab1d5f02afa" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw29_56ab1d5f02afa">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw30_56ab1d5f02afa">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Vasek_Smidl/publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes/links/55e94dff08ae65b6389aee89.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Vasek_Smidl">Vasek Smidl</a>, <span class="js-publication-date"> Sep 04, 2015 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw32_56ab1d5f02afa" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw33_56ab1d5f02afa" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw34_56ab1d5f02afa" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw35_56ab1d5f02afa" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw36_56ab1d5f02afa" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw37_56ab1d5f02afa" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw31_56ab1d5f02afa" src="https://www.researchgate.net/c/o1o9o3/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FVasek_Smidl%2Fpublication%2F280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes%2Flinks%2F55e94dff08ae65b6389aee89.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw28_56ab1d5f02afa"  itemprop="articleBody">  <p>Page 1</p> <p>arXiv:1508.01050v1  [stat.CO]  5 Aug 2015<br />Adaptive Multiple Importance Sampling for Gaussian<br />Processes<br />Xiaoyu Xionga,∗, V´ aclavˇSm´ ıdlb, Maurizio Filipponea<br />aSchool of Computing Science, University of Glasgow, UK<br />bInstitute of Information Theory and Automation, Prague, Czech Republic<br />Abstract<br />In applications of Gaussian processes where quantification of uncertainty is<br />a strict requirement, it is necessary to accurately characterize the posterior<br />distribution over Gaussian process covariance parameters. Normally, this is<br />done by means of Markov chain Monte Carlo (MCMC) algorithms. Focusing<br />on Gaussian process regression where the marginal likelihood is computable<br />but expensive to evaluate, this paper studies algorithms based on importance<br />sampling to carry out expectations under the posterior distribution over co-<br />variance parameters. The results indicate that expectations computed using<br />Adaptive Multiple Importance Sampling converge faster per unit of compu-<br />tation than those computed with MCMC algorithms for models with few<br />covariance parameters, and converge as fast as MCMC for models with up<br />to around twenty covariance parameters.<br />1. Introduction<br />Gaussian Processes (GPs) have been proved to be a successful class of<br />statistical inference methods for data analysis in several applied domains,<br />such as pattern recognition [47, 3, 12], neuroimaging [13], signal processing<br />[28], Bayesian optimization [26], and emulation and calibration of computer<br />codes [27]. The features that make GPs appealing are the nonparametric<br />∗Corresponding author<br />Email addresses: x.xiong.1@research.gla.ac.uk (Xiaoyu Xiong),<br />smidl@utia.cas.cz (V´ aclavˇSm´ ıdl), maurizio.filippone@glasgow.ac.uk (Maurizio<br />Filippone)</p>  <p>Page 2</p> <p>formulation that yields the possibility to flexibly model data, and the pa-<br />rameterization that makes it possible to gain some understanding on the<br />system under study. These properties hinge on the parameterization of the<br />GP covariance function and on the way GP covariance parameters are opti-<br />mized or inferred.<br />It is established that optimizing covariance parameters can severely affect<br />the ability of the model to quantify uncertainty in predictions [36, 12, 13, 53].<br />Therefore, in applications where this is undesirable, it is necessary to ac-<br />curately characterize the posterior distribution over covariance parameters<br />and propagate this source of uncertainty forward to predictions. Inference<br />of GP covariance parameters is analytically intractable, and standard in-<br />ference methods require repeatedly calculating the so called marginal likeli-<br />hood. When the likelihood function is not Gaussian, e.g., in classification,<br />in ordinal regression, in modeling of stochastic volatility, in Cox-processes,<br />the marginal likelihood cannot be computed analytically, and this has moti-<br />vated a large body of the literature to develop approximate inference meth-<br />ods [56, 43, 31, 47, 41, 24], reparameterization techniques [36, 34, 54, 14],<br />and exact inference with unbiased computations of the marginal likelihood<br />[12, 10]. Even in the case of a Gaussian likelihood, which makes the marginal<br />likelihood computable, inference is generally costly because the computation<br />of the marginal likelihood has time complexity scaling with the cube of the<br />number of input vectors [11].<br />This paper focuses on the problem of inferring GP covariance parameters<br />when the marginal likelihood is computable but expensive, namely when<br />GPs are used for regression and the likelihood is Gaussian. To date, most<br />of the literature proposes the use of Markov chain Monte Carlo (MCMC)<br />methods to characterize the posterior distribution over covariance parame-<br />ters [36, 34, 14, 29]. Generally, MCMC methods are based on the iteration of<br />the following two operations (i) proposal and (ii) an accept/reject step. The<br />literature provides optimal acceptance rates for a number of popular MCMC<br />algorithms, e.g., around 25% for the MH algorithm [49] and about 65% for the<br />HMC algorithm [2, 38]. Given that calculating the marginal likelihood and<br />its gradient with respect to the covariance parameters is expensive, whenever<br />MCMC algorithms reject a proposal, a considerable amount of computations<br />is therefore wasted. In this work, we investigate the possibility to carry out<br />expectations under the posterior distribution over covariance parameters us-<br />ing algorithms based on importance sampling. In particular, we compare<br />the Adaptive Multiple Importance Sampling (AMIS) algorithm [5, 55] with<br />2</p>  <p>Page 3</p> <p>a number of popular MCMC algorithms, namely MH, HMC, and Slice Sam-<br />pling (SS) [37]. Along with HMC, we also compare recently proposed No<br />U-Turn Samplers (NUTS) [25], where the number-of-steps and the stepsize<br />parameters are automatically tuned. The algorithms are compared on con-<br />vergence speed with respect to computational complexity.<br />A first attempt in the direction of demonstrating the applicability of<br />importance sampling methods for inference in GPs can be found in [46]. The<br />main contribution of this work is the extensive comparison of importance<br />sampling-based inference with MCMC-based inference for GP regression with<br />squared exponential covariances (i) with a global length-scale parameter (for<br />simplicity, we call it the RBF kernel case, where RBF stands for Radial<br />Basis Function) and (ii) with Automatic Relevance Determination (ARD)<br />(for simplicity, we call it the ARD kernel case). The results demonstrate<br />the value of our proposal. In particular, the results indicate that AMIS<br />is generally competitive with, if not outperforming, MCMC algorithm in<br />terms of convergence speed over computational cost of expectations under<br />the posterior distribution of covariance parameters. Furthermore, the results<br />suggest that AMIS is a valid alternative to MCMC algorithms even in the<br />case of moderately large dimensional parameter spaces, which is common<br />when employing ARD covariances.<br />The paper is organised as follows. In section 2 we provide a brief review<br />of GP regression and Bayesian inference. Section 3 presents the sampling<br />approaches considered in this work. Section 4 reports the experiments and<br />results. In section 5, we conclude the paper.<br />2. Gaussian Processes and Bayesian Inference<br />2.1. Gaussian Processes<br />Let X be the set of all input vectors xi∈ Rd(1 ≤ i ≤ n) and let y be the<br />vector consisting of the corresponding observations yi. In most GP models,<br />the assumption is that observations are conditionally independent given a<br />set of n latent variables. Such latent variables are modeled as realizations of<br />a function f(x) at the input vectors x1,...,xn, i.e., f = {f(x1),...,f(xN)}.<br />Latent variables are used to express the likelihood function, that under the<br />assumption of independence becomes p(y | f) =?n<br />depending on the particular type of data being modeled (e.g., Gaussian for<br />regression, Bernoulli for probit classification with probability P(yi= 1) =<br />Φ(f(xi)) with Φ defined as the cumulative normal distribution).<br />i=1p(yi| fi), with p(yi| fi)<br />3</p>  <p>Page 4</p> <p>What characterizes GP models is the way the latent variables are spec-<br />ified. In particular, in GP models the assumption is that the function f(x)<br />is distributed as a GP, which implies that the latent variables f are jointly<br />distributed as a Gaussian f ∼ N(0,K), where K is the covariance matrix.<br />The entries of the covariance matrix K are specified by a covariance (kernel)<br />function between pair of input vectors.<br />In the GP regression setting, the observations y are modeled to be Gaus-<br />sian distributed with mean of f (latent variables) and covariance λI, where I<br />denotes the identity matrix. In this setting, the likelihood and the GP priors<br />form a conjugate pair, so latent variables can be integrated out of the model<br />leading to p(y | X,θ) ∼ N(0,C), where C = K + λI. For simplicity of<br />notation, we gathered all the parameters of the covariance matrix C in the<br />vector θ. In this work, two kinds of covariance function are considered. The<br />first is the Radial Basis Function (RBF) kernel defined as:<br />¶−τ ? xn− xm?2©+ λδnm<br />with δnm = 1 if n = m and zero otherwise. The parameter τ defines the<br />length-scale of the interaction between the input vectors, σ represents the<br />marginal variance for each latent variable and λ is the variance of noise on the<br />observations. For simplicity of notation, let θ denotes the vector comprising<br />σ, τ and λ, i.e., θ = (σ,τ,λ).<br />The second is the Automatic Relevance Determination (ARD) kernel<br />which takes the form:<br /><br />i=1<br />c(xn,xm) = σ exp(1)<br />c(xn,xm) = σexp<br /><br />−<br />d<br />?<br />τi(xn(i)− xm(i))2<br /><br /><br />+ λδnm<br />(2)<br />where we denote the parameters by θ = (σ,τ1,...,τd,λ). The advantage of<br />the ARD kernel is that it accounts for the influence of each feature on the<br />prediction of y, with a larger value of parameter (τ1,...,τd) indicating a higher<br />influence of the corresponding feature [28].<br />2.2. Bayesian Inference<br />When making predictions, using a point estimate of θ has been reported<br />to potentially underestimate the uncertainty in predictions or yield inaccu-<br />rate assessment of the relative influence of different features [13] [12] [3].<br />Therefore, a Bayesian approach is usually adopted to overcome these lim-<br />itations which entails characterizing the posterior distribution p(θ | y,X).<br />4</p>  <p>Page 5</p> <p>Generally speaking, under the Bayesian framework, predictions can be seen<br />as the expectation of a function of θ with respect to the posterior distri-<br />bution, i.e., Ep(θ|y,X)[f(θ)]. Setting f(θ) = p(y⋆| y,θ,X,x⋆), the resulting<br />predictive distribution for the label y⋆, associated with a new input vector<br />x⋆, is as follows:<br />?<br />Under the Bayes’ rule, we obtain the desired posterior distribution over<br />θ as<br />p(θ | y,X) =<br />where p(θ) encodes any prior knowledge on the parameters θ. The denomi-<br />nator of this expression is intractable, so it is not possible to characterize the<br />posterior distribution analytically.<br />Despite this complication, it is possible to resort to a Monte Carlo ap-<br />proximation to compute expectations under the posterior distribution over<br />covariance parameters:<br />p(y⋆| y,X,x⋆) =p(y⋆| y,θ,X,x⋆)p(θ | y,X)dθ.(3)<br />p(y | X,θ)p(θ)<br />?p(y | X,θ)p(θ)dθ<br />(4)<br />p(y⋆| y,X,x⋆) ≃1<br />N<br />N<br />?<br />i=1<br />p(y⋆| θ(i),y,X,x⋆)(5)<br />where θ(i)denotes the ith of N samples from p(θ | y,X).<br />is usually not feasible to draw samples from p(θ | y,X) directly. Therefore,<br />MCMC methods are usually employed to generate samples from the posterior<br />p(θ | y,X).<br />An alternative way to compute expectations, is by means of importance<br />sampling, which takes the following form:<br />?<br />where q(θ | y,X) is the importance distribution. The corresponding Monte<br />Carlo approximation is of the form:<br />However, it<br />Ep(θ|y,X)[f(θ)] =f(θ)p(θ | y,X)<br />q(θ | y,X)q(θ | y,X)dθ<br />(6)<br />Ep(θ|y,X)[f(θ)] ≃1<br />N<br />N<br />?<br />i=1<br />f(θ(i))p(θ(i)| y,X)<br />q(θ(i)| y,X)<br />(7)<br />where now the samples θ(i)are drawn from the importance sampling distri-<br />bution q(θ | y,X). The key to make this Monte Carlo estimator accurate is<br />5</p>  <p>Page 6</p> <p>to choose q(θ | y,X) to be similar to the function that needs to be integrated.<br />It is easy to verify that when q(θ | y,X) is proportional to the function that<br />needs to be integrated, the variance of the importance sampling estimator<br />is zero. Therefore, the success of importance sampling relies on construct-<br />ing a tractable importance distribution q(θ | y,X) that well approximates<br />f(θ)p(θ | y,X). In the remainder of this paper we will discuss and employ<br />methods that adaptively construct q(θ | y,X).<br />Both Monte Carlo approximations in equations (5) and (7) converge to the<br />desired expectation, and in practice, they can estimate the desired integrals<br />in equations (3) and (6) to a given level of precision [17, 15]. The focus<br />of this work is to experimentally study the convergence properties of the<br />expectations in equations (3) and (6) with respect to the computational effort<br />needed to carry out the Monte Carlo approximations in equations (5) and<br />(7).<br />3. Sampling methods considered in this work<br />In this section, we present the state-of-the-art MCMC and AIS sam-<br />pling methods considered in this work. The aim is to find out whether AIS<br />(AMIS/MAMIS) can improve speed of convergence with respect to compu-<br />tational complexity, compared to MCMC approaches (MH, HMC, NUTS,<br />NUTDA, SS). In the remainder of this paper, for the sake of succinctness,<br />we denote log[p(y | X,θ)p(θ)] by L(θ), which is the logarithm of the target<br />posterior distribution up to constant.<br />3.1. Metropolis-Hastings - MH<br />The Metropolis-Hastings algorithms [23] [33] samples from exp(L(θ)) by<br />repeatedly considering randomly generated samples [35], accepting the pro-<br />ß<br />exp(L(θ))q(θ′|θ)<br />distribution, θ′denotes the proposed new sample, and θ denotes the current<br />sample. The proposal distribution q is commonly selected to be a succession<br />of random multivariate Gaussian of the form q(θ′| θ) ∼ N(θ′| θ,Σ) , with<br />the mean θ being the former state, and Σ being the covariance. Due to<br />the symmetric property of q, the acceptance function reduces to the form<br />min<br />Tuning the MH involves a proper choice of the covariance Σ. However, as<br />noted in [14], it is not trivial to select the right covariance as information of<br />posed moves with probability min1,exp(L(θ′))q(θ|θ′)<br />™<br />, where q is the proposal<br />¶1,exp(L(θ′) − L(θ))<br />©. This generalise to other symmetric proposals.<br />6</p>  <p>Page 7</p> <p>the desired target distribution is required in most cases. Very small values of<br />Σ will cause slow convergence to the stationary state, while very large ones<br />will result in chains getting stuck in certain regions of the space. Ways to<br />optimally tune the MH algorithms have been proposed [48] [16] [50]. Studies<br />on adaptively tuning the MH have also been reported [19] [20] [21] [22].<br />3.2. Hybrid Monte Carlo - HMC<br />HMC [35] [9] originated from Physics, where the Hamiltonian dynamics<br />function is defined by the sum of a potential energy function of the position<br />vector and a kinetic energy function of the momentum vector. When HMC<br />is applied to obtaining samples from a target distribution, the parameters<br />of interest θ, take the role of the position, and an auxiliary ”momentum”<br />variable p, which is commonly assumed to be independently drawn from<br />N(p | 0,M), needs to be introduced. Thus the extended target distribution<br />p(θ,p) takes the form exp(L(θ) −1<br />Consequently, the minus log of the augmented target distribution plus some<br />constant will give an analogy with the Hamiltonian:<br />2pTM−1p) up to a normalising constant.<br />H(θ,p) = −L(θ) +1<br />2pTM−1p + const. (8)<br />where −L(θ) is the potential energy and1<br />Generating a new sample of parameters involves two steps. The first is to<br />draw p randomly from N(p | 0,M), then propose a new θ(L),p(L)through<br />a number of L reversible ”leapfrog” steps that follows the Hamiltonian dy-<br />namics scheme:<br />p(t+ǫ/2)= p(t)+ǫ<br />2pTM−1p is the kinetic energy.<br />2∇θL(θ(t)) (9)<br />θ(t+ǫ)= θ(t)+ ǫM−1p(t+ǫ/2)<br />(10)<br />p(t+ǫ)= p(t+ǫ/2)+ǫ<br />2∇θL(θ(t+ǫ))(11)<br />where ǫ is stepsize, ∇θdenotes the gradient with respect to θ. The stepsize ǫ<br />is often chosen randomly to ensure ergodicity, and the exploit of the gradient<br />gives HMC one major advantage of avoiding the random walk behaviour that<br />occurs in MH. The proposed θ(L),p(L)will then be accepted with probability:<br />min<br />¶1,exp(−H(θ(L),p(L)) + H(θ,f))<br />©<br />(12)<br />7</p>  <p>Page 8</p> <p>Due to the ”shear” transformations of the equations 9, 10, 11, where the<br />update of one variable θ depends only on the other unchangeable variable<br />p [39], the leapfrog integrator is volume-preserving. The crucial property of<br />reversibility and preservation of volume of HMC ensures sampling from the<br />invariant target distribution of interest. Another benefit of HMC is its better<br />scalability with dimensionality compared to simple Metropolis approaches,<br />details of which can be found in [39]. The choice of the number of steps L and<br />stepsize ǫ can heavily affect the performance of HMC, thus careful tuning of<br />these two parameters is usually needed when applying HMC. Using knowl-<br />edge of scales and correlation of the position variables can also improve the<br />performance of HMC [39]. Specifically, this is achieved by transforming the<br />position variables θ to L−1θ or using a mass matrix M = Σ−1or (diag(Σ))−1,<br />where Σ denotes the estimate of the covariance of θ, and L denotes the lower<br />triangular of the Cholesky decompositon of Σ. We will discuss next NUTS<br />which automatically tunes ǫ, L.<br />3.2.1. No-U-Turn Sampler - NUTS<br />As mentioned above, the performance of HMC can be significantly influ-<br />enced by the choice of ǫ and L. Too large an ǫ will lead to very low acceptance<br />rate, while too small an ǫ will result in waste of computation time and also<br />the risk of undesirable random walk when L is not large. Choosing L can<br />be problematic as well. When L is too large, by taking too many steps or<br />looping back where the position variable was before, it will wastes a lot of<br />expensive computations. When L is too small, the resulting random walk<br />behaviour will cause slow exploration of the state and thus poor-mixing of<br />the samples. Therefore, [25] introduced NUTS, an extension to HMC, which<br />is tuning-free in the sense that it eliminates the need to choose the num-<br />ber of L and automatically sets the parameter ǫ. NUTS begins with a slice<br />sampling step, where a slice variable u is drawn uniformly from the inter-<br />val [0,exp(L(θ) −1<br />is randomly drawn from N(p | 0,M). This gives the conditional distribu-<br />tion p(θ,p | u) ∼ U(θ,p | {θ′,p′| exp(L(θ) −1<br />θ′,p′denote the proposed position and momentum respectively. Then the<br />leapfrog integrator is used to build up a trajectory that doubles the pre-<br />vious steps (either forwards or backwards) continuously. By doing this an<br />implicit balanced binary tree is built with each leaf node corresponding to<br />the position-momentum variables. The doubling stops when the subpath<br />from the leftmost to the rightmost nodes of any balanced subtree of the<br />2pTM−1p)], where θ denotes the current sample and p<br />2pTM−1p) ≥ u}), where<br />8</p>  <p>Page 9</p> <p>whole binary tree starts to make a ”U-turn”, that is, the samples start to<br />retrace their steps. The proposed position-momentum variables are sampled<br />incrementally from the subtree during the doubling process, and a transition<br />kernel that leaves the target distribution invariant is used at the end of the<br />simulation to accept/reject the proposed new samples. In this way we do not<br />need to choose the number of steps L.<br />3.2.2. No-U-Turn Sampler with Dual Averaging - NUTSDA<br />To address the issue of setting the stepsize ǫ, [25] adopts an adaptation<br />of the stochastic optimisation with a dual averaging scheme of [40], which<br />takes the following form:<br />√t<br />γt + t0<br />i=1<br />ǫt+1← logǫ1−<br />1<br />t<br />?<br />Hi;<br />ǫt+1← t−κlogǫt+1+ (1 − tκ)logǫt<br />(13)<br />where t denotes the number of iterations; ǫ1is the initial value of epsilon,<br />found by the heuristic that aims to obtain an acceptance rate of 0.5 using<br />the Langevin proposal with stepsize ǫ1; γ &gt; 0, t0&gt; 0 are free parameters<br />that determine the shrinkage towards logǫ1and the stabilisation of the initial<br />iterations respectively; Ht= δ − HNUTS, with δ denoting the desired target<br />mean acceptance rate, HNUTSbeing the average acceptance rate during the<br />final iteration of doubling. The term t−κ(κ ∈ (0.5,1]) is chosen to ensure the<br />averaged value ǫtconverge to a value for large t and hence the expectation<br />of Ht(function of ǫt) converge to 0.<br />3.3. Slice Sampling - SS<br />Slice sampling [37], is another auxiliary method in which the joint prob-<br />ability density of the auxiliary variable u and parameters of interest θ takes<br />the form p(θ,u), such that<br />?<br />0<br />?exp(L(θ))dθ. This gives the marginal distribution for θ:<br />p(θ) =<br />0<br />p(θ,u) =<br />1/Z if 0 &lt; u &lt; exp(L(θ))<br />otherwise<br />(14)<br />where Z =<br />?exp(L(θ))<br />(1/Z)du = exp(L(θ))/Z(15)<br />as desired.<br />from p(θ,u), then ignore u. Specifically, this is done by alternating uniform<br />To sample from the target distribution p(θ), we can sample<br />9</p>  <p>Page 10</p> <p>sampling of u from the vertical interval from 0 to the density exp(L(θ))<br />evaluated at the current state, with uniform sampling of θ from the union<br />of intervals which are the horizontal ”slices” defined by the vertical posi-<br />tion. Single-variable slice sampling is easily implemented and one can do<br />component-wise univariate slice sampling for a multivariate distribution by<br />updating each variable in turn. This approach is reported in [37] to be easier<br />to implement than Gibbs sampling and be able to sample more efficiently<br />than simple Metropolis scheme, due to its ability to adaptively choose the<br />scale of changes appropriate to the region of the target distribution being<br />sampled. For example, if our rough guess at the initial interval (an estimate<br />for the scale of the horizontal slice) is too small compared to the true width<br />of the target density, it can be expanded by stepping out or doubling, while<br />if the initial interval is too large, it can be shrunk by an efficient shrinkage<br />procedure. More elaborate multivariate slice samplers can not only adapt to<br />the scale of variables, but also to the dependencies between variables. Sam-<br />pling efficiency can also be improved by the ”overrelaxed” univariate slice<br />sampling and the ”reflective” multivariate slice sampling that can suppress<br />random walks.<br />3.4. Adaptive Monte Carlo<br />3.4.1. Adaptive MCMC<br />In order to improve sampling efficiency, many adaptive MCMC methods<br />have been developed during past years. Two adaptation criteria are very<br />common in the literature. One is the optimal acceptance probability, where<br />the size and shape of the proposal distribution is scaled according to an op-<br />timal acceptance rate [16, 50]. However, as noted in [19], this hand-tuning<br />is time consuming as the acceptance probability does not take into account<br />the shape of the target distribution and can be difficult when the parameters<br />are of different scales and correlated. To avoid this difficulty, other adap-<br />tation schemes employ moment matching, where moments of the proposal<br />distribution (e.g. mean and covariance) are matched with those of the target<br />distribution [19, 20, 22]. A further approach to adaptation takes advantage<br />of the regeneration of the chain [18, 52]. The work in [1] proposes a general<br />adaptation framework using stochastic approximation schemes to learn the<br />optimal parameters of the proposal distribution for several statistical crite-<br />ria. However, devising valid adaptive MCMC methods is generally difficult<br />in practice [1, 21].<br />10</p>  <p>Page 11</p> <p>3.4.2. Adaptive Importance Sampling - AIS<br />The difficulty of devising adaptive MCMC approaches lies in that the<br />chain resulting from the adaptivity is no longer Markovian, and thus more<br />elaborate ergodicity results are needed, as indicated in [19, 20, 22, 1]. There-<br />fore, [4] proposed a universal adaptive sampling scheme called population<br />Monte Carlo (PMC), where the difference from Sequential Monte Carlo<br />(SMC) [8] is that the target distribution becomes static. This method is<br />reported to have better adaptivity than MCMC due to the fact that it is<br />validating by importance sampling justifications, which makes the ergodicity<br />not an issue in this case. At each iteration of PMC, sampling importance re-<br />sampling [51] is used to generate samples that are assumed to be marginally<br />distributed from the target distribution and hence the approach is unbiased<br />and can be stopped at any time. Moreover, the importance distribution can<br />be adapted using part (generated at each iteration) or all of the importance<br />sample sequence. [6, 7] also introduced updating mechanisms for the weights<br />of the mixture in the so called D-kernel PMC, which leads to a diminution ei-<br />ther in Kullback divergence between the mixture and the target distribution<br />or in the asymptotic variance for a function of interest. An earlier adaptive<br />importance sampling strategy is illustrated in [42].<br />3.4.3. Adaptive Multiple Importance Sampling - AMIS<br />[5] proposed a new perspective of adaptive importance sampling, called<br />adaptive multiple importance sampling (AMIS), where the difference with<br />the aforementioned PMC methods is that the importance weights of all sim-<br />ulations, produced previously as well as currently, are re-evaluated at each<br />iteration. This method follows the “deterministic multiple mixture” sam-<br />pling scheme of [45]. The corresponding importance weight takes the form<br />wt<br />i= π(yt<br />i)/<br />1<br />?T<br />j=0Nj<br />T<br />?<br />l=0<br />Nlql(yt<br />i) (16)<br />where T is the total number of iterations, π(.) denotes the target distribu-<br />tion, ql(.) denotes the importance density at each iteration, yt<br />drawn from qt(.) with 0 ≤ t ≤ T,1 ≤ i ≤ Nt. The fixed denominator in (16)<br />gives the name “deterministic multiple mixture”. The motivation is that this<br />construction can achieve an upper bound on the asymptotic variance of the<br />estimator without rejecting any simulations [45]. In AMIS, the parameters γ<br />of a parametric importance function ql(γ) are sequentially updated using the<br />iare samples<br />11</p>  <p>Page 12</p> <p>entire sequence of weighted importance samples, based on efficiency criteria<br />such as moment matching, minimum Kullback divergence with respect to<br />the target or minimum variance of the weights (see, e.g., [44] for stochastic<br />gradient-based optimization of these efficiency criteria). This leads to a se-<br />quence of importance distributions, q1(”<br />brought by AMIS compared with other AIS techniques, convergence of this<br />algorithm still remains an issue.<br />γ1),...,qT(?<br />γT). Despite the efficiency<br />3.4.4. Modified AMIS - MAMIS<br />The work in [32] proposed a modified version of AMIS called MAMIS,<br />aiming at solving the convergence issue in the original AMIS scheme. The dif-<br />ference is that the new parameters ’<br />produced at iteration t, i.e., xt<br />Then the weights of all simulations are updated according to (16) to give the<br />final output. The sample size Ntis suggested to grow at each iteration so as<br />to improve the accuracy of ’<br />issues of AMIS, but convergence is slower due to the fact that less samples<br />are used to update the importance distribution.<br />γt+1are estimated based only on samples<br />1,...,xt<br />Nt, with classical weights π(yt<br />i)/q(yt<br />i,? γt).<br />γt+1. MAMIS effectively solves any convergence<br />4. Experiments<br />4.1. Datasets<br />The sampling methods considered in this work are implemented on three<br />UCI datasets, which are listed in Table 1. The number of data points for the<br />Concrete and Housing datasets are 1030 with 8 features for each data point<br />and 506 with 13 features for each data point respectively. For the original<br />Parkinsons dataset, the number of data points is 5875 with 20 features for<br />each data point, which are records for 42 patients measured at different time.<br />In our experiment, we randomly sampled 4 records for each patient, resulting<br />in 168 data points in total.<br />12</p>  <p>Page 13</p> <p>Data sets<br />Concrete<br />Housing<br />Parkinsons<br />nd<br />8<br />13<br />20<br />1030<br />506<br />168<br />Table 1: Datasets used in this paper. n denotes the number of data points, d denotes the<br />dimension of the features.<br />4.2. Experimental setup<br />We compare three different covariances for the proposals in the MH al-<br />gorithm. The first is the identity matrix. The second and third covariances<br />are based on the inverse of the negative Hessian (denoted by H) evaluated at<br />the mode (denoted by m); one uses the full matrix, whereas another uses its<br />diagonal only, namely diag((−H)−1). The mode m is found by the maximum<br />likelihood optimisation using the “BFGS” method.<br />Thus the proposals that we compare in this work take the form of N(θ |<br />m,αI), N(θ | m,α(−H)−1), and N(θ | m,α diag((−H)−1)), where α is a<br />tuning parameter. We tune α in pilot runs until we get the desired acceptance<br />rate (around 25%). By using these tuned proposals for starting a chain, we<br />are able to avoid the poor-mixing of samples due to the long-stay of random<br />walk in a region largely influenced by the starting distribution.<br />The approximate distribution N(θ | m,(−H)−1) is used to be the initial<br />importance density for AMIS/MAMIS. It is also used to initialize several in-<br />dependent sequences of samples from other samplers considered in this work.<br />In this way, valid summary inference from multiple independent sequences<br />can be obtained [17].<br />As mentioned in the previous section (introduction of HMC), to exploit<br />the scales and correlation of the position variables, we also chose three kinds<br />of mass matrix for HMC, namely the identity matrix, the inverse of the<br />approximate covariance, and the inverse of the diagonal of the approximate<br />covariance. The maximum leapfrog steps is 10. We then tune the stepsize ǫ<br />until a suggested acceptance rate (around 65%) is reached. The three forms<br />of mass matrix apply to NUTS, NUTSDA as well. For AMIS/MAMIS, we<br />explored two kinds of update of the covariance of the importance density. One<br />updates the full covariance, whereas the other updates only the diagonal of<br />the covariance.<br />NUTS requires the tuning of a stepsize ǫ. After a few trials, we set the<br />13</p>  <p>Page 14</p> <p>stepsize of NUTS to 0.1. Although tuning leapfrog steps and stepsize is not<br />an issue in NUTDA, the parameters (γ,t0,κ) for the dual averaging scheme<br />therein have to be tuned by hand to produce reasonable result. By trying a<br />few settings for each parameter, finally the values γ = 0.05,t0= 30,κ = 0.75<br />were used in both the RBF and ARD kernel case.<br />The slice sampling algorithm adopted in this paper makes component-<br />wise updates of the parameters, where a new sample is drawn according<br />to the “stepping out” and “shrinkage” procedures described in [37]. In our<br />implementation, we set the estimate of the typical size of a slice w to 1.5,<br />and set the integer m limiting the size of a slice to mw to infinity, i.e, no<br />limit is put on the number of steps.<br />Table 2 shows the experimental settings for AMIS/MAMIS. AMIS/MAMIS<br />is implemented for the three datasets in both the RBF and ARD kernel cases.<br />Alternative MAMIS is also implemented for the Housing and Parkinsons<br />datasets in the ARD kernel case, see 4.4.2.<br />RBF kernel<br />T<br />1120<br />46<br />ARD kernel<br />Nt<br />280<br />53000 + 1000t<br />Nt<br />25<br />26t<br />T<br />AMIS<br />MAMIS<br />100<br />Table 2: Experimental settings for AMIS/MAMIS. T is the total number of iterations, Nt<br />is the sample size at each iteration t.<br />4.3. Convergence analysis<br />Since the classicˆR score is for MCMC convergence analysis and not suit-<br />able for AIS, convergence analysis here is performed based on the IQR (in-<br />terquartile range) of the expectation of norm of parameters (Ep(θ|y,X)[?θ?])<br />over 100 replicates of all algorithms, against the number of n3operations,<br />which is reported to be a more reliable measure of complexity than running<br />time, as many other factors such as implementation details that do not relate<br />directly to the actual computing complexity of the algorithms can affect the<br />running time [14].<br />For AMIS/MAMIS/SS/MH, the computing complexity lies in the com-<br />putation of the function of L(θ), one evaluation of which takes one Cholesky<br />decomposition of the covariance matrix (the corresponding complexity is1<br />operations). While for HMC/NUTS/NUTSDA, the complexity lies in the<br />3n3<br />14</p>  <p>Page 15</p> <p>evaluation of the function that returns both L(θ) and its gradient. One<br />evaluation of such a function causes complexity of7<br />Cholesky decomposition of the covariance matrix and 2 for computation of<br />the inverse of the covariance matrix when computing the gradient). When a<br />new sample is generated, the number of the evaluations of the aforementioned<br />two functions are accumulated to compute the corresponding computational<br />complexity.<br />3n3operations (1<br />3for one<br />4.4. Results<br />4.4.1. Notation of samplers<br />Table 3 shows the notation for the different samplers considered in this<br />paper.<br />AMIS/MAMIS<br />AMIS/MAMIS where the full covariance<br />matrix of the proposal distribution is<br />updated at each iteration<br />AMIS/MAMIS where only the diagonal of<br />the covariance matrix of the proposal<br />distribution is updated at each iteration<br />MH where the covariance of the starting<br />proposal distribution for tuning is the<br />identity matrix<br />MH where the covariance of the starting<br />proposal distribution for tuning is the<br />diagonal of the approximate covariance<br />MH where the covariance of the starting<br />proposal distribution for tuning is the<br />approximate covariance<br />HMC family where the mass matrix is the<br />identity matrix<br />HMC family where the mass matrix is the<br />inverse of the diagonal of the approximate<br />covariance<br />HMC family where the mass matrix is the<br />inverse of the approximate covariance<br />AMIS-D/MAMIS-D<br />MH-I<br />MH-D<br />MH-H<br />HMC-I/NUT-I/NUTDA-I<br />HMC-D/NUT-D/NUTDA-D<br />HMC-H/NUT-H/NUTDA-H<br />Table 3: Notation for different samplers.<br />15</p>  <p>Page 16</p> <p>4.4.2. Convergence of samplers<br />In this section, we present the comparison of convergence of all samplers<br />considered in this paper.<br />Appendix A and Appendix B show the convergence results for the sam-<br />plers with the RBF covariance (RBF kernel case) and ARD covariance (ARD<br />kernel case) respectively. The top-left of figures in Appendix A and Appendix B<br />demonstrate the result of AMIS/MAMIS. It can be seen that AMIS/MAMIS<br />that exploits the full covariance structure of the proposal distribution per-<br />forms better than the one that only updates the diagonal of the covariance<br />matrix of the proposal density. For the MH family (MH-I/MH-D/MH-H) and<br />HMC family (standard HMC , NUTS, NUTSDA), figures in Appendix A<br />and Appendix B show that, the methods that make use of the scales and<br />correlation of the parameters, perform better than the one that does not<br />in most cases. Also, NUTS/NUTSDA turns out to converge much faster<br />than the standard HMC due to the fact that standard HMC has to be tuned<br />costly in pilot runs. For MH and standard HMC, the computational cost of<br />tuning is counted when comparing the convergence, as is shown in top-right<br />and middle-left of figures in Appendix A and Appendix B where the end<br />of tuning (EOT) is indicated by three vertical dotted lines, corresponding to<br />the three variants respectively from left to right. For NUTSDA, the compu-<br />tational cost of tuning the parameters of the dual averaging scheme is also<br />counted when determining the convergence, as is displayed in bottom-left of<br />figures in Appendix A and Appendix B with EOT indicated by three verti-<br />cal dotted lines, relating to the three variants respectively from left to right.<br />Table 4 shows the corresponding computational cost of tuning:<br />Concrete dataset<br />RBFARD<br />6747<br />6042<br />10851<br />1402<br />1357<br />682<br />Housing dataset<br />RBF<br />4779<br />7281<br />10987<br />1193<br />1124<br />670<br />Parkinsons dataset<br />RBF ARD<br />1561<br />8883<br />10871<br />1338<br />975<br />728<br />ARD<br />3924<br />7726<br />8860<br />7433<br />2424<br />1866<br />HMC-I<br />HMC-D<br />HMC-H<br />NUTDA-I<br />NUTDA-D<br />NUTDA-H<br />5910<br />7316<br />9451<br />3528<br />1582<br />1023<br />1340<br />8469<br />8736<br />6488<br />1951<br />1794<br />Table 4: Computational cost of tuning for HMC/NUTSDA. Unit: number of n3operations.<br />16</p>  <p>Page 17</p> <p>Figure 1 shows the final result of AMIS, best of MAMIS, best of MH<br />family, best of HMC family and SS for the three datasets in both the RBF<br />and ARD kernel case. It is impressive to see that AMIS/MAIMS performs<br />best among all methods in terms of convergence speed in the RBF kernel<br />case. In the ARD kernel case, AMIS also converges much faster than the<br />other approaches. However, our experiments show that in the ARD kernel<br />case although MAMIS converges faster than the other approaches in the<br />Concrete dataset, it converges slowly in the Housing and Parkinsons datasets,<br />which is probably due to the higher dimensionality compared to the previous<br />cases.<br />In cases where MAMIS converges slowly, we can exploit the fact that<br />AMIS converges faster than MAMIS by running AMIS for a fixed number<br />of iterations and then switch to MAMIS. In this way, we can ensure fast<br />convergence of the adaptive scheme while taking advantage of the proof of<br />convergence. In the experiments, we tested this AMIS-MAMIS combination<br />in cases where MAMIS converges slowly. We treated samples from AMIS as<br />tuning cost for MAMIS to get an accurate initial importance density as is<br />shown in bottom-right of B.6 B.7 with EOT (end of tuning) indicated by the<br />vertical dotted line. Three settings (Table 5) of AMIS-MAMIS were tested<br />for the Parkinsons dataset.<br />Ntfor<br />MAMIS<br />number of samples<br />generated from AMIS for<br />tuning the initial<br />importance density of<br />MAMIS<br />13000<br />13000<br />26000<br />the<br />corresponding<br />tuning cost<br />AMIS-MAMIS<br />AMIS-MAMIS’<br />AMIS-MAMIS”<br />1000t<br />5000t<br />5000t<br />4333<br />4333<br />8667<br />Table 5: Settings for AMIS-MAMIS. Ntis the sample size at each iteration t. Unit of the<br />tuning cost: number of n3operations.<br />For the Housing dataset, we tested only AMIS-MAMIS in Table 5. The<br />results for the Housing and Parkinsons datasets in the ARD kernel case prove<br />the convergence of AMIS-MAMIS. In particular, AMIS-MAMIS and AMIS-<br />MAMIS” seem to compete well with the other MCMC approaches in terms of<br />convergence for the Housing dataset and the Parkinsons dataset respectively.<br />17</p>  <p>Page 18</p> <p>As is shown in the bottom-right of Figure B.7, the best performance of AMIS-<br />MAMIS” for the Parkinsons dataset suggests that for higher dimensional<br />problem, a more accurate initialization and a larger sample size at each<br />iteration for MAMIS are necessary to achieve faster convergence.<br />Another attempt that we make in this paper to improve convergence<br />speed of the adaptive importance sampling schemes is to regularize the es-<br />timation of the parameters of the importance distribution as illustrated in<br />[55]. The regularization stems from the use of an informative prior on γ of<br />the importance distribution qt(γ) of MAMIS and treat the update of these<br />parameters in a Bayesian fashion [30]. This construction makes it possible to<br />avoid situations where the importance distribution degenerates to low rank<br />due to few importance weights dominating all the others. In this work, we<br />use an informative prior based on a Gaussian approximation to the posterior<br />over covariance parameters. We denote this method by MAMIS-P and in<br />the ARD kernel case it was tested only in the Housing dataset. The result<br />indicates that even though MAMIS-P improves on MAMIS, its convergence<br />is slower than AMIS-MAMIS (bottom-right of Figure B.6).<br />18</p>  <p>Page 19</p> <p>RBF<br />Concrete dataset<br />ARD<br />Concrete dataset<br />0 200040006000 8000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />AMIS<br />MAMIS<br />MH−H<br />NUTDA−H<br />SS<br />0 20004000 60008000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />AMIS<br />MAMIS<br />MH−H<br />NUTDA−H<br />SS<br />0200040006000 8000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />Housing dataset<br />number of n3 operations<br />IQR<br />AMIS<br />MAMIS<br />MH−H<br />NUTDA−H<br />SS<br />0200040006000 8000<br />0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0<br />Housing dataset<br />number of n3 operations<br />IQR<br />AMIS<br />AMIS−MAMIS<br />MH−D<br />NUTDA−H<br />SS<br />0 2000400060008000<br />0.00<br />0.05<br />0.10<br />0.15<br />Parkinsons dataset<br />number of n3 operations<br />IQR<br />AMIS<br />MAMIS<br />MH−H<br />NUTDA−H<br />SS<br />020004000 60008000100001200014000<br />0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />Parkinsons dataset<br />number of n3 operations<br />IQR<br />AMIS<br />AMIS−MAMIS&quot;<br />MH−H<br />NUTDA−H<br />SS<br />Figure 1: Convergence of AMIS, Best of MAMIS, Best of MH family, Best of HMC family,<br />SS for all datasets.<br />19</p>  <p>Page 20</p> <p>5. Conclusions<br />In this paper we proposed the use of adaptive importance sampling tech-<br />niques to carry out expectations under the posterior distribution of covariance<br />parameters in Gaussian process regression. The motivation for our proposal<br />is based on the observation that calculating the marginal likelihood and its<br />gradient with respect to covariance parameters in GP regression is expensive<br />and standard MCMC algorithms reject proposals leading to a waste of com-<br />putations. The results support our intuition that importance sampling-based<br />inference of covariance parameters is competitive with MCMC algorithms.<br />In particular, the results indicate that it is possible to achieve convergence<br />of expectations under the posterior distribution of covariance parameters<br />faster than employing MCMC methods in a wide range of scenarios. Even in<br />the case of around twenty parameters, adaptive importance sampling is still<br />competitive with MCMC approaches.<br />We believe that these results are important for a number of reasons. First,<br />importance sampling-based algorithms are generally easy to implement and<br />tune, and can be massively parallelized. Second, these results immediately<br />offer the possibility to derive inference methods for GP models where the<br />marginal likelihood cannot be computed analytically but can be estimated<br />unbiasedly; we are currently investigating the behavior of adaptive impor-<br />tance sampling in these scenarios.<br />References<br />[1] Andrieu, C., Robert, C. P., 2001. Controlled MCMC for optimal sam-<br />pling. Bernoulli 9, 395–422.<br />[2] Beskos, A., Pillai, N., Roberts, G. O., Sanz-Serna, J. M., Stuart, A. M.,<br />2013. Optimal tuning of hybrid Monte Carlo algorithm. Bernoulli 19,<br />1501–1534.<br />[3] Bishop, C. M., Aug. 2007. Pattern Recognition and Machine Learning<br />(Information Science and Statistics), 1st Edition. Springer.<br />[4] Cappe, O., Guillin, A., Marin, J. M., Robert, C. P., 2004. Population<br />monte carlo. Journal of Computational and Graphical Statistics 13, 907–<br />929.<br />20</p>  <p>Page 21</p> <p>[5] Cornuet, J.-M., Marin, J.-M., Mira, A., Robert, C. P., 2012. Adaptive<br />multiple importance sampling. Scandinavian Journal of Statistics 39,<br />798–812.<br />[6] Douc, R., Guillin, A., Marin, J.-M., Robert, C., 2007a. Convergence of<br />adaptive mixtures of importance sampling schemes. Ann. Statist. 35,<br />420–448.<br />[7] Douc, R., Guillin, A., Marin, J.-M., Robert, C., 2007b. Minimum vari-<br />ance importance sampling via population monte carlo. ESAIM: Probab.<br />Stat. 11, 427–447.<br />[8] Doucet, A., deFreitas, N., Gordon, N., 2001. Sequential MCMC in Prac-<br />tice. New York : Springer-Verlag.<br />[9] Duane, S., Kennedy, A. D., Pendleton, B. J., Roweth, D., 1987. Hybrid<br />Monte Carlo. Physics Letters B 195 (2), 216–222.<br />[10] Filippone, M., 2014. Bayesian inference for Gaussian process classifiers<br />with annealing and pseudo-marginal MCMC. In: 22nd International<br />Conference on Pattern Recognition, ICPR 2014, Stockholm, Sweden,<br />August 24-28, 2014. IEEE, pp. 614–619.<br />[11] Filippone, M., Engler, R., 2015. Enabling scalable stochastic gradient-<br />based inference for Gaussian processes by employing the Unbiased LIn-<br />ear System SolvEr (ULISSE). In: Proceedings of the 32nd International<br />Conference on Machine Learning, ICML 2015, Lille, France, July 6-11,<br />2015.<br />[12] Filippone, M., Girolami, M., 2014. Pseudo-marginal Bayesian inference<br />for Gaussian processes. IEEE Transactions on Pattern Analysis and Ma-<br />chine Intelligence 36 (11), 2214–2226.<br />[13] Filippone, M., Marquand, A. F., Blain, C. R. V., Williams, S. C. R.,<br />Mour˜ ao-Miranda, J., Girolami, M., 2012. Probabilistic Prediction of<br />Neurological Disorders with a Statistical Assessment of Neuroimaging<br />Data Modalities. Annals of Applied Statistics 6 (4), 1883–1905.<br />[14] Filippone, M., Zhong, M., Girolami, M., 2013. A comparative evalua-<br />tion of stochastic-based inference methods for Gaussian process models.<br />Machine Learning 93 (1), 93–114.<br />21</p>  <p>Page 22</p> <p>[15] Flegal, J. M., Haran, M., Jones, G. L., Mar. 2007. Markov Chain Monte<br />Carlo: Can We Trust the Third Significant Figure? Statistical Science<br />23 (2), 250–260.<br />[16] Gelman, A., Roberts, G. O., Gilks, W. R., 1996. Efficient Metropolis<br />jumping rules. In: Bayesian statistics, 5 (Alicante, 1994). Oxford Sci.<br />Publ. Oxford Univ. Press, New York, pp. 599–607.<br />[17] Gelman, A., Rubin, D. B., 1992. Inference from iterative simulation<br />using multiple sequences. Statistical Science 7 (4), 457–472.<br />[18] Gilks, W., Roberts, G., Sahu, S., 1998. Adaptive markov chain monte<br />carlo through regeneration. J. Am. Star. Ass. 93, 1045–1054.<br />[19] Haario, H., Saksman, E., Tamminen, J., 1999. Adaptive proposal distri-<br />bution for random walk metropolis algorithm. Computational Statistics<br />14, 375–395.<br />[20] Haario, H., Saksman, E., Tamminen, J., 2001. An adaptive metropolis<br />algorithm. Bernoulli 7, 223–242.<br />[21] Haario, H., Saksman, E., Tamminen, J., 2003. Componentwise adapta-<br />tion for MCMC. Tech. Rep. Preprint 342, Dept. of Mathematics, Uni-<br />versity of Helsinki.<br />[22] Haario, H., Saksman, E., Tamminen, J., 2005. Componentwise adapta-<br />tion for high dimensional MCMC. Computational Statistics 20, 265–273.<br />[23] Hastings, W., April 1970. Monte carlo sampling methods using Markov<br />chains and their applications. Biometrika 57.<br />[24] Hensman,<br />2015. MCMC for variationally sparse Gaussian processes. Tech. rep.,<br />arXiv:1506.04000.<br />J., Alexander, Filippone, M., Ghahramani, Z., Jun.<br />[25] Hoffman, M. D., Gelman, A., Nov. 2012. The No-U-Turn Sampler:<br />Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. Journal<br />of Machine Learning Research to appear.<br />[26] Jones, D. R., Schonlau, M., Welch, W. J., 1998. Efficient Global Op-<br />timization of Expensive Black-Box Functions. Journal of Global Opti-<br />mization 13 (4), 455–492.<br />22</p>  <p>Page 23</p> <p>[27] Kennedy, M. C., O’Hagan, A., 2001. Bayesian calibration of computer<br />models. Journal of the Royal Statistical Society: Series B (Statistical<br />Methodology) 63 (3), 425–464.<br />[28] Kim, S., Valente, F., Filippone, M., Vinciarelli, A., 2014. Predicting<br />continuous conflict perception with bayesian gaussian processes. IEEE<br />Transactions on Affective Computing to appear.<br />[29] Knorr-Held, L., Rue, H., Dec. 2002. On Block Updating in Markov<br />Random Field Models for Disease Mapping. Scandinavian Journal of<br />Statistics 29 (4), 597–614.<br />[30] Kulhav´ y, R., 1996. Recursive nonlinear estimation: A geometric ap-<br />proach. Springer.<br />[31] Kuss, M., Rasmussen, C. E., 2005. Assessing Approximate Inference for<br />Binary Gaussian Process Classification. Journal of Machine Learning<br />Research 6, 1679–1704.<br />[32] Marin, J.-M., Pudlo, P., Sedki, M., 2014. Consistency of the adaptive<br />multiple importance sampling. eprint arXiv:1211.2548v2.<br />[33] Metropolis, N., Rosenbluth, A., Teller, A., Teller, E., 1953. Equation of<br />state calulations by fast computing machines. The Journal of Chemical<br />Physics 21, 1087–1092.<br />[34] Murray, I., Adams, R. P., 2010. Slice sampling covariance hyperparam-<br />eters of latent Gaussian models. In: Lafferty, J. D., Williams, C. K. I.,<br />Shawe-Taylor, J., Zemel, R. S., Culotta, A. (Eds.), Advances in Neural<br />Information Processing Systems 23: 24th Annual Conference on Neu-<br />ral Information Processing Systems 2010. Proceedings of a meeting held<br />6-9 December 2010, Vancouver, British Columbia, Canada. Curran As-<br />sociates, Inc., pp. 1732–1740.<br />[35] Neal, R. M., Sep. 1993. Probabilistic inference using Markov chain<br />Monte Carlo methods. Tech. Rep. CRG-TR-93-1, Dept. of Computer<br />Science, University of Toronto.<br />[36] Neal, R. M., 1999. Regression and classification using Gaussian process<br />priors (with discussion). Bayesian Statistics 6, 475–501.<br />23</p>  <p>Page 24</p> <p>[37] Neal, R. M., 2003. Slice Sampling. Annals of Statistics 31, 705–767.<br />[38] Neal, R. M., 2010. MCMC using Hamiltonian dynamics. in Handbook<br />of Markov Chain Monte Carlo (eds S. Brooks, A. Gelman, G. Jones, XL<br />Meng). Chapman and Hall/CRC Press.<br />[39] Neal, R. M., 2011. Handbook of Markov Monte Carlo, chapter 5: MCMC<br />using Hamitonian Dynamics. CRC Press.<br />[40] Nesterov, Y., 2009. Primal-dual subgradient methods for convex prob-<br />lems. Mathematical Programming 120 (1), 221–259.<br />[41] Nickisch, H., Rasmussen, C. E., Oct. 2008. Approximations for Binary<br />Gaussian Process Classification. Journal of Machine Learning Research<br />9, 2035–2078.<br />[42] Oh, M. S., Berger, J. O., 1992. Adaptive importance sampling in monte<br />carlo integration. Journal of Statistical Computing and Simulation 41,<br />143–168.<br />[43] Opper, M., Winther, O., 2000. Gaussian processes for classification:<br />Mean-field algorithms. Neural Computation 12 (11), 2655–2684.<br />[44] Ortiz, L., Kaelbling, L., 2000. Adaptive importance sampling for esti-<br />mation in structured domains. In: Proceedings of the Sixteenth Annual<br />Conference on Uncertainty in Artificial Intelligence (UAI-2000). Morgan<br />Kaufmann Publishers, San Francisco, CA., pp. 446–454.<br />[45] Owen, A., Zhou, Y., 2000. Safe and effective importance sampling. J.<br />Amer. Statist. Assoc. 95, 135–143.<br />[46] Petelin, D., Gasperin, M., Smidl, V., 2014. Adaptive importance sam-<br />pling for Bayesian inference in Gaussian process models. In: Proceedings<br />of the 19th IFAC World Congress. pp. 5011–5016.<br />[47] Rasmussen, C. E., Williams, C., 2006. Gaussian Processes for Machine<br />Learning. MIT Press.<br />[48] Roberts, G., Rosenthal, J., 2001. Optimal scaling for various metropolis-<br />hastings algorithms. Statistical Science 16, 351–367.<br />24</p>  <p>Page 25</p> <p>[49] Roberts, G. O., Gelman, A., Gilks, W. R., 1997. Weak convergence<br />and optimal scaling of random walk Metropolis algorithms. Annals of<br />Applied Probability 7, 110–120.<br />[50] Roberts, G. O., Sahu, S. K., 1997. Updating Schemes, Correlation Struc-<br />ture, Blocking and Parameterization for the Gibbs Sampler. Journal of<br />the Royal Statistical Society. Series B (Methodological) 59 (2).<br />[51] Rubin, D., 1988. Using the SIR algorithm to simulate posterior distri-<br />butions. In Bayesian Statistics 3 (J. M. Bernardo, M. H. DeGroot, D.<br />V. Lindley and A. F. M. Smith, eds.) 395-402. Oxford Univ. Press.<br />[52] Sahu, S., Zhigljavsky, A., 2003. Self regenerative markov chain monte<br />carlo with adaptation. Bernoulli 9, 395–422.<br />[53] Taylor, M. B., Diggle, J. P., 2012. INLA or MCMC? A Tutorial and<br />Comparative Evaluation for Spatial Prediction in log-Gaussian Cox Pro-<br />cesses. ArXiv:1202.1738.<br />[54] Vanhatalo, J., Vehtari, A., 2007. Sparse Log Gaussian Processes via<br />MCMC for Spatial Epidemiology. Journal of Machine Learning Research<br />- Proceedings Track 1, 73–89.<br />[55]ˇSm´ ıdl, V., Hofman, R., 2014. Efficient Sequential Monte Carlo Sam-<br />pling for Continuous Monitoring of a Radiation Situation. Technomet-<br />rics 56 (4), 514–528.<br />[56] Williams, C. K. I., Barber, D., 1998. Bayesian classification with Gaus-<br />sian processes. IEEE Transactions on Pattern Analysis and Machine<br />Intelligence 20, 1342–1351.<br />25</p>  <p>Page 26</p> <p>Appendix A. Convergence of samplers with the RBF covariance<br />Concrete dataset - RBF covariance<br />0 2000 400060008000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />AMIS<br />AMIS−D<br />MAMIS<br />MAMIS−D<br />0 200040006000 8000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />MH−I<br />MH−D<br />MH−H<br />EOT(MH−I)<br />EOT(MH−D)<br />EOT(MH−H)<br />0 2000 4000 6000 8000 1000012000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />HMC−I<br />HMC−D<br />HMC−H<br />EOT(HMC−I)<br />EOT(HMC−D)<br />EOT(HMC−H)<br />020004000 6000 8000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />NUT−I<br />NUT−D<br />NUT−H<br />0 200040006000 8000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />NUTDA−I<br />NUTDA−D<br />NUTDA−H<br />EOT(NUTDA−I)<br />EOT(NUTDA−D)<br />EOT(NUTDA−H)<br />Figure A.2: Convergence of AMIS/MAMIS, MH, HMC, NUTS, NUTSDA, SS for the<br />Concrete dataset. EOT stands for ”end of tuning”.<br />26</p>  <p>Page 27</p> <p>Housing dataset - RBF covariance<br />0 20004000 60008000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />AMIS<br />AMIS−D<br />MAMIS<br />MAMIS−D<br />0 20004000 6000 8000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />MH−I<br />MH−D<br />MH−H<br />EOT(MH−I)<br />EOT(MH−D)<br />EOT(MH−H)<br />0 2000 40006000 8000 1000012000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />HMC−I<br />HMC−D<br />HMC−H<br />EOT(HMC−I)<br />EOT(HMC−D)<br />EOT(HMC−H)<br />0 2000 400060008000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />NUT−I<br />NUT−D<br />NUT−H<br />020004000 60008000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />NUTDA−I<br />NUTDA−D<br />NUTDA−H<br />EOT(NUTDA−I)<br />EOT(NUTDA−D)<br />EOT(NUTDA−H)<br />Figure A.3: Convergence of AMIS/MAMIS, MH, HMC, NUTS, NUTSDA, SS for the<br />Housing dataset. EOT stands for ”end of tuning”.<br />27</p>  <p>Page 28</p> <p>Parkinsons dataset - RBF covariance<br />0 20004000 60008000<br />0.00<br />0.01<br />0.02<br />0.03<br />0.04<br />0.05<br />number of n3 operations<br />IQR<br />AMIS<br />AMIS−D<br />MAMIS<br />MAMIS−D<br />0 2000400060008000<br />0.00<br />0.05<br />0.10<br />0.15<br />number of n3 operations<br />IQR<br />MH−I<br />MH−D<br />MH−H<br />EOT(MH−I)<br />EOT(MH−D)<br />EOT(MH−H)<br />0 2000 40006000 800010000 12000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />HMC−I<br />HMC−D<br />HMC−H<br />EOT(HMC−I)<br />EOT(HMC−D)<br />EOT(HMC−H)<br />0 20004000 6000 8000<br />0.00<br />0.05<br />0.10<br />0.15<br />number of n3 operations<br />IQR<br />NUT−I<br />NUT−D<br />NUT−H<br />0 2000400060008000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />NUTDA−I<br />NUTDA−D<br />NUTDA−H<br />EOT(NUTDA−I)<br />EOT(NUTDA−D)<br />EOT(NUTDA−H)<br />Figure A.4: Convergence of AMIS/MAMIS, MH, HMC, NUTS, NUTSDA, SS for the<br />Parkinsons dataset. EOT stands for ”end of tuning”.<br />28</p>  <p>Page 29</p> <p>Appendix B. Convergence of samplers with the ARD covariance<br />Concrete dataset - ARD covariance<br />0 20004000 6000 8000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />AMIS<br />AMIS−D<br />MAMIS<br />MAMIS−D<br />02000 4000 60008000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />MH−I<br />MH−D<br />MH−H<br />EOT(MH−I)<br />EOT(MH−D)<br />EOT(MH−H)<br />02000 40006000 800010000<br />0.0<br />0.1<br />0.2<br />0.3<br />0.4<br />number of n3 operations<br />IQR<br />HMC−I<br />HMC−D<br />HMC−H<br />EOT(HMC−I)<br />EOT(HMC−D)<br />EOT(HMC−H)<br />02000400060008000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />NUT−I<br />NUT−D<br />NUT−H<br />0 2000400060008000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />NUTDA−I<br />NUTDA−D<br />NUTDA−H<br />EOT(NUTDA−I)<br />EOT(NUTDA−D)<br />EOT(NUTDA−H)<br />Figure B.5: Convergence of AMIS/MAMIS, MH, HMC, NUTS, NUTSDA, SS for the<br />Concrete dataset. EOT stands for ”end of tuning”.<br />29</p>  <p>Page 30</p> <p>Housing dataset - ARD covariance<br />020004000 60008000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />AMIS<br />AMIS−D<br />0 2000 40006000 8000<br />0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0<br />number of n3 operations<br />IQR<br />MH−I<br />MH−D<br />MH−H<br />EOT(MH−I)<br />EOT(MH−D)<br />EOT(MH−H)<br />0 2000400060008000<br />0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0<br />number of n3 operations<br />IQR<br />HMC−I<br />HMC−D<br />HMC−H<br />EOT(HMC−I)<br />EOT(HMC−D)<br />EOT(HMC−H)<br />02000 4000 60008000<br />0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0<br />number of n3 operations<br />IQR<br />NUT−I<br />NUT−D<br />NUT−H<br />020004000 6000 8000<br />0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0<br />number of n3 operations<br />IQR<br />NUTDA−I<br />NUTDA−D<br />NUTDA−H<br />EOT(NUTDA−I)<br />EOT(NUTDA−D)<br />EOT(NUTDA−H)<br />020004000 60008000<br />0.0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />number of n3 operations<br />IQR<br />AMIS−MAMIS<br />MAMIS−P<br />EOT(AMIS−MAMIS)<br />Figure B.6: Convergence of AMIS/MAMIS, MH, HMC, NUTS, NUTSDA, SS for the<br />Housing dataset. EOT stands for ”end of tuning”.<br />30</p>  <p>Page 31</p> <p>Parkinsons dataset - ARD covariance<br />0 200040006000 8000<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />number of n3 operations<br />IQR<br />AMIS<br />AMIS−D<br />0 2000 4000 60008000<br />0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0<br />number of n3 operations<br />IQR<br />MH−I<br />MH−D<br />MH−H<br />EOT(MH−I)<br />EOT(MH−D)<br />EOT(MH−H)<br />0 200040006000 8000 10000 12000<br />0.0<br />0.5<br />1.0<br />1.5<br />2.0<br />number of n3 operations<br />IQR<br />HMC−I<br />HMC−D<br />HMC−H<br />EOT(HMC−I)<br />EOT(HMC−D)<br />EOT(HMC−H)<br />0 50001000015000<br />0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0<br />number of n3 operations<br />IQR<br />NUT−I<br />NUT−D<br />NUT−H<br />0 5000 1000015000<br />0.0<br />0.5<br />1.0<br />1.5<br />2.0<br />number of n3 operations<br />IQR<br />NUTDA−I<br />NUTDA−D<br />NUTDA−H<br />EOT(NUTDA−I)<br />EOT(NUTDA−D)<br />EOT(NUTDA−H)<br />0 20004000600080001000012000 14000<br />0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />number of n3 operations<br />IQR<br />AMIS−MAMIS<br />AMIS−MAMIS’<br />AMIS−MAMIS&quot;<br />EOT(AMIS−MAMIS)<br />EOT(AMIS−MAMIS’)<br />EOT(AMIS−MAMIS&quot;)<br />Figure B.7: Convergence of AMIS/MAMIS, MH, HMC, NUTS, NUTSDA, SS for the<br />Parkinsons dataset. EOT stands for ”end of tuning”.<br />31</p>  <a href="https://www.researchgate.net/profile/Vasek_Smidl/publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes/links/55e94dff08ae65b6389aee89.pdf">Download full-text</a> </div> <div id="rgw20_56ab1d5f02afa" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw21_56ab1d5f02afa">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw22_56ab1d5f02afa"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Vasek_Smidl/publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes/links/55e94dff08ae65b6389aee89.pdf" class="publication-viewer" title="55e94dff08ae65b6389aee89.pdf">55e94dff08ae65b6389aee89.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Vasek_Smidl">Vasek Smidl</a> &middot; Sep 4, 2015 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw23_56ab1d5f02afa"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://de.arxiv.org/pdf/1508.01050" target="_blank" rel="nofollow" class="publication-viewer" title="Adaptive Multiple Importance Sampling for Gaussian Processes">Adaptive Multiple Importance Sampling for Gaussian...</a> </div>  <div class="details">   Available from <a href="http://de.arxiv.org/pdf/1508.01050" target="_blank" rel="nofollow">de.arxiv.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw25_56ab1d5f02afa" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw26_56ab1d5f02afa">  </ul> </div> </div>   <div id="rgw16_56ab1d5f02afa" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw17_56ab1d5f02afa"> <div> <h5> <a href="publication/233391051_Consistency_of_the_Adaptive_Multiple_Importance_Sampling" class="color-inherit ga-similar-publication-title"><span class="publication-title">Consistency of the Adaptive Multiple Importance Sampling</span></a>  </h5>  <div class="authors"> <a href="researcher/12567635_Jean-Michel_Marin" class="authors ga-similar-publication-author">Jean-Michel Marin</a>, <a href="researcher/19500935_Pierre_Pudlo" class="authors ga-similar-publication-author">Pierre Pudlo</a>, <a href="researcher/59407974_Mohammed_Sedki" class="authors ga-similar-publication-author">Mohammed Sedki</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56ab1d5f02afa"> <div> <h5> <a href="publication/282792615_MCMC-driven_adaptive_multiple_importance_sampling" class="color-inherit ga-similar-publication-title"><span class="publication-title">MCMC-driven adaptive multiple importance sampling</span></a>  </h5>  <div class="authors"> <a href="researcher/59385370_L_Martino" class="authors ga-similar-publication-author">L. Martino</a>, <a href="researcher/2065165513_V_Elvira" class="authors ga-similar-publication-author">V. Elvira</a>, <a href="researcher/8585845_D_Luengo" class="authors ga-similar-publication-author">D. Luengo</a>, <a href="researcher/38286247_J_Corander" class="authors ga-similar-publication-author">J. Corander</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56ab1d5f02afa"> <div> <h5> <a href="publication/279261214_Estimating_Source_Term_Parameters_through_Probabilistic_Bayesian_inference_An_Approach_based_on_an_Adaptive_Multiple_Importance_Sampling_Algorithm" class="color-inherit ga-similar-publication-title"><span class="publication-title">Estimating Source Term Parameters through Probabilistic Bayesian inference: An Approach based on an Adaptive Multiple Importance Sampling Algorithm</span></a>  </h5>  <div class="authors"> <a href="researcher/2072157235_Harizo_Rajaona" class="authors ga-similar-publication-author">Harizo Rajaona</a>, <a href="researcher/82298952_Patrick_Armand" class="authors ga-similar-publication-author">Patrick Armand</a>, <a href="researcher/54570685_Francois_Septier" class="authors ga-similar-publication-author">François Septier</a>, <a href="researcher/8950890_Yves_Delignon" class="authors ga-similar-publication-author">Yves Delignon</a>, <a href="researcher/54024626_Christophe_Olry" class="authors ga-similar-publication-author">Christophe Olry</a>, <a href="researcher/78507995_Jacques_Moussafir" class="authors ga-similar-publication-author">Jacques Moussafir</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw39_56ab1d5f02afa" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw40_56ab1d5f02afa">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw41_56ab1d5f02afa" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=Xb4GkcDeHgsQo030kaFjso4GDFJByUwjRokoxYVN1WNSlMSWiCacTBIMqrKD-2qD" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="G3MrGu/JEB/Se8PSOiktyv0mc7XOebwaEauR3YXuhwC+TgEp4yXYDhPRYwXaXndb5GXhzQgqlGzedzjIBWKHVS0IvEKzzDnhPVZRhyYDG/puRXLzWBx9cmmJ5UQ5qpJaMzC2pXysX1qnj7px7ucYSSD8cQV10ChXHHNCgmo1ZD1isLRfpkp2KVLePsNW8t0RlsEknX3cJnBbyjbYB0+pocqYOUwuazh7gH1dfTEFFMtxOQrRwRS7Yi9H4WEdGr/kIy3ZDQ3YuUI+ADdO1NJD/zkVQqjFTMxu6dHxXsHeoqM="/> <input type="hidden" name="urlAfterLogin" value="publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjgwNzczMDExX0FkYXB0aXZlX011bHRpcGxlX0ltcG9ydGFuY2VfU2FtcGxpbmdfZm9yX0dhdXNzaWFuX1Byb2Nlc3Nlcw%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjgwNzczMDExX0FkYXB0aXZlX011bHRpcGxlX0ltcG9ydGFuY2VfU2FtcGxpbmdfZm9yX0dhdXNzaWFuX1Byb2Nlc3Nlcw%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjgwNzczMDExX0FkYXB0aXZlX011bHRpcGxlX0ltcG9ydGFuY2VfU2FtcGxpbmdfZm9yX0dhdXNzaWFuX1Byb2Nlc3Nlcw%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw42_56ab1d5f02afa"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 530;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Xiaoyu Xiong","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A309409379487746%401450780352548_m\/Xiaoyu_Xiong.png","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Xiaoyu_Xiong","institution":"University of Glasgow","institutionUrl":false,"widgetId":"rgw4_56ab1d5f02afa"},"id":"rgw4_56ab1d5f02afa","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=9648467","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab1d5f02afa"},"id":"rgw3_56ab1d5f02afa","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=280773011","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":280773011,"title":"Adaptive Multiple Importance Sampling for Gaussian Processes","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"08\/2015;","publicationDateRobot":"2015-08","article":""}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1508.01050","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Adaptive Multiple Importance Sampling for Gaussian Processes"},{"key":"rft.date","value":"2015"},{"key":"rft.au","value":"Xiaoyu Xiong,V\u00e1clav \u0160m\u00eddl,Maurizio Filippone"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56ab1d5f02afa"},"id":"rgw6_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=280773011","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":280773011,"peopleItems":[{"data":{"authorNameOnPublication":"Xiaoyu Xiong","accountUrl":"profile\/Xiaoyu_Xiong","accountKey":"Xiaoyu_Xiong","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A309409379487746%401450780352548_m\/Xiaoyu_Xiong.png","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Xiaoyu Xiong","profile":{"professionalInstitution":{"professionalInstitutionName":"University of Glasgow","professionalInstitutionUrl":"institution\/University_of_Glasgow"}},"professionalInstitutionName":"University of Glasgow","professionalInstitutionUrl":"institution\/University_of_Glasgow","url":"profile\/Xiaoyu_Xiong","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A309409379487746%401450780352548_l\/Xiaoyu_Xiong.png","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Xiaoyu_Xiong","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw9_56ab1d5f02afa"},"id":"rgw9_56ab1d5f02afa","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=9648467&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University of Glasgow","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":3,"publicationUid":280773011,"widgetId":"rgw8_56ab1d5f02afa"},"id":"rgw8_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=9648467&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=3&publicationUid=280773011","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Vasek Smidl","accountUrl":"profile\/Vasek_Smidl","accountKey":"Vasek_Smidl","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272548118134785%401441991943162_m\/Vasek_Smidl.png","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Vasek Smidl","profile":{"professionalInstitution":{"professionalInstitutionName":"Academy of Sciences of the Czech Republic","professionalInstitutionUrl":"institution\/Academy_of_Sciences_of_the_Czech_Republic"}},"professionalInstitutionName":"Academy of Sciences of the Czech Republic","professionalInstitutionUrl":"institution\/Academy_of_Sciences_of_the_Czech_Republic","url":"profile\/Vasek_Smidl","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272548118134785%401441991943162_l\/Vasek_Smidl.png","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Vasek_Smidl","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw11_56ab1d5f02afa"},"id":"rgw11_56ab1d5f02afa","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=2357087&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Academy of Sciences of the Czech Republic","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":3,"publicationUid":280773011,"widgetId":"rgw10_56ab1d5f02afa"},"id":"rgw10_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=2357087&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=3&publicationUid=280773011","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Maurizio Filippone","accountUrl":"profile\/Maurizio_Filippone","accountKey":"Maurizio_Filippone","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273666076311560%401442258485613_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Maurizio Filippone","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":false}},"professionalInstitutionName":false,"professionalInstitutionUrl":false,"url":"profile\/Maurizio_Filippone","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273666076311560%401442258485613_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Maurizio_Filippone","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw13_56ab1d5f02afa"},"id":"rgw13_56ab1d5f02afa","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=4709876&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":false,"score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":3,"publicationUid":280773011,"widgetId":"rgw12_56ab1d5f02afa"},"id":"rgw12_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=4709876&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=3&publicationUid=280773011","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab1d5f02afa"},"id":"rgw7_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=280773011&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":280773011,"abstract":"<noscript><\/noscript><div>In applications of Gaussian processes where quantification of uncertainty is<br \/>\na strict requirement, it is necessary to accurately characterize the posterior<br \/>\ndistribution over Gaussian process covariance parameters. Normally, this is<br \/>\ndone by means of Markov chain Monte Carlo (MCMC) algorithms. Focusing on<br \/>\nGaussian process regression where the marginal likelihood is computable but<br \/>\nexpensive to evaluate, this paper studies algorithms based on importance<br \/>\nsampling to carry out expectations under the posterior distribution over<br \/>\ncovariance parameters. The results indicate that expectations computed using<br \/>\nAdaptive Multiple Importance Sampling converge faster per unit of computation<br \/>\nthan those computed with MCMC algorithms for models with few covariance<br \/>\nparameters, and converge as fast as MCMC for models with up to around twenty<br \/>\ncovariance parameters.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw14_56ab1d5f02afa"},"id":"rgw14_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=280773011","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/links\/55e94dff08ae65b6389aee89\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw15_56ab1d5f02afa"},"id":"rgw15_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab1d5f02afa"},"id":"rgw5_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=280773011&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":12567635,"url":"researcher\/12567635_Jean-Michel_Marin","fullname":"Jean-Michel Marin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":19500935,"url":"researcher\/19500935_Pierre_Pudlo","fullname":"Pierre Pudlo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":59407974,"url":"researcher\/59407974_Mohammed_Sedki","fullname":"Mohammed Sedki","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Nov 2012","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/233391051_Consistency_of_the_Adaptive_Multiple_Importance_Sampling","usePlainButton":true,"publicationUid":233391051,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/233391051_Consistency_of_the_Adaptive_Multiple_Importance_Sampling","title":"Consistency of the Adaptive Multiple Importance Sampling","displayTitleAsLink":true,"authors":[{"id":12567635,"url":"researcher\/12567635_Jean-Michel_Marin","fullname":"Jean-Michel Marin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":19500935,"url":"researcher\/19500935_Pierre_Pudlo","fullname":"Pierre Pudlo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":59407974,"url":"researcher\/59407974_Mohammed_Sedki","fullname":"Mohammed Sedki","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/233391051_Consistency_of_the_Adaptive_Multiple_Importance_Sampling","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/233391051_Consistency_of_the_Adaptive_Multiple_Importance_Sampling\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab1d5f02afa"},"id":"rgw17_56ab1d5f02afa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=233391051","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":59385370,"url":"researcher\/59385370_L_Martino","fullname":"L. Martino","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2065165513,"url":"researcher\/2065165513_V_Elvira","fullname":"V. Elvira","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8585845,"url":"researcher\/8585845_D_Luengo","fullname":"D. Luengo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38286247,"url":"researcher\/38286247_J_Corander","fullname":"J. Corander","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/282792615_MCMC-driven_adaptive_multiple_importance_sampling","usePlainButton":true,"publicationUid":282792615,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/282792615_MCMC-driven_adaptive_multiple_importance_sampling","title":"MCMC-driven adaptive multiple importance sampling","displayTitleAsLink":true,"authors":[{"id":59385370,"url":"researcher\/59385370_L_Martino","fullname":"L. Martino","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2065165513,"url":"researcher\/2065165513_V_Elvira","fullname":"V. Elvira","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8585845,"url":"researcher\/8585845_D_Luengo","fullname":"D. Luengo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38286247,"url":"researcher\/38286247_J_Corander","fullname":"J. Corander","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/282792615_MCMC-driven_adaptive_multiple_importance_sampling","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/282792615_MCMC-driven_adaptive_multiple_importance_sampling\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab1d5f02afa"},"id":"rgw18_56ab1d5f02afa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=282792615","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2072157235,"url":"researcher\/2072157235_Harizo_Rajaona","fullname":"Harizo Rajaona","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":82298952,"url":"researcher\/82298952_Patrick_Armand","fullname":"Patrick Armand","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":54570685,"url":"researcher\/54570685_Francois_Septier","fullname":"Fran\u00e7ois Septier","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":8950890,"url":"researcher\/8950890_Yves_Delignon","fullname":"Yves Delignon","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":2,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Sep 2014","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/279261214_Estimating_Source_Term_Parameters_through_Probabilistic_Bayesian_inference_An_Approach_based_on_an_Adaptive_Multiple_Importance_Sampling_Algorithm","usePlainButton":true,"publicationUid":279261214,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/279261214_Estimating_Source_Term_Parameters_through_Probabilistic_Bayesian_inference_An_Approach_based_on_an_Adaptive_Multiple_Importance_Sampling_Algorithm","title":"Estimating Source Term Parameters through Probabilistic Bayesian inference: An Approach based on an Adaptive Multiple Importance Sampling Algorithm","displayTitleAsLink":true,"authors":[{"id":2072157235,"url":"researcher\/2072157235_Harizo_Rajaona","fullname":"Harizo Rajaona","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":82298952,"url":"researcher\/82298952_Patrick_Armand","fullname":"Patrick Armand","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":54570685,"url":"researcher\/54570685_Francois_Septier","fullname":"Fran\u00e7ois Septier","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8950890,"url":"researcher\/8950890_Yves_Delignon","fullname":"Yves Delignon","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":54024626,"url":"researcher\/54024626_Christophe_Olry","fullname":"Christophe Olry","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":78507995,"url":"researcher\/78507995_Jacques_Moussafir","fullname":"Jacques Moussafir","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/279261214_Estimating_Source_Term_Parameters_through_Probabilistic_Bayesian_inference_An_Approach_based_on_an_Adaptive_Multiple_Importance_Sampling_Algorithm","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/279261214_Estimating_Source_Term_Parameters_through_Probabilistic_Bayesian_inference_An_Approach_based_on_an_Adaptive_Multiple_Importance_Sampling_Algorithm\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab1d5f02afa"},"id":"rgw19_56ab1d5f02afa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=279261214","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw16_56ab1d5f02afa"},"id":"rgw16_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=280773011&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":280773011,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":280773011,"publicationType":"article","linkId":"55e94dff08ae65b6389aee89","fileName":"55e94dff08ae65b6389aee89.pdf","fileUrl":"profile\/Vasek_Smidl\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/links\/55e94dff08ae65b6389aee89.pdf","name":"Vasek Smidl","nameUrl":"profile\/Vasek_Smidl","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Sep 4, 2015","fileSize":"317.98 KB","widgetId":"rgw22_56ab1d5f02afa"},"id":"rgw22_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=280773011&linkId=55e94dff08ae65b6389aee89&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":280773011,"publicationType":"article","linkId":"55d12ba808ae6a881385eab6","fileName":"Adaptive Multiple Importance Sampling for Gaussian Processes","fileUrl":"http:\/\/de.arxiv.org\/pdf\/1508.01050","name":"de.arxiv.org","nameUrl":"http:\/\/de.arxiv.org\/pdf\/1508.01050","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw23_56ab1d5f02afa"},"id":"rgw23_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=280773011&linkId=55d12ba808ae6a881385eab6&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw21_56ab1d5f02afa"},"id":"rgw21_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=280773011&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":14,"valueFormatted":"14","widgetId":"rgw24_56ab1d5f02afa"},"id":"rgw24_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=280773011","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw20_56ab1d5f02afa"},"id":"rgw20_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=280773011&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":280773011,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw26_56ab1d5f02afa"},"id":"rgw26_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=280773011&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":14,"valueFormatted":"14","widgetId":"rgw27_56ab1d5f02afa"},"id":"rgw27_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=280773011","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw25_56ab1d5f02afa"},"id":"rgw25_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=280773011&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"arXiv:1508.01050v1  [stat.CO]  5 Aug 2015\nAdaptive Multiple Importance Sampling for Gaussian\nProcesses\nXiaoyu Xionga,\u2217, V\u00b4 aclav\u02c7Sm\u00b4 \u0131dlb, Maurizio Filipponea\naSchool of Computing Science, University of Glasgow, UK\nbInstitute of Information Theory and Automation, Prague, Czech Republic\nAbstract\nIn applications of Gaussian processes where quantification of uncertainty is\na strict requirement, it is necessary to accurately characterize the posterior\ndistribution over Gaussian process covariance parameters. Normally, this is\ndone by means of Markov chain Monte Carlo (MCMC) algorithms. Focusing\non Gaussian process regression where the marginal likelihood is computable\nbut expensive to evaluate, this paper studies algorithms based on importance\nsampling to carry out expectations under the posterior distribution over co-\nvariance parameters. The results indicate that expectations computed using\nAdaptive Multiple Importance Sampling converge faster per unit of compu-\ntation than those computed with MCMC algorithms for models with few\ncovariance parameters, and converge as fast as MCMC for models with up\nto around twenty covariance parameters.\n1. Introduction\nGaussian Processes (GPs) have been proved to be a successful class of\nstatistical inference methods for data analysis in several applied domains,\nsuch as pattern recognition [47, 3, 12], neuroimaging [13], signal processing\n[28], Bayesian optimization [26], and emulation and calibration of computer\ncodes [27]. The features that make GPs appealing are the nonparametric\n\u2217Corresponding author\nEmail addresses: x.xiong.1@research.gla.ac.uk (Xiaoyu Xiong),\nsmidl@utia.cas.cz (V\u00b4 aclav\u02c7Sm\u00b4 \u0131dl), maurizio.filippone@glasgow.ac.uk (Maurizio\nFilippone)"},{"page":2,"text":"formulation that yields the possibility to flexibly model data, and the pa-\nrameterization that makes it possible to gain some understanding on the\nsystem under study. These properties hinge on the parameterization of the\nGP covariance function and on the way GP covariance parameters are opti-\nmized or inferred.\nIt is established that optimizing covariance parameters can severely affect\nthe ability of the model to quantify uncertainty in predictions [36, 12, 13, 53].\nTherefore, in applications where this is undesirable, it is necessary to ac-\ncurately characterize the posterior distribution over covariance parameters\nand propagate this source of uncertainty forward to predictions. Inference\nof GP covariance parameters is analytically intractable, and standard in-\nference methods require repeatedly calculating the so called marginal likeli-\nhood. When the likelihood function is not Gaussian, e.g., in classification,\nin ordinal regression, in modeling of stochastic volatility, in Cox-processes,\nthe marginal likelihood cannot be computed analytically, and this has moti-\nvated a large body of the literature to develop approximate inference meth-\nods [56, 43, 31, 47, 41, 24], reparameterization techniques [36, 34, 54, 14],\nand exact inference with unbiased computations of the marginal likelihood\n[12, 10]. Even in the case of a Gaussian likelihood, which makes the marginal\nlikelihood computable, inference is generally costly because the computation\nof the marginal likelihood has time complexity scaling with the cube of the\nnumber of input vectors [11].\nThis paper focuses on the problem of inferring GP covariance parameters\nwhen the marginal likelihood is computable but expensive, namely when\nGPs are used for regression and the likelihood is Gaussian. To date, most\nof the literature proposes the use of Markov chain Monte Carlo (MCMC)\nmethods to characterize the posterior distribution over covariance parame-\nters [36, 34, 14, 29]. Generally, MCMC methods are based on the iteration of\nthe following two operations (i) proposal and (ii) an accept\/reject step. The\nliterature provides optimal acceptance rates for a number of popular MCMC\nalgorithms, e.g., around 25% for the MH algorithm [49] and about 65% for the\nHMC algorithm [2, 38]. Given that calculating the marginal likelihood and\nits gradient with respect to the covariance parameters is expensive, whenever\nMCMC algorithms reject a proposal, a considerable amount of computations\nis therefore wasted. In this work, we investigate the possibility to carry out\nexpectations under the posterior distribution over covariance parameters us-\ning algorithms based on importance sampling. In particular, we compare\nthe Adaptive Multiple Importance Sampling (AMIS) algorithm [5, 55] with\n2"},{"page":3,"text":"a number of popular MCMC algorithms, namely MH, HMC, and Slice Sam-\npling (SS) [37]. Along with HMC, we also compare recently proposed No\nU-Turn Samplers (NUTS) [25], where the number-of-steps and the stepsize\nparameters are automatically tuned. The algorithms are compared on con-\nvergence speed with respect to computational complexity.\nA first attempt in the direction of demonstrating the applicability of\nimportance sampling methods for inference in GPs can be found in [46]. The\nmain contribution of this work is the extensive comparison of importance\nsampling-based inference with MCMC-based inference for GP regression with\nsquared exponential covariances (i) with a global length-scale parameter (for\nsimplicity, we call it the RBF kernel case, where RBF stands for Radial\nBasis Function) and (ii) with Automatic Relevance Determination (ARD)\n(for simplicity, we call it the ARD kernel case). The results demonstrate\nthe value of our proposal. In particular, the results indicate that AMIS\nis generally competitive with, if not outperforming, MCMC algorithm in\nterms of convergence speed over computational cost of expectations under\nthe posterior distribution of covariance parameters. Furthermore, the results\nsuggest that AMIS is a valid alternative to MCMC algorithms even in the\ncase of moderately large dimensional parameter spaces, which is common\nwhen employing ARD covariances.\nThe paper is organised as follows. In section 2 we provide a brief review\nof GP regression and Bayesian inference. Section 3 presents the sampling\napproaches considered in this work. Section 4 reports the experiments and\nresults. In section 5, we conclude the paper.\n2. Gaussian Processes and Bayesian Inference\n2.1. Gaussian Processes\nLet X be the set of all input vectors xi\u2208 Rd(1 \u2264 i \u2264 n) and let y be the\nvector consisting of the corresponding observations yi. In most GP models,\nthe assumption is that observations are conditionally independent given a\nset of n latent variables. Such latent variables are modeled as realizations of\na function f(x) at the input vectors x1,...,xn, i.e., f = {f(x1),...,f(xN)}.\nLatent variables are used to express the likelihood function, that under the\nassumption of independence becomes p(y | f) =?n\ndepending on the particular type of data being modeled (e.g., Gaussian for\nregression, Bernoulli for probit classification with probability P(yi= 1) =\n\u03a6(f(xi)) with \u03a6 defined as the cumulative normal distribution).\ni=1p(yi| fi), with p(yi| fi)\n3"},{"page":4,"text":"What characterizes GP models is the way the latent variables are spec-\nified. In particular, in GP models the assumption is that the function f(x)\nis distributed as a GP, which implies that the latent variables f are jointly\ndistributed as a Gaussian f \u223c N(0,K), where K is the covariance matrix.\nThe entries of the covariance matrix K are specified by a covariance (kernel)\nfunction between pair of input vectors.\nIn the GP regression setting, the observations y are modeled to be Gaus-\nsian distributed with mean of f (latent variables) and covariance \u03bbI, where I\ndenotes the identity matrix. In this setting, the likelihood and the GP priors\nform a conjugate pair, so latent variables can be integrated out of the model\nleading to p(y | X,\u03b8) \u223c N(0,C), where C = K + \u03bbI. For simplicity of\nnotation, we gathered all the parameters of the covariance matrix C in the\nvector \u03b8. In this work, two kinds of covariance function are considered. The\nfirst is the Radial Basis Function (RBF) kernel defined as:\n\u00b6\u2212\u03c4 ? xn\u2212 xm?2\u00a9+ \u03bb\u03b4nm\nwith \u03b4nm = 1 if n = m and zero otherwise. The parameter \u03c4 defines the\nlength-scale of the interaction between the input vectors, \u03c3 represents the\nmarginal variance for each latent variable and \u03bb is the variance of noise on the\nobservations. For simplicity of notation, let \u03b8 denotes the vector comprising\n\u03c3, \u03c4 and \u03bb, i.e., \u03b8 = (\u03c3,\u03c4,\u03bb).\nThe second is the Automatic Relevance Determination (ARD) kernel\nwhich takes the form:\n\uf8f1\ni=1\nc(xn,xm) = \u03c3 exp(1)\nc(xn,xm) = \u03c3exp\n\uf8f2\n\uf8f3\u2212\nd\n?\n\u03c4i(xn(i)\u2212 xm(i))2\n\uf8fc\n\uf8fd\n\uf8fe+ \u03bb\u03b4nm\n(2)\nwhere we denote the parameters by \u03b8 = (\u03c3,\u03c41,...,\u03c4d,\u03bb). The advantage of\nthe ARD kernel is that it accounts for the influence of each feature on the\nprediction of y, with a larger value of parameter (\u03c41,...,\u03c4d) indicating a higher\ninfluence of the corresponding feature [28].\n2.2. Bayesian Inference\nWhen making predictions, using a point estimate of \u03b8 has been reported\nto potentially underestimate the uncertainty in predictions or yield inaccu-\nrate assessment of the relative influence of different features [13] [12] [3].\nTherefore, a Bayesian approach is usually adopted to overcome these lim-\nitations which entails characterizing the posterior distribution p(\u03b8 | y,X).\n4"},{"page":5,"text":"Generally speaking, under the Bayesian framework, predictions can be seen\nas the expectation of a function of \u03b8 with respect to the posterior distri-\nbution, i.e., Ep(\u03b8|y,X)[f(\u03b8)]. Setting f(\u03b8) = p(y\u22c6| y,\u03b8,X,x\u22c6), the resulting\npredictive distribution for the label y\u22c6, associated with a new input vector\nx\u22c6, is as follows:\n?\nUnder the Bayes\u2019 rule, we obtain the desired posterior distribution over\n\u03b8 as\np(\u03b8 | y,X) =\nwhere p(\u03b8) encodes any prior knowledge on the parameters \u03b8. The denomi-\nnator of this expression is intractable, so it is not possible to characterize the\nposterior distribution analytically.\nDespite this complication, it is possible to resort to a Monte Carlo ap-\nproximation to compute expectations under the posterior distribution over\ncovariance parameters:\np(y\u22c6| y,X,x\u22c6) =p(y\u22c6| y,\u03b8,X,x\u22c6)p(\u03b8 | y,X)d\u03b8.(3)\np(y | X,\u03b8)p(\u03b8)\n?p(y | X,\u03b8)p(\u03b8)d\u03b8\n(4)\np(y\u22c6| y,X,x\u22c6) \u22431\nN\nN\n?\ni=1\np(y\u22c6| \u03b8(i),y,X,x\u22c6)(5)\nwhere \u03b8(i)denotes the ith of N samples from p(\u03b8 | y,X).\nis usually not feasible to draw samples from p(\u03b8 | y,X) directly. Therefore,\nMCMC methods are usually employed to generate samples from the posterior\np(\u03b8 | y,X).\nAn alternative way to compute expectations, is by means of importance\nsampling, which takes the following form:\n?\nwhere q(\u03b8 | y,X) is the importance distribution. The corresponding Monte\nCarlo approximation is of the form:\nHowever, it\nEp(\u03b8|y,X)[f(\u03b8)] =f(\u03b8)p(\u03b8 | y,X)\nq(\u03b8 | y,X)q(\u03b8 | y,X)d\u03b8\n(6)\nEp(\u03b8|y,X)[f(\u03b8)] \u22431\nN\nN\n?\ni=1\nf(\u03b8(i))p(\u03b8(i)| y,X)\nq(\u03b8(i)| y,X)\n(7)\nwhere now the samples \u03b8(i)are drawn from the importance sampling distri-\nbution q(\u03b8 | y,X). The key to make this Monte Carlo estimator accurate is\n5"},{"page":6,"text":"to choose q(\u03b8 | y,X) to be similar to the function that needs to be integrated.\nIt is easy to verify that when q(\u03b8 | y,X) is proportional to the function that\nneeds to be integrated, the variance of the importance sampling estimator\nis zero. Therefore, the success of importance sampling relies on construct-\ning a tractable importance distribution q(\u03b8 | y,X) that well approximates\nf(\u03b8)p(\u03b8 | y,X). In the remainder of this paper we will discuss and employ\nmethods that adaptively construct q(\u03b8 | y,X).\nBoth Monte Carlo approximations in equations (5) and (7) converge to the\ndesired expectation, and in practice, they can estimate the desired integrals\nin equations (3) and (6) to a given level of precision [17, 15]. The focus\nof this work is to experimentally study the convergence properties of the\nexpectations in equations (3) and (6) with respect to the computational effort\nneeded to carry out the Monte Carlo approximations in equations (5) and\n(7).\n3. Sampling methods considered in this work\nIn this section, we present the state-of-the-art MCMC and AIS sam-\npling methods considered in this work. The aim is to find out whether AIS\n(AMIS\/MAMIS) can improve speed of convergence with respect to compu-\ntational complexity, compared to MCMC approaches (MH, HMC, NUTS,\nNUTDA, SS). In the remainder of this paper, for the sake of succinctness,\nwe denote log[p(y | X,\u03b8)p(\u03b8)] by L(\u03b8), which is the logarithm of the target\nposterior distribution up to constant.\n3.1. Metropolis-Hastings - MH\nThe Metropolis-Hastings algorithms [23] [33] samples from exp(L(\u03b8)) by\nrepeatedly considering randomly generated samples [35], accepting the pro-\n\u00df\nexp(L(\u03b8))q(\u03b8\u2032|\u03b8)\ndistribution, \u03b8\u2032denotes the proposed new sample, and \u03b8 denotes the current\nsample. The proposal distribution q is commonly selected to be a succession\nof random multivariate Gaussian of the form q(\u03b8\u2032| \u03b8) \u223c N(\u03b8\u2032| \u03b8,\u03a3) , with\nthe mean \u03b8 being the former state, and \u03a3 being the covariance. Due to\nthe symmetric property of q, the acceptance function reduces to the form\nmin\nTuning the MH involves a proper choice of the covariance \u03a3. However, as\nnoted in [14], it is not trivial to select the right covariance as information of\nposed moves with probability min1,exp(L(\u03b8\u2032))q(\u03b8|\u03b8\u2032)\n\u2122\n, where q is the proposal\n\u00b61,exp(L(\u03b8\u2032) \u2212 L(\u03b8))\n\u00a9. This generalise to other symmetric proposals.\n6"},{"page":7,"text":"the desired target distribution is required in most cases. Very small values of\n\u03a3 will cause slow convergence to the stationary state, while very large ones\nwill result in chains getting stuck in certain regions of the space. Ways to\noptimally tune the MH algorithms have been proposed [48] [16] [50]. Studies\non adaptively tuning the MH have also been reported [19] [20] [21] [22].\n3.2. Hybrid Monte Carlo - HMC\nHMC [35] [9] originated from Physics, where the Hamiltonian dynamics\nfunction is defined by the sum of a potential energy function of the position\nvector and a kinetic energy function of the momentum vector. When HMC\nis applied to obtaining samples from a target distribution, the parameters\nof interest \u03b8, take the role of the position, and an auxiliary \u201dmomentum\u201d\nvariable p, which is commonly assumed to be independently drawn from\nN(p | 0,M), needs to be introduced. Thus the extended target distribution\np(\u03b8,p) takes the form exp(L(\u03b8) \u22121\nConsequently, the minus log of the augmented target distribution plus some\nconstant will give an analogy with the Hamiltonian:\n2pTM\u22121p) up to a normalising constant.\nH(\u03b8,p) = \u2212L(\u03b8) +1\n2pTM\u22121p + const. (8)\nwhere \u2212L(\u03b8) is the potential energy and1\nGenerating a new sample of parameters involves two steps. The first is to\ndraw p randomly from N(p | 0,M), then propose a new \u03b8(L),p(L)through\na number of L reversible \u201dleapfrog\u201d steps that follows the Hamiltonian dy-\nnamics scheme:\np(t+\u01eb\/2)= p(t)+\u01eb\n2pTM\u22121p is the kinetic energy.\n2\u2207\u03b8L(\u03b8(t)) (9)\n\u03b8(t+\u01eb)= \u03b8(t)+ \u01ebM\u22121p(t+\u01eb\/2)\n(10)\np(t+\u01eb)= p(t+\u01eb\/2)+\u01eb\n2\u2207\u03b8L(\u03b8(t+\u01eb))(11)\nwhere \u01eb is stepsize, \u2207\u03b8denotes the gradient with respect to \u03b8. The stepsize \u01eb\nis often chosen randomly to ensure ergodicity, and the exploit of the gradient\ngives HMC one major advantage of avoiding the random walk behaviour that\noccurs in MH. The proposed \u03b8(L),p(L)will then be accepted with probability:\nmin\n\u00b61,exp(\u2212H(\u03b8(L),p(L)) + H(\u03b8,f))\n\u00a9\n(12)\n7"},{"page":8,"text":"Due to the \u201dshear\u201d transformations of the equations 9, 10, 11, where the\nupdate of one variable \u03b8 depends only on the other unchangeable variable\np [39], the leapfrog integrator is volume-preserving. The crucial property of\nreversibility and preservation of volume of HMC ensures sampling from the\ninvariant target distribution of interest. Another benefit of HMC is its better\nscalability with dimensionality compared to simple Metropolis approaches,\ndetails of which can be found in [39]. The choice of the number of steps L and\nstepsize \u01eb can heavily affect the performance of HMC, thus careful tuning of\nthese two parameters is usually needed when applying HMC. Using knowl-\nedge of scales and correlation of the position variables can also improve the\nperformance of HMC [39]. Specifically, this is achieved by transforming the\nposition variables \u03b8 to L\u22121\u03b8 or using a mass matrix M = \u03a3\u22121or (diag(\u03a3))\u22121,\nwhere \u03a3 denotes the estimate of the covariance of \u03b8, and L denotes the lower\ntriangular of the Cholesky decompositon of \u03a3. We will discuss next NUTS\nwhich automatically tunes \u01eb, L.\n3.2.1. No-U-Turn Sampler - NUTS\nAs mentioned above, the performance of HMC can be significantly influ-\nenced by the choice of \u01eb and L. Too large an \u01eb will lead to very low acceptance\nrate, while too small an \u01eb will result in waste of computation time and also\nthe risk of undesirable random walk when L is not large. Choosing L can\nbe problematic as well. When L is too large, by taking too many steps or\nlooping back where the position variable was before, it will wastes a lot of\nexpensive computations. When L is too small, the resulting random walk\nbehaviour will cause slow exploration of the state and thus poor-mixing of\nthe samples. Therefore, [25] introduced NUTS, an extension to HMC, which\nis tuning-free in the sense that it eliminates the need to choose the num-\nber of L and automatically sets the parameter \u01eb. NUTS begins with a slice\nsampling step, where a slice variable u is drawn uniformly from the inter-\nval [0,exp(L(\u03b8) \u22121\nis randomly drawn from N(p | 0,M). This gives the conditional distribu-\ntion p(\u03b8,p | u) \u223c U(\u03b8,p | {\u03b8\u2032,p\u2032| exp(L(\u03b8) \u22121\n\u03b8\u2032,p\u2032denote the proposed position and momentum respectively. Then the\nleapfrog integrator is used to build up a trajectory that doubles the pre-\nvious steps (either forwards or backwards) continuously. By doing this an\nimplicit balanced binary tree is built with each leaf node corresponding to\nthe position-momentum variables. The doubling stops when the subpath\nfrom the leftmost to the rightmost nodes of any balanced subtree of the\n2pTM\u22121p)], where \u03b8 denotes the current sample and p\n2pTM\u22121p) \u2265 u}), where\n8"},{"page":9,"text":"whole binary tree starts to make a \u201dU-turn\u201d, that is, the samples start to\nretrace their steps. The proposed position-momentum variables are sampled\nincrementally from the subtree during the doubling process, and a transition\nkernel that leaves the target distribution invariant is used at the end of the\nsimulation to accept\/reject the proposed new samples. In this way we do not\nneed to choose the number of steps L.\n3.2.2. No-U-Turn Sampler with Dual Averaging - NUTSDA\nTo address the issue of setting the stepsize \u01eb, [25] adopts an adaptation\nof the stochastic optimisation with a dual averaging scheme of [40], which\ntakes the following form:\n\u221at\n\u03b3t + t0\ni=1\n\u01ebt+1\u2190 log\u01eb1\u2212\n1\nt\n?\nHi;\n\u01ebt+1\u2190 t\u2212\u03balog\u01ebt+1+ (1 \u2212 t\u03ba)log\u01ebt\n(13)\nwhere t denotes the number of iterations; \u01eb1is the initial value of epsilon,\nfound by the heuristic that aims to obtain an acceptance rate of 0.5 using\nthe Langevin proposal with stepsize \u01eb1; \u03b3 > 0, t0> 0 are free parameters\nthat determine the shrinkage towards log\u01eb1and the stabilisation of the initial\niterations respectively; Ht= \u03b4 \u2212 HNUTS, with \u03b4 denoting the desired target\nmean acceptance rate, HNUTSbeing the average acceptance rate during the\nfinal iteration of doubling. The term t\u2212\u03ba(\u03ba \u2208 (0.5,1]) is chosen to ensure the\naveraged value \u01ebtconverge to a value for large t and hence the expectation\nof Ht(function of \u01ebt) converge to 0.\n3.3. Slice Sampling - SS\nSlice sampling [37], is another auxiliary method in which the joint prob-\nability density of the auxiliary variable u and parameters of interest \u03b8 takes\nthe form p(\u03b8,u), such that\n?\n0\n?exp(L(\u03b8))d\u03b8. This gives the marginal distribution for \u03b8:\np(\u03b8) =\n0\np(\u03b8,u) =\n1\/Z if 0 < u < exp(L(\u03b8))\notherwise\n(14)\nwhere Z =\n?exp(L(\u03b8))\n(1\/Z)du = exp(L(\u03b8))\/Z(15)\nas desired.\nfrom p(\u03b8,u), then ignore u. Specifically, this is done by alternating uniform\nTo sample from the target distribution p(\u03b8), we can sample\n9"},{"page":10,"text":"sampling of u from the vertical interval from 0 to the density exp(L(\u03b8))\nevaluated at the current state, with uniform sampling of \u03b8 from the union\nof intervals which are the horizontal \u201dslices\u201d defined by the vertical posi-\ntion. Single-variable slice sampling is easily implemented and one can do\ncomponent-wise univariate slice sampling for a multivariate distribution by\nupdating each variable in turn. This approach is reported in [37] to be easier\nto implement than Gibbs sampling and be able to sample more efficiently\nthan simple Metropolis scheme, due to its ability to adaptively choose the\nscale of changes appropriate to the region of the target distribution being\nsampled. For example, if our rough guess at the initial interval (an estimate\nfor the scale of the horizontal slice) is too small compared to the true width\nof the target density, it can be expanded by stepping out or doubling, while\nif the initial interval is too large, it can be shrunk by an efficient shrinkage\nprocedure. More elaborate multivariate slice samplers can not only adapt to\nthe scale of variables, but also to the dependencies between variables. Sam-\npling efficiency can also be improved by the \u201doverrelaxed\u201d univariate slice\nsampling and the \u201dreflective\u201d multivariate slice sampling that can suppress\nrandom walks.\n3.4. Adaptive Monte Carlo\n3.4.1. Adaptive MCMC\nIn order to improve sampling efficiency, many adaptive MCMC methods\nhave been developed during past years. Two adaptation criteria are very\ncommon in the literature. One is the optimal acceptance probability, where\nthe size and shape of the proposal distribution is scaled according to an op-\ntimal acceptance rate [16, 50]. However, as noted in [19], this hand-tuning\nis time consuming as the acceptance probability does not take into account\nthe shape of the target distribution and can be difficult when the parameters\nare of different scales and correlated. To avoid this difficulty, other adap-\ntation schemes employ moment matching, where moments of the proposal\ndistribution (e.g. mean and covariance) are matched with those of the target\ndistribution [19, 20, 22]. A further approach to adaptation takes advantage\nof the regeneration of the chain [18, 52]. The work in [1] proposes a general\nadaptation framework using stochastic approximation schemes to learn the\noptimal parameters of the proposal distribution for several statistical crite-\nria. However, devising valid adaptive MCMC methods is generally difficult\nin practice [1, 21].\n10"},{"page":11,"text":"3.4.2. Adaptive Importance Sampling - AIS\nThe difficulty of devising adaptive MCMC approaches lies in that the\nchain resulting from the adaptivity is no longer Markovian, and thus more\nelaborate ergodicity results are needed, as indicated in [19, 20, 22, 1]. There-\nfore, [4] proposed a universal adaptive sampling scheme called population\nMonte Carlo (PMC), where the difference from Sequential Monte Carlo\n(SMC) [8] is that the target distribution becomes static. This method is\nreported to have better adaptivity than MCMC due to the fact that it is\nvalidating by importance sampling justifications, which makes the ergodicity\nnot an issue in this case. At each iteration of PMC, sampling importance re-\nsampling [51] is used to generate samples that are assumed to be marginally\ndistributed from the target distribution and hence the approach is unbiased\nand can be stopped at any time. Moreover, the importance distribution can\nbe adapted using part (generated at each iteration) or all of the importance\nsample sequence. [6, 7] also introduced updating mechanisms for the weights\nof the mixture in the so called D-kernel PMC, which leads to a diminution ei-\nther in Kullback divergence between the mixture and the target distribution\nor in the asymptotic variance for a function of interest. An earlier adaptive\nimportance sampling strategy is illustrated in [42].\n3.4.3. Adaptive Multiple Importance Sampling - AMIS\n[5] proposed a new perspective of adaptive importance sampling, called\nadaptive multiple importance sampling (AMIS), where the difference with\nthe aforementioned PMC methods is that the importance weights of all sim-\nulations, produced previously as well as currently, are re-evaluated at each\niteration. This method follows the \u201cdeterministic multiple mixture\u201d sam-\npling scheme of [45]. The corresponding importance weight takes the form\nwt\ni= \u03c0(yt\ni)\/\n1\n?T\nj=0Nj\nT\n?\nl=0\nNlql(yt\ni) (16)\nwhere T is the total number of iterations, \u03c0(.) denotes the target distribu-\ntion, ql(.) denotes the importance density at each iteration, yt\ndrawn from qt(.) with 0 \u2264 t \u2264 T,1 \u2264 i \u2264 Nt. The fixed denominator in (16)\ngives the name \u201cdeterministic multiple mixture\u201d. The motivation is that this\nconstruction can achieve an upper bound on the asymptotic variance of the\nestimator without rejecting any simulations [45]. In AMIS, the parameters \u03b3\nof a parametric importance function ql(\u03b3) are sequentially updated using the\niare samples\n11"},{"page":12,"text":"entire sequence of weighted importance samples, based on efficiency criteria\nsuch as moment matching, minimum Kullback divergence with respect to\nthe target or minimum variance of the weights (see, e.g., [44] for stochastic\ngradient-based optimization of these efficiency criteria). This leads to a se-\nquence of importance distributions, q1(\u201d\nbrought by AMIS compared with other AIS techniques, convergence of this\nalgorithm still remains an issue.\n\u03b31),...,qT(?\n\u03b3T). Despite the efficiency\n3.4.4. Modified AMIS - MAMIS\nThe work in [32] proposed a modified version of AMIS called MAMIS,\naiming at solving the convergence issue in the original AMIS scheme. The dif-\nference is that the new parameters \u2019\nproduced at iteration t, i.e., xt\nThen the weights of all simulations are updated according to (16) to give the\nfinal output. The sample size Ntis suggested to grow at each iteration so as\nto improve the accuracy of \u2019\nissues of AMIS, but convergence is slower due to the fact that less samples\nare used to update the importance distribution.\n\u03b3t+1are estimated based only on samples\n1,...,xt\nNt, with classical weights \u03c0(yt\ni)\/q(yt\ni,? \u03b3t).\n\u03b3t+1. MAMIS effectively solves any convergence\n4. Experiments\n4.1. Datasets\nThe sampling methods considered in this work are implemented on three\nUCI datasets, which are listed in Table 1. The number of data points for the\nConcrete and Housing datasets are 1030 with 8 features for each data point\nand 506 with 13 features for each data point respectively. For the original\nParkinsons dataset, the number of data points is 5875 with 20 features for\neach data point, which are records for 42 patients measured at different time.\nIn our experiment, we randomly sampled 4 records for each patient, resulting\nin 168 data points in total.\n12"},{"page":13,"text":"Data sets\nConcrete\nHousing\nParkinsons\nnd\n8\n13\n20\n1030\n506\n168\nTable 1: Datasets used in this paper. n denotes the number of data points, d denotes the\ndimension of the features.\n4.2. Experimental setup\nWe compare three different covariances for the proposals in the MH al-\ngorithm. The first is the identity matrix. The second and third covariances\nare based on the inverse of the negative Hessian (denoted by H) evaluated at\nthe mode (denoted by m); one uses the full matrix, whereas another uses its\ndiagonal only, namely diag((\u2212H)\u22121). The mode m is found by the maximum\nlikelihood optimisation using the \u201cBFGS\u201d method.\nThus the proposals that we compare in this work take the form of N(\u03b8 |\nm,\u03b1I), N(\u03b8 | m,\u03b1(\u2212H)\u22121), and N(\u03b8 | m,\u03b1 diag((\u2212H)\u22121)), where \u03b1 is a\ntuning parameter. We tune \u03b1 in pilot runs until we get the desired acceptance\nrate (around 25%). By using these tuned proposals for starting a chain, we\nare able to avoid the poor-mixing of samples due to the long-stay of random\nwalk in a region largely influenced by the starting distribution.\nThe approximate distribution N(\u03b8 | m,(\u2212H)\u22121) is used to be the initial\nimportance density for AMIS\/MAMIS. It is also used to initialize several in-\ndependent sequences of samples from other samplers considered in this work.\nIn this way, valid summary inference from multiple independent sequences\ncan be obtained [17].\nAs mentioned in the previous section (introduction of HMC), to exploit\nthe scales and correlation of the position variables, we also chose three kinds\nof mass matrix for HMC, namely the identity matrix, the inverse of the\napproximate covariance, and the inverse of the diagonal of the approximate\ncovariance. The maximum leapfrog steps is 10. We then tune the stepsize \u01eb\nuntil a suggested acceptance rate (around 65%) is reached. The three forms\nof mass matrix apply to NUTS, NUTSDA as well. For AMIS\/MAMIS, we\nexplored two kinds of update of the covariance of the importance density. One\nupdates the full covariance, whereas the other updates only the diagonal of\nthe covariance.\nNUTS requires the tuning of a stepsize \u01eb. After a few trials, we set the\n13"},{"page":14,"text":"stepsize of NUTS to 0.1. Although tuning leapfrog steps and stepsize is not\nan issue in NUTDA, the parameters (\u03b3,t0,\u03ba) for the dual averaging scheme\ntherein have to be tuned by hand to produce reasonable result. By trying a\nfew settings for each parameter, finally the values \u03b3 = 0.05,t0= 30,\u03ba = 0.75\nwere used in both the RBF and ARD kernel case.\nThe slice sampling algorithm adopted in this paper makes component-\nwise updates of the parameters, where a new sample is drawn according\nto the \u201cstepping out\u201d and \u201cshrinkage\u201d procedures described in [37]. In our\nimplementation, we set the estimate of the typical size of a slice w to 1.5,\nand set the integer m limiting the size of a slice to mw to infinity, i.e, no\nlimit is put on the number of steps.\nTable 2 shows the experimental settings for AMIS\/MAMIS. AMIS\/MAMIS\nis implemented for the three datasets in both the RBF and ARD kernel cases.\nAlternative MAMIS is also implemented for the Housing and Parkinsons\ndatasets in the ARD kernel case, see 4.4.2.\nRBF kernel\nT\n1120\n46\nARD kernel\nNt\n280\n53000 + 1000t\nNt\n25\n26t\nT\nAMIS\nMAMIS\n100\nTable 2: Experimental settings for AMIS\/MAMIS. T is the total number of iterations, Nt\nis the sample size at each iteration t.\n4.3. Convergence analysis\nSince the classic\u02c6R score is for MCMC convergence analysis and not suit-\nable for AIS, convergence analysis here is performed based on the IQR (in-\nterquartile range) of the expectation of norm of parameters (Ep(\u03b8|y,X)[?\u03b8?])\nover 100 replicates of all algorithms, against the number of n3operations,\nwhich is reported to be a more reliable measure of complexity than running\ntime, as many other factors such as implementation details that do not relate\ndirectly to the actual computing complexity of the algorithms can affect the\nrunning time [14].\nFor AMIS\/MAMIS\/SS\/MH, the computing complexity lies in the com-\nputation of the function of L(\u03b8), one evaluation of which takes one Cholesky\ndecomposition of the covariance matrix (the corresponding complexity is1\noperations). While for HMC\/NUTS\/NUTSDA, the complexity lies in the\n3n3\n14"},{"page":15,"text":"evaluation of the function that returns both L(\u03b8) and its gradient. One\nevaluation of such a function causes complexity of7\nCholesky decomposition of the covariance matrix and 2 for computation of\nthe inverse of the covariance matrix when computing the gradient). When a\nnew sample is generated, the number of the evaluations of the aforementioned\ntwo functions are accumulated to compute the corresponding computational\ncomplexity.\n3n3operations (1\n3for one\n4.4. Results\n4.4.1. Notation of samplers\nTable 3 shows the notation for the different samplers considered in this\npaper.\nAMIS\/MAMIS\nAMIS\/MAMIS where the full covariance\nmatrix of the proposal distribution is\nupdated at each iteration\nAMIS\/MAMIS where only the diagonal of\nthe covariance matrix of the proposal\ndistribution is updated at each iteration\nMH where the covariance of the starting\nproposal distribution for tuning is the\nidentity matrix\nMH where the covariance of the starting\nproposal distribution for tuning is the\ndiagonal of the approximate covariance\nMH where the covariance of the starting\nproposal distribution for tuning is the\napproximate covariance\nHMC family where the mass matrix is the\nidentity matrix\nHMC family where the mass matrix is the\ninverse of the diagonal of the approximate\ncovariance\nHMC family where the mass matrix is the\ninverse of the approximate covariance\nAMIS-D\/MAMIS-D\nMH-I\nMH-D\nMH-H\nHMC-I\/NUT-I\/NUTDA-I\nHMC-D\/NUT-D\/NUTDA-D\nHMC-H\/NUT-H\/NUTDA-H\nTable 3: Notation for different samplers.\n15"},{"page":16,"text":"4.4.2. Convergence of samplers\nIn this section, we present the comparison of convergence of all samplers\nconsidered in this paper.\nAppendix A and Appendix B show the convergence results for the sam-\nplers with the RBF covariance (RBF kernel case) and ARD covariance (ARD\nkernel case) respectively. The top-left of figures in Appendix A and Appendix B\ndemonstrate the result of AMIS\/MAMIS. It can be seen that AMIS\/MAMIS\nthat exploits the full covariance structure of the proposal distribution per-\nforms better than the one that only updates the diagonal of the covariance\nmatrix of the proposal density. For the MH family (MH-I\/MH-D\/MH-H) and\nHMC family (standard HMC , NUTS, NUTSDA), figures in Appendix A\nand Appendix B show that, the methods that make use of the scales and\ncorrelation of the parameters, perform better than the one that does not\nin most cases. Also, NUTS\/NUTSDA turns out to converge much faster\nthan the standard HMC due to the fact that standard HMC has to be tuned\ncostly in pilot runs. For MH and standard HMC, the computational cost of\ntuning is counted when comparing the convergence, as is shown in top-right\nand middle-left of figures in Appendix A and Appendix B where the end\nof tuning (EOT) is indicated by three vertical dotted lines, corresponding to\nthe three variants respectively from left to right. For NUTSDA, the compu-\ntational cost of tuning the parameters of the dual averaging scheme is also\ncounted when determining the convergence, as is displayed in bottom-left of\nfigures in Appendix A and Appendix B with EOT indicated by three verti-\ncal dotted lines, relating to the three variants respectively from left to right.\nTable 4 shows the corresponding computational cost of tuning:\nConcrete dataset\nRBFARD\n6747\n6042\n10851\n1402\n1357\n682\nHousing dataset\nRBF\n4779\n7281\n10987\n1193\n1124\n670\nParkinsons dataset\nRBF ARD\n1561\n8883\n10871\n1338\n975\n728\nARD\n3924\n7726\n8860\n7433\n2424\n1866\nHMC-I\nHMC-D\nHMC-H\nNUTDA-I\nNUTDA-D\nNUTDA-H\n5910\n7316\n9451\n3528\n1582\n1023\n1340\n8469\n8736\n6488\n1951\n1794\nTable 4: Computational cost of tuning for HMC\/NUTSDA. Unit: number of n3operations.\n16"},{"page":17,"text":"Figure 1 shows the final result of AMIS, best of MAMIS, best of MH\nfamily, best of HMC family and SS for the three datasets in both the RBF\nand ARD kernel case. It is impressive to see that AMIS\/MAIMS performs\nbest among all methods in terms of convergence speed in the RBF kernel\ncase. In the ARD kernel case, AMIS also converges much faster than the\nother approaches. However, our experiments show that in the ARD kernel\ncase although MAMIS converges faster than the other approaches in the\nConcrete dataset, it converges slowly in the Housing and Parkinsons datasets,\nwhich is probably due to the higher dimensionality compared to the previous\ncases.\nIn cases where MAMIS converges slowly, we can exploit the fact that\nAMIS converges faster than MAMIS by running AMIS for a fixed number\nof iterations and then switch to MAMIS. In this way, we can ensure fast\nconvergence of the adaptive scheme while taking advantage of the proof of\nconvergence. In the experiments, we tested this AMIS-MAMIS combination\nin cases where MAMIS converges slowly. We treated samples from AMIS as\ntuning cost for MAMIS to get an accurate initial importance density as is\nshown in bottom-right of B.6 B.7 with EOT (end of tuning) indicated by the\nvertical dotted line. Three settings (Table 5) of AMIS-MAMIS were tested\nfor the Parkinsons dataset.\nNtfor\nMAMIS\nnumber of samples\ngenerated from AMIS for\ntuning the initial\nimportance density of\nMAMIS\n13000\n13000\n26000\nthe\ncorresponding\ntuning cost\nAMIS-MAMIS\nAMIS-MAMIS\u2019\nAMIS-MAMIS\u201d\n1000t\n5000t\n5000t\n4333\n4333\n8667\nTable 5: Settings for AMIS-MAMIS. Ntis the sample size at each iteration t. Unit of the\ntuning cost: number of n3operations.\nFor the Housing dataset, we tested only AMIS-MAMIS in Table 5. The\nresults for the Housing and Parkinsons datasets in the ARD kernel case prove\nthe convergence of AMIS-MAMIS. In particular, AMIS-MAMIS and AMIS-\nMAMIS\u201d seem to compete well with the other MCMC approaches in terms of\nconvergence for the Housing dataset and the Parkinsons dataset respectively.\n17"},{"page":18,"text":"As is shown in the bottom-right of Figure B.7, the best performance of AMIS-\nMAMIS\u201d for the Parkinsons dataset suggests that for higher dimensional\nproblem, a more accurate initialization and a larger sample size at each\niteration for MAMIS are necessary to achieve faster convergence.\nAnother attempt that we make in this paper to improve convergence\nspeed of the adaptive importance sampling schemes is to regularize the es-\ntimation of the parameters of the importance distribution as illustrated in\n[55]. The regularization stems from the use of an informative prior on \u03b3 of\nthe importance distribution qt(\u03b3) of MAMIS and treat the update of these\nparameters in a Bayesian fashion [30]. This construction makes it possible to\navoid situations where the importance distribution degenerates to low rank\ndue to few importance weights dominating all the others. In this work, we\nuse an informative prior based on a Gaussian approximation to the posterior\nover covariance parameters. We denote this method by MAMIS-P and in\nthe ARD kernel case it was tested only in the Housing dataset. The result\nindicates that even though MAMIS-P improves on MAMIS, its convergence\nis slower than AMIS-MAMIS (bottom-right of Figure B.6).\n18"},{"page":19,"text":"RBF\nConcrete dataset\nARD\nConcrete dataset\n0 200040006000 8000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nAMIS\nMAMIS\nMH\u2212H\nNUTDA\u2212H\nSS\n0 20004000 60008000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nAMIS\nMAMIS\nMH\u2212H\nNUTDA\u2212H\nSS\n0200040006000 8000\n0.00\n0.05\n0.10\n0.15\n0.20\nHousing dataset\nnumber of n3 operations\nIQR\nAMIS\nMAMIS\nMH\u2212H\nNUTDA\u2212H\nSS\n0200040006000 8000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nHousing dataset\nnumber of n3 operations\nIQR\nAMIS\nAMIS\u2212MAMIS\nMH\u2212D\nNUTDA\u2212H\nSS\n0 2000400060008000\n0.00\n0.05\n0.10\n0.15\nParkinsons dataset\nnumber of n3 operations\nIQR\nAMIS\nMAMIS\nMH\u2212H\nNUTDA\u2212H\nSS\n020004000 60008000100001200014000\n0.0\n0.2\n0.4\n0.6\n0.8\nParkinsons dataset\nnumber of n3 operations\nIQR\nAMIS\nAMIS\u2212MAMIS\"\nMH\u2212H\nNUTDA\u2212H\nSS\nFigure 1: Convergence of AMIS, Best of MAMIS, Best of MH family, Best of HMC family,\nSS for all datasets.\n19"},{"page":20,"text":"5. Conclusions\nIn this paper we proposed the use of adaptive importance sampling tech-\nniques to carry out expectations under the posterior distribution of covariance\nparameters in Gaussian process regression. The motivation for our proposal\nis based on the observation that calculating the marginal likelihood and its\ngradient with respect to covariance parameters in GP regression is expensive\nand standard MCMC algorithms reject proposals leading to a waste of com-\nputations. The results support our intuition that importance sampling-based\ninference of covariance parameters is competitive with MCMC algorithms.\nIn particular, the results indicate that it is possible to achieve convergence\nof expectations under the posterior distribution of covariance parameters\nfaster than employing MCMC methods in a wide range of scenarios. Even in\nthe case of around twenty parameters, adaptive importance sampling is still\ncompetitive with MCMC approaches.\nWe believe that these results are important for a number of reasons. First,\nimportance sampling-based algorithms are generally easy to implement and\ntune, and can be massively parallelized. Second, these results immediately\noffer the possibility to derive inference methods for GP models where the\nmarginal likelihood cannot be computed analytically but can be estimated\nunbiasedly; we are currently investigating the behavior of adaptive impor-\ntance sampling in these scenarios.\nReferences\n[1] Andrieu, C., Robert, C. P., 2001. Controlled MCMC for optimal sam-\npling. Bernoulli 9, 395\u2013422.\n[2] Beskos, A., Pillai, N., Roberts, G. O., Sanz-Serna, J. M., Stuart, A. M.,\n2013. Optimal tuning of hybrid Monte Carlo algorithm. Bernoulli 19,\n1501\u20131534.\n[3] Bishop, C. M., Aug. 2007. Pattern Recognition and Machine Learning\n(Information Science and Statistics), 1st Edition. Springer.\n[4] Cappe, O., Guillin, A., Marin, J. M., Robert, C. P., 2004. Population\nmonte carlo. Journal of Computational and Graphical Statistics 13, 907\u2013\n929.\n20"},{"page":21,"text":"[5] Cornuet, J.-M., Marin, J.-M., Mira, A., Robert, C. P., 2012. Adaptive\nmultiple importance sampling. Scandinavian Journal of Statistics 39,\n798\u2013812.\n[6] Douc, R., Guillin, A., Marin, J.-M., Robert, C., 2007a. Convergence of\nadaptive mixtures of importance sampling schemes. Ann. Statist. 35,\n420\u2013448.\n[7] Douc, R., Guillin, A., Marin, J.-M., Robert, C., 2007b. Minimum vari-\nance importance sampling via population monte carlo. ESAIM: Probab.\nStat. 11, 427\u2013447.\n[8] Doucet, A., deFreitas, N., Gordon, N., 2001. Sequential MCMC in Prac-\ntice. New York : Springer-Verlag.\n[9] Duane, S., Kennedy, A. D., Pendleton, B. J., Roweth, D., 1987. Hybrid\nMonte Carlo. Physics Letters B 195 (2), 216\u2013222.\n[10] Filippone, M., 2014. Bayesian inference for Gaussian process classifiers\nwith annealing and pseudo-marginal MCMC. In: 22nd International\nConference on Pattern Recognition, ICPR 2014, Stockholm, Sweden,\nAugust 24-28, 2014. IEEE, pp. 614\u2013619.\n[11] Filippone, M., Engler, R., 2015. Enabling scalable stochastic gradient-\nbased inference for Gaussian processes by employing the Unbiased LIn-\near System SolvEr (ULISSE). In: Proceedings of the 32nd International\nConference on Machine Learning, ICML 2015, Lille, France, July 6-11,\n2015.\n[12] Filippone, M., Girolami, M., 2014. Pseudo-marginal Bayesian inference\nfor Gaussian processes. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence 36 (11), 2214\u20132226.\n[13] Filippone, M., Marquand, A. F., Blain, C. R. V., Williams, S. C. R.,\nMour\u02dc ao-Miranda, J., Girolami, M., 2012. Probabilistic Prediction of\nNeurological Disorders with a Statistical Assessment of Neuroimaging\nData Modalities. Annals of Applied Statistics 6 (4), 1883\u20131905.\n[14] Filippone, M., Zhong, M., Girolami, M., 2013. A comparative evalua-\ntion of stochastic-based inference methods for Gaussian process models.\nMachine Learning 93 (1), 93\u2013114.\n21"},{"page":22,"text":"[15] Flegal, J. M., Haran, M., Jones, G. L., Mar. 2007. Markov Chain Monte\nCarlo: Can We Trust the Third Significant Figure? Statistical Science\n23 (2), 250\u2013260.\n[16] Gelman, A., Roberts, G. O., Gilks, W. R., 1996. Efficient Metropolis\njumping rules. In: Bayesian statistics, 5 (Alicante, 1994). Oxford Sci.\nPubl. Oxford Univ. Press, New York, pp. 599\u2013607.\n[17] Gelman, A., Rubin, D. B., 1992. Inference from iterative simulation\nusing multiple sequences. Statistical Science 7 (4), 457\u2013472.\n[18] Gilks, W., Roberts, G., Sahu, S., 1998. Adaptive markov chain monte\ncarlo through regeneration. J. Am. Star. Ass. 93, 1045\u20131054.\n[19] Haario, H., Saksman, E., Tamminen, J., 1999. Adaptive proposal distri-\nbution for random walk metropolis algorithm. Computational Statistics\n14, 375\u2013395.\n[20] Haario, H., Saksman, E., Tamminen, J., 2001. An adaptive metropolis\nalgorithm. Bernoulli 7, 223\u2013242.\n[21] Haario, H., Saksman, E., Tamminen, J., 2003. Componentwise adapta-\ntion for MCMC. Tech. Rep. Preprint 342, Dept. of Mathematics, Uni-\nversity of Helsinki.\n[22] Haario, H., Saksman, E., Tamminen, J., 2005. Componentwise adapta-\ntion for high dimensional MCMC. Computational Statistics 20, 265\u2013273.\n[23] Hastings, W., April 1970. Monte carlo sampling methods using Markov\nchains and their applications. Biometrika 57.\n[24] Hensman,\n2015. MCMC for variationally sparse Gaussian processes. Tech. rep.,\narXiv:1506.04000.\nJ., Alexander, Filippone, M., Ghahramani, Z., Jun.\n[25] Hoffman, M. D., Gelman, A., Nov. 2012. The No-U-Turn Sampler:\nAdaptively Setting Path Lengths in Hamiltonian Monte Carlo. Journal\nof Machine Learning Research to appear.\n[26] Jones, D. R., Schonlau, M., Welch, W. J., 1998. Efficient Global Op-\ntimization of Expensive Black-Box Functions. Journal of Global Opti-\nmization 13 (4), 455\u2013492.\n22"},{"page":23,"text":"[27] Kennedy, M. C., O\u2019Hagan, A., 2001. Bayesian calibration of computer\nmodels. Journal of the Royal Statistical Society: Series B (Statistical\nMethodology) 63 (3), 425\u2013464.\n[28] Kim, S., Valente, F., Filippone, M., Vinciarelli, A., 2014. Predicting\ncontinuous conflict perception with bayesian gaussian processes. IEEE\nTransactions on Affective Computing to appear.\n[29] Knorr-Held, L., Rue, H., Dec. 2002. On Block Updating in Markov\nRandom Field Models for Disease Mapping. Scandinavian Journal of\nStatistics 29 (4), 597\u2013614.\n[30] Kulhav\u00b4 y, R., 1996. Recursive nonlinear estimation: A geometric ap-\nproach. Springer.\n[31] Kuss, M., Rasmussen, C. E., 2005. Assessing Approximate Inference for\nBinary Gaussian Process Classification. Journal of Machine Learning\nResearch 6, 1679\u20131704.\n[32] Marin, J.-M., Pudlo, P., Sedki, M., 2014. Consistency of the adaptive\nmultiple importance sampling. eprint arXiv:1211.2548v2.\n[33] Metropolis, N., Rosenbluth, A., Teller, A., Teller, E., 1953. Equation of\nstate calulations by fast computing machines. The Journal of Chemical\nPhysics 21, 1087\u20131092.\n[34] Murray, I., Adams, R. P., 2010. Slice sampling covariance hyperparam-\neters of latent Gaussian models. In: Lafferty, J. D., Williams, C. K. I.,\nShawe-Taylor, J., Zemel, R. S., Culotta, A. (Eds.), Advances in Neural\nInformation Processing Systems 23: 24th Annual Conference on Neu-\nral Information Processing Systems 2010. Proceedings of a meeting held\n6-9 December 2010, Vancouver, British Columbia, Canada. Curran As-\nsociates, Inc., pp. 1732\u20131740.\n[35] Neal, R. M., Sep. 1993. Probabilistic inference using Markov chain\nMonte Carlo methods. Tech. Rep. CRG-TR-93-1, Dept. of Computer\nScience, University of Toronto.\n[36] Neal, R. M., 1999. Regression and classification using Gaussian process\npriors (with discussion). Bayesian Statistics 6, 475\u2013501.\n23"},{"page":24,"text":"[37] Neal, R. M., 2003. Slice Sampling. Annals of Statistics 31, 705\u2013767.\n[38] Neal, R. M., 2010. MCMC using Hamiltonian dynamics. in Handbook\nof Markov Chain Monte Carlo (eds S. Brooks, A. Gelman, G. Jones, XL\nMeng). Chapman and Hall\/CRC Press.\n[39] Neal, R. M., 2011. Handbook of Markov Monte Carlo, chapter 5: MCMC\nusing Hamitonian Dynamics. CRC Press.\n[40] Nesterov, Y., 2009. Primal-dual subgradient methods for convex prob-\nlems. Mathematical Programming 120 (1), 221\u2013259.\n[41] Nickisch, H., Rasmussen, C. E., Oct. 2008. Approximations for Binary\nGaussian Process Classification. Journal of Machine Learning Research\n9, 2035\u20132078.\n[42] Oh, M. S., Berger, J. O., 1992. Adaptive importance sampling in monte\ncarlo integration. Journal of Statistical Computing and Simulation 41,\n143\u2013168.\n[43] Opper, M., Winther, O., 2000. Gaussian processes for classification:\nMean-field algorithms. Neural Computation 12 (11), 2655\u20132684.\n[44] Ortiz, L., Kaelbling, L., 2000. Adaptive importance sampling for esti-\nmation in structured domains. In: Proceedings of the Sixteenth Annual\nConference on Uncertainty in Artificial Intelligence (UAI-2000). Morgan\nKaufmann Publishers, San Francisco, CA., pp. 446\u2013454.\n[45] Owen, A., Zhou, Y., 2000. Safe and effective importance sampling. J.\nAmer. Statist. Assoc. 95, 135\u2013143.\n[46] Petelin, D., Gasperin, M., Smidl, V., 2014. Adaptive importance sam-\npling for Bayesian inference in Gaussian process models. In: Proceedings\nof the 19th IFAC World Congress. pp. 5011\u20135016.\n[47] Rasmussen, C. E., Williams, C., 2006. Gaussian Processes for Machine\nLearning. MIT Press.\n[48] Roberts, G., Rosenthal, J., 2001. Optimal scaling for various metropolis-\nhastings algorithms. Statistical Science 16, 351\u2013367.\n24"},{"page":25,"text":"[49] Roberts, G. O., Gelman, A., Gilks, W. R., 1997. Weak convergence\nand optimal scaling of random walk Metropolis algorithms. Annals of\nApplied Probability 7, 110\u2013120.\n[50] Roberts, G. O., Sahu, S. K., 1997. Updating Schemes, Correlation Struc-\nture, Blocking and Parameterization for the Gibbs Sampler. Journal of\nthe Royal Statistical Society. Series B (Methodological) 59 (2).\n[51] Rubin, D., 1988. Using the SIR algorithm to simulate posterior distri-\nbutions. In Bayesian Statistics 3 (J. M. Bernardo, M. H. DeGroot, D.\nV. Lindley and A. F. M. Smith, eds.) 395-402. Oxford Univ. Press.\n[52] Sahu, S., Zhigljavsky, A., 2003. Self regenerative markov chain monte\ncarlo with adaptation. Bernoulli 9, 395\u2013422.\n[53] Taylor, M. B., Diggle, J. P., 2012. INLA or MCMC? A Tutorial and\nComparative Evaluation for Spatial Prediction in log-Gaussian Cox Pro-\ncesses. ArXiv:1202.1738.\n[54] Vanhatalo, J., Vehtari, A., 2007. Sparse Log Gaussian Processes via\nMCMC for Spatial Epidemiology. Journal of Machine Learning Research\n- Proceedings Track 1, 73\u201389.\n[55]\u02c7Sm\u00b4 \u0131dl, V., Hofman, R., 2014. Efficient Sequential Monte Carlo Sam-\npling for Continuous Monitoring of a Radiation Situation. Technomet-\nrics 56 (4), 514\u2013528.\n[56] Williams, C. K. I., Barber, D., 1998. Bayesian classification with Gaus-\nsian processes. IEEE Transactions on Pattern Analysis and Machine\nIntelligence 20, 1342\u20131351.\n25"},{"page":26,"text":"Appendix A. Convergence of samplers with the RBF covariance\nConcrete dataset - RBF covariance\n0 2000 400060008000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nAMIS\nAMIS\u2212D\nMAMIS\nMAMIS\u2212D\n0 200040006000 8000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nMH\u2212I\nMH\u2212D\nMH\u2212H\nEOT(MH\u2212I)\nEOT(MH\u2212D)\nEOT(MH\u2212H)\n0 2000 4000 6000 8000 1000012000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nHMC\u2212I\nHMC\u2212D\nHMC\u2212H\nEOT(HMC\u2212I)\nEOT(HMC\u2212D)\nEOT(HMC\u2212H)\n020004000 6000 8000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nNUT\u2212I\nNUT\u2212D\nNUT\u2212H\n0 200040006000 8000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nNUTDA\u2212I\nNUTDA\u2212D\nNUTDA\u2212H\nEOT(NUTDA\u2212I)\nEOT(NUTDA\u2212D)\nEOT(NUTDA\u2212H)\nFigure A.2: Convergence of AMIS\/MAMIS, MH, HMC, NUTS, NUTSDA, SS for the\nConcrete dataset. EOT stands for \u201dend of tuning\u201d.\n26"},{"page":27,"text":"Housing dataset - RBF covariance\n0 20004000 60008000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nAMIS\nAMIS\u2212D\nMAMIS\nMAMIS\u2212D\n0 20004000 6000 8000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nMH\u2212I\nMH\u2212D\nMH\u2212H\nEOT(MH\u2212I)\nEOT(MH\u2212D)\nEOT(MH\u2212H)\n0 2000 40006000 8000 1000012000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nHMC\u2212I\nHMC\u2212D\nHMC\u2212H\nEOT(HMC\u2212I)\nEOT(HMC\u2212D)\nEOT(HMC\u2212H)\n0 2000 400060008000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nNUT\u2212I\nNUT\u2212D\nNUT\u2212H\n020004000 60008000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nNUTDA\u2212I\nNUTDA\u2212D\nNUTDA\u2212H\nEOT(NUTDA\u2212I)\nEOT(NUTDA\u2212D)\nEOT(NUTDA\u2212H)\nFigure A.3: Convergence of AMIS\/MAMIS, MH, HMC, NUTS, NUTSDA, SS for the\nHousing dataset. EOT stands for \u201dend of tuning\u201d.\n27"},{"page":28,"text":"Parkinsons dataset - RBF covariance\n0 20004000 60008000\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nnumber of n3 operations\nIQR\nAMIS\nAMIS\u2212D\nMAMIS\nMAMIS\u2212D\n0 2000400060008000\n0.00\n0.05\n0.10\n0.15\nnumber of n3 operations\nIQR\nMH\u2212I\nMH\u2212D\nMH\u2212H\nEOT(MH\u2212I)\nEOT(MH\u2212D)\nEOT(MH\u2212H)\n0 2000 40006000 800010000 12000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nHMC\u2212I\nHMC\u2212D\nHMC\u2212H\nEOT(HMC\u2212I)\nEOT(HMC\u2212D)\nEOT(HMC\u2212H)\n0 20004000 6000 8000\n0.00\n0.05\n0.10\n0.15\nnumber of n3 operations\nIQR\nNUT\u2212I\nNUT\u2212D\nNUT\u2212H\n0 2000400060008000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nNUTDA\u2212I\nNUTDA\u2212D\nNUTDA\u2212H\nEOT(NUTDA\u2212I)\nEOT(NUTDA\u2212D)\nEOT(NUTDA\u2212H)\nFigure A.4: Convergence of AMIS\/MAMIS, MH, HMC, NUTS, NUTSDA, SS for the\nParkinsons dataset. EOT stands for \u201dend of tuning\u201d.\n28"},{"page":29,"text":"Appendix B. Convergence of samplers with the ARD covariance\nConcrete dataset - ARD covariance\n0 20004000 6000 8000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nAMIS\nAMIS\u2212D\nMAMIS\nMAMIS\u2212D\n02000 4000 60008000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nMH\u2212I\nMH\u2212D\nMH\u2212H\nEOT(MH\u2212I)\nEOT(MH\u2212D)\nEOT(MH\u2212H)\n02000 40006000 800010000\n0.0\n0.1\n0.2\n0.3\n0.4\nnumber of n3 operations\nIQR\nHMC\u2212I\nHMC\u2212D\nHMC\u2212H\nEOT(HMC\u2212I)\nEOT(HMC\u2212D)\nEOT(HMC\u2212H)\n02000400060008000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nNUT\u2212I\nNUT\u2212D\nNUT\u2212H\n0 2000400060008000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nNUTDA\u2212I\nNUTDA\u2212D\nNUTDA\u2212H\nEOT(NUTDA\u2212I)\nEOT(NUTDA\u2212D)\nEOT(NUTDA\u2212H)\nFigure B.5: Convergence of AMIS\/MAMIS, MH, HMC, NUTS, NUTSDA, SS for the\nConcrete dataset. EOT stands for \u201dend of tuning\u201d.\n29"},{"page":30,"text":"Housing dataset - ARD covariance\n020004000 60008000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nAMIS\nAMIS\u2212D\n0 2000 40006000 8000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnumber of n3 operations\nIQR\nMH\u2212I\nMH\u2212D\nMH\u2212H\nEOT(MH\u2212I)\nEOT(MH\u2212D)\nEOT(MH\u2212H)\n0 2000400060008000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnumber of n3 operations\nIQR\nHMC\u2212I\nHMC\u2212D\nHMC\u2212H\nEOT(HMC\u2212I)\nEOT(HMC\u2212D)\nEOT(HMC\u2212H)\n02000 4000 60008000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnumber of n3 operations\nIQR\nNUT\u2212I\nNUT\u2212D\nNUT\u2212H\n020004000 6000 8000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnumber of n3 operations\nIQR\nNUTDA\u2212I\nNUTDA\u2212D\nNUTDA\u2212H\nEOT(NUTDA\u2212I)\nEOT(NUTDA\u2212D)\nEOT(NUTDA\u2212H)\n020004000 60008000\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nnumber of n3 operations\nIQR\nAMIS\u2212MAMIS\nMAMIS\u2212P\nEOT(AMIS\u2212MAMIS)\nFigure B.6: Convergence of AMIS\/MAMIS, MH, HMC, NUTS, NUTSDA, SS for the\nHousing dataset. EOT stands for \u201dend of tuning\u201d.\n30"},{"page":31,"text":"Parkinsons dataset - ARD covariance\n0 200040006000 8000\n0.00\n0.05\n0.10\n0.15\n0.20\nnumber of n3 operations\nIQR\nAMIS\nAMIS\u2212D\n0 2000 4000 60008000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnumber of n3 operations\nIQR\nMH\u2212I\nMH\u2212D\nMH\u2212H\nEOT(MH\u2212I)\nEOT(MH\u2212D)\nEOT(MH\u2212H)\n0 200040006000 8000 10000 12000\n0.0\n0.5\n1.0\n1.5\n2.0\nnumber of n3 operations\nIQR\nHMC\u2212I\nHMC\u2212D\nHMC\u2212H\nEOT(HMC\u2212I)\nEOT(HMC\u2212D)\nEOT(HMC\u2212H)\n0 50001000015000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nnumber of n3 operations\nIQR\nNUT\u2212I\nNUT\u2212D\nNUT\u2212H\n0 5000 1000015000\n0.0\n0.5\n1.0\n1.5\n2.0\nnumber of n3 operations\nIQR\nNUTDA\u2212I\nNUTDA\u2212D\nNUTDA\u2212H\nEOT(NUTDA\u2212I)\nEOT(NUTDA\u2212D)\nEOT(NUTDA\u2212H)\n0 20004000600080001000012000 14000\n0.0\n0.2\n0.4\n0.6\n0.8\nnumber of n3 operations\nIQR\nAMIS\u2212MAMIS\nAMIS\u2212MAMIS\u2019\nAMIS\u2212MAMIS\"\nEOT(AMIS\u2212MAMIS)\nEOT(AMIS\u2212MAMIS\u2019)\nEOT(AMIS\u2212MAMIS\")\nFigure B.7: Convergence of AMIS\/MAMIS, MH, HMC, NUTS, NUTSDA, SS for the\nParkinsons dataset. EOT stands for \u201dend of tuning\u201d.\n31"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Vasek_Smidl\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/links\/55e94dff08ae65b6389aee89.pdf","widgetId":"rgw28_56ab1d5f02afa"},"id":"rgw28_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=280773011&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw29_56ab1d5f02afa"},"id":"rgw29_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=280773011&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":280773011,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"55e94dff08ae65b6389aee89","name":"Vasek Smidl","date":"Sep 04, 2015 ","nameLink":"profile\/Vasek_Smidl","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Vasek_Smidl\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/links\/55e94dff08ae65b6389aee89.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Vasek_Smidl\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/links\/55e94dff08ae65b6389aee89.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"2f2d96b65e358f6e342840822352c236","showFileSizeNote":false,"fileSize":"317.98 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"55e94dff08ae65b6389aee89","name":"Vasek Smidl","date":"Sep 04, 2015 ","nameLink":"profile\/Vasek_Smidl","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Vasek_Smidl\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/links\/55e94dff08ae65b6389aee89.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Vasek_Smidl\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/links\/55e94dff08ae65b6389aee89.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"2f2d96b65e358f6e342840822352c236","showFileSizeNote":false,"fileSize":"317.98 KB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=qjcxNwQDMK_xaLsm_tidDYi7B45i_McfrwJCr7ku7hSn8OZX2PC9mZPjUacqIZFtfSAf31Xn7ECohhD87PZdWw.28fEzfz54u8okW5tNQSxkkN0Y5o6LpmbNdfR4nHcM5wKYMAjvqGECECpiOpzHHdhprJ9XEhake3rpH0CxsRupQ","clickOnPill":"publication.PublicationFigures.html?_sg=Mi0GVBHG8stp4ofwDtIivGG16-hubmDiDHumxwo7lrV-n9Voy45BdLe0EHP76SRdnVTcinaGvzpkvuE9znVN7Q.ycnqDdBRgLpbabuhWXlhchDBiINYoKVW19wP9wbAksQYOodf5OYfTA8KbwEBd7ic-TcWEWVscuwTAGqHuTpxaA"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1o9o3\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FVasek_Smidl%2Fpublication%2F280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes%2Flinks%2F55e94dff08ae65b6389aee89.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1o9o3\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=Q1dJr6wSfnQNekRdiNqgYi6dzryApKQifl9wT824t72-M695mX8fvb7HQDQ_L1kQDC3pgfZezD81xRlAU-yo8Q","urlHash":"03ad1764500acfe36695f57d176caa31","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=s5mAUby4jO26kQvKaHhc-ljvCIvIuSKWxSCdqirvXlEzqBM3qINav3p7bvfEJRlt_L9EVgL_BGC78gSqSRJUZA6IIhSycgKz3IaTqa5H9b0.5io06ubcoDYvjre6gD11IuL70RURH0f8n8nnv-Ah0zIwQWrLt74MdF_wVOdLErvDRb5aVA2SOoGWcnG6Ys6pxA.dXX0UsBb0G6dGOGuXE-jkKU1tjp4ooZApUcoAOtwIFGjj8qq6jeb2cWtuRLgBgb4xM-mYhFWJBffje30N60c7Q","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"55e94dff08ae65b6389aee89","trackedDownloads":{"55e94dff08ae65b6389aee89":{"v":false,"d":false}},"assetId":"AS:269869098729477@1441353215784","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":280773011,"commentCursorPromo":null,"widgetId":"rgw31_56ab1d5f02afa"},"id":"rgw31_56ab1d5f02afa","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FVasek_Smidl%2Fpublication%2F280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes%2Flinks%2F55e94dff08ae65b6389aee89.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A269869098729477%401441353215784&publicationUid=280773011&linkId=55e94dff08ae65b6389aee89&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Adaptive Multiple Importance Sampling for Gaussian Processes","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=onQWPQe9dyn4DHloFix91s_iILuBBgX37FYAe_cGKKgQTW83xHEwJvpqAss8U4cQ9uBym_QUXAmNkq9-Z7ac6dmTRzOZz8koxcWdDr5rBdo.HD9gRV0QQ3fTtM5R2okw4bzjDGiEBCxNFtcWnmp5oBPo7TQnAXnrVvlZ-KxBwsAn8bJaJD8Rb3eNr0ACUo24Xg.95iLFvUW_rgPQUDlRzNr3YO0K5vpnX9G2GqIlPoqShy01zpsmTMHrf6-TxX3mL_Qerf37VWzEj_XiZNzEEuQsQ","publicationUid":280773011,"trackedDownloads":{"55e94dff08ae65b6389aee89":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw33_56ab1d5f02afa"},"id":"rgw33_56ab1d5f02afa","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw34_56ab1d5f02afa"},"id":"rgw34_56ab1d5f02afa","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw35_56ab1d5f02afa"},"id":"rgw35_56ab1d5f02afa","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw36_56ab1d5f02afa"},"id":"rgw36_56ab1d5f02afa","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw37_56ab1d5f02afa"},"id":"rgw37_56ab1d5f02afa","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw32_56ab1d5f02afa"},"id":"rgw32_56ab1d5f02afa","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw30_56ab1d5f02afa"},"id":"rgw30_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1d5f02afa"},"id":"rgw2_56ab1d5f02afa","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":280773011},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=280773011&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1d5f02afa"},"id":"rgw1_56ab1d5f02afa","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"YQrQ9ZNbAEGZpG8qB1ZIV\/j3s2cRiDMmoVaEaib1smuLfJpS3+F2mcTR0eNLVTsKWED7EY4amkXZJYzOQns02nySUpzQsEkhN43hi81Rz5JW1+y7vCxyCfWBE4VGQ9Szdj9s\/8XNsI\/lUNfhQfCV7L+v7nxxFlGaaumWk9tQrhohApHqKgRsvnCkIDhCZA0H1xiCA4j9LJNl7s95veX3VXhL8N07Y6rudcVxg04+AHvJEaRJa9f9jOS9SXPMEwU07UL+A7EHNrKQ+zBttT8QqOYfrho\/eNM6vcRcM1prM8k=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Adaptive Multiple Importance Sampling for Gaussian Processes\" \/>\n<meta property=\"og:description\" content=\"In applications of Gaussian processes where quantification of uncertainty is\na strict requirement, it is necessary to accurately characterize the posterior\ndistribution over Gaussian process...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/links\/55e94dff08ae65b6389aee89\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\" \/>\n<meta property=\"rg:id\" content=\"PB:280773011\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Adaptive Multiple Importance Sampling for Gaussian Processes\" \/>\n<meta name=\"citation_author\" content=\"Xiaoyu Xiong\" \/>\n<meta name=\"citation_author\" content=\"V\u00e1clav \u0160m\u00eddl\" \/>\n<meta name=\"citation_author\" content=\"Maurizio Filippone\" \/>\n<meta name=\"citation_publication_date\" content=\"2015\/08\/05\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Vasek_Smidl\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/links\/55e94dff08ae65b6389aee89.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-c7a10189-e627-4c50-b5a6-278be91fa2c7","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":511,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw38_56ab1d5f02afa"},"id":"rgw38_56ab1d5f02afa","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-c7a10189-e627-4c50-b5a6-278be91fa2c7", "4ffab71f4686a352917325573087e877677cde04");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-c7a10189-e627-4c50-b5a6-278be91fa2c7", "4ffab71f4686a352917325573087e877677cde04");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw39_56ab1d5f02afa"},"id":"rgw39_56ab1d5f02afa","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes","requestToken":"G3MrGu\/JEB\/Se8PSOiktyv0mc7XOebwaEauR3YXuhwC+TgEp4yXYDhPRYwXaXndb5GXhzQgqlGzedzjIBWKHVS0IvEKzzDnhPVZRhyYDG\/puRXLzWBx9cmmJ5UQ5qpJaMzC2pXysX1qnj7px7ucYSSD8cQV10ChXHHNCgmo1ZD1isLRfpkp2KVLePsNW8t0RlsEknX3cJnBbyjbYB0+pocqYOUwuazh7gH1dfTEFFMtxOQrRwRS7Yi9H4WEdGr\/kIy3ZDQ3YuUI+ADdO1NJD\/zkVQqjFTMxu6dHxXsHeoqM=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=Xb4GkcDeHgsQo030kaFjso4GDFJByUwjRokoxYVN1WNSlMSWiCacTBIMqrKD-2qD","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjgwNzczMDExX0FkYXB0aXZlX011bHRpcGxlX0ltcG9ydGFuY2VfU2FtcGxpbmdfZm9yX0dhdXNzaWFuX1Byb2Nlc3Nlcw%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw41_56ab1d5f02afa"},"id":"rgw41_56ab1d5f02afa","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw40_56ab1d5f02afa"},"id":"rgw40_56ab1d5f02afa","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw42_56ab1d5f02afa"},"id":"rgw42_56ab1d5f02afa","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
