<!DOCTYPE html> <html lang="en" class="" id="rgw37_56ab1ed39c415"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="NZNUEIO9UzHQ6WyKVqUyl3/WREgc3tcP6EfxPoylUyS36rx3wUsp+UOltjSc1CQkChs6j051sd/bVd1K4HXezNDDb+2uadgIe1SpdFukCMJFthQnu17xwvsbks5Qi0h9qXKEXJ+tm8uQ/EqlMZbn/pvZDqAIWlmwV4X4BMMR/wdB8f+jxVNTdjwakzEHp3wIr6EYGDwapcA0Zan2uatpODRbJQRdwtfXHZkRIddUyyRettNKmOaUb9cUBSTPocXhnDAadWGjNR7ZqxlSBIwebpiK/8F7hC05RhEZn4/Be4Y="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-1b0cb345-1fde-400d-9f6a-1877abd0c011",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/267706055_Practical_Variational_Inference_for_Neural_Networks" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Practical Variational Inference for Neural Networks" />
<meta property="og:description" content="Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/267706055_Practical_Variational_Inference_for_Neural_Networks/links/5459d5750cf2cf516483e4b5/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/267706055_Practical_Variational_Inference_for_Neural_Networks" />
<meta property="rg:id" content="PB:267706055" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Practical Variational Inference for Neural Networks" />
<meta name="citation_author" content="Alex Graves" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/267706055_Practical_Variational_Inference_for_Neural_Networks" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/267706055_Practical_Variational_Inference_for_Neural_Networks" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Practical Variational Inference for Neural Networks</title>
<meta name="description" content="Practical Variational Inference for Neural Networks on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1ed39c415" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1ed39c415" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw8_56ab1ed39c415">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Practical%20Variational%20Inference%20for%20Neural%20Networks&rft.title=NIPS&rft.jtitle=NIPS&rft.au=Alex%20Graves&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Practical Variational Inference for Neural Networks</h1> <meta itemprop="headline" content="Practical Variational Inference for Neural Networks">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/267706055_Practical_Variational_Inference_for_Neural_Networks/links/5459d5750cf2cf516483e4b5/smallpreview.png">  <div id="rgw10_56ab1ed39c415" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw11_56ab1ed39c415"> <a href="researcher/2027624990_Alex_Graves" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Alex Graves" alt="Alex Graves" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Alex Graves</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56ab1ed39c415">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2027624990_Alex_Graves"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Alex Graves" alt="Alex Graves" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2027624990_Alex_Graves" class="display-name">Alex Graves</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">     NIPS                  </div> <div id="rgw13_56ab1ed39c415" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural net-works. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically re-duce the number of network weights and lead to improved generalisation. Exper-imental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.</div> </p>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw26_56ab1ed39c415">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw25_56ab1ed39c415"  itemprop="articleBody">  <p>Page 1</p> <p>Practical Variational Inference for Neural Networks<br />Alex Graves<br />Department of Computer Science<br />University of Toronto, Canada<br />graves@cs.toronto.edu<br />Abstract<br />Variational methods have been previously explored as a tractable approximation<br />to Bayesian inference for neural networks. However the approaches proposed so<br />far have only been applicable to a few simple network architectures. This paper<br />introduces an easy-to-implement stochastic variational method (or equivalently,<br />minimum description length loss function) that can be applied to most neural net-<br />works. Along the way it revisits several common regularisers from a variational<br />perspective. It also provides a simple pruning heuristic that can both drastically re-<br />duce the number of network weights and lead to improved generalisation. Exper-<br />imental results are provided for a hierarchical multidimensional recurrent neural<br />network applied to the TIMIT speech corpus.<br />1 Introduction<br />In the eighteen years since variational inference was first proposed for neural networks [10] it has not<br />seen widespread use. We believe this is largely due to the difficulty of deriving analytical solutions<br />to the required integrals over the variational posteriors. Such solutions are complicated for even<br />the simplest network architectures, such as radial basis networks [2] and single layer feedforward<br />networks with linear outputs [10, 1, 14], and are generally unavailable for more complex systems.<br />The approach taken here is to forget about analytical solutions and search instead for variational<br />distributions whose expectation values (and derivatives thereof) can be efficiently approximated with<br />numerical integration. While it may seem perverse to replace one intractable integral (over the true<br />posterior) with another (over the variational posterior), the point is that the variational posterior is far<br />easier to draw probable samples from, and correspondingly more amenable to numerical methods.<br />The result is a stochastic method for variational inference with a diagonal Gaussian posterior that can<br />be applied to any differentiable log-loss parametric model—which includes most neural networks1<br />Variational inference can be reformulated as the optimisation of a Minimum Description length<br />(MDL; [21]) loss function; indeed it was in this form that variational inference was first considered<br />for neural networks. One advantage of the MDL interpretation is that it leads to a clear separation<br />between prediction accuracy and model complexity, which can help to both analyse and optimise the<br />network. Another benefit is that recasting inference as optimisation makes it to easier to implement<br />in existing, gradient-descent-based neural network software.<br />2 Neural Networks<br />For the purposes of this paper a neural network is a parametric model that assigns a conditional<br />probability Pr(D|w) to some dataset D, given a set w = {wi}W<br />weights. The elements (x,y) of D, each consisting of an input x and a target y, are assumed to be<br />1An important exception are energy-based models such as restricted Boltzmann machines [24] whose log-<br />loss is intractable.<br />i=1of real-valued parameters, or<br />1</p>  <p>Page 2</p> <p>drawn independently from a joint distribution p(x,y)2. The network loss LN(w,D) is defined as<br />the negative log probability of the data given the weights.<br />LN(w,D) = −lnPr(D|w) = −<br />?<br />(x,y)∈D<br />lnPr(y|x,w)<br />(1)<br />The logarithm could be taken to any base, but to avoid confusion we will use the natural loga-<br />rithm ln throughout. We assume that the partial derivatives of LN(w,D) with respect to the net-<br />work weights can be efficiently calculated (using, for example, backpropagation or backpropagation<br />through time [22]).<br />3Variational Inference<br />Performing Bayesian inference on a neural network requires the posterior distribution of the net-<br />work weights given the data. If the weights have a prior probability P(w|α) that depends on some<br />parameters α, the posterior can be written Pr(w|D,α). Unfortunately, for most neural networks<br />Pr(w|D,α) cannot be calculated analytically, or even efficiently sampled from. Variational in-<br />ference addresses this problem by approximating Pr(w|D,α) with a more tractable distribution<br />Q(w|β). The approximation is fitted by minimising the variational free energy F with respect to<br />the parameters β, where<br />?<br />and for some function g of a random variable x with distribution p(x), ?g?x∼pdenotes the expecta-<br />tion of g over p. A fully Bayesian approach would infer the prior parameters α from a hyperprior;<br />however in this paper they are found by simply minimising F with respect to α as well as β.<br />F = −<br />ln<br />?Pr(D|w)P(w|α)<br />Q(w|β)<br />??<br />w∼Q(β)<br />(2)<br />4 Minimum Description Length<br />F can be reinterpreted as a minimum description length loss function [12] by rearranging Eq. (2)<br />and substituting in from Eq. (1) to get<br />F =?LN(w,D)?<br />where DKL(Q(β)||P(α)) is the Kullback-Leibler divergence between Q(β) and P(α). Shannon’s<br />source coding theorem [23] tells us that the first term on the right hand side of Eq. (3) is a lower<br />bound on the expected amount of information (measured in nats, due to the use of natural loga-<br />rithms) required to transmit the targets in D to a receiver who knows the inputs, using the outputs<br />of a network whose weights are sampled from Q(β). Since this term decreases as the network’s<br />prediction accuracy increases, we identify it as the error loss LE(β,D):<br />LE(β,D) =?LN(w,D)?<br />Shannon’s bound can almost be achieved in practice using arithmetic coding [26]. The second term<br />on the right hand side of Eq. (3) is the expected number of nats required by a receiver who knows<br />P(α) to pick a sample from Q(β). Since this term measures the cost of ‘describing’ the network<br />weights to the receiver, we identify it as the complexity loss LC(α,β):<br />w∼Q(β)+ DKL(Q(β)||P(α)),<br />(3)<br />w∼Q(β)<br />(4)<br />LC(α,β) = DKL(Q(β)||P(α))<br />(5)<br />LC(α,β) can be realised with bits-back coding [25, 10]. Although originally conceived as a thought<br />experiment, bits-back coding has been used for an actual compression scheme [5]. Putting the terms<br />together F can be rephrased as an MDL loss function L(α,β,D) that measures the total number of<br />nats required to transmit the training targets using the network, given α and β:<br />L(α,β,D) = LE(β,D) + LC(α,β)<br />(6)<br />The network is then trained on D by minimising L(α,β,D) with respect to α and β, just like<br />an ordinary neural network loss function. One advantage of using a transmission cost as a loss<br />2Unsupervised learning can be treated as a special case where x = ∅<br />2</p>  <p>Page 3</p> <p>function is that we can immediately determine whether the network has compressed the targets past<br />a reasonable benchmark (such as that given by an off-the-shelf compressor). If it has, we can be<br />fairly certain that the network is learning underlying patterns in the data and not simply memorising<br />the training set. We would therefore expect it to generalise well to new data. In practice we have<br />found that as long as significant compression is taking place, decreasing L(α,β,D) on the training<br />setdoesnotincreaseLE(β,D)onthetestset, anditisthereforeunnecessarytosacrificeanytraining<br />data for early stopping.<br />Two transmission costs were ignored in the above discussion. One is the cost of transmitting the<br />model with w unspecified (for example software that implements the network architecture, the train-<br />ingalgorithmetc.). Theotheristhecostoftransmittingtheprior. Ifeitheroftheseareusedtoencode<br />a significant amount of information about D, the MDL principle will break down and the generali-<br />sation guarantees that come with compression will be lost. The easiest way to prevent this is to keep<br />both costs very small compared to D. In particular the prior should not contain too many parameters.<br />5 Choice of Distributions<br />We now derive the form of LE(β,D) and LC(α,β) for various choices of Q(β) and P(α). We also<br />derive the gradients of LE(β,D) and LC(α,β) with respect to β and the optimal values of α given<br />β. All continuous distributions are implicitly assumed to be quantised at some very fine resolution,<br />and we will limit ourselves to diagonal posteriors of the form Q(β) =?W<br />i=1qi(βi), meaning that<br />LC(α,β) =?W<br />5.1 Delta Posterior<br />i=1DKL(qi(βi)||P(α)).<br />Perhaps the simplest nontrivial distribution for Q(β) is a delta distribution that assigns probability<br />1 to a particular set of weights w and 0 to all other weights. In this case β = w, LE(β,D) =<br />LN(w,D) and LC(α,β) = LC(α,w) = −logP(w|α) + C. where C is a constant that depends<br />only on the discretisation of Q(β). Although C has no effect on the gradient used for training, it is<br />usually large enough to ensure that the network cannot compress the data using the coding scheme<br />described in the previous section3. If the prior is uniform, and all realisable weight values are equally<br />likely then LC(α,β) is a constant and we recover ordinary maximum likelihood training.<br />If the prior is a Laplace distribution then α = {µ,b}, P(w|α) =?W<br />LC(α,w) = W ln2b +1<br />b<br />i=1<br />If µ = 0 and b is fixed, this is equivalent to ordinary L1 regularisation. However we can instead<br />determine the optimal prior parameters ˆ α for w as follows: ˆ µ = µ1/2(w) (the median weight value)<br />andˆb =<br />W<br />If the prior is Gaussian then α = {µ,σ2}, P(w|α) =?W<br />LC(α,w) = W ln(2πσ2) +<br />2σ2<br />i=1<br />With µ = 0 and σ2fixed this is equivalent to L2 regularisation (also known as weight decay for<br />neural networks). The optimal ˆ α given w are ˆ µ =<br />i=1<br />1<br />2bexp<br />?<br />−|wi−µ|<br />b<br />?<br />and<br />W<br />?<br />|wi− µ| + C =⇒<br />∂LC(α,w)<br />∂wi<br />=sgn(wi− µ)<br />b<br />(7)<br />1<br />?W<br />i=1|wi− ˆ µ|.<br />i=1<br />1<br />√2πσ2exp<br />?<br />−(wi−µ)2<br />2σ2<br />?<br />and<br />√<br />1<br />W<br />?<br />(wi− µ)2+ C =⇒<br />∂LC(α,w)<br />∂wi<br />=wi− µ<br />σ2<br />(8)<br />1<br />W<br />?W<br />i=1wiand ˆ σ2=<br />1<br />W<br />?W<br />i=1(wi− ˆ µ)2<br />5.2Gaussian Posterior<br />A more interesting distribution for Q(β) is a diagonal Gaussian. In this case each weight requires a<br />separate mean and variance, so β = {µ,σ2} with the mean vector µ and variance vector σ2both<br />3The floating point resolution of the computer architecture used to train the network could in principle be<br />used to upper-bound the discretisation constant, and hence the compression; but in practice the bound would<br />be prohibitively high.<br />3</p>  <p>Page 4</p> <p>the same size as w. For a general network architecture we cannot compute either LE(β,D) or its<br />derivatives exactly, so we resort to sampling. Applying Monte-Carlo integration to Eq. (4) gives<br />LE(β,D) ≈1<br />S<br />S<br />?<br />k=1<br />LN(wk,D)<br />(9)<br />with wkdrawn independently from Q(β). A combination of the Gaussian characteristic function<br />and integration by parts can be used to derive the following identities for the derivatives of multi-<br />variate Gaussian expectations [18]:<br />∇Σ?V (a)?a∼N=1<br />where N is a multivariate Gaussian with mean vector µ and covariance matrix Σ, and V is an<br />arbitrary function of a. Differentiating Eq. (4) and applying these identities yields<br />?∂LN(w,D)<br />k=1<br />∂LE(β,D)<br />∂σ2<br />i<br />2 ∂w2<br />i<br />w∼Q(β)<br />∇µ?V (a)?a∼N= ?∇aV (a)?a∼N,<br />2?∇a∇aV (a)?a∼N<br />(10)<br />∂LE(β,D)<br />∂µi<br />=<br />∂wi<br />?<br />w∼Q(β)<br />?<br />≈1<br />S<br />S<br />?<br />∂LN(wk,D)<br />∂wi<br />??∂LN(w,D)<br />(11)<br />=1<br />?∂2LN(w,D)<br />≈1<br />2 ∂wi<br />?2?<br />w∼Q(β)<br />≈<br />1<br />2S<br />S<br />?<br />k=1<br />?∂LN(wk,D)<br />∂wi<br />?2<br />(12)<br />where the first approximation in Eq. (12) comes from substituting the negative diagonal of the em-<br />pirical Fisher information matrix for the diagonal of the Hessian. This approximation is exact if the<br />conditional distribution Pr(D|w) matches the empirical distribution of D (i.e. if the network per-<br />fectly models the data); we would therefore expect it to improve as LE(β,D) decreases. For simple<br />networks whose second derivatives can be calculated efficiently the approximation is unnecessary<br />and the diagonal Hessian can be sampled instead.<br />A simplification of the above distribution is to consider the variances of Q(β) fixed and optimise<br />only the means. Then the sampling used to calculate the derivatives in Eq. (11) is equivalent to<br />adding zero-mean, fixed-variance Gaussian noise to the network weights during training. In par-<br />ticular, if the prior P(α) is uniform and a single weight sample is taken for each element of D,<br />then minimising L(α,β,D) is identical to minimising LN(w,D) with weight noise or synaptic<br />noise [13]. Note that the quantisation of the uniform prior adds a large constant to LC(α,β), mak-<br />ing it unfeasible to compress the data with our MDL coding scheme; in practice early stopping is<br />required to prevent overfitting when training with weight noise.<br />If the prior is Gaussian then α = {µ,σ2} and<br />W<br />?<br />=⇒<br />∂µi<br />σ2<br />The optimal prior parameters ˆ α given β are<br />LC(α,β) =<br />i=1<br />lnσ<br />σi<br />+<br />1<br />2σ2<br />?<br />(µi− µ)2+ σ2<br />i− σ2?<br />(13)<br />∂LC(α,β)<br />=µi− µ<br />,<br />∂LC(α,β)<br />∂σ2<br />i<br />=1<br />2<br />?1<br />σ2−<br />1<br />σ2<br />i<br />?<br />(14)<br />ˆ µ =<br />1<br />W<br />W<br />?<br />i=1<br />µi,ˆ σ2=<br />1<br />W<br />W<br />?<br />i=1<br />?<br />σ2<br />i+ (µi− ˆ µ)2?<br />(15)<br />If a Gaussian prior is used with the fixed variance ‘weight noise’ posterior described above, it is still<br />possible to choose the optimal prior parameters for each β. This requires only a slight modification<br />of standard weight-noise training, with the derivatives on the left of Eq. (14) added to the weight<br />gradient and α optimised after every weight update. But because the prior is no longer uniform the<br />network is able to compress the data, making it feasible to dispense with early stopping.<br />The terms in the sum on the right hand side of Eq. (13) are the complexity costs of individual<br />network weights. These costs give valuable insight into the internal structure of the network, since<br />(with a limited budget of bits to spend) the network will assign more bits to more important weights.<br />Importance can be used, for example, to prune away spurious weights [15] or determine which<br />inputs are relevant [16].<br />4</p>  <p>Page 5</p> <p>6 Optimisation<br />If the derivatives of LE(β,D) are stochastic, we require an optimiser that can tolerate noisy gradient<br />estimates. Steepest descent with momentum [19] and RPROP [20] both work well in practice.<br />Although stochastic derivatives should in principle be estimated using the same weight samples for<br />the entire dataset, it is in practice much more efficient to pick different weight samples for each<br />(x,y) ∈ D. If both the prior and posterior are Gaussian this yields<br />∂L(α,β,D)<br />∂µi<br />σ2<br />(x,y)∈D<br />∂L(α,β,D)<br />∂σ2<br />i<br />2σ2<br />i<br />(x,y)∈D<br />where LN(wk,x,y) = −lnPr(y|x,w) and a separate set of S weight samples {wk}S<br />from Q(β) for each (x,y). For large datasets it is usually sufficient to set S = 1; however perfor-<br />mance can in some cases be substantially improved by using more samples, at the cost of longer<br />training times.<br />If the data is divided into B equally-sized batches such that D = {bj}B<br />is used, with the parameters updated after each batch gradient calculation, the following online loss<br />function (and corresponding derivatives) should be employed:<br />≈µi− µ<br />+<br />?<br />?<br />1<br />S<br />S<br />?<br />?<br />k=1<br />∂LN(wk,x,y)<br />∂wi<br />(16)<br />≈1<br />?1<br />σ2−<br />1<br />+<br />1<br />2S<br />S<br />?<br />k=1<br />?∂LN(wk,x,y)<br />∂wi<br />?2<br />(17)<br />k=1is drawn<br />j=1, and an ‘online’ optimiser<br />L(α,β,bj) =1<br />BLC(α,β) + LE(β,bj)<br />(18)<br />Note the 1/B factor for the complexity loss. This is because the weights (to which the complex-<br />ity cost applies) are only transmitted once for the entire dataset, whereas the error cost must be<br />transmitted separately for each batch.<br />During training, the prior parameters α should be set to their optimal values after every update to<br />β. For more complex priors where the optimal α cannot be found in closed form (such as mixture<br />distributions), α and β can instead be optimised simultaneously with gradient descent [17, 10].<br />Ideally a trained network should be evaluated on some previously unseen input x?using the expected<br />distribution ?Pr(.|x?,w)?w∼Q(β). However the maximum a posteriori approximation Pr(.|x?,w∗),<br />where w∗is the mode of Q(β), appears to work well in practice (at least for diagonal Gaussian<br />posteriors). This is equivalent to removing weight noise during testing.<br />7 Pruning<br />Removing weights from a neural network (a process usually referred to as pruning) has been re-<br />peatedly proposed as a means of reducing complexity and thereby improving generalisation [15, 7].<br />This would seem redundant for variational inference, which automatically limits the network com-<br />plexity. However pruning can reduce the computational cost and memory demands of the network.<br />Furthermore we have found that if the network is retrained after pruning, the final performance can<br />be improved. A possible explanation is that pruning reduces the noise in the gradient estimates<br />(because the pruned weights are not sampled) without increasing network complexity.<br />Weights w that are more probable under Q(β) tend to give lower LN(w,D) and pruning a weight<br />is equivalent to fixing it to zero. These two facts suggest a pruning heuristic where a weight is<br />removed if its probability density at zero is sufficiently high under Q(β). For a diagonal posterior<br />we can define the relative probability of each wiat zero as the density of qi(βi) at zero divided by<br />the density of qi(βi) at its mode. We can then define a pruning heuristic by removing all weights<br />whose relative probability at zero exceeds some threshold γ, with 0 ≤ γ ≤ 1. If qi(βi) is Gaussian<br />this yields<br />?<br />exp<br />−µ2<br />2σ2<br />i<br />i<br />?<br />&gt; γ =⇒<br />????<br />µi<br />σi<br />????&lt; λ<br />(19)<br />5</p>  <p>Page 6</p> <p>“In wage negotiations the industry bargains as a unit with a single union.”<br />Figure 1: Two representations of a TIMIT utterance. Note the lower resolution and greater<br />decorrelation of the MFC coefficients (top) compared to the spectrogram (bottom).<br />where we have used the reparameterisation λ =<br />are pruned. As λ grows the amount of pruning increases, and the probability of the pruned weight<br />vector under Q(β) (and therefore the likely network performance) decreases. A good rule of thumb<br />for how high λ can safely be set is the point at which the pruned weights become less probable than<br />an average weight sampled from qi(βi). For a Gaussian this is<br />?<br />If the network is retrained after pruning, the cost of transmitting which weights have been removed<br />should in principle be added to LC(α,β) (since this information could be used to overfit the training<br />data). However the extra cost does not depend on the network parameters, and can therefore be<br />ignored for the purposes of optimisation.<br />When a Gaussian prior is used its mean tends to be near zero. This implies that ‘cheaper’ weights,<br />where qi(βi) ≈ P(α), have high relative probability at zero and are thus more likely to be pruned.<br />√−2lnγ, with λ ≥ 0. If λ = 0 no weights<br />λ =<br />2ln√2 ≈ 0.83<br />(20)<br />8 Experiments<br />We tested all the combinations of posterior and prior described in Section 5 on a hierarchical mul-<br />tidimensional recurrent neural network [9] trained to do phoneme recognition on the TIMIT speech<br />corpus [4]. We also assessed the pruning heuristic from Section 7 by applying it with various thresh-<br />olds to a trained network and observing the impact on performance and network size.<br />TIMIT is a popular phoneme recognition benchmark. The core training and test sets (which we used<br />for our experiments) contain respectively 3696 and 192 phonetically transcribed utterances. We<br />defined a validation set by randomly selecting 184 sequences from the training set. The reduced set<br />of 39 phonemes [6] was used during both training and testing. The audio data was presented to the<br />network in the form of spectrogram images. One such image is contrasted with the mel-frequency<br />cepstrum representation used for most speech recognition systems in Fig. 1.<br />Hierarchical multidimensional recurrent neural networks containing Long Short-Term Memory [11]<br />hidden layers and a CTC output layer [8] have proven effective for offline handwriting recogni-<br />tion [9]. The same architecture is employed here, with a spectrogram in place of a handwriting<br />image, and phoneme labels in place of characters. Since the network scans through the spectrogram<br />in all directions, both vertical and horizontal correlations can be captured.<br />The network topology was identical for all experiments. It was the same as that of the handwriting<br />recognition network in [9] except that the dimensions of the three subsampling windows used to<br />progressively decrease resolution were now 2×4, 2×4 and 1×4, and the CTC layer now contained<br />40 output units (one for each phoneme, plus an extra for ‘blank’). This gave a total of 15 layers,<br />1306 units (not counting the inputs or bias), and 139,536 weights. All network parameters were<br />trained with online steepest descent (weight updates after every sequence) using a learning rate of<br />10−4and a momentum of 0.9. For the networks with stochastic derivatives (i.e those with Gaussian<br />posteriors) a single weight sample was drawn for each sequence. Prefix search CTC decoding [8]<br />was used to transcribe the test set, with probability threshold 0.995. When parameters in the pos-<br />terior or prior were fixed, the best value was found empirically. All networks were initialised with<br />random weights (or random weight means if the posterior was Gaussian), chosen from a Gaussian<br />6</p>  <p>Page 7</p> <p>Adaptive weight noise<br />Adapt. prior weight noise<br />Weight noise<br />Maximum likelihood<br />Figure 2: Error curves for four networks during training. The green, blue and red curves cor-<br />respond to the average per-sequence error loss LE(β,D) on the training, test and validation sets<br />respectively. Adaptive weight noise does not overfit, and normal weight noise overfits much more<br />slowly than maximum likelihood. Adaptive weight noise led to longer training times and noisier<br />error curves.<br />Table 1: Results for different priors and posteriors. All distribution parameters were learned by<br />the network unless fixed values are specified. ‘Error’ is the phoneme error rate on the core test set<br />(total edit distance between the network transcriptions and the target transcriptions, multiplied by<br />100). ‘Epochs’ is the number of passes through the training set after which the error was recorded.<br />‘Ratio’ is the compression ratio of the training set transcription targets relative to a uniform code<br />over the 39 phoneme labels (≈ 5.3 bits per phoneme); this could only be calculated for the networks<br />with Gaussian priors and posteriors.<br />Name Posterior PriorErrorEpochs Ratio<br />Adaptive L1<br />Adaptive L2<br />Adaptive mean L2<br />L2<br />Maximum likelihood<br />L1<br />Adaptive mean L1<br />Weight noise<br />Adaptive prior weight noise<br />Adaptive weight noise<br />Delta<br />Delta<br />Delta<br />Delta<br />Delta<br />Delta<br />Delta<br />Gauss σi = 0.075<br />Gauss σi = 0.075<br />Gauss<br />Laplace<br />Gauss<br />Gauss σ2= 0.1<br />Gauss µ = 0,σ2= 0.1<br />Uniform<br />Laplace µ = 0,b = 1/12<br />Laplace b = 1/12<br />Uniform<br />Gauss<br />Gauss<br />49.0<br />35.1<br />28.0<br />27.4<br />27.1<br />26.0<br />25.4<br />25.4<br />24.7<br />23.8<br />7<br />421<br />53<br />59<br />44<br />545<br />765<br />220<br />260<br />384<br />–<br />–<br />–<br />–<br />–<br />–<br />–<br />–<br />0.542<br />0.286<br />with mean 0, standard deviation 0.1. For the adaptive Gaussian posterior, the standard deviations<br />of the weights were initialised to 0.075 then optimised during training; this ensured that the vari-<br />ances (which are the standard deviations squared) remained positive. The networks with Gaussian<br />posteriors and priors did not require early stopping and were trained on all 3696 utterances in the<br />training set; all other networks used the validation set for early stopping and hence were trained on<br />3512 utterances. These were also the only networks for which the transmission cost of the network<br />weights could be measured (since it did not depend on the quantisation of the posterior or prior).<br />The networks were evaluated on the test set using the parameters giving lowest LE(β,D) on the<br />training set (or validation set if present). All experiments were stopped after 100 training epochs<br />with no improvement in either L(α,β,D), LE(β,D) or the number of transcription errors on the<br />training or validation set. The reason for such conservative stopping criteria was that the error curves<br />of some of the networks were extremely noisy (see Fig. 2).<br />Table 1 shows the results for the different posteriors and priors. L2 regularisation was no better<br />than unregularised maximum likelihood, while L1 gave a slight improvement; this is consistent<br />with our previous experience of recurrent neural networks. The fully adaptive L1 and L2 networks<br />performed very badly, apparently because the priors became excessively narrow (σ2≈ 0.003 for<br />L2 and b ≈ 0.002 for L1). L1 with fixed variance and adaptive mean was somewhat better than L1<br />with mean fixed at 0 (although the adaptive mean was very close to zero, settling around 0.0064).<br />The networks with Gaussian posteriors outperformed those with delta posteriors, with the best score<br />obtained using a fully adaptive posterior.<br />Table 2 shows the effect of pruning on the trained ‘adaptive weight noise’ network from Table 1.<br />The pruned networks were retrained using the same optimisation as before, with the error recorded<br />before and after retraining. As well as being highly effective at removing weights, pruning led to<br />improved performance following retraining in some cases. Notice the slow increase in initial error<br />up to λ = 0.5 and sharp rise thereafter; this is consistent with the ‘safe’ threshold of λ ≈ 0.83<br />7</p>  <p>Page 8</p> <p>Table 2: Effect of Network Pruning. ‘λ’ is the threshold used for pruning. ‘Weights’ is the number<br />of weights left after pruning and ‘Percent’ is the same figure expressed as a percentage of the original<br />weights. ‘Initial Error’ is the test error immediately after pruning and ‘Retrain Error’ is the test error<br />following ‘Retrain Epochs’ of subsequent retraining. ‘Bits/weight’ is the average bit cost (as defined<br />in Eq. (13)) of the unpruned weights.<br />Weights PercentInitial errorRetrain error<br />λ<br />Retrain Epochs Bits/weight<br />0<br />0.01<br />0.05<br />0.1<br />0.2<br />0.5<br />1<br />2<br />139,536<br />107,974<br />63,079<br />52,984<br />43,182<br />31,120<br />22,806<br />16,029<br />100%<br />77.4%<br />45.2%<br />37.9%<br />30.9%<br />22.3%<br />16.3%<br />11.5%<br />23.8<br />23.8<br />23.9<br />23.9<br />23.9<br />24.0<br />24.5<br />28.0<br />23.8<br />24.0<br />23.5<br />23.3<br />23.7<br />23.3<br />24.1<br />24.5<br />0<br />972<br />35<br />351<br />740<br />125<br />403<br />335<br />0.53<br />0.72<br />1.15<br />1.40<br />1.82<br />2.21<br />3.19<br />3.55<br />input gates H forget gatesV forget gates<br />cells<br />output gates<br />cells<br />Figure 3: Weight costs in an 2D LSTM recurrent connection. Each dot corresponds to a weight;<br />the lighter the colour the more bits the weight costs. The vertical axis shows the LSTM cell the<br />weightcomesfrom; thehorizontalaxisshowstheLSTMunittheweightgoesto. Notethelowcostof<br />the ‘V forget gates’ (these mediate vertical correlations between frequency bands in the spectrogram,<br />which are apparently less important to transcription than horizontal correlations between timesteps);<br />the high cost of the ‘cells’ (LSTM’s main processing units); the bright horizontal and vertical bands<br />(corresponding to units with ‘important’ outputs and inputs respectively); and the bright diagonal<br />through the cells (corresponding to self connections).<br />mentioned in Section 7. The lowest final phoneme error rate of 23.3 would until recently have been<br />the best recorded on TIMIT; however the application of deep belief networks has now improved the<br />benchmark to 20.5 [3].<br />Acknowledgements<br />I would like to thank Geoffrey Hinton, Christian Osendorfer, Justin Bayer and Thomas R¨ uckstieß<br />for helpful discussions and suggestions. Alex Graves is a Junior Fellow of the Canadian Institute for<br />Advanced Research.<br />Figure 4: The ‘cell’ weights from Fig. 3 pruned at different thresholds. Black dots are pruned<br />weights, white dots are remaining weights. ‘Cheaper’ weights tend to be removed first as λ grows.<br />8</p>  <p>Page 9</p> <p>References<br />[1] D. Barber and C. M. Bishop. Ensemble learning in Bayesian neural networks., pages 215–237. Springer-<br />Verlag, Berlin, 1998.<br />[2] D. Barber and B. Schottky. Radial basis functions: A bayesian treatment. In NIPS, 1997.<br />[3] G. E. Dahl, M. Ranzato, A. rahman Mohamed, and G. Hinton.<br />covariance restricted boltzmann machine. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel,<br />and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 469–477. 2010.<br />[4] DARPA-ISTO. The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT), speech disc<br />cd1-1.1 edition, 1990.<br />[5] B. J. Frey. Graphical models for machine learning and digital communication. MIT Press, Cambridge,<br />MA, USA, 1998.<br />[6] K. fu Lee and H. wuen Hon. Speaker-independent phone recognition using hidden markov models. IEEE<br />Transactions on Acoustics, Speech, and Signal Processing, 1989.<br />[7] C. L. Giles and C. W. Omlin. Pruning recurrent neural networks for improved generalization performance.<br />IEEE Transactions on Neural Networks, 5:848–851, 1994.<br />[8] A. Graves, S. Fern´ andez, F. Gomez, and J. Schmidhuber. Connectionist temporal classification: La-<br />belling unsegmented sequence data with recurrent neural networks. In Proceedings of the International<br />Conference on Machine Learning, ICML 2006, Pittsburgh, USA, 2006.<br />[9] A. Graves and J. Schmidhuber. Offline handwriting recognition with multidimensional recurrent neural<br />networks. In NIPS, pages 545–552, 2008.<br />[10] G. E. Hinton and D. van Camp. Keeping the neural networks simple by minimizing the description length<br />of the weights. In COLT, pages 5–13, 1993.<br />[11] S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735–1780,<br />1997.<br />[12] A. Honkela and H. Valpola. Variational learning and bits-back coding: An information-theoretic view to<br />bayesian learning. IEEE Transactions on Neural Networks, 15:800–810, 2004.<br />[13] K.-C. Jim, C. Giles, and B. Horne. An analysis of noise in recurrent neural networks: convergence and<br />generalization. Neural Networks, IEEE Transactions on, 7(6):1424 –1438, nov 1996.<br />[14] N. D. Lawrence. Variational Inference in Probabilistic Models. PhD thesis, University of Cambridge,<br />2000.<br />[15] Y. Le Cun, J. Denker, and S. Solla. Optimal brain damage. In D. S. Touretzky, editor, Advances in Neural<br />Information Processing Systems, volume 2, pages 598–605. Morgan Kaufmann, San Mateo, CA, 1990.<br />[16] D. J. C. MacKay. Probable networks and plausible predictions - a review of practical bayesian methods<br />for supervised neural networks. Neural Computation, 1995.<br />[17] S. J. Nowlan and G. E. Hinton. Simplifying neural networks by soft weight sharing. Neural Computation,<br />4:173–193, 1992.<br />[18] M. Opper and C. Archambeau. The variational gaussian approximation revisited. Neural Computation,<br />21(3):786–792, 2009.<br />[19] D. Plaut, S. Nowlan, and G. E. Hinton. Experiments on learning by back propagation. Technical Report<br />CMU-CS-86-126, Department of Computer Science, Carnegie Mellon University, Pittsburgh, PA, 1986.<br />[20] M. Riedmiller and T. Braun. A direst adaptive method for faster backpropagation learning: The rprop<br />algorithm. In International Symposium on Neural Networks, 1993.<br />[21] J. Rissanen. Modeling by shortest data description. Automatica, 14(5):465 – 471, 1978.<br />[22] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors,<br />pages 696–699. MIT Press, Cambridge, MA, USA, 1988.<br />[23] C. E. Shannon. A mathematical theory of communication. Bell system technical journal, 27, 1948.<br />[24] P. Smolensky. Information processing in dynamical systems: foundations of harmony theory, pages 194–<br />281. MIT Press, Cambridge, MA, USA, 1986.<br />[25] C. S. Wallace. Classification by minimum-message-length inference. In Proceedings of the international<br />conference on Advances in computing and information, ICCI’90, pages 72–81, New York, NY, USA,<br />1990. Springer-Verlag New York, Inc.<br />[26] I. H. Witten, R. M. Neal, and J. G. Cleary. Arithmetic coding for data compression. Commun. ACM,<br />30:520–540, June 1987.<br />Phone recognition with the mean-<br />9</p>   </div> <div id="rgw18_56ab1ed39c415" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw19_56ab1ed39c415">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw20_56ab1ed39c415"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://books.nips.cc/papers/files/nips24/NIPS2011_1263.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Practical Variational Inference for Neural Networks">Practical Variational Inference for Neural Network...</a> </div>  <div class="details">   Available from <a href="http://books.nips.cc/papers/files/nips24/NIPS2011_1263.pdf" target="_blank" rel="nofollow">books.nips.cc</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw27_56ab1ed39c415" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (14) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw28_56ab1ed39c415" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw29_56ab1ed39c415" >  <div class="indent-left">  <div id="rgw30_56ab1ed39c415" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/281768642_Hessian-Free_Optimization_For_Learning_Deep_Multidimensional_Recurrent_Neural_Networks">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1509.03475" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: de.arxiv.org </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw31_56ab1ed39c415">  <li class="citation-context-item"> "For phoneme recognition, the regularization method suggested in [24] was used. We applied Gaussian weight noise of standard deviation σ = {0.03, " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/281768642_Hessian-Free_Optimization_For_Learning_Deep_Multidimensional_Recurrent_Neural_Networks"> <span class="publication-title js-publication-title">Hessian-Free Optimization For Learning Deep Multidimensional Recurrent Neural Networks</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2081050578_Minhyung_Cho" class="authors js-author-name ga-publications-authors">Minhyung Cho</a> &middot;     <a href="researcher/57790954_Chandra_Shekhar_Dhir" class="authors js-author-name ga-publications-authors">Chandra Shekhar Dhir</a> &middot;     <a href="researcher/2008763278_Jaehyung_Lee" class="authors js-author-name ga-publications-authors">Jaehyung Lee</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Multidimensional recurrent neural network (MDRNN) has shown a remarkable
performance in speech and handwriting recognition. The performance of MDRNN is
improved by further increasing its depth, and the difficulty of learning the
deeper network is overcome by Hessian-free (HF) optimization. Considering that
connectionist temporal classification (CTC) is utilized as an objective of
learning MDRNN for sequence labelling, the non-convexity of CTC poses a problem
to apply HF to the network. As a solution to this, a convex approximation of
CTC is formulated and its relationship with the EM algorithm and the Fisher
information matrix is discussed. MDRNN up to the depth of 15 layers is
successfully trained using HF, resulting in improved performance for sequence
labelling. </span> </div>    <div class="publication-meta publication-meta">  <span class="ico-publication-preview reset-background"></span> Preview    &middot; Article &middot; Sep 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw32_56ab1ed39c415" >  <div class="indent-left">  <div id="rgw33_56ab1ed39c415" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/277959252_Bayesian_Convolutional_Neural_Networks_with_Bernoulli_Approximate_Variational_Inference">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1506.02158" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: de.arxiv.org </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw34_56ab1ed39c415">  <li class="citation-context-item"> "This is done by minimising the Kullback-Leibler divergence from the full model. Many have followed this approach in the past for standard NN models [5] [6] [7] [8]. But the variational approach used to approximate the posterior in Bayesian NNs can be fairly computationally expensive – the use of Gaussian approximating distributions increases the number of model parameters considerably, without increasing model capacity by much. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/277959252_Bayesian_Convolutional_Neural_Networks_with_Bernoulli_Approximate_Variational_Inference"> <span class="publication-title js-publication-title">Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2069013556_Yarin_Gal" class="authors js-author-name ga-publications-authors">Yarin Gal</a> &middot;     <a href="researcher/8159937_Zoubin_Ghahramani" class="authors js-author-name ga-publications-authors">Zoubin Ghahramani</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> We present an efficient Bayesian convolutional neural network (convnet). The
model offers better robustness to over-fitting on small data than traditional
approaches. This is by placing a probability distribution over the convnet&#39;s
kernels (also known as filters). We approximate the model&#39;s intractable
posterior with Bernoulli variational distributions. This requires no additional
model parameters. Our model can be implemented using existing tools in the
field. This is by extending the recent interpretation of dropout as approximate
inference in the Gaussian process to the case of Bayesian neural networks. The
model achieves a considerable improvement in classification accuracy compared
to previous approaches. We finish with state-of-the-art results on CIFAR-10
following our new interpretation. </span> </div>    <div class="publication-meta publication-meta">  <span class="ico-publication-preview reset-background"></span> Preview    &middot; Article &middot; Jun 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   "  id="rgw35_56ab1ed39c415" >  <div class="indent-left">  <div id="rgw36_56ab1ed39c415" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/228095632_Variational_Bayesian_Inference_with_Stochastic_Search">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Michael_Jordan13" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Michael Jordan </div> </div>   </div>  </div>  <div class="indent-right">      </div>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/228095632_Variational_Bayesian_Inference_with_Stochastic_Search"> <span class="publication-title js-publication-title">Variational Bayesian Inference with Stochastic Search</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/70672430_John_Paisley" class="authors js-author-name ga-publications-authors">John Paisley</a> &middot;     <a href="researcher/2064238818_David_Blei" class="authors js-author-name ga-publications-authors">David Blei</a> &middot;     <a href="researcher/65912024_Michael_Jordan" class="authors js-author-name ga-publications-authors">Michael Jordan</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Mean-field variational inference is a method for approximate Bayesian
posterior inference. It approximates a full posterior distribution with a
factorized set of distributions by maximizing a lower bound on the marginal
likelihood. This requires the ability to integrate a sum of terms in the log
joint likelihood using this factorized distribution. Often not all integrals
are in closed form, which is typically handled by using a lower bound. We
present an alternative algorithm based on stochastic optimization that allows
for direct optimization of the variational lower bound. This method uses
control variates to reduce the variance of the stochastic search gradient, in
which existing lower bounds can play an important role. We demonstrate the
approach on two non-conjugate models: logistic regression and an approximation
to the HDP. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Jun 2012  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Michael_Jordan13/publication/228095632_Variational_Bayesian_Inference_with_Stochastic_Search/links/53fe1e670cf21edafd14cb6c.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw22_56ab1ed39c415" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw23_56ab1ed39c415">  </ul> </div> </div>   <div id="rgw14_56ab1ed39c415" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw15_56ab1ed39c415"> <div> <h5> <a href="publication/285548706_State_of_Neural_Networks_Is_Strong" class="color-inherit ga-similar-publication-title"><span class="publication-title">State of Neural Networks Is Strong</span></a>  </h5>  <div class="authors"> <a href="researcher/2066277764_Kenji_Doya" class="authors ga-similar-publication-author">Kenji Doya</a>, <a href="researcher/2066274529_DeLiang_Wang" class="authors ga-similar-publication-author">DeLiang Wang</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw16_56ab1ed39c415"> <div> <h5> <a href="publication/291366272_Risk_analysis_in_maintainability_of_high-rise_buildings_under_tropical_conditions_using_ensemble_neural_network" class="color-inherit ga-similar-publication-title"><span class="publication-title">Risk analysis in maintainability of high-rise buildings under tropical conditions using ensemble neural network</span></a>  </h5>  <div class="authors"> <a href="researcher/2005929635_Nayanthara_De_Silva" class="authors ga-similar-publication-author">Nayanthara De Silva</a>, <a href="researcher/2095370966_Malik_Ranasinghe" class="authors ga-similar-publication-author">Malik Ranasinghe</a>, <a href="researcher/2095273496_Chathura_Ranjan_De_Silva" class="authors ga-similar-publication-author">Chathura Ranjan De Silva</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56ab1ed39c415"> <div> <h5> <a href="publication/292074166_Mastering_the_game_of_Go_with_deep_neural_networks_and_tree_search" class="color-inherit ga-similar-publication-title"><span class="publication-title">Mastering the game of Go with deep neural networks and tree search</span></a>  </h5>  <div class="authors"> <a href="researcher/2089221891_David_Silver" class="authors ga-similar-publication-author">David Silver</a>, <a href="researcher/2061306317_Aja_Huang" class="authors ga-similar-publication-author">Aja Huang</a>, <a href="researcher/2041100462_Chris_J_Maddison" class="authors ga-similar-publication-author">Chris J. Maddison</a>, <a href="researcher/15937545_Arthur_Guez" class="authors ga-similar-publication-author">Arthur Guez</a>, <a href="researcher/2096158425_Laurent_Sifre" class="authors ga-similar-publication-author">Laurent Sifre</a>, <a href="researcher/2096157657_George_van_den_Driessche" class="authors ga-similar-publication-author">George van den Driessche</a>, <a href="researcher/2096123054_Julian_Schrittwieser" class="authors ga-similar-publication-author">Julian Schrittwieser</a>, <a href="researcher/2040168607_Ioannis_Antonoglou" class="authors ga-similar-publication-author">Ioannis Antonoglou</a>, <a href="researcher/2096181106_Veda_Panneershelvam" class="authors ga-similar-publication-author">Veda Panneershelvam</a>, <a href="researcher/2096100996_Marc_Lanctot" class="authors ga-similar-publication-author">Marc Lanctot</a>, <a href="researcher/2096172703_Sander_Dieleman" class="authors ga-similar-publication-author">Sander Dieleman</a>, <a href="researcher/2096144373_Dominik_Grewe" class="authors ga-similar-publication-author">Dominik Grewe</a>, <a href="researcher/2096095253_John_Nham" class="authors ga-similar-publication-author">John Nham</a>, <a href="researcher/2054792132_Nal_Kalchbrenner" class="authors ga-similar-publication-author">Nal Kalchbrenner</a>, <a href="researcher/16196090_Ilya_Sutskever" class="authors ga-similar-publication-author">Ilya Sutskever</a>, <a href="researcher/38454838_Timothy_Lillicrap" class="authors ga-similar-publication-author">Timothy Lillicrap</a>, <a href="researcher/2096095748_Madeleine_Leach" class="authors ga-similar-publication-author">Madeleine Leach</a>, <a href="researcher/2040167286_Koray_Kavukcuoglu" class="authors ga-similar-publication-author">Koray Kavukcuoglu</a>, <a href="researcher/2096053138_Thore_Graepel" class="authors ga-similar-publication-author">Thore Graepel</a>, <a href="researcher/38470272_Demis_Hassabis" class="authors ga-similar-publication-author">Demis Hassabis</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw38_56ab1ed39c415" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw39_56ab1ed39c415">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw40_56ab1ed39c415" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=QY9q7y7bRP7lGKY1NWxiKwd57D4pmNsYM5dlSGyVOyjBp8XHDBzmoW_iafVGso0F" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="QHpoB9Azc3si773OvevjUWGNqi/1K1ew4jrRCivnwTkJAj/b7BwZrbWamYjorpbSCTi0N3mL+qzNe/o3CcatIFo94hjTL3+PSpcej/F9wwy/+cxjVJw7pmv5yGfKZg9IEyipUowv4U1h30UdBjhEwjUCHAugK9UX7t1KCMuLZCQ+IAdp3wfZW3voBfE+QcLbqup0IMVLPL/Id9wA2pvPaeZ8UZEVOZxCDIn7mTmf4HQkPgbrU6mCbEMPlVkcLv2r4Txlny3G9kOJiDqdBsZUEZ6+3HcwrlIf59iw6av8gIE="/> <input type="hidden" name="urlAfterLogin" value="publication/267706055_Practical_Variational_Inference_for_Neural_Networks"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjY3NzA2MDU1X1ByYWN0aWNhbF9WYXJpYXRpb25hbF9JbmZlcmVuY2VfZm9yX05ldXJhbF9OZXR3b3Jrcw%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjY3NzA2MDU1X1ByYWN0aWNhbF9WYXJpYXRpb25hbF9JbmZlcmVuY2VfZm9yX05ldXJhbF9OZXR3b3Jrcw%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjY3NzA2MDU1X1ByYWN0aWNhbF9WYXJpYXRpb25hbF9JbmZlcmVuY2VfZm9yX05ldXJhbF9OZXR3b3Jrcw%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw41_56ab1ed39c415"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 649;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"profileUrl":"researcher\/2027624990_Alex_Graves","fullname":"Alex Graves","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2549355721578\/images\/template\/default\/profile\/profile_default_m.png","profileStats":[{"data":{"impactPoints":"4.63","widgetId":"rgw5_56ab1ed39c415"},"id":"rgw5_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorImpactPoints.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorImpactPoints.html?authorUid=2027624990","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationCount":6,"widgetId":"rgw6_56ab1ed39c415"},"id":"rgw6_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorPublicationCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorPublicationCount.html?authorUid=2027624990","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"followerCount":1,"widgetId":"rgw7_56ab1ed39c415"},"id":"rgw7_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorFollowerCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorFollowerCount.html?authorUid=2027624990","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw4_56ab1ed39c415"},"id":"rgw4_56ab1ed39c415","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorBadge.html?authorUid=2027624990","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw3_56ab1ed39c415"},"id":"rgw3_56ab1ed39c415","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=267706055","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":267706055,"title":"Practical Variational Inference for Neural Networks","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"NIPS","publicationDate":"","publicationDateRobot":false,"article":""}},"source":false,"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Practical Variational Inference for Neural Networks"},{"key":"rft.title","value":"NIPS"},{"key":"rft.jtitle","value":"NIPS"},{"key":"rft.au","value":"Alex Graves"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw9_56ab1ed39c415"},"id":"rgw9_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=267706055","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":267706055,"peopleItems":[{"data":{"authorUrl":"researcher\/2027624990_Alex_Graves","authorNameOnPublication":"Alex Graves","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Alex Graves","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2027624990_Alex_Graves","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56ab1ed39c415"},"id":"rgw12_56ab1ed39c415","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2027624990&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56ab1ed39c415"},"id":"rgw11_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2027624990&authorNameOnPublication=Alex%20Graves","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw10_56ab1ed39c415"},"id":"rgw10_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=267706055&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":267706055,"abstract":"<noscript><\/noscript><div>Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural net-works. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically re-duce the number of network weights and lead to improved generalisation. Exper-imental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw13_56ab1ed39c415"},"id":"rgw13_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=267706055","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/267706055_Practical_Variational_Inference_for_Neural_Networks\/links\/5459d5750cf2cf516483e4b5\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw8_56ab1ed39c415"},"id":"rgw8_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=267706055&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2066277764,"url":"researcher\/2066277764_Kenji_Doya","fullname":"Kenji Doya","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2066274529,"url":"researcher\/2066274529_DeLiang_Wang","fullname":"DeLiang Wang","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/285548706_State_of_Neural_Networks_Is_Strong","usePlainButton":true,"publicationUid":285548706,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/285548706_State_of_Neural_Networks_Is_Strong","title":"State of Neural Networks Is Strong","displayTitleAsLink":true,"authors":[{"id":2066277764,"url":"researcher\/2066277764_Kenji_Doya","fullname":"Kenji Doya","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2066274529,"url":"researcher\/2066274529_DeLiang_Wang","fullname":"DeLiang Wang","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/285548706_State_of_Neural_Networks_Is_Strong","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/285548706_State_of_Neural_Networks_Is_Strong\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw15_56ab1ed39c415"},"id":"rgw15_56ab1ed39c415","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=285548706","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2005929635,"url":"researcher\/2005929635_Nayanthara_De_Silva","fullname":"Nayanthara De Silva","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095370966,"url":"researcher\/2095370966_Malik_Ranasinghe","fullname":"Malik Ranasinghe","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095273496,"url":"researcher\/2095273496_Chathura_Ranjan_De_Silva","fullname":"Chathura Ranjan De Silva","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2016","journal":"Facilities","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291366272_Risk_analysis_in_maintainability_of_high-rise_buildings_under_tropical_conditions_using_ensemble_neural_network","usePlainButton":true,"publicationUid":291366272,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/291366272_Risk_analysis_in_maintainability_of_high-rise_buildings_under_tropical_conditions_using_ensemble_neural_network","title":"Risk analysis in maintainability of high-rise buildings under tropical conditions using ensemble neural network","displayTitleAsLink":true,"authors":[{"id":2005929635,"url":"researcher\/2005929635_Nayanthara_De_Silva","fullname":"Nayanthara De Silva","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095370966,"url":"researcher\/2095370966_Malik_Ranasinghe","fullname":"Malik Ranasinghe","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095273496,"url":"researcher\/2095273496_Chathura_Ranjan_De_Silva","fullname":"Chathura Ranjan De Silva","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Facilities 02\/2016; 34(1\/2):2-27. DOI:10.1108\/F-05-2014-0047"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291366272_Risk_analysis_in_maintainability_of_high-rise_buildings_under_tropical_conditions_using_ensemble_neural_network","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291366272_Risk_analysis_in_maintainability_of_high-rise_buildings_under_tropical_conditions_using_ensemble_neural_network\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56ab1ed39c415"},"id":"rgw16_56ab1ed39c415","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291366272","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2089221891,"url":"researcher\/2089221891_David_Silver","fullname":"David Silver","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2061306317,"url":"researcher\/2061306317_Aja_Huang","fullname":"Aja Huang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2041100462,"url":"researcher\/2041100462_Chris_J_Maddison","fullname":"Chris J. Maddison","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":15937545,"url":"researcher\/15937545_Arthur_Guez","fullname":"Arthur Guez","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":16,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Nature","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/292074166_Mastering_the_game_of_Go_with_deep_neural_networks_and_tree_search","usePlainButton":true,"publicationUid":292074166,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"41.46","url":"publication\/292074166_Mastering_the_game_of_Go_with_deep_neural_networks_and_tree_search","title":"Mastering the game of Go with deep neural networks and tree search","displayTitleAsLink":true,"authors":[{"id":2089221891,"url":"researcher\/2089221891_David_Silver","fullname":"David Silver","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2061306317,"url":"researcher\/2061306317_Aja_Huang","fullname":"Aja Huang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2041100462,"url":"researcher\/2041100462_Chris_J_Maddison","fullname":"Chris J. Maddison","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":15937545,"url":"researcher\/15937545_Arthur_Guez","fullname":"Arthur Guez","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2096158425,"url":"researcher\/2096158425_Laurent_Sifre","fullname":"Laurent Sifre","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2096157657,"url":"researcher\/2096157657_George_van_den_Driessche","fullname":"George van den Driessche","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2096123054,"url":"researcher\/2096123054_Julian_Schrittwieser","fullname":"Julian Schrittwieser","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2040168607,"url":"researcher\/2040168607_Ioannis_Antonoglou","fullname":"Ioannis Antonoglou","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2096181106,"url":"researcher\/2096181106_Veda_Panneershelvam","fullname":"Veda Panneershelvam","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2096100996,"url":"researcher\/2096100996_Marc_Lanctot","fullname":"Marc Lanctot","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2096172703,"url":"researcher\/2096172703_Sander_Dieleman","fullname":"Sander Dieleman","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2096144373,"url":"researcher\/2096144373_Dominik_Grewe","fullname":"Dominik Grewe","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2096095253,"url":"researcher\/2096095253_John_Nham","fullname":"John Nham","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2054792132,"url":"researcher\/2054792132_Nal_Kalchbrenner","fullname":"Nal Kalchbrenner","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":16196090,"url":"researcher\/16196090_Ilya_Sutskever","fullname":"Ilya Sutskever","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38454838,"url":"researcher\/38454838_Timothy_Lillicrap","fullname":"Timothy Lillicrap","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2096095748,"url":"researcher\/2096095748_Madeleine_Leach","fullname":"Madeleine Leach","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2040167286,"url":"researcher\/2040167286_Koray_Kavukcuoglu","fullname":"Koray Kavukcuoglu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2096053138,"url":"researcher\/2096053138_Thore_Graepel","fullname":"Thore Graepel","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38470272,"url":"researcher\/38470272_Demis_Hassabis","fullname":"Demis Hassabis","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Nature 01\/2016; 529(7587):484-489. DOI:10.1038\/nature16961"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/292074166_Mastering_the_game_of_Go_with_deep_neural_networks_and_tree_search","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/292074166_Mastering_the_game_of_Go_with_deep_neural_networks_and_tree_search\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab1ed39c415"},"id":"rgw17_56ab1ed39c415","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=292074166","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw14_56ab1ed39c415"},"id":"rgw14_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=267706055&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":267706055,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":267706055,"publicationType":"article","linkId":"5459d5750cf2cf516483e4b5","fileName":"Practical Variational Inference for Neural Networks","fileUrl":"http:\/\/books.nips.cc\/papers\/files\/nips24\/NIPS2011_1263.pdf","name":"books.nips.cc","nameUrl":"http:\/\/books.nips.cc\/papers\/files\/nips24\/NIPS2011_1263.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw20_56ab1ed39c415"},"id":"rgw20_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=267706055&linkId=5459d5750cf2cf516483e4b5&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw19_56ab1ed39c415"},"id":"rgw19_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=267706055&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":0,"valueFormatted":"0","widgetId":"rgw21_56ab1ed39c415"},"id":"rgw21_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=267706055","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw18_56ab1ed39c415"},"id":"rgw18_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=267706055&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":267706055,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw23_56ab1ed39c415"},"id":"rgw23_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=267706055&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":0,"valueFormatted":"0","widgetId":"rgw24_56ab1ed39c415"},"id":"rgw24_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=267706055","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw22_56ab1ed39c415"},"id":"rgw22_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=267706055&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Practical Variational Inference for Neural Networks\nAlex Graves\nDepartment of Computer Science\nUniversity of Toronto, Canada\ngraves@cs.toronto.edu\nAbstract\nVariational methods have been previously explored as a tractable approximation\nto Bayesian inference for neural networks. However the approaches proposed so\nfar have only been applicable to a few simple network architectures. This paper\nintroduces an easy-to-implement stochastic variational method (or equivalently,\nminimum description length loss function) that can be applied to most neural net-\nworks. Along the way it revisits several common regularisers from a variational\nperspective. It also provides a simple pruning heuristic that can both drastically re-\nduce the number of network weights and lead to improved generalisation. Exper-\nimental results are provided for a hierarchical multidimensional recurrent neural\nnetwork applied to the TIMIT speech corpus.\n1 Introduction\nIn the eighteen years since variational inference was first proposed for neural networks [10] it has not\nseen widespread use. We believe this is largely due to the difficulty of deriving analytical solutions\nto the required integrals over the variational posteriors. Such solutions are complicated for even\nthe simplest network architectures, such as radial basis networks [2] and single layer feedforward\nnetworks with linear outputs [10, 1, 14], and are generally unavailable for more complex systems.\nThe approach taken here is to forget about analytical solutions and search instead for variational\ndistributions whose expectation values (and derivatives thereof) can be efficiently approximated with\nnumerical integration. While it may seem perverse to replace one intractable integral (over the true\nposterior) with another (over the variational posterior), the point is that the variational posterior is far\neasier to draw probable samples from, and correspondingly more amenable to numerical methods.\nThe result is a stochastic method for variational inference with a diagonal Gaussian posterior that can\nbe applied to any differentiable log-loss parametric model\u2014which includes most neural networks1\nVariational inference can be reformulated as the optimisation of a Minimum Description length\n(MDL; [21]) loss function; indeed it was in this form that variational inference was first considered\nfor neural networks. One advantage of the MDL interpretation is that it leads to a clear separation\nbetween prediction accuracy and model complexity, which can help to both analyse and optimise the\nnetwork. Another benefit is that recasting inference as optimisation makes it to easier to implement\nin existing, gradient-descent-based neural network software.\n2 Neural Networks\nFor the purposes of this paper a neural network is a parametric model that assigns a conditional\nprobability Pr(D|w) to some dataset D, given a set w = {wi}W\nweights. The elements (x,y) of D, each consisting of an input x and a target y, are assumed to be\n1An important exception are energy-based models such as restricted Boltzmann machines [24] whose log-\nloss is intractable.\ni=1of real-valued parameters, or\n1"},{"page":2,"text":"drawn independently from a joint distribution p(x,y)2. The network loss LN(w,D) is defined as\nthe negative log probability of the data given the weights.\nLN(w,D) = \u2212lnPr(D|w) = \u2212\n?\n(x,y)\u2208D\nlnPr(y|x,w)\n(1)\nThe logarithm could be taken to any base, but to avoid confusion we will use the natural loga-\nrithm ln throughout. We assume that the partial derivatives of LN(w,D) with respect to the net-\nwork weights can be efficiently calculated (using, for example, backpropagation or backpropagation\nthrough time [22]).\n3Variational Inference\nPerforming Bayesian inference on a neural network requires the posterior distribution of the net-\nwork weights given the data. If the weights have a prior probability P(w|\u03b1) that depends on some\nparameters \u03b1, the posterior can be written Pr(w|D,\u03b1). Unfortunately, for most neural networks\nPr(w|D,\u03b1) cannot be calculated analytically, or even efficiently sampled from. Variational in-\nference addresses this problem by approximating Pr(w|D,\u03b1) with a more tractable distribution\nQ(w|\u03b2). The approximation is fitted by minimising the variational free energy F with respect to\nthe parameters \u03b2, where\n?\nand for some function g of a random variable x with distribution p(x), ?g?x\u223cpdenotes the expecta-\ntion of g over p. A fully Bayesian approach would infer the prior parameters \u03b1 from a hyperprior;\nhowever in this paper they are found by simply minimising F with respect to \u03b1 as well as \u03b2.\nF = \u2212\nln\n?Pr(D|w)P(w|\u03b1)\nQ(w|\u03b2)\n??\nw\u223cQ(\u03b2)\n(2)\n4 Minimum Description Length\nF can be reinterpreted as a minimum description length loss function [12] by rearranging Eq. (2)\nand substituting in from Eq. (1) to get\nF =?LN(w,D)?\nwhere DKL(Q(\u03b2)||P(\u03b1)) is the Kullback-Leibler divergence between Q(\u03b2) and P(\u03b1). Shannon\u2019s\nsource coding theorem [23] tells us that the first term on the right hand side of Eq. (3) is a lower\nbound on the expected amount of information (measured in nats, due to the use of natural loga-\nrithms) required to transmit the targets in D to a receiver who knows the inputs, using the outputs\nof a network whose weights are sampled from Q(\u03b2). Since this term decreases as the network\u2019s\nprediction accuracy increases, we identify it as the error loss LE(\u03b2,D):\nLE(\u03b2,D) =?LN(w,D)?\nShannon\u2019s bound can almost be achieved in practice using arithmetic coding [26]. The second term\non the right hand side of Eq. (3) is the expected number of nats required by a receiver who knows\nP(\u03b1) to pick a sample from Q(\u03b2). Since this term measures the cost of \u2018describing\u2019 the network\nweights to the receiver, we identify it as the complexity loss LC(\u03b1,\u03b2):\nw\u223cQ(\u03b2)+ DKL(Q(\u03b2)||P(\u03b1)),\n(3)\nw\u223cQ(\u03b2)\n(4)\nLC(\u03b1,\u03b2) = DKL(Q(\u03b2)||P(\u03b1))\n(5)\nLC(\u03b1,\u03b2) can be realised with bits-back coding [25, 10]. Although originally conceived as a thought\nexperiment, bits-back coding has been used for an actual compression scheme [5]. Putting the terms\ntogether F can be rephrased as an MDL loss function L(\u03b1,\u03b2,D) that measures the total number of\nnats required to transmit the training targets using the network, given \u03b1 and \u03b2:\nL(\u03b1,\u03b2,D) = LE(\u03b2,D) + LC(\u03b1,\u03b2)\n(6)\nThe network is then trained on D by minimising L(\u03b1,\u03b2,D) with respect to \u03b1 and \u03b2, just like\nan ordinary neural network loss function. One advantage of using a transmission cost as a loss\n2Unsupervised learning can be treated as a special case where x = \u2205\n2"},{"page":3,"text":"function is that we can immediately determine whether the network has compressed the targets past\na reasonable benchmark (such as that given by an off-the-shelf compressor). If it has, we can be\nfairly certain that the network is learning underlying patterns in the data and not simply memorising\nthe training set. We would therefore expect it to generalise well to new data. In practice we have\nfound that as long as significant compression is taking place, decreasing L(\u03b1,\u03b2,D) on the training\nsetdoesnotincreaseLE(\u03b2,D)onthetestset, anditisthereforeunnecessarytosacrificeanytraining\ndata for early stopping.\nTwo transmission costs were ignored in the above discussion. One is the cost of transmitting the\nmodel with w unspecified (for example software that implements the network architecture, the train-\ningalgorithmetc.). Theotheristhecostoftransmittingtheprior. Ifeitheroftheseareusedtoencode\na significant amount of information about D, the MDL principle will break down and the generali-\nsation guarantees that come with compression will be lost. The easiest way to prevent this is to keep\nboth costs very small compared to D. In particular the prior should not contain too many parameters.\n5 Choice of Distributions\nWe now derive the form of LE(\u03b2,D) and LC(\u03b1,\u03b2) for various choices of Q(\u03b2) and P(\u03b1). We also\nderive the gradients of LE(\u03b2,D) and LC(\u03b1,\u03b2) with respect to \u03b2 and the optimal values of \u03b1 given\n\u03b2. All continuous distributions are implicitly assumed to be quantised at some very fine resolution,\nand we will limit ourselves to diagonal posteriors of the form Q(\u03b2) =?W\ni=1qi(\u03b2i), meaning that\nLC(\u03b1,\u03b2) =?W\n5.1 Delta Posterior\ni=1DKL(qi(\u03b2i)||P(\u03b1)).\nPerhaps the simplest nontrivial distribution for Q(\u03b2) is a delta distribution that assigns probability\n1 to a particular set of weights w and 0 to all other weights. In this case \u03b2 = w, LE(\u03b2,D) =\nLN(w,D) and LC(\u03b1,\u03b2) = LC(\u03b1,w) = \u2212logP(w|\u03b1) + C. where C is a constant that depends\nonly on the discretisation of Q(\u03b2). Although C has no effect on the gradient used for training, it is\nusually large enough to ensure that the network cannot compress the data using the coding scheme\ndescribed in the previous section3. If the prior is uniform, and all realisable weight values are equally\nlikely then LC(\u03b1,\u03b2) is a constant and we recover ordinary maximum likelihood training.\nIf the prior is a Laplace distribution then \u03b1 = {\u00b5,b}, P(w|\u03b1) =?W\nLC(\u03b1,w) = W ln2b +1\nb\ni=1\nIf \u00b5 = 0 and b is fixed, this is equivalent to ordinary L1 regularisation. However we can instead\ndetermine the optimal prior parameters \u02c6 \u03b1 for w as follows: \u02c6 \u00b5 = \u00b51\/2(w) (the median weight value)\nand\u02c6b =\nW\nIf the prior is Gaussian then \u03b1 = {\u00b5,\u03c32}, P(w|\u03b1) =?W\nLC(\u03b1,w) = W ln(2\u03c0\u03c32) +\n2\u03c32\ni=1\nWith \u00b5 = 0 and \u03c32fixed this is equivalent to L2 regularisation (also known as weight decay for\nneural networks). The optimal \u02c6 \u03b1 given w are \u02c6 \u00b5 =\ni=1\n1\n2bexp\n?\n\u2212|wi\u2212\u00b5|\nb\n?\nand\nW\n?\n|wi\u2212 \u00b5| + C =\u21d2\n\u2202LC(\u03b1,w)\n\u2202wi\n=sgn(wi\u2212 \u00b5)\nb\n(7)\n1\n?W\ni=1|wi\u2212 \u02c6 \u00b5|.\ni=1\n1\n\u221a2\u03c0\u03c32exp\n?\n\u2212(wi\u2212\u00b5)2\n2\u03c32\n?\nand\n\u221a\n1\nW\n?\n(wi\u2212 \u00b5)2+ C =\u21d2\n\u2202LC(\u03b1,w)\n\u2202wi\n=wi\u2212 \u00b5\n\u03c32\n(8)\n1\nW\n?W\ni=1wiand \u02c6 \u03c32=\n1\nW\n?W\ni=1(wi\u2212 \u02c6 \u00b5)2\n5.2Gaussian Posterior\nA more interesting distribution for Q(\u03b2) is a diagonal Gaussian. In this case each weight requires a\nseparate mean and variance, so \u03b2 = {\u00b5,\u03c32} with the mean vector \u00b5 and variance vector \u03c32both\n3The floating point resolution of the computer architecture used to train the network could in principle be\nused to upper-bound the discretisation constant, and hence the compression; but in practice the bound would\nbe prohibitively high.\n3"},{"page":4,"text":"the same size as w. For a general network architecture we cannot compute either LE(\u03b2,D) or its\nderivatives exactly, so we resort to sampling. Applying Monte-Carlo integration to Eq. (4) gives\nLE(\u03b2,D) \u22481\nS\nS\n?\nk=1\nLN(wk,D)\n(9)\nwith wkdrawn independently from Q(\u03b2). A combination of the Gaussian characteristic function\nand integration by parts can be used to derive the following identities for the derivatives of multi-\nvariate Gaussian expectations [18]:\n\u2207\u03a3?V (a)?a\u223cN=1\nwhere N is a multivariate Gaussian with mean vector \u00b5 and covariance matrix \u03a3, and V is an\narbitrary function of a. Differentiating Eq. (4) and applying these identities yields\n?\u2202LN(w,D)\nk=1\n\u2202LE(\u03b2,D)\n\u2202\u03c32\ni\n2 \u2202w2\ni\nw\u223cQ(\u03b2)\n\u2207\u00b5?V (a)?a\u223cN= ?\u2207aV (a)?a\u223cN,\n2?\u2207a\u2207aV (a)?a\u223cN\n(10)\n\u2202LE(\u03b2,D)\n\u2202\u00b5i\n=\n\u2202wi\n?\nw\u223cQ(\u03b2)\n?\n\u22481\nS\nS\n?\n\u2202LN(wk,D)\n\u2202wi\n??\u2202LN(w,D)\n(11)\n=1\n?\u22022LN(w,D)\n\u22481\n2 \u2202wi\n?2?\nw\u223cQ(\u03b2)\n\u2248\n1\n2S\nS\n?\nk=1\n?\u2202LN(wk,D)\n\u2202wi\n?2\n(12)\nwhere the first approximation in Eq. (12) comes from substituting the negative diagonal of the em-\npirical Fisher information matrix for the diagonal of the Hessian. This approximation is exact if the\nconditional distribution Pr(D|w) matches the empirical distribution of D (i.e. if the network per-\nfectly models the data); we would therefore expect it to improve as LE(\u03b2,D) decreases. For simple\nnetworks whose second derivatives can be calculated efficiently the approximation is unnecessary\nand the diagonal Hessian can be sampled instead.\nA simplification of the above distribution is to consider the variances of Q(\u03b2) fixed and optimise\nonly the means. Then the sampling used to calculate the derivatives in Eq. (11) is equivalent to\nadding zero-mean, fixed-variance Gaussian noise to the network weights during training. In par-\nticular, if the prior P(\u03b1) is uniform and a single weight sample is taken for each element of D,\nthen minimising L(\u03b1,\u03b2,D) is identical to minimising LN(w,D) with weight noise or synaptic\nnoise [13]. Note that the quantisation of the uniform prior adds a large constant to LC(\u03b1,\u03b2), mak-\ning it unfeasible to compress the data with our MDL coding scheme; in practice early stopping is\nrequired to prevent overfitting when training with weight noise.\nIf the prior is Gaussian then \u03b1 = {\u00b5,\u03c32} and\nW\n?\n=\u21d2\n\u2202\u00b5i\n\u03c32\nThe optimal prior parameters \u02c6 \u03b1 given \u03b2 are\nLC(\u03b1,\u03b2) =\ni=1\nln\u03c3\n\u03c3i\n+\n1\n2\u03c32\n?\n(\u00b5i\u2212 \u00b5)2+ \u03c32\ni\u2212 \u03c32?\n(13)\n\u2202LC(\u03b1,\u03b2)\n=\u00b5i\u2212 \u00b5\n,\n\u2202LC(\u03b1,\u03b2)\n\u2202\u03c32\ni\n=1\n2\n?1\n\u03c32\u2212\n1\n\u03c32\ni\n?\n(14)\n\u02c6 \u00b5 =\n1\nW\nW\n?\ni=1\n\u00b5i,\u02c6 \u03c32=\n1\nW\nW\n?\ni=1\n?\n\u03c32\ni+ (\u00b5i\u2212 \u02c6 \u00b5)2?\n(15)\nIf a Gaussian prior is used with the fixed variance \u2018weight noise\u2019 posterior described above, it is still\npossible to choose the optimal prior parameters for each \u03b2. This requires only a slight modification\nof standard weight-noise training, with the derivatives on the left of Eq. (14) added to the weight\ngradient and \u03b1 optimised after every weight update. But because the prior is no longer uniform the\nnetwork is able to compress the data, making it feasible to dispense with early stopping.\nThe terms in the sum on the right hand side of Eq. (13) are the complexity costs of individual\nnetwork weights. These costs give valuable insight into the internal structure of the network, since\n(with a limited budget of bits to spend) the network will assign more bits to more important weights.\nImportance can be used, for example, to prune away spurious weights [15] or determine which\ninputs are relevant [16].\n4"},{"page":5,"text":"6 Optimisation\nIf the derivatives of LE(\u03b2,D) are stochastic, we require an optimiser that can tolerate noisy gradient\nestimates. Steepest descent with momentum [19] and RPROP [20] both work well in practice.\nAlthough stochastic derivatives should in principle be estimated using the same weight samples for\nthe entire dataset, it is in practice much more efficient to pick different weight samples for each\n(x,y) \u2208 D. If both the prior and posterior are Gaussian this yields\n\u2202L(\u03b1,\u03b2,D)\n\u2202\u00b5i\n\u03c32\n(x,y)\u2208D\n\u2202L(\u03b1,\u03b2,D)\n\u2202\u03c32\ni\n2\u03c32\ni\n(x,y)\u2208D\nwhere LN(wk,x,y) = \u2212lnPr(y|x,w) and a separate set of S weight samples {wk}S\nfrom Q(\u03b2) for each (x,y). For large datasets it is usually sufficient to set S = 1; however perfor-\nmance can in some cases be substantially improved by using more samples, at the cost of longer\ntraining times.\nIf the data is divided into B equally-sized batches such that D = {bj}B\nis used, with the parameters updated after each batch gradient calculation, the following online loss\nfunction (and corresponding derivatives) should be employed:\n\u2248\u00b5i\u2212 \u00b5\n+\n?\n?\n1\nS\nS\n?\n?\nk=1\n\u2202LN(wk,x,y)\n\u2202wi\n(16)\n\u22481\n?1\n\u03c32\u2212\n1\n+\n1\n2S\nS\n?\nk=1\n?\u2202LN(wk,x,y)\n\u2202wi\n?2\n(17)\nk=1is drawn\nj=1, and an \u2018online\u2019 optimiser\nL(\u03b1,\u03b2,bj) =1\nBLC(\u03b1,\u03b2) + LE(\u03b2,bj)\n(18)\nNote the 1\/B factor for the complexity loss. This is because the weights (to which the complex-\nity cost applies) are only transmitted once for the entire dataset, whereas the error cost must be\ntransmitted separately for each batch.\nDuring training, the prior parameters \u03b1 should be set to their optimal values after every update to\n\u03b2. For more complex priors where the optimal \u03b1 cannot be found in closed form (such as mixture\ndistributions), \u03b1 and \u03b2 can instead be optimised simultaneously with gradient descent [17, 10].\nIdeally a trained network should be evaluated on some previously unseen input x?using the expected\ndistribution ?Pr(.|x?,w)?w\u223cQ(\u03b2). However the maximum a posteriori approximation Pr(.|x?,w\u2217),\nwhere w\u2217is the mode of Q(\u03b2), appears to work well in practice (at least for diagonal Gaussian\nposteriors). This is equivalent to removing weight noise during testing.\n7 Pruning\nRemoving weights from a neural network (a process usually referred to as pruning) has been re-\npeatedly proposed as a means of reducing complexity and thereby improving generalisation [15, 7].\nThis would seem redundant for variational inference, which automatically limits the network com-\nplexity. However pruning can reduce the computational cost and memory demands of the network.\nFurthermore we have found that if the network is retrained after pruning, the final performance can\nbe improved. A possible explanation is that pruning reduces the noise in the gradient estimates\n(because the pruned weights are not sampled) without increasing network complexity.\nWeights w that are more probable under Q(\u03b2) tend to give lower LN(w,D) and pruning a weight\nis equivalent to fixing it to zero. These two facts suggest a pruning heuristic where a weight is\nremoved if its probability density at zero is sufficiently high under Q(\u03b2). For a diagonal posterior\nwe can define the relative probability of each wiat zero as the density of qi(\u03b2i) at zero divided by\nthe density of qi(\u03b2i) at its mode. We can then define a pruning heuristic by removing all weights\nwhose relative probability at zero exceeds some threshold \u03b3, with 0 \u2264 \u03b3 \u2264 1. If qi(\u03b2i) is Gaussian\nthis yields\n?\nexp\n\u2212\u00b52\n2\u03c32\ni\ni\n?\n> \u03b3 =\u21d2\n????\n\u00b5i\n\u03c3i\n????< \u03bb\n(19)\n5"},{"page":6,"text":"\u201cIn wage negotiations the industry bargains as a unit with a single union.\u201d\nFigure 1: Two representations of a TIMIT utterance. Note the lower resolution and greater\ndecorrelation of the MFC coefficients (top) compared to the spectrogram (bottom).\nwhere we have used the reparameterisation \u03bb =\nare pruned. As \u03bb grows the amount of pruning increases, and the probability of the pruned weight\nvector under Q(\u03b2) (and therefore the likely network performance) decreases. A good rule of thumb\nfor how high \u03bb can safely be set is the point at which the pruned weights become less probable than\nan average weight sampled from qi(\u03b2i). For a Gaussian this is\n?\nIf the network is retrained after pruning, the cost of transmitting which weights have been removed\nshould in principle be added to LC(\u03b1,\u03b2) (since this information could be used to overfit the training\ndata). However the extra cost does not depend on the network parameters, and can therefore be\nignored for the purposes of optimisation.\nWhen a Gaussian prior is used its mean tends to be near zero. This implies that \u2018cheaper\u2019 weights,\nwhere qi(\u03b2i) \u2248 P(\u03b1), have high relative probability at zero and are thus more likely to be pruned.\n\u221a\u22122ln\u03b3, with \u03bb \u2265 0. If \u03bb = 0 no weights\n\u03bb =\n2ln\u221a2 \u2248 0.83\n(20)\n8 Experiments\nWe tested all the combinations of posterior and prior described in Section 5 on a hierarchical mul-\ntidimensional recurrent neural network [9] trained to do phoneme recognition on the TIMIT speech\ncorpus [4]. We also assessed the pruning heuristic from Section 7 by applying it with various thresh-\nolds to a trained network and observing the impact on performance and network size.\nTIMIT is a popular phoneme recognition benchmark. The core training and test sets (which we used\nfor our experiments) contain respectively 3696 and 192 phonetically transcribed utterances. We\ndefined a validation set by randomly selecting 184 sequences from the training set. The reduced set\nof 39 phonemes [6] was used during both training and testing. The audio data was presented to the\nnetwork in the form of spectrogram images. One such image is contrasted with the mel-frequency\ncepstrum representation used for most speech recognition systems in Fig. 1.\nHierarchical multidimensional recurrent neural networks containing Long Short-Term Memory [11]\nhidden layers and a CTC output layer [8] have proven effective for offline handwriting recogni-\ntion [9]. The same architecture is employed here, with a spectrogram in place of a handwriting\nimage, and phoneme labels in place of characters. Since the network scans through the spectrogram\nin all directions, both vertical and horizontal correlations can be captured.\nThe network topology was identical for all experiments. It was the same as that of the handwriting\nrecognition network in [9] except that the dimensions of the three subsampling windows used to\nprogressively decrease resolution were now 2\u00d74, 2\u00d74 and 1\u00d74, and the CTC layer now contained\n40 output units (one for each phoneme, plus an extra for \u2018blank\u2019). This gave a total of 15 layers,\n1306 units (not counting the inputs or bias), and 139,536 weights. All network parameters were\ntrained with online steepest descent (weight updates after every sequence) using a learning rate of\n10\u22124and a momentum of 0.9. For the networks with stochastic derivatives (i.e those with Gaussian\nposteriors) a single weight sample was drawn for each sequence. Prefix search CTC decoding [8]\nwas used to transcribe the test set, with probability threshold 0.995. When parameters in the pos-\nterior or prior were fixed, the best value was found empirically. All networks were initialised with\nrandom weights (or random weight means if the posterior was Gaussian), chosen from a Gaussian\n6"},{"page":7,"text":"Adaptive weight noise\nAdapt. prior weight noise\nWeight noise\nMaximum likelihood\nFigure 2: Error curves for four networks during training. The green, blue and red curves cor-\nrespond to the average per-sequence error loss LE(\u03b2,D) on the training, test and validation sets\nrespectively. Adaptive weight noise does not overfit, and normal weight noise overfits much more\nslowly than maximum likelihood. Adaptive weight noise led to longer training times and noisier\nerror curves.\nTable 1: Results for different priors and posteriors. All distribution parameters were learned by\nthe network unless fixed values are specified. \u2018Error\u2019 is the phoneme error rate on the core test set\n(total edit distance between the network transcriptions and the target transcriptions, multiplied by\n100). \u2018Epochs\u2019 is the number of passes through the training set after which the error was recorded.\n\u2018Ratio\u2019 is the compression ratio of the training set transcription targets relative to a uniform code\nover the 39 phoneme labels (\u2248 5.3 bits per phoneme); this could only be calculated for the networks\nwith Gaussian priors and posteriors.\nName Posterior PriorErrorEpochs Ratio\nAdaptive L1\nAdaptive L2\nAdaptive mean L2\nL2\nMaximum likelihood\nL1\nAdaptive mean L1\nWeight noise\nAdaptive prior weight noise\nAdaptive weight noise\nDelta\nDelta\nDelta\nDelta\nDelta\nDelta\nDelta\nGauss \u03c3i = 0.075\nGauss \u03c3i = 0.075\nGauss\nLaplace\nGauss\nGauss \u03c32= 0.1\nGauss \u00b5 = 0,\u03c32= 0.1\nUniform\nLaplace \u00b5 = 0,b = 1\/12\nLaplace b = 1\/12\nUniform\nGauss\nGauss\n49.0\n35.1\n28.0\n27.4\n27.1\n26.0\n25.4\n25.4\n24.7\n23.8\n7\n421\n53\n59\n44\n545\n765\n220\n260\n384\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n0.542\n0.286\nwith mean 0, standard deviation 0.1. For the adaptive Gaussian posterior, the standard deviations\nof the weights were initialised to 0.075 then optimised during training; this ensured that the vari-\nances (which are the standard deviations squared) remained positive. The networks with Gaussian\nposteriors and priors did not require early stopping and were trained on all 3696 utterances in the\ntraining set; all other networks used the validation set for early stopping and hence were trained on\n3512 utterances. These were also the only networks for which the transmission cost of the network\nweights could be measured (since it did not depend on the quantisation of the posterior or prior).\nThe networks were evaluated on the test set using the parameters giving lowest LE(\u03b2,D) on the\ntraining set (or validation set if present). All experiments were stopped after 100 training epochs\nwith no improvement in either L(\u03b1,\u03b2,D), LE(\u03b2,D) or the number of transcription errors on the\ntraining or validation set. The reason for such conservative stopping criteria was that the error curves\nof some of the networks were extremely noisy (see Fig. 2).\nTable 1 shows the results for the different posteriors and priors. L2 regularisation was no better\nthan unregularised maximum likelihood, while L1 gave a slight improvement; this is consistent\nwith our previous experience of recurrent neural networks. The fully adaptive L1 and L2 networks\nperformed very badly, apparently because the priors became excessively narrow (\u03c32\u2248 0.003 for\nL2 and b \u2248 0.002 for L1). L1 with fixed variance and adaptive mean was somewhat better than L1\nwith mean fixed at 0 (although the adaptive mean was very close to zero, settling around 0.0064).\nThe networks with Gaussian posteriors outperformed those with delta posteriors, with the best score\nobtained using a fully adaptive posterior.\nTable 2 shows the effect of pruning on the trained \u2018adaptive weight noise\u2019 network from Table 1.\nThe pruned networks were retrained using the same optimisation as before, with the error recorded\nbefore and after retraining. As well as being highly effective at removing weights, pruning led to\nimproved performance following retraining in some cases. Notice the slow increase in initial error\nup to \u03bb = 0.5 and sharp rise thereafter; this is consistent with the \u2018safe\u2019 threshold of \u03bb \u2248 0.83\n7"},{"page":8,"text":"Table 2: Effect of Network Pruning. \u2018\u03bb\u2019 is the threshold used for pruning. \u2018Weights\u2019 is the number\nof weights left after pruning and \u2018Percent\u2019 is the same figure expressed as a percentage of the original\nweights. \u2018Initial Error\u2019 is the test error immediately after pruning and \u2018Retrain Error\u2019 is the test error\nfollowing \u2018Retrain Epochs\u2019 of subsequent retraining. \u2018Bits\/weight\u2019 is the average bit cost (as defined\nin Eq. (13)) of the unpruned weights.\nWeights PercentInitial errorRetrain error\n\u03bb\nRetrain Epochs Bits\/weight\n0\n0.01\n0.05\n0.1\n0.2\n0.5\n1\n2\n139,536\n107,974\n63,079\n52,984\n43,182\n31,120\n22,806\n16,029\n100%\n77.4%\n45.2%\n37.9%\n30.9%\n22.3%\n16.3%\n11.5%\n23.8\n23.8\n23.9\n23.9\n23.9\n24.0\n24.5\n28.0\n23.8\n24.0\n23.5\n23.3\n23.7\n23.3\n24.1\n24.5\n0\n972\n35\n351\n740\n125\n403\n335\n0.53\n0.72\n1.15\n1.40\n1.82\n2.21\n3.19\n3.55\ninput gates H forget gatesV forget gates\ncells\noutput gates\ncells\nFigure 3: Weight costs in an 2D LSTM recurrent connection. Each dot corresponds to a weight;\nthe lighter the colour the more bits the weight costs. The vertical axis shows the LSTM cell the\nweightcomesfrom; thehorizontalaxisshowstheLSTMunittheweightgoesto. Notethelowcostof\nthe \u2018V forget gates\u2019 (these mediate vertical correlations between frequency bands in the spectrogram,\nwhich are apparently less important to transcription than horizontal correlations between timesteps);\nthe high cost of the \u2018cells\u2019 (LSTM\u2019s main processing units); the bright horizontal and vertical bands\n(corresponding to units with \u2018important\u2019 outputs and inputs respectively); and the bright diagonal\nthrough the cells (corresponding to self connections).\nmentioned in Section 7. The lowest final phoneme error rate of 23.3 would until recently have been\nthe best recorded on TIMIT; however the application of deep belief networks has now improved the\nbenchmark to 20.5 [3].\nAcknowledgements\nI would like to thank Geoffrey Hinton, Christian Osendorfer, Justin Bayer and Thomas R\u00a8 uckstie\u00df\nfor helpful discussions and suggestions. Alex Graves is a Junior Fellow of the Canadian Institute for\nAdvanced Research.\nFigure 4: The \u2018cell\u2019 weights from Fig. 3 pruned at different thresholds. Black dots are pruned\nweights, white dots are remaining weights. \u2018Cheaper\u2019 weights tend to be removed first as \u03bb grows.\n8"},{"page":9,"text":"References\n[1] D. Barber and C. M. Bishop. Ensemble learning in Bayesian neural networks., pages 215\u2013237. Springer-\nVerlag, Berlin, 1998.\n[2] D. Barber and B. Schottky. Radial basis functions: A bayesian treatment. In NIPS, 1997.\n[3] G. E. Dahl, M. Ranzato, A. rahman Mohamed, and G. Hinton.\ncovariance restricted boltzmann machine. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel,\nand A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 469\u2013477. 2010.\n[4] DARPA-ISTO. The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus (TIMIT), speech disc\ncd1-1.1 edition, 1990.\n[5] B. J. Frey. Graphical models for machine learning and digital communication. MIT Press, Cambridge,\nMA, USA, 1998.\n[6] K. fu Lee and H. wuen Hon. Speaker-independent phone recognition using hidden markov models. IEEE\nTransactions on Acoustics, Speech, and Signal Processing, 1989.\n[7] C. L. Giles and C. W. Omlin. Pruning recurrent neural networks for improved generalization performance.\nIEEE Transactions on Neural Networks, 5:848\u2013851, 1994.\n[8] A. Graves, S. Fern\u00b4 andez, F. Gomez, and J. Schmidhuber. Connectionist temporal classification: La-\nbelling unsegmented sequence data with recurrent neural networks. In Proceedings of the International\nConference on Machine Learning, ICML 2006, Pittsburgh, USA, 2006.\n[9] A. Graves and J. Schmidhuber. Offline handwriting recognition with multidimensional recurrent neural\nnetworks. In NIPS, pages 545\u2013552, 2008.\n[10] G. E. Hinton and D. van Camp. Keeping the neural networks simple by minimizing the description length\nof the weights. In COLT, pages 5\u201313, 1993.\n[11] S. Hochreiter and J. Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):1735\u20131780,\n1997.\n[12] A. Honkela and H. Valpola. Variational learning and bits-back coding: An information-theoretic view to\nbayesian learning. IEEE Transactions on Neural Networks, 15:800\u2013810, 2004.\n[13] K.-C. Jim, C. Giles, and B. Horne. An analysis of noise in recurrent neural networks: convergence and\ngeneralization. Neural Networks, IEEE Transactions on, 7(6):1424 \u20131438, nov 1996.\n[14] N. D. Lawrence. Variational Inference in Probabilistic Models. PhD thesis, University of Cambridge,\n2000.\n[15] Y. Le Cun, J. Denker, and S. Solla. Optimal brain damage. In D. S. Touretzky, editor, Advances in Neural\nInformation Processing Systems, volume 2, pages 598\u2013605. Morgan Kaufmann, San Mateo, CA, 1990.\n[16] D. J. C. MacKay. Probable networks and plausible predictions - a review of practical bayesian methods\nfor supervised neural networks. Neural Computation, 1995.\n[17] S. J. Nowlan and G. E. Hinton. Simplifying neural networks by soft weight sharing. Neural Computation,\n4:173\u2013193, 1992.\n[18] M. Opper and C. Archambeau. The variational gaussian approximation revisited. Neural Computation,\n21(3):786\u2013792, 2009.\n[19] D. Plaut, S. Nowlan, and G. E. Hinton. Experiments on learning by back propagation. Technical Report\nCMU-CS-86-126, Department of Computer Science, Carnegie Mellon University, Pittsburgh, PA, 1986.\n[20] M. Riedmiller and T. Braun. A direst adaptive method for faster backpropagation learning: The rprop\nalgorithm. In International Symposium on Neural Networks, 1993.\n[21] J. Rissanen. Modeling by shortest data description. Automatica, 14(5):465 \u2013 471, 1978.\n[22] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating errors,\npages 696\u2013699. MIT Press, Cambridge, MA, USA, 1988.\n[23] C. E. Shannon. A mathematical theory of communication. Bell system technical journal, 27, 1948.\n[24] P. Smolensky. Information processing in dynamical systems: foundations of harmony theory, pages 194\u2013\n281. MIT Press, Cambridge, MA, USA, 1986.\n[25] C. S. Wallace. Classification by minimum-message-length inference. In Proceedings of the international\nconference on Advances in computing and information, ICCI\u201990, pages 72\u201381, New York, NY, USA,\n1990. Springer-Verlag New York, Inc.\n[26] I. H. Witten, R. M. Neal, and J. G. Cleary. Arithmetic coding for data compression. Commun. ACM,\n30:520\u2013540, June 1987.\nPhone recognition with the mean-\n9"}],"widgetId":"rgw25_56ab1ed39c415"},"id":"rgw25_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=267706055&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw26_56ab1ed39c415"},"id":"rgw26_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=267706055&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":267706055,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":267706055,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithSlurp","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2081050578,"url":"researcher\/2081050578_Minhyung_Cho","fullname":"Minhyung Cho","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":57790954,"url":"researcher\/57790954_Chandra_Shekhar_Dhir","fullname":"Chandra Shekhar Dhir","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273675781931011%401442260799143_m"},{"id":2008763278,"url":"researcher\/2008763278_Jaehyung_Lee","fullname":"Jaehyung Lee","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":false,"isSlurp":true,"isNoText":false,"publicationType":"Article","publicationDate":"Sep 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/281768642_Hessian-Free_Optimization_For_Learning_Deep_Multidimensional_Recurrent_Neural_Networks","usePlainButton":true,"publicationUid":281768642,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/281768642_Hessian-Free_Optimization_For_Learning_Deep_Multidimensional_Recurrent_Neural_Networks","title":"Hessian-Free Optimization For Learning Deep Multidimensional Recurrent Neural Networks","displayTitleAsLink":true,"authors":[{"id":2081050578,"url":"researcher\/2081050578_Minhyung_Cho","fullname":"Minhyung Cho","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":57790954,"url":"researcher\/57790954_Chandra_Shekhar_Dhir","fullname":"Chandra Shekhar Dhir","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273675781931011%401442260799143_m"},{"id":2008763278,"url":"researcher\/2008763278_Jaehyung_Lee","fullname":"Jaehyung Lee","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Multidimensional recurrent neural network (MDRNN) has shown a remarkable\nperformance in speech and handwriting recognition. The performance of MDRNN is\nimproved by further increasing its depth, and the difficulty of learning the\ndeeper network is overcome by Hessian-free (HF) optimization. Considering that\nconnectionist temporal classification (CTC) is utilized as an objective of\nlearning MDRNN for sequence labelling, the non-convexity of CTC poses a problem\nto apply HF to the network. As a solution to this, a convex approximation of\nCTC is formulated and its relationship with the EM algorithm and the Fisher\ninformation matrix is discussed. MDRNN up to the depth of 15 layers is\nsuccessfully trained using HF, resulting in improved performance for sequence\nlabelling.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/281768642_Hessian-Free_Optimization_For_Learning_Deep_Multidimensional_Recurrent_Neural_Networks","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1509.03475","sourceName":"de.arxiv.org","hasSourceUrl":true},"publicationUid":281768642,"publicationUrl":"publication\/281768642_Hessian-Free_Optimization_For_Learning_Deep_Multidimensional_Recurrent_Neural_Networks","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/281768642_Hessian-Free_Optimization_For_Learning_Deep_Multidimensional_Recurrent_Neural_Networks\/links\/5632bbfd08ae911fcd49173e\/smallpreview.png","linkId":"5632bbfd08ae911fcd49173e","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=281768642&reference=5632bbfd08ae911fcd49173e&eventCode=&origin=publication_list","widgetId":"rgw30_56ab1ed39c415"},"id":"rgw30_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=281768642&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":267706055,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/281768642_Hessian-Free_Optimization_For_Learning_Deep_Multidimensional_Recurrent_Neural_Networks\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["For phoneme recognition, the regularization method suggested in [24] was used. We applied Gaussian weight noise of standard deviation \u03c3 = {0.03, "],"widgetId":"rgw31_56ab1ed39c415"},"id":"rgw31_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw29_56ab1ed39c415"},"id":"rgw29_56ab1ed39c415","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=281768642&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithSlurp","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2069013556,"url":"researcher\/2069013556_Yarin_Gal","fullname":"Yarin Gal","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":false,"isSlurp":true,"isNoText":false,"publicationType":"Article","publicationDate":"Jun 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":1,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/277959252_Bayesian_Convolutional_Neural_Networks_with_Bernoulli_Approximate_Variational_Inference","usePlainButton":true,"publicationUid":277959252,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/277959252_Bayesian_Convolutional_Neural_Networks_with_Bernoulli_Approximate_Variational_Inference","title":"Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference","displayTitleAsLink":true,"authors":[{"id":2069013556,"url":"researcher\/2069013556_Yarin_Gal","fullname":"Yarin Gal","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"We present an efficient Bayesian convolutional neural network (convnet). The\nmodel offers better robustness to over-fitting on small data than traditional\napproaches. This is by placing a probability distribution over the convnet's\nkernels (also known as filters). We approximate the model's intractable\nposterior with Bernoulli variational distributions. This requires no additional\nmodel parameters. Our model can be implemented using existing tools in the\nfield. This is by extending the recent interpretation of dropout as approximate\ninference in the Gaussian process to the case of Bayesian neural networks. The\nmodel achieves a considerable improvement in classification accuracy compared\nto previous approaches. We finish with state-of-the-art results on CIFAR-10\nfollowing our new interpretation.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/277959252_Bayesian_Convolutional_Neural_Networks_with_Bernoulli_Approximate_Variational_Inference","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1506.02158","sourceName":"de.arxiv.org","hasSourceUrl":true},"publicationUid":277959252,"publicationUrl":"publication\/277959252_Bayesian_Convolutional_Neural_Networks_with_Bernoulli_Approximate_Variational_Inference","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/277959252_Bayesian_Convolutional_Neural_Networks_with_Bernoulli_Approximate_Variational_Inference\/links\/55ee47ee08ae199d47beed2c\/smallpreview.png","linkId":"55ee47ee08ae199d47beed2c","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=277959252&reference=55ee47ee08ae199d47beed2c&eventCode=&origin=publication_list","widgetId":"rgw33_56ab1ed39c415"},"id":"rgw33_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=277959252&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":267706055,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/277959252_Bayesian_Convolutional_Neural_Networks_with_Bernoulli_Approximate_Variational_Inference\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["This is done by minimising the Kullback-Leibler divergence from the full model. Many have followed this approach in the past for standard NN models [5] [6] [7] [8]. But the variational approach used to approximate the posterior in Bayesian NNs can be fairly computationally expensive \u2013 the use of Gaussian approximating distributions increases the number of model parameters considerably, without increasing model capacity by much. "],"widgetId":"rgw34_56ab1ed39c415"},"id":"rgw34_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw32_56ab1ed39c415"},"id":"rgw32_56ab1ed39c415","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=277959252&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":70672430,"url":"researcher\/70672430_John_Paisley","fullname":"John Paisley","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2064238818,"url":"researcher\/2064238818_David_Blei","fullname":"David Blei","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":65912024,"url":"researcher\/65912024_Michael_Jordan","fullname":"Michael Jordan","last":true,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278648436346888%401443446372379_m"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[[]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Jun 2012","journal":null,"showEnrichedPublicationItem":false,"citationCount":21,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search","usePlainButton":true,"publicationUid":228095632,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search","title":"Variational Bayesian Inference with Stochastic Search","displayTitleAsLink":true,"authors":[{"id":70672430,"url":"researcher\/70672430_John_Paisley","fullname":"John Paisley","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2064238818,"url":"researcher\/2064238818_David_Blei","fullname":"David Blei","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":65912024,"url":"researcher\/65912024_Michael_Jordan","fullname":"Michael Jordan","last":true,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278648436346888%401443446372379_m"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Mean-field variational inference is a method for approximate Bayesian\nposterior inference. It approximates a full posterior distribution with a\nfactorized set of distributions by maximizing a lower bound on the marginal\nlikelihood. This requires the ability to integrate a sum of terms in the log\njoint likelihood using this factorized distribution. Often not all integrals\nare in closed form, which is typically handled by using a lower bound. We\npresent an alternative algorithm based on stochastic optimization that allows\nfor direct optimization of the variational lower bound. This method uses\ncontrol variates to reduce the variance of the stochastic search gradient, in\nwhich existing lower bounds can play an important role. We demonstrate the\napproach on two non-conjugate models: logistic regression and an approximation\nto the HDP.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Michael_Jordan13\/publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search\/links\/53fe1e670cf21edafd14cb6c.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Michael_Jordan13","sourceName":"Michael Jordan","hasSourceUrl":true},"publicationUid":228095632,"publicationUrl":"publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search\/links\/53fe1e670cf21edafd14cb6c\/smallpreview.png","linkId":"53fe1e670cf21edafd14cb6c","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=228095632&reference=53fe1e670cf21edafd14cb6c&eventCode=&origin=publication_list","widgetId":"rgw36_56ab1ed39c415"},"id":"rgw36_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=228095632&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"53fe1e670cf21edafd14cb6c","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":267706055,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw35_56ab1ed39c415"},"id":"rgw35_56ab1ed39c415","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=228095632&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":267706055,"publicationLink":"publication\/267706055_Practical_Variational_Inference_for_Neural_Networks","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw28_56ab1ed39c415"},"id":"rgw28_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=267706055&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=14","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":14,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw27_56ab1ed39c415"},"id":"rgw27_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=267706055&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/267706055_Practical_Variational_Inference_for_Neural_Networks","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1ed39c415"},"id":"rgw2_56ab1ed39c415","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":267706055},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=267706055&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1ed39c415"},"id":"rgw1_56ab1ed39c415","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"NZNUEIO9UzHQ6WyKVqUyl3\/WREgc3tcP6EfxPoylUyS36rx3wUsp+UOltjSc1CQkChs6j051sd\/bVd1K4HXezNDDb+2uadgIe1SpdFukCMJFthQnu17xwvsbks5Qi0h9qXKEXJ+tm8uQ\/EqlMZbn\/pvZDqAIWlmwV4X4BMMR\/wdB8f+jxVNTdjwakzEHp3wIr6EYGDwapcA0Zan2uatpODRbJQRdwtfXHZkRIddUyyRettNKmOaUb9cUBSTPocXhnDAadWGjNR7ZqxlSBIwebpiK\/8F7hC05RhEZn4\/Be4Y=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/267706055_Practical_Variational_Inference_for_Neural_Networks\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Practical Variational Inference for Neural Networks\" \/>\n<meta property=\"og:description\" content=\"Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/267706055_Practical_Variational_Inference_for_Neural_Networks\/links\/5459d5750cf2cf516483e4b5\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/267706055_Practical_Variational_Inference_for_Neural_Networks\" \/>\n<meta property=\"rg:id\" content=\"PB:267706055\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Practical Variational Inference for Neural Networks\" \/>\n<meta name=\"citation_author\" content=\"Alex Graves\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/267706055_Practical_Variational_Inference_for_Neural_Networks\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/267706055_Practical_Variational_Inference_for_Neural_Networks\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-1b0cb345-1fde-400d-9f6a-1877abd0c011","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":627,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw37_56ab1ed39c415"},"id":"rgw37_56ab1ed39c415","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-1b0cb345-1fde-400d-9f6a-1877abd0c011", "3614c8522e82e53b166e3ee08b377164d6c1d6be");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-1b0cb345-1fde-400d-9f6a-1877abd0c011", "3614c8522e82e53b166e3ee08b377164d6c1d6be");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw38_56ab1ed39c415"},"id":"rgw38_56ab1ed39c415","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/267706055_Practical_Variational_Inference_for_Neural_Networks","requestToken":"QHpoB9Azc3si773OvevjUWGNqi\/1K1ew4jrRCivnwTkJAj\/b7BwZrbWamYjorpbSCTi0N3mL+qzNe\/o3CcatIFo94hjTL3+PSpcej\/F9wwy\/+cxjVJw7pmv5yGfKZg9IEyipUowv4U1h30UdBjhEwjUCHAugK9UX7t1KCMuLZCQ+IAdp3wfZW3voBfE+QcLbqup0IMVLPL\/Id9wA2pvPaeZ8UZEVOZxCDIn7mTmf4HQkPgbrU6mCbEMPlVkcLv2r4Txlny3G9kOJiDqdBsZUEZ6+3HcwrlIf59iw6av8gIE=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=QY9q7y7bRP7lGKY1NWxiKwd57D4pmNsYM5dlSGyVOyjBp8XHDBzmoW_iafVGso0F","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjY3NzA2MDU1X1ByYWN0aWNhbF9WYXJpYXRpb25hbF9JbmZlcmVuY2VfZm9yX05ldXJhbF9OZXR3b3Jrcw%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw40_56ab1ed39c415"},"id":"rgw40_56ab1ed39c415","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw39_56ab1ed39c415"},"id":"rgw39_56ab1ed39c415","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw41_56ab1ed39c415"},"id":"rgw41_56ab1ed39c415","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
