<!DOCTYPE html> <html lang="en" class="" id="rgw39_56ab1f18415d8"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="HPj8UgMF89A3vpJQwgnYWDNsxIk3ftNnSXo2d4EIcsiPVoYA9GwzmfteIuBNnVDKDLi16ylp98CroD1rM/dtfXc5JHiWTzz0mwTUZJbg2g8Jbo0ULu/kF9d1Hn2LDYfffo6TZsRSo0yfcH+Ks99nnGgEsPBd0KGGBV/MKVc3pOip1zjDVihDX3YiXeru9ltcZYibCOHEdGMtG+pleiamr4kFEPvQiNPJLljqP8kiWDvE0welkTsEjeIkvgJnDan+q8HE6nkvjCTyyudziu3S7to6gpkm3Lnqfk4b2sDTt4M="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-d681910f-6abe-427b-bbec-7f5cf65314b4",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/259400035_Auto-Encoding_Variational_Bayes" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Auto-Encoding Variational Bayes" />
<meta property="og:description" content="Can we efficiently learn the parameters of directed probabilistic models, in
the presence of continuous latent variables with intractable posterior
distributions, and in case of large datasets? We..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/259400035_Auto-Encoding_Variational_Bayes/links/02d4e0f60cf2df62f0a56786/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/259400035_Auto-Encoding_Variational_Bayes" />
<meta property="rg:id" content="PB:259400035" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Auto-Encoding Variational Bayes" />
<meta name="citation_author" content="Diederik P Kingma" />
<meta name="citation_author" content="Max Welling" />
<meta name="citation_publication_date" content="2013/12/20" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/259400035_Auto-Encoding_Variational_Bayes" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/259400035_Auto-Encoding_Variational_Bayes" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Auto-Encoding Variational Bayes</title>
<meta name="description" content="Auto-Encoding Variational Bayes on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1f18415d8" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1f18415d8" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw7_56ab1f18415d8">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Auto-Encoding%20Variational%20Bayes&rft.title=ICLR&rft.jtitle=ICLR&rft.date=2013&rft.au=Diederik%20P%20Kingma%2CMax%20Welling&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Auto-Encoding Variational Bayes</h1> <meta itemprop="headline" content="Auto-Encoding Variational Bayes">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/259400035_Auto-Encoding_Variational_Bayes/links/02d4e0f60cf2df62f0a56786/smallpreview.png">  <div id="rgw9_56ab1f18415d8" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw10_56ab1f18415d8"> <a href="researcher/2040421796_Diederik_P_Kingma" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Diederik P Kingma" alt="Diederik P Kingma" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Diederik P Kingma</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw11_56ab1f18415d8">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2040421796_Diederik_P_Kingma"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Diederik P Kingma" alt="Diederik P Kingma" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2040421796_Diederik_P_Kingma" class="display-name">Diederik P Kingma</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56ab1f18415d8"> <a href="researcher/69847505_Max_Welling" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Max Welling" alt="Max Welling" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Max Welling</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw13_56ab1f18415d8">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/69847505_Max_Welling"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Max Welling" alt="Max Welling" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/69847505_Max_Welling" class="display-name">Max Welling</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">     ICLR   <meta itemprop="datePublished" content="2013-12">  12/2013;               <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1312.6114" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw14_56ab1f18415d8" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>Can we efficiently learn the parameters of directed probabilistic models, in<br />
the presence of continuous latent variables with intractable posterior<br />
distributions, and in case of large datasets? We introduce a novel learning and<br />
approximate inference method that works efficiently, under some mild<br />
conditions, even in the on-line and intractable case. The method involves<br />
optimization of a stochastic objective function that can be straightforwardly<br />
optimized w.r.t. all parameters, using standard gradient-based optimization<br />
methods.<br />
The method does not require the typically expensive sampling loops per<br />
datapoint required for Monte Carlo EM, and all parameter updates correspond to<br />
optimization of the variational lower bound of the marginal likelihood, unlike<br />
the wake-sleep algorithm. These theoretical advantages are reflected in<br />
experimental results.</div> </p>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw27_56ab1f18415d8">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw26_56ab1f18415d8"  itemprop="articleBody">  <p>Page 1</p> <p>000<br />001<br />002<br />003<br />004<br />005<br />006<br />007<br />008<br />009<br />010<br />011<br />012<br />013<br />014<br />015<br />016<br />017<br />018<br />019<br />020<br />021<br />022<br />023<br />024<br />025<br />026<br />027<br />028<br />029<br />030<br />031<br />032<br />033<br />034<br />035<br />036<br />037<br />038<br />039<br />040<br />041<br />042<br />043<br />044<br />045<br />046<br />047<br />048<br />049<br />050<br />051<br />052<br />053<br />Auto-Encoding Variational Bayes<br />Anonymous Author(s)<br />Affiliation<br />Address<br />email<br />Abstract<br />Can we efficiently learn the parameters of directed probabilistic models, in the<br />presence of continuous latent variables with intractable posterior distributions?<br />We introduce an unsupervised on-line learning algorithm that efficiently optimizes<br />the variational lower bound on the marginal likelihood and that, under some mild<br />conditions, even works in the intractable case. The algorithm optimizes a proba-<br />bilistic encoder (also called a recognition network) to approximate the intractable<br />posterior distribution of the latent variables. The crucial element is a reparame-<br />terization of the variational bound with an independent noise variable, yielding<br />a stochastic objective function which can be jointly optimized w.r.t. variational<br />and generative parameters using standard gradient-based stochastic optimization<br />methods. Theoretical advantages are reflected in experimental results.<br />1 Introduction<br />How to efficiently learn the parameters of directed probabilistic models whose continuous latent<br />variables have intractable posterior distributions? The variational approach to approximate Bayesian<br />inference involves the introduction of an approximate posterior to the intractable posterior, used<br />to maximize the variational lower bound on the marginal likelihood. Unfortunately, the common<br />mean-field approach requires analytical solutions to expectations w.r.t. the approximate posterior,<br />which are also intractable in the general case. We show how for continuous latent variables, a<br />reparameterization of the expectation w.r.t. the approximate posterior yields a novel and practical<br />estimator of the variational lower bound that can be differentiated and jointly optimized w.r.t. all<br />parameters, i.e. both the variational parameters and regular parameters, using standard stochastic<br />gradient ascent techniques.<br />The objective contains, in addition to regularization terms dictated by the variational bound, a noisy<br />data reconstruction term, exposing a novel connection between auto-encoders and stochastic vari-<br />ational inference. In contrast to a typical objective for auto-encoders [BCV13], all parameters up-<br />dates, including those of the noise distribution, correspond to optimization of the variational lower<br />bound on the marginal likelihood. From the learned generative model it is straightforward to gener-<br />ate samples, without the typical requirement of running Markov chains. The probabilistic encoder<br />can be used for fast approximate inference of latent variables, i.e. for recognition, representation<br />or visualization purposes. Furthermore, the lower bound estimator can be used for unsupervised<br />inference tasks such as denoising and inpainting.<br />2 Method<br />The strategy in the following section can be used to derive a lower bound estimator (a stochastic<br />objective function) for a variety of directed graphical models with continuous latent variables. We<br />will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables<br />per datapoint, and where we like to perform ML or MAP inference on the (global) parameters, and<br />1<br />arXiv:1312.6114v5  [stat.ML]  9 Jan 2014</p>  <p>Page 2</p> <p>054<br />055<br />056<br />057<br />058<br />059<br />060<br />061<br />062<br />063<br />064<br />065<br />066<br />067<br />068<br />069<br />070<br />071<br />072<br />073<br />074<br />075<br />076<br />077<br />078<br />079<br />080<br />081<br />082<br />083<br />084<br />085<br />086<br />087<br />088<br />089<br />090<br />091<br />092<br />093<br />094<br />095<br />096<br />097<br />098<br />099<br />100<br />101<br />102<br />103<br />104<br />105<br />106<br />107<br />x<br />z<br />φ<br />θ<br />N<br />Figure1: Thetypeofdirectedgraphicalmodelunderconsideration. Solidlinesdenotethegenerative<br />model pθ(z)pθ(x|z), dashed lines denote the variational approximation qφ(z|x) to the intractable<br />posterior pθ(z|x). The variational parameters φ are learned jointly with the generative model pa-<br />rameters θ.<br />variationalinferenceonthelatentvariables. Itis, forexample, straightforwardtoextendthisscenario<br />to the case where we also perform variational inference on the global parameters; that algorithm is<br />put in the appendix, but experiments with that case are left to future work. Note that our method can<br />be applied to online, non-stationary settings, e.g. streaming data, but here we assume a fixed dataset<br />for simplicity.<br />2.1Problem scenario<br />Let us consider some dataset X = {x(i)}N<br />or discrete variable x. We assume that the data are generated by some random process, involving<br />an unobserved continuous random variable z. The process consists of two steps: (1) a value z(i)<br />is generated from some prior distribution pθ∗(z); (2) a value x(i)is generated from some condi-<br />tional distribution pθ∗(x|z). We assume that the prior pθ∗(z) and likelihood pθ∗(x|z) come from<br />parametric families of distributions pθ(z) and pθ(x|z), and that their PDFs are differentiable almost<br />everywhere w.r.t. both θ and z. Unfortunately, a lot of this process is hidden from our view: the true<br />parameters θ∗as well as the values of the latent variables z(i)are unknown to us.<br />Very importantly, we do not make the usual simplifying assumptions common in the literature.<br />Conversely, we are here interested in a general algorithm that even works in the case of:<br />i=1consisting of N i.i.d. samples of some continuous<br />1. Intractability:<br />?pθ(z)pθ(x|z)dz is intractable (so we cannot evaluate or differentiate the marginal like-<br />the EM algorithm cannot be used), and where the required integrals for any reasonable<br />mean-field Variational Bayes are also intractable. These intractabilities are quite common<br />and already appear in case of moderately complicated likelihood functions pθ(x|z), e.g. a<br />neural network with a nonlinear hidden layer.<br />2. A large dataset: we have so much data that batch optimization is too costly; we would like<br />to make parameter updates using small minibatches or even single datapoints. Sampling-<br />based solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a<br />typically expensive sampling loop per datapoint.<br />the case where the integral of the marginal likelihood pθ(x)=<br />lihood), where the true posterior density pθ(z|x) = pθ(x|z)pθ(z)/pθ(x) is intractable (so<br />We are interested in, and propose a solution to, three related problems in the above scenario:<br />1. Efficient approximate maximum likelihood (ML) or maximum a posteriori (MAP) estima-<br />tion for the parameters θ. The parameters can be of interest themselves, e.g. if we are<br />analyzing some natural process. They also allow us to mimic the hidden random process<br />and generate artificial data that resembles the real data.<br />2. Efficient approximate posterior inference of the latent variable z given an observed value x<br />for a choice of parameters θ. This is useful for coding or data representation tasks.<br />3. Efficient approximate marginal inference of the variable x. This allows us to perform all<br />kindsof inferencetaskswherea prioroverx isrequired. Common applicationsincomputer<br />vision include image denoising, inpainting and super-resolution.<br />2</p>  <p>Page 3</p> <p>108<br />109<br />110<br />111<br />112<br />113<br />114<br />115<br />116<br />117<br />118<br />119<br />120<br />121<br />122<br />123<br />124<br />125<br />126<br />127<br />128<br />129<br />130<br />131<br />132<br />133<br />134<br />135<br />136<br />137<br />138<br />139<br />140<br />141<br />142<br />143<br />144<br />145<br />146<br />147<br />148<br />149<br />150<br />151<br />152<br />153<br />154<br />155<br />156<br />157<br />158<br />159<br />160<br />161<br />For the purpose of solving the above problems, let us introduce the parametric variational approx-<br />imation qφ(z|x): an approximation to the intractable true posterior pθ(z|x). Note that in contrast<br />with the approximate posterior in mean-field variational inference, it is not necesarilly factorial and<br />its parameters are not computed from some closed-form expectation. Instead, its parameters φ are<br />learned jointly with the parameters of the generative model.<br />From a coding theory perspective, the unobserved variables z have an interpretation as a latent<br />representationorcode. Inthispaperwewillthereforealsorefertoqφ(z|x)asa(variational)encoder<br />or recognition model, since given a datapoint x it produces a distribution (e.g. a Gaussian) over the<br />possible valuesof thecodezfrom whichthe datapointx could havebeen generated. Ina similar vein<br />we will refer to pθ(x|z) as a (generative) decoder, since given a code z it produces a distribution<br />over the possible corresponding values of x.<br />2.2The variational bound<br />The marginal likelihood is composed of a sum over the marginal likelihoods of individual datapoints<br />logpθ(x(1),··· ,x(N)) =?N<br />logpθ(x(i)) = DKL(qφ(z|x(i))||pθ(z|x(i))) + L(θ,φ;x(i))<br />The first RHS term is the KL divergence of the approximate from the true posterior, which is non-<br />negative. The second RHS term L(θ,φ;x(i)) denotes the variational lower bound on the marginal<br />likelihood of datapoint i:<br />?<br />Note that the bound equals the true marginal when the divergence of the approximate from true<br />posterior distribution is zero.<br />The expectation on the RHS of eq. (2) can obviously be written as a sum of three separate expec-<br />tations, of which the second and third component can sometimes be analytically solved, e.g. when<br />both pθ(x) and qφ(z|x) are Gaussian. For generality we will here assume that each of these expec-<br />tations are intractable.<br />We would like to optimize the lower bound L(θ,φ;x(i)) (eq. (2)) using stochastic gradients. Note<br />that following these gradients would either decrease the KL divergence between the approximate<br />and true posterior distributions, or increase the marginal likelihood, or both. A na¨ ıve attempt to<br />compute a stochastic gradient would be to draw samples {z(l)}L<br />the following Monte Carlo estimate of the lower bound:<br />?<br />While the above expression is an unbiased estimator of the marginal likelihood (i.e. it will equal<br />the lower bound in the limit L → ∞), differentiating it w.r.t. the parameters φ will not result in an<br />unbiasedgradient: thevariationalparametersφindirectlyinfluencetheestimatethroughthesamples<br />z(l)∼ qφ(z|x), and it is impossible to differentiate through this sampling process. Existing work<br />on stochastic variational bayes provide workarounds [BJP12], but not a solution to this problem.<br />i=1logpθ(x(i)), which can each be rewritten as:<br />(1)<br />logpθ(x(i)) ≥ L(θ,φ;x(i)) =qφ(z|x)<br />?<br />logpθ(x(i)|z) + logpθ(z) − logqφ(z|x)<br />?<br />dz<br />(2)<br />l=1from qφand then differentiate<br />L(θ,φ;x(i)) ?1<br />L<br />L<br />?<br />l=1<br />logpθ(x(i)|z(l)) + logpθ(z(l)) − logqφ(z(l)|x(i))<br />?<br />where<br />z(l)∼ qφ(z|x)<br />2.3 Our estimator of the lower bound<br />Under certain mild conditions outlined in section 2.4 for a chosen approximate posterior qφ(z|x)<br />we can reparameterize its conditional samples ? z ∼ qφ(z|x) as<br />where we choose a prior p(?) and a function gφ(?,x) such that the following holds:<br />?<br />=p(?)logpθ(x(i)|z) + logpθ(z) − logqφ(z|x)<br />? z = gφ(?,x)<br />qφ(z|x)<br />?<br />with<br />? ∼ p(?)<br />(3)<br />L(θ,φ;x(i)) =<br />?<br />logpθ(x(i)|z) + logpθ(z) − logqφ(z|x)<br />?<br />dz<br />?<br />?????<br />z=gφ(?,x(i))<br />d?<br />(4)<br />3</p>  <p>Page 4</p> <p>162<br />163<br />164<br />165<br />166<br />167<br />168<br />169<br />170<br />171<br />172<br />173<br />174<br />175<br />176<br />177<br />178<br />179<br />180<br />181<br />182<br />183<br />184<br />185<br />186<br />187<br />188<br />189<br />190<br />191<br />192<br />193<br />194<br />195<br />196<br />197<br />198<br />199<br />200<br />201<br />202<br />203<br />204<br />205<br />206<br />207<br />208<br />209<br />210<br />211<br />212<br />213<br />214<br />215<br />Algorithm 1 Pseudocode for computing a stochastic gradient using our estimator. See section 2.3<br />for meaning of the functions fθ,φand gφ. The minibatch XM= {x(i)}M<br />subset of the full dataset X. We use settings M = 100 and L = 1 in experiments.<br />Require: θ,φ (Current value of parameters)<br />g ← 0<br />XM← Random subset (minibatch) of M datapoints from dataset<br />for each x ∈ XMdo<br />for l is 1 to L do<br />? ← Random sample from p(?)<br />g ← g + ∇θ,φfθ,φ(x,gφ(?,x))<br />end for<br />end for<br />return (N/(M · L)) · g<br />i=1is a randomly drawn<br />For notational conciseness we introduce a shorthand notation fθ,φ(x,z) for the sum of three PDFs:<br />fθ,φ(x,z) = logpθ(x|z) + logpθ(z) − logqφ(z|x)<br />Using eq. (4), the Monte Carlo estimate of the variational lower bound, given datapoint x(i), is:<br />(5)<br />L(θ,φ;x(i)) ?1<br />L<br />L<br />?<br />l=1<br />fθ,φ(x(i),gφ(?(l),x(i)))<br />where<br />?(l)∼ p(?)<br />(6)<br />Theestimatoronlydependsonsamplesfromp(?)whichareobviouslynotinfluencedbyφ, therefore<br />wecanuseitasanobjectivefunctionthatcanbedifferentiatedandjointlyoptimizedw.r.t. bothθ and<br />φ. Given multiple datapoints from the dataset X, we can easily construct a minibatch-based version<br />of the estimator: L(θ,φ;X) ?<br />a randomly drawn subset of the full dataset X. In our experiments we found that the number of<br />samples L per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M =<br />100. Derivatives∇θ,φ?L(θ;XM)canbetaken, andtheresultinggradientscanbeusedinconjunction<br />approach to compute the stochastic gradients.<br />A connection with auto-encoders becomes clear when looking at the objective function given at<br />eq. (6). The variational approximation qφ(z|x(i)) (the encoder) maps a datapoint x(i)to a distribu-<br />tion over latent variables z from which the datapoint could have been generated. The function gφ(.)<br />is chosen such that it maps a datapoint x(i)and a random noise vector ?(l)to a sample from the<br />approximate posterior for that datapoint: z(i,l)= gφ(?(l),x(i)) where z(i,l)∼ qφ(z|x(i)). Subse-<br />quently, the sample z(i,l)is then input to function fθ,φ(.), which consists of three parts. The first<br />part (logpθ(x(i)|z(i,l))) can be interpreted as the negative reconstruction error in neural network<br />parlance. The second and third part can be interpreted as regularization terms that make sure the<br />code activations have high entropy due to the term logqφ(z|x), while not being too far from the<br />prior due to the term logpθ(z).<br />N<br />M<br />?M<br />i=1L(θ,φ;x(i)) where the minibatch XM= {x(i)}M<br />i=1is<br />with stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic<br />2.4The alternative parameterization trick<br />In order to solve our problem we invoked an alternative method for generating samples from<br />qφ(z|x). The essential parameterization trick is quite simple. Let qφ(z|x) be some conditional<br />distribution parameterized by φ. It is then often possible to express the random variable z given x<br />as a determinstic variable z = gφ(?,x), where ? is an auxiliary variable with independent marginal<br />p(?), and gφ(.) is some vector-valued function parameterized by φ.<br />This reparameterization is useful for our case since it can be used to rewrite an expectation w.r.t<br />qφ(z|x) such that the Monte Carlo estimate of the expectation is differentiable w.r.t. φ. A proof<br />is as follows. Given the deterministic mapping z = gφ(?,x) we know that qφ(z|x)?<br />1Note that for infinitesimals we use the notational convention dz =?<br />4<br />idzi =<br />p(?)?<br />id?i. Therefore1,?qφ(z|x)f(z)dz =<br />?p(?)f(z)d? =<br />?p(?)f(gφ(?,x))d?. It follows<br />idzi</p>  <p>Page 5</p> <p>216<br />217<br />218<br />219<br />220<br />221<br />222<br />223<br />224<br />225<br />226<br />227<br />228<br />229<br />230<br />231<br />232<br />233<br />234<br />235<br />236<br />237<br />238<br />239<br />240<br />241<br />242<br />243<br />244<br />245<br />246<br />247<br />248<br />249<br />250<br />251<br />252<br />253<br />254<br />255<br />256<br />257<br />258<br />259<br />260<br />261<br />262<br />263<br />264<br />265<br />266<br />267<br />268<br />269<br />that a differentiable estimator can be constructed:<br />where ?(l)∼ p(?). In section 2.3 we applied this trick to obtain a differentiable estimator of the<br />variational lower bound.<br />Take, for example, the univariate Gaussian case: let z be distributed as p(z|x) = N(x,σ). The<br />random variable z is partially explained by x, but there is some uncertainty left indicated by σ.<br />In this case, a deterministic parameterization is z = x + σ?, where ? is an independent auxiliary<br />variable ? ∼ N(0,1). In this univariate Gaussian case, φ = {σ} and gφ(?,y) = y + σ?.<br />When can we do this, i.e., for which qφ(z|x) can we choose such a gφ(.) and p(?)? There are three<br />basic approaches:<br />?qφ(z|x)f(z)dz ?<br />1<br />L<br />?L<br />l=1f(gφ(x,?(l)))<br />1. Tractable inverse CDF. In this case, let ? ∼ U(0,I), and let gφ(?,x) be the inverse CDF of<br />qφ(z|x). Examples: Exponential, Cauchy, Logistic, Rayleigh, Pareto, Weibull, Reciprocal,<br />Gompertz, Gumbel and Erlang distributions.<br />2. Analogous to the Gaussian example, for any ”location-scale” family of distributions (with<br />differentiablelog-PDF) wecan choosethe standarddistribution (withlocation = 0, scale =<br />1) as the auxiliary variable E, and let g(.) = location + scale · ?. Examples: Laplace,<br />Elliptical, Student’s t, Logistic, Uniform, Triangular and Gaussian distributions.<br />3. Composition: It is often possible to express variables as functions of component variables<br />with distributions that are reparameterizable using either of the above two approaches. Ex-<br />amples: Log-Normal (exponentiation of normally distributed variable), Gamma (a sum<br />over exponentially distributed variables), Dirichlet (weighted sum of Gamma variates),<br />Beta, Chi-Squared, and F distributions.<br />When all three approaches fail, good approximations to the inverse CDF exist requiring computa-<br />tions with time complexity comparable to the PDF (see e.g. [Dev86] for some methods).<br />3Example<br />Here we’ll give an example generative model and posterior approximation used in experiments.<br />Let the prior over the latent variables be the centered isotropic Gaussian pθ(z) = N(z;0,I). Note<br />that in this case, the prior lacks parameters. Let pθ(x|z) (the decoder) be a multivariate Bernoulli<br />whose probabilities are computed from z with a fully-connected neural network with a single hidden<br />layer:<br />logpθ(x|z) =<br />D<br />?<br />i=1<br />xilogyi− (1 − xi) · log(1 − yi)<br />where y = fσ(W2tanh(W1z + b1) + b2)<br />(7)<br />where fσ(.) is the elementwise sigmoid activation function. While there is much freedom in the<br />choice of the approximate posterior qφ(z|x) (encoder / recognition model), we’ll for a moment<br />assume a relatively simple case: let’s assume that the true posterior pθ(z|x) takes on a approximate<br />Gaussian form with an approximately diagonal covariance. In this case, we can let the variational<br />approximate posterior be a multivariate Gaussian with a diagonal covariance structure2:<br />logqφ(z|x) = logN(z;µ,σ2I)<br />(8)<br />where µ and σ are yet unspecified functions of x. We can sample from qφ(z|x) using ? z =<br />lower bound is:<br />hφ(x,?) = µ+σ ?? where ? ∼ N(0,I). With ? we signify an element-wise product. Therefore,<br />given a minibatch XMof data, and using the fθ,φ(.) abbrevation of eq. (5), our estimator of the<br />L(θ,φ;x(i)) ?1<br />L<br />L<br />?<br />l=1<br />fθ,φ(x(i),z(i,l))??<br />z(i,l)=µ(i)+σ(i)??(l)<br />where<br />?(l)∼ N(0,I)<br />(9)<br />2Note that this is just a (simplifying) choice, and not a limitation of our method.<br />5</p>  <p>Page 6</p> <p>270<br />271<br />272<br />273<br />274<br />275<br />276<br />277<br />278<br />279<br />280<br />281<br />282<br />283<br />284<br />285<br />286<br />287<br />288<br />289<br />290<br />291<br />292<br />293<br />294<br />295<br />296<br />297<br />298<br />299<br />300<br />301<br />302<br />303<br />304<br />305<br />306<br />307<br />308<br />309<br />310<br />311<br />312<br />313<br />314<br />315<br />316<br />317<br />318<br />319<br />320<br />321<br />322<br />323<br />whereµ(i)andσ(i)denotethemeanands.d. oftheapproximationoftheposteriorqφ(z|x(i)), which<br />we didn’t yet specify. Let the mean µ(i)and variance σ(i)of the Gaussian encoding distribution be<br />the following nonlinear function of x, (a neural network):<br />logqφ(z|x) = logN(z;µ,σ2I)<br />where µ = W4h + b4, and<br />logσ2= W5h + b5, and<br />h = tanh(W3x + b3) (10)<br />Note that the generative (decoding) parameters are θ = {Wj,bj}2<br />parameters are φ = {Wj,bj}5<br />eq. 6, and the lower bound can subsequently be differentiated and optimized w.r.t. the parameters.<br />In this model both pθ(z) and qφ(z|x) are Gaussian; in this special case, the second and third term<br />of fθ,φ(eq. (5)) can be solved analytically. This results in an estimator with a lower variance than<br />the generic estimator given in eq. (9). The resulting estimator is:<br />j=1and the variational (encoding)<br />j=3. These definitions for the encoder and decoder can be plugged in<br />L(θ,φ;x(i)) ?1<br />2<br />J<br />?<br />j=1<br />?<br />1 + log((σ(i)<br />j)2) − (µ(i)<br />j)2− (σ(i)<br />j)2?<br />+1<br />L<br />L<br />?<br />l=1<br />logpθ(x(i)|z(i,l))<br />(11)<br />See the appendix for the derivation.<br />4Related work<br />Perhaps the most relevant related method is the Wake-Sleep algorithm [HDFN95]. Like AEVB,<br />the wake-sleep algorithm employs an encoder (called a recognition network) that approximates the<br />true posterior. A well-known drawback of the wake-sleep algorithm is that it lacks a theoretically<br />justified method for learning the parameters of the recognition network: its updates correspond<br />to optimization of the divergence KL(p||q) instead of the divergence KL(q||p) dictated by the<br />lower bound. A theoretical advantage of the wake-sleep algorithm is that it also applies to models<br />with discrete latent variables. Wake-Sleep has the same computational complexity as AEVB per<br />datapoint.<br />AEVB corresponds to the optimization of a type of auto-encoder, exposing a connection between<br />directed probabilistic models and auto-encoders. A connection between linear auto-encoders and a<br />certain class of generative linear-Gaussian models has long been known. In [Row98] it was shown<br />that PCA corresponds to the maximum-likelihood (ML) solution of a special case of the linear-<br />Gaussian model with a prior p(z) = N(0,I) and a conditional distribution p(x|z) = N(x;Wz,?I),<br />specifically the case with infinitesimally small ?.<br />In relevant recent work on autoencoders [VLL+10] it was shown that the training criterion of un-<br />regularized autoencoders corresponds to maximization of a lower bound (see the infomax princi-<br />ple [Lin89]) of the mutual information between input X and latent representation Z. Maximiz-<br />ing (w.r.t. parameters) of the mutual information is equivalent to maximizing the conditional en-<br />tropy, which is lower bounded by the expected loglikelihood of the data under the autoencoding<br />model [VLL+10], i.e. the negative reconstrution error. However, it is well known that this recon-<br />struction criterion is in itself not sufficient for learning useful representations [BCV13]. Regular-<br />ization techniques have been proposed to make autoencoders learn useful representations, such as<br />denoising, contractive and sparse autoencoder variants [BCV13]. Our objective function contains<br />(hyper-parameter free) regularization terms dictated by the variational bound (see eq. (5)). Related<br />are also encoder-decoder architectures such as the predictive sparse decomposition (PSD) [KRL08],<br />from which we drew some inspiration. Also relevant are the recently introduced Generative Stochas-<br />tic Networks [BTL13] where noisy auto-encoders learn the transition operator of a Markov chain<br />that samples from the data distribution. In [SL10] a recognition network was employed for efficient<br />learning with Deep Boltzmann Machines. These methods are targeted at either unnormalized mod-<br />els (i.e. undirected models like Boltzmann machines) or limited to sparse coding models, in contrast<br />to our proposed algorithm for learning a general class of directed probabilistic models.<br />The recently proposed NADE method [GMW13], also learns a directed probabilistic model using<br />an auto-encoding structure, however their method applies to binary latent variables, their objective<br />does not correspond to the variational bound and their encoder doesn’t correspond to a variational<br />approximation of the posterior distribution.<br />6</p>  <p>Page 7</p> <p>324<br />325<br />326<br />327<br />328<br />329<br />330<br />331<br />332<br />333<br />334<br />335<br />336<br />337<br />338<br />339<br />340<br />341<br />342<br />343<br />344<br />345<br />346<br />347<br />348<br />349<br />350<br />351<br />352<br />353<br />354<br />355<br />356<br />357<br />358<br />359<br />360<br />361<br />362<br />363<br />364<br />365<br />366<br />367<br />368<br />369<br />370<br />371<br />372<br />373<br />374<br />375<br />376<br />377<br />105<br />106<br />107<br />108<br /># Training samples evaluated<br />150<br />140<br />130<br />120<br />110<br />100<br />L<br />MNIST, Nz=3<br />105<br />106<br />107<br />108<br />150<br />140<br />130<br />120<br />110<br />100<br />MNIST, Nz=5<br />105<br />106<br />107<br />108<br />150<br />140<br />130<br />120<br />110<br />100<br />MNIST, Nz=10<br />105<br />106<br />107<br />108<br />150<br />140<br />130<br />120<br />110<br />100<br />MNIST, Nz=20<br />105<br />106<br />107<br />108<br />150<br />140<br />130<br />120<br />110<br />100<br />MNIST, Nz=200<br />105<br />106<br />107<br />108<br />0<br />200<br />400<br />600<br />800<br />1000<br />1200<br />1400<br />1600<br />L<br />Frey Face, Nz=2<br />Wake-Sleep (test)<br />Wake-Sleep (train)<br />AEVB (test)<br />AEVB (train)<br />105<br />106<br />107<br />108<br />0<br />200<br />400<br />600<br />800<br />1000<br />1200<br />1400<br />1600Frey Face, Nz=5<br />105<br />106<br />107<br />108<br />0<br />200<br />400<br />600<br />800<br />1000<br />1200<br />1400<br />1600Frey Face, Nz=10<br />105<br />106<br />107<br />108<br />0<br />200<br />400<br />600<br />800<br />1000<br />1200<br />1400<br />1600Frey Face, Nz=20<br />Figure 2: Comparison of our AEVB method to the wake-sleep algorithm, in terms of optimizing the<br />lower bound, for different dimensionality of latent space (Nz). Our method converged considerably<br />faster and reached a better solution in all experiments. Interestingly enough, more latent variables<br />does not result in more overfitting, which is explained by the regularizing effect of the lower bound.<br />Vertical axis: the estimated average variational lower bound per datapoint. The estimator variance<br />was small (&lt; 1) and omitted. Horizontal axis: amount of training points evaluated. Computa-<br />tion took around 20-40 minutes per million training samples with a Intel Xeon CPU running at an<br />effective 40 GFLOPS.<br />5Experiments<br />We trained generative models of images from the MNIST and Frey Face datasets3and compared<br />learning algorithms in terms of the variational lower bound, and the estimated marginal likelihood.<br />The generative model (encoder) and variational approximation (decoder) from section 3 were used,<br />where the described encoder and decoder have an equal number of hidden units. Since the Frey<br />Face data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except<br />that the means were constrained to the interval (0,1) using a sigmoidal activation function at the<br />decoder output. Note that with hidden units we refer to the hidden layer of the neural networks of<br />the encoder and decoder.<br />Parameters are updated using stochastic gradient ascent where gradients are computed by differenti-<br />ating the lower bound estimator ∇θ,φL(θ,φ;X) (see algorithm 1), plus a small weight decay term<br />corresponding to a prior p(θ) = N(0,I). Optimization of this objective is equivalent to approxi-<br />mate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower<br />bound.<br />We compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the<br />same encoder (also called recognition network) for the wake-sleep algorithm and the variational<br />auto-encoder. All parameters, both variational and generative, were initialized by random sampling<br />from N(0,0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were<br />adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01,<br />0.02, 0.1} based on performance on the training set in the first few iterations. Minibatches of size<br />M = 100 were used, with L = 1 samples per datapoint.<br />Likelihood lower bound<br />(a.k.a. recognition networks) having 500 hidden units in case of MNIST, and 200 hidden units in<br />case of the Frey Face dataset (to prevent overfitting, since it is a considerably smaller dataset).<br />Figure 2 shows the results when comparing the lower bounds. Interestingly, superfluous latent<br />variables did not result in overfitting, which is explained by the regularizing nature of the variational<br />bound.<br />We trained generative models (decoders) and corresponding encoders<br />3Available at http://www.cs.nyu.edu/˜roweis/data.html<br />7</p>  <p>Page 8</p> <p>378<br />379<br />380<br />381<br />382<br />383<br />384<br />385<br />386<br />387<br />388<br />389<br />390<br />391<br />392<br />393<br />394<br />395<br />396<br />397<br />398<br />399<br />400<br />401<br />402<br />403<br />404<br />405<br />406<br />407<br />408<br />409<br />410<br />411<br />412<br />413<br />414<br />415<br />416<br />417<br />418<br />419<br />420<br />421<br />422<br />423<br />424<br />425<br />426<br />427<br />428<br />429<br />430<br />431<br />0102030405060<br /># Training samples evaluated (millions)<br />160<br />150<br />140<br />130<br />120<br />110<br />100<br />Marginal log-likelihood<br />Ntrain = 1000<br />0102030405060<br />160<br />155<br />150<br />145<br />140<br />135<br />130<br />125<br />Ntrain = 50000<br />Wake-Sleep (train)<br />Wake-Sleep (test)<br />MCEM (train)<br />MCEM (test)<br />AEVB (train)<br />AEVB (test)<br />Figure 3: Comparison of AEVB to the wake-sleep algorithm and Monte Carlo EM, in terms of the<br />estimated marginal likelihood, for a different number of training points. The Monte Carlo EM algo-<br />rithm is (unlike AEVB and the wake-sleep method) asymptotically unbiased but cannot be applied<br />online such that it becomes inefficient for large datasets (right figure).<br />(a) Learned Frey Face manifold(b) Learned MNIST manifold<br />Figure 4: Visualisations of learned data manifold for generative models with two-dimensional latent<br />space, learned with AEVB. Since the prior of the latent space is Gaussian, linearly spaced coor-<br />dinates on the unit square were transformed through the inverse CDF of the Gaussian to produce<br />values of the latent variables z. For each of these values z, we plotted the corresponding generative<br />pθ(x|z) with the learned parameters θ.<br />Marginal likelihood<br />likelihood of the learned generative models using an MCMC estimator. More information about the<br />marginal likelihood estimator is available in the appendix. For the encoder and decoder we again<br />used neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional<br />latent space the estimates became unreliable. Again, the MNIST dataset was used. The AEVB<br />and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo<br />(HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for<br />the three algorithms, for a small and large training set size. Results are in figure 3.<br />For very low-dimensional latent space it is possible to estimate the marginal<br />8</p>  <p>Page 9</p> <p>432<br />433<br />434<br />435<br />436<br />437<br />438<br />439<br />440<br />441<br />442<br />443<br />444<br />445<br />446<br />447<br />448<br />449<br />450<br />451<br />452<br />453<br />454<br />455<br />456<br />457<br />458<br />459<br />460<br />461<br />462<br />463<br />464<br />465<br />466<br />467<br />468<br />469<br />470<br />471<br />472<br />473<br />474<br />475<br />476<br />477<br />478<br />479<br />480<br />481<br />482<br />483<br />484<br />485<br />(a) 2-D latent space(b) 5-D latent space(c) 10-D latent space(d) 20-D latent space<br />Figure 5: Random samples from learned generative models of MNIST for different dimensionalities<br />of latent space.<br />6 Conclusion<br />We have introduced a novel online learning and approximate inference method for models with of<br />continuous latent variables, that works for the case where mean-field VB and EM methods are in-<br />tractable. The proposed estimator can be straightforwardly differentiated and optimized w.r.t. all<br />parameters, resulting in stochastic gradients that are easily plugged into existing stochastic gradient<br />optimization methods. The method learns an encoder, or variational approximation to the poste-<br />rior, that can be used for fast approximate inference of the distribution of the latent variables. The<br />theoretical advantages are reflected in experimental results.<br />References<br />[BCV13] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A re-<br />view and new perspectives. 2013.<br />David M Blei, Michael I Jordan, and John W Paisley. Variational bayesian inference<br />with stochastic search. In Proceedings of the 29th International Conference on Machine<br />Learning (ICML-12), pages 1367–1374, 2012.<br />Yoshua Bengio and´Eric Thibodeau-Laufer. Deep generative stochastic networks train-<br />able by backprop. arXiv preprint arXiv:1306.1091, 2013.<br />Luc Devroye. Sample-based non-uniform random variate generation. In Proceedings of<br />the 18th conference on Winter simulation, pages 260–265. ACM, 1986.<br />John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online<br />learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–<br />2159, 2010.<br />[DKPR87] Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid<br />monte carlo. Physics letters B, 195(2):216–222, 1987.<br />[GMW13] Karol Gregor, Andriy Mnih, and Daan Wierstra. Deep autoregressive networks. arXiv<br />preprint arXiv:1310.8499, 2013.<br />[HDFN95] Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The” wake-<br />sleep” algorithm for unsupervised neural networks. SCIENCE, pages 1158–1158, 1995.<br />[KRL08]Koray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann LeCun. Fast inference in sparse<br />coding algorithms with applications to object recognition. Technical Report CBLL-<br />TR-2008-12-01, Computational and Biological Learning Lab, Courant Institute, NYU,<br />2008.<br />[Lin89] Ralph Linsker. An application of the principle of maximum information preservation to<br />linear systems. Morgan Kaufmann Publishers Inc., 1989.<br />[Row98]Sam Roweis. EM algorithms for PCA and SPCA. Advances in neural information<br />processing systems, pages 626–632, 1998.<br />[SL10]Ruslan Salakhutdinov and Hugo Larochelle. Efficient learning of deep boltzmann ma-<br />chines. In International Conference on Artificial Intelligence and Statistics, pages 693–<br />700, 2010.<br />[BJP12]<br />[BTL13]<br />[Dev86]<br />[DHS10]<br />9</p>  <p>Page 10</p> <p>486<br />487<br />488<br />489<br />490<br />491<br />492<br />493<br />494<br />495<br />496<br />497<br />498<br />499<br />500<br />501<br />502<br />503<br />504<br />505<br />506<br />507<br />508<br />509<br />510<br />511<br />512<br />513<br />514<br />515<br />516<br />517<br />518<br />519<br />520<br />521<br />522<br />523<br />524<br />525<br />526<br />527<br />528<br />529<br />530<br />531<br />532<br />533<br />534<br />535<br />536<br />537<br />538<br />539<br />[VLL+10] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine<br />Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep<br />network with a local denoising criterion. The Journal of Machine Learning Research,<br />9999:3371–3408, 2010.<br />A Derivation of the alternative estimator<br />Here we give the derivation of the alternative estimator for the example model introduced in the<br />paper.<br />The estimator in our example can be improved upon, in terms of variance, by realizing that the ex-<br />pectation of the second and third term of fθ(.) are tractable and can be solved analytically, since both<br />the variational approximation qθ(z|x) and the prior pθ(z) are Gaussian. Let J be the dimensionality<br />of z. Then in this case:<br />?<br />= −J<br />qθ(z|x)logp(z)dz =<br />?<br />N(z;µ,σ2)logN(z;0,I)dz<br />?<br />2log(2π) −1<br />2<br />J<br />j=1<br />(µ2<br />j+ σ2<br />j)<br />And:<br />−<br />?<br />qθ(z|x)logqθ(z|x)dz = −<br />?<br />N(z;µ,σ2)logN(z;µ,σ2)dz<br />?<br />=J<br />2log(2π) +1<br />2<br />J<br />j=1<br />(1 + logσ2<br />j)<br />The both the variational encoder qθ(z|x) and the prior pθ(z) are Gaussian,<br />?qθ(z|x)logpθ(z)dz and?qθ(z|x)logqθ(z|x)dz are tractable and can be solved analytically.<br />L(θ;x(i)) =<br />?<br />=1<br />2<br />j=1<br />?<br />where<br />?(i,l)∼ N(0,I)<br />so the<br />This results in the following estimator:<br />?<br />=qθ(z|x(i))<br />?<br />?1<br />2<br />j=1<br />qθ(z|x(i))<br />?<br />?<br />logpθ(x(i)|z) + logpθ(z) − logqθ(z|x(i))<br />?<br />dz<br />logpθ(z) − logqθ(z|x(i))<br />?<br />dz +<br />?<br />?<br />p(?)logpθ(x(i)|z)??<br />p(?)logpθ(x(i)|z)??<br />L<br />?<br />z=gθ(?,x(i))d?<br />J<br />?<br />1 + log((σ(i)<br />j)2) − (µ(i)<br />j)2− (σ(i)<br />j)2?<br />j)2?<br />+<br />z=gθ(?,x(i))d?<br />J<br />?<br />1 + log((σ(i)<br />j)2) − (µ(i)<br />j)2− (σ(i)<br />+1<br />L<br />l=1<br />logpθ(x(i)|z(i,l))??<br />z(i,l)=µ(i)+σ(i)??(i,l)<br />Note that pθ(x|z), µ and σ were defined earlier. µ(i)simply denotes the variational mean evaluated<br />at datapoint i, and µ(i)<br />j<br />is simply the j-th element of that vector. The same notation is used for σ.<br />BMarginal likelihood estimator<br />We derived the following marginal likelihood estimator that produces good estimates of the marginal<br />likelihood as long as the dimensionality of the sampled space is low (less then 5 dimensions), and<br />sufficient samples are taken. Let pθ(x,z) = pθ(z)pθ(x|z) be the generative model we are sampling<br />from, and for a given datapoint x(i)we would like to estimate the marginal likelihood pθ(x(i)).<br />The estimation process consists of three stages:<br />10</p>  <p>Page 11</p> <p>540<br />541<br />542<br />543<br />544<br />545<br />546<br />547<br />548<br />549<br />550<br />551<br />552<br />553<br />554<br />555<br />556<br />557<br />558<br />559<br />560<br />561<br />562<br />563<br />564<br />565<br />566<br />567<br />568<br />569<br />570<br />571<br />572<br />573<br />574<br />575<br />576<br />577<br />578<br />579<br />580<br />581<br />582<br />583<br />584<br />585<br />586<br />587<br />588<br />589<br />590<br />591<br />592<br />593<br />1. SampleLvalues{z(l)}fromtheposteriorusinggradient-basedMCMC,e.g. HybridMonte<br />Carlo, using ∇zlogpθ(z|x) = ∇zlogpθ(z) + ∇zlogpθ(x|z).<br />2. Fit a density estimator q(z) to these samples {z(l)}.<br />3. Again, sample L new values from the posterior. Plug these samples, as well as the fitted<br />q(z), into the following estimator:<br />?<br />L<br />l=1<br />pθ(z)pθ(x(i)|z(l))<br />Derivation of the estimator:<br />?q(z)dz<br />?<br />=<br />pθ(z|x(i))<br />pθ(x(i)) ?<br />1<br />L<br />?<br />q(z(l))<br />?−1<br />where<br />z(l)∼ pθ(z|x(i))<br />1<br />pθ(x(i))=pθ(x(i))<br />pθ(x(i),z)<br />pθ(x(i))<br />?<br />?1<br />L<br />l=1<br />=<br />?q(z)pθ(x(i),z)<br />pθ(x(i))<br />q(z)<br />pθ(x(i),z)dz<br />q(z)<br />pθ(x(i),z)dz<br />pθ(x(i),z)dz<br />=<br />L<br />?<br />q(z(l))<br />pθ(z)pθ(x(i)|z(l))<br />where<br />z(l)∼ pθ(z|x(i))<br />CMonte Carlo EM<br />The Monte Carlo EM algorithm does not employ an encoder, instead it samples from the pos-<br />terior of the latent variables using gradients of the posterior computed with ∇zlogpθ(z|x) =<br />∇zlogpθ(z) + ∇zlogpθ(x|z). The Monte Carlo EM procedure consists of 10 HMC leapfrog<br />steps with an automatically tuned stepsize such that the acceptance rate was 90%, followed by 5<br />weight updates steps using the acquired sample. For all algorithms the parameters were updated<br />using the Adagrad stepsizes (with accompanying annealing schedule).<br />The marginal likelihood was estimated with the first 1000 datapoints from the train and test sets,<br />for each datapoint sampling 50 values from the posterior of the latent variables using Hybrid Monte<br />Carlo with 4 leapfrog steps.<br />D Full VB<br />As written in the paper, it is possible to perform variational inference on both the parameters θ and<br />the latent variables z, as opposed to just the latent variables as we did in the paper. Here, we’ll derive<br />our estimator for that case.<br />Let pα(θ) be some hyperprior for the parameters introduced above, parameterized by α. The<br />marginal likelihood can be written as:<br />logpα(X) = DKL(qφ(θ)||pα(θ|X)) + L(φ;X)<br />where the first RHS term denotes a KL divergence of the approximate from the true posterior, and<br />where L(φ;X) denotes the variational lower bound to the marginal likelihood:<br />?<br />Note that this is a lower bound since the KL divergence is non-negative; the bound equals the true<br />marginal when the approximate and true posteriors match exactly. The term logpθ(X) is composed<br />of a sum over the marginal likelihoods of individual datapoints logpθ(X) =?N<br />logpθ(x(i)) = DKL(qφ(z|x(i))||pθ(z|x(i))) + L(θ,φ;x(i))<br />(12)<br />L(φ;X) =qφ(θ)(logpθ(X) + logpα(θ) − logqφ(θ)) dθ<br />(13)<br />i=1logpθ(x(i)),<br />which can each be rewritten as:<br />(14)<br />11</p>  <p>Page 12</p> <p>594<br />595<br />596<br />597<br />598<br />599<br />600<br />601<br />602<br />603<br />604<br />605<br />606<br />607<br />608<br />609<br />610<br />611<br />612<br />613<br />614<br />615<br />616<br />617<br />618<br />619<br />620<br />621<br />622<br />623<br />624<br />625<br />626<br />627<br />628<br />629<br />630<br />631<br />632<br />633<br />634<br />635<br />636<br />637<br />638<br />639<br />640<br />641<br />642<br />643<br />644<br />645<br />646<br />647<br />where again the first RHS term is the KL divergence of the approximate from the true posterior, and<br />L(θ,φ;x) is the variational lower bound of the marginal likelihood of datapoint i:<br />L(θ,φ;x(i)) =<br />?<br />qφ(z|x)<br />?<br />logpθ(x(i)|z) + logpθ(z) − logqφ(z|x)<br />?<br />dz<br />(15)<br />TheexpectationsontheRHSofeqs (13)and(15)canobviouslybewrittenasasumofthreeseparate<br />expectations, of which the second and third component can sometimes be analytically solved, e.g.<br />when both pθ(x) and qφ(z|x) are Gaussian. For generality we will here assume that each of these<br />expectations is intractable.<br />Given the equations above it is easy to construct a Monte Carlo estimator of the lower bound, for<br />example the following:<br />L(φ;X) ?1<br />L<br />L<br />?<br />l=1<br />N<br />?<br />logpθ(x(l)|z(l)) + logpθ(z(l)) − logqφ(z(l))<br />?<br />+ logpα(θ(l)) − logqφ(θ(l))<br />(16)<br />where x(l)∼ X (random datapoints from the dataset) and θ(l)∼ qφ(θ) and z(l)∼ qφ(z|x(l)).<br />We would like to optimize the lower bound L(φ;X) (eq. (13)) w.r.t. φ using stochastic gradient-<br />based optimization. Note that following these gradients would either decrease the KL divergence<br />between the approximate and true posteriors, or increase the marginal likelihood. While the estima-<br />tor above is unbiased, differentiating it w.r.t. the parameters φ will not result in an unbiased gradient:<br />the variational parameters φ indirectly influence the estimate through the samples z(l)∼ qφ(z|x),<br />and it is impossible to differentiate through this sampling process.<br />D.1Our estimator of the lower bound<br />Under certain mild conditions outlined in section (see paper) for chosen approximate posteriors<br />qφ(θ) and qφ(z|x) we can reparameterize its conditional samples ? z ∼ qφ(z|x) as<br />where we choose a prior p(?) and a function gφ(?,x) such that the following holds:<br />?<br />=p(?) logpθ(x(i)|z) + logpθ(z) − logqφ(z|x)<br />? z = gφ(?,x)<br />?<br />with<br />? ∼ p(?)<br />(17)<br />L(θ,φ;x(i)) =qφ(z|x) logpθ(x(i)|z) + logpθ(z) − logqφ(z|x)<br />?<br />dz<br />?<br />??????<br />z=gφ(?,x(i))<br />d?<br />(18)<br />The same can be done for the approximate posterior qφ(θ):<br />?θ = hφ(ζ)<br />with<br />ζ ∼ p(ζ)<br />(19)<br />where we, similarly as above, choose a prior p(ζ) and a function hφ(ζ) such that the following<br />holds:<br />?<br />=p(ζ)(logpθ(X) + logpα(θ) − logqφ(θ))<br />L(φ;X) =qφ(θ)(logpθ(X) + logpα(θ) − logqφ(θ)) dθ<br />?<br />????<br />θ=hφ(ζ)<br />dζ<br />(20)<br />For notational conciseness we introduce a shorthand notation fφ(x,z,θ):<br />fφ(x,z,θ) = N · (logpθ(x|z) + logpθ(z) − logqφ(z|x)) + logpα(θ) − logqφ(θ)<br />Using equations (20) and (18), the Monte Carlo estimate of the variational lower bound, given<br />datapoint x(i), is:<br />(21)<br />L(φ;X) ?1<br />L<br />L<br />?<br />l=1<br />fφ(x(l),gφ(?(l),x(l)),hφ(ζ(l)))<br />(22)<br />12</p>  <p>Page 13</p> <p>648<br />649<br />650<br />651<br />652<br />653<br />654<br />655<br />656<br />657<br />658<br />659<br />660<br />661<br />662<br />663<br />664<br />665<br />666<br />667<br />668<br />669<br />670<br />671<br />672<br />673<br />674<br />675<br />676<br />677<br />678<br />679<br />680<br />681<br />682<br />683<br />684<br />685<br />686<br />687<br />688<br />689<br />690<br />691<br />692<br />693<br />694<br />695<br />696<br />697<br />698<br />699<br />700<br />701<br />Algorithm 2 Pseudocode for computing a stochastic gradient using our estimator. See section D.1<br />for meaning of the functions fφ, gφand hφ.<br />Require: φ (Current value of variational parameters)<br />g ← 0<br />for l is 1 to L do<br />x ← Random draw from dataset X<br />? ← Random draw from prior p(?)<br />ζ ← Random draw from prior p(ζ)<br />g ← g +1<br />end for<br />return g<br />L∇φfφ(x,gφ(?,x),hφ(ζ))<br />where ?(l)∼ p(?) and ζ(l)∼ p(ζ). The estimator only depends on samples from p(?) and p(ζ)<br />which are obviously not influenced by φ, therefore the estimator can be differentiated w.r.t. φ.<br />The resulting stochastic gradients can be used in conjunction with stochastic optimization methods<br />such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to computing stochastic<br />gradients.<br />D.2Example<br />D.2.1Gaussian priors and variational approximations<br />While there is much freedom in the choice of qφ(z|x) and qφ(θ), we’ll for a moment assume<br />a relatively simple case. Let the prior over the parameters and latent variables be the centered<br />isotropic Gaussian pα(θ) = N(z;0,I) and pθ(z) = N(z;0,I). Note that in this case, the prior<br />lacks parameters. Let’s also assume that the true posteriors are approximatily Gaussian with an<br />approximately diagonal covariance. In this case, we can let the variational approximate posteriors<br />be multivariate Gaussians with a diagonal covariance structure:<br />logqφ(θ) = logN(θ;µθ,σ2<br />logqφ(z|x) = logN(z;µz,σ2<br />where µzand σzare yet unspecified functions of x. Since they are Gaussian, we can parameterize<br />the variational approximate posteriors:<br />θI)<br />zI)<br />(23)<br />qφ(θ)<br />qφ(z|x)<br />as<br />as<br />?θ = µθ+ σθ? ζ<br />where<br />where<br />ζ ∼ N(0,I)<br />? ∼ N(0,I)<br />? z = µz+ σz? ?<br />With ? we signify an element-wise product. These can be plugged into the lower bound defined<br />above (eqs (21) and (22)).<br />In this case it is possible to construct an alternative estimator with a lower variance, since in this<br />model pα(θ), pθ(z), qφ(θ) and qφ(z|x) are Gaussian, and therefore four terms of fφcan be solved<br />analytically. The resulting estimator is:<br /><br />2<br />j=1<br />L(φ;X) ?1<br />L<br />L<br />?<br />J<br />?<br />l=1<br />N ·<br />1<br />J<br />?<br />?<br />1 + log((σ(l)<br />z,j)2) − (µ(l)<br />z,j)2− (σ(l)<br />z,j)2?<br />+ logpθ(x(i)z(i))<br /><br /><br />+1<br />2<br />j=1<br />?<br />1 + log((σ(l)<br />θ,j)2) − (µ(l)<br />θ,j)2− (σ(l)<br />θ,j)2?<br />(24)<br />µ(i)<br />j<br />and σ(i)<br />j<br />simply denote the j-th element of vectors µ(i)and σ(i).<br />D.2.2Example with auto-encoder<br />In the example above, we still left unspecified the generative PDF pθ(x|z) (the decoder), as well as<br />the mean µzand s.d. σzof the variational approximation qφ(z|x) (the encoder). Although we can<br />choose any differentiable PDF for both, let, for example, the probabilistic decoder be a multivariate<br />13</p>  <p>Page 14</p> <p>702<br />703<br />704<br />705<br />706<br />707<br />708<br />709<br />710<br />711<br />712<br />713<br />714<br />715<br />716<br />717<br />718<br />719<br />720<br />721<br />722<br />723<br />724<br />725<br />726<br />727<br />728<br />729<br />730<br />731<br />732<br />733<br />734<br />735<br />736<br />737<br />738<br />739<br />740<br />741<br />742<br />743<br />744<br />745<br />746<br />747<br />748<br />749<br />750<br />751<br />752<br />753<br />754<br />755<br />Bernoulli whose probabilities are computed from z with a fully-connected neural network with a<br />single hidden layer:<br />logpθ(x|z) =<br />D<br />?<br />i=1<br />xilogyi− (1 − xi) · log(1 − yi)<br />where y = exp(W2tanh(W1z + b1) + b2)<br />(25)<br />Let the Gaussian mean and covariance of the encoder be a function of x, using a similar neural<br />network:<br />logqφ(z|x) = logN(z;µ,σ2I)<br />where µ = W4h + b4<br />logσ2= W5h + b5<br />h = tanh(W3x + b3)<br />(26)<br />Note that the generative parameters are θ = {Wj,bj}2<br />φθ∪φzwhere φθ= {µθ,σθ} and φz= {Wj,bj}5<br />eq. 22 or eq. 24 (for a lower variance), and the lower bound can subsequently can be differentiated<br />and optimized w.r.t. the parameters.<br />j=1. The variational parameters are φ =<br />j=3. These definitions above can be plugged in<br />14</p>   </div> <div id="rgw19_56ab1f18415d8" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw20_56ab1f18415d8">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw21_56ab1f18415d8"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://export.arxiv.org/pdf/1312.6114" target="_blank" rel="nofollow" class="publication-viewer" title="Auto-Encoding Variational Bayes">Auto-Encoding Variational Bayes</a> </div>  <div class="details">   Available from <a href="http://export.arxiv.org/pdf/1312.6114" target="_blank" rel="nofollow">export.arxiv.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw28_56ab1f18415d8" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (49) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw29_56ab1f18415d8" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw30_56ab1f18415d8" >  <div class="indent-left">  <div id="rgw31_56ab1f18415d8" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/284219020_Generating_Sentences_from_a_Continuous_Space">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1511.06349" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: de.arxiv.org </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw32_56ab1f18415d8">  <li class="citation-context-item"> "Naively, maximum likelihood learning in such a model presents an intractable inference problem over the latent variable. Drawing inspiration from recent successes in modeling images (Gregor et al., 2015), handwriting, and natural speech (Chung et al., 2015), our model circumvents these difficulties using the architecture of a variational autoencoder and takes advantage of recent advances in variational inference (Kingma &amp; Welling, 2015; Rezende et al., * First two authors contributed equally. Work was done when all authors were at Google, Inc. 1 arXiv:1511.06349v1 " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/284219020_Generating_Sentences_from_a_Continuous_Space"> <span class="publication-title js-publication-title">Generating Sentences from a Continuous Space</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2049611927_Samuel_R_Bowman" class="authors js-author-name ga-publications-authors">Samuel R. Bowman</a> &middot;     <a href="researcher/2057227262_Luke_Vilnis" class="authors js-author-name ga-publications-authors">Luke Vilnis</a> &middot;     <a href="researcher/69685462_Oriol_Vinyals" class="authors js-author-name ga-publications-authors">Oriol Vinyals</a> &middot;     <a href="researcher/2078879465_Andrew_M_Dai" class="authors js-author-name ga-publications-authors">Andrew M. Dai</a> &middot;     <a href="researcher/2085202363_Rafal_Jozefowicz" class="authors js-author-name ga-publications-authors">Rafal Jozefowicz</a> &middot;     <a href="researcher/67738097_Samy_Bengio" class="authors js-author-name ga-publications-authors">Samy Bengio</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> The standard unsupervised recurrent neural network language model (RNNLM)
generates sentences one word at a time and does not work from an explicit
global distributed sentence representation. In this work, we present an
RNN-based variational autoencoder language model that incorporates distributed
latent representations of entire sentences. This factorization allows it to
explicitly model holistic properties of sentences such as style, topic, and
high-level syntactic features. Samples from the prior over these sentence
representations remarkably produce diverse and well-formed sentences through
simple deterministic decoding. By examining paths through this latent space, we
are able to generate coherent novel sentences that interpolate between known
sentences. We present techniques for solving the difficult learning problem
presented by this model, demonstrate strong performance in the imputation of
missing tokens, and explore many interesting properties of the latent sentence
space. </span> </div>    <div class="publication-meta publication-meta">  <span class="ico-publication-preview reset-background"></span> Preview    &middot; Article &middot; Nov 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw33_56ab1f18415d8" >  <div class="indent-left">  <div id="rgw34_56ab1f18415d8" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/284476380_Variational_Auto-encoded_Deep_Gaussian_Processes">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Andreas_Damianou" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Andreas Damianou </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw35_56ab1f18415d8">  <li class="citation-context-item"> "The main challenge is that exact inference on directed nonlinear probabilistic models is typically intractable due to the required marginalisation of the latent components. This has lead to the development of probabilistic generative models based on neural networks (Kingma &amp; Welling, 2013; Mnih &amp; Gregor, 2014; Rezende et al., 2014), in which probabilistic distributions are defined for the input and output of individual layers. Efficient approximated inference methods have been developed in this context based on stochastic variational inference or stochastic back-propagation. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/284476380_Variational_Auto-encoded_Deep_Gaussian_Processes"> <span class="publication-title js-publication-title">Variational Auto-encoded Deep Gaussian Processes</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2054900589_Zhenwen_Dai" class="authors js-author-name ga-publications-authors">Zhenwen Dai</a> &middot;     <a href="researcher/59288229_Andreas_Damianou" class="authors js-author-name ga-publications-authors">Andreas Damianou</a> &middot;     <a href="researcher/21048246_Javier_Gonzalez" class="authors js-author-name ga-publications-authors">Javier González</a> &middot;     <a href="researcher/39663468_Neil_Lawrence" class="authors js-author-name ga-publications-authors">Neil Lawrence</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> We develop a scalable deep non-parametric generative model by augmenting deep
Gaussian processes with a recognition model. Inference is performed in a novel
scalable variational framework where the variational posterior distributions
are reparametrized through a multilayer perceptron. The key aspect of this
reformulation is that it prevents the proliferation of variational parameters
which otherwise grow linearly in proportion to the sample size. We derive a new
formulation of the variational lower bound that allows us to distribute most of
the computation in a way that enables to handle datasets of the size of
mainstream deep learning tasks. We show the efficacy of the method on a variety
of challenges including deep unsupervised learning and deep Bayesian
optimization. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Nov 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Andreas_Damianou/publication/284476380_Variational_Auto-encoded_Deep_Gaussian_Processes/links/567c62e308ae19758384e03a.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw36_56ab1f18415d8" >  <div class="indent-left">  <div id="rgw37_56ab1f18415d8" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/284219537_Adversarial_Autoencoders">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1511.05644" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: de.arxiv.org </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw38_56ab1f18415d8">  <li class="citation-context-item"> "In this case, the stochasticity in q(z) comes from both the data-distribution and the randomness of the Gaussian distribution at the output of the encoder. We can use the same re-parametrization trick of (Kingma &amp; Welling, 2014) for back-propagation through the encoder network. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/284219537_Adversarial_Autoencoders"> <span class="publication-title js-publication-title">Adversarial Autoencoders</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2045971008_Alireza_Makhzani" class="authors js-author-name ga-publications-authors">Alireza Makhzani</a> &middot;     <a href="researcher/2061288721_Jonathon_Shlens" class="authors js-author-name ga-publications-authors">Jonathon Shlens</a> &middot;     <a href="researcher/2085220925_Navdeep_Jaitly" class="authors js-author-name ga-publications-authors">Navdeep Jaitly</a> &middot;     <a href="researcher/2061320296_Ian_Goodfellow" class="authors js-author-name ga-publications-authors">Ian Goodfellow</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> In this paper we propose a new method for regularizing autoencoders by
imposing an arbitrary prior on the latent representation of the autoencoder.
Our method, named &quot;adversarial autoencoder&quot;, uses the recently proposed
generative adversarial networks (GAN) in order to match the aggregated
posterior of the hidden code vector of the autoencoder with an arbitrary prior.
Matching the aggregated posterior to the prior ensures that there are no
&quot;holes&quot; in the prior, and generating from any part of prior space results in
meaningful samples. As a result, the decoder of the adversarial autoencoder
learns a deep generative model that maps the imposed prior to the data
distribution. We show how adversarial autoencoders can be used to disentangle
style and content of images and achieve competitive generative performance on
MNIST, Street View House Numbers and Toronto Face datasets. </span> </div>    <div class="publication-meta publication-meta">  <span class="ico-publication-preview reset-background"></span> Preview    &middot; Article &middot; Nov 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw23_56ab1f18415d8" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw24_56ab1f18415d8">  </ul> </div> </div>   <div id="rgw15_56ab1f18415d8" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw16_56ab1f18415d8"> <div> <h5> <a href="publication/277144892_Global_convergence_rate_analysis_of_unconstrained_optimization_methods_based_on_probabilistic_models" class="color-inherit ga-similar-publication-title"><span class="publication-title">Global convergence rate analysis of unconstrained optimization methods based on probabilistic models</span></a>  </h5>  <div class="authors"> <a href="researcher/70110435_Coralia_Cartis" class="authors ga-similar-publication-author">Coralia Cartis</a>, <a href="researcher/2074408136_Katya_Scheinberg" class="authors ga-similar-publication-author">Katya Scheinberg</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56ab1f18415d8"> <div> <h5> <a href="publication/276845965_Population_model-based_optimization" class="color-inherit ga-similar-publication-title"><span class="publication-title">Population model-based optimization</span></a>  </h5>  <div class="authors"> <a href="researcher/2047669094_Xi_Chen" class="authors ga-similar-publication-author">Xi Chen</a>, <a href="researcher/74910194_Enlu_Zhou" class="authors ga-similar-publication-author">Enlu Zhou</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56ab1f18415d8"> <div> <h5> <a href="publication/282050524_Dynamic_optimization_of_chemical_engineering_problems_using_affinity_propagation_based_estimation_of_distribution_algorithm" class="color-inherit ga-similar-publication-title"><span class="publication-title">Dynamic optimization of chemical engineering problems using affinity propagation based estimation of distribution algorithm</span></a>  </h5>  <div class="authors"> <a href="researcher/2024321173_Na_Luo" class="authors ga-similar-publication-author">Na Luo</a>, <a href="researcher/2081485005_Wei_Feng" class="authors ga-similar-publication-author">Wei Feng</a>, <a href="researcher/2086959026_Xiaoqiang_Wang" class="authors ga-similar-publication-author">Xiaoqiang Wang</a>, <a href="researcher/9578814_Feng_Qian" class="authors ga-similar-publication-author">Feng Qian</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw40_56ab1f18415d8" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw41_56ab1f18415d8">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw42_56ab1f18415d8" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=PDEvhRJe2MznX_M2cFMDVtB2lA9_4YZjC9LxPlNouSYlZs2nUO3kdQQ74UEm3Jjj" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="Ghu2GKqZHqxGGqZSSW+Sth7PrEV2eNex7qAyGjnnngK4TC4ZM8/aV0fQvyWO6jySywzATjeKPSUxaJWObu3fgdl3z2odpjOjOqr2CnLFEnP5H3gW459NHYnCPwhKqYzV0I5dC0HjSsJIhxLCK6IZfAm+zdSj6QlRlrLZx4QSgH/DO4+qN8DwUMy4UiTiLpzFQTXDZ1KCB/bC8y/Ig/4+7js9zDf/2WJTd+599z31rnJyA5ofA2dQoPZt9CMGF7K5P2UeDsGmJiVZHndpGCcQ8qRSN775Kz4z49LMsB5Ur+A="/> <input type="hidden" name="urlAfterLogin" value="publication/259400035_Auto-Encoding_Variational_Bayes"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjU5NDAwMDM1X0F1dG8tRW5jb2RpbmdfVmFyaWF0aW9uYWxfQmF5ZXM%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjU5NDAwMDM1X0F1dG8tRW5jb2RpbmdfVmFyaWF0aW9uYWxfQmF5ZXM%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjU5NDAwMDM1X0F1dG8tRW5jb2RpbmdfVmFyaWF0aW9uYWxfQmF5ZXM%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw43_56ab1f18415d8"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 1179;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"profileUrl":"researcher\/2040421796_Diederik_P_Kingma","fullname":"Diederik P. Kingma","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2549355721578\/images\/template\/default\/profile\/profile_default_m.png","profileStats":[null,{"data":{"publicationCount":6,"widgetId":"rgw5_56ab1f18415d8"},"id":"rgw5_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorPublicationCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorPublicationCount.html?authorUid=2040421796","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"followerCount":1,"widgetId":"rgw6_56ab1f18415d8"},"id":"rgw6_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorFollowerCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorFollowerCount.html?authorUid=2040421796","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw4_56ab1f18415d8"},"id":"rgw4_56ab1f18415d8","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorBadge.html?authorUid=2040421796","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw3_56ab1f18415d8"},"id":"rgw3_56ab1f18415d8","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=259400035","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":259400035,"title":"Auto-Encoding Variational Bayes","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"ICLR","publicationDate":"12\/2013;","publicationDateRobot":"2013-12","article":""}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1312.6114","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Auto-Encoding Variational Bayes"},{"key":"rft.title","value":"ICLR"},{"key":"rft.jtitle","value":"ICLR"},{"key":"rft.date","value":"2013"},{"key":"rft.au","value":"Diederik P Kingma,Max Welling"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw8_56ab1f18415d8"},"id":"rgw8_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=259400035","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":259400035,"peopleItems":[{"data":{"authorUrl":"researcher\/2040421796_Diederik_P_Kingma","authorNameOnPublication":"Diederik P Kingma","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Diederik P Kingma","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2040421796_Diederik_P_Kingma","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw11_56ab1f18415d8"},"id":"rgw11_56ab1f18415d8","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2040421796&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw10_56ab1f18415d8"},"id":"rgw10_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2040421796&authorNameOnPublication=Diederik%20P%20Kingma","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/69847505_Max_Welling","authorNameOnPublication":"Max Welling","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Max Welling","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/69847505_Max_Welling","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw13_56ab1f18415d8"},"id":"rgw13_56ab1f18415d8","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=69847505&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw12_56ab1f18415d8"},"id":"rgw12_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=69847505&authorNameOnPublication=Max%20Welling","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw9_56ab1f18415d8"},"id":"rgw9_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=259400035&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":259400035,"abstract":"<noscript><\/noscript><div>Can we efficiently learn the parameters of directed probabilistic models, in<br \/>\nthe presence of continuous latent variables with intractable posterior<br \/>\ndistributions, and in case of large datasets? We introduce a novel learning and<br \/>\napproximate inference method that works efficiently, under some mild<br \/>\nconditions, even in the on-line and intractable case. The method involves<br \/>\noptimization of a stochastic objective function that can be straightforwardly<br \/>\noptimized w.r.t. all parameters, using standard gradient-based optimization<br \/>\nmethods.<br \/>\nThe method does not require the typically expensive sampling loops per<br \/>\ndatapoint required for Monte Carlo EM, and all parameter updates correspond to<br \/>\noptimization of the variational lower bound of the marginal likelihood, unlike<br \/>\nthe wake-sleep algorithm. These theoretical advantages are reflected in<br \/>\nexperimental results.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw14_56ab1f18415d8"},"id":"rgw14_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=259400035","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/259400035_Auto-Encoding_Variational_Bayes\/links\/02d4e0f60cf2df62f0a56786\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw7_56ab1f18415d8"},"id":"rgw7_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=259400035&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":70110435,"url":"researcher\/70110435_Coralia_Cartis","fullname":"Coralia Cartis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2074408136,"url":"researcher\/2074408136_Katya_Scheinberg","fullname":"Katya Scheinberg","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"May 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/277144892_Global_convergence_rate_analysis_of_unconstrained_optimization_methods_based_on_probabilistic_models","usePlainButton":true,"publicationUid":277144892,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/277144892_Global_convergence_rate_analysis_of_unconstrained_optimization_methods_based_on_probabilistic_models","title":"Global convergence rate analysis of unconstrained optimization methods based on probabilistic models","displayTitleAsLink":true,"authors":[{"id":70110435,"url":"researcher\/70110435_Coralia_Cartis","fullname":"Coralia Cartis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2074408136,"url":"researcher\/2074408136_Katya_Scheinberg","fullname":"Katya Scheinberg","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/277144892_Global_convergence_rate_analysis_of_unconstrained_optimization_methods_based_on_probabilistic_models","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/277144892_Global_convergence_rate_analysis_of_unconstrained_optimization_methods_based_on_probabilistic_models\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56ab1f18415d8"},"id":"rgw16_56ab1f18415d8","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=277144892","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2047669094,"url":"researcher\/2047669094_Xi_Chen","fullname":"Xi Chen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":74910194,"url":"researcher\/74910194_Enlu_Zhou","fullname":"Enlu Zhou","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Mar 2015","journal":"Journal of Global Optimization","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/276845965_Population_model-based_optimization","usePlainButton":true,"publicationUid":276845965,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.29","url":"publication\/276845965_Population_model-based_optimization","title":"Population model-based optimization","displayTitleAsLink":true,"authors":[{"id":2047669094,"url":"researcher\/2047669094_Xi_Chen","fullname":"Xi Chen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":74910194,"url":"researcher\/74910194_Enlu_Zhou","fullname":"Enlu Zhou","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of Global Optimization 03\/2015; 63(1). DOI:10.1007\/s10898-015-0288-1"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/276845965_Population_model-based_optimization","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/276845965_Population_model-based_optimization\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab1f18415d8"},"id":"rgw17_56ab1f18415d8","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=276845965","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2024321173,"url":"researcher\/2024321173_Na_Luo","fullname":"Na Luo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2081485005,"url":"researcher\/2081485005_Wei_Feng","fullname":"Wei Feng","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2086959026,"url":"researcher\/2086959026_Xiaoqiang_Wang","fullname":"Xiaoqiang Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9578814,"url":"researcher\/9578814_Feng_Qian","fullname":"Feng Qian","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Mar 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/282050524_Dynamic_optimization_of_chemical_engineering_problems_using_affinity_propagation_based_estimation_of_distribution_algorithm","usePlainButton":true,"publicationUid":282050524,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/282050524_Dynamic_optimization_of_chemical_engineering_problems_using_affinity_propagation_based_estimation_of_distribution_algorithm","title":"Dynamic optimization of chemical engineering problems using affinity propagation based estimation of distribution algorithm","displayTitleAsLink":true,"authors":[{"id":2024321173,"url":"researcher\/2024321173_Na_Luo","fullname":"Na Luo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2081485005,"url":"researcher\/2081485005_Wei_Feng","fullname":"Wei Feng","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2086959026,"url":"researcher\/2086959026_Xiaoqiang_Wang","fullname":"Xiaoqiang Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9578814,"url":"researcher\/9578814_Feng_Qian","fullname":"Feng Qian","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/282050524_Dynamic_optimization_of_chemical_engineering_problems_using_affinity_propagation_based_estimation_of_distribution_algorithm","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/282050524_Dynamic_optimization_of_chemical_engineering_problems_using_affinity_propagation_based_estimation_of_distribution_algorithm\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab1f18415d8"},"id":"rgw18_56ab1f18415d8","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=282050524","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw15_56ab1f18415d8"},"id":"rgw15_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=259400035&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":259400035,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":259400035,"publicationType":"article","linkId":"02d4e0f60cf2df62f0a56786","fileName":"Auto-Encoding Variational Bayes","fileUrl":"http:\/\/export.arxiv.org\/pdf\/1312.6114","name":"export.arxiv.org","nameUrl":"http:\/\/export.arxiv.org\/pdf\/1312.6114","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw21_56ab1f18415d8"},"id":"rgw21_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=259400035&linkId=02d4e0f60cf2df62f0a56786&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw20_56ab1f18415d8"},"id":"rgw20_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=259400035&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":6,"valueFormatted":"6","widgetId":"rgw22_56ab1f18415d8"},"id":"rgw22_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=259400035","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw19_56ab1f18415d8"},"id":"rgw19_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=259400035&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":259400035,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw24_56ab1f18415d8"},"id":"rgw24_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=259400035&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":6,"valueFormatted":"6","widgetId":"rgw25_56ab1f18415d8"},"id":"rgw25_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=259400035","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw23_56ab1f18415d8"},"id":"rgw23_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=259400035&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"000\n001\n002\n003\n004\n005\n006\n007\n008\n009\n010\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n050\n051\n052\n053\nAuto-Encoding Variational Bayes\nAnonymous Author(s)\nAffiliation\nAddress\nemail\nAbstract\nCan we efficiently learn the parameters of directed probabilistic models, in the\npresence of continuous latent variables with intractable posterior distributions?\nWe introduce an unsupervised on-line learning algorithm that efficiently optimizes\nthe variational lower bound on the marginal likelihood and that, under some mild\nconditions, even works in the intractable case. The algorithm optimizes a proba-\nbilistic encoder (also called a recognition network) to approximate the intractable\nposterior distribution of the latent variables. The crucial element is a reparame-\nterization of the variational bound with an independent noise variable, yielding\na stochastic objective function which can be jointly optimized w.r.t. variational\nand generative parameters using standard gradient-based stochastic optimization\nmethods. Theoretical advantages are reflected in experimental results.\n1 Introduction\nHow to efficiently learn the parameters of directed probabilistic models whose continuous latent\nvariables have intractable posterior distributions? The variational approach to approximate Bayesian\ninference involves the introduction of an approximate posterior to the intractable posterior, used\nto maximize the variational lower bound on the marginal likelihood. Unfortunately, the common\nmean-field approach requires analytical solutions to expectations w.r.t. the approximate posterior,\nwhich are also intractable in the general case. We show how for continuous latent variables, a\nreparameterization of the expectation w.r.t. the approximate posterior yields a novel and practical\nestimator of the variational lower bound that can be differentiated and jointly optimized w.r.t. all\nparameters, i.e. both the variational parameters and regular parameters, using standard stochastic\ngradient ascent techniques.\nThe objective contains, in addition to regularization terms dictated by the variational bound, a noisy\ndata reconstruction term, exposing a novel connection between auto-encoders and stochastic vari-\national inference. In contrast to a typical objective for auto-encoders [BCV13], all parameters up-\ndates, including those of the noise distribution, correspond to optimization of the variational lower\nbound on the marginal likelihood. From the learned generative model it is straightforward to gener-\nate samples, without the typical requirement of running Markov chains. The probabilistic encoder\ncan be used for fast approximate inference of latent variables, i.e. for recognition, representation\nor visualization purposes. Furthermore, the lower bound estimator can be used for unsupervised\ninference tasks such as denoising and inpainting.\n2 Method\nThe strategy in the following section can be used to derive a lower bound estimator (a stochastic\nobjective function) for a variety of directed graphical models with continuous latent variables. We\nwill restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables\nper datapoint, and where we like to perform ML or MAP inference on the (global) parameters, and\n1\narXiv:1312.6114v5  [stat.ML]  9 Jan 2014"},{"page":2,"text":"054\n055\n056\n057\n058\n059\n060\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n100\n101\n102\n103\n104\n105\n106\n107\nx\nz\n\u03c6\n\u03b8\nN\nFigure1: Thetypeofdirectedgraphicalmodelunderconsideration. Solidlinesdenotethegenerative\nmodel p\u03b8(z)p\u03b8(x|z), dashed lines denote the variational approximation q\u03c6(z|x) to the intractable\nposterior p\u03b8(z|x). The variational parameters \u03c6 are learned jointly with the generative model pa-\nrameters \u03b8.\nvariationalinferenceonthelatentvariables. Itis, forexample, straightforwardtoextendthisscenario\nto the case where we also perform variational inference on the global parameters; that algorithm is\nput in the appendix, but experiments with that case are left to future work. Note that our method can\nbe applied to online, non-stationary settings, e.g. streaming data, but here we assume a fixed dataset\nfor simplicity.\n2.1Problem scenario\nLet us consider some dataset X = {x(i)}N\nor discrete variable x. We assume that the data are generated by some random process, involving\nan unobserved continuous random variable z. The process consists of two steps: (1) a value z(i)\nis generated from some prior distribution p\u03b8\u2217(z); (2) a value x(i)is generated from some condi-\ntional distribution p\u03b8\u2217(x|z). We assume that the prior p\u03b8\u2217(z) and likelihood p\u03b8\u2217(x|z) come from\nparametric families of distributions p\u03b8(z) and p\u03b8(x|z), and that their PDFs are differentiable almost\neverywhere w.r.t. both \u03b8 and z. Unfortunately, a lot of this process is hidden from our view: the true\nparameters \u03b8\u2217as well as the values of the latent variables z(i)are unknown to us.\nVery importantly, we do not make the usual simplifying assumptions common in the literature.\nConversely, we are here interested in a general algorithm that even works in the case of:\ni=1consisting of N i.i.d. samples of some continuous\n1. Intractability:\n?p\u03b8(z)p\u03b8(x|z)dz is intractable (so we cannot evaluate or differentiate the marginal like-\nthe EM algorithm cannot be used), and where the required integrals for any reasonable\nmean-field Variational Bayes are also intractable. These intractabilities are quite common\nand already appear in case of moderately complicated likelihood functions p\u03b8(x|z), e.g. a\nneural network with a nonlinear hidden layer.\n2. A large dataset: we have so much data that batch optimization is too costly; we would like\nto make parameter updates using small minibatches or even single datapoints. Sampling-\nbased solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a\ntypically expensive sampling loop per datapoint.\nthe case where the integral of the marginal likelihood p\u03b8(x)=\nlihood), where the true posterior density p\u03b8(z|x) = p\u03b8(x|z)p\u03b8(z)\/p\u03b8(x) is intractable (so\nWe are interested in, and propose a solution to, three related problems in the above scenario:\n1. Efficient approximate maximum likelihood (ML) or maximum a posteriori (MAP) estima-\ntion for the parameters \u03b8. The parameters can be of interest themselves, e.g. if we are\nanalyzing some natural process. They also allow us to mimic the hidden random process\nand generate artificial data that resembles the real data.\n2. Efficient approximate posterior inference of the latent variable z given an observed value x\nfor a choice of parameters \u03b8. This is useful for coding or data representation tasks.\n3. Efficient approximate marginal inference of the variable x. This allows us to perform all\nkindsof inferencetaskswherea prioroverx isrequired. Common applicationsincomputer\nvision include image denoising, inpainting and super-resolution.\n2"},{"page":3,"text":"108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\nFor the purpose of solving the above problems, let us introduce the parametric variational approx-\nimation q\u03c6(z|x): an approximation to the intractable true posterior p\u03b8(z|x). Note that in contrast\nwith the approximate posterior in mean-field variational inference, it is not necesarilly factorial and\nits parameters are not computed from some closed-form expectation. Instead, its parameters \u03c6 are\nlearned jointly with the parameters of the generative model.\nFrom a coding theory perspective, the unobserved variables z have an interpretation as a latent\nrepresentationorcode. Inthispaperwewillthereforealsorefertoq\u03c6(z|x)asa(variational)encoder\nor recognition model, since given a datapoint x it produces a distribution (e.g. a Gaussian) over the\npossible valuesof thecodezfrom whichthe datapointx could havebeen generated. Ina similar vein\nwe will refer to p\u03b8(x|z) as a (generative) decoder, since given a code z it produces a distribution\nover the possible corresponding values of x.\n2.2The variational bound\nThe marginal likelihood is composed of a sum over the marginal likelihoods of individual datapoints\nlogp\u03b8(x(1),\u00b7\u00b7\u00b7 ,x(N)) =?N\nlogp\u03b8(x(i)) = DKL(q\u03c6(z|x(i))||p\u03b8(z|x(i))) + L(\u03b8,\u03c6;x(i))\nThe first RHS term is the KL divergence of the approximate from the true posterior, which is non-\nnegative. The second RHS term L(\u03b8,\u03c6;x(i)) denotes the variational lower bound on the marginal\nlikelihood of datapoint i:\n?\nNote that the bound equals the true marginal when the divergence of the approximate from true\nposterior distribution is zero.\nThe expectation on the RHS of eq. (2) can obviously be written as a sum of three separate expec-\ntations, of which the second and third component can sometimes be analytically solved, e.g. when\nboth p\u03b8(x) and q\u03c6(z|x) are Gaussian. For generality we will here assume that each of these expec-\ntations are intractable.\nWe would like to optimize the lower bound L(\u03b8,\u03c6;x(i)) (eq. (2)) using stochastic gradients. Note\nthat following these gradients would either decrease the KL divergence between the approximate\nand true posterior distributions, or increase the marginal likelihood, or both. A na\u00a8 \u0131ve attempt to\ncompute a stochastic gradient would be to draw samples {z(l)}L\nthe following Monte Carlo estimate of the lower bound:\n?\nWhile the above expression is an unbiased estimator of the marginal likelihood (i.e. it will equal\nthe lower bound in the limit L \u2192 \u221e), differentiating it w.r.t. the parameters \u03c6 will not result in an\nunbiasedgradient: thevariationalparameters\u03c6indirectlyinfluencetheestimatethroughthesamples\nz(l)\u223c q\u03c6(z|x), and it is impossible to differentiate through this sampling process. Existing work\non stochastic variational bayes provide workarounds [BJP12], but not a solution to this problem.\ni=1logp\u03b8(x(i)), which can each be rewritten as:\n(1)\nlogp\u03b8(x(i)) \u2265 L(\u03b8,\u03c6;x(i)) =q\u03c6(z|x)\n?\nlogp\u03b8(x(i)|z) + logp\u03b8(z) \u2212 logq\u03c6(z|x)\n?\ndz\n(2)\nl=1from q\u03c6and then differentiate\nL(\u03b8,\u03c6;x(i)) ?1\nL\nL\n?\nl=1\nlogp\u03b8(x(i)|z(l)) + logp\u03b8(z(l)) \u2212 logq\u03c6(z(l)|x(i))\n?\nwhere\nz(l)\u223c q\u03c6(z|x)\n2.3 Our estimator of the lower bound\nUnder certain mild conditions outlined in section 2.4 for a chosen approximate posterior q\u03c6(z|x)\nwe can reparameterize its conditional samples ? z \u223c q\u03c6(z|x) as\nwhere we choose a prior p(?) and a function g\u03c6(?,x) such that the following holds:\n?\n=p(?)logp\u03b8(x(i)|z) + logp\u03b8(z) \u2212 logq\u03c6(z|x)\n? z = g\u03c6(?,x)\nq\u03c6(z|x)\n?\nwith\n? \u223c p(?)\n(3)\nL(\u03b8,\u03c6;x(i)) =\n?\nlogp\u03b8(x(i)|z) + logp\u03b8(z) \u2212 logq\u03c6(z|x)\n?\ndz\n?\n?????\nz=g\u03c6(?,x(i))\nd?\n(4)\n3"},{"page":4,"text":"162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\nAlgorithm 1 Pseudocode for computing a stochastic gradient using our estimator. See section 2.3\nfor meaning of the functions f\u03b8,\u03c6and g\u03c6. The minibatch XM= {x(i)}M\nsubset of the full dataset X. We use settings M = 100 and L = 1 in experiments.\nRequire: \u03b8,\u03c6 (Current value of parameters)\ng \u2190 0\nXM\u2190 Random subset (minibatch) of M datapoints from dataset\nfor each x \u2208 XMdo\nfor l is 1 to L do\n? \u2190 Random sample from p(?)\ng \u2190 g + \u2207\u03b8,\u03c6f\u03b8,\u03c6(x,g\u03c6(?,x))\nend for\nend for\nreturn (N\/(M \u00b7 L)) \u00b7 g\ni=1is a randomly drawn\nFor notational conciseness we introduce a shorthand notation f\u03b8,\u03c6(x,z) for the sum of three PDFs:\nf\u03b8,\u03c6(x,z) = logp\u03b8(x|z) + logp\u03b8(z) \u2212 logq\u03c6(z|x)\nUsing eq. (4), the Monte Carlo estimate of the variational lower bound, given datapoint x(i), is:\n(5)\nL(\u03b8,\u03c6;x(i)) ?1\nL\nL\n?\nl=1\nf\u03b8,\u03c6(x(i),g\u03c6(?(l),x(i)))\nwhere\n?(l)\u223c p(?)\n(6)\nTheestimatoronlydependsonsamplesfromp(?)whichareobviouslynotinfluencedby\u03c6, therefore\nwecanuseitasanobjectivefunctionthatcanbedifferentiatedandjointlyoptimizedw.r.t. both\u03b8 and\n\u03c6. Given multiple datapoints from the dataset X, we can easily construct a minibatch-based version\nof the estimator: L(\u03b8,\u03c6;X) ?\na randomly drawn subset of the full dataset X. In our experiments we found that the number of\nsamples L per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M =\n100. Derivatives\u2207\u03b8,\u03c6?L(\u03b8;XM)canbetaken, andtheresultinggradientscanbeusedinconjunction\napproach to compute the stochastic gradients.\nA connection with auto-encoders becomes clear when looking at the objective function given at\neq. (6). The variational approximation q\u03c6(z|x(i)) (the encoder) maps a datapoint x(i)to a distribu-\ntion over latent variables z from which the datapoint could have been generated. The function g\u03c6(.)\nis chosen such that it maps a datapoint x(i)and a random noise vector ?(l)to a sample from the\napproximate posterior for that datapoint: z(i,l)= g\u03c6(?(l),x(i)) where z(i,l)\u223c q\u03c6(z|x(i)). Subse-\nquently, the sample z(i,l)is then input to function f\u03b8,\u03c6(.), which consists of three parts. The first\npart (logp\u03b8(x(i)|z(i,l))) can be interpreted as the negative reconstruction error in neural network\nparlance. The second and third part can be interpreted as regularization terms that make sure the\ncode activations have high entropy due to the term logq\u03c6(z|x), while not being too far from the\nprior due to the term logp\u03b8(z).\nN\nM\n?M\ni=1L(\u03b8,\u03c6;x(i)) where the minibatch XM= {x(i)}M\ni=1is\nwith stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic\n2.4The alternative parameterization trick\nIn order to solve our problem we invoked an alternative method for generating samples from\nq\u03c6(z|x). The essential parameterization trick is quite simple. Let q\u03c6(z|x) be some conditional\ndistribution parameterized by \u03c6. It is then often possible to express the random variable z given x\nas a determinstic variable z = g\u03c6(?,x), where ? is an auxiliary variable with independent marginal\np(?), and g\u03c6(.) is some vector-valued function parameterized by \u03c6.\nThis reparameterization is useful for our case since it can be used to rewrite an expectation w.r.t\nq\u03c6(z|x) such that the Monte Carlo estimate of the expectation is differentiable w.r.t. \u03c6. A proof\nis as follows. Given the deterministic mapping z = g\u03c6(?,x) we know that q\u03c6(z|x)?\n1Note that for infinitesimals we use the notational convention dz =?\n4\nidzi =\np(?)?\nid?i. Therefore1,?q\u03c6(z|x)f(z)dz =\n?p(?)f(z)d? =\n?p(?)f(g\u03c6(?,x))d?. It follows\nidzi"},{"page":5,"text":"216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\nthat a differentiable estimator can be constructed:\nwhere ?(l)\u223c p(?). In section 2.3 we applied this trick to obtain a differentiable estimator of the\nvariational lower bound.\nTake, for example, the univariate Gaussian case: let z be distributed as p(z|x) = N(x,\u03c3). The\nrandom variable z is partially explained by x, but there is some uncertainty left indicated by \u03c3.\nIn this case, a deterministic parameterization is z = x + \u03c3?, where ? is an independent auxiliary\nvariable ? \u223c N(0,1). In this univariate Gaussian case, \u03c6 = {\u03c3} and g\u03c6(?,y) = y + \u03c3?.\nWhen can we do this, i.e., for which q\u03c6(z|x) can we choose such a g\u03c6(.) and p(?)? There are three\nbasic approaches:\n?q\u03c6(z|x)f(z)dz ?\n1\nL\n?L\nl=1f(g\u03c6(x,?(l)))\n1. Tractable inverse CDF. In this case, let ? \u223c U(0,I), and let g\u03c6(?,x) be the inverse CDF of\nq\u03c6(z|x). Examples: Exponential, Cauchy, Logistic, Rayleigh, Pareto, Weibull, Reciprocal,\nGompertz, Gumbel and Erlang distributions.\n2. Analogous to the Gaussian example, for any \u201dlocation-scale\u201d family of distributions (with\ndifferentiablelog-PDF) wecan choosethe standarddistribution (withlocation = 0, scale =\n1) as the auxiliary variable E, and let g(.) = location + scale \u00b7 ?. Examples: Laplace,\nElliptical, Student\u2019s t, Logistic, Uniform, Triangular and Gaussian distributions.\n3. Composition: It is often possible to express variables as functions of component variables\nwith distributions that are reparameterizable using either of the above two approaches. Ex-\namples: Log-Normal (exponentiation of normally distributed variable), Gamma (a sum\nover exponentially distributed variables), Dirichlet (weighted sum of Gamma variates),\nBeta, Chi-Squared, and F distributions.\nWhen all three approaches fail, good approximations to the inverse CDF exist requiring computa-\ntions with time complexity comparable to the PDF (see e.g. [Dev86] for some methods).\n3Example\nHere we\u2019ll give an example generative model and posterior approximation used in experiments.\nLet the prior over the latent variables be the centered isotropic Gaussian p\u03b8(z) = N(z;0,I). Note\nthat in this case, the prior lacks parameters. Let p\u03b8(x|z) (the decoder) be a multivariate Bernoulli\nwhose probabilities are computed from z with a fully-connected neural network with a single hidden\nlayer:\nlogp\u03b8(x|z) =\nD\n?\ni=1\nxilogyi\u2212 (1 \u2212 xi) \u00b7 log(1 \u2212 yi)\nwhere y = f\u03c3(W2tanh(W1z + b1) + b2)\n(7)\nwhere f\u03c3(.) is the elementwise sigmoid activation function. While there is much freedom in the\nchoice of the approximate posterior q\u03c6(z|x) (encoder \/ recognition model), we\u2019ll for a moment\nassume a relatively simple case: let\u2019s assume that the true posterior p\u03b8(z|x) takes on a approximate\nGaussian form with an approximately diagonal covariance. In this case, we can let the variational\napproximate posterior be a multivariate Gaussian with a diagonal covariance structure2:\nlogq\u03c6(z|x) = logN(z;\u00b5,\u03c32I)\n(8)\nwhere \u00b5 and \u03c3 are yet unspecified functions of x. We can sample from q\u03c6(z|x) using ? z =\nlower bound is:\nh\u03c6(x,?) = \u00b5+\u03c3 ?? where ? \u223c N(0,I). With ? we signify an element-wise product. Therefore,\ngiven a minibatch XMof data, and using the f\u03b8,\u03c6(.) abbrevation of eq. (5), our estimator of the\nL(\u03b8,\u03c6;x(i)) ?1\nL\nL\n?\nl=1\nf\u03b8,\u03c6(x(i),z(i,l))??\nz(i,l)=\u00b5(i)+\u03c3(i)??(l)\nwhere\n?(l)\u223c N(0,I)\n(9)\n2Note that this is just a (simplifying) choice, and not a limitation of our method.\n5"},{"page":6,"text":"270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\nwhere\u00b5(i)and\u03c3(i)denotethemeanands.d. oftheapproximationoftheposteriorq\u03c6(z|x(i)), which\nwe didn\u2019t yet specify. Let the mean \u00b5(i)and variance \u03c3(i)of the Gaussian encoding distribution be\nthe following nonlinear function of x, (a neural network):\nlogq\u03c6(z|x) = logN(z;\u00b5,\u03c32I)\nwhere \u00b5 = W4h + b4, and\nlog\u03c32= W5h + b5, and\nh = tanh(W3x + b3) (10)\nNote that the generative (decoding) parameters are \u03b8 = {Wj,bj}2\nparameters are \u03c6 = {Wj,bj}5\neq. 6, and the lower bound can subsequently be differentiated and optimized w.r.t. the parameters.\nIn this model both p\u03b8(z) and q\u03c6(z|x) are Gaussian; in this special case, the second and third term\nof f\u03b8,\u03c6(eq. (5)) can be solved analytically. This results in an estimator with a lower variance than\nthe generic estimator given in eq. (9). The resulting estimator is:\nj=1and the variational (encoding)\nj=3. These definitions for the encoder and decoder can be plugged in\nL(\u03b8,\u03c6;x(i)) ?1\n2\nJ\n?\nj=1\n?\n1 + log((\u03c3(i)\nj)2) \u2212 (\u00b5(i)\nj)2\u2212 (\u03c3(i)\nj)2?\n+1\nL\nL\n?\nl=1\nlogp\u03b8(x(i)|z(i,l))\n(11)\nSee the appendix for the derivation.\n4Related work\nPerhaps the most relevant related method is the Wake-Sleep algorithm [HDFN95]. Like AEVB,\nthe wake-sleep algorithm employs an encoder (called a recognition network) that approximates the\ntrue posterior. A well-known drawback of the wake-sleep algorithm is that it lacks a theoretically\njustified method for learning the parameters of the recognition network: its updates correspond\nto optimization of the divergence KL(p||q) instead of the divergence KL(q||p) dictated by the\nlower bound. A theoretical advantage of the wake-sleep algorithm is that it also applies to models\nwith discrete latent variables. Wake-Sleep has the same computational complexity as AEVB per\ndatapoint.\nAEVB corresponds to the optimization of a type of auto-encoder, exposing a connection between\ndirected probabilistic models and auto-encoders. A connection between linear auto-encoders and a\ncertain class of generative linear-Gaussian models has long been known. In [Row98] it was shown\nthat PCA corresponds to the maximum-likelihood (ML) solution of a special case of the linear-\nGaussian model with a prior p(z) = N(0,I) and a conditional distribution p(x|z) = N(x;Wz,?I),\nspecifically the case with infinitesimally small ?.\nIn relevant recent work on autoencoders [VLL+10] it was shown that the training criterion of un-\nregularized autoencoders corresponds to maximization of a lower bound (see the infomax princi-\nple [Lin89]) of the mutual information between input X and latent representation Z. Maximiz-\ning (w.r.t. parameters) of the mutual information is equivalent to maximizing the conditional en-\ntropy, which is lower bounded by the expected loglikelihood of the data under the autoencoding\nmodel [VLL+10], i.e. the negative reconstrution error. However, it is well known that this recon-\nstruction criterion is in itself not sufficient for learning useful representations [BCV13]. Regular-\nization techniques have been proposed to make autoencoders learn useful representations, such as\ndenoising, contractive and sparse autoencoder variants [BCV13]. Our objective function contains\n(hyper-parameter free) regularization terms dictated by the variational bound (see eq. (5)). Related\nare also encoder-decoder architectures such as the predictive sparse decomposition (PSD) [KRL08],\nfrom which we drew some inspiration. Also relevant are the recently introduced Generative Stochas-\ntic Networks [BTL13] where noisy auto-encoders learn the transition operator of a Markov chain\nthat samples from the data distribution. In [SL10] a recognition network was employed for efficient\nlearning with Deep Boltzmann Machines. These methods are targeted at either unnormalized mod-\nels (i.e. undirected models like Boltzmann machines) or limited to sparse coding models, in contrast\nto our proposed algorithm for learning a general class of directed probabilistic models.\nThe recently proposed NADE method [GMW13], also learns a directed probabilistic model using\nan auto-encoding structure, however their method applies to binary latent variables, their objective\ndoes not correspond to the variational bound and their encoder doesn\u2019t correspond to a variational\napproximation of the posterior distribution.\n6"},{"page":7,"text":"324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n105\n106\n107\n108\n# Training samples evaluated\n150\n140\n130\n120\n110\n100\nL\nMNIST, Nz=3\n105\n106\n107\n108\n150\n140\n130\n120\n110\n100\nMNIST, Nz=5\n105\n106\n107\n108\n150\n140\n130\n120\n110\n100\nMNIST, Nz=10\n105\n106\n107\n108\n150\n140\n130\n120\n110\n100\nMNIST, Nz=20\n105\n106\n107\n108\n150\n140\n130\n120\n110\n100\nMNIST, Nz=200\n105\n106\n107\n108\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nL\nFrey Face, Nz=2\nWake-Sleep (test)\nWake-Sleep (train)\nAEVB (test)\nAEVB (train)\n105\n106\n107\n108\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600Frey Face, Nz=5\n105\n106\n107\n108\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600Frey Face, Nz=10\n105\n106\n107\n108\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600Frey Face, Nz=20\nFigure 2: Comparison of our AEVB method to the wake-sleep algorithm, in terms of optimizing the\nlower bound, for different dimensionality of latent space (Nz). Our method converged considerably\nfaster and reached a better solution in all experiments. Interestingly enough, more latent variables\ndoes not result in more overfitting, which is explained by the regularizing effect of the lower bound.\nVertical axis: the estimated average variational lower bound per datapoint. The estimator variance\nwas small (< 1) and omitted. Horizontal axis: amount of training points evaluated. Computa-\ntion took around 20-40 minutes per million training samples with a Intel Xeon CPU running at an\neffective 40 GFLOPS.\n5Experiments\nWe trained generative models of images from the MNIST and Frey Face datasets3and compared\nlearning algorithms in terms of the variational lower bound, and the estimated marginal likelihood.\nThe generative model (encoder) and variational approximation (decoder) from section 3 were used,\nwhere the described encoder and decoder have an equal number of hidden units. Since the Frey\nFace data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except\nthat the means were constrained to the interval (0,1) using a sigmoidal activation function at the\ndecoder output. Note that with hidden units we refer to the hidden layer of the neural networks of\nthe encoder and decoder.\nParameters are updated using stochastic gradient ascent where gradients are computed by differenti-\nating the lower bound estimator \u2207\u03b8,\u03c6L(\u03b8,\u03c6;X) (see algorithm 1), plus a small weight decay term\ncorresponding to a prior p(\u03b8) = N(0,I). Optimization of this objective is equivalent to approxi-\nmate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower\nbound.\nWe compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the\nsame encoder (also called recognition network) for the wake-sleep algorithm and the variational\nauto-encoder. All parameters, both variational and generative, were initialized by random sampling\nfrom N(0,0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were\nadapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01,\n0.02, 0.1} based on performance on the training set in the first few iterations. Minibatches of size\nM = 100 were used, with L = 1 samples per datapoint.\nLikelihood lower bound\n(a.k.a. recognition networks) having 500 hidden units in case of MNIST, and 200 hidden units in\ncase of the Frey Face dataset (to prevent overfitting, since it is a considerably smaller dataset).\nFigure 2 shows the results when comparing the lower bounds. Interestingly, superfluous latent\nvariables did not result in overfitting, which is explained by the regularizing nature of the variational\nbound.\nWe trained generative models (decoders) and corresponding encoders\n3Available at http:\/\/www.cs.nyu.edu\/\u02dcroweis\/data.html\n7"},{"page":8,"text":"378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n0102030405060\n# Training samples evaluated (millions)\n160\n150\n140\n130\n120\n110\n100\nMarginal log-likelihood\nNtrain = 1000\n0102030405060\n160\n155\n150\n145\n140\n135\n130\n125\nNtrain = 50000\nWake-Sleep (train)\nWake-Sleep (test)\nMCEM (train)\nMCEM (test)\nAEVB (train)\nAEVB (test)\nFigure 3: Comparison of AEVB to the wake-sleep algorithm and Monte Carlo EM, in terms of the\nestimated marginal likelihood, for a different number of training points. The Monte Carlo EM algo-\nrithm is (unlike AEVB and the wake-sleep method) asymptotically unbiased but cannot be applied\nonline such that it becomes inefficient for large datasets (right figure).\n(a) Learned Frey Face manifold(b) Learned MNIST manifold\nFigure 4: Visualisations of learned data manifold for generative models with two-dimensional latent\nspace, learned with AEVB. Since the prior of the latent space is Gaussian, linearly spaced coor-\ndinates on the unit square were transformed through the inverse CDF of the Gaussian to produce\nvalues of the latent variables z. For each of these values z, we plotted the corresponding generative\np\u03b8(x|z) with the learned parameters \u03b8.\nMarginal likelihood\nlikelihood of the learned generative models using an MCMC estimator. More information about the\nmarginal likelihood estimator is available in the appendix. For the encoder and decoder we again\nused neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional\nlatent space the estimates became unreliable. Again, the MNIST dataset was used. The AEVB\nand Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo\n(HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for\nthe three algorithms, for a small and large training set size. Results are in figure 3.\nFor very low-dimensional latent space it is possible to estimate the marginal\n8"},{"page":9,"text":"432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n(a) 2-D latent space(b) 5-D latent space(c) 10-D latent space(d) 20-D latent space\nFigure 5: Random samples from learned generative models of MNIST for different dimensionalities\nof latent space.\n6 Conclusion\nWe have introduced a novel online learning and approximate inference method for models with of\ncontinuous latent variables, that works for the case where mean-field VB and EM methods are in-\ntractable. The proposed estimator can be straightforwardly differentiated and optimized w.r.t. all\nparameters, resulting in stochastic gradients that are easily plugged into existing stochastic gradient\noptimization methods. The method learns an encoder, or variational approximation to the poste-\nrior, that can be used for fast approximate inference of the distribution of the latent variables. The\ntheoretical advantages are reflected in experimental results.\nReferences\n[BCV13] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A re-\nview and new perspectives. 2013.\nDavid M Blei, Michael I Jordan, and John W Paisley. Variational bayesian inference\nwith stochastic search. In Proceedings of the 29th International Conference on Machine\nLearning (ICML-12), pages 1367\u20131374, 2012.\nYoshua Bengio and\u00b4Eric Thibodeau-Laufer. Deep generative stochastic networks train-\nable by backprop. arXiv preprint arXiv:1306.1091, 2013.\nLuc Devroye. Sample-based non-uniform random variate generation. In Proceedings of\nthe 18th conference on Winter simulation, pages 260\u2013265. ACM, 1986.\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online\nlearning and stochastic optimization. Journal of Machine Learning Research, 12:2121\u2013\n2159, 2010.\n[DKPR87] Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid\nmonte carlo. Physics letters B, 195(2):216\u2013222, 1987.\n[GMW13] Karol Gregor, Andriy Mnih, and Daan Wierstra. Deep autoregressive networks. arXiv\npreprint arXiv:1310.8499, 2013.\n[HDFN95] Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The\u201d wake-\nsleep\u201d algorithm for unsupervised neural networks. SCIENCE, pages 1158\u20131158, 1995.\n[KRL08]Koray Kavukcuoglu, Marc\u2019Aurelio Ranzato, and Yann LeCun. Fast inference in sparse\ncoding algorithms with applications to object recognition. Technical Report CBLL-\nTR-2008-12-01, Computational and Biological Learning Lab, Courant Institute, NYU,\n2008.\n[Lin89] Ralph Linsker. An application of the principle of maximum information preservation to\nlinear systems. Morgan Kaufmann Publishers Inc., 1989.\n[Row98]Sam Roweis. EM algorithms for PCA and SPCA. Advances in neural information\nprocessing systems, pages 626\u2013632, 1998.\n[SL10]Ruslan Salakhutdinov and Hugo Larochelle. Efficient learning of deep boltzmann ma-\nchines. In International Conference on Artificial Intelligence and Statistics, pages 693\u2013\n700, 2010.\n[BJP12]\n[BTL13]\n[Dev86]\n[DHS10]\n9"},{"page":10,"text":"486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n[VLL+10] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine\nManzagol. Stacked denoising autoencoders: Learning useful representations in a deep\nnetwork with a local denoising criterion. The Journal of Machine Learning Research,\n9999:3371\u20133408, 2010.\nA Derivation of the alternative estimator\nHere we give the derivation of the alternative estimator for the example model introduced in the\npaper.\nThe estimator in our example can be improved upon, in terms of variance, by realizing that the ex-\npectation of the second and third term of f\u03b8(.) are tractable and can be solved analytically, since both\nthe variational approximation q\u03b8(z|x) and the prior p\u03b8(z) are Gaussian. Let J be the dimensionality\nof z. Then in this case:\n?\n= \u2212J\nq\u03b8(z|x)logp(z)dz =\n?\nN(z;\u00b5,\u03c32)logN(z;0,I)dz\n?\n2log(2\u03c0) \u22121\n2\nJ\nj=1\n(\u00b52\nj+ \u03c32\nj)\nAnd:\n\u2212\n?\nq\u03b8(z|x)logq\u03b8(z|x)dz = \u2212\n?\nN(z;\u00b5,\u03c32)logN(z;\u00b5,\u03c32)dz\n?\n=J\n2log(2\u03c0) +1\n2\nJ\nj=1\n(1 + log\u03c32\nj)\nThe both the variational encoder q\u03b8(z|x) and the prior p\u03b8(z) are Gaussian,\n?q\u03b8(z|x)logp\u03b8(z)dz and?q\u03b8(z|x)logq\u03b8(z|x)dz are tractable and can be solved analytically.\nL(\u03b8;x(i)) =\n?\n=1\n2\nj=1\n?\nwhere\n?(i,l)\u223c N(0,I)\nso the\nThis results in the following estimator:\n?\n=q\u03b8(z|x(i))\n?\n?1\n2\nj=1\nq\u03b8(z|x(i))\n?\n?\nlogp\u03b8(x(i)|z) + logp\u03b8(z) \u2212 logq\u03b8(z|x(i))\n?\ndz\nlogp\u03b8(z) \u2212 logq\u03b8(z|x(i))\n?\ndz +\n?\n?\np(?)logp\u03b8(x(i)|z)??\np(?)logp\u03b8(x(i)|z)??\nL\n?\nz=g\u03b8(?,x(i))d?\nJ\n?\n1 + log((\u03c3(i)\nj)2) \u2212 (\u00b5(i)\nj)2\u2212 (\u03c3(i)\nj)2?\nj)2?\n+\nz=g\u03b8(?,x(i))d?\nJ\n?\n1 + log((\u03c3(i)\nj)2) \u2212 (\u00b5(i)\nj)2\u2212 (\u03c3(i)\n+1\nL\nl=1\nlogp\u03b8(x(i)|z(i,l))??\nz(i,l)=\u00b5(i)+\u03c3(i)??(i,l)\nNote that p\u03b8(x|z), \u00b5 and \u03c3 were defined earlier. \u00b5(i)simply denotes the variational mean evaluated\nat datapoint i, and \u00b5(i)\nj\nis simply the j-th element of that vector. The same notation is used for \u03c3.\nBMarginal likelihood estimator\nWe derived the following marginal likelihood estimator that produces good estimates of the marginal\nlikelihood as long as the dimensionality of the sampled space is low (less then 5 dimensions), and\nsufficient samples are taken. Let p\u03b8(x,z) = p\u03b8(z)p\u03b8(x|z) be the generative model we are sampling\nfrom, and for a given datapoint x(i)we would like to estimate the marginal likelihood p\u03b8(x(i)).\nThe estimation process consists of three stages:\n10"},{"page":11,"text":"540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n1. SampleLvalues{z(l)}fromtheposteriorusinggradient-basedMCMC,e.g. HybridMonte\nCarlo, using \u2207zlogp\u03b8(z|x) = \u2207zlogp\u03b8(z) + \u2207zlogp\u03b8(x|z).\n2. Fit a density estimator q(z) to these samples {z(l)}.\n3. Again, sample L new values from the posterior. Plug these samples, as well as the fitted\nq(z), into the following estimator:\n?\nL\nl=1\np\u03b8(z)p\u03b8(x(i)|z(l))\nDerivation of the estimator:\n?q(z)dz\n?\n=\np\u03b8(z|x(i))\np\u03b8(x(i)) ?\n1\nL\n?\nq(z(l))\n?\u22121\nwhere\nz(l)\u223c p\u03b8(z|x(i))\n1\np\u03b8(x(i))=p\u03b8(x(i))\np\u03b8(x(i),z)\np\u03b8(x(i))\n?\n?1\nL\nl=1\n=\n?q(z)p\u03b8(x(i),z)\np\u03b8(x(i))\nq(z)\np\u03b8(x(i),z)dz\nq(z)\np\u03b8(x(i),z)dz\np\u03b8(x(i),z)dz\n=\nL\n?\nq(z(l))\np\u03b8(z)p\u03b8(x(i)|z(l))\nwhere\nz(l)\u223c p\u03b8(z|x(i))\nCMonte Carlo EM\nThe Monte Carlo EM algorithm does not employ an encoder, instead it samples from the pos-\nterior of the latent variables using gradients of the posterior computed with \u2207zlogp\u03b8(z|x) =\n\u2207zlogp\u03b8(z) + \u2207zlogp\u03b8(x|z). The Monte Carlo EM procedure consists of 10 HMC leapfrog\nsteps with an automatically tuned stepsize such that the acceptance rate was 90%, followed by 5\nweight updates steps using the acquired sample. For all algorithms the parameters were updated\nusing the Adagrad stepsizes (with accompanying annealing schedule).\nThe marginal likelihood was estimated with the first 1000 datapoints from the train and test sets,\nfor each datapoint sampling 50 values from the posterior of the latent variables using Hybrid Monte\nCarlo with 4 leapfrog steps.\nD Full VB\nAs written in the paper, it is possible to perform variational inference on both the parameters \u03b8 and\nthe latent variables z, as opposed to just the latent variables as we did in the paper. Here, we\u2019ll derive\nour estimator for that case.\nLet p\u03b1(\u03b8) be some hyperprior for the parameters introduced above, parameterized by \u03b1. The\nmarginal likelihood can be written as:\nlogp\u03b1(X) = DKL(q\u03c6(\u03b8)||p\u03b1(\u03b8|X)) + L(\u03c6;X)\nwhere the first RHS term denotes a KL divergence of the approximate from the true posterior, and\nwhere L(\u03c6;X) denotes the variational lower bound to the marginal likelihood:\n?\nNote that this is a lower bound since the KL divergence is non-negative; the bound equals the true\nmarginal when the approximate and true posteriors match exactly. The term logp\u03b8(X) is composed\nof a sum over the marginal likelihoods of individual datapoints logp\u03b8(X) =?N\nlogp\u03b8(x(i)) = DKL(q\u03c6(z|x(i))||p\u03b8(z|x(i))) + L(\u03b8,\u03c6;x(i))\n(12)\nL(\u03c6;X) =q\u03c6(\u03b8)(logp\u03b8(X) + logp\u03b1(\u03b8) \u2212 logq\u03c6(\u03b8)) d\u03b8\n(13)\ni=1logp\u03b8(x(i)),\nwhich can each be rewritten as:\n(14)\n11"},{"page":12,"text":"594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\nwhere again the first RHS term is the KL divergence of the approximate from the true posterior, and\nL(\u03b8,\u03c6;x) is the variational lower bound of the marginal likelihood of datapoint i:\nL(\u03b8,\u03c6;x(i)) =\n?\nq\u03c6(z|x)\n?\nlogp\u03b8(x(i)|z) + logp\u03b8(z) \u2212 logq\u03c6(z|x)\n?\ndz\n(15)\nTheexpectationsontheRHSofeqs (13)and(15)canobviouslybewrittenasasumofthreeseparate\nexpectations, of which the second and third component can sometimes be analytically solved, e.g.\nwhen both p\u03b8(x) and q\u03c6(z|x) are Gaussian. For generality we will here assume that each of these\nexpectations is intractable.\nGiven the equations above it is easy to construct a Monte Carlo estimator of the lower bound, for\nexample the following:\nL(\u03c6;X) ?1\nL\nL\n?\nl=1\nN\n?\nlogp\u03b8(x(l)|z(l)) + logp\u03b8(z(l)) \u2212 logq\u03c6(z(l))\n?\n+ logp\u03b1(\u03b8(l)) \u2212 logq\u03c6(\u03b8(l))\n(16)\nwhere x(l)\u223c X (random datapoints from the dataset) and \u03b8(l)\u223c q\u03c6(\u03b8) and z(l)\u223c q\u03c6(z|x(l)).\nWe would like to optimize the lower bound L(\u03c6;X) (eq. (13)) w.r.t. \u03c6 using stochastic gradient-\nbased optimization. Note that following these gradients would either decrease the KL divergence\nbetween the approximate and true posteriors, or increase the marginal likelihood. While the estima-\ntor above is unbiased, differentiating it w.r.t. the parameters \u03c6 will not result in an unbiased gradient:\nthe variational parameters \u03c6 indirectly influence the estimate through the samples z(l)\u223c q\u03c6(z|x),\nand it is impossible to differentiate through this sampling process.\nD.1Our estimator of the lower bound\nUnder certain mild conditions outlined in section (see paper) for chosen approximate posteriors\nq\u03c6(\u03b8) and q\u03c6(z|x) we can reparameterize its conditional samples ? z \u223c q\u03c6(z|x) as\nwhere we choose a prior p(?) and a function g\u03c6(?,x) such that the following holds:\n?\n=p(?) logp\u03b8(x(i)|z) + logp\u03b8(z) \u2212 logq\u03c6(z|x)\n? z = g\u03c6(?,x)\n?\nwith\n? \u223c p(?)\n(17)\nL(\u03b8,\u03c6;x(i)) =q\u03c6(z|x) logp\u03b8(x(i)|z) + logp\u03b8(z) \u2212 logq\u03c6(z|x)\n?\ndz\n?\n??????\nz=g\u03c6(?,x(i))\nd?\n(18)\nThe same can be done for the approximate posterior q\u03c6(\u03b8):\n?\u03b8 = h\u03c6(\u03b6)\nwith\n\u03b6 \u223c p(\u03b6)\n(19)\nwhere we, similarly as above, choose a prior p(\u03b6) and a function h\u03c6(\u03b6) such that the following\nholds:\n?\n=p(\u03b6)(logp\u03b8(X) + logp\u03b1(\u03b8) \u2212 logq\u03c6(\u03b8))\nL(\u03c6;X) =q\u03c6(\u03b8)(logp\u03b8(X) + logp\u03b1(\u03b8) \u2212 logq\u03c6(\u03b8)) d\u03b8\n?\n????\n\u03b8=h\u03c6(\u03b6)\nd\u03b6\n(20)\nFor notational conciseness we introduce a shorthand notation f\u03c6(x,z,\u03b8):\nf\u03c6(x,z,\u03b8) = N \u00b7 (logp\u03b8(x|z) + logp\u03b8(z) \u2212 logq\u03c6(z|x)) + logp\u03b1(\u03b8) \u2212 logq\u03c6(\u03b8)\nUsing equations (20) and (18), the Monte Carlo estimate of the variational lower bound, given\ndatapoint x(i), is:\n(21)\nL(\u03c6;X) ?1\nL\nL\n?\nl=1\nf\u03c6(x(l),g\u03c6(?(l),x(l)),h\u03c6(\u03b6(l)))\n(22)\n12"},{"page":13,"text":"648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\nAlgorithm 2 Pseudocode for computing a stochastic gradient using our estimator. See section D.1\nfor meaning of the functions f\u03c6, g\u03c6and h\u03c6.\nRequire: \u03c6 (Current value of variational parameters)\ng \u2190 0\nfor l is 1 to L do\nx \u2190 Random draw from dataset X\n? \u2190 Random draw from prior p(?)\n\u03b6 \u2190 Random draw from prior p(\u03b6)\ng \u2190 g +1\nend for\nreturn g\nL\u2207\u03c6f\u03c6(x,g\u03c6(?,x),h\u03c6(\u03b6))\nwhere ?(l)\u223c p(?) and \u03b6(l)\u223c p(\u03b6). The estimator only depends on samples from p(?) and p(\u03b6)\nwhich are obviously not influenced by \u03c6, therefore the estimator can be differentiated w.r.t. \u03c6.\nThe resulting stochastic gradients can be used in conjunction with stochastic optimization methods\nsuch as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to computing stochastic\ngradients.\nD.2Example\nD.2.1Gaussian priors and variational approximations\nWhile there is much freedom in the choice of q\u03c6(z|x) and q\u03c6(\u03b8), we\u2019ll for a moment assume\na relatively simple case. Let the prior over the parameters and latent variables be the centered\nisotropic Gaussian p\u03b1(\u03b8) = N(z;0,I) and p\u03b8(z) = N(z;0,I). Note that in this case, the prior\nlacks parameters. Let\u2019s also assume that the true posteriors are approximatily Gaussian with an\napproximately diagonal covariance. In this case, we can let the variational approximate posteriors\nbe multivariate Gaussians with a diagonal covariance structure:\nlogq\u03c6(\u03b8) = logN(\u03b8;\u00b5\u03b8,\u03c32\nlogq\u03c6(z|x) = logN(z;\u00b5z,\u03c32\nwhere \u00b5zand \u03c3zare yet unspecified functions of x. Since they are Gaussian, we can parameterize\nthe variational approximate posteriors:\n\u03b8I)\nzI)\n(23)\nq\u03c6(\u03b8)\nq\u03c6(z|x)\nas\nas\n?\u03b8 = \u00b5\u03b8+ \u03c3\u03b8? \u03b6\nwhere\nwhere\n\u03b6 \u223c N(0,I)\n? \u223c N(0,I)\n? z = \u00b5z+ \u03c3z? ?\nWith ? we signify an element-wise product. These can be plugged into the lower bound defined\nabove (eqs (21) and (22)).\nIn this case it is possible to construct an alternative estimator with a lower variance, since in this\nmodel p\u03b1(\u03b8), p\u03b8(z), q\u03c6(\u03b8) and q\u03c6(z|x) are Gaussian, and therefore four terms of f\u03c6can be solved\nanalytically. The resulting estimator is:\n\uf8eb\n2\nj=1\nL(\u03c6;X) ?1\nL\nL\n?\nJ\n?\nl=1\nN \u00b7\n\uf8ed1\nJ\n?\n?\n1 + log((\u03c3(l)\nz,j)2) \u2212 (\u00b5(l)\nz,j)2\u2212 (\u03c3(l)\nz,j)2?\n+ logp\u03b8(x(i)z(i))\n\uf8f6\n\uf8f8\n+1\n2\nj=1\n?\n1 + log((\u03c3(l)\n\u03b8,j)2) \u2212 (\u00b5(l)\n\u03b8,j)2\u2212 (\u03c3(l)\n\u03b8,j)2?\n(24)\n\u00b5(i)\nj\nand \u03c3(i)\nj\nsimply denote the j-th element of vectors \u00b5(i)and \u03c3(i).\nD.2.2Example with auto-encoder\nIn the example above, we still left unspecified the generative PDF p\u03b8(x|z) (the decoder), as well as\nthe mean \u00b5zand s.d. \u03c3zof the variational approximation q\u03c6(z|x) (the encoder). Although we can\nchoose any differentiable PDF for both, let, for example, the probabilistic decoder be a multivariate\n13"},{"page":14,"text":"702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\nBernoulli whose probabilities are computed from z with a fully-connected neural network with a\nsingle hidden layer:\nlogp\u03b8(x|z) =\nD\n?\ni=1\nxilogyi\u2212 (1 \u2212 xi) \u00b7 log(1 \u2212 yi)\nwhere y = exp(W2tanh(W1z + b1) + b2)\n(25)\nLet the Gaussian mean and covariance of the encoder be a function of x, using a similar neural\nnetwork:\nlogq\u03c6(z|x) = logN(z;\u00b5,\u03c32I)\nwhere \u00b5 = W4h + b4\nlog\u03c32= W5h + b5\nh = tanh(W3x + b3)\n(26)\nNote that the generative parameters are \u03b8 = {Wj,bj}2\n\u03c6\u03b8\u222a\u03c6zwhere \u03c6\u03b8= {\u00b5\u03b8,\u03c3\u03b8} and \u03c6z= {Wj,bj}5\neq. 22 or eq. 24 (for a lower variance), and the lower bound can subsequently can be differentiated\nand optimized w.r.t. the parameters.\nj=1. The variational parameters are \u03c6 =\nj=3. These definitions above can be plugged in\n14"}],"widgetId":"rgw26_56ab1f18415d8"},"id":"rgw26_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=259400035&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw27_56ab1f18415d8"},"id":"rgw27_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=259400035&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":259400035,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":259400035,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithSlurp","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2049611927,"url":"researcher\/2049611927_Samuel_R_Bowman","fullname":"Samuel R. Bowman","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2057227262,"url":"researcher\/2057227262_Luke_Vilnis","fullname":"Luke Vilnis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69685462,"url":"researcher\/69685462_Oriol_Vinyals","fullname":"Oriol Vinyals","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2078879465,"url":"researcher\/2078879465_Andrew_M_Dai","fullname":"Andrew M. Dai","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":2,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":false,"isSlurp":true,"isNoText":false,"publicationType":"Article","publicationDate":"Nov 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/284219020_Generating_Sentences_from_a_Continuous_Space","usePlainButton":true,"publicationUid":284219020,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/284219020_Generating_Sentences_from_a_Continuous_Space","title":"Generating Sentences from a Continuous Space","displayTitleAsLink":true,"authors":[{"id":2049611927,"url":"researcher\/2049611927_Samuel_R_Bowman","fullname":"Samuel R. Bowman","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2057227262,"url":"researcher\/2057227262_Luke_Vilnis","fullname":"Luke Vilnis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69685462,"url":"researcher\/69685462_Oriol_Vinyals","fullname":"Oriol Vinyals","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2078879465,"url":"researcher\/2078879465_Andrew_M_Dai","fullname":"Andrew M. Dai","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2085202363,"url":"researcher\/2085202363_Rafal_Jozefowicz","fullname":"Rafal Jozefowicz","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":67738097,"url":"researcher\/67738097_Samy_Bengio","fullname":"Samy Bengio","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"The standard unsupervised recurrent neural network language model (RNNLM)\ngenerates sentences one word at a time and does not work from an explicit\nglobal distributed sentence representation. In this work, we present an\nRNN-based variational autoencoder language model that incorporates distributed\nlatent representations of entire sentences. This factorization allows it to\nexplicitly model holistic properties of sentences such as style, topic, and\nhigh-level syntactic features. Samples from the prior over these sentence\nrepresentations remarkably produce diverse and well-formed sentences through\nsimple deterministic decoding. By examining paths through this latent space, we\nare able to generate coherent novel sentences that interpolate between known\nsentences. We present techniques for solving the difficult learning problem\npresented by this model, demonstrate strong performance in the imputation of\nmissing tokens, and explore many interesting properties of the latent sentence\nspace.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/284219020_Generating_Sentences_from_a_Continuous_Space","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1511.06349","sourceName":"de.arxiv.org","hasSourceUrl":true},"publicationUid":284219020,"publicationUrl":"publication\/284219020_Generating_Sentences_from_a_Continuous_Space","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/284219020_Generating_Sentences_from_a_Continuous_Space\/links\/5653acc508aefe619b197329\/smallpreview.png","linkId":"5653acc508aefe619b197329","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=284219020&reference=5653acc508aefe619b197329&eventCode=&origin=publication_list","widgetId":"rgw31_56ab1f18415d8"},"id":"rgw31_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=284219020&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":259400035,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/284219020_Generating_Sentences_from_a_Continuous_Space\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Naively, maximum likelihood learning in such a model presents an intractable inference problem over the latent variable. Drawing inspiration from recent successes in modeling images (Gregor et al., 2015), handwriting, and natural speech (Chung et al., 2015), our model circumvents these difficulties using the architecture of a variational autoencoder and takes advantage of recent advances in variational inference (Kingma & Welling, 2015; Rezende et al., * First two authors contributed equally. Work was done when all authors were at Google, Inc. 1 arXiv:1511.06349v1 "],"widgetId":"rgw32_56ab1f18415d8"},"id":"rgw32_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw30_56ab1f18415d8"},"id":"rgw30_56ab1f18415d8","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=284219020&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2054900589,"url":"researcher\/2054900589_Zhenwen_Dai","fullname":"Zhenwen Dai","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":59288229,"url":"researcher\/59288229_Andreas_Damianou","fullname":"Andreas Damianou","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272454912311306%401441969721269_m\/Andreas_Damianou.png"},{"id":21048246,"url":"researcher\/21048246_Javier_Gonzalez","fullname":"Javier Gonz\u00e1lez","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39663468,"url":"researcher\/39663468_Neil_Lawrence","fullname":"Neil Lawrence","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Nov 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/284476380_Variational_Auto-encoded_Deep_Gaussian_Processes","usePlainButton":true,"publicationUid":284476380,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/284476380_Variational_Auto-encoded_Deep_Gaussian_Processes","title":"Variational Auto-encoded Deep Gaussian Processes","displayTitleAsLink":true,"authors":[{"id":2054900589,"url":"researcher\/2054900589_Zhenwen_Dai","fullname":"Zhenwen Dai","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":59288229,"url":"researcher\/59288229_Andreas_Damianou","fullname":"Andreas Damianou","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272454912311306%401441969721269_m\/Andreas_Damianou.png"},{"id":21048246,"url":"researcher\/21048246_Javier_Gonzalez","fullname":"Javier Gonz\u00e1lez","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39663468,"url":"researcher\/39663468_Neil_Lawrence","fullname":"Neil Lawrence","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"We develop a scalable deep non-parametric generative model by augmenting deep\nGaussian processes with a recognition model. Inference is performed in a novel\nscalable variational framework where the variational posterior distributions\nare reparametrized through a multilayer perceptron. The key aspect of this\nreformulation is that it prevents the proliferation of variational parameters\nwhich otherwise grow linearly in proportion to the sample size. We derive a new\nformulation of the variational lower bound that allows us to distribute most of\nthe computation in a way that enables to handle datasets of the size of\nmainstream deep learning tasks. We show the efficacy of the method on a variety\nof challenges including deep unsupervised learning and deep Bayesian\noptimization.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/284476380_Variational_Auto-encoded_Deep_Gaussian_Processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Andreas_Damianou\/publication\/284476380_Variational_Auto-encoded_Deep_Gaussian_Processes\/links\/567c62e308ae19758384e03a.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Andreas_Damianou","sourceName":"Andreas Damianou","hasSourceUrl":true},"publicationUid":284476380,"publicationUrl":"publication\/284476380_Variational_Auto-encoded_Deep_Gaussian_Processes","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/284476380_Variational_Auto-encoded_Deep_Gaussian_Processes\/links\/567c62e308ae19758384e03a\/smallpreview.png","linkId":"567c62e308ae19758384e03a","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=284476380&reference=567c62e308ae19758384e03a&eventCode=&origin=publication_list","widgetId":"rgw34_56ab1f18415d8"},"id":"rgw34_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=284476380&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"567c62e308ae19758384e03a","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":259400035,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/284476380_Variational_Auto-encoded_Deep_Gaussian_Processes\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["The main challenge is that exact inference on directed nonlinear probabilistic models is typically intractable due to the required marginalisation of the latent components. This has lead to the development of probabilistic generative models based on neural networks (Kingma & Welling, 2013; Mnih & Gregor, 2014; Rezende et al., 2014), in which probabilistic distributions are defined for the input and output of individual layers. Efficient approximated inference methods have been developed in this context based on stochastic variational inference or stochastic back-propagation. "],"widgetId":"rgw35_56ab1f18415d8"},"id":"rgw35_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw33_56ab1f18415d8"},"id":"rgw33_56ab1f18415d8","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=284476380&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithSlurp","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2045971008,"url":"researcher\/2045971008_Alireza_Makhzani","fullname":"Alireza Makhzani","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2061288721,"url":"researcher\/2061288721_Jonathon_Shlens","fullname":"Jonathon Shlens","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2085220925,"url":"researcher\/2085220925_Navdeep_Jaitly","fullname":"Navdeep Jaitly","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2061320296,"url":"researcher\/2061320296_Ian_Goodfellow","fullname":"Ian Goodfellow","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":false,"isSlurp":true,"isNoText":false,"publicationType":"Article","publicationDate":"Nov 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/284219537_Adversarial_Autoencoders","usePlainButton":true,"publicationUid":284219537,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/284219537_Adversarial_Autoencoders","title":"Adversarial Autoencoders","displayTitleAsLink":true,"authors":[{"id":2045971008,"url":"researcher\/2045971008_Alireza_Makhzani","fullname":"Alireza Makhzani","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2061288721,"url":"researcher\/2061288721_Jonathon_Shlens","fullname":"Jonathon Shlens","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2085220925,"url":"researcher\/2085220925_Navdeep_Jaitly","fullname":"Navdeep Jaitly","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2061320296,"url":"researcher\/2061320296_Ian_Goodfellow","fullname":"Ian Goodfellow","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"In this paper we propose a new method for regularizing autoencoders by\nimposing an arbitrary prior on the latent representation of the autoencoder.\nOur method, named \"adversarial autoencoder\", uses the recently proposed\ngenerative adversarial networks (GAN) in order to match the aggregated\nposterior of the hidden code vector of the autoencoder with an arbitrary prior.\nMatching the aggregated posterior to the prior ensures that there are no\n\"holes\" in the prior, and generating from any part of prior space results in\nmeaningful samples. As a result, the decoder of the adversarial autoencoder\nlearns a deep generative model that maps the imposed prior to the data\ndistribution. We show how adversarial autoencoders can be used to disentangle\nstyle and content of images and achieve competitive generative performance on\nMNIST, Street View House Numbers and Toronto Face datasets.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/284219537_Adversarial_Autoencoders","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1511.05644","sourceName":"de.arxiv.org","hasSourceUrl":true},"publicationUid":284219537,"publicationUrl":"publication\/284219537_Adversarial_Autoencoders","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/284219537_Adversarial_Autoencoders\/links\/5653ae1b08aeafc2aabb5df7\/smallpreview.png","linkId":"5653ae1b08aeafc2aabb5df7","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=284219537&reference=5653ae1b08aeafc2aabb5df7&eventCode=&origin=publication_list","widgetId":"rgw37_56ab1f18415d8"},"id":"rgw37_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=284219537&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":259400035,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/284219537_Adversarial_Autoencoders\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["In this case, the stochasticity in q(z) comes from both the data-distribution and the randomness of the Gaussian distribution at the output of the encoder. We can use the same re-parametrization trick of (Kingma & Welling, 2014) for back-propagation through the encoder network. "],"widgetId":"rgw38_56ab1f18415d8"},"id":"rgw38_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw36_56ab1f18415d8"},"id":"rgw36_56ab1f18415d8","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=284219537&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":259400035,"publicationLink":"publication\/259400035_Auto-Encoding_Variational_Bayes","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw29_56ab1f18415d8"},"id":"rgw29_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=259400035&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=49","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":49,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw28_56ab1f18415d8"},"id":"rgw28_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=259400035&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/259400035_Auto-Encoding_Variational_Bayes","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1f18415d8"},"id":"rgw2_56ab1f18415d8","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":259400035},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=259400035&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1f18415d8"},"id":"rgw1_56ab1f18415d8","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"HPj8UgMF89A3vpJQwgnYWDNsxIk3ftNnSXo2d4EIcsiPVoYA9GwzmfteIuBNnVDKDLi16ylp98CroD1rM\/dtfXc5JHiWTzz0mwTUZJbg2g8Jbo0ULu\/kF9d1Hn2LDYfffo6TZsRSo0yfcH+Ks99nnGgEsPBd0KGGBV\/MKVc3pOip1zjDVihDX3YiXeru9ltcZYibCOHEdGMtG+pleiamr4kFEPvQiNPJLljqP8kiWDvE0welkTsEjeIkvgJnDan+q8HE6nkvjCTyyudziu3S7to6gpkm3Lnqfk4b2sDTt4M=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/259400035_Auto-Encoding_Variational_Bayes\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Auto-Encoding Variational Bayes\" \/>\n<meta property=\"og:description\" content=\"Can we efficiently learn the parameters of directed probabilistic models, in\nthe presence of continuous latent variables with intractable posterior\ndistributions, and in case of large datasets? We...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/259400035_Auto-Encoding_Variational_Bayes\/links\/02d4e0f60cf2df62f0a56786\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/259400035_Auto-Encoding_Variational_Bayes\" \/>\n<meta property=\"rg:id\" content=\"PB:259400035\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Auto-Encoding Variational Bayes\" \/>\n<meta name=\"citation_author\" content=\"Diederik P Kingma\" \/>\n<meta name=\"citation_author\" content=\"Max Welling\" \/>\n<meta name=\"citation_publication_date\" content=\"2013\/12\/20\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/259400035_Auto-Encoding_Variational_Bayes\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/259400035_Auto-Encoding_Variational_Bayes\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-d681910f-6abe-427b-bbec-7f5cf65314b4","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":1160,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw39_56ab1f18415d8"},"id":"rgw39_56ab1f18415d8","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-d681910f-6abe-427b-bbec-7f5cf65314b4", "42788024f085c76c3064d3ff4165495f4bdc98f1");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-d681910f-6abe-427b-bbec-7f5cf65314b4", "42788024f085c76c3064d3ff4165495f4bdc98f1");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw40_56ab1f18415d8"},"id":"rgw40_56ab1f18415d8","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/259400035_Auto-Encoding_Variational_Bayes","requestToken":"Ghu2GKqZHqxGGqZSSW+Sth7PrEV2eNex7qAyGjnnngK4TC4ZM8\/aV0fQvyWO6jySywzATjeKPSUxaJWObu3fgdl3z2odpjOjOqr2CnLFEnP5H3gW459NHYnCPwhKqYzV0I5dC0HjSsJIhxLCK6IZfAm+zdSj6QlRlrLZx4QSgH\/DO4+qN8DwUMy4UiTiLpzFQTXDZ1KCB\/bC8y\/Ig\/4+7js9zDf\/2WJTd+599z31rnJyA5ofA2dQoPZt9CMGF7K5P2UeDsGmJiVZHndpGCcQ8qRSN775Kz4z49LMsB5Ur+A=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=PDEvhRJe2MznX_M2cFMDVtB2lA9_4YZjC9LxPlNouSYlZs2nUO3kdQQ74UEm3Jjj","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjU5NDAwMDM1X0F1dG8tRW5jb2RpbmdfVmFyaWF0aW9uYWxfQmF5ZXM%3D","signupCallToAction":"Join for free","widgetId":"rgw42_56ab1f18415d8"},"id":"rgw42_56ab1f18415d8","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw41_56ab1f18415d8"},"id":"rgw41_56ab1f18415d8","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw43_56ab1f18415d8"},"id":"rgw43_56ab1f18415d8","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
