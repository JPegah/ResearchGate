<!DOCTYPE html> <html lang="en" class="" id="rgw35_56aba22f19918"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="QZGqFrPPzw+qDr/Pimu2hqZKA/cEi+iXn/FgSIWotypVbaE5t/WJelN8eol6IrxVv+65gwZpjgyAOBCve11o9XPp2oVKA0oD08TOJLs3BzI7HL4ElTbr8pdW7pj5NRLInzT1cnjj7p97i/h0YyHoWRPMOnwGJ+Tx0J9b7IBXH2A2h568QiDQYvOEfphVKqR0PB7M++hxf42fPhuM1yVP2RpDy0GL9WMo0O4+bzKVGQpgnHcm58AooGsUavMYoAgdv3wLUlpuUrJVR8gR+VjdWqWzsbJXWn4EUNN+mX3thZw="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-c64b4405-4ef6-4be5-80a7-58c65c33f935",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="ParEGO: A Hybrid Algorithm With On-Line Landscape Approximation for Expension Multiobjective Optimization Problems" />
<meta property="og:description" content="This paper concerns multiobjective optimization in scenarios where each solution evaluation is financially and/or temporally expensive. We make use of nine relatively low-dimensional,..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems/links/0deec530c884b04ef0000000/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems" />
<meta property="rg:id" content="PB:3418871" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1109/TEVC.2005.851274" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="ParEGO: A Hybrid Algorithm With On-Line Landscape Approximation for Expension Multiobjective Optimization Problems" />
<meta name="citation_author" content="Joshua Knowles" />
<meta name="citation_publication_date" content="2006/03/01" />
<meta name="citation_journal_title" content="IEEE Transactions on Evolutionary Computation" />
<meta name="citation_issn" content="1089-778X" />
<meta name="citation_volume" content="10" />
<meta name="citation_issue" content="1" />
<meta name="citation_firstpage" content="50" />
<meta name="citation_lastpage" content="66" />
<meta name="citation_doi" content="10.1109/TEVC.2005.851274" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Joshua_Knowles/publication/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems/links/0deec530c884b04ef0000000.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>ParEGO: A Hybrid Algorithm With On-Line Landscape Approximation for Expension Multiobjective Optimization Problems (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: ParEGO: A Hybrid Algorithm With On-Line Landscape Approximation for Expension Multiobjective Optimization Problems on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56aba22f19918" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56aba22f19918" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56aba22f19918">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1109%2FTEVC.2005.851274&rft.atitle=ParEGO%3A%20A%20Hybrid%20Algorithm%20With%20On-Line%20Landscape%20Approximation%20for%20Expension%20Multiobjective%20Optimization%20Problems&rft.title=Evolutionary%20Computation%2C%20IEEE%20Transactions%20on&rft.jtitle=Evolutionary%20Computation%2C%20IEEE%20Transactions%20on&rft.volume=10&rft.issue=1&rft.date=2006&rft.pages=50%20-%2066&rft.issn=1089-778X&rft.au=Joshua%20Knowles&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">ParEGO: A Hybrid Algorithm With On-Line Landscape Approximation for Expension Multiobjective Optimization Problems</h1> <meta itemprop="headline" content="ParEGO: A Hybrid Algorithm With On-Line Landscape Approximation for Expension Multiobjective Optimization Problems">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems/links/0deec530c884b04ef0000000/smallpreview.png">  <div id="rgw8_56aba22f19918" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56aba22f19918" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Joshua_Knowles" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272291355426850%401441930726775_m/Joshua_Knowles.png" title="Joshua Damian Knowles" alt="Joshua Damian Knowles" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Joshua Damian Knowles</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw10_56aba22f19918" data-account-key="Joshua_Knowles">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Joshua_Knowles"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272291355426850%401441930726775_l/Joshua_Knowles.png" title="Joshua Damian Knowles" alt="Joshua Damian Knowles" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Joshua_Knowles" class="display-name">Joshua Damian Knowles</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_of_Birmingham" title="University of Birmingham">University of Birmingham</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">  <div> Sch. of Chem., Univ. of Manchester, UK </div>      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/1089-778X_IEEE_Transactions_on_Evolutionary_Computation"><span itemprop="name">IEEE Transactions on Evolutionary Computation</span></a> </span>    (Impact Factor: 3.65).     <meta itemprop="datePublished" content="2006-03">  03/2006;  10(1):50 - 66.    DOI:&nbsp;10.1109/TEVC.2005.851274           <div class="pub-source"> Source: <a href="http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=1583627" rel="nofollow">IEEE Xplore</a> </div>  </div> <div id="rgw11_56aba22f19918" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>This paper concerns multiobjective optimization in scenarios where each solution evaluation is financially and/or temporally expensive. We make use of nine relatively low-dimensional, nonpathological, real-valued functions, such as arise in many applications, and assess the performance of two algorithms after just 100 and 250 (or 260) function evaluations. The results show that NSGA-II, a popular multiobjective evolutionary algorithm, performs well compared with random search, even within the restricted number of evaluations used. A significantly better performance (particularly, in the worst case) is, however, achieved on our test set by an algorithm proposed herein-ParEGO-which is an extension of the single-objective efficient global optimization (EGO) algorithm of Jones et al. ParEGO uses a design-of-experiments inspired initialization procedure and learns a Gaussian processes model of the search landscape, which is updated after every function evaluation. Overall, ParEGO exhibits a promising performance for multiobjective optimization problems where evaluations are expensive or otherwise restricted in number.</div> </p>  </div>   </div>      <div class="action-container"> <div id="rgw12_56aba22f19918" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw26_56aba22f19918">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw27_56aba22f19918">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Joshua_Knowles/publication/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems/links/0deec530c884b04ef0000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Joshua_Knowles">Joshua Damian Knowles</a>, <span class="js-publication-date"> Feb 25, 2014 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw29_56aba22f19918" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw30_56aba22f19918" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw31_56aba22f19918" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw32_56aba22f19918" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw33_56aba22f19918" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw34_56aba22f19918" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw28_56aba22f19918" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FJoshua_Knowles%2Fpublication%2F3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems%2Flinks%2F0deec530c884b04ef0000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw25_56aba22f19918"  itemprop="articleBody">  <p>Page 1</p> <p>50 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 1, FEBRUARY 2005<br />ParEGO: A Hybrid Algorithm With On-Line<br />Landscape Approximation for Expensive<br />Multiobjective Optimization Problems<br />Joshua Knowles<br />Abstract—This paper concerns multiobjective optimization in<br />scenarios where each solution evaluation is financially and/or tem-<br />porally expensive.We make use of nine relativelylow-dimensional,<br />nonpathological, real-valued functions, such as arise in many ap-<br />plications, and assess the performance of two algorithms after<br />just 100 and 250 (or 260) function evaluations. The results show<br />that NSGA-II, a popular multiobjective evolutionary algorithm,<br />performs well compared with random search, even within the<br />restricted number of evaluations used. A significantly better per-<br />formance (particularly, in the worst case) is, however, achieved on<br />our test set by an algorithm proposed herein—ParEGO—which<br />is an extension of the single-objective efficient global optimization<br />(EGO) algorithm of Jones et al. ParEGO uses a design-of-exper-<br />iments inspired initialization procedure and learns a Gaussian<br />processes model of the search landscape, which is updated after<br />every function evaluation. Overall, ParEGO exhibits a promising<br />performance for multiobjective optimization problems where<br />evaluations are expensive or otherwise restricted in number.<br />Index Terms—Design and analysis of computer experiments<br />(DACE), efficient global optimization (EGO), expensive black-box<br />functions, Kriging, landscape approximation, metamodels, mul-<br />tiobjective optimization, nondominated sorting genetic algorithm<br />II (NSGA-II), Pareto optima, performance assessment, response<br />surfaces, test suites.<br />I. INTRODUCTION<br />A<br />search methods in their disciplines, a new and greater variety<br />of real-world optimization problems is gradually coming to<br />light. Many of the problems fall right into the “backyard” of<br />evolutionary computation (EC) methods—especially those for<br />whichconventional techniquesare not easilyadapted,including<br />nonconvex, mixed integer, nonlinear, constrained, and/or noisy<br />cost functions; for all these problems we may happily look<br />forward to a wealth of EC success stories. But some other<br />problems now appearing seem to pose a particular challenge to<br />EC and other heuristic search methods because they combine<br />the requirement of optimizing multiple, incommensurable<br />objectives with the feature of being prohibitively expensive to<br />evaluate. It is on these latter problems that we focus in this<br />paper.<br />S SCIENTISTS and engineers become increasingly<br />aware of the potential benefits of applying heuristic<br />Manuscript received October 8, 2004; revised April 20, 2005. The work<br />of J. Knowles was supported in part by a David Phillips Fellowship from the<br />Biotechnology and Biological Sciences Research Council (BBSRC), U.K.<br />The author is with the School of Chemistry, University of Manchester, Man-<br />chester M60 1QD, U.K. (e-mail: j.knowles@manchester.ac.uk; http://dbk.ch.<br />umist.ac.uk/knowles/).<br />Digital Object Identifier 10.1109/TEVC.2005.851274<br />More precisely, our motivation comes from a number of op-<br />timization scenarios arising in the experimental sciences. These<br />have a more-or-less common list of features that we can take to<br />loosely specify a class of problems, as follows:<br />1) the problem has multiple, possibly incommensurable,<br />objectives;<br />2) the time taken to perform one evaluation is of the order of<br />minutes or hours;<br />3) only one evaluation can be performed at one time (no par-<br />allelism is possible);<br />4) the total number of evaluations to be performed is limited<br />by financial, time or resource constraints;<br />5) no realistic simulator or other method of approximating<br />the full evaluation is readily available;<br />6) noise is low (repeated evaluations yield very similar<br />results);<br />7) the overall gains in quality (or reductions in cost) that can<br />be achieved are high;<br />8) the search landscape is locally smooth but multimodal;<br />9) the dimensionality of the search space is low-to-medium.<br />Although problems exhibiting (many of) these features in-<br />clude some familiar ones from engineering design optimization<br />(e.g., [29]), there are also less well-known combinatorial bio-<br />chemistry and materials science applications [14], [24], [58],<br />as well as instrument configuration problems [54], [55], in this<br />class.<br />An example of the latter which motivates the work herein<br />is [45]. It reports on a series of experiments directed at im-<br />proving the viability of metabolomics,1by optimizing the con-<br />figurationofaGC-TOFmassspectrometer(GC-MS).Threeob-<br />jectives in the optimization were considered: 1) maximizing the<br />signal-to-noise ratio in the chromatogram; 2) maximizing the<br />numberof“true”peaksinthechromatogram;and3)minimizing<br />the processing time—the time for the instrument to analyze one<br />sample. Features 2–4 in the list given above arise in this partic-<br />ular problem because the evaluations are actually “wet experi-<br />ments”2that must be performed on a physical machine (which<br />is itself very costly and, hence, only one is available), and may<br />also require costly consumables and/or the use of highly skilled<br />operators. Feature 5 applies here because it is practically infea-<br />sible to simulate accurately the output of such a complicated<br />1Metabolomics [2] relies on efficiently identifying the hundreds or thousands<br />of different compounds within a biological sample to enable metabolic pro-<br />cesses to be accurately monitored.<br />2The term used in biology for laboratory as opposed to computer-based<br />(“dry”) experiments.<br />1089-778X/$20.00 © 2006 IEEE</p>  <p>Page 2</p> <p>KNOWLES: ParEGO: A HYBRID ALGORITHM WITH ON-LINE LANDSCAPE APPROXIMATION 51<br />instrument over a range of chemical inputs. Thus, considering<br />2–5, it is clear that the evolutionary algorithm (EA) or other op-<br />timization methods must search the space in a smaller number<br />of evaluations than in many other EA applications.<br />Fortunately, features 6–9 reduce the difficulty of the task<br />somewhat. Low noise means that individual experiments need<br />not be repeated, and there were only eight real-valued settings<br />of the GC-MS (i.e., search space dimensions). Moreover, it<br />is known from human experts that getting the configuration<br />“right” yields significant improvement in the chromatograms;<br />since this is usually achieved from hand-tuning by experts,<br />the search space must be, at least locally, smooth. Therefore,<br />reasonably large basins of attraction probably exist. However,<br />it is also known that some limited epistasis does exist [54].<br />In the experiments reported in [45], a multiobjective evolu-<br />tionary algorithm (MOEA), PESA-II [12], was used to perform<br />theoptimization[45]andsome180GC-MSconfigurationswere<br />assayed in all (taking several days and consuming expensive in-<br />strument-time). At the end, a particular configuration that of-<br />fered the best compromise solution was chosen from the Pareto<br />front obtained. This yielded a threefold increase in the number<br />of peaks observed, coupled with a small increase in the poten-<br />tial throughput (i.e., future samples could be processed more<br />quickly), over the hand-tuned configuration usually employed.<br />The choice of employing an MOEA was motivated primarily<br />by the feature 1 above, and the wish to explore the tradeoffs of<br />the three objectives, enabling the input of expert knowledge in<br />the final choice of GC-MS configuration. The ability to do this<br />from a single optimization run, with little or no knowledge of<br />the cost landscape, makes an MOEA approach a logical choice.<br />Moreover, results reported in [45] show that GC-MS config-<br />urations from later experiments were statistically significantly<br />better than earlier configurations (computed using a Fisher’s<br />permutation test), suggesting that the MOEA used was better<br />than a random search would have been.<br />Nonetheless, the scientific justification for theuse of MOEAs<br />inthisparticularoptimizationcontextisnotassolidaswewould<br />like because, although MOEAs have been applied in a wealth of<br />different contexts and have exhibited competitive performance<br />againstseveralotheroptimizationtechniques,theirperformance<br />when the number of function evaluations is severely limited is<br />largely untested in the literature (though see [36] for some ex-<br />periments underpinning the use of PESA-II for the GC-MS op-<br />timization). A central aim of this paper is, thus, to undertake<br />testing of an MOEA in contexts similar to the GC-MS con-<br />figuration problem, over a realistically small number of func-<br />tion evaluations. To this end, we choose to employ NSGA-II,<br />a well-respected, modern example of an MOEA,3and test it,<br />using a small population size, over just 100 and 260 function<br />evaluations.<br />In many engineering applications that share similar features<br />to our scenario above, standard EAs are eschewed because<br />of their low efficiency with respect to function evaluations. A<br />wealth of other global optimization methods (see next section)<br />use more principled methods of exploring the search space<br />3The IEEE TEC paper describing NSGA-II for multiobjective optimization<br />was judged as the “Fast-Breaking Paper in Engineering” by Web of Science<br />(ESI) in February 2004.<br />under these restricted conditions, most notably those methods<br />based on modeling the landscape online during the search.<br />However, to our knowledge, the performance of these methods<br />has never been compared with an MOEA in a straightforward<br />Pareto optimization context. Thus, a second aim of this paper is<br />to extend such an algorithm to do multiobjective optimization,<br />and to compare it with the NSGA-II. We choose for this, a<br />frequently cited algorithm from the global optimization lit-<br />erature, specifically designed for expensive functions of low<br />dimension—the efficient global optimization (EGO) algorithm<br />[35]. The version that we propose for Pareto optimization is<br />given the designation “ParEGO” from here on.<br />To undertake the performance assessment of ParEGO and<br />NSGA-II, we do not use expensive functions here, but employ<br />a suite of test functions enabling us to collect performance data<br />over multiple algorithm runs. The selection of problems in the<br />suite includes functions exhibiting a diverse range of problem<br />difficulties, yet is restricted to problems that we think model<br />scientific/engineering contexts like the GC-MS optimization<br />problem. For this reason, the problems are relatively low-di-<br />mensional (having up to eight real-valued dimensions), are not<br />pathologically rugged, and have either two or three objectives.<br />In measuring performance, we give some rough judgments<br />on the worst-case performance in addition to applying normal<br />statistical tests to estimate the location of the distribution.<br />Worst-case performance is of particular relevance in scenarios<br />where only one shot at the experiments/optimization is usually<br />possible, although it is difficult to estimate empirically. The<br />choice of test functions and the methods for measuring perfor-<br />mance used here should provide a basis for future comparison<br />of other algorithms for expensive, low-dimensional multiobjec-<br />tive problems. To facilitate this, problems, assessment methods,<br />and results are available for download at [37].<br />Organization: The rest of this paper is organized as follows.<br />In Section II, a review of relevant literature in engineering<br />optimization methods, landscape approximation, and MOEAs<br />is given. An outline of EGO, a powerful global optimization<br />method for expensive functions, is provided in Section III,<br />together with results demonstrating our implementation of the<br />approach. Section IV extends EGO to the multiobjective algo-<br />rithm, ParEGO, which we propose here. Section V describes<br />the choice of test functions, and Section VI details our methods<br />for performance assessment and comparison. Section VII sets<br />out the method of comparison, giving all parameter settings,<br />Section VIII presents the results, and Section IX provides a<br />summary and conclusion.<br />II. RELATED WORK<br />The rise of EAs for multiobjective optimization in recent<br />years is now well documented, with thorough reviews of the<br />history and current state of the art available in [6], [9], and [16].<br />Much of the success that the field is enjoying relies on the flex-<br />ibility with which MOEAs can deal with various optimization<br />contexts and features, such as: high dimensionality; integer,<br />real and mixed parameters; lack of knowledge about ranges<br />of fitness functions; nondifferentiability of the cost landscape;<br />constraints, and so on. The initial high computational overhead</p>  <p>Page 3</p> <p>52 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 1, FEBRUARY 2005<br />of some early methods has also been overcome by a gradual<br />progression with the introduction of more efficient algorithms<br />such as niched Pareto genetic algorithm (NPGA) [30] and<br />Pareto archived evolution strategy (PAES) [40], which first<br />attempted to use reduce the cost of niching, through to the<br />micro-GA [7], the popular NSGA-II [17], and, more recently,<br />particle-swarm optimization algorithms [8]. More efficient data<br />structures that can be used to speed up the niching, selection,<br />and archiving processes that these MOEAs rely upon are also<br />now available [32].<br />In engineering, MOEAs have found many applications [15],<br />from airplane wing optimization [29], [44] and spacecraft tra-<br />jectory planning [13] to the design of irrigation networks [5];<br />several of these obviously entail expensive simulation or exper-<br />imental steps. Nonetheless, and perhaps due to the focus made<br />on improvements to the computational overhead of MOEA’s<br />per evaluation, their effectiveness in applications where very<br />few function evaluations can be afforded has only recently been<br />investigated in a few isolated papers. For example, two MOEAs<br />that sample the search space by making explicit use of all pre-<br />viously visited points, and explicitly avoid, oversampling of fit<br />regions, were proposed in [31], and in [25], the case of dynamic<br />multiobjective optimization, where the fitness function changes<br />overtime,requiringarapidreoptimizationtotrackit,wastackled<br />using a hybrid approach combining deterministic local search<br />routines that are efficient in function evaluations, with a (1<br />1)-ES. These kinds of methods appear to be promising, outper-<br />forming more conventional MOEAs on the test problems used,<br />when the number of function evaluations is the limiting factor.<br />Nonetheless,itwouldseemthatfurthersavingsofpreciousfunc-<br />tion evaluations could be made by learning the cost landscape<br />from all previously visited points, and by using this to estimate<br />the “best” place to sample next (either to improve the model the<br />most or to obtain a better solution, or both).<br />Learning a cost landscape from a set of solution/cost pairs<br />is variously called surrogate, approximate, or metamodeling<br />in the literature. The idea of selecting the next solution to<br />sample in order to maximize the improvement in the model<br />is known as active learning [10] in the machine intelligence<br />literature. These approaches have received little attention (and<br />only very recently) in evolutionary multiobjective optimization<br />but metamodeling has a relatively rich history in optimization<br />of a single objective. In design engineering, metamodeling<br />is usually known as the response surface method [1], [42],<br />and involves fitting a low-order polynomial via some form of<br />least squares regression. A closely related approach, deriving<br />from geology is Kriging, whereby Gaussian process models<br />are parameterised by maximum-likelihood estimation. A par-<br />ticular example of this is known as the design and analysis<br />of computer experiments (DACE) model [52], which forms<br />the basis of the EGO algorithm, described in the next section.<br />Optimization methods using response surfaces and/or Kriging<br />metamodels have been successfully used in aeronautical de-<br />sign/engineering applications where only tens or hundreds of<br />function evaluations are possible [57]. In the case of EGO,<br />four low-dimensional multimodal test functions have been<br />optimized to within 1% of optimal in the order of 100 function<br />evaluations [35].<br />In the EA community, response surface and Kriging methods<br />havealsobeenusedwithinEAs,tomodeleitherthegloballand-<br />scape [4], [21], [29], [41], [50] or a local region of it [51], in<br />order to reduce the number of expensive function evaluations<br />thatneedtobe carriedoutduringoptimization.Inaddition,sim-<br />ilar goals have been obtained using artificial neural networks<br />(either MLPs or RBF networks). Detailed reviews of all these<br />metamodel-basedEAscannowbefoundinrecentreviewpapers<br />[33], [34], [48], along with new approaches that try to manage<br />the metamodels optimally with respect to the conflicting con-<br />cerns of optimization and the design of experiments (i.e., im-<br />proving the model) [47], [48]. Finally, a related but somewhat<br />different approach was proposed recently in [11], in which op-<br />timization of an expensive function is tackled by modeling fit-<br />nesstransitionsusinga finitestatemachine and,offline,running<br />a number of different EAs on the model, in order to choose the<br />most efficient one. This approach may be attractive when one<br />has many optimization tasks to do on the same or very similar<br />landscapes but a certain investment is needed to build up a suf-<br />ficiently good model.<br />Returning to the case of multiple objective functions, we see<br />that a few recent papers have begun to investigate the use of<br />metamodels in MOEAs. Most of these have considered the use<br />of neural networks. The study in [43] demonstrates the use of<br />a neural network approximation combined with the NSGA-II<br />algorithm; the neural net having<br />tive function. In this approach, generations using the real evalu-<br />ationfunctionare alternated withgenerations usingamultilayer<br />perceptron model of the function derived from the samples col-<br />lected from the earlier generations. Some speedup is observed<br />overusingtheoriginalexactfitnessfunctionalone,butthestudy<br />is limited to a single, curve-fitting problem. More recently, [27]<br />and [28] propose and compare two more methods employing<br />neural networks. The first method follows Nain and Deb’s ap-<br />proach, though with a different MOEA being used. The second<br />methodisasignificantdeparturefrommostothermetamodeling<br />schemes, however: an inverse neural network is used to map<br />back from a desired point in the objective space (beyond the<br />current Pareto front) to an estimate of the decision parameters<br />that would achieve it. Test function results presented in [28] for<br />the latter look particularly promising, though fairly long runs<br />(of 20000 evaluations) are considered, and it is not clear that on<br />much shorter runs the method would offer significant gains. Fi-<br />nally, a different type of neural network, a self-organizing map,<br />has been employed in [3] to replace standard variation opera-<br />torswithadaptiveones.However,sofar,testshavenotexplicitly<br />comparedtheperformanceofthisapproachwithothermethods.<br />Mostrecently,threepapers[20],[22],and[23]haveappeared<br />that use a Kriging approach based on the DACE model [52]<br />within an MOEA. This approach is the most similar to the one<br />developed independently by the author and described herein.<br />outputs, one for each objec-<br />III. EGO ALGORITHM<br />The EGO algorithm for global optimization of expensive<br />black-box functions was first introduced and described in [35].<br />It makes use of “Kriging” to model the search landscape from<br />solutions visited during the search. More specifically, it exploits</p>  <p>Page 4</p> <p>KNOWLES: ParEGO: A HYBRID ALGORITHM WITH ON-LINE LANDSCAPE APPROXIMATION53<br />a version of the DACE model of [52], based on Gaussian pro-<br />cesses. We do not give a mathematical description of EGO or<br />DACE here, but simply provide some motivation for its choice<br />in this study, and a brief description of the main procedures<br />involved. The reader is referred to [35] for a fuller explanation.<br />TheDACEmodelusedinJones[35],likeanyothermethodof<br />supervisedlearning,issubjecttocertainno-free-lunchtheorems<br />[59], similar to those for optimization, that means its general<br />application cannot be recommended on any theoretical basis.<br />Nonetheless, on functions where we expect a degree of local<br />smoothness, where noise is low, and the number of dimensions<br />is not excessive,the DACE model seems to be a good choice for<br />building the kind of approximation that is needed by a search<br />algorithm restricted to a low number of function evaluations. In<br />particular:<br />•<br />The model always interpolates the points (independently<br />of the chosen parameters of the model).<br />Themodelhasasmall,fixednumberofparameters:<br />of them, where<br />is the dimension of the function to be<br />optimized.<br />The likelihood of the model given the data has a simple<br />closed form expression from which it is possible to<br />compute the maximum-likelihood model—giving the<br />approach a sound statistical interpretation.<br />The error in the expected cost of a solution also has a<br />simple,closedformexpression.Thus,themodelestimates<br />its own uncertainty.<br />•<br />•<br />•<br />Thelastpointaboveisofparticularrelevance.Knowledgeofthe<br />errorintheresponsesurfaceisausefulpropertywhensearching<br />a costlandscape,and EGOmakesuseofthispropertyexplicitly,<br />as we explain below.<br />The EGO algorithm begins by first generating a number of<br />solutions in a latin hypercube (i.e., a space-filling design), and<br />by then finding the maximum-likelihood DACE model that best<br />explains these solutions (making use of some suitable optimiza-<br />tion algorithm). To generate a new solution to evaluate, EGO<br />searches for the solution that maximizes what Jones et al. [35]<br />call “the expected improvement”—the part of the curve of the<br />standard error in the model that lies below the best cost sam-<br />pled so far (see Fig. 1). This effectivelymeans that EGO weighs<br />up both the predicted value of solutions, and the error in this<br />prediction, in order to find the one that has the greatest poten-<br />tial to improve the minimum cost. EGO does NOT just choose<br />the solution that the model predicts would minimize the cost.<br />Rather, it automatically balances exploitation and exploration:<br />where a solution has low predicted cost and low error, it may<br />not be as desirable as a solution whose predicted cost is higher<br />but whose associated error of prediction is also higher. Impor-<br />tantly, the EGO algorithm uses a closed form expression for the<br />expected improvement, and it is thus possible to search the de-<br />cision space globally for the solution that maximizes this. Once<br />a new solution has been chosen and evaluated (using the true,<br />expensive cost function), the DACE model is updated with this<br />new information, and the next solution is chosen using this up-<br />dated model.<br />Computing the maximum-likelihood model at each step of<br />EGO requires several hundred or thousand matrix inversions on<br />a square matrix of dimension equal to the number of solutions<br />Fig. 1.<br />above) can be treated as if the value there were a realization of a normal random<br />variable with mean and standard deviation given by the DACE model and its<br />standard error [adapted from [35]].<br />The uncertainty about the function’s value at a point (such as ? ? ?<br />so far sampled. As such, it is rather expensive in terms of com-<br />putation per evaluation, and this overhead increases with time.<br />However,ifEGO is efficientwithrespectto thenumberof func-<br />tion evaluations needed to reach a given cost, the computational<br />costcanbeconsideredirrelevantinmanyoptimizationcontexts.<br />Before we go on to explain how we extend EGO to the multi-<br />objective optimization case, we first give details of our own im-<br />plementation of the basic algorithm, and some illustrative test<br />results.<br />A. Implementation of EGO<br />EGO was implemented in C using the open source matpack<br />library4for coding the matrix operations. The initial solutions<br />are generated using a latin hypercube routine, following a de-<br />scription in Numerical Recipes [49]. The number of initial so-<br />lutionsissetto ,where<br />to be optimized, as suggested in [35].<br />In order to optimize the likelihood function, the Nelder and<br />Mead downhill simplex routine from Numerical Recipes [49]<br />has been used. This is restarted 20 times in order to give a more<br />robust search of the model space. In order to find the best solu-<br />tion to visit next, a genetic algorithm is embedded within EGO,<br />to search the decision parameter space globally. We have not<br />used branch and bound to find the best solution to visit, as sug-<br />gested by Jones et al., simply because of the difficulty of imple-<br />menting it, particularly for the more complicated case of con-<br />strained optimization, which we would like to tackle in future.<br />Because the computational overhead increases with each<br />function evaluation, we have found it necessary to cap the size<br />of the correlation matrix (on which the model is based) to have<br />a maximum dimension of 80.5This means that beyond the<br />80th function evaluation, 80 solutions are selected at random<br />without replacement from the list of all previously visited<br />solutions—and the DACE model for that iteration is based on<br />these. Potentially, this reduces the accuracy and reliability of<br />EGO (potentially causing it to revisit points). However, beyond<br />matrix sizes of about 80–100, EGO was taking several minutes<br />isthedimensionofthefunction<br />4www.matpack.de.<br />5A different library for matrix operations (from http://cpplapack.source-<br />forge.net/doc/html/), based on ATLAS, is being investigated with the hope that<br />it will reduce this problem.</p>  <p>Page 5</p> <p>54 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 1, FEBRUARY 2005<br />TABLE I<br />RESULTS OF RUNNING OUR IMPLEMENTATION OF EGO ON 6 OF OUR 13 TEST FUNCTIONS.<br />THE REMAINING SEVEN ARE IN TABLE II<br />Column 1 gives the name of the function, while columns 2 and 3 give, respectively, its dimension ? and its<br />number of local optima. Column 4 shows the median best evaluation reached from 51 runs of EGO, where<br />the number of function evaluations used in each run, is just ???, where ? is the dimension of the function.<br />Columns 5 and 6 show the median evaluation reached by 51 runs of random search for the number of func-<br />tion evaluations indicated. The median value which is closest to EGO’s is shown in bold font. Columns 7<br />and 8 summarize the results of Mann–Whitney rank-sum tests between EGO and the random search algo-<br />rithm. The “EGO wins” column gives the number of iterations that a random search can be run (in terms<br />of a multiple of EGO’s number of iterations), and still be statistically beaten by EGO (with highest asso-<br />ciated confidence level). The final column indicates the same thing, but where no difference between the<br />algorithms’ performance can be found (and the lowest confidence level at which the test would have found<br />a difference had there been one). On five test functions, EGO is 250 times faster than random search, on<br />four others it is between 50 and 250 times faster. An interpretation of the results is that, e.g., on ?<br />finds a function value that is shared or bettered by less than approximately ???????? ? ?????? of the<br />entire search space, in just 40 function evaluations. EGO generally performs worst on the functions with<br />very many local optima, though it does well on both the generalized Griewank and Schwefel functions (here<br />with six and ten dimensions, respectively, rather than the usual 30—see TABLE II).<br />, EGO<br />per evaluation (and this begins to increase dramatically). For<br />testing, this is prohibitive, although in practice a function eval-<br />uation can easily take much longer than this. In ParEGO, we<br />also cap the size of the model but use a slightly more advanced<br />selection procedure, as described in Section IV.<br />B. Testing the Implementation of EGO<br />Testing of our implementation of EGO was carried out on<br />multimodal (epistatic) functions, up to a dimension of 10, taken<br />from [41]. On each function, we ran EGO (51 times) for just<br />evaluations (where is the dimension of the function). In<br />ordertogetsomeideaofthequalityoftheseresults,wecompare<br />with a random search algorithm (Tables I and II), and with the<br />publishedresultsofLiangetal.[41],whoimplementanEAalso<br />withlandscapeapproximation(TableIII).Theformerresultsare<br />generally more useful: these allow one to estimate the quality of<br />solutions reached by EGO in terms of the number of solutions<br />inthesearchspace thathavethatcostora lowerone.Theresults<br />show that EGO is up to 250 times faster than random search (on<br />four of the functions), more than 50 times faster on a further<br />four, and, at worst, about the same as random search on one<br />highly multimodal function.<br />The results presented in Table III are useful only in that they<br />suggest EGO is performing very well on some functions (far<br />better than the EA of Liang et al.); but in general, it is hard to<br />make comparisons because Liang et al.’s algorithm is run for<br />1 to 3 orders of magnitude longer (in terms of evaluations) than<br />we run EGO for.<br />IV. EXTENDING EGO TO THE MULTIOBJECTIVE CASE: ParEGO<br />The EGO algorithm could be extended for use with multi-<br />objective optimization problems in a number of different ways.<br />The simplest approach, which we investigate here, converts the<br />different cost values of a solution into a single cost via a pa-<br />rameterized scalarizing weight vector. By choosing a different<br />(parameterization of the) weight vector at each iteration of the</p>  <p>Page 6</p> <p>KNOWLES: ParEGO: A HYBRID ALGORITHM WITH ON-LINE LANDSCAPE APPROXIMATION55<br />TABLE II<br />RESULTS ON THE REMAINING SEVEN TEST FUNCTIONS. SEE TABLE I CAPTION. THE NUMBER<br />OF LOCAL OPTIMA IN THE GRIEWANK FUNCTION IS NOT KNOWN—THOUGH FOR<br />A 30-DIMENSIONAL VERSION OF THE SAME FUNCTION IT IS ? 10<br />TABLE III<br />COMPARISON OF EGO WITH PUBLISHED RESULTS OF RUNNING<br />EANA [41] ON THE 11 LOWER DIMENSIONAL FUNCTIONS<br />For EGO, we ran for exactly ??? function evaluations (column 2) and quote<br />the mean and best cost found from 51 runs (columns 3 and 4, respectively).<br />For EANA, the algorithm was stopped when it reached either the global op-<br />timum (up to the precision of the computer) or some unspecified evalua-<br />tion limit [41]. Thus, the mean number of evaluations is reported (column 5)<br />and the mean cost of the solution over the 50 runs performed (last column).<br />Comparison is difficult but on some functions, EGO occasionally reaches<br />comparablefitnesslevelsoneortwoordersofmagnitudefasterthanthemean<br />number of function evaluations quoted for EANA. (EANA is an evolution<br />strategy, also, based on building landscape approximations.)<br />search, an approximation to the whole Pareto front can be built<br />up gradually.<br />ParEGO begins by normalizing the<br />specttotheknown(orestimated)limitsofthecostspace,sothat<br />each cost function lies in the range [ 0,1 ]. Then, at each itera-<br />tion of the algorithm, a weight vector<br />random from the set of evenly distributed vectors defined by<br />cost functions with re-<br />is drawn uniformly at<br />(1)<br />with<br />many vectors there are in total. The scalar cost of a solution is<br />then computed using the augmented Tchebycheff function<br />, so that the choice of determines how<br />(2)<br />where<br />be possible to use other scalarizing functions—the augmented<br />Tchebycheff function has been chosen here because the non-<br />linear part of the function means that points on nonconvex re-<br />gions of the Pareto front can be minimizers of this function and,<br />thus, nonsupported solutions can be obtained, while the linear<br />is a small positive value which we set to 0.05. It would</p>  <p>Page 7</p> <p>56IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 1, FEBRUARY 2005<br />part of the function ensures that solutions that are weakly domi-<br />nated by Pareto optimal solutions are rewarded less than Pareto<br />optimal ones. Using (2), the scalar costs of all previously vis-<br />itedsolutionsiscomputedand,usingalloraselectionofthese,a<br />DACEmodelofthelandscapeisconstructedbymaximum-like-<br />lihood. The solution that maximizes the expected improvement<br />with respect to this DACE model is determined. This becomes<br />the next point, and is evaluated on the real, expensive cost func-<br />tion, completing one iteration of ParEGO.<br />Pseudocode for the entire ParEGO algorithm is given in<br />Algorithm 1. As in our implementation of EGO, the Nelder<br />and Mead downhill simplex algorithm is used (with 20 restarts)<br />to maximize the likelihood of the DACE model (line 25 of<br />Algorithm 1). The EA used within ParEGO to search for the<br />solution which maximizes the expected improvement (line 28)<br />is implemented as follows.<br />•<br />Population size: 20 solutions.<br />•<br />Population update: Steady state (one offspring produced<br />per generation, from either a crossover or cloning event,<br />followed by a mutation).<br />•<br />Generations/evaluations: 10000 evaluations.<br />•<br />Reproductive selection: Binary tournament without<br />replacement.<br />•<br />Crossover: Simulated binary crossover [18] with proba-<br />bility 0.2, producing one offspring.<br />•<br />Mutation:Parametervalueshiftedby<br />eter range), where<br />is drawn uniformly at random from<br />(0.0001, 1), and<br />, the per- gene mutation probability, is<br />.<br />•<br />Replacement: Offspring replaces (first) parent if it is<br />better, else it is discarded.<br />•<br />Initialization: Five solutions are mutants6of the five best<br />solutions evaluated on the real fitness function under the<br />prevailing<br />vector; the remaining 15 solutions are gener-<br />ated in a latin hypercube in parameter space.<br />In practice, on a very expensive cost function, all solutions<br />evaluated should be used to update the DACE model, at every<br />iteration. However, in our experiments (because of the need to<br />do21runsonalargenumberoffunctionstocollectperformance<br />data), we used a simple, heuristic method of choosing a subset<br />of the solutions evaluated to update the model, as follows: At<br />each iteration: 1) if the iteration number<br />solutions evaluated so far, are used to update the<br />model and 2) if<br />a subset of<br />used, where the first half of them are the best<br />theprevailingscalarizingvector<br />at random without replacement.Further details of the parameter<br />settings used in ParEGO are given in Table V.<br />.(param-<br />is less than 25, all<br />solutions is<br />solutions under<br />andtheotherhalfareselected<br />V. TEST FUNCTION SUITE<br />A. Notes on the Selection of Functions<br />A number of good attempts at designing test function suites<br />and/or general schemes for test function generation have been<br />6The mutation is carried out as described above except that mutants are<br />checked to ensure they are different from their parents.<br />Alg. 1.ParEGO pseudocode.<br />proposed in the multiobjective optimization literature, of which<br />those described in [19], [46], and [56] are some of the best. The<br />DTLZ functions in [19] are very popular, partly because they</p>  <p>Page 8</p> <p>KNOWLES: ParEGO: A HYBRID ALGORITHM WITH ON-LINE LANDSCAPE APPROXIMATION57<br />are scalable in the number of objectives and parameters, and<br />partly because each function is accompanied by a description<br />explainingexactlywhatdifficultyitposestoanMOEA,andalso<br />allowing this to be easily tuned. However, while it is tempting<br />to employ this suite wholesale, there is (at least) one drawback<br />to it, which leads us to include functions from [46] and [56]<br />as well.<br />ThedrawbackinDebetal.’ssuiteisthatmostofthefunctions<br />work on a scheme where only<br />sition parallel to the Pareto front in the objective space, with all<br />the remaining parameters controlling distance to the front (i.e.,<br />the problems are separable). From this, a natural bias is given<br />to recombination-based EAs because, once a few points on the<br />true Pareto front are found, recombination tends to propagate<br />and preserve the parameters which are common to them (which<br />is all of them, bar the first<br />), while searching over the other<br />parameters, leading to a very fast spreading out over the entire<br />front. Unfortunately, many real-world multiobjective optimiza-<br />tion problems are not structured like this; rather, each different<br />optimal tradeoff solution requires changing all or many of the<br />decision parameters.<br />In [46], similar observations in respect of the DTLZ func-<br />tionshavebeenmade,andfurthermore,itwasalsoobservedthat<br />the Pareto fronts of many other test functions consist of piece-<br />wise linear curves and/or single points in the parameter space.<br />The paper goes on to propose a scheme for generating functions<br />that, like [19], allows some different problem difficulties to be<br />incorporated and tuned, combined with a greater control of the<br />curves/surfaces in parameter space comprising the Pareto front.<br />Twoexamplefunctionsusingthisgeneralschemearegiven,and<br />it is these that we employ in our suite.<br />The functions we employ from [56] are not part of an<br />overall tunable scheme but have been popular in the literature.<br />VLMOP2 has a concave Pareto front while VLMOP3’s Pareto<br />front is both nonlinear and asymmetric.<br />Overall, the final selection of nine test functions includes<br />functions from two to eight decision parameters; functions with<br />a very low density of solutions at the Pareto front; functions<br />with locally optimal Pareto fronts; functions where the Pareto<br />set follows a complicated curve in the parameter space; func-<br />tions where the Pareto front is disconnected in objective space;<br />and functions where the density of points parallel to the Pareto<br />front is nonuniformly distributed. There is, thus, a good deal of<br />variety in the difficulties that they pose. We have nonetheless<br />been restrictive in some particular aspects: all functions are un-<br />constrained and while difficult, are not overly high-dimensional<br />(in parameter space), and have a reasonable, rather than patho-<br />logical degree of ruggedness. And, we have kept to functions<br />of two and three objectives only. These restrictions accord with<br />our description (in Section I) of certain kinds of expensive en-<br />gineering/scientific problem, where we hope to obtain good re-<br />sults in a very small number of function evaluations.<br />In the following, we describe each of our test functions in<br />turn.<br />parameters control the po-<br />KNO1: This test function is the only one not borrowed from<br />the existing literature. There are just two parameters and two<br />objectives in the problem<br />The distancefrom the Paretofront, controlled by , is a function<br />of the sum of the two decision parameters, while the location<br />transverse to the Pareto front is controlled by the difference be-<br />tween the two parameters. Thus, each point on the Pareto front<br />is unique in both parameters: the Pareto set is made up of all<br />pairs of parameters that sum to 4.4116. The true Pareto front<br />has a greater extent than, and lies just beyond, a locally optimal<br />Pareto front with a much larger basin of attraction. Altogether,<br />there are 15 locally optimal fronts in this function.<br />OKA1: The OKA1 test function [46] is defined as<br />The Pareto optima (in the parameter space) lie on the curve<br />. In addition, the density of<br />solutions falls away near to the Pareto front.<br />OKA2: The OKA2 test function [46] is defined as<br />The Pareto optima lie on a spiral-shaped curve in the three-di-<br />mensional parameter space. In addition, the density of solutions<br />falls away steeply near to the Pareto front.<br />VLMOP2: Veldhuizen and Lamont’s MOP2 function [56] is<br />scalable in the number of decision variables. We use just two</p>  <p>Page 9</p> <p>58IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 1, FEBRUARY 2005<br />The Pareto front is concave and the Pareto optima lie on the<br />diagonal passing from<br />,<br />rameter space.<br />VLMOP3: Veldhuizen and Lamont’s MOP3 [56] is a three-<br />objective function of two decision variables<br />to, in pa-<br />This test function has a disconnected Pareto optimal set, and<br />the Pareto front is a curve “following a convoluted path through<br />objective space.”<br />DTLZ1a: We have selected four of the test functions de-<br />scribed in [19], and have made alterations to them in order to<br />keep the number of decision variables small, and/or to reduce<br />the ruggedness to “reasonable” levels. We append an “a” to the<br />name of these functions to show that we have changed them<br />slightly.<br />The first function from [19], DTLZ1, is used with two objec-<br />tives and six decision variables here. This gives the following<br />function DTLZ1a:<br />The ruggedness of the function is controlled by the<br />In the original DTLZ1,<br />total of<br />local Pareto optimal fronts. This is excessively<br />rugged for our purposes, hence, we use<br />optimal set consists of all solutions where all but the first de-<br />cision variables are equal to 0.5, and the first decision variable<br />may take any value in [0,1].<br />DTLZ2a and DTLZ4a: The next two test problems have<br />three objective functions defined on eight decision variables.<br />DTLZ2a and DTLZ4a are given by<br />function.<br />is used in the cosine term, giving a<br />instead. The Pareto<br />if function is DTLZ2a<br />if function is DTLZ4a<br />The Pareto front is 1/8 of a sphere of radius1, centered on 0,0,0.<br />The Pareto optimal set consists of all solutions where all but<br />the first decision variables are equal to 0.5, and the first deci-<br />sion variable may take any value in [0,1]. The effect of setting<br />is to severely bias the density distribution of solutions<br />toward the<br />and<br />DTLZ7a: This problem has four disconnected regions in the<br />Pareto front (in objective space)<br />planes.<br />VI. SELECTED PERFORMANCE ANALYSIS TECHNIQUES<br />The output of a multiobjective optimizer for a single run is<br />an approximation set: the set of all nondominated points found<br />during the run. Assessing the performance of optimizers, thus,<br />relies upon some means of measuring or comparing the quality<br />of one or more approximation sets. Many such performance in-<br />dicators7exist in the literature but it has been shown in some<br />recent works [38], [39], [61] that several of these are not consis-<br />tent with the partial ordering of approximation sets that follows<br />logically from the basic principles of Pareto optimization. For<br />example, in the worst case, two sets of nondominated points,<br />and may be compared, where every point in<br />by at least one point in<br />, and yet<br />under the chosen indicator [39]. This is particularly the case for<br />indicatorsthattrytoassessoneisolatedaspect,suchasthediver-<br />sity, extent, or proximity to the Pareto front, of a nondominated<br />set.Fortunately,indicatorsthatsimultaneouslytakeintoaccount<br />all of these aspects of what it means to be a good nondominated<br />set do exist, and moreover, they are compatible and/or complete<br />with respect to the true partial ordering of nondominated sets.<br />Two such “well-behaved indicators,” which we choose to use in<br />our experiments, are the hypervolume indicator and the binary<br />epsilon indicator described below. For a more detailed discus-<br />sionofassessmentmethods,thereaderisreferredto[61],where<br />a full classification of indicators is given.<br />In addition to choosing appropriate performance indicators,<br />one must also decide how to collate and interpret the results<br />from multiple runs, and how to present results graphically, if<br />at all. Since, in our optimization scenario, it is likely that there<br />would be just one (or, perhaps, two) shots at performing the<br />optimization, the average-case and worst-case performance are<br />paramount. Hence, for each, test function the following infor-<br />mation is given:<br />is dominated<br />yields a better evaluation<br />7We prefer the term “indicator,” following [61], because the terms “measure”<br />and“metric” have reservedmeanings in mathematicsthat not allindicators con-<br />form to.</p>  <p>Page 10</p> <p>KNOWLES: ParEGO: A HYBRID ALGORITHM WITH ON-LINE LANDSCAPE APPROXIMATION59<br />• the mean/median results of individual runs (as well as<br />standard deviation/interquartile ranges);<br />the statistical significance of differences in the distribu-<br />tions;<br />for the two-objective functions only, plots of the median,<br />and worst attainment surfaces achieved;<br />forthethree-objectivefunctionsonly,plotsoftheworstat-<br />tainment surfaces achieved only (since 3-D plots showing<br />other surfaces are visually confusing).<br />We give all details in the following sections and also make our<br />assessmentmethodssourcecodeavailableforpublicuseat[37].<br />•<br />•<br />•<br />A. Hypervolume Indicator (a.k.a the<br />Measure) [60]<br />This indicator assesses the size (hypervolume or Lebesgue<br />integral)oftheregiondominatedbyasampleset ofpoints,thus,<br />larger values generally indicate better nondominated sets. The<br />region itself must be bounded from above in some way, and for<br />this some point<br />is chosen, which must itself be dominated by<br />every point in the sample set.<br />The hypervolume indicator<br />thatgiventwosets,<br />and ,andusingthesameboundingpoint,<br />if<br />, then it cannot be the case that<br />(where better is used in the sense defined in [61]). However,<br />it must be noted that this result does not imply that<br />than<br />, either. Nonetheless, in the absence of preferences, and<br />withareasonablechoiceofboundingpoint,the<br />results that concur with intuition—proximity to the true PF, ex-<br />tent and evenness of solutions all tend to be rewarded.<br />In order to choose a bounding point for application of the<br />measure, we use the following method. First, the collection<br />of nondominated point sets from all runs of all algorithms are<br />aggregated into a single superset. Then, the ideal and the anti-<br />ideal point of this superset is found. The bounding point is then<br />theanti-idealpointshiftedby timestherange,ineachobjective<br />is well-behaved in the sense<br />is better than<br />is better<br />measuregives<br />where<br />respectively, on the th objective, found within the superset.<br />We use<br />here.<br />For the analysis of multiple runs, we compute the<br />sure of each individual run, and report the mean and the<br />standard deviation of these. Since the distribution of ParEGO<br />and NSGA-II’s results are not necessarily normal, we use<br />the Mann–Whitney rank-sum test to indicate if there is a<br />statistically significant difference in the position of the two<br />distributions.<br />andare the maximum and minimum value,<br />mea-<br />B. Additive Binary Epsilon Indicator<br />There are two ways in which the hypervolume indicator,<br />used on its own, can be misleading. First, there is no way, from<br />looking at<br />values in isolation, of inferring whether one set is<br />actually better than another in a strict sense. Second, the choice<br />of bounding point is rather arbitrary, and this can affect the<br />ordering of some pairs of sets.<br />Fig.2.<br />of an optimizer. The two diagonal lines intersect the five surfaces at various<br />points; in both cases, the circle indicates the intersection that weakly dominates<br />at least ? ? ? ? ? ? ? surfaces and is also weakly dominated by at least three<br />surfaces. Therefore, these two points both lie on the third summary attainment<br />surface.<br />Fiveattainmentsurfacesareshown,representingtheoutputoffiveruns<br />A method that avoids these particular difficulties is the addi-<br />tive binary epsilon indicator [61]. This takes a pair of nondom-<br />inated sets<br />and and returns a pair of numbers (,)<br />where<br />minimization.(Note,<br />). A pair of numbers (<br />strictly better than<br />indicates that neither set is strictly better than the other—they<br />are incomparable [61]. However, if<br />weaker sense, it is better because the minimum<br />so that approximation set<br />-dominates<br />value needed forto -dominate<br />unnormalized objective values have been used in the computa-<br />tion of<br />and.<br />To summarize the results of multiple runs, we compute the<br />binary epsilon indicator for each pair of runs in turn and then<br />report on the median and interquartile range values of<br />. We prefer the median and IQR to the mean and SD be-<br />cause if the median for either algorithm is less than zero, then<br />this has a precise interpretation—that it was strictly better than<br />the other algorithm on at least 50% of runs. We again use the<br />Mann–Whitney rank sum test to decide if the distribution of<br />values is different from the distribution of<br />in , assuming<br />isreadas<br />,<br />epsilon-dominates<br />) indicates that is<br />[61]. A pair of numbers (,)<br />is less than, then in a<br />value needed<br />is smaller than the<br />. In the results we present,<br />and<br />values.<br />C. Median and Worst Attainment Surface Plots<br />An attainment surface is the surface uniquely determined by<br />a set of nondominated points that divides the objective space<br />into the region dominated by the set and the region that is not<br />dominated by it [26]. Given<br />runs of an algorithm, it would be<br />nice to summarize the<br />results attainment surfaces obtained,<br />using only one or two summary surfaces. Such summary attain-<br />ment surfaces can be defined by imagining a diagonal line in<br />the direction of increasing objective values cutting through the<br />results attainment surfaces (see Fig. 2). The intersection on</p>  <p>Page 11</p> <p>60 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 1, FEBRUARY 2005<br />Fig. 3.<br />attainment surfaces that they define. The interpretation of the median<br />attainment surface is that, for every point on it (independently), a point<br />(weakly) dominating this was obtained in at least 50% of the nondominated<br />sets. Similarly, the worst attainment surface indicates the goals achieved in<br />100% of the sets. The best attainment surface indicates the goals achieved by<br />the aggregation of all sets. In this study, the best attainment surface is irrelevant<br />and is not included in plotted results.<br />Five sets of nondominated points and the best, median, and worst<br />this line that weakly dominates at least<br />faces and is weakly dominated by at least<br />point on the “ th summary attainment surface.” This surface is<br />the union of all the goals that have been attained in at least<br />runs (independently).<br />Obviously,fromthedefinitionofthe thsummaryattainment<br />surface, it is possible to define a median (summary) surface (for<br />an odd number of approximation sets) and a best and worst<br />(summary) surface. For illustration, we plot five approximation<br />sets and their sample median, best, and worst attainment sur-<br />faces in Fig. 3. Notice that the interpretation of the median sur-<br />face is that, for every point on it, a point (weakly) dominating<br />this was obtained in more than half of the algorithm runs. Sim-<br />ilarly, the worst attainment surface has the interpretation that in<br />every algorithm run the entire surface was (weakly) dominated.<br />Thus, plots of these surfaces provide more information than a<br />mere scatter plot of the aggregation of (nondominated) points<br />found from several runs.<br />For the two-objective problems in this paper, we give the me-<br />dian and worst attainment surface of ParEGO and of NSGA-II<br />on the same plot, with ParEGO’s surfaces shown in solid and<br />NSGA-II surfaces shown in dashed lines. For the three-objec-<br />tive problems, we show just the worst attainment surface of the<br />two algorithms, each on separate plots but with the same axes.<br />The worst attainment surface gives a very rough indication of<br />the worst-case performance of the two algorithms, although it<br />has no statistical significance.<br />To actually compute the points plotted, we use an algorithm<br />based on the one described in [53], and available from [37].<br />of the sur-<br />of them, defines one<br />VII. EXPERIMENTAL DETAILS<br />To evaluate NSGA-II and ParEGO on the test suite, each al-<br />gorithm is run 21 times, and all solutions visited are stored. The<br />TABLE IV<br />NSGA-II PARAMETER SETTINGS, WHERE ? IS THE NUMBER<br />OF DECISION PARAMETER DIMENSIONS<br />TABLE V<br />ParEGO PARAMETER SETTINGS, WHERE ? IS THE NUMBER<br />OF DECISION PARAMETER DIMENSIONS<br />nondominated sets achieved after a particular number of func-<br />tion evaluations can then be determined and used to estimate<br />performance.<br />Performance was also compared with a random search run 21<br />times for up to 10000 function evaluations on all problems.<br />A. NSGA-II Parameter Settings<br />The implementation of NSGA-II that we use is the one avail-<br />able for download from Deb’s KANGAL web page. We use de-<br />fault settings (Table IV), as supplied, with one exception. The<br />population size is reduced to 20 in order to maximize the per-<br />formance over the short run experiments. This size was deter-<br />mined from some runs in which sizes from 10 to 50 were tried.<br />With a population size of 20, 13 generations gives 260 evalua-<br />tions in all. This is comparable to the 250 evaluations given to<br />ParEGO. The performance of the two algorithms is also com-<br />pared at 100 evaluations.<br />B. ParEGO Parameter Settings<br />The main choices in the parameter settings of ParEGO con-<br />cern the number of scalarizing vectors to use [specified via the<br />parameter<br />in (1)], and in the parameters of the internal EA.<br />For the former, we chose a number which would allow sev-<br />eral “passes” over each scalarizing vector. For the EA within<br />ParEGO, a small number of preliminary experiments were car-<br />ried out to determine settings that led to robust optimization<br />of the expected improvement of the DACE model. The full set<br />of parameter settings used in all runs of ParEGO are given in<br />Table V.<br />VIII. RESULTS<br />Compared with a random search of the space, NSGA-II does<br />well on most problems, even though only 100 or 260 function<br />evaluations are used. An impression of this can be had from the<br />plots (Figs. 4 and 5) showing the median and worst attainment</p>  <p>Page 12</p> <p>KNOWLES: ParEGO: A HYBRID ALGORITHM WITH ON-LINE LANDSCAPE APPROXIMATION61<br />Fig.4.<br />after 250/260 function evaluations (right).<br />Attainmentsurfaceplotwith1000randomsearchpointsalsoshown,ontheKNO1functionafter100functionevaluations(left),andontheOKA1function<br />Fig. 5.<br />evaluations (right).<br />Attainment surface plot with 1000 random points also shown, on OKA2 after 250/260 function evaluations (left), and on VLMOP2 after 100 function<br />TABLE VI<br />VALUES OF THE ? MEASURE AFTER 100/100 EVALUATIONS OF ParEGO/NSGA-II.<br />LARGER VALUES INDICATE BETTER PERFORMANCE. THE DISTRIBUTIONS OF THE<br />? VALUES ARE TESTED USING THE MANN–WHITNEY RANK SUM TEST.<br />THE ? VALUES AND SIGNIFICANCE LEVEL ARE INDICATED. ParEGO<br />IS SIGNIFICANTLY BETTER THAN NSGA-II UNLESS STATED<br />surfaces for NSGA-II, compared with 1000 random solutions,<br />plotted in objective space, on four of the test functions. Plots<br />of the attainment surfaces for the random search algorithm, not<br />shown here, suggest that NSGA-II is better than random search<br />over 260 function evaluations on all the test problems.<br />Tables VI and VII give the results of applying the<br />to the 21 runs of ParEGO and NSGA-II after, respectively, 100<br />and 250 (260 for NSGA-II) function evaluations. From these<br />measure<br />results, it is clear that ParEGO consistently dominates a larger<br />region of the objective space (modulo, the chosen bound point)<br />at 250 evaluations. Not only is the mean higher, but the stan-<br />dard deviation is consistently lower, sometimes by several or-<br />ders of magnitude, suggesting a much more reliable, and better<br />worst-caseperformance.TheMann–Whitneytestshowsthatthe<br />differences between the two populations are significant at the<br />99% confidence level, on all functions.</p>  <p>Page 13</p> <p>62IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 1, FEBRUARY 2005<br />TABLE VII<br />VALUES OF THE ? MEASURE AFTER 250/260 EVALUATIONS OF ParEGO/NSGA-II.<br />LARGER VALUES INDICATE BETTER PERFORMANCE. THE DISTRIBUTIONS OF THE<br />? VALUES ARE TESTED USING THE MANN–WHITNEY RANK SUM TEST.<br />THE ? VALUES AND SIGNIFICANCE LEVEL ARE INDICATED.<br />ParEGO IS BETTER IN ALL CASES<br />TABLE VIII<br />MEDIAN AND INTERQUARTILE RANGE VALUES OF THE BINARY EPSILON INDICATOR AFTER<br />100/100 EVALUATIONS OF ParEGO/NSGA-II. LOWER VALUES INDICATE BETTER<br />PERFORMANCE. THE DISTRIBUTIONS OF THE ? VALUES ARE TESTED USING THE<br />MANN–WHITNEY RANK SUM TEST. THE ? VALUES AND SIGNIFICANCE LEVEL<br />ARE INDICATED. ParEGO IS SIGNIFICANTLY BETTER THAN NSGA-II UNLESS STATED<br />TABLE IX<br />MEDIAN AND INTERQUARTILE RANGE VALUES OF THE BINARY EPSILON INDICATOR<br />AFTER 250/260 EVALUATIONS OF ParEGO/NSGA-II. LOWER VALUES INDICATE<br />BETTER PERFORMANCE. THE DISTRIBUTIONS OF THE ? VALUES ARE TESTED<br />USING THE MANN–WHITNEY RANK SUM TEST. THE ? VALUES AND<br />SIGNIFICANCE LEVEL ARE INDICATED. ParEGO IS SIGNIFICANTLY<br />BETTER THAN NSGA-II UNLESS STATED<br />At 100 function evaluations, the outcome is much the<br />same, except for the functions DTLZ4a and DTLZ7a, where<br />NSGA-II obtains a higher mean value (and is significantly<br />better on DTLZ7a). A possible explanation for the relatively<br />poorer performance of ParEGO on these two functions is<br />that it has actually only “chosen” 13 of the solutions evalu-<br />ated—the first 87 solutions being the latin hypercube—since<br />these functions both have 8 decision variables. If one were<br />interested in a strong performance after 100 evaluations on<br />these functions, a sparser latin hypercube may be worth<br />considering.<br />Tables VIII and IX give the equivalent results for the addi-<br />tive binary epsilon indicator. The results are largely in agree-<br />ment with those of the<br />measure, despite the fact that the<br />method of measurement is quite different. Once again, it is ev-<br />ident that ParEGO has less variation, and that after 250 (260<br />for NSGA-II) evaluations it is not significantly behind on any<br />problem. Furthermore, on one problem, OKA2, its median in-<br />dicator score is negative, indicating that its nondominated sets<br />are better in a strict sense than NSGA-IIs on more than 50%<br />of runs. On two or three other problems, its median indicator<br />value is also very close to zero, and we can confirm that on</p>  <p>Page 14</p> <p>KNOWLES: ParEGO: A HYBRID ALGORITHM WITH ON-LINE LANDSCAPE APPROXIMATION63<br />Fig. 6. Worst attainment surfaces after 250/260 function evaluations for ParEGO (left) and NSGA-II (right) on the DTLZ2a function.<br />Fig. 7. Worst attainment surfaces after 250/260 function evaluations for ParEGO (left) and NSGA-II (right) on the DTLZ4a function.<br />Fig. 8. Worst attainment surfaces after 250/260 function evaluations for ParEGO (left) and NSGA-II (right) on the DTLZ7a function.<br />these problems its nondominated sets are strictly better than<br />NSGA-IIs on several of the runs.<br />The plots of the median and worst attainment surfaces<br />(Figs. 4–9) tend to confirm the outcome of the mea-<br />sures, above. In particular, the worst attainment surface for<br />NSGA-II is much worse than for ParEGO on several of the<br />problems.<br />Finally, Fig. 10 shows the decision space solutions, and cor-<br />responding points in objective space, visited by NSGA-II and<br />ParEGO on the first run on function OKA1. This plot illus-<br />trates a considerable difference in the ways that ParEGO and<br />NSGA-II sample the decision space, which has also been ob-<br />served in other decision space plots: NSGA-II is much more<br />aggressive at honing in on good points, exploiting them much</p>  <p>Page 15</p> <p>64IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 1, FEBRUARY 2005<br />Fig. 9. Worst attainment surfaces after 250/260 function evaluations for ParEGO (left) and NSGA-II (right) on the VLMOP3 function.<br />Fig. 10.<br />and of NSGA-II after 250/260 function evaluations on the OKA1 test function. Notice how ParEGO explores the parameter space much more evenly and yet still<br />finds more good points.<br />Solutions in parameter space (left) with the Pareto set shownas a dark line.Thecorresponding points in objective space (right) for the firstrun of ParEGO<br />more than ParEGO. However, despite ParEGO’s greater explo-<br />ration, it still manages to exploit the good solutions it has found<br />very effectively, and so returns both very good and “very av-<br />erage” points in the objective space. The different behavior of<br />the two algorithms can be easily understood as NSGA-II has no<br />means of stopping its exploitation (i.e., its repeated sampling)<br />of a fit point, other than its niching policy, whereas ParEGO<br />does not visit points that have little or no expectation of being<br />improved (according to its landscape model), no matter how fit<br />they currently are with respect to other solutions.<br />IX. SUMMARY AND CONCLUSION<br />In many optimization scenarios, the number of fitness eval-<br />uations that can be performed is severely limited by cost con-<br />straints.Inthisstudy,theperformanceoftwomultiobjectiveop-<br />timization algorithms, ParEGO and NSGA-II, was measured on<br />much shorter runs than used in most previous MOEA studies. A<br />suite of nine difficult, but low-dimensional, multiobjective test<br />functions of limited ruggedness were used to evaluate the al-<br />gorithms, and on these, it was confirmed that both ParEGO and<br />NSGA-IIoutperformedarandomsearch,evenovera verysmall<br />number of function evaluations.<br />ParEGOgenerallyoutperformedNSGA-IIonthetestedfunc-<br />tions, atboth100 and 250function evaluations,especiallywhen<br />the worst-case performance was measured. This suggests that<br />ParEGO may offer a more effective search on problems like the<br />instrumentsetupoptimizationproblem,describedherein,where<br />only one function evaluation can be performed at a time. We are<br />currently evaluating it on this problem, as well as some other<br />closely related “wet experiment” problems.<br />Although ParEGO’s performance was good on the tested<br />functions, there are at least two potential drawbacks of the<br />current version.<br />1) Normalization of the cost space relies on knowledge of<br />the cost limits.<br />2) The use of uniformly random scalarizing vectors does<br />not necessarily result in the best distribution of nondom-<br />inated points. Some parts of the Pareto front may be far<br />easier to find than others, so that this may lead to a poor<br />approximation.<br />In future work, it is our intention to investigate a method based<br />on adapting the scalarizing vectors, as a function of the points<br />visitedintheobjectivespace,inordertoobtainabetterimprove-<br />ment to the current nondominated front at each iteration.</p>  <p>Page 16</p> <p>KNOWLES: ParEGO: A HYBRID ALGORITHM WITH ON-LINE LANDSCAPE APPROXIMATION65<br />The addition of mechanisms to deal with integer and mixed-<br />integer problems, as well as constraints is also under way.<br />ACKNOWLEDGMENT<br />The author thanks D. Kell for leading the collaborative work<br />on the instrument setup optimization problem that led to this<br />paper, and to J. Handl and the anonymous reviewers for helpful<br />comments and suggestions.<br />REFERENCES<br />[1] G. E. P. Box and N. R. Draper, Empirical Model Building and Response<br />Surfaces. New York: Wiley, 1987.<br />[2] M. Brown, W. Dunn, D. Ellis, R. Goodacre, J. Handl, J. Knowles, S.<br />O’Hagan,I.Spasic,andD.Kell,“AMetabolomepipeline:Fromconcept<br />to data to knowledge,” Metabolomics, vol. 1, no. 1, pp. 39–51, 2005.<br />[3] D. Büche, G. Guidati, P. Stoll, and P. Kourmoursakos, “Self-organizing<br />maps for Pareto optimization of airfoils,” in Parallel Problem Solving<br />from Nature—PPSN VII, J. J. Merelo Guervós, Ed.<br />Springer-Verlag, 2002, vol. 2439, Lecture Notes in Computer Science,<br />pp. 122–131.<br />[4] D. Buche, N. Schraudolph, and P. Koumoutsakos, “Accelerating evolu-<br />tionaryalgorithmswithGaussianprocessfitnessfunctionmodels,”IEEE<br />Trans. Syst., Man, Cybern. C, vol. 35, no. 2, pp. 184–194, May 2005, to<br />be published.<br />[5] P. B. Cheung, L. F. Reis, K. T. Formiga, F. H. Chaudhry, and W. G.<br />Ticona, “Multiobjective evolutionary algorithms applied to the rehabil-<br />itation of a water distribution system: A comparative study,” in Lec-<br />ture Notes in Computer Science, C. M. Fonseca et al., Eds.<br />Germany: Springer-Verlag, 2003, vol. 2632, Proc. 2nd Int. Conf. Evol.<br />Multi-Criterion Optim (EMO), pp. 662–676.<br />[6] C. A. Coello Coello, “An updated survey of GA-based multiobjective<br />optimization techniques,” ACM Computing Surveys, vol. 32, no. 2, pp.<br />109–143, 2000.<br />[7] C. A. Coello Coello and G. Toscano Pulido, “A micro-genetic algorithm<br />for multiobjective optimization,” in Lecture Notes in Computer Science,<br />E. Zitzler et al., Eds.Berlin, Germany: Springer-Verlag, 2001, vol.<br />1993, Proc. 1st Int. Conf. Evol. Multi-Criterion Optimization, pp.<br />126–140.<br />[8] C. A. Coello Coello, G. Toscano Pulido, and M. Salazar Lechuga,<br />“Handling multiple objectives with particle swarm optimization,” IEEE<br />Trans. Evol. Comput., vol. 8, no. 3, pp. 256–279, Jun. 2004.<br />[9] C. A. Coello Coello, D. A. Van Veldhuizen, and G. B. Lamont, Evo-<br />lutionary Algorithms for Solving Multi-Objective Problems.<br />MA: Kluwer, 2002. ISBN 0-3064-6762-3.<br />[10] D. A. Cohn, L. Atlas, and R. E. Ladner, “Improving generalization with<br />active learning,” Mach. Learn., vol. 15, no. 2, pp. 201–221, 1994.<br />[11] D. Corne, M. Oates, and D. Kell, “Landscape state machines: Tools for<br />evolutionary algorithm performance analyzes and landscape/algorithm<br />mapping,” in Applications of Evolutionary Computing.<br />many:Springer-Verlag,2003,vol.2611,LectureNotesinComputerSci-<br />ence, pp. 187–198.<br />[12] D. W. Corne, N. R. Jerram, J. D. Knowles, and M. J. Oates, “PESA-II:<br />Region-based selection in evolutionary multiobjective optimization,” in<br />Proc.GeneticandEvol.Comput.Conf.(GECCO),L.Spectoretal.,Eds.,<br />San Francisco, CA, 2001, pp. 283–290.<br />[13] V. Coverstone-Caroll, J. Hartmann, and W. Mason, “Optimal multi-ob-<br />jective low-thrust spacecraft trajectories,” Computer Methods in Appl.<br />Mechanics Eng., vol. 186, pp. 387–402, 2000.<br />[14] Z.S.Davies,R.J.Gilbert,R.J.Merry, D.B.Kell,M.K.Theodorou,and<br />G. W. Griffith, “Efficient improvement of silage additives by using ge-<br />netic algorithms,” Appl. Environ. Microbiol., pp. 1435–1443, Apr. 2000.<br />[15] K.Deb,“Evolutionaryalgorithmsfor multi-criterionoptimizationin en-<br />gineeringdesign,”in EvolutionaryAlgorithmsin Engineering andCom-<br />puter Science, K. Miettinen et al., Eds.<br />ch. 8, pp. 135–161.<br />[16] K. Deb, Multi-Objective Optimization Using Evolutionary Algo-<br />rithms. Chichester, U.K.: Wiley, 2001. ISBN 0-471-87339-X.<br />[17] K. Deb, S. Agrawal, A. Pratab, and T. Meyarivan, “A fast elitist non-<br />dominated sorting genetic algorithm for multi-objective optimization:<br />NSGA-II,”IndianInst.fTechnol.,Kanpur,India,KanGALRep.200001,<br />2000.<br />Berlin, Germany:<br />Berlin,<br />Norwell,<br />Berlin, Ger-<br />Chichester, U.K.: Wiley, 1999,<br />[18] K. Deb and H. Beyer, “Self-adaptive genetic algorithms with simulated<br />binary crossover,” Evol. Comput., vol. 9, no. 2, pp. 197–221, 2001.<br />[19] K. Deb, L. Thiele, M. Laumanns, and E. Zitzler, “Scalable test problems<br />for evolutionary multi-objective optimization,” Comput. Eng. Netw.<br />Lab. (TIK), Swiss Fed. Inst. Technol. (ETH), Zurich, Switzerland, Tech.<br />Rep. 112, 2001.<br />[20] M. Emmerich, N. Beume, and B. Naujoks, “An EMO algorithm using<br />the hypervolume measure as selection criterion,” in Evolutionary<br />Multi-Criterion Optimization—EMO 2005, vol. 3410, LNCS. Berlin,<br />Germany, 2005, pp. 62–76.<br />[21] M. Emmerich, A. Giotis, M. Özdemir, T. Bäck, and K. Giannkoglou,<br />“Metamodel-assisted evolution strategies,” in Parallel Problem Solving<br />from Nature—PPSN VII. Berlin, Germany: Springer-Verlag, 2002,<br />vol. 2439, LNCS, pp. 361–370.<br />[22] M. Emmerich and B. Naujoks, “Metamodel-assisted multiobjective op-<br />timization strategies and their application in airfoil design,” in Adaptive<br />Computing in Design and Manufacture VI, I. Parmee, Ed.<br />many: Springer-Verlag, 2004, pp. 249–260.<br />[23]<br />, “Metamodel-assisted multiobjective optimization strategies with<br />implicit constraints and their application in airfoil design,” in Proc. ER-<br />COFTAC, K. Giannakoglou et al., Eds., 2004. CD-ROM.<br />[24] J. R. G. Evans, M. J. Edirisinghe, and P. V. C. J. Eames, “Combinatorial<br />searches of inorganic materials using the inkjet printer: Science philos-<br />ophy and technology,” J. Eur. CeramiC Soc., vol. 21, pp. 2291–2299,<br />2001.<br />[25] M.Farina,K.Deb,andP.Amato,“Dynamicmultiobjectiveoptimization<br />problems: Test cases, approximations, and applications,” IEEE Trans.<br />Evol. Comput., vol. 8, no. 5, pp. 425–442, Oct. 2004.<br />[26] C. M. Fonseca and P. J. Fleming, “On the performance assess-<br />ment and comparison of stochastic multiobjective optimizers,” in<br />Parallel Problem Solving from Nature—PPSN IV, H.-M. Voigt, W.<br />Ebeling, I. Rechenberg, and H.-P. Schwefel, Eds.<br />Springer-Verlag, 1996, Lecture Notes in Computer Science, pp.<br />584–593.<br />[27] A. Gaspar-Cunha and A. Vieira, “A multi-objective evolutionary<br />algorithm using approximate fitness evaluation,” in Proc. Int. Congr.<br />Evol. Methods for Design, Optimization and Control With Applications<br />to Industrial Problems EUROGEN, G. Bugeda, J. A-Dsidri, J. Pe-<br />riaux, M. Schoenauer, and G. Winter, Eds., 2003, [Online]. Available:<br />http://www.defi.isep.ipp.pt/~asv/papers/moga.pdf.<br />[28] A. Gaspar-Cunha and A. S. Vieira, “A hybrid multi-objective evolu-<br />tionary algorithm using an inverse neural network,” in Proc. Hybrid<br />Metaheuristics (HM 2004) Workshop at ECAI 2004, Valencia, Spain,<br />2004, pp. 25–30.<br />[29] K. C. Giannakoglou, “Design of optimal aerodynamic shapes using sto-<br />chastic optimization methods and computational intelligence,” Int. Rev.<br />J. Progr. Aerosp. Sci., vol. 38, pp. 43–76, 2002.<br />[30] J.HornandN.Nafpliotis,“Multiobjectiveoptimizationusingtheniched<br />pareto geneticalgorithm,”Univ. Illinoisat Urbana–Champaign,Urbana,<br />IL, USA, Tech. Rep. IlliGAl Report 93005, 1993.<br />[31] E. J. Hughes, “Multi-objective binary search optimization,” in Lecture<br />Notes in Computer Science, vol. 2632, Proc. 2nd Int. Conf. Evol. Multi-<br />Criterion Optimization (EMO), C. M. Fonseca, P. J. Fleming, E. Zitzler,<br />K. Deb, and L. Thiele, Eds. Berlin, Germany, 2003, pp. 102–117.<br />[32] M.T.Jensen,“Reducingtherun-timecomplexityofmultiobjectiveEAs:<br />The NSGA-II and other algorithms,” IEEE Trans. Evol. Comput., vol. 7,<br />no. 5, pp. 503–515, Oct. 2003.<br />[33] Y. Jin, “A comprehensive survey of fitness approximation in evolu-<br />tionary computation,” Soft Computing, vol. 9, no. 1, pp. 3–12, 2005.<br />[34] Y. Jin, M. Olhofer, and B. Sendhoff, “A framework for evolutionary<br />optimization with approximate fitness functions,” IEEE Trans. Evol.<br />Comput., vol. 6, no. 5, pp. 481–494, Oct. 2002.<br />[35] D. Jones, M. Schonlau, and W. Welch, “Efficient global optimization of<br />expensive black-box functions,” J. Global Optim., vol. 13, pp. 455–492,<br />1998.<br />[36] J. Knowles, “Parameter setting for PESA-II on the closed-loop spec-<br />trometer optimization problem: A simulation study using short run<br />experiments,” Internal report available on request from j.knowle-<br />sumist.ac.uk, Apr. 2004.<br />[37]<br />, (2004). Supporting material for this paper. [Online]. Available:<br />http://dbk.ch.umist.ac.uk/knowles/parego/<br />[38] J. Knowles and D. Corne, “On metrics for comparing nondominated<br />sets,”inProc.Congr.Evol.Comput.(CEC),vol.1,Piscataway,NJ,2002,<br />pp. 711–716.<br />[39] J. D. Knowles, “Local-search and hybrid evolutionary algorithms for<br />Pareto optimization,” Ph.D. dissertation, Univ. Reading, Reading, U.K.,<br />2002.<br />Berlin, Ger-<br />Berlin, Germany:</p>  <p>Page 17</p> <p>66 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 1, FEBRUARY 2005<br />[40] J.D.KnowlesandD.W.Corne,“Approximatingthenondominatedfront<br />using the Pareto archived evolution strategy,” Evol. Comput., vol. 8, no.<br />2, pp. 149–172, 2000.<br />[41] K.-H. Liang, X. Yao, and C. Newton, “Evolutionary search of approx-<br />imated ?-dimensional landscapes,” Int. J. Knowledge-Based Intelligent<br />Eng. Syst., vol. 4, no. 3, pp. 172–183, 2000.<br />[42] R. Myers and D. Montgomery, Response Surface Methodology.<br />York: Wiley, 1995.<br />[43] P. K. S. Nain and K. Deb, “A computationally effective multi-objective<br />search and optimization technique using coarse-to-fine grain modeling,”<br />IITK, Kanpur, India, Tech. Rep. Kangal Rep. 2002005, 2002.<br />[44] B. Naujoks, L. Willmes, T. Bäck, and W. Haase, “Evaluating multi-cri-<br />teria evolutionary algorithms for airfoil optimization,” in Parallel<br />Problem Solving from Nature—PPSN VII, J. J. Merelo Guervós et al.,<br />Eds.Berlin, Germany: Springer-Verlag, 2002, vol. 2439, Lecture<br />Notes in Computer Science, pp. 841–850.<br />[45] S. O’Hagan, W. B. Dunn, M. Brown, J. D. Knowles, and D. B. Kell,<br />“Closed-loop, multiobjective optimization of analytical instrumen-<br />tation: Gas chromatography/time-of-flight mass spectrometry of the<br />metabolomes of human serum and of yeast fermentations,” Analytical<br />Chemistry, vol. 77, no. 1, pp. 290–303, 2004.<br />[46] T. Okabe, Y. Jin, M. Olhofer, and B. Sendhoff, “On test functions for<br />evolutionary multi-objective optimization,” in Lecture Notes in Com-<br />puter Science. Berlin, Germany: Springer-Verlag, 2004, Proc. 8th Int.<br />Conf. Parallel Problem Solving from Nature, pp. 792–802.<br />[47] Y.S.Ong,P.B.Nair,andA.J.Kean,“Evolutionaryoptimizationofcom-<br />putationallyexpensiveproblemsviasurrogatemodeling,”AIAAJournal,<br />vol. 41, no. 4, pp. 687–696, 2003.<br />[48] Y. S. Ong, P. B. Nair, A. J. Keane, and Z. Z. Zhou, “Surrogate-assisted<br />evolutionary optimization frameworks for high-fidelity engineering<br />design problems,” in Knowledge Incorporation in Evolutionary Com-<br />putation. ser. Studies in Fuzziness and Soft Computing Series, Y. Jin,<br />Ed. Berlin, Germany: Springer-Verlag, 2004.<br />[49] W. Press, S. Teukolsky, W. Vetterling, and B. Flannery, Numerical<br />Recipes in C: The Art of Scientific Computing.<br />Cambridge Univ. Press, 1992.<br />[50] A. Ratle, “Accelerating the convergence of evolutionary algorithms by<br />fitness landscape approximation,” in Parallel Problem Solving from Na-<br />ture—PPSN V, 1998, pp. 87–96.<br />[51] R. G. Regis and C. A. Shoemaker, “Local function approximation in<br />evolutionary algorithms for the optimization of costly functions,” IEEE<br />Trans. Evol. Comput., vol. 8, no. 5, pp. 490–505, Oct. 2004.<br />[52] J. Sacks, W. Welch, T. Mitchell, and H. Wynn, “Design and analysis<br />of computer experiments (with discussion),” Statist. Sci., vol. 4, no.<br />409–435, 1989.<br />New<br />Cambridge, U.K.:<br />[53] K. I. Smith, R. M. Everson, and J. E. Fieldsend, “Dominance measures<br />for multi-objective simulated annealing,” in Proc. Congr. Evol. Comput.<br />(CEC), vol. 1, Portland, OR, 2004, pp. 23–30.<br />[54] S. Vaidyanathan, D. I. Broadhurst, D. B. Kell, and R. Goodacre, “Ex-<br />planatoryoptimizationofproteinmassspectrometryviageneticsearch,”<br />Analytical Chemistry, vol. 75, no. 23, pp. 6679–6686, 2003.<br />[55] S. Vaidyanathan, D. Kell, and R. Goodacre, “Selective detection of pro-<br />teins in mixtures using electrospray ionization mass spectrometry: In-<br />fluence of instrumental settings and implications for proteomics,” Ana-<br />lytical Chemistry, vol. 76, pp. 5024–5032, 2004.<br />[56] D. A. V. Veldhuizen and G. B. Lamont, “Multiobjective evolutionary<br />algorithm test suites,” in Proc. ACM Symp. Appl. Comput., J. Carroll et<br />al., Eds., San Antonio, TX, 1999, pp. 351–357.<br />[57] A. Vicini and D. Quagliarella, “Airfoil and wing design using hybrid<br />optimization strategies,” AIAA Journal, vol. 37, no. 5, 1999.<br />[58] D. Weuster-Botz and C. Wandrey, “Medium optimization by genetic al-<br />gorithm for continuous production of formate dehydrogenase,” Process<br />Biochemistry, vol. 30, pp. 563–571, 1995.<br />[59] D. H. Wolpert, “The lack of a priori distinctions between learning algo-<br />rithms,” Neural Comput., vol. 8, pp. 1341–1390, 1996.<br />[60] E. Zitzler, “Evolutionary algorithms for multiobjective optimization:<br />Methods and applications,” Ph.D. dissertation, Swiss Federal Inst.<br />Technol. (ETH), Zurich, Switzerland, 1999.<br />[61] E.Zitzler,L.Thiele,M.Laumanns,C.M.Fonseca,andV.G.daFonseca,<br />“Performance assessment of multiobjective optimizers: An analysis and<br />review,” IEEE Trans. Evol. Comput., vol. 7, no. 2, pp. 117–132, Apr.<br />2003.<br />Joshua Knowles received the B.Sc. (Hons.) degree<br />in physics with mathematics, the M.Sc. degree in in-<br />formation systems engineering (at Distinction level),<br />and the Ph.D. degree from the University of Reading,<br />Reading,U.K.,in1993,1996,and2002,respectively.<br />From 2001 to 2003, he worked as a European<br />commission Marie Curie Research Fellow at<br />IRIDIA, Free University of Brussels, Brussels,<br />Belgium. Since 2003, he has held a BBSRC David<br />Phillips Fellowship at the University of Manchester,<br />Manchester, U.K., where his research is focused on<br />applications of multiobjective optimization and machine learning to computa-<br />tional biology. He was a tutorial speaker at EMO2005 and is a semi-plenary<br />speaker at MOPGP 2006.<br />Dr. Knowles is a member of the EPSRC Peer Review College.</p>  <a href="https://www.researchgate.net/profile/Joshua_Knowles/publication/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems/links/0deec530c884b04ef0000000.pdf">Download full-text</a> </div> <div id="rgw17_56aba22f19918" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw18_56aba22f19918">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw19_56aba22f19918"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Joshua_Knowles/publication/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems/links/0deec530c884b04ef0000000.pdf" class="publication-viewer" title="download.pdf">download.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Joshua_Knowles">Joshua Damian Knowles</a> &middot; Jan 20, 2016 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw20_56aba22f19918"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.7331&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow" class="publication-viewer" title="ParEGO: A Hybrid Algorithm With On-Line Landscape Approximation for Expension Multiobjective Optimization Problems">ParEGO: A Hybrid Algorithm With On-Line Landscape ...</a> </div>  <div class="details">   Available from <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.7331&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow">psu.edu</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw22_56aba22f19918" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw23_56aba22f19918">  </ul> </div> </div>   <div id="rgw13_56aba22f19918" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw14_56aba22f19918"> <div> <h5> <a href="publication/291386550_Monkey_King_Evolution_A_new_memetic_evolutionary_algorithm_and_its_application_in_vehicle_fuel_consumption_optimization" class="color-inherit ga-similar-publication-title"><span class="publication-title">Monkey King Evolution: A new memetic evolutionary algorithm and its application in vehicle fuel consumption optimization</span></a>  </h5>  <div class="authors"> <a href="researcher/70475109_Zhenyu_Meng" class="authors ga-similar-publication-author">Zhenyu Meng</a>, <a href="researcher/2083049625_Jeng-Shyang_Pan" class="authors ga-similar-publication-author">Jeng-Shyang Pan</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw15_56aba22f19918"> <div> <h5> <a href="publication/290510111_An_evolutionary_algorithm_for_global_optimization_based_on_self-organizing_maps" class="color-inherit ga-similar-publication-title"><span class="publication-title">An evolutionary algorithm for global optimization based on self-organizing maps</span></a>  </h5>  <div class="authors"> <a href="researcher/2094109144_Sami_Barmada" class="authors ga-similar-publication-author">Sami Barmada</a>, <a href="researcher/5781343_Marco_Raugi" class="authors ga-similar-publication-author">Marco Raugi</a>, <a href="researcher/13063578_Mauro_Tucci" class="authors ga-similar-publication-author">Mauro Tucci</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw16_56aba22f19918"> <div> <h5> <a href="publication/292116775_Structure_prediction_and_its_applications_in_computational_materials_design" class="color-inherit ga-similar-publication-title"><span class="publication-title">Structure prediction and its applications in computational materials design</span></a>  </h5>  <div class="authors"> <a href="researcher/2096014788_Q_Zhu" class="authors ga-similar-publication-author">Q. Zhu</a>, <a href="researcher/2096031716_AR_Oganov" class="authors ga-similar-publication-author">A.R. Oganov</a>, <a href="researcher/2096169673_Q_Zeng" class="authors ga-similar-publication-author">Q. Zeng</a>, <a href="researcher/2095993820_X_Zhou" class="authors ga-similar-publication-author">X. Zhou</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw36_56aba22f19918" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw37_56aba22f19918">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw38_56aba22f19918" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=2MSkKopC5gdlSs2MpplA69W5h_8nq2pYx6rJU43FpbOeYlGg-0mZUvHE05uXvclC" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="/ZQkwzHiMYbMfbX7bgrtrjpiQoPbmzjXozNMuZurxzoxJKBWr1fSqp8UdAEx1O4wmmt6lG8I6XQy71JDhwodUzLNYuKBCc5EOGrVLvKzTRQT2mh/v4fgROFpPjEW73tp7tQmV8dDvPtqoC06GxGbQH4WxIz+4LS32Yr1Bp2B56Xb23SLu8wkuNouKtNR+gTrf6muwkwR/jHsHhbbQd+h3LqtNndsL4W2xW8lm5RDuupglGl0XpxsYo9qFzrV2ucThT2aceQ0AjYA87CaGAaBY895itCNUugX/1trnR1+h0M="/> <input type="hidden" name="urlAfterLogin" value="publication/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMzQxODg3MV9QYXJFR09fQV9IeWJyaWRfQWxnb3JpdGhtX1dpdGhfT24tTGluZV9MYW5kc2NhcGVfQXBwcm94aW1hdGlvbl9mb3JfRXhwZW5zaW9uX011bHRpb2JqZWN0aXZlX09wdGltaXphdGlvbl9Qcm9ibGVtcw%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMzQxODg3MV9QYXJFR09fQV9IeWJyaWRfQWxnb3JpdGhtX1dpdGhfT24tTGluZV9MYW5kc2NhcGVfQXBwcm94aW1hdGlvbl9mb3JfRXhwZW5zaW9uX011bHRpb2JqZWN0aXZlX09wdGltaXphdGlvbl9Qcm9ibGVtcw%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMzQxODg3MV9QYXJFR09fQV9IeWJyaWRfQWxnb3JpdGhtX1dpdGhfT24tTGluZV9MYW5kc2NhcGVfQXBwcm94aW1hdGlvbl9mb3JfRXhwZW5zaW9uX011bHRpb2JqZWN0aXZlX09wdGltaXphdGlvbl9Qcm9ibGVtcw%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw39_56aba22f19918"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 823;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Joshua Damian Knowles","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272291355426850%401441930726775_m\/Joshua_Knowles.png","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Joshua_Knowles","institution":"University of Birmingham","institutionUrl":false,"widgetId":"rgw4_56aba22f19918"},"id":"rgw4_56aba22f19918","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=1291523","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56aba22f19918"},"id":"rgw3_56aba22f19918","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=3418871","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":3418871,"title":"ParEGO: A Hybrid Algorithm With On-Line Landscape Approximation for Expension Multiobjective Optimization Problems","journalTitle":"IEEE Transactions on Evolutionary Computation","journalDetailsTooltip":{"data":{"journalTitle":"IEEE Transactions on Evolutionary Computation","journalAbbrev":"IEEE T EVOLUT COMPUT","publisher":"Institute of Electrical and Electronics Engineers; IEEE Neural Networks Council, Institute of Electrical and Electronics Engineers","issn":"1089-778X","impactFactor":"3.65","fiveYearImpactFactor":"6.29","citedHalfLife":">10.0","immediacyIndex":"0.59","eigenFactor":"0.01","articleInfluence":"1.87","widgetId":"rgw6_56aba22f19918"},"id":"rgw6_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=1089-778X","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":"Sch. of Chem., Univ. of Manchester, UK","type":"Article","details":{"doi":"10.1109\/TEVC.2005.851274","journalInfos":{"journal":"","publicationDate":"03\/2006;","publicationDateRobot":"2006-03","article":"10(1):50 - 66.","journalTitle":"IEEE Transactions on Evolutionary Computation","journalUrl":"journal\/1089-778X_IEEE_Transactions_on_Evolutionary_Computation","impactFactor":3.65}},"source":{"sourceUrl":"http:\/\/ieeexplore.ieee.org\/xpl\/freeabs_all.jsp?arnumber=1583627","sourceName":"IEEE Xplore"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1109\/TEVC.2005.851274"},{"key":"rft.atitle","value":"ParEGO: A Hybrid Algorithm With On-Line Landscape Approximation for Expension Multiobjective Optimization Problems"},{"key":"rft.title","value":"Evolutionary Computation, IEEE Transactions on"},{"key":"rft.jtitle","value":"Evolutionary Computation, IEEE Transactions on"},{"key":"rft.volume","value":"10"},{"key":"rft.issue","value":"1"},{"key":"rft.date","value":"2006"},{"key":"rft.pages","value":"50 - 66"},{"key":"rft.issn","value":"1089-778X"},{"key":"rft.au","value":"Joshua Knowles"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56aba22f19918"},"id":"rgw7_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=3418871","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":3418871,"peopleItems":[{"data":{"authorNameOnPublication":"Joshua Damian Knowles","accountUrl":"profile\/Joshua_Knowles","accountKey":"Joshua_Knowles","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272291355426850%401441930726775_m\/Joshua_Knowles.png","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Joshua Damian Knowles","profile":{"professionalInstitution":{"professionalInstitutionName":"University of Birmingham","professionalInstitutionUrl":"institution\/University_of_Birmingham"}},"professionalInstitutionName":"University of Birmingham","professionalInstitutionUrl":"institution\/University_of_Birmingham","url":"profile\/Joshua_Knowles","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272291355426850%401441930726775_l\/Joshua_Knowles.png","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Joshua_Knowles","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw10_56aba22f19918"},"id":"rgw10_56aba22f19918","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1291523&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University of Birmingham","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":1,"accountCount":1,"publicationUid":3418871,"widgetId":"rgw9_56aba22f19918"},"id":"rgw9_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1291523&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=1&accountCount=1&publicationUid=3418871","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56aba22f19918"},"id":"rgw8_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=3418871&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":3418871,"abstract":"<noscript><\/noscript><div>This paper concerns multiobjective optimization in scenarios where each solution evaluation is financially and\/or temporally expensive. We make use of nine relatively low-dimensional, nonpathological, real-valued functions, such as arise in many applications, and assess the performance of two algorithms after just 100 and 250 (or 260) function evaluations. The results show that NSGA-II, a popular multiobjective evolutionary algorithm, performs well compared with random search, even within the restricted number of evaluations used. A significantly better performance (particularly, in the worst case) is, however, achieved on our test set by an algorithm proposed herein-ParEGO-which is an extension of the single-objective efficient global optimization (EGO) algorithm of Jones et al. ParEGO uses a design-of-experiments inspired initialization procedure and learns a Gaussian processes model of the search landscape, which is updated after every function evaluation. Overall, ParEGO exhibits a promising performance for multiobjective optimization problems where evaluations are expensive or otherwise restricted in number.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw11_56aba22f19918"},"id":"rgw11_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=3418871","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems\/links\/0deec530c884b04ef0000000\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw12_56aba22f19918"},"id":"rgw12_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56aba22f19918"},"id":"rgw5_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=3418871&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":70475109,"url":"researcher\/70475109_Zhenyu_Meng","fullname":"Zhenyu Meng","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083049625,"url":"researcher\/2083049625_Jeng-Shyang_Pan","fullname":"Jeng-Shyang Pan","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291386550_Monkey_King_Evolution_A_new_memetic_evolutionary_algorithm_and_its_application_in_vehicle_fuel_consumption_optimization","usePlainButton":true,"publicationUid":291386550,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/291386550_Monkey_King_Evolution_A_new_memetic_evolutionary_algorithm_and_its_application_in_vehicle_fuel_consumption_optimization","title":"Monkey King Evolution: A new memetic evolutionary algorithm and its application in vehicle fuel consumption optimization","displayTitleAsLink":true,"authors":[{"id":70475109,"url":"researcher\/70475109_Zhenyu_Meng","fullname":"Zhenyu Meng","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083049625,"url":"researcher\/2083049625_Jeng-Shyang_Pan","fullname":"Jeng-Shyang Pan","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291386550_Monkey_King_Evolution_A_new_memetic_evolutionary_algorithm_and_its_application_in_vehicle_fuel_consumption_optimization","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291386550_Monkey_King_Evolution_A_new_memetic_evolutionary_algorithm_and_its_application_in_vehicle_fuel_consumption_optimization\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw14_56aba22f19918"},"id":"rgw14_56aba22f19918","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291386550","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2094109144,"url":"researcher\/2094109144_Sami_Barmada","fullname":"Sami Barmada","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":5781343,"url":"researcher\/5781343_Marco_Raugi","fullname":"Marco Raugi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":13063578,"url":"researcher\/13063578_Mauro_Tucci","fullname":"Mauro Tucci","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Engineering Optimization","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/290510111_An_evolutionary_algorithm_for_global_optimization_based_on_self-organizing_maps","usePlainButton":true,"publicationUid":290510111,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.08","url":"publication\/290510111_An_evolutionary_algorithm_for_global_optimization_based_on_self-organizing_maps","title":"An evolutionary algorithm for global optimization based on self-organizing maps","displayTitleAsLink":true,"authors":[{"id":2094109144,"url":"researcher\/2094109144_Sami_Barmada","fullname":"Sami Barmada","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":5781343,"url":"researcher\/5781343_Marco_Raugi","fullname":"Marco Raugi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":13063578,"url":"researcher\/13063578_Mauro_Tucci","fullname":"Mauro Tucci","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Engineering Optimization 01\/2016;  DOI:10.1080\/0305215X.2015.1128424"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/290510111_An_evolutionary_algorithm_for_global_optimization_based_on_self-organizing_maps","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/290510111_An_evolutionary_algorithm_for_global_optimization_based_on_self-organizing_maps\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw15_56aba22f19918"},"id":"rgw15_56aba22f19918","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=290510111","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2096014788,"url":"researcher\/2096014788_Q_Zhu","fullname":"Q. Zhu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2096031716,"url":"researcher\/2096031716_AR_Oganov","fullname":"A.R. Oganov","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2096169673,"url":"researcher\/2096169673_Q_Zeng","fullname":"Q. Zeng","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095993820,"url":"researcher\/2095993820_X_Zhou","fullname":"X. Zhou","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/292116775_Structure_prediction_and_its_applications_in_computational_materials_design","usePlainButton":true,"publicationUid":292116775,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/292116775_Structure_prediction_and_its_applications_in_computational_materials_design","title":"Structure prediction and its applications in computational materials design","displayTitleAsLink":true,"authors":[{"id":2096014788,"url":"researcher\/2096014788_Q_Zhu","fullname":"Q. Zhu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2096031716,"url":"researcher\/2096031716_AR_Oganov","fullname":"A.R. Oganov","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2096169673,"url":"researcher\/2096169673_Q_Zeng","fullname":"Q. Zeng","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095993820,"url":"researcher\/2095993820_X_Zhou","fullname":"X. Zhou","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/292116775_Structure_prediction_and_its_applications_in_computational_materials_design","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/292116775_Structure_prediction_and_its_applications_in_computational_materials_design\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56aba22f19918"},"id":"rgw16_56aba22f19918","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=292116775","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw13_56aba22f19918"},"id":"rgw13_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=3418871&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":3418871,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":3418871,"publicationType":"article","linkId":"0deec530c884b04ef0000000","fileName":"download.pdf","fileUrl":"profile\/Joshua_Knowles\/publication\/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems\/links\/0deec530c884b04ef0000000.pdf","name":"Joshua Damian Knowles","nameUrl":"profile\/Joshua_Knowles","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Jan 20, 2016","fileSize":"932.58 KB","widgetId":"rgw19_56aba22f19918"},"id":"rgw19_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=3418871&linkId=0deec530c884b04ef0000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":3418871,"publicationType":"article","linkId":"0e5fb8c2f0c41c4932eb6e06","fileName":"ParEGO: A Hybrid Algorithm With On-Line Landscape Approximation for Expension Multiobjective Optimization Problems","fileUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.98.7331&amp;rep=rep1&amp;type=pdf","name":"psu.edu","nameUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.98.7331&amp;rep=rep1&amp;type=pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw20_56aba22f19918"},"id":"rgw20_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=3418871&linkId=0e5fb8c2f0c41c4932eb6e06&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw18_56aba22f19918"},"id":"rgw18_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=3418871&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":65,"valueFormatted":"65","widgetId":"rgw21_56aba22f19918"},"id":"rgw21_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=3418871","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw17_56aba22f19918"},"id":"rgw17_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=3418871&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":3418871,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw23_56aba22f19918"},"id":"rgw23_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=3418871&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":65,"valueFormatted":"65","widgetId":"rgw24_56aba22f19918"},"id":"rgw24_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=3418871","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw22_56aba22f19918"},"id":"rgw22_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=3418871&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"50 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 1, FEBRUARY 2005\nParEGO: A Hybrid Algorithm With On-Line\nLandscape Approximation for Expensive\nMultiobjective Optimization Problems\nJoshua Knowles\nAbstract\u2014This paper concerns multiobjective optimization in\nscenarios where each solution evaluation is financially and\/or tem-\nporally expensive.We make use of nine relativelylow-dimensional,\nnonpathological, real-valued functions, such as arise in many ap-\nplications, and assess the performance of two algorithms after\njust 100 and 250 (or 260) function evaluations. The results show\nthat NSGA-II, a popular multiobjective evolutionary algorithm,\nperforms well compared with random search, even within the\nrestricted number of evaluations used. A significantly better per-\nformance (particularly, in the worst case) is, however, achieved on\nour test set by an algorithm proposed herein\u2014ParEGO\u2014which\nis an extension of the single-objective efficient global optimization\n(EGO) algorithm of Jones et al. ParEGO uses a design-of-exper-\niments inspired initialization procedure and learns a Gaussian\nprocesses model of the search landscape, which is updated after\nevery function evaluation. Overall, ParEGO exhibits a promising\nperformance for multiobjective optimization problems where\nevaluations are expensive or otherwise restricted in number.\nIndex Terms\u2014Design and analysis of computer experiments\n(DACE), efficient global optimization (EGO), expensive black-box\nfunctions, Kriging, landscape approximation, metamodels, mul-\ntiobjective optimization, nondominated sorting genetic algorithm\nII (NSGA-II), Pareto optima, performance assessment, response\nsurfaces, test suites.\nI. INTRODUCTION\nA\nsearch methods in their disciplines, a new and greater variety\nof real-world optimization problems is gradually coming to\nlight. Many of the problems fall right into the \u201cbackyard\u201d of\nevolutionary computation (EC) methods\u2014especially those for\nwhichconventional techniquesare not easilyadapted,including\nnonconvex, mixed integer, nonlinear, constrained, and\/or noisy\ncost functions; for all these problems we may happily look\nforward to a wealth of EC success stories. But some other\nproblems now appearing seem to pose a particular challenge to\nEC and other heuristic search methods because they combine\nthe requirement of optimizing multiple, incommensurable\nobjectives with the feature of being prohibitively expensive to\nevaluate. It is on these latter problems that we focus in this\npaper.\nS SCIENTISTS and engineers become increasingly\naware of the potential benefits of applying heuristic\nManuscript received October 8, 2004; revised April 20, 2005. The work\nof J. Knowles was supported in part by a David Phillips Fellowship from the\nBiotechnology and Biological Sciences Research Council (BBSRC), U.K.\nThe author is with the School of Chemistry, University of Manchester, Man-\nchester M60 1QD, U.K. (e-mail: j.knowles@manchester.ac.uk; http:\/\/dbk.ch.\numist.ac.uk\/knowles\/).\nDigital Object Identifier 10.1109\/TEVC.2005.851274\nMore precisely, our motivation comes from a number of op-\ntimization scenarios arising in the experimental sciences. These\nhave a more-or-less common list of features that we can take to\nloosely specify a class of problems, as follows:\n1) the problem has multiple, possibly incommensurable,\nobjectives;\n2) the time taken to perform one evaluation is of the order of\nminutes or hours;\n3) only one evaluation can be performed at one time (no par-\nallelism is possible);\n4) the total number of evaluations to be performed is limited\nby financial, time or resource constraints;\n5) no realistic simulator or other method of approximating\nthe full evaluation is readily available;\n6) noise is low (repeated evaluations yield very similar\nresults);\n7) the overall gains in quality (or reductions in cost) that can\nbe achieved are high;\n8) the search landscape is locally smooth but multimodal;\n9) the dimensionality of the search space is low-to-medium.\nAlthough problems exhibiting (many of) these features in-\nclude some familiar ones from engineering design optimization\n(e.g., [29]), there are also less well-known combinatorial bio-\nchemistry and materials science applications [14], [24], [58],\nas well as instrument configuration problems [54], [55], in this\nclass.\nAn example of the latter which motivates the work herein\nis [45]. It reports on a series of experiments directed at im-\nproving the viability of metabolomics,1by optimizing the con-\nfigurationofaGC-TOFmassspectrometer(GC-MS).Threeob-\njectives in the optimization were considered: 1) maximizing the\nsignal-to-noise ratio in the chromatogram; 2) maximizing the\nnumberof\u201ctrue\u201dpeaksinthechromatogram;and3)minimizing\nthe processing time\u2014the time for the instrument to analyze one\nsample. Features 2\u20134 in the list given above arise in this partic-\nular problem because the evaluations are actually \u201cwet experi-\nments\u201d2that must be performed on a physical machine (which\nis itself very costly and, hence, only one is available), and may\nalso require costly consumables and\/or the use of highly skilled\noperators. Feature 5 applies here because it is practically infea-\nsible to simulate accurately the output of such a complicated\n1Metabolomics [2] relies on efficiently identifying the hundreds or thousands\nof different compounds within a biological sample to enable metabolic pro-\ncesses to be accurately monitored.\n2The term used in biology for laboratory as opposed to computer-based\n(\u201cdry\u201d) experiments.\n1089-778X\/$20.00 \u00a9 2006 IEEE"},{"page":2,"text":"KNOWLES: ParEGO: A HYBRID ALGORITHM WITH ON-LINE LANDSCAPE APPROXIMATION 51\ninstrument over a range of chemical inputs. Thus, considering\n2\u20135, it is clear that the evolutionary algorithm (EA) or other op-\ntimization methods must search the space in a smaller number\nof evaluations than in many other EA applications.\nFortunately, features 6\u20139 reduce the difficulty of the task\nsomewhat. Low noise means that individual experiments need\nnot be repeated, and there were only eight real-valued settings\nof the GC-MS (i.e., search space dimensions). Moreover, it\nis known from human experts that getting the configuration\n\u201cright\u201d yields significant improvement in the chromatograms;\nsince this is usually achieved from hand-tuning by experts,\nthe search space must be, at least locally, smooth. Therefore,\nreasonably large basins of attraction probably exist. However,\nit is also known that some limited epistasis does exist [54].\nIn the experiments reported in [45], a multiobjective evolu-\ntionary algorithm (MOEA), PESA-II [12], was used to perform\ntheoptimization[45]andsome180GC-MSconfigurationswere\nassayed in all (taking several days and consuming expensive in-\nstrument-time). At the end, a particular configuration that of-\nfered the best compromise solution was chosen from the Pareto\nfront obtained. This yielded a threefold increase in the number\nof peaks observed, coupled with a small increase in the poten-\ntial throughput (i.e., future samples could be processed more\nquickly), over the hand-tuned configuration usually employed.\nThe choice of employing an MOEA was motivated primarily\nby the feature 1 above, and the wish to explore the tradeoffs of\nthe three objectives, enabling the input of expert knowledge in\nthe final choice of GC-MS configuration. The ability to do this\nfrom a single optimization run, with little or no knowledge of\nthe cost landscape, makes an MOEA approach a logical choice.\nMoreover, results reported in [45] show that GC-MS config-\nurations from later experiments were statistically significantly\nbetter than earlier configurations (computed using a Fisher\u2019s\npermutation test), suggesting that the MOEA used was better\nthan a random search would have been.\nNonetheless, the scientific justification for theuse of MOEAs\ninthisparticularoptimizationcontextisnotassolidaswewould\nlike because, although MOEAs have been applied in a wealth of\ndifferent contexts and have exhibited competitive performance\nagainstseveralotheroptimizationtechniques,theirperformance\nwhen the number of function evaluations is severely limited is\nlargely untested in the literature (though see [36] for some ex-\nperiments underpinning the use of PESA-II for the GC-MS op-\ntimization). A central aim of this paper is, thus, to undertake\ntesting of an MOEA in contexts similar to the GC-MS con-\nfiguration problem, over a realistically small number of func-\ntion evaluations. To this end, we choose to employ NSGA-II,\na well-respected, modern example of an MOEA,3and test it,\nusing a small population size, over just 100 and 260 function\nevaluations.\nIn many engineering applications that share similar features\nto our scenario above, standard EAs are eschewed because\nof their low efficiency with respect to function evaluations. A\nwealth of other global optimization methods (see next section)\nuse more principled methods of exploring the search space\n3The IEEE TEC paper describing NSGA-II for multiobjective optimization\nwas judged as the \u201cFast-Breaking Paper in Engineering\u201d by Web of Science\n(ESI) in February 2004.\nunder these restricted conditions, most notably those methods\nbased on modeling the landscape online during the search.\nHowever, to our knowledge, the performance of these methods\nhas never been compared with an MOEA in a straightforward\nPareto optimization context. Thus, a second aim of this paper is\nto extend such an algorithm to do multiobjective optimization,\nand to compare it with the NSGA-II. We choose for this, a\nfrequently cited algorithm from the global optimization lit-\nerature, specifically designed for expensive functions of low\ndimension\u2014the efficient global optimization (EGO) algorithm\n[35]. The version that we propose for Pareto optimization is\ngiven the designation \u201cParEGO\u201d from here on.\nTo undertake the performance assessment of ParEGO and\nNSGA-II, we do not use expensive functions here, but employ\na suite of test functions enabling us to collect performance data\nover multiple algorithm runs. The selection of problems in the\nsuite includes functions exhibiting a diverse range of problem\ndifficulties, yet is restricted to problems that we think model\nscientific\/engineering contexts like the GC-MS optimization\nproblem. For this reason, the problems are relatively low-di-\nmensional (having up to eight real-valued dimensions), are not\npathologically rugged, and have either two or three objectives.\nIn measuring performance, we give some rough judgments\non the worst-case performance in addition to applying normal\nstatistical tests to estimate the location of the distribution.\nWorst-case performance is of particular relevance in scenarios\nwhere only one shot at the experiments\/optimization is usually\npossible, although it is difficult to estimate empirically. The\nchoice of test functions and the methods for measuring perfor-\nmance used here should provide a basis for future comparison\nof other algorithms for expensive, low-dimensional multiobjec-\ntive problems. To facilitate this, problems, assessment methods,\nand results are available for download at [37].\nOrganization: The rest of this paper is organized as follows.\nIn Section II, a review of relevant literature in engineering\noptimization methods, landscape approximation, and MOEAs\nis given. An outline of EGO, a powerful global optimization\nmethod for expensive functions, is provided in Section III,\ntogether with results demonstrating our implementation of the\napproach. Section IV extends EGO to the multiobjective algo-\nrithm, ParEGO, which we propose here. Section V describes\nthe choice of test functions, and Section VI details our methods\nfor performance assessment and comparison. Section VII sets\nout the method of comparison, giving all parameter settings,\nSection VIII presents the results, and Section IX provides a\nsummary and conclusion.\nII. RELATED WORK\nThe rise of EAs for multiobjective optimization in recent\nyears is now well documented, with thorough reviews of the\nhistory and current state of the art available in [6], [9], and [16].\nMuch of the success that the field is enjoying relies on the flex-\nibility with which MOEAs can deal with various optimization\ncontexts and features, such as: high dimensionality; integer,\nreal and mixed parameters; lack of knowledge about ranges\nof fitness functions; nondifferentiability of the cost landscape;\nconstraints, and so on. The initial high computational overhead"},{"page":3,"text":"52 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 1, FEBRUARY 2005\nof some early methods has also been overcome by a gradual\nprogression with the introduction of more efficient algorithms\nsuch as niched Pareto genetic algorithm (NPGA) [30] and\nPareto archived evolution strategy (PAES) [40], which first\nattempted to use reduce the cost of niching, through to the\nmicro-GA [7], the popular NSGA-II [17], and, more recently,\nparticle-swarm optimization algorithms [8]. More efficient data\nstructures that can be used to speed up the niching, selection,\nand archiving processes that these MOEAs rely upon are also\nnow available [32].\nIn engineering, MOEAs have found many applications [15],\nfrom airplane wing optimization [29], [44] and spacecraft tra-\njectory planning [13] to the design of irrigation networks [5];\nseveral of these obviously entail expensive simulation or exper-\nimental steps. Nonetheless, and perhaps due to the focus made\non improvements to the computational overhead of MOEA\u2019s\nper evaluation, their effectiveness in applications where very\nfew function evaluations can be afforded has only recently been\ninvestigated in a few isolated papers. For example, two MOEAs\nthat sample the search space by making explicit use of all pre-\nviously visited points, and explicitly avoid, oversampling of fit\nregions, were proposed in [31], and in [25], the case of dynamic\nmultiobjective optimization, where the fitness function changes\novertime,requiringarapidreoptimizationtotrackit,wastackled\nusing a hybrid approach combining deterministic local search\nroutines that are efficient in function evaluations, with a (1\n1)-ES. These kinds of methods appear to be promising, outper-\nforming more conventional MOEAs on the test problems used,\nwhen the number of function evaluations is the limiting factor.\nNonetheless,itwouldseemthatfurthersavingsofpreciousfunc-\ntion evaluations could be made by learning the cost landscape\nfrom all previously visited points, and by using this to estimate\nthe \u201cbest\u201d place to sample next (either to improve the model the\nmost or to obtain a better solution, or both).\nLearning a cost landscape from a set of solution\/cost pairs\nis variously called surrogate, approximate, or metamodeling\nin the literature. The idea of selecting the next solution to\nsample in order to maximize the improvement in the model\nis known as active learning [10] in the machine intelligence\nliterature. These approaches have received little attention (and\nonly very recently) in evolutionary multiobjective optimization\nbut metamodeling has a relatively rich history in optimization\nof a single objective. In design engineering, metamodeling\nis usually known as the response surface method [1], [42],\nand involves fitting a low-order polynomial via some form of\nleast squares regression. A closely related approach, deriving\nfrom geology is Kriging, whereby Gaussian process models\nare parameterised by maximum-likelihood estimation. A par-\nticular example of this is known as the design and analysis\nof computer experiments (DACE) model [52], which forms\nthe basis of the EGO algorithm, described in the next section.\nOptimization methods using response surfaces and\/or Kriging\nmetamodels have been successfully used in aeronautical de-\nsign\/engineering applications where only tens or hundreds of\nfunction evaluations are possible [57]. In the case of EGO,\nfour low-dimensional multimodal test functions have been\noptimized to within 1% of optimal in the order of 100 function\nevaluations [35].\nIn the EA community, response surface and Kriging methods\nhavealsobeenusedwithinEAs,tomodeleitherthegloballand-\nscape [4], [21], [29], [41], [50] or a local region of it [51], in\norder to reduce the number of expensive function evaluations\nthatneedtobe carriedoutduringoptimization.Inaddition,sim-\nilar goals have been obtained using artificial neural networks\n(either MLPs or RBF networks). Detailed reviews of all these\nmetamodel-basedEAscannowbefoundinrecentreviewpapers\n[33], [34], [48], along with new approaches that try to manage\nthe metamodels optimally with respect to the conflicting con-\ncerns of optimization and the design of experiments (i.e., im-\nproving the model) [47], [48]. Finally, a related but somewhat\ndifferent approach was proposed recently in [11], in which op-\ntimization of an expensive function is tackled by modeling fit-\nnesstransitionsusinga finitestatemachine and,offline,running\na number of different EAs on the model, in order to choose the\nmost efficient one. This approach may be attractive when one\nhas many optimization tasks to do on the same or very similar\nlandscapes but a certain investment is needed to build up a suf-\nficiently good model.\nReturning to the case of multiple objective functions, we see\nthat a few recent papers have begun to investigate the use of\nmetamodels in MOEAs. Most of these have considered the use\nof neural networks. The study in [43] demonstrates the use of\na neural network approximation combined with the NSGA-II\nalgorithm; the neural net having\ntive function. In this approach, generations using the real evalu-\nationfunctionare alternated withgenerations usingamultilayer\nperceptron model of the function derived from the samples col-\nlected from the earlier generations. Some speedup is observed\noverusingtheoriginalexactfitnessfunctionalone,butthestudy\nis limited to a single, curve-fitting problem. More recently, [27]\nand [28] propose and compare two more methods employing\nneural networks. The first method follows Nain and Deb\u2019s ap-\nproach, though with a different MOEA being used. The second\nmethodisasignificantdeparturefrommostothermetamodeling\nschemes, however: an inverse neural network is used to map\nback from a desired point in the objective space (beyond the\ncurrent Pareto front) to an estimate of the decision parameters\nthat would achieve it. Test function results presented in [28] for\nthe latter look particularly promising, though fairly long runs\n(of 20000 evaluations) are considered, and it is not clear that on\nmuch shorter runs the method would offer significant gains. Fi-\nnally, a different type of neural network, a self-organizing map,\nhas been employed in [3] to replace standard variation opera-\ntorswithadaptiveones.However,sofar,testshavenotexplicitly\ncomparedtheperformanceofthisapproachwithothermethods.\nMostrecently,threepapers[20],[22],and[23]haveappeared\nthat use a Kriging approach based on the DACE model [52]\nwithin an MOEA. This approach is the most similar to the one\ndeveloped independently by the author and described herein.\noutputs, one for each objec-\nIII. EGO ALGORITHM\nThe EGO algorithm for global optimization of expensive\nblack-box functions was first introduced and described in [35].\nIt makes use of \u201cKriging\u201d to model the search landscape from\nsolutions visited during the search. More specifically, it exploits"},{"page":4,"text":"KNOWLES: ParEGO: A HYBRID ALGORITHM WITH ON-LINE LANDSCAPE APPROXIMATION53\na version of the DACE model of [52], based on Gaussian pro-\ncesses. We do not give a mathematical description of EGO or\nDACE here, but simply provide some motivation for its choice\nin this study, and a brief description of the main procedures\ninvolved. The reader is referred to [35] for a fuller explanation.\nTheDACEmodelusedinJones[35],likeanyothermethodof\nsupervisedlearning,issubjecttocertainno-free-lunchtheorems\n[59], similar to those for optimization, that means its general\napplication cannot be recommended on any theoretical basis.\nNonetheless, on functions where we expect a degree of local\nsmoothness, where noise is low, and the number of dimensions\nis not excessive,the DACE model seems to be a good choice for\nbuilding the kind of approximation that is needed by a search\nalgorithm restricted to a low number of function evaluations. In\nparticular:\n\u2022\nThe model always interpolates the points (independently\nof the chosen parameters of the model).\nThemodelhasasmall,fixednumberofparameters:\nof them, where\nis the dimension of the function to be\noptimized.\nThe likelihood of the model given the data has a simple\nclosed form expression from which it is possible to\ncompute the maximum-likelihood model\u2014giving the\napproach a sound statistical interpretation.\nThe error in the expected cost of a solution also has a\nsimple,closedformexpression.Thus,themodelestimates\nits own uncertainty.\n\u2022\n\u2022\n\u2022\nThelastpointaboveisofparticularrelevance.Knowledgeofthe\nerrorintheresponsesurfaceisausefulpropertywhensearching\na costlandscape,and EGOmakesuseofthispropertyexplicitly,\nas we explain below.\nThe EGO algorithm begins by first generating a number of\nsolutions in a latin hypercube (i.e., a space-filling design), and\nby then finding the maximum-likelihood DACE model that best\nexplains these solutions (making use of some suitable optimiza-\ntion algorithm). To generate a new solution to evaluate, EGO\nsearches for the solution that maximizes what Jones et al. [35]\ncall \u201cthe expected improvement\u201d\u2014the part of the curve of the\nstandard error in the model that lies below the best cost sam-\npled so far (see Fig. 1). This effectivelymeans that EGO weighs\nup both the predicted value of solutions, and the error in this\nprediction, in order to find the one that has the greatest poten-\ntial to improve the minimum cost. EGO does NOT just choose\nthe solution that the model predicts would minimize the cost.\nRather, it automatically balances exploitation and exploration:\nwhere a solution has low predicted cost and low error, it may\nnot be as desirable as a solution whose predicted cost is higher\nbut whose associated error of prediction is also higher. Impor-\ntantly, the EGO algorithm uses a closed form expression for the\nexpected improvement, and it is thus possible to search the de-\ncision space globally for the solution that maximizes this. Once\na new solution has been chosen and evaluated (using the true,\nexpensive cost function), the DACE model is updated with this\nnew information, and the next solution is chosen using this up-\ndated model.\nComputing the maximum-likelihood model at each step of\nEGO requires several hundred or thousand matrix inversions on\na square matrix of dimension equal to the number of solutions\nFig. 1.\nabove) can be treated as if the value there were a realization of a normal random\nvariable with mean and standard deviation given by the DACE model and its\nstandard error [adapted from [35]].\nThe uncertainty about the function\u2019s value at a point (such as ? ? ?\nso far sampled. As such, it is rather expensive in terms of com-\nputation per evaluation, and this overhead increases with time.\nHowever,ifEGO is efficientwithrespectto thenumberof func-\ntion evaluations needed to reach a given cost, the computational\ncostcanbeconsideredirrelevantinmanyoptimizationcontexts.\nBefore we go on to explain how we extend EGO to the multi-\nobjective optimization case, we first give details of our own im-\nplementation of the basic algorithm, and some illustrative test\nresults.\nA. Implementation of EGO\nEGO was implemented in C using the open source matpack\nlibrary4for coding the matrix operations. The initial solutions\nare generated using a latin hypercube routine, following a de-\nscription in Numerical Recipes [49]. The number of initial so-\nlutionsissetto ,where\nto be optimized, as suggested in [35].\nIn order to optimize the likelihood function, the Nelder and\nMead downhill simplex routine from Numerical Recipes [49]\nhas been used. This is restarted 20 times in order to give a more\nrobust search of the model space. In order to find the best solu-\ntion to visit next, a genetic algorithm is embedded within EGO,\nto search the decision parameter space globally. We have not\nused branch and bound to find the best solution to visit, as sug-\ngested by Jones et al., simply because of the difficulty of imple-\nmenting it, particularly for the more complicated case of con-\nstrained optimization, which we would like to tackle in future.\nBecause the computational overhead increases with each\nfunction evaluation, we have found it necessary to cap the size\nof the correlation matrix (on which the model is based) to have\na maximum dimension of 80.5This means that beyond the\n80th function evaluation, 80 solutions are selected at random\nwithout replacement from the list of all previously visited\nsolutions\u2014and the DACE model for that iteration is based on\nthese. Potentially, this reduces the accuracy and reliability of\nEGO (potentially causing it to revisit points). However, beyond\nmatrix sizes of about 80\u2013100, EGO was taking several minutes\nisthedimensionofthefunction\n4www.matpack.de.\n5A different library for matrix operations (from http:\/\/cpplapack.source-\nforge.net\/doc\/html\/), based on ATLAS, is being investigated with the hope that\nit will reduce this problem."},{"page":5,"text":"54 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 1, FEBRUARY 2005\nTABLE I\nRESULTS OF RUNNING OUR IMPLEMENTATION OF EGO ON 6 OF OUR 13 TEST FUNCTIONS.\nTHE REMAINING SEVEN ARE IN TABLE II\nColumn 1 gives the name of the function, while columns 2 and 3 give, respectively, its dimension ? and its\nnumber of local optima. Column 4 shows the median best evaluation reached from 51 runs of EGO, where\nthe number of function evaluations used in each run, is just ???, where ? is the dimension of the function.\nColumns 5 and 6 show the median evaluation reached by 51 runs of random search for the number of func-\ntion evaluations indicated. The median value which is closest to EGO\u2019s is shown in bold font. Columns 7\nand 8 summarize the results of Mann\u2013Whitney rank-sum tests between EGO and the random search algo-\nrithm. The \u201cEGO wins\u201d column gives the number of iterations that a random search can be run (in terms\nof a multiple of EGO\u2019s number of iterations), and still be statistically beaten by EGO (with highest asso-\nciated confidence level). The final column indicates the same thing, but where no difference between the\nalgorithms\u2019 performance can be found (and the lowest confidence level at which the test would have found\na difference had there been one). On five test functions, EGO is 250 times faster than random search, on\nfour others it is between 50 and 250 times faster. An interpretation of the results is that, e.g., on ?\nfinds a function value that is shared or bettered by less than approximately ???????? ? ?????? of the\nentire search space, in just 40 function evaluations. EGO generally performs worst on the functions with\nvery many local optima, though it does well on both the generalized Griewank and Schwefel functions (here\nwith six and ten dimensions, respectively, rather than the usual 30\u2014see TABLE II).\n, EGO\nper evaluation (and this begins to increase dramatically). For\ntesting, this is prohibitive, although in practice a function eval-\nuation can easily take much longer than this. In ParEGO, we\nalso cap the size of the model but use a slightly more advanced\nselection procedure, as described in Section IV.\nB. Testing the Implementation of EGO\nTesting of our implementation of EGO was carried out on\nmultimodal (epistatic) functions, up to a dimension of 10, taken\nfrom [41]. On each function, we ran EGO (51 times) for just\nevaluations (where is the dimension of the function). In\nordertogetsomeideaofthequalityoftheseresults,wecompare\nwith a random search algorithm (Tables I and II), and with the\npublishedresultsofLiangetal.[41],whoimplementanEAalso\nwithlandscapeapproximation(TableIII).Theformerresultsare\ngenerally more useful: these allow one to estimate the quality of\nsolutions reached by EGO in terms of the number of solutions\ninthesearchspace thathavethatcostora lowerone.Theresults\nshow that EGO is up to 250 times faster than random search (on\nfour of the functions), more than 50 times faster on a further\nfour, and, at worst, about the same as random search on one\nhighly multimodal function.\nThe results presented in Table III are useful only in that they\nsuggest EGO is performing very well on some functions (far\nbetter than the EA of Liang et al.); but in general, it is hard to\nmake comparisons because Liang et al.\u2019s algorithm is run for\n1 to 3 orders of magnitude longer (in terms of evaluations) than\nwe run EGO for.\nIV. EXTENDING EGO TO THE MULTIOBJECTIVE CASE: ParEGO\nThe EGO algorithm could be extended for use with multi-\nobjective optimization problems in a number of different ways.\nThe simplest approach, which we investigate here, converts the\ndifferent cost values of a solution into a single cost via a pa-\nrameterized scalarizing weight vector. By choosing a different\n(parameterization of the) weight vector at each iteration of the"},{"page":6,"text":"KNOWLES: ParEGO: A HYBRID ALGORITHM WITH ON-LINE LANDSCAPE APPROXIMATION55\nTABLE II\nRESULTS ON THE REMAINING SEVEN TEST FUNCTIONS. SEE TABLE I CAPTION. THE NUMBER\nOF LOCAL OPTIMA IN THE GRIEWANK FUNCTION IS NOT KNOWN\u2014THOUGH FOR\nA 30-DIMENSIONAL VERSION OF THE SAME FUNCTION IT IS ? 10\nTABLE III\nCOMPARISON OF EGO WITH PUBLISHED RESULTS OF RUNNING\nEANA [41] ON THE 11 LOWER DIMENSIONAL FUNCTIONS\nFor EGO, we ran for exactly ??? function evaluations (column 2) and quote\nthe mean and best cost found from 51 runs (columns 3 and 4, respectively).\nFor EANA, the algorithm was stopped when it reached either the global op-\ntimum (up to the precision of the computer) or some unspecified evalua-\ntion limit [41]. Thus, the mean number of evaluations is reported (column 5)\nand the mean cost of the solution over the 50 runs performed (last column).\nComparison is difficult but on some functions, EGO occasionally reaches\ncomparablefitnesslevelsoneortwoordersofmagnitudefasterthanthemean\nnumber of function evaluations quoted for EANA. (EANA is an evolution\nstrategy, also, based on building landscape approximations.)\nsearch, an approximation to the whole Pareto front can be built\nup gradually.\nParEGO begins by normalizing the\nspecttotheknown(orestimated)limitsofthecostspace,sothat\neach cost function lies in the range [ 0,1 ]. Then, at each itera-\ntion of the algorithm, a weight vector\nrandom from the set of evenly distributed vectors defined by\ncost functions with re-\nis drawn uniformly at\n(1)\nwith\nmany vectors there are in total. The scalar cost of a solution is\nthen computed using the augmented Tchebycheff function\n, so that the choice of determines how\n(2)\nwhere\nbe possible to use other scalarizing functions\u2014the augmented\nTchebycheff function has been chosen here because the non-\nlinear part of the function means that points on nonconvex re-\ngions of the Pareto front can be minimizers of this function and,\nthus, nonsupported solutions can be obtained, while the linear\nis a small positive value which we set to 0.05. It would"},{"page":7,"text":"56IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 1, FEBRUARY 2005\npart of the function ensures that solutions that are weakly domi-\nnated by Pareto optimal solutions are rewarded less than Pareto\noptimal ones. Using (2), the scalar costs of all previously vis-\nitedsolutionsiscomputedand,usingalloraselectionofthese,a\nDACEmodelofthelandscapeisconstructedbymaximum-like-\nlihood. The solution that maximizes the expected improvement\nwith respect to this DACE model is determined. This becomes\nthe next point, and is evaluated on the real, expensive cost func-\ntion, completing one iteration of ParEGO.\nPseudocode for the entire ParEGO algorithm is given in\nAlgorithm 1. As in our implementation of EGO, the Nelder\nand Mead downhill simplex algorithm is used (with 20 restarts)\nto maximize the likelihood of the DACE model (line 25 of\nAlgorithm 1). The EA used within ParEGO to search for the\nsolution which maximizes the expected improvement (line 28)\nis implemented as follows.\n\u2022\nPopulation size: 20 solutions.\n\u2022\nPopulation update: Steady state (one offspring produced\nper generation, from either a crossover or cloning event,\nfollowed by a mutation).\n\u2022\nGenerations\/evaluations: 10000 evaluations.\n\u2022\nReproductive selection: Binary tournament without\nreplacement.\n\u2022\nCrossover: Simulated binary crossover [18] with proba-\nbility 0.2, producing one offspring.\n\u2022\nMutation:Parametervalueshiftedby\neter range), where\nis drawn uniformly at random from\n(0.0001, 1), and\n, the per- gene mutation probability, is\n.\n\u2022\nReplacement: Offspring replaces (first) parent if it is\nbetter, else it is discarded.\n\u2022\nInitialization: Five solutions are mutants6of the five best\nsolutions evaluated on the real fitness function under the\nprevailing\nvector; the remaining 15 solutions are gener-\nated in a latin hypercube in parameter space.\nIn practice, on a very expensive cost function, all solutions\nevaluated should be used to update the DACE model, at every\niteration. However, in our experiments (because of the need to\ndo21runsonalargenumberoffunctionstocollectperformance\ndata), we used a simple, heuristic method of choosing a subset\nof the solutions evaluated to update the model, as follows: At\neach iteration: 1) if the iteration number\nsolutions evaluated so far, are used to update the\nmodel and 2) if\na subset of\nused, where the first half of them are the best\ntheprevailingscalarizingvector\nat random without replacement.Further details of the parameter\nsettings used in ParEGO are given in Table V.\n.(param-\nis less than 25, all\nsolutions is\nsolutions under\nandtheotherhalfareselected\nV. TEST FUNCTION SUITE\nA. Notes on the Selection of Functions\nA number of good attempts at designing test function suites\nand\/or general schemes for test function generation have been\n6The mutation is carried out as described above except that mutants are\nchecked to ensure they are different from their parents.\nAlg. 1.ParEGO pseudocode.\nproposed in the multiobjective optimization literature, of which\nthose described in [19], [46], and [56] are some of the best. The\nDTLZ functions in [19] are very popular, partly because they"},{"page":8,"text":"KNOWLES: ParEGO: A HYBRID ALGORITHM WITH ON-LINE LANDSCAPE APPROXIMATION57\nare scalable in the number of objectives and parameters, and\npartly because each function is accompanied by a description\nexplainingexactlywhatdifficultyitposestoanMOEA,andalso\nallowing this to be easily tuned. However, while it is tempting\nto employ this suite wholesale, there is (at least) one drawback\nto it, which leads us to include functions from [46] and [56]\nas well.\nThedrawbackinDebetal.\u2019ssuiteisthatmostofthefunctions\nwork on a scheme where only\nsition parallel to the Pareto front in the objective space, with all\nthe remaining parameters controlling distance to the front (i.e.,\nthe problems are separable). From this, a natural bias is given\nto recombination-based EAs because, once a few points on the\ntrue Pareto front are found, recombination tends to propagate\nand preserve the parameters which are common to them (which\nis all of them, bar the first\n), while searching over the other\nparameters, leading to a very fast spreading out over the entire\nfront. Unfortunately, many real-world multiobjective optimiza-\ntion problems are not structured like this; rather, each different\noptimal tradeoff solution requires changing all or many of the\ndecision parameters.\nIn [46], similar observations in respect of the DTLZ func-\ntionshavebeenmade,andfurthermore,itwasalsoobservedthat\nthe Pareto fronts of many other test functions consist of piece-\nwise linear curves and\/or single points in the parameter space.\nThe paper goes on to propose a scheme for generating functions\nthat, like [19], allows some different problem difficulties to be\nincorporated and tuned, combined with a greater control of the\ncurves\/surfaces in parameter space comprising the Pareto front.\nTwoexamplefunctionsusingthisgeneralschemearegiven,and\nit is these that we employ in our suite.\nThe functions we employ from [56] are not part of an\noverall tunable scheme but have been popular in the literature.\nVLMOP2 has a concave Pareto front while VLMOP3\u2019s Pareto\nfront is both nonlinear and asymmetric.\nOverall, the final selection of nine test functions includes\nfunctions from two to eight decision parameters; functions with\na very low density of solutions at the Pareto front; functions\nwith locally optimal Pareto fronts; functions where the Pareto\nset follows a complicated curve in the parameter space; func-\ntions where the Pareto front is disconnected in objective space;\nand functions where the density of points parallel to the Pareto\nfront is nonuniformly distributed. There is, thus, a good deal of\nvariety in the difficulties that they pose. We have nonetheless\nbeen restrictive in some particular aspects: all functions are un-\nconstrained and while difficult, are not overly high-dimensional\n(in parameter space), and have a reasonable, rather than patho-\nlogical degree of ruggedness. And, we have kept to functions\nof two and three objectives only. These restrictions accord with\nour description (in Section I) of certain kinds of expensive en-\ngineering\/scientific problem, where we hope to obtain good re-\nsults in a very small number of function evaluations.\nIn the following, we describe each of our test functions in\nturn.\nparameters control the po-\nKNO1: This test function is the only one not borrowed from\nthe existing literature. There are just two parameters and two\nobjectives in the problem\nThe distancefrom the Paretofront, controlled by , is a function\nof the sum of the two decision parameters, while the location\ntransverse to the Pareto front is controlled by the difference be-\ntween the two parameters. Thus, each point on the Pareto front\nis unique in both parameters: the Pareto set is made up of all\npairs of parameters that sum to 4.4116. The true Pareto front\nhas a greater extent than, and lies just beyond, a locally optimal\nPareto front with a much larger basin of attraction. Altogether,\nthere are 15 locally optimal fronts in this function.\nOKA1: The OKA1 test function [46] is defined as\nThe Pareto optima (in the parameter space) lie on the curve\n. In addition, the density of\nsolutions falls away near to the Pareto front.\nOKA2: The OKA2 test function [46] is defined as\nThe Pareto optima lie on a spiral-shaped curve in the three-di-\nmensional parameter space. In addition, the density of solutions\nfalls away steeply near to the Pareto front.\nVLMOP2: Veldhuizen and Lamont\u2019s MOP2 function [56] is\nscalable in the number of decision variables. We use just two"},{"page":9,"text":"58IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 1, FEBRUARY 2005\nThe Pareto front is concave and the Pareto optima lie on the\ndiagonal passing from\n,\nrameter space.\nVLMOP3: Veldhuizen and Lamont\u2019s MOP3 [56] is a three-\nobjective function of two decision variables\nto, in pa-\nThis test function has a disconnected Pareto optimal set, and\nthe Pareto front is a curve \u201cfollowing a convoluted path through\nobjective space.\u201d\nDTLZ1a: We have selected four of the test functions de-\nscribed in [19], and have made alterations to them in order to\nkeep the number of decision variables small, and\/or to reduce\nthe ruggedness to \u201creasonable\u201d levels. We append an \u201ca\u201d to the\nname of these functions to show that we have changed them\nslightly.\nThe first function from [19], DTLZ1, is used with two objec-\ntives and six decision variables here. This gives the following\nfunction DTLZ1a:\nThe ruggedness of the function is controlled by the\nIn the original DTLZ1,\ntotal of\nlocal Pareto optimal fronts. This is excessively\nrugged for our purposes, hence, we use\noptimal set consists of all solutions where all but the first de-\ncision variables are equal to 0.5, and the first decision variable\nmay take any value in [0,1].\nDTLZ2a and DTLZ4a: The next two test problems have\nthree objective functions defined on eight decision variables.\nDTLZ2a and DTLZ4a are given by\nfunction.\nis used in the cosine term, giving a\ninstead. The Pareto\nif function is DTLZ2a\nif function is DTLZ4a\nThe Pareto front is 1\/8 of a sphere of radius1, centered on 0,0,0.\nThe Pareto optimal set consists of all solutions where all but\nthe first decision variables are equal to 0.5, and the first deci-\nsion variable may take any value in [0,1]. The effect of setting\nis to severely bias the density distribution of solutions\ntoward the\nand\nDTLZ7a: This problem has four disconnected regions in the\nPareto front (in objective space)\nplanes.\nVI. SELECTED PERFORMANCE ANALYSIS TECHNIQUES\nThe output of a multiobjective optimizer for a single run is\nan approximation set: the set of all nondominated points found\nduring the run. Assessing the performance of optimizers, thus,\nrelies upon some means of measuring or comparing the quality\nof one or more approximation sets. Many such performance in-\ndicators7exist in the literature but it has been shown in some\nrecent works [38], [39], [61] that several of these are not consis-\ntent with the partial ordering of approximation sets that follows\nlogically from the basic principles of Pareto optimization. For\nexample, in the worst case, two sets of nondominated points,\nand may be compared, where every point in\nby at least one point in\n, and yet\nunder the chosen indicator [39]. This is particularly the case for\nindicatorsthattrytoassessoneisolatedaspect,suchasthediver-\nsity, extent, or proximity to the Pareto front, of a nondominated\nset.Fortunately,indicatorsthatsimultaneouslytakeintoaccount\nall of these aspects of what it means to be a good nondominated\nset do exist, and moreover, they are compatible and\/or complete\nwith respect to the true partial ordering of nondominated sets.\nTwo such \u201cwell-behaved indicators,\u201d which we choose to use in\nour experiments, are the hypervolume indicator and the binary\nepsilon indicator described below. For a more detailed discus-\nsionofassessmentmethods,thereaderisreferredto[61],where\na full classification of indicators is given.\nIn addition to choosing appropriate performance indicators,\none must also decide how to collate and interpret the results\nfrom multiple runs, and how to present results graphically, if\nat all. Since, in our optimization scenario, it is likely that there\nwould be just one (or, perhaps, two) shots at performing the\noptimization, the average-case and worst-case performance are\nparamount. Hence, for each, test function the following infor-\nmation is given:\nis dominated\nyields a better evaluation\n7We prefer the term \u201cindicator,\u201d following [61], because the terms \u201cmeasure\u201d\nand\u201cmetric\u201d have reservedmeanings in mathematicsthat not allindicators con-\nform to."},{"page":10,"text":"KNOWLES: ParEGO: A HYBRID ALGORITHM WITH ON-LINE LANDSCAPE APPROXIMATION59\n\u2022 the mean\/median results of individual runs (as well as\nstandard deviation\/interquartile ranges);\nthe statistical significance of differences in the distribu-\ntions;\nfor the two-objective functions only, plots of the median,\nand worst attainment surfaces achieved;\nforthethree-objectivefunctionsonly,plotsoftheworstat-\ntainment surfaces achieved only (since 3-D plots showing\nother surfaces are visually confusing).\nWe give all details in the following sections and also make our\nassessmentmethodssourcecodeavailableforpublicuseat[37].\n\u2022\n\u2022\n\u2022\nA. Hypervolume Indicator (a.k.a the\nMeasure) [60]\nThis indicator assesses the size (hypervolume or Lebesgue\nintegral)oftheregiondominatedbyasampleset ofpoints,thus,\nlarger values generally indicate better nondominated sets. The\nregion itself must be bounded from above in some way, and for\nthis some point\nis chosen, which must itself be dominated by\nevery point in the sample set.\nThe hypervolume indicator\nthatgiventwosets,\nand ,andusingthesameboundingpoint,\nif\n, then it cannot be the case that\n(where better is used in the sense defined in [61]). However,\nit must be noted that this result does not imply that\nthan\n, either. Nonetheless, in the absence of preferences, and\nwithareasonablechoiceofboundingpoint,the\nresults that concur with intuition\u2014proximity to the true PF, ex-\ntent and evenness of solutions all tend to be rewarded.\nIn order to choose a bounding point for application of the\nmeasure, we use the following method. First, the collection\nof nondominated point sets from all runs of all algorithms are\naggregated into a single superset. Then, the ideal and the anti-\nideal point of this superset is found. The bounding point is then\ntheanti-idealpointshiftedby timestherange,ineachobjective\nis well-behaved in the sense\nis better than\nis better\nmeasuregives\nwhere\nrespectively, on the th objective, found within the superset.\nWe use\nhere.\nFor the analysis of multiple runs, we compute the\nsure of each individual run, and report the mean and the\nstandard deviation of these. Since the distribution of ParEGO\nand NSGA-II\u2019s results are not necessarily normal, we use\nthe Mann\u2013Whitney rank-sum test to indicate if there is a\nstatistically significant difference in the position of the two\ndistributions.\nandare the maximum and minimum value,\nmea-\nB. Additive Binary Epsilon Indicator\nThere are two ways in which the hypervolume indicator,\nused on its own, can be misleading. First, there is no way, from\nlooking at\nvalues in isolation, of inferring whether one set is\nactually better than another in a strict sense. Second, the choice\nof bounding point is rather arbitrary, and this can affect the\nordering of some pairs of sets.\nFig.2.\nof an optimizer. The two diagonal lines intersect the five surfaces at various\npoints; in both cases, the circle indicates the intersection that weakly dominates\nat least ? ? ? ? ? ? ? surfaces and is also weakly dominated by at least three\nsurfaces. Therefore, these two points both lie on the third summary attainment\nsurface.\nFiveattainmentsurfacesareshown,representingtheoutputoffiveruns\nA method that avoids these particular difficulties is the addi-\ntive binary epsilon indicator [61]. This takes a pair of nondom-\ninated sets\nand and returns a pair of numbers (,)\nwhere\nminimization.(Note,\n). A pair of numbers (\nstrictly better than\nindicates that neither set is strictly better than the other\u2014they\nare incomparable [61]. However, if\nweaker sense, it is better because the minimum\nso that approximation set\n-dominates\nvalue needed forto -dominate\nunnormalized objective values have been used in the computa-\ntion of\nand.\nTo summarize the results of multiple runs, we compute the\nbinary epsilon indicator for each pair of runs in turn and then\nreport on the median and interquartile range values of\n. We prefer the median and IQR to the mean and SD be-\ncause if the median for either algorithm is less than zero, then\nthis has a precise interpretation\u2014that it was strictly better than\nthe other algorithm on at least 50% of runs. We again use the\nMann\u2013Whitney rank sum test to decide if the distribution of\nvalues is different from the distribution of\nin , assuming\nisreadas\n,\nepsilon-dominates\n) indicates that is\n[61]. A pair of numbers (,)\nis less than, then in a\nvalue needed\nis smaller than the\n. In the results we present,\nand\nvalues.\nC. Median and Worst Attainment Surface Plots\nAn attainment surface is the surface uniquely determined by\na set of nondominated points that divides the objective space\ninto the region dominated by the set and the region that is not\ndominated by it [26]. Given\nruns of an algorithm, it would be\nnice to summarize the\nresults attainment surfaces obtained,\nusing only one or two summary surfaces. Such summary attain-\nment surfaces can be defined by imagining a diagonal line in\nthe direction of increasing objective values cutting through the\nresults attainment surfaces (see Fig. 2). The intersection on"},{"page":11,"text":"60 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 1, FEBRUARY 2005\nFig. 3.\nattainment surfaces that they define. The interpretation of the median\nattainment surface is that, for every point on it (independently), a point\n(weakly) dominating this was obtained in at least 50% of the nondominated\nsets. Similarly, the worst attainment surface indicates the goals achieved in\n100% of the sets. The best attainment surface indicates the goals achieved by\nthe aggregation of all sets. In this study, the best attainment surface is irrelevant\nand is not included in plotted results.\nFive sets of nondominated points and the best, median, and worst\nthis line that weakly dominates at least\nfaces and is weakly dominated by at least\npoint on the \u201c th summary attainment surface.\u201d This surface is\nthe union of all the goals that have been attained in at least\nruns (independently).\nObviously,fromthedefinitionofthe thsummaryattainment\nsurface, it is possible to define a median (summary) surface (for\nan odd number of approximation sets) and a best and worst\n(summary) surface. For illustration, we plot five approximation\nsets and their sample median, best, and worst attainment sur-\nfaces in Fig. 3. Notice that the interpretation of the median sur-\nface is that, for every point on it, a point (weakly) dominating\nthis was obtained in more than half of the algorithm runs. Sim-\nilarly, the worst attainment surface has the interpretation that in\nevery algorithm run the entire surface was (weakly) dominated.\nThus, plots of these surfaces provide more information than a\nmere scatter plot of the aggregation of (nondominated) points\nfound from several runs.\nFor the two-objective problems in this paper, we give the me-\ndian and worst attainment surface of ParEGO and of NSGA-II\non the same plot, with ParEGO\u2019s surfaces shown in solid and\nNSGA-II surfaces shown in dashed lines. For the three-objec-\ntive problems, we show just the worst attainment surface of the\ntwo algorithms, each on separate plots but with the same axes.\nThe worst attainment surface gives a very rough indication of\nthe worst-case performance of the two algorithms, although it\nhas no statistical significance.\nTo actually compute the points plotted, we use an algorithm\nbased on the one described in [53], and available from [37].\nof the sur-\nof them, defines one\nVII. EXPERIMENTAL DETAILS\nTo evaluate NSGA-II and ParEGO on the test suite, each al-\ngorithm is run 21 times, and all solutions visited are stored. The\nTABLE IV\nNSGA-II PARAMETER SETTINGS, WHERE ? IS THE NUMBER\nOF DECISION PARAMETER DIMENSIONS\nTABLE V\nParEGO PARAMETER SETTINGS, WHERE ? IS THE NUMBER\nOF DECISION PARAMETER DIMENSIONS\nnondominated sets achieved after a particular number of func-\ntion evaluations can then be determined and used to estimate\nperformance.\nPerformance was also compared with a random search run 21\ntimes for up to 10000 function evaluations on all problems.\nA. NSGA-II Parameter Settings\nThe implementation of NSGA-II that we use is the one avail-\nable for download from Deb\u2019s KANGAL web page. We use de-\nfault settings (Table IV), as supplied, with one exception. The\npopulation size is reduced to 20 in order to maximize the per-\nformance over the short run experiments. This size was deter-\nmined from some runs in which sizes from 10 to 50 were tried.\nWith a population size of 20, 13 generations gives 260 evalua-\ntions in all. This is comparable to the 250 evaluations given to\nParEGO. The performance of the two algorithms is also com-\npared at 100 evaluations.\nB. ParEGO Parameter Settings\nThe main choices in the parameter settings of ParEGO con-\ncern the number of scalarizing vectors to use [specified via the\nparameter\nin (1)], and in the parameters of the internal EA.\nFor the former, we chose a number which would allow sev-\neral \u201cpasses\u201d over each scalarizing vector. For the EA within\nParEGO, a small number of preliminary experiments were car-\nried out to determine settings that led to robust optimization\nof the expected improvement of the DACE model. The full set\nof parameter settings used in all runs of ParEGO are given in\nTable V.\nVIII. RESULTS\nCompared with a random search of the space, NSGA-II does\nwell on most problems, even though only 100 or 260 function\nevaluations are used. An impression of this can be had from the\nplots (Figs. 4 and 5) showing the median and worst attainment"},{"page":12,"text":"KNOWLES: ParEGO: A HYBRID ALGORITHM WITH ON-LINE LANDSCAPE APPROXIMATION61\nFig.4.\nafter 250\/260 function evaluations (right).\nAttainmentsurfaceplotwith1000randomsearchpointsalsoshown,ontheKNO1functionafter100functionevaluations(left),andontheOKA1function\nFig. 5.\nevaluations (right).\nAttainment surface plot with 1000 random points also shown, on OKA2 after 250\/260 function evaluations (left), and on VLMOP2 after 100 function\nTABLE VI\nVALUES OF THE ? MEASURE AFTER 100\/100 EVALUATIONS OF ParEGO\/NSGA-II.\nLARGER VALUES INDICATE BETTER PERFORMANCE. THE DISTRIBUTIONS OF THE\n? VALUES ARE TESTED USING THE MANN\u2013WHITNEY RANK SUM TEST.\nTHE ? VALUES AND SIGNIFICANCE LEVEL ARE INDICATED. ParEGO\nIS SIGNIFICANTLY BETTER THAN NSGA-II UNLESS STATED\nsurfaces for NSGA-II, compared with 1000 random solutions,\nplotted in objective space, on four of the test functions. Plots\nof the attainment surfaces for the random search algorithm, not\nshown here, suggest that NSGA-II is better than random search\nover 260 function evaluations on all the test problems.\nTables VI and VII give the results of applying the\nto the 21 runs of ParEGO and NSGA-II after, respectively, 100\nand 250 (260 for NSGA-II) function evaluations. From these\nmeasure\nresults, it is clear that ParEGO consistently dominates a larger\nregion of the objective space (modulo, the chosen bound point)\nat 250 evaluations. Not only is the mean higher, but the stan-\ndard deviation is consistently lower, sometimes by several or-\nders of magnitude, suggesting a much more reliable, and better\nworst-caseperformance.TheMann\u2013Whitneytestshowsthatthe\ndifferences between the two populations are significant at the\n99% confidence level, on all functions."},{"page":13,"text":"62IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 1, FEBRUARY 2005\nTABLE VII\nVALUES OF THE ? MEASURE AFTER 250\/260 EVALUATIONS OF ParEGO\/NSGA-II.\nLARGER VALUES INDICATE BETTER PERFORMANCE. THE DISTRIBUTIONS OF THE\n? VALUES ARE TESTED USING THE MANN\u2013WHITNEY RANK SUM TEST.\nTHE ? VALUES AND SIGNIFICANCE LEVEL ARE INDICATED.\nParEGO IS BETTER IN ALL CASES\nTABLE VIII\nMEDIAN AND INTERQUARTILE RANGE VALUES OF THE BINARY EPSILON INDICATOR AFTER\n100\/100 EVALUATIONS OF ParEGO\/NSGA-II. LOWER VALUES INDICATE BETTER\nPERFORMANCE. THE DISTRIBUTIONS OF THE ? VALUES ARE TESTED USING THE\nMANN\u2013WHITNEY RANK SUM TEST. THE ? VALUES AND SIGNIFICANCE LEVEL\nARE INDICATED. ParEGO IS SIGNIFICANTLY BETTER THAN NSGA-II UNLESS STATED\nTABLE IX\nMEDIAN AND INTERQUARTILE RANGE VALUES OF THE BINARY EPSILON INDICATOR\nAFTER 250\/260 EVALUATIONS OF ParEGO\/NSGA-II. LOWER VALUES INDICATE\nBETTER PERFORMANCE. THE DISTRIBUTIONS OF THE ? VALUES ARE TESTED\nUSING THE MANN\u2013WHITNEY RANK SUM TEST. THE ? VALUES AND\nSIGNIFICANCE LEVEL ARE INDICATED. ParEGO IS SIGNIFICANTLY\nBETTER THAN NSGA-II UNLESS STATED\nAt 100 function evaluations, the outcome is much the\nsame, except for the functions DTLZ4a and DTLZ7a, where\nNSGA-II obtains a higher mean value (and is significantly\nbetter on DTLZ7a). A possible explanation for the relatively\npoorer performance of ParEGO on these two functions is\nthat it has actually only \u201cchosen\u201d 13 of the solutions evalu-\nated\u2014the first 87 solutions being the latin hypercube\u2014since\nthese functions both have 8 decision variables. If one were\ninterested in a strong performance after 100 evaluations on\nthese functions, a sparser latin hypercube may be worth\nconsidering.\nTables VIII and IX give the equivalent results for the addi-\ntive binary epsilon indicator. The results are largely in agree-\nment with those of the\nmeasure, despite the fact that the\nmethod of measurement is quite different. Once again, it is ev-\nident that ParEGO has less variation, and that after 250 (260\nfor NSGA-II) evaluations it is not significantly behind on any\nproblem. Furthermore, on one problem, OKA2, its median in-\ndicator score is negative, indicating that its nondominated sets\nare better in a strict sense than NSGA-IIs on more than 50%\nof runs. On two or three other problems, its median indicator\nvalue is also very close to zero, and we can confirm that on"},{"page":14,"text":"KNOWLES: ParEGO: A HYBRID ALGORITHM WITH ON-LINE LANDSCAPE APPROXIMATION63\nFig. 6. Worst attainment surfaces after 250\/260 function evaluations for ParEGO (left) and NSGA-II (right) on the DTLZ2a function.\nFig. 7. Worst attainment surfaces after 250\/260 function evaluations for ParEGO (left) and NSGA-II (right) on the DTLZ4a function.\nFig. 8. Worst attainment surfaces after 250\/260 function evaluations for ParEGO (left) and NSGA-II (right) on the DTLZ7a function.\nthese problems its nondominated sets are strictly better than\nNSGA-IIs on several of the runs.\nThe plots of the median and worst attainment surfaces\n(Figs. 4\u20139) tend to confirm the outcome of the mea-\nsures, above. In particular, the worst attainment surface for\nNSGA-II is much worse than for ParEGO on several of the\nproblems.\nFinally, Fig. 10 shows the decision space solutions, and cor-\nresponding points in objective space, visited by NSGA-II and\nParEGO on the first run on function OKA1. This plot illus-\ntrates a considerable difference in the ways that ParEGO and\nNSGA-II sample the decision space, which has also been ob-\nserved in other decision space plots: NSGA-II is much more\naggressive at honing in on good points, exploiting them much"},{"page":15,"text":"64IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 1, FEBRUARY 2005\nFig. 9. Worst attainment surfaces after 250\/260 function evaluations for ParEGO (left) and NSGA-II (right) on the VLMOP3 function.\nFig. 10.\nand of NSGA-II after 250\/260 function evaluations on the OKA1 test function. Notice how ParEGO explores the parameter space much more evenly and yet still\nfinds more good points.\nSolutions in parameter space (left) with the Pareto set shownas a dark line.Thecorresponding points in objective space (right) for the firstrun of ParEGO\nmore than ParEGO. However, despite ParEGO\u2019s greater explo-\nration, it still manages to exploit the good solutions it has found\nvery effectively, and so returns both very good and \u201cvery av-\nerage\u201d points in the objective space. The different behavior of\nthe two algorithms can be easily understood as NSGA-II has no\nmeans of stopping its exploitation (i.e., its repeated sampling)\nof a fit point, other than its niching policy, whereas ParEGO\ndoes not visit points that have little or no expectation of being\nimproved (according to its landscape model), no matter how fit\nthey currently are with respect to other solutions.\nIX. SUMMARY AND CONCLUSION\nIn many optimization scenarios, the number of fitness eval-\nuations that can be performed is severely limited by cost con-\nstraints.Inthisstudy,theperformanceoftwomultiobjectiveop-\ntimization algorithms, ParEGO and NSGA-II, was measured on\nmuch shorter runs than used in most previous MOEA studies. A\nsuite of nine difficult, but low-dimensional, multiobjective test\nfunctions of limited ruggedness were used to evaluate the al-\ngorithms, and on these, it was confirmed that both ParEGO and\nNSGA-IIoutperformedarandomsearch,evenovera verysmall\nnumber of function evaluations.\nParEGOgenerallyoutperformedNSGA-IIonthetestedfunc-\ntions, atboth100 and 250function evaluations,especiallywhen\nthe worst-case performance was measured. This suggests that\nParEGO may offer a more effective search on problems like the\ninstrumentsetupoptimizationproblem,describedherein,where\nonly one function evaluation can be performed at a time. We are\ncurrently evaluating it on this problem, as well as some other\nclosely related \u201cwet experiment\u201d problems.\nAlthough ParEGO\u2019s performance was good on the tested\nfunctions, there are at least two potential drawbacks of the\ncurrent version.\n1) Normalization of the cost space relies on knowledge of\nthe cost limits.\n2) The use of uniformly random scalarizing vectors does\nnot necessarily result in the best distribution of nondom-\ninated points. Some parts of the Pareto front may be far\neasier to find than others, so that this may lead to a poor\napproximation.\nIn future work, it is our intention to investigate a method based\non adapting the scalarizing vectors, as a function of the points\nvisitedintheobjectivespace,inordertoobtainabetterimprove-\nment to the current nondominated front at each iteration."},{"page":16,"text":"KNOWLES: ParEGO: A HYBRID ALGORITHM WITH ON-LINE LANDSCAPE APPROXIMATION65\nThe addition of mechanisms to deal with integer and mixed-\ninteger problems, as well as constraints is also under way.\nACKNOWLEDGMENT\nThe author thanks D. Kell for leading the collaborative work\non the instrument setup optimization problem that led to this\npaper, and to J. Handl and the anonymous reviewers for helpful\ncomments and suggestions.\nREFERENCES\n[1] G. E. P. Box and N. R. Draper, Empirical Model Building and Response\nSurfaces. New York: Wiley, 1987.\n[2] M. Brown, W. Dunn, D. Ellis, R. Goodacre, J. Handl, J. Knowles, S.\nO\u2019Hagan,I.Spasic,andD.Kell,\u201cAMetabolomepipeline:Fromconcept\nto data to knowledge,\u201d Metabolomics, vol. 1, no. 1, pp. 39\u201351, 2005.\n[3] D. B\u00fcche, G. Guidati, P. Stoll, and P. Kourmoursakos, \u201cSelf-organizing\nmaps for Pareto optimization of airfoils,\u201d in Parallel Problem Solving\nfrom Nature\u2014PPSN VII, J. J. Merelo Guerv\u00f3s, Ed.\nSpringer-Verlag, 2002, vol. 2439, Lecture Notes in Computer Science,\npp. 122\u2013131.\n[4] D. Buche, N. Schraudolph, and P. Koumoutsakos, \u201cAccelerating evolu-\ntionaryalgorithmswithGaussianprocessfitnessfunctionmodels,\u201dIEEE\nTrans. Syst., Man, Cybern. C, vol. 35, no. 2, pp. 184\u2013194, May 2005, to\nbe published.\n[5] P. B. Cheung, L. F. Reis, K. T. Formiga, F. H. Chaudhry, and W. G.\nTicona, \u201cMultiobjective evolutionary algorithms applied to the rehabil-\nitation of a water distribution system: A comparative study,\u201d in Lec-\nture Notes in Computer Science, C. M. Fonseca et al., Eds.\nGermany: Springer-Verlag, 2003, vol. 2632, Proc. 2nd Int. Conf. Evol.\nMulti-Criterion Optim (EMO), pp. 662\u2013676.\n[6] C. A. Coello Coello, \u201cAn updated survey of GA-based multiobjective\noptimization techniques,\u201d ACM Computing Surveys, vol. 32, no. 2, pp.\n109\u2013143, 2000.\n[7] C. A. Coello Coello and G. Toscano Pulido, \u201cA micro-genetic algorithm\nfor multiobjective optimization,\u201d in Lecture Notes in Computer Science,\nE. Zitzler et al., Eds.Berlin, Germany: Springer-Verlag, 2001, vol.\n1993, Proc. 1st Int. Conf. Evol. Multi-Criterion Optimization, pp.\n126\u2013140.\n[8] C. A. Coello Coello, G. Toscano Pulido, and M. Salazar Lechuga,\n\u201cHandling multiple objectives with particle swarm optimization,\u201d IEEE\nTrans. Evol. Comput., vol. 8, no. 3, pp. 256\u2013279, Jun. 2004.\n[9] C. A. Coello Coello, D. A. Van Veldhuizen, and G. B. Lamont, Evo-\nlutionary Algorithms for Solving Multi-Objective Problems.\nMA: Kluwer, 2002. ISBN 0-3064-6762-3.\n[10] D. A. Cohn, L. Atlas, and R. E. Ladner, \u201cImproving generalization with\nactive learning,\u201d Mach. Learn., vol. 15, no. 2, pp. 201\u2013221, 1994.\n[11] D. Corne, M. Oates, and D. Kell, \u201cLandscape state machines: Tools for\nevolutionary algorithm performance analyzes and landscape\/algorithm\nmapping,\u201d in Applications of Evolutionary Computing.\nmany:Springer-Verlag,2003,vol.2611,LectureNotesinComputerSci-\nence, pp. 187\u2013198.\n[12] D. W. Corne, N. R. Jerram, J. D. Knowles, and M. J. Oates, \u201cPESA-II:\nRegion-based selection in evolutionary multiobjective optimization,\u201d in\nProc.GeneticandEvol.Comput.Conf.(GECCO),L.Spectoretal.,Eds.,\nSan Francisco, CA, 2001, pp. 283\u2013290.\n[13] V. Coverstone-Caroll, J. Hartmann, and W. Mason, \u201cOptimal multi-ob-\njective low-thrust spacecraft trajectories,\u201d Computer Methods in Appl.\nMechanics Eng., vol. 186, pp. 387\u2013402, 2000.\n[14] Z.S.Davies,R.J.Gilbert,R.J.Merry, D.B.Kell,M.K.Theodorou,and\nG. W. Griffith, \u201cEfficient improvement of silage additives by using ge-\nnetic algorithms,\u201d Appl. Environ. Microbiol., pp. 1435\u20131443, Apr. 2000.\n[15] K.Deb,\u201cEvolutionaryalgorithmsfor multi-criterionoptimizationin en-\ngineeringdesign,\u201din EvolutionaryAlgorithmsin Engineering andCom-\nputer Science, K. Miettinen et al., Eds.\nch. 8, pp. 135\u2013161.\n[16] K. Deb, Multi-Objective Optimization Using Evolutionary Algo-\nrithms. Chichester, U.K.: Wiley, 2001. ISBN 0-471-87339-X.\n[17] K. Deb, S. Agrawal, A. Pratab, and T. Meyarivan, \u201cA fast elitist non-\ndominated sorting genetic algorithm for multi-objective optimization:\nNSGA-II,\u201dIndianInst.fTechnol.,Kanpur,India,KanGALRep.200001,\n2000.\nBerlin, Germany:\nBerlin,\nNorwell,\nBerlin, Ger-\nChichester, U.K.: Wiley, 1999,\n[18] K. Deb and H. Beyer, \u201cSelf-adaptive genetic algorithms with simulated\nbinary crossover,\u201d Evol. Comput., vol. 9, no. 2, pp. 197\u2013221, 2001.\n[19] K. Deb, L. Thiele, M. Laumanns, and E. Zitzler, \u201cScalable test problems\nfor evolutionary multi-objective optimization,\u201d Comput. Eng. Netw.\nLab. (TIK), Swiss Fed. Inst. Technol. (ETH), Zurich, Switzerland, Tech.\nRep. 112, 2001.\n[20] M. Emmerich, N. Beume, and B. Naujoks, \u201cAn EMO algorithm using\nthe hypervolume measure as selection criterion,\u201d in Evolutionary\nMulti-Criterion Optimization\u2014EMO 2005, vol. 3410, LNCS. Berlin,\nGermany, 2005, pp. 62\u201376.\n[21] M. Emmerich, A. Giotis, M. \u00d6zdemir, T. B\u00e4ck, and K. Giannkoglou,\n\u201cMetamodel-assisted evolution strategies,\u201d in Parallel Problem Solving\nfrom Nature\u2014PPSN VII. Berlin, Germany: Springer-Verlag, 2002,\nvol. 2439, LNCS, pp. 361\u2013370.\n[22] M. Emmerich and B. Naujoks, \u201cMetamodel-assisted multiobjective op-\ntimization strategies and their application in airfoil design,\u201d in Adaptive\nComputing in Design and Manufacture VI, I. Parmee, Ed.\nmany: Springer-Verlag, 2004, pp. 249\u2013260.\n[23]\n, \u201cMetamodel-assisted multiobjective optimization strategies with\nimplicit constraints and their application in airfoil design,\u201d in Proc. ER-\nCOFTAC, K. Giannakoglou et al., Eds., 2004. CD-ROM.\n[24] J. R. G. Evans, M. J. Edirisinghe, and P. V. C. J. Eames, \u201cCombinatorial\nsearches of inorganic materials using the inkjet printer: Science philos-\nophy and technology,\u201d J. Eur. CeramiC Soc., vol. 21, pp. 2291\u20132299,\n2001.\n[25] M.Farina,K.Deb,andP.Amato,\u201cDynamicmultiobjectiveoptimization\nproblems: Test cases, approximations, and applications,\u201d IEEE Trans.\nEvol. Comput., vol. 8, no. 5, pp. 425\u2013442, Oct. 2004.\n[26] C. M. Fonseca and P. J. Fleming, \u201cOn the performance assess-\nment and comparison of stochastic multiobjective optimizers,\u201d in\nParallel Problem Solving from Nature\u2014PPSN IV, H.-M. Voigt, W.\nEbeling, I. Rechenberg, and H.-P. Schwefel, Eds.\nSpringer-Verlag, 1996, Lecture Notes in Computer Science, pp.\n584\u2013593.\n[27] A. Gaspar-Cunha and A. Vieira, \u201cA multi-objective evolutionary\nalgorithm using approximate fitness evaluation,\u201d in Proc. Int. Congr.\nEvol. Methods for Design, Optimization and Control With Applications\nto Industrial Problems EUROGEN, G. Bugeda, J. A-Dsidri, J. Pe-\nriaux, M. Schoenauer, and G. Winter, Eds., 2003, [Online]. Available:\nhttp:\/\/www.defi.isep.ipp.pt\/~asv\/papers\/moga.pdf.\n[28] A. Gaspar-Cunha and A. S. Vieira, \u201cA hybrid multi-objective evolu-\ntionary algorithm using an inverse neural network,\u201d in Proc. Hybrid\nMetaheuristics (HM 2004) Workshop at ECAI 2004, Valencia, Spain,\n2004, pp. 25\u201330.\n[29] K. C. Giannakoglou, \u201cDesign of optimal aerodynamic shapes using sto-\nchastic optimization methods and computational intelligence,\u201d Int. Rev.\nJ. Progr. Aerosp. Sci., vol. 38, pp. 43\u201376, 2002.\n[30] J.HornandN.Nafpliotis,\u201cMultiobjectiveoptimizationusingtheniched\npareto geneticalgorithm,\u201dUniv. Illinoisat Urbana\u2013Champaign,Urbana,\nIL, USA, Tech. Rep. IlliGAl Report 93005, 1993.\n[31] E. J. Hughes, \u201cMulti-objective binary search optimization,\u201d in Lecture\nNotes in Computer Science, vol. 2632, Proc. 2nd Int. Conf. Evol. Multi-\nCriterion Optimization (EMO), C. M. Fonseca, P. J. Fleming, E. Zitzler,\nK. Deb, and L. Thiele, Eds. Berlin, Germany, 2003, pp. 102\u2013117.\n[32] M.T.Jensen,\u201cReducingtherun-timecomplexityofmultiobjectiveEAs:\nThe NSGA-II and other algorithms,\u201d IEEE Trans. Evol. Comput., vol. 7,\nno. 5, pp. 503\u2013515, Oct. 2003.\n[33] Y. Jin, \u201cA comprehensive survey of fitness approximation in evolu-\ntionary computation,\u201d Soft Computing, vol. 9, no. 1, pp. 3\u201312, 2005.\n[34] Y. Jin, M. Olhofer, and B. Sendhoff, \u201cA framework for evolutionary\noptimization with approximate fitness functions,\u201d IEEE Trans. Evol.\nComput., vol. 6, no. 5, pp. 481\u2013494, Oct. 2002.\n[35] D. Jones, M. Schonlau, and W. Welch, \u201cEfficient global optimization of\nexpensive black-box functions,\u201d J. Global Optim., vol. 13, pp. 455\u2013492,\n1998.\n[36] J. Knowles, \u201cParameter setting for PESA-II on the closed-loop spec-\ntrometer optimization problem: A simulation study using short run\nexperiments,\u201d Internal report available on request from j.knowle-\nsumist.ac.uk, Apr. 2004.\n[37]\n, (2004). Supporting material for this paper. [Online]. Available:\nhttp:\/\/dbk.ch.umist.ac.uk\/knowles\/parego\/\n[38] J. Knowles and D. Corne, \u201cOn metrics for comparing nondominated\nsets,\u201dinProc.Congr.Evol.Comput.(CEC),vol.1,Piscataway,NJ,2002,\npp. 711\u2013716.\n[39] J. D. Knowles, \u201cLocal-search and hybrid evolutionary algorithms for\nPareto optimization,\u201d Ph.D. dissertation, Univ. Reading, Reading, U.K.,\n2002.\nBerlin, Ger-\nBerlin, Germany:"},{"page":17,"text":"66 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 10, NO. 1, FEBRUARY 2005\n[40] J.D.KnowlesandD.W.Corne,\u201cApproximatingthenondominatedfront\nusing the Pareto archived evolution strategy,\u201d Evol. Comput., vol. 8, no.\n2, pp. 149\u2013172, 2000.\n[41] K.-H. Liang, X. Yao, and C. Newton, \u201cEvolutionary search of approx-\nimated ?-dimensional landscapes,\u201d Int. J. Knowledge-Based Intelligent\nEng. Syst., vol. 4, no. 3, pp. 172\u2013183, 2000.\n[42] R. Myers and D. Montgomery, Response Surface Methodology.\nYork: Wiley, 1995.\n[43] P. K. S. Nain and K. Deb, \u201cA computationally effective multi-objective\nsearch and optimization technique using coarse-to-fine grain modeling,\u201d\nIITK, Kanpur, India, Tech. Rep. Kangal Rep. 2002005, 2002.\n[44] B. Naujoks, L. Willmes, T. B\u00e4ck, and W. Haase, \u201cEvaluating multi-cri-\nteria evolutionary algorithms for airfoil optimization,\u201d in Parallel\nProblem Solving from Nature\u2014PPSN VII, J. J. Merelo Guerv\u00f3s et al.,\nEds.Berlin, Germany: Springer-Verlag, 2002, vol. 2439, Lecture\nNotes in Computer Science, pp. 841\u2013850.\n[45] S. O\u2019Hagan, W. B. Dunn, M. Brown, J. D. Knowles, and D. B. Kell,\n\u201cClosed-loop, multiobjective optimization of analytical instrumen-\ntation: Gas chromatography\/time-of-flight mass spectrometry of the\nmetabolomes of human serum and of yeast fermentations,\u201d Analytical\nChemistry, vol. 77, no. 1, pp. 290\u2013303, 2004.\n[46] T. Okabe, Y. Jin, M. Olhofer, and B. Sendhoff, \u201cOn test functions for\nevolutionary multi-objective optimization,\u201d in Lecture Notes in Com-\nputer Science. Berlin, Germany: Springer-Verlag, 2004, Proc. 8th Int.\nConf. Parallel Problem Solving from Nature, pp. 792\u2013802.\n[47] Y.S.Ong,P.B.Nair,andA.J.Kean,\u201cEvolutionaryoptimizationofcom-\nputationallyexpensiveproblemsviasurrogatemodeling,\u201dAIAAJournal,\nvol. 41, no. 4, pp. 687\u2013696, 2003.\n[48] Y. S. Ong, P. B. Nair, A. J. Keane, and Z. Z. Zhou, \u201cSurrogate-assisted\nevolutionary optimization frameworks for high-fidelity engineering\ndesign problems,\u201d in Knowledge Incorporation in Evolutionary Com-\nputation. ser. Studies in Fuzziness and Soft Computing Series, Y. Jin,\nEd. Berlin, Germany: Springer-Verlag, 2004.\n[49] W. Press, S. Teukolsky, W. Vetterling, and B. Flannery, Numerical\nRecipes in C: The Art of Scientific Computing.\nCambridge Univ. Press, 1992.\n[50] A. Ratle, \u201cAccelerating the convergence of evolutionary algorithms by\nfitness landscape approximation,\u201d in Parallel Problem Solving from Na-\nture\u2014PPSN V, 1998, pp. 87\u201396.\n[51] R. G. Regis and C. A. Shoemaker, \u201cLocal function approximation in\nevolutionary algorithms for the optimization of costly functions,\u201d IEEE\nTrans. Evol. Comput., vol. 8, no. 5, pp. 490\u2013505, Oct. 2004.\n[52] J. Sacks, W. Welch, T. Mitchell, and H. Wynn, \u201cDesign and analysis\nof computer experiments (with discussion),\u201d Statist. Sci., vol. 4, no.\n409\u2013435, 1989.\nNew\nCambridge, U.K.:\n[53] K. I. Smith, R. M. Everson, and J. E. Fieldsend, \u201cDominance measures\nfor multi-objective simulated annealing,\u201d in Proc. Congr. Evol. Comput.\n(CEC), vol. 1, Portland, OR, 2004, pp. 23\u201330.\n[54] S. Vaidyanathan, D. I. Broadhurst, D. B. Kell, and R. Goodacre, \u201cEx-\nplanatoryoptimizationofproteinmassspectrometryviageneticsearch,\u201d\nAnalytical Chemistry, vol. 75, no. 23, pp. 6679\u20136686, 2003.\n[55] S. Vaidyanathan, D. Kell, and R. Goodacre, \u201cSelective detection of pro-\nteins in mixtures using electrospray ionization mass spectrometry: In-\nfluence of instrumental settings and implications for proteomics,\u201d Ana-\nlytical Chemistry, vol. 76, pp. 5024\u20135032, 2004.\n[56] D. A. V. Veldhuizen and G. B. Lamont, \u201cMultiobjective evolutionary\nalgorithm test suites,\u201d in Proc. ACM Symp. Appl. Comput., J. Carroll et\nal., Eds., San Antonio, TX, 1999, pp. 351\u2013357.\n[57] A. Vicini and D. Quagliarella, \u201cAirfoil and wing design using hybrid\noptimization strategies,\u201d AIAA Journal, vol. 37, no. 5, 1999.\n[58] D. Weuster-Botz and C. Wandrey, \u201cMedium optimization by genetic al-\ngorithm for continuous production of formate dehydrogenase,\u201d Process\nBiochemistry, vol. 30, pp. 563\u2013571, 1995.\n[59] D. H. Wolpert, \u201cThe lack of a priori distinctions between learning algo-\nrithms,\u201d Neural Comput., vol. 8, pp. 1341\u20131390, 1996.\n[60] E. Zitzler, \u201cEvolutionary algorithms for multiobjective optimization:\nMethods and applications,\u201d Ph.D. dissertation, Swiss Federal Inst.\nTechnol. (ETH), Zurich, Switzerland, 1999.\n[61] E.Zitzler,L.Thiele,M.Laumanns,C.M.Fonseca,andV.G.daFonseca,\n\u201cPerformance assessment of multiobjective optimizers: An analysis and\nreview,\u201d IEEE Trans. Evol. Comput., vol. 7, no. 2, pp. 117\u2013132, Apr.\n2003.\nJoshua Knowles received the B.Sc. (Hons.) degree\nin physics with mathematics, the M.Sc. degree in in-\nformation systems engineering (at Distinction level),\nand the Ph.D. degree from the University of Reading,\nReading,U.K.,in1993,1996,and2002,respectively.\nFrom 2001 to 2003, he worked as a European\ncommission Marie Curie Research Fellow at\nIRIDIA, Free University of Brussels, Brussels,\nBelgium. Since 2003, he has held a BBSRC David\nPhillips Fellowship at the University of Manchester,\nManchester, U.K., where his research is focused on\napplications of multiobjective optimization and machine learning to computa-\ntional biology. He was a tutorial speaker at EMO2005 and is a semi-plenary\nspeaker at MOPGP 2006.\nDr. Knowles is a member of the EPSRC Peer Review College."}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Joshua_Knowles\/publication\/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems\/links\/0deec530c884b04ef0000000.pdf","widgetId":"rgw25_56aba22f19918"},"id":"rgw25_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=3418871&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw26_56aba22f19918"},"id":"rgw26_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=3418871&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":3418871,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"0deec530c884b04ef0000000","name":"Joshua Damian Knowles","date":"Feb 25, 2014 ","nameLink":"profile\/Joshua_Knowles","filename":"download.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Joshua_Knowles\/publication\/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems\/links\/0deec530c884b04ef0000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Joshua_Knowles\/publication\/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems\/links\/0deec530c884b04ef0000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"ec027752bb08779a0f31521559544ad9","showFileSizeNote":false,"fileSize":"932.58 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"0deec530c884b04ef0000000","name":"Joshua Damian Knowles","date":"Feb 25, 2014 ","nameLink":"profile\/Joshua_Knowles","filename":"download.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Joshua_Knowles\/publication\/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems\/links\/0deec530c884b04ef0000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Joshua_Knowles\/publication\/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems\/links\/0deec530c884b04ef0000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"ec027752bb08779a0f31521559544ad9","showFileSizeNote":false,"fileSize":"932.58 KB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=jLTMeZITmVmKFq68GYfjh0VCv5HijSOBVdj3g1F07bjcPeY-Is1f1EM4xQJT--uFN7jUJsJujVNkfYW_25A9iQ.RlqGvmoW-3iFdnVMfYej9wYMgYtTzLEmTPZgSmlQLZ2DnZRmp8sU-I7SoF6GFOwFR6qU1WdiYuIYWbgbFuKdeQ","clickOnPill":"publication.PublicationFigures.html?_sg=IrneOIeDqs38JteQl3PE_cvSB09vBPR5kc9ePjeJPUwr7BwD8ywTOCNUvAwVQqo1jF0jG4Pmqfgx8X3TbHy3Zw.7nDB8JCFAzj47KiF-u0qE2ig62lAQNBEcMQ6nGbzpKOs_hgXMqytb_T1c9_JA5dfW_4TXEbRoVuZqHV8qo8kHQ"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FJoshua_Knowles%2Fpublication%2F3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems%2Flinks%2F0deec530c884b04ef0000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=ixc1k4p_xRDrsnaRJ1_iag9HF3lPN-CXVTkC4najQG0q6-27yJXkSmSz6ai3duXxSmT-vnPTh7XeKu4HLoCn-A","urlHash":"902021b47895db59331b407cf3c9d96d","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=tIRVST35wn6Oq4EWJgqf2BnouWO0pwFmy15lJgRi7wyciY7WGo2929gP3AcWIukGsiAZasCpTUUHpZ5mqM0tvhCYrl5ZuM9bl1bSSAMlzzc.RXcObOr2N1drt5BaytzjOHZDRLaZ-azOtLQ2SBE4ynMjUesDD2shgxXtlakGTDjX8zubz9gsq7G3NTYVTOBWZg.3NlGpCXATtqCVBgExGuCrinS78Lw4EOuMEz87e2tS_qAM5nDk5H9Y6MIgqsNEr4DLDPol1ZieaxdnEfAfpKDCA","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"0deec530c884b04ef0000000","trackedDownloads":{"0deec530c884b04ef0000000":{"v":false,"d":false}},"assetId":"AS:98882319028230@1400586790683","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":3418871,"commentCursorPromo":null,"widgetId":"rgw28_56aba22f19918"},"id":"rgw28_56aba22f19918","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FJoshua_Knowles%2Fpublication%2F3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems%2Flinks%2F0deec530c884b04ef0000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A98882319028230%401400586790683&publicationUid=3418871&linkId=0deec530c884b04ef0000000&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"ParEGO: A Hybrid Algorithm With On-Line Landscape Approximation for Expension Multiobjective Optimization Problems","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=r9jGGSLVFwrt6rPhPddbeAQsRj5KQFbJfNQorPsoan0eEyP1BG5SUob07-pjs6ONfXHRV3-Xtpl_xQ_EMuOXMYwuaCW2h0UfMkQpGRipucg.MluNuIWD9WuOZMuZk4gPh7OKuK7Ha2VOGHUZJJALweipqs3Dtun1qwD0w70sko0Qk5fL2oH5JlIW_4YHyzI4dQ.3zztV4sVj3LxuVvny28z6DOwGZD3q2-lG_TIiSfF89_nehL8UZNNKd4mMSAGRWjM-TXzggu4U3TTbqQYdpOpbQ","publicationUid":3418871,"trackedDownloads":{"0deec530c884b04ef0000000":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw30_56aba22f19918"},"id":"rgw30_56aba22f19918","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw31_56aba22f19918"},"id":"rgw31_56aba22f19918","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw32_56aba22f19918"},"id":"rgw32_56aba22f19918","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw33_56aba22f19918"},"id":"rgw33_56aba22f19918","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw34_56aba22f19918"},"id":"rgw34_56aba22f19918","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw29_56aba22f19918"},"id":"rgw29_56aba22f19918","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw27_56aba22f19918"},"id":"rgw27_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56aba22f19918"},"id":"rgw2_56aba22f19918","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":3418871},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=3418871&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56aba22f19918"},"id":"rgw1_56aba22f19918","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"QZGqFrPPzw+qDr\/Pimu2hqZKA\/cEi+iXn\/FgSIWotypVbaE5t\/WJelN8eol6IrxVv+65gwZpjgyAOBCve11o9XPp2oVKA0oD08TOJLs3BzI7HL4ElTbr8pdW7pj5NRLInzT1cnjj7p97i\/h0YyHoWRPMOnwGJ+Tx0J9b7IBXH2A2h568QiDQYvOEfphVKqR0PB7M++hxf42fPhuM1yVP2RpDy0GL9WMo0O4+bzKVGQpgnHcm58AooGsUavMYoAgdv3wLUlpuUrJVR8gR+VjdWqWzsbJXWn4EUNN+mX3thZw=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"ParEGO: A Hybrid Algorithm With On-Line Landscape Approximation for Expension Multiobjective Optimization Problems\" \/>\n<meta property=\"og:description\" content=\"This paper concerns multiobjective optimization in scenarios where each solution evaluation is financially and\/or temporally expensive. We make use of nine relatively low-dimensional,...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems\/links\/0deec530c884b04ef0000000\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems\" \/>\n<meta property=\"rg:id\" content=\"PB:3418871\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1109\/TEVC.2005.851274\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"ParEGO: A Hybrid Algorithm With On-Line Landscape Approximation for Expension Multiobjective Optimization Problems\" \/>\n<meta name=\"citation_author\" content=\"Joshua Knowles\" \/>\n<meta name=\"citation_publication_date\" content=\"2006\/03\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"IEEE Transactions on Evolutionary Computation\" \/>\n<meta name=\"citation_issn\" content=\"1089-778X\" \/>\n<meta name=\"citation_volume\" content=\"10\" \/>\n<meta name=\"citation_issue\" content=\"1\" \/>\n<meta name=\"citation_firstpage\" content=\"50\" \/>\n<meta name=\"citation_lastpage\" content=\"66\" \/>\n<meta name=\"citation_doi\" content=\"10.1109\/TEVC.2005.851274\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Joshua_Knowles\/publication\/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems\/links\/0deec530c884b04ef0000000.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-c64b4405-4ef6-4be5-80a7-58c65c33f935","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":802,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw35_56aba22f19918"},"id":"rgw35_56aba22f19918","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-c64b4405-4ef6-4be5-80a7-58c65c33f935", "3342c9eb98fe8d727b1de7191dc49aeb3f7db965");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-c64b4405-4ef6-4be5-80a7-58c65c33f935", "3342c9eb98fe8d727b1de7191dc49aeb3f7db965");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw36_56aba22f19918"},"id":"rgw36_56aba22f19918","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/3418871_ParEGO_A_Hybrid_Algorithm_With_On-Line_Landscape_Approximation_for_Expension_Multiobjective_Optimization_Problems","requestToken":"\/ZQkwzHiMYbMfbX7bgrtrjpiQoPbmzjXozNMuZurxzoxJKBWr1fSqp8UdAEx1O4wmmt6lG8I6XQy71JDhwodUzLNYuKBCc5EOGrVLvKzTRQT2mh\/v4fgROFpPjEW73tp7tQmV8dDvPtqoC06GxGbQH4WxIz+4LS32Yr1Bp2B56Xb23SLu8wkuNouKtNR+gTrf6muwkwR\/jHsHhbbQd+h3LqtNndsL4W2xW8lm5RDuupglGl0XpxsYo9qFzrV2ucThT2aceQ0AjYA87CaGAaBY895itCNUugX\/1trnR1+h0M=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=2MSkKopC5gdlSs2MpplA69W5h_8nq2pYx6rJU43FpbOeYlGg-0mZUvHE05uXvclC","encodedUrlAfterLogin":"cHVibGljYXRpb24vMzQxODg3MV9QYXJFR09fQV9IeWJyaWRfQWxnb3JpdGhtX1dpdGhfT24tTGluZV9MYW5kc2NhcGVfQXBwcm94aW1hdGlvbl9mb3JfRXhwZW5zaW9uX011bHRpb2JqZWN0aXZlX09wdGltaXphdGlvbl9Qcm9ibGVtcw%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw38_56aba22f19918"},"id":"rgw38_56aba22f19918","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw37_56aba22f19918"},"id":"rgw37_56aba22f19918","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw39_56aba22f19918"},"id":"rgw39_56aba22f19918","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
