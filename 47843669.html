<!DOCTYPE html> <html lang="en" class="" id="rgw38_56ab9f7b0d775"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="ubJMXgSpz4eL2AZBIndrrqGht3N1DM6cgMOndPPHHVvYDGHhU/DgnNFaIe1flKWQRz7zdX7yLTad4fDdsQ4RiwsoSNhpShX3HkzzT84/prXEUEunL7gasqGDK0H4zNDM6upSLOElu/D0gcxfIWqcaid3r0llI8wzUa/VNyUkQ8nJxMnc3tNnC+OM1uaJ4FTy+JwyyKElxi2mr/2FE0dBiYXYDaH2PGswCFu+/Y4CWeCIgrNfQOvkAGU+/tM8VMdOB2KYO6Z9pcH7FKBYOtx9sB8MZvTDqP1vs8wzU79Li4g="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-340d01a6-7a15-4a59-bfe2-ea65610226db",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Nonparametric Bayesian sparse factor models with application to gene
expression modeling" />
<meta property="og:description" content="A nonparametric Bayesian extension of Factor Analysis (FA) is proposed where
observed data $\mathbf{Y}$ is modeled as a linear superposition, $\mathbf{G}$,
of a potentially infinite number of..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling/links/55554b9d08ae6fd2d821cdba/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling" />
<meta property="rg:id" content="PB:47843669" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1214/10-AOAS435" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Nonparametric Bayesian sparse factor models with application to gene
expression modeling" />
<meta name="citation_author" content="David Knowles" />
<meta name="citation_author" content="Zoubin Ghahramani" />
<meta name="citation_publication_date" content="2010/11/29" />
<meta name="citation_journal_title" content="The Annals of Applied Statistics" />
<meta name="citation_issn" content="1932-6157" />
<meta name="citation_volume" content="5" />
<meta name="citation_issue" content="2011" />
<meta name="citation_doi" content="10.1214/10-AOAS435" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/David_Knowles2/publication/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling/links/55554b9d08ae6fd2d821cdba.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Nonparametric Bayesian sparse factor models with application to gene
expression modeling (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Nonparametric Bayesian sparse factor models with application to gene
expression modeling on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab9f7b0d775" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab9f7b0d775" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab9f7b0d775">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1214%2F10-AOAS435&rft.atitle=Nonparametric%20Bayesian%20sparse%20factor%20models%20with%20application%20to%20gene%0Aexpression%20modeling&rft.title=Annals%20of%20Applied%20Statistics%20-%20ANN%20APPL%20STAT&rft.jtitle=Annals%20of%20Applied%20Statistics%20-%20ANN%20APPL%20STAT&rft.volume=5&rft.issue=2011&rft.date=2010&rft.issn=1932-6157&rft.au=David%20Knowles%2CZoubin%20Ghahramani&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Nonparametric Bayesian sparse factor models with application to gene
expression modeling</h1> <meta itemprop="headline" content="Nonparametric Bayesian sparse factor models with application to gene
expression modeling">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling/links/55554b9d08ae6fd2d821cdba/smallpreview.png">  <div id="rgw8_56ab9f7b0d775" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56ab9f7b0d775" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/David_Knowles2" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="David A. Knowles" alt="David A. Knowles" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">David A. Knowles</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw10_56ab9f7b0d775" data-account-key="David_Knowles2">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/David_Knowles2"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="David A. Knowles" alt="David A. Knowles" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/David_Knowles2" class="display-name">David A. Knowles</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Stanford_University" title="Stanford University">Stanford University</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56ab9f7b0d775"> <a href="researcher/8159937_Zoubin_Ghahramani" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Zoubin Ghahramani</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56ab9f7b0d775">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/8159937_Zoubin_Ghahramani"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/8159937_Zoubin_Ghahramani" class="display-name">Zoubin Ghahramani</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/1932-6157_The_Annals_of_Applied_Statistics"><span itemprop="name">The Annals of Applied Statistics</span></a> </span>    (Impact Factor: 1.46).     <meta itemprop="datePublished" content="2010-11">  11/2010;  5(2011).    DOI:&nbsp;10.1214/10-AOAS435           <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1011.6293" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw13_56ab9f7b0d775" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>A nonparametric Bayesian extension of Factor Analysis (FA) is proposed where<br />
observed data $\mathbf{Y}$ is modeled as a linear superposition, $\mathbf{G}$,<br />
of a potentially infinite number of hidden factors, $\mathbf{X}$. The Indian<br />
Buffet Process (IBP) is used as a prior on $\mathbf{G}$ to incorporate sparsity<br />
and to allow the number of latent features to be inferred. The model's utility<br />
for modeling gene expression data is investigated using randomly generated data<br />
sets based on a known sparse connectivity matrix for E. Coli, and on three<br />
biological data sets of increasing complexity.</div> </p>  </div>   </div>      <div class="action-container"> <div id="rgw14_56ab9f7b0d775" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw29_56ab9f7b0d775">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw30_56ab9f7b0d775">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/David_Knowles2/publication/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling/links/55554b9d08ae6fd2d821cdba.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/David_Knowles2">David A. Knowles</a>, <span class="js-publication-date"> May 15, 2015 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw32_56ab9f7b0d775" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw33_56ab9f7b0d775" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw34_56ab9f7b0d775" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw35_56ab9f7b0d775" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw36_56ab9f7b0d775" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw37_56ab9f7b0d775" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw31_56ab9f7b0d775" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FDavid_Knowles2%2Fpublication%2F47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling%2Flinks%2F55554b9d08ae6fd2d821cdba.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw28_56ab9f7b0d775"  itemprop="articleBody">  <p>Page 1</p> <p>Submitted to the Annals of Applied Statistics<br />arXiv: math.PR/0000000<br />NONPARAMETRIC BAYESIAN SPARSE FACTOR<br />MODELS WITH APPLICATION TO GENE EXPRESSION<br />MODELLING<br />By David Knowles∗<br />and Zoubin Ghahramani†<br />University of Cambridge<br />A nonparametric Bayesian extension of Factor Analysis (FA) is<br />proposed where observed data Y is modeled as a linear superposi-<br />tion, G, of a potentially infinite number of hidden factors, X. The<br />Indian Buffet Process (IBP) is used as a prior on G to incorporate<br />sparsity and to allow the number of latent features to be inferred.<br />The model’s utility for modeling gene expression data is investigated<br />using randomly generated datasets based on a known sparse connec-<br />tivity matrix for E. Coli, and on three biological datasets of increasing<br />complexity.<br />1. Introduction.<br />ysis (FA) and Independent Components Analysis (ICA) are models which<br />explain observed data, yn∈ RD, in terms of a linear superposition of inde-<br />pendent hidden factors, xn∈ RK, so<br />Principal Components Analysis (PCA), Factor Anal-<br />yn= Gxn+ ?n<br />(1)<br />where G is the factor loading matrix and ?nis a noise vector, usually as-<br />sumed to be Gaussian. These algorithms can be expressed in terms of per-<br />forming inference in appropriate probabilistic models. The latent factors are<br />usually considered as random variables, and the mixing matrix as a parame-<br />ter to estimate. In both PCA and FA the latent factors are given a standard<br />(zero mean, unit variance) normal prior. In PCA the noise is assumed to<br />be isotropic, whereas in FA the noise covariance is only constrained to be<br />diagonal. A standard approach in these models is to integrate out the latent<br />factors and find the maximum likelihood estimate of the mixing matrix. In<br />ICA the latent factors are assumed to be heavy-tailed, so it is not usually<br />∗Supported by Microsoft Research through the Roger Needham Scholarship at Wolfson<br />College, University of Cambridge.<br />†Supported by EPSRC Grant EP/F027400/1<br />Keywords and phrases: Nonparametric Bayes, Sparsity, Factor Analysis, Markov Chain<br />Monte Carlo, Indian Buffet Process<br />AMS 2000 subject classifications: Primary 62H25; secondary 62F15<br />1<br />arXiv:1011.6293v1  [stat.AP]  29 Nov 2010</p>  <p>Page 2</p> <p>2<br />KNOWLES ET AL.<br />possible to integrate them out. In this paper we take a fully Bayesian ap-<br />proach, viewing not only the hidden factors but also the mixing coefficients<br />as random variables whose posterior distribution given data we aim to infer.<br />Sparsity plays an important role in latent feature models, and is desir-<br />able for several reasons. It gives improved predictive performance, because<br />factors irrelevant to a particular dimension are not included. Sparse models<br />are more readily interpretable since a smaller number of factors are asso-<br />ciated with observed dimensions. In many real world situations there is an<br />intuitive reason why we expect sparsity: for example, in gene regulatory<br />networks a transcription factor will only regulate genes with specific motifs.<br />In our previous work (Knowles and Ghahramani, 2007) we investigated the<br />use of sparsity the on latent factors xn, but this formulation is not appro-<br />priate in the case of modeling gene expression, where, as described above,<br />a transcription factor will regulate only a small set of genes, corresponding<br />to sparsity in the factor loadings, G. Here we propose a novel approach to<br />sparse latent factor modeling where we place sparse priors on the factor<br />loading matrix, G. The Bayesian Factor Regression Model of West et al.<br />(2007) is closely related to our work in this way, although the hierarchical<br />sparsity prior they use is somewhat different. An alternative “soft” approach<br />to incorporating sparsity is to put a Gamma(a,b) (usually exponential, i.e.<br />a = 1) prior on the precision of each element of G independently, result-<br />ing in the elements of G being marginally Student-t distributed a priori:<br />see Fokoue (2004), Fevotte and Godsill (2006), and Archambeau and Bach<br />(2009). A LASSO-based approach to generating a sparse factor loading has<br />also been developed (Witten, Tibshirani and Hastie, 2009; Zou, Hastie and<br />Tibshirani, 2004). We compare these sparsity schemes empirically in the<br />context of gene expression modeling.<br />A problematic issue with this type of model is how to choose the latent<br />dimensionality of the factor space, K. Model selection can be used to choose<br />between different values of K, but generally requires significant manual in-<br />put and still requires the range of K over which to search to be specified.<br />Zhang et al. (2004) applied Reversible Jump MCMC to PCA, which has<br />many of the advantages of our approach: a posterior distribution over the<br />number of latent dimensions can be approximated, and the number of la-<br />tent dimensions could potentially be unbounded. However, RJ MCMC is<br />considerably more complex to implement for sparse Factor Analysis than<br />our proposed framework.<br />We use the Indian Buffet Process (Griffiths and Ghahramani, 2006), which<br />defines a distribution over infinite binary matrices, to provide sparsity and<br />a framework for inferring the appropriate latent dimension of the dataset</p>  <p>Page 3</p> <p>NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS<br />3<br />using a straightforward Gibbs sampling algorithm. The Indian Buffet Pro-<br />cess (IBP) allows a potentially unbounded number of latent factors, so we<br />do not have to specify a maximum number of latent dimensions a priori. We<br />denote our model “NSFA” for “Non-parametric Sparse Factor Analysis”.<br />Our model is closely related to that of Rai and Daum´ e III (2008), and is a<br />simultaneous development.<br />2. The Model.<br />Z be a binary matrix whose (d,k)-th element represents whether observed<br />dimension d includes any contribution from factor k. We then model the<br />mixing matrix by<br />p(gdk|Zdk,λk) = ZdkN?gdk;0,λ−1<br />where λkis the inverse variance (precision) of the kth factor and δ0is a delta<br />function (pont-mass) at 0. Distributions of this type are sometimes known<br />as “spike and slab” distributions. We allow a potentially infinite number of<br />hidden sources, so that Z has infinitely many columns, although only a finite<br />number will have non-zero entries. This construction allows us to use the<br />IBP to provide sparsity and define a generative process for the number of<br />latent factors.<br />We assume independent Gaussian noise, ?n, with diagonal covariance ma-<br />trix Ψ. We find that for many applications assuming isotropic noise is too<br />restrictive, but this option is available for situations where there is strong<br />prior belief that all observed dimensions should have the same noise vari-<br />ance. The latent factors, xn, are given Gaussian priors. Figure 1 shows the<br />complete graphical model.<br />We will define our model in terms of Equation 1. Let<br />k<br />?+ (1 − Zdk)δ0(gdk)(2)<br />2.1. Defining a distribution over infinite binary matrices.<br />our infinite model by taking the limit of a series of finite models.<br />We now define<br />Start with a finite model.<br />finite K model and taking the limit as K → ∞. We then show how the<br />infinite case corresponds to a simple stochastic process.<br />We have D dimensions and K hidden sources. Recall that zdkof matrix<br />Z tells us whether hidden source k contributes to dimension d. We assume<br />that the probability of a source k contributing to any dimension is πk, and<br />that the rows are generated independently. We find<br />We derive the distribution on Z by defining a<br />P(Z|π) =<br />K<br />?<br />k=1<br />D<br />?<br />d=1<br />P(zdk|πk) =<br />K<br />?<br />k=1<br />πmk<br />k(1 − πk)D−mk<br />(3)</p>  <p>Page 4</p> <p>4<br />KNOWLES ET AL.<br />G<br />X<br />Y<br />Z<br />λ<br />α<br />Ψ<br />K<br />D<br />N<br />Fig 1. Graphical model<br />where mk=?D<br />choose the conjugate Beta(r,s) distribution for πk. For now we take r =α<br />and s = 1, where α is the strength parameter of the IBP. The model is<br />defined by<br />d=1zdkis the number of dimensions to which source k con-<br />tributes. The inner term of the product is a binomial distribution, so we<br />K<br />πk|α ∼ Beta<br />zdk|πk∼ Bernoulli(πk)<br />?α<br />K,1<br />?<br />(4)<br />(5)<br />Due to the conjugacy between the binomial and beta distributions we are<br />able to integrate out π to find<br />P(Z) =<br />K<br />?<br />k=1<br />α<br />KΓ(mk+α<br />K)Γ(D − mk+ 1)<br />Γ(D + 1 +α<br />K)<br />(6)<br />where Γ(.) is the Gamma function.<br />Take the infinite limit.<br />to order the non-zero rows of Z which allows us to take the limit K → ∞<br />and find<br />Griffiths and Ghahramani (2006) define a scheme<br />P(Z) =<br />αK+<br />h&gt;0Kh!exp(−αHD)<br />?<br />K+<br />?<br />k=1<br />(D − mk)!(mk− 1)!<br />N!<br />(7)</p>  <p>Page 5</p> <p>NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS<br />5<br />where K+ is the number of active features (i.e. non-zero columns of Z),<br />HD=?D<br />Go to an Indian Buffet. This distribution corresponds to a simple stochas-<br />tic process, the Indian Buffet Process. Consider a buffet with a seemingly<br />infinite number of dishes (hidden sources) arranged in a line. The first cus-<br />tomer (observed dimension) starts at the left and samples Poisson(α) dishes.<br />The ith customer moves from left to right sampling dishes with probability<br />mk<br />iwhere mkis the number of customers to have previously sampled dish k.<br />Having reached the end of the previously sampled dishes, he tries Poisson(α<br />new dishes. Figure 2 shows two draws from the IBP for two different values<br />of α.<br />j=1<br />1<br />jis the D-th harmonic number, and Khis the number of rows<br />whose entries correspond to the binary number h.<br />i)<br />factors (dishes)<br />genes (customers)<br />5 101520<br />5<br />10<br />15<br />20<br />25<br />30<br />35<br />40<br />45<br />50<br />(a) α = 4<br />factors (dishes)<br />genes (customers)<br />10 203040<br />5<br />10<br />15<br />20<br />25<br />30<br />35<br />40<br />45<br />50<br />(b) α = 8<br />Fig 2. Draws from the one parameter IBP for two different values of α.<br />If we apply the same ordering scheme to the matrix generated by this<br />process as for the finite model, we recover the correct exchangeable distri-<br />bution. Since the distribution is exchangeable with respect to the customers<br />we find by considering the last customer that<br />P(zkt= 1|z−kn) =mk,−t<br />D<br />(8)<br />where mk,−t=?<br />dimension follows a Poisson(α) distribution, and the expected number of<br />s?=tzks, which is used in sampling Z. By exchangeability<br />and considering the first customer, the number of active sources for each</p>  <p>Page 6</p> <p>6<br />KNOWLES ET AL.<br />entries in Z is Dα. We also see that the number of active features, K+=<br />?D<br />3. Related work.The Bayesian Factor Regression Model (BFRM)<br />of West et al. (2007) is closely related to the finite version of our model.<br />The key difference is the use of a hierarchical sparsity prior. Each element<br />of G has prior of the form<br />gdk∼ (1 − πdk)δ0(gdk) + πdkN?gdk;0,λ−1<br />The finite IBP model is equivalent to setting πdk= πk∼ Beta(α/K,1) and<br />then integrating out πk. In BFRM a hierarchical prior is used:<br />d=1Poisson(α<br />d) = Poisson(αHD).<br />k<br />?<br />πdk∼ (1 − ρk)δ0(πdk) + ρkBeta(πdk;am,a(1 − m))<br />where ρk∼ Beta(sr,s(1 − r)). Non-zero elements of πdkare given a diffuse<br />prior favoring larger probabilities (a = 10,m = 0.75 are suggested in West<br />et al. (2007)), and ρkis given a prior which strongly favors small values,<br />corresponding to a sparse solution (e.g. s = D,r =5<br />Note that on integrating out πdk, the prior on gdkis<br />gdk∼ (1 − mρk)δ0(gdk) + mρkN?gdk;0,λ−1<br />This hierarchical sparsity prior is motivated by improved interpretability<br />in terms of less uncertainty in the posterior as to whether an element of G<br />is non-zero. However, this comes at a cost of significantly increased compu-<br />tation and reduced predictive performance, suggesting that the uncertainty<br />removed from the posterior was actually important.<br />The LASSO-based Sparse PCA (SPCA) method of Zou, Hastie and Tib-<br />shirani (2004) and Witten, Tibshirani and Hastie (2009) has similar aims to<br />our work in terms of providing a sparse variant of PCA to aid interpreta-<br />tion of the results. However, since SPCA is not formulated as a generative<br />model it is not necessarily clear how to choose the regularization parameters<br />or dimensionality without resorting to cross-validation. In our experimental<br />comparison to SPCA we adjust the regularization constants such that each<br />component explains roughly the same proportion of the total variance as<br />the corresponding standard (non-sparse) principal component.<br />D).<br />k<br />?<br />4. Inference.<br />sources X, which sources are active Z, the mixing matrix G, and all hyper-<br />parameters. We use Gibbs sampling, but with Metropolis-Hastings (MH)<br />steps for sampling new features. We draw samples from the marginal distri-<br />bution of the model parameters given the data by successively sampling the<br />Given the observed data Y, we wish to infer the hidden</p>  <p>Page 7</p> <p>NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS<br />7<br />conditional distributions of each parameter in turn, given all other parame-<br />ters.<br />Since we assume independent Gaussian noise, the likelihood function is<br />P(Y|G,X,ψ) =<br />N<br />?<br />t=1<br />1<br />(2π)<br />D<br />2|ψ|<br />1<br />2<br />exp<br />?<br />−1<br />2(yn− Gxn)Tψ−1(yn− Gxn)<br />?<br />(9)<br />where ψ is a diagonal noise covariance matrix.<br />Notation.<br />variables not explicitly conditioned upon in the current state of the Markov<br />chain. The r-th row and c-th column of matrix A are denoted Ar:and A:c<br />respectively.<br />We use − to denote the “rest” of the model, i.e. the values of all<br />Mixture coefficients.<br />ual element of the IBP matrix, Zdk, determining whether factor k is active<br />for dimension d. Recall that λkis the precision (inverse covariance) of the<br />factor loadings for the k-th factor. The ratio of likelihoods can be calculated<br />using Equation 9. Integrating out the (d,k)-th element of the factor loading<br />matrix, gdk(whose prior is given by Equation 2) we obtain<br />?P(Y|gdk,−)N?gdk;0,λ−1<br />?<br />We first derive a Gibbs sampling step for an individ-<br />P(Y|Zdk= 1,−)<br />P(Y|Zdk= 0,−)=<br />k<br />?dgdk<br />P(Y|gdk= 0,−)<br />?1<br />(10)<br />=<br />λk<br />λexp<br />2λµ2<br />?<br />(11)<br />where we have defined λ = ψ−1<br />matrix of residualsˆE = Y − GX evaluated with gdk= 0. The dominant<br />calculation is that for µ since the calculation for λ can be cached. This<br />operation is O(N) and must be calculated D × K times, so sampling the<br />IBP matrix, Z and factor loading matrix, G is order O(NDK).<br />From the exchangeability of the IBP we can imagine that dimension d<br />was the last to be observed, so that the ratio of the priors is<br />dXT<br />k:Xk:+ λkand µ =<br />ψ−1<br />d<br />λXT<br />k:ˆEd:with the<br />P(Zdk= 1|−)<br />P(Zdk= 0|−)=<br />m−d,k<br />N − 1 − m−d,k<br />(12)<br />where m−d,kis the number of dimensions for which factor k is active, ex-<br />cluding the current dimension d. Multiplying Equations 11 and 12 gives the<br />expression for the ratio of posterior probabilities for Zdkbeing 1 or 0, which<br />is used for sampling. If Zdkis set to 1, we sample gdk|− ∼ N?µ,λ−1?with<br />µ,λ defined as for Equation 11.</p>  <p>Page 8</p> <p>8<br />KNOWLES ET AL.<br />factors (dishes)<br />genes (customers)<br />24681012<br />2<br />4<br />6<br />8<br />10<br />12<br />14<br />16<br />18<br />20<br />Fig 3. A diagram to illustrate the definition of κd, for d = 10.<br />Adding new features.<br />non-zero columns contribute to the likelihood and are held in memory. How-<br />ever, the zero columns still need to be taken into account since the number<br />of active factors can change. Let κdbe the number of columns of Z which<br />contain 1 only in row d, i.e. the number of features which are active only<br />for dimension d. Note that due to the form of the prior for elements of Z<br />given in Equation 12, κd= 0 for all d after a sampling sweep of Z. Figure 3<br />illustrates κdfor a sample Z matrix.<br />New features are proposed by sampling κdwith a MH step. It is possible<br />to integrate out either the new elements of the mixing matrix, g (a 1 × κd<br />vector), or the new rows of the latent feature matrix, X?(a κd×N matrix),<br />but not both. Since the latter generally has higher dimension, we choose to<br />integrate out X?and include gTas part of the proposal. Thus the proposal<br />is ξ = {κd,g}, and we propose a move ξ → ξ∗with probability J(ξ∗|ξ). In<br />this case ξ = ∅ since as noted above κd= 0 initially. The simplest proposal,<br />following Meeds et al. (2006), would be to use the prior on ξ∗, i.e.<br />Z is a matrix with infinitely many columns, but the<br />J(ξ) = P(κd|α) · p(g|κd,λk) = Poisson(κd;γ) · N(g;0,λ−1<br />α<br />D−1.<br />Unfortunately, the rate constant of the Poisson prior tends to be so small<br />k)<br />where γ =</p>  <p>Page 9</p> <p>NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS<br />9<br />that new features are very rarely proposed, resulting in slow mixing. To<br />remedy this we modify the proposal distribution for κdand introduce two<br />tunable parameters, π and λ.<br />(13)J(κd) = (1 − π)Poisson(κd;λγ) + π1(κd= 1)<br />Thus the Poisson rate is multiplied by a factor λ, and a spike at κd= 1 is<br />added with mass π. The proposal is accepted with probability min(1,aξ→ξ∗)<br />where<br />(14)<br />aξ→ξ∗ =P(ξ∗|−,Y )J(ξ|ξ∗)<br />P(ξ|−,Y )J(ξ∗|ξ)<br />where<br />=P(Y |ξ∗,−)P(κd|α)p(g|κd,λk)<br />P(Y |−)J(κd)p(g|κd,λk)<br />= al· ap<br />al=P(Y |ξ∗,−)<br />P(Y |−)<br />ap=P(κd|α)<br />J(κd)<br />(15)<br />=<br />Poisson(κd;γ)<br />Poisson(κd;λγ)<br />(16)<br />Note that we take J(ξ|ξ∗) = 1 since ξ = ∅. To calculate the likelihood ratio,<br />al, we need the collapsed likelihood under the new proposal:<br />P(Yd:|ξ∗,−) =<br />N<br />?<br />N<br />?<br />n=1<br />?<br />P(Ydn|ξ∗,x?<br />n,−)P(x?<br />n)dx?<br />(17)<br />=<br />n=1<br />(2πψ−1<br />d)−1<br />2(2π)<br />κd<br />2 |M|−1<br />2exp<br />?1<br />2(mT<br />nMmn− ψ−1<br />d<br />ˆE2<br />dn)<br />?<br />(18)<br />where we have defined M = ψ−1<br />matrix of residualsˆE = Y − GX. The likelihood under the current sample<br />is:<br />?<br />Substituting these likelihood terms into the expression for the ratio of like-<br />lihood terms, al, gives<br />dggT+Iκdand mn= M−1ψ−1<br />dgˆEdnwith the<br />P(Yd:|ξ,−) =<br />N<br />n=1<br />(2πψ−1<br />d)−1<br />2exp<br />?<br />−1<br />2ψ−1<br />d<br />ˆE2<br />dn<br />?<br />(19)<br />al= (2π)<br />Nκd<br />2 |M|−N<br />2 exp<br />?<br />1<br />2<br />?<br />n<br />mT<br />nMmn<br />?<br />(20)<br />We found that appropriate scheduling of the sampler improved mixing,<br />particularly with respect to adding new features. The final scheme we settled<br />on is described in Algorithm 1.</p>  <p>Page 10</p> <p>10<br />KNOWLES ET AL.<br />IBP parameters.<br />with conjugate Gamma(e,f) prior (note that we use the inverse scale param-<br />eterization of the Gamma distribution). The conditional prior of Equation 7,<br />acts as the likelihood term and the posterior update is as follows:<br />We can choose to sample the IBP strength parameter α,<br />P(α|Z) ∝ P(Z|α)P(α) = Gamma(α;K++ e,f + HD)<br />where K+is the number of active sources and HD=?D<br />The remaining sampling steps are standard, but are included here for<br />completeness.<br />(21)<br />j=1<br />1<br />jis the D-th<br />harmonic number.<br />Latent variables.<br />for each t ∈ [1,...,N] we have<br />(22)P(xn|−) ∝ P(yn|xn,−)P(xn) = N (xn;µn,Λ)<br />where we have defined Λ = GTψ−1G + I and µn= Λ−1GTψ−1yn. Note<br />that since Λ does not depend on n we only need to compute and invert<br />it once per iteration. Calculating Λ is order O(K2D), and inverting it is<br />O(K3). Calculating µtis order O(KD) and must be calculated for all N<br />xt’s, a total of O(NKD). Thus sampling X is order O(K2+ K3+ NKD).<br />Sampling the columns xnof the latent variable matrix X<br />Factor precision.<br />strained to be equal, we have λk= λ ∼ Gamma(c,d). The posterior update<br />is then given by λ|G ∼ Gamma(c +<br />However, if the variances are allowed to be different for each column of<br />G, we set λk∼ Gamma(c,d), and the posterior update is given by λk|G ∼<br />Gamma(c +mk<br />dk). In this case we may also wish to share power<br />across factors, in which case we also sample d. Putting a Gamma prior on d<br />such that d ∼ Gamma(c0,d0), the posterior update is d|λk∼ Gamma(c0+<br />cK,d0+?K<br />Noise variance.The additive Gaussian noise can be constrained to be<br />isotropic, in which case the inverse variance is given a Gamma prior: ψ−1<br />ψ−1∼ Gamma(a,b) which gives the posterior update ψ−1|− ∼ Gamma(a+<br />ND<br />d,nˆE2<br />However, if the noise is only assumed to be independent (which we have<br />found to be more appropriate for gene expression data), then each dimen-<br />sion has a separate noise variance, whose inverse is given a Gamma prior:<br />ψ−1<br />d<br />∼ Gamma(a,b) which gives the posterior update ψ−1<br />N<br />nE2<br />power between dimensions by giving the hyperparameter b a hyperprior<br />If the mixture coefficient prior precisions λk are con-<br />?<br />kmk<br />2<br />,d +?<br />d,kG2<br />dk).<br />2,d +?<br />dG2<br />k=1λk).<br />d<br />=<br />2,b +?<br />dn).<br />d|− ∼ Gamma(a+<br />2,b +?<br />dn) where the matrix of residualsˆE = Y − GX. We can share</p>  <p>Page 11</p> <p>NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS<br />11<br />Gamma(a0,b0) resulting in the Gibbs update b|− ∼ Gamma(a0+ aD,b0+<br />?D<br />d=1ψ−1<br />variances in each dimension, so we will refer to this variant as sc.<br />d). This hierarchical prior results in soft coupling between the noise<br />Algorithm 1 One iteration of the NSFA sampler<br />for d = 1 to D do<br />for k = 1 to K do<br />Sample Zdk<br />end for<br />Sample κd<br />end for<br />for n = 1 to N do<br />Sample X:n<br />end for<br />Sample α,φ,λg<br />5. Results.We compare the following models:<br />• FA - Bayesian Factor Analysis, see for example Kaufman and Press<br />(1973) or Rowe and Press (1998)<br />• AFA - Factor Analysis with ARD prior to determine active sources<br />• FOK - The sparse Factor Analysis method of Fokoue (2004), Fevotte<br />and Godsill (2006) and Archambeau and Bach (2009)<br />• SPCA - The Sparse PCA method of Zou, Hastie and Tibshirani (2004)<br />• BFRM - Bayesian Factor Regression Model of West et al. (2007).<br />• SFA - Sparse Factor Analysis, using the finite IBP<br />• NSFA - The proposed model: Nonparametric Sparse Factor Analysis<br />Note that all of these models can be learned using the software package<br />we provide simply by using appropriate settings.<br />5.1. Synthetic data.<br />IBP itself would clearly bias towards our model, we instead use the D =<br />100 gene by K = 16 factor E. Coli connectivity matrix derived in Kao<br />et al. (2004) from RegulonDB and current literature. We ignore whether the<br />connection is believed to be up or down regulation, resulting in a binary<br />matrix Z. We generate random datasets with N = 100 samples by drawing<br />the non-zero elements of G (corresponding to the elements of Z which are<br />non-zero), and all elements of X, from a zero mean unit variance Gaussian,<br />calculating Y = GX + E, where E is Gaussian white noise with variance<br />set to give a signal to noise ratio of 10.<br />Since generating a connectivity matrix Z from the</p>  <p>Page 12</p> <p>12<br />KNOWLES ET AL.<br />Fig 4. Boxplot of reconstruction errors for simulated data derived from the E. Coli con-<br />nectivity matrix of Kao et al. (2004). Ten datasets were generated and the reconstruction<br />error calculated for the last ten samples from each algorithm. Numbers refer to the number<br />of latent factors used, K. a1 denotes fixing α = 1. sn denotes sharing power between noise<br />dimensions.<br />Here we will define the reconstruction error, Eras<br />Er(G,ˆG) =<br />1<br />DK<br />K<br />?<br />k=1<br />min<br />ˆk∈{1,..,ˆ K}<br />D<br />?<br />d=1<br />(Gdk− Gdˆk)2<br />whereˆG,ˆK are the inferred quantities. Although we minimize over permu-<br />tations, we do not minimize over rotations since, as noted in Fokoue (2004),<br />the sparsity of the prior stops the solution being rotation invariant. We av-<br />erage this error over the last ten samples of the MCMC run. This error<br />function does not penalize inferring extra spurious factors, so we will inves-<br />tigate this possibility separately. The precision and recall of active elements<br />of the Z achieved by each algorithm (after thresholding for the non-sparse<br />algorithms) are presented in the supplementary material (see Supplement<br />A), but omitted here since the results are consistent with the reconstruction<br />error.<br />The reconstruction error for each method with different numbers of latent<br />features is shown in Figure 4. Ten random datasets were used and for the<br />sampling methods (all but SPCA) the results were averaged over the last<br />ten samples out of 1000. Unsurprisingly, plain Factor Analysis (FA) performs</p>  <p>Page 13</p> <p>NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS<br />13<br />the worst, with increasing overfitting as the number of factors is increased.<br />ForˆK = 20 the variance is also very high, since the four spurious features fit<br />noise. Using an ARD prior on the features (AFA) improves the performance,<br />and overfitting no longer occurs. The reconstruction error is actually less<br />forˆK = 20, but this is an artifact due to the reconstruction error not<br />penalizing additional spurious features in the inferred G. The Sparse PCA<br />(SPCA) of Zou, Hastie and Tibshirani (2004) shows improved reconstruction<br />compared to the non-sparse methods (FA and AFA) but does not perform<br />as well as the Bayesian sparse models. Sparse factor analysis (SFA), the<br />finite version of the full infinite model, performs very well. The Bayesian<br />Factor Regression Model (BFRM) performs significantly better than the<br />ARD factor analysis (AFA), but not as well as our sparse model (SFA). It<br />is interesting that for BFRM the reconstruction error decreases significantly<br />with increasingˆK, suggesting that the default priors may actually encourage<br />too much sparsity for this dataset. Fokoue’s method (FOK) only performs<br />marginally better than AFA, suggesting that this “soft” sparsity scheme is<br />not as effective at finding the underlying sparsity in the data. Overfitting<br />is also seen, with the error increasing withˆK. This could potentially be<br />resolved by placing an appropriate per factor ARD-like prior over the scale<br />parameters of the Gamma distributions controlling the precision of elements<br />of G. Finally, the Non-parametric Sparse Factor Analysis (NSFA) proposed<br />here and in Rai and Daum´ e III (2008) performs very well. With fixed α = 1<br />(a1) or inferring α we see very similar performance. Using the soft coupling<br />(sc) variant which shares power between dimensions when fitting the noise<br />variances seems to reduce the variance of the sampler, which is reasonable<br />in this example since the noise was in fact isotropic.<br />Since the reconstruction error does not penalize spurious factors it is im-<br />portant to check that NSFA is not scoring well simply by inferring many<br />additional factors. Histograms for the number of latent features inferred for<br />the nonparametric sparse model are shown in Figure 5. This represents an<br />approximate posterior over K. For fixed α = 1 the distribution is centered<br />around the true value of K = 16, with minimal bias (EK = 16.1). The vari-<br />ance is significant (standard deviation of 1.46), but is reasonable considering<br />the noise level (SNR=10) and that in some of the random datasets, elements<br />of Z which are 1 could be masked by very small corresponding values of G.<br />This hypothesis is supported by the results of a similar experiment where<br />G was set equal to Z. In this case, the sampler always converged to at least<br />16 features, but would also sometimes infer spurious features from noise (re-<br />sults not shown). When inferring α some bias and skew are noticeable. The<br />mean of the posterior is now at 18.3 with standard deviation 2.0, suggesting</p>  <p>Page 14</p> <p>14<br />KNOWLES ET AL.<br />1314<br />Number of latent factors<br />15161718192021<br />0<br />50<br />100<br />150<br />200<br />250<br />300<br />freq<br />14 15 16 17 18 19 20 21 22 23 24 25 26 27<br />Number of latent factors<br />0<br />50<br />100<br />150<br />200<br />250<br />freq<br />Fig 5. Histograms of the number of latent features inferred by the nonparametric sparse<br />FA sampler for the last 100 samples out of 1000. Left: With α = 1. Right: Inferring α.<br />there is little to gain from sampling α in this data.<br />5.2. Convergence.<br />of new features is drawn from the prior. Figure 6 shows how the different<br />proposals for κdeffect how quickly the sampler reaches a sensible number<br />of features. If we use the prior as the proposal distribution, mixing is very<br />slow, taking around 5000 iterations to converge, as shown in Figure 6(a). If<br />a mass of 0.1 is added at κd= 1 (see Equation 13), then the sampler reaches<br />the equilibrium number of features in around 1500 iterations, as shown in<br />Figure 6(b)). However, if we try to add features even faster, for example<br />by setting the factor λ = 50 in Equation 13, then the sampling noise is<br />greatly increased, as shown in Figure 6(c), and the computational cost also<br />increases significantly because so many spurious features are proposed only<br />to be rejected.<br />NSFA can suffer from slow convergence if the number<br />(a) Prior.<br />0<br />2<br />iterations/1000<br />4<br />6<br />8<br />10<br />0<br />20<br />40<br />60<br />80<br />num active factors<br />(b) Prior plus 0.1I(κ = 1).<br />0<br />400<br />iterations<br />800<br />1200<br />0<br />20<br />40<br />60<br />80<br />100<br />num active factors<br />(c) Factor λ = 50.<br />Fig 6. The effect of different proposal distributions for the number of new features.</p>  <p>Page 15</p> <p>NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS<br />15<br />(a) Log likelihood of test data under each<br />model based on the last 100 MCMC samples.<br />The boxplots show variation across 10 differ-<br />ent random splits of the data into training<br />and test sets.<br />01000 2000<br />3000<br />0<br />1<br />2<br />3<br />4<br />5<br />6<br />iterations<br />active factors<br />(b) Number of active latent features<br />during a typical MCMC run of the<br />NSFA model.<br />Fig 7. Results on E. Coli time-series dataset from Kao et al. (2004) (N = 24,D = 100,<br />3000 MCMC iterations).<br />5.3. Biological data: E. Coli time-series dataset.<br />mance of each algorithm on the biological data where no ground truth is<br />available, we calculated the test set log likelihood under the posterior. Ten<br />percent of entries from Y were removed at random, ten times, to give ten<br />datasets for inference. We do not use mean square error as a measure of<br />predictive performance because of the large variation in the signal to noise<br />ratio across gene expression level probes.<br />The test log likelihood achieved by the various algorithms on the E. Coli<br />dataset from Kao et al. (2004), including 100 genes at 24 time-points, is<br />shown in Figure 7(a). On this simple dataset incorporating sparsity doesn’t<br />improve predictive performance. Overfitting the number of latent factors<br />does damage performance, although using the ARD or sparse prior alleviates<br />the problem. Based on predictive performance of the finite models, five is a<br />sensible number of features for this dataset: the NSFA model infers a median<br />number of 4 features, with some probability of there being 5, as shown in<br />Figure 7(b).<br />To assess the perfor-<br />5.4. Breast cancer dataset.<br />dictive performance on the breast cancer dataset of West et al. (2007), in-<br />cluding 226 genes across 251 individuals. We find that all the finite models<br />are sensitive to the choice of the number of factors, K. The samplers were<br />found to have converged after around 1000 samples according to standard<br />We assess these algorithms in terms of pre-</p>  <p>Page 16</p> <p>16<br />KNOWLES ET AL.<br />multiple chain convergence measures, so 3000 MCMC iterations were used<br />for all models. The predictive log likelihood was calculated using the final<br />100 MCMC samples. Figure 8(a) shows test set log likelihoods for 10 ran-<br />dom divisions of the data into training and test sets. Factor analysis (FA)<br />shows significant overfitting as the number of latent features is increased<br />from 20 to 40. Using the ARD prior prevents this overfitting (AFA), giving<br />improved performance when using 20 features and only slightly reduced per-<br />formance when 40 features are used. The sparse finite model (SFA) shows<br />an advantage over AFA in terms of predictive performance as long as un-<br />derfitting does not occur: performance is comparable when using only 10<br />features. However, the performance of SFA is sensitive to the choice of the<br />number of factors, K. The performance of the sparse nonparametric model<br />(NSFA) is comparable to the sparse finite model when an appropriate num-<br />ber of features is chosen, but avoids the time consuming model selection<br />process. Fokoue’s method (FOK) was run with K = 20 and various settings<br />of the hyperparameter d which controls the overall sparsity of the solution.<br />The model’s predictive performance depends strongly on the setting of this<br />parameter, with results approaching the performance of the sparse models<br />(SFA and NSFA) for d = 10−4. The performance of BFRM on this dataset<br />is noticeably worse than the other sparse models.<br />We now consider the computation cost of the algorithms. As described<br />in Section 4, sampling Z and G takes order O(NKD) operations per itera-<br />tion, and sampling X takes O(K2+ K3+ ND). However, for the moderate<br />values encountered for datasets 1 and 2 the main computational cost is sam-<br />pling the non-zero elements of G, which takes O((1−s)DK) where s is the<br />sparsity of the model. Figure 8(c) shows the mean CPU time per iteration<br />divided by the number of features at that iteration. Naturally, straight FA<br />is the fastest, taking only around 0.025s per iteration per feature. The value<br />increases slightly with increasing K, suggesting that here the O(K2D+K3)<br />calculation and inversion of λ, the precision of the conditional on X, must be<br />contributing. The computational cost of adding the ARD prior is negligible<br />(AFA). The CPU time per iteration is just over double for the sparse finite<br />model (SFA), but the cost actually decreases with increasing K, because<br />the sparsity of the solution increases to avoid overfitting. There are fewer<br />non-zero elements of G to sample per feature, so the CPU time per feature<br />decreases. The CPU time per iteration per feature for the non-parametric<br />sparse model (NSFA) is somewhat higher than for the finite model because<br />of the cost of the feature birth and death process. However, Figure 8(b)<br />shows the absolute CPU time per iteration, where we see that the nonpara-<br />metric model is only marginally more expensive than the finite model of</p>  <p>Page 17</p> <p>NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS<br />17<br />10<br />20<br />FA<br />401020<br />AFA<br />4010152040<br />1e-3   1e-4    1e-5   1e-6<br />FOK<br />5 1020<br />−3800<br />−3600<br />−3400<br />−3200<br />−3000<br />−2800<br />log likelihood of test data<br />SFABFRM<br />NSFA<br />(a) Predictive performance: log likelihood of test (the 10%<br />missing) data under each model based on the last 100 MCMC<br />samples. Higher values indicate better performance. The box-<br />plots show variation across 10 different random splits of the<br />data into training and test sets.<br />1<br />2<br />3<br />4<br />5<br />cpu time per iteration<br />10<br />20<br />FA<br />401020<br />AFA<br />40101520 40<br />1e-3   1e-4    1e-5   1e-6<br />FOK, K=20<br />51020<br />SFABFRM<br />NSFA<br />(b) CPU time (in seconds) per iteration, averaged across the<br />3000 iteration run.<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />cpu time per iteration per feature<br />10<br />20<br />FA<br />4010 20<br />AFA<br />4010152040<br />1e-3   1e-4    1e-5   1e-6<br />FOK<br />51020<br />SFABFRM<br />NSFA<br />(c) CPU time (in seconds) per iteration divided by the number<br />of features at that iteration, averaged across all iterations.<br />Fig 8. Results on breast cancer dataset (N = 251,D = 226, 3000 MCMC iterations).</p>  <p>Page 18</p> <p>18<br />KNOWLES ET AL.<br />10 20<br />AFA<br />401020<br />SFA<br />40<br />−3.15<br />−3.1<br />−3.05<br />−3<br />−2.95<br />−2.9<br />−2.85<br />x 10<br />5<br />log likelihood of test data<br />NSFA<br />Fig 9. Test set log likelihoods on Prostate cancer dataset from Yu et al. (2004), including<br />12557 genes across 171 individuals (1000 MCMC iterations).<br />appropriate sizeˆK = 15 and cheaper than choosing an unnecessarily large<br />finite model (SFA with K = 20,40). Fokoue’s method (FOK) has compara-<br />ble computational performance to the sparse finite model, but interestingly<br />has increased cost for the optimal setting of d = 10−4. The parameter space<br />for FOK is continuous, making search easier but requiring a normal random<br />variable for every element of G. BFRM pays a considerable computational<br />cost for both the hierarchical sparsity prior and the DP prior on X. SPCA<br />was not run on this dataset but results on the synthetic data in Section 5.1<br />suggest it is somewhat faster than the sampling methods, but not hugely<br />so. The computational cost of SPCA is ND2+ mO(D2K + DK2+ D3) in<br />the N &gt; D case (where m is the number of iterations to convergence) and<br />ND2+ mO(D2K + DK2) in the D &gt; N case taking the limit λ → ∞. In<br />either case an individual iteration of SPCA is more expensive than one sam-<br />pling iteration of NSFA (since K &lt; D) but fewer iterations will generally be<br />required to reach convergence of SPCA than are required to ensure mixing<br />of NSFA.<br />5.5. Prostate cancer dataset.<br />of AFA, FOK and NSFA on the prostate cancer dataset of Yu et al. (2004),<br />for ten random splits into training and test data. The boxplots show varia-<br />tion from ten random splits into training and test data. The large number<br />of genes (D = 12557 across N = 171 individuals) in this dataset makes in-<br />ference slower, but the problem is manageable since the computational com-<br />Figure 9 shows the predictive performance</p>  <p>Page 19</p> <p>NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS<br />19<br />plexity is linear in the number of genes. Despite the large number of genes,<br />the appropriate number of latent factors, in terms of maximizing predictive<br />performance, is still small, around 10 (NSFA infers a median of 12 factors).<br />This may seem small relative to the number of genes, but it should be noted<br />that the genes included in the breast cancer and E. Coli datasets are those<br />capturing the most variability. Surprisingly, SFA actually performed slightly<br />worse on this dataset than AFA. Both are highly sensitive to the number of<br />latent factors chosen. NSFA however gives better predictive log likelihoods<br />than either finite model for any fixed number of latent factors K. Running<br />1000 iterations of NSFA on this dataset takes under 8 hours. BFRM and<br />FOK were impractically slow to run on this dataset.<br />6. Discussion.<br />datasets that sparsity can improve predictive performance, as well as provid-<br />ing a more easily interpretable solution. Using the IBP to provide sparsity<br />is straightforward, and allows the number of latent factors to be inferred<br />within a well defined theoretical framework. This has several advantages<br />over manually choosing the number of latent factors. Choosing too few la-<br />tent factors damages predictive performance, as seen for the breast cancer<br />dataset. Although choosing too many latent factors can be compensated for<br />by using appropriate ARD-like priors, we find this is typically more compu-<br />tationally expensive than the birth and death process of the IBP. Manual<br />model selection is an alternative but is time consuming. Finally we show that<br />running NSFA on full gene expression datasets with 10000+ genes is feasible<br />so long as the number of latent factors remains relatively small. An inter-<br />esting direction for this research is how to incorporate prior knowledge, for<br />example if certain transcription factors are known to regulate specific genes.<br />Incorporating this knowledge could both improve the performance of the<br />model and improve interpretability by associating latent variables with spe-<br />cific transcription factors. Another possibility is incorporating correlations<br />in the Indian Buffet Process, which has been proposed for simpler mod-<br />els (Courville, Eck and Bengio, 2009; Doshi-Velez and Ghahramani, 2009).<br />This would be appropriate in a gene expression setting where multiple tran-<br />scription factors might be expected to share sets of regulated genes due to<br />common motifs. Unfortunately, performing MCMC in all but the simplest<br />of these models suffers from slow mixing.<br />We have seen that in both the E. Coli and breast cancer<br />Acknowledgements.<br />for helpful comments.<br />We would like to thank the anonymous reviewers</p>  <p>Page 20</p> <p>20<br />KNOWLES ET AL.<br />SUPPLEMENTARY MATERIAL<br />Supplement A: Graphs of precision and recall for the synthetic<br />data experiment.<br />(doi: ??????; .pdf). The precision and recall of active elements of the Z<br />matrix achieved by each algorithm (after thresholding for the non-sparse<br />algorithms) on the synethic data experiment, described in Section 5.1. The<br />results are consistent with the reconstruction error.<br />References.<br />Archambeau, C. and Bach, F. (2009). Sparse Probabilistic Projections. In Proceed-<br />ings of the Conference on Neural Information Processing Systems (NIPS) (D. Koller,<br />D. Schuurmans, Y. Bengio and L. Bottou, eds.) 73-80. MIT Press, Vancouver,<br />Canada.<br />Courville, A. C., Eck, D. and Bengio, Y. (2009). An Infinite Factor Model Hierarchy<br />Via a Noisy-Or Mechanism. In Advances in Neural Information Processing Systems 21.<br />The MIT Press, Cambridge, MA, USA.<br />Doshi-Velez, F. and Ghahramani, Z. (2009). Correlated Nonparametric Latent Feature<br />Models In Conference on Uncertainty in Artificial Intelligence.<br />Fevotte, C. and Godsill, S. J. (2006). A Bayesian Approach for Blind Separation of<br />Sparse Sources. Audio, Speech, and Language Processing, IEEE Transactions on 14<br />2174-2188.<br />Fokoue, E. (2004). Stochastic determination of the intrinsic structure in Bayesian fac-<br />tor analysis. Technical Report No. 17, Statistical and Applied Mathematical Sciences<br />Institute.<br />Griffiths, T. L. and Ghahramani, Z. (2006). Infinite Latent Feature Models and the<br />Indian Buffet Process. In Advances in Neural Information Processing Systems 18. The<br />MIT Press, Cambridge, MA, USA.<br />Kao, K. C., Yang, Y.-L., Boscolo, R., Sabatti, C., Roychowdhury, V. and<br />Liao, J. C. (2004). Transcriptome-based determination of multiple transcription reg-<br />ulator activities in Escherichia coli by using network component analysis. Proceedings<br />of the National Academy of Sciences of the United States of America (PNAS) 101<br />641–646.<br />Kaufman, G. M. and Press, S. J. (1973). Bayesian factor analysis. Technical Report<br />No. 662-73, Sloan School of Management, University of Chicago.<br />Knowles, D. and Ghahramani, Z. (2007). Infinite Sparse Factor Analysis and Infinite<br />Independent Components Analysis. In 7th International Conference on Independent<br />Component Analysis and Signal Separation 381-388.<br />Meeds, E., Ghahramani, Z., Neal, R. and Roweis, S. (2006). Modeling Dyadic Data<br />with Binary Latent Factors. In Neural Information Processing Systems 19.<br />Rai, P. and Daum´ e III, H. (2008). The Infinite Hierarchical Factor Regression Model. In<br />Neural Information Processing Systems.<br />Rowe, D. B. and Press, S. J. (1998). Gibbs Sampling and Hill Climbing in Bayesian<br />Factor Analysis. Technical Report No. 255, Department of Statistics, University of<br />California Riverside.<br />West, M., Chang, J., Lucas, J., Nevins, J. R., Wang, Q. and Carvalho, C. (2007).<br />High-Dimensional Sparse Factor Modelling: Applications in Gene Expression Genomics<br />Technical Report, ISDS, Duke University.</p>  <p>Page 21</p> <p>NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS<br />21<br />Witten, D. M., Tibshirani, R. and Hastie, T. (2009). A penalized matrix decom-<br />position, with applications to sparse principal components and canonical correlation<br />analysis. Biostatistics 10 515–534.<br />Yu, Y. P., Landsittel, D., Jing, L., Nelson, J., Ren, B., Liu, L., McDonald, C.,<br />Thomas, R., Dhir, R., Finkelstein, S., Michalopoulos, G., Becich, M. and<br />Luo, J.-H. (2004). Gene expression alterations in prostate cancer predicting tumor<br />aggression and preceding development of malignancy. Journal of Clinical Oncology 22<br />2790–2799.<br />Zhang, Z., Chan, K. L., Kwok, J. T. and yan Yeung, D. (2004). Bayesian Inference<br />on Principal Component Analysis using Reversible Jump Markov Chain Monte Carlo.<br />In Proceedings of the 19th National Conference on Artificial Intelligence, San Jose,<br />California, USA 372-377. AAAI Press.<br />Zou, H., Hastie, T. and Tibshirani, R. (2004). Sparse Principal Component Analysis.<br />Journal of Computational and Graphical Statistics 15 2006.<br />Cambridge University Engineering Department<br />Trumpington Street<br />Cambridge<br />CB2 1PZ<br />UK<br />E-mail: dak33@cam.ac.uk<br />zoubin@eng.cam.ac.uk</p>  <a href="https://www.researchgate.net/profile/David_Knowles2/publication/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling/links/55554b9d08ae6fd2d821cdba.pdf">Download full-text</a> </div> <div id="rgw19_56ab9f7b0d775" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw20_56ab9f7b0d775">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw21_56ab9f7b0d775"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/David_Knowles2/publication/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling/links/55554b9d08ae6fd2d821cdba.pdf" class="publication-viewer" title="1011.6293.pdf">1011.6293.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/David_Knowles2">David A. Knowles</a> &middot; Jan 20, 2016 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw22_56ab9f7b0d775"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://arxiv.org/pdf/1011.6293.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Nonparametric Bayesian sparse factor models with application to gene
expression modeling">Nonparametric Bayesian sparse factor models with a...</a> </div>  <div class="details">   Available from <a href="http://arxiv.org/pdf/1011.6293.pdf" target="_blank" rel="nofollow">ArXiv</a>  </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw23_56ab9f7b0d775"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://arxiv.org/pdf/1011.6293" target="_blank" rel="nofollow" class="publication-viewer" title="Nonparametric Bayesian sparse factor models with application to gene
expression modeling">Nonparametric Bayesian sparse factor models with a...</a> </div>  <div class="details">   Available from <a href="http://arxiv.org/pdf/1011.6293" target="_blank" rel="nofollow">arxiv.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw25_56ab9f7b0d775" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw26_56ab9f7b0d775">  </ul> </div> </div>   <div id="rgw15_56ab9f7b0d775" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw16_56ab9f7b0d775"> <div> <h5> <a href="publication/282603175_Minimax_Lower_Bounds_for_Noisy_Matrix_Completion_Under_Sparse_Factor_Models" class="color-inherit ga-similar-publication-title"><span class="publication-title">Minimax Lower Bounds for Noisy Matrix Completion Under Sparse Factor Models</span></a>  </h5>  <div class="authors"> <a href="researcher/2082334634_Abhinav_V_Sambasivan" class="authors ga-similar-publication-author">Abhinav V. Sambasivan</a>, <a href="researcher/2082335849_Jarvis_D_Haupt" class="authors ga-similar-publication-author">Jarvis D. Haupt</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56ab9f7b0d775"> <div> <h5> <a href="publication/230935081_Doubly_sparse_factor_models_for_unifying_feature_transformation_and_feature_selection" class="color-inherit ga-similar-publication-title"><span class="publication-title">Doubly sparse factor models for unifying feature transformation and feature selection</span></a>  </h5>  <div class="authors"> <a href="researcher/39921049_Kentaro_Katahira" class="authors ga-similar-publication-author">Kentaro Katahira</a>, <a href="researcher/38354789_Narihisa_Matsumoto" class="authors ga-similar-publication-author">Narihisa Matsumoto</a>, <a href="researcher/39498179_Yasuko_Sugase-Miyamoto" class="authors ga-similar-publication-author">Yasuko Sugase-Miyamoto</a>, <a href="researcher/38860008_Kazuo_Okanoya" class="authors ga-similar-publication-author">Kazuo Okanoya</a>, <a href="researcher/48033604_Masato_Okada" class="authors ga-similar-publication-author">Masato Okada</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56ab9f7b0d775"> <div> <h5> <a href="publication/49739891_High-Dimensional_Sparse_Factor_Modeling_Applications_in_Gene_Expression_Genomics" class="color-inherit ga-similar-publication-title"><span class="publication-title">High-Dimensional Sparse Factor Modeling: Applications in Gene Expression Genomics</span></a>  </h5>  <div class="authors"> <a href="researcher/28812743_Carlos_M_Carvalho" class="authors ga-similar-publication-author">Carlos M Carvalho</a>, <a href="researcher/38550383_Jeffrey_Chang" class="authors ga-similar-publication-author">Jeffrey Chang</a>, <a href="researcher/39290919_Joseph_E_Lucas" class="authors ga-similar-publication-author">Joseph E Lucas</a>, <a href="researcher/39391194_Joseph_R_Nevins" class="authors ga-similar-publication-author">Joseph R Nevins</a>, <a href="researcher/38762711_Quanli_Wang" class="authors ga-similar-publication-author">Quanli Wang</a>, <a href="researcher/38910148_Mike_West" class="authors ga-similar-publication-author">Mike West</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw39_56ab9f7b0d775" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw40_56ab9f7b0d775">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw41_56ab9f7b0d775" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=-McXKYND0KseG3Ww2LRRZ8XfnwTFXxqZkQG0VcMERJQPhKUksKlhlu5VH1SAlcKO" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="kXK4Fgj+YrugXhHP1oX1z0PTV9cltTZsQb9fcSPerXuEWxothmHHHyMa68odpCTQEuYR12sn45QSyLMickUJwderRGI1MSKyQrrAEN8hNxpfAzaSU2gAk7Vaex1hSP/B0C9dcGc9BSQKSOR+CP+c3YzDHx/vEK/GnY9xpSawR0yoyIZCgAQ+K3a0C2OKbbnQeuit6PLvC3X/16TY8pkJVaNIs9jjePUoULW+Zm6ybbUvIJMYurk/7DZke6K5Rh4IknPaRLDonNsmSfeXCYOmnXtrPAq9vEOOUkVumzTOfTg="/> <input type="hidden" name="urlAfterLogin" value="publication/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vNDc4NDM2NjlfTm9ucGFyYW1ldHJpY19CYXllc2lhbl9zcGFyc2VfZmFjdG9yX21vZGVsc193aXRoX2FwcGxpY2F0aW9uX3RvX2dlbmVleHByZXNzaW9uX21vZGVsaW5n"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vNDc4NDM2NjlfTm9ucGFyYW1ldHJpY19CYXllc2lhbl9zcGFyc2VfZmFjdG9yX21vZGVsc193aXRoX2FwcGxpY2F0aW9uX3RvX2dlbmVleHByZXNzaW9uX21vZGVsaW5n"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vNDc4NDM2NjlfTm9ucGFyYW1ldHJpY19CYXllc2lhbl9zcGFyc2VfZmFjdG9yX21vZGVsc193aXRoX2FwcGxpY2F0aW9uX3RvX2dlbmVleHByZXNzaW9uX21vZGVsaW5n"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw42_56ab9f7b0d775"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 418;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2198378204065/javascript/min/lib/error_logging.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"David A. Knowles","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/David_Knowles2","institution":"Stanford University","institutionUrl":false,"widgetId":"rgw4_56ab9f7b0d775"},"id":"rgw4_56ab9f7b0d775","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=7693199","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab9f7b0d775"},"id":"rgw3_56ab9f7b0d775","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=47843669","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":47843669,"title":"Nonparametric Bayesian sparse factor models with application to gene\nexpression modeling","journalTitle":"The Annals of Applied Statistics","journalDetailsTooltip":{"data":{"journalTitle":"The Annals of Applied Statistics","journalAbbrev":"ANN APPL STAT","publisher":"Institute of Mathematical Statistics, Institute of Mathematical Statistics","issn":"1932-6157","impactFactor":"1.46","fiveYearImpactFactor":"2.31","citedHalfLife":"4.80","immediacyIndex":"0.16","eigenFactor":"0.02","articleInfluence":"2.17","widgetId":"rgw6_56ab9f7b0d775"},"id":"rgw6_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=1932-6157","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Article","details":{"doi":"10.1214\/10-AOAS435","journalInfos":{"journal":"","publicationDate":"11\/2010;","publicationDateRobot":"2010-11","article":"5(2011).","journalTitle":"The Annals of Applied Statistics","journalUrl":"journal\/1932-6157_The_Annals_of_Applied_Statistics","impactFactor":1.46}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1011.6293","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1214\/10-AOAS435"},{"key":"rft.atitle","value":"Nonparametric Bayesian sparse factor models with application to gene\nexpression modeling"},{"key":"rft.title","value":"Annals of Applied Statistics - ANN APPL STAT"},{"key":"rft.jtitle","value":"Annals of Applied Statistics - ANN APPL STAT"},{"key":"rft.volume","value":"5"},{"key":"rft.issue","value":"2011"},{"key":"rft.date","value":"2010"},{"key":"rft.issn","value":"1932-6157"},{"key":"rft.au","value":"David Knowles,Zoubin Ghahramani"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56ab9f7b0d775"},"id":"rgw7_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=47843669","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":47843669,"peopleItems":[{"data":{"authorNameOnPublication":"David A. Knowles","accountUrl":"profile\/David_Knowles2","accountKey":"David_Knowles2","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"David A. Knowles","profile":{"professionalInstitution":{"professionalInstitutionName":"Stanford University","professionalInstitutionUrl":"institution\/Stanford_University"}},"professionalInstitutionName":"Stanford University","professionalInstitutionUrl":"institution\/Stanford_University","url":"profile\/David_Knowles2","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"David_Knowles2","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw10_56ab9f7b0d775"},"id":"rgw10_56ab9f7b0d775","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=7693199&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Stanford University","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":2,"accountCount":1,"publicationUid":47843669,"widgetId":"rgw9_56ab9f7b0d775"},"id":"rgw9_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=7693199&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=2&accountCount=1&publicationUid=47843669","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/8159937_Zoubin_Ghahramani","authorNameOnPublication":"Zoubin Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Zoubin Ghahramani","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/8159937_Zoubin_Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56ab9f7b0d775"},"id":"rgw12_56ab9f7b0d775","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=8159937&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56ab9f7b0d775"},"id":"rgw11_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=8159937&authorNameOnPublication=Zoubin%20Ghahramani","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab9f7b0d775"},"id":"rgw8_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=47843669&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":47843669,"abstract":"<noscript><\/noscript><div>A nonparametric Bayesian extension of Factor Analysis (FA) is proposed where<br \/>\nobserved data $\\mathbf{Y}$ is modeled as a linear superposition, $\\mathbf{G}$,<br \/>\nof a potentially infinite number of hidden factors, $\\mathbf{X}$. The Indian<br \/>\nBuffet Process (IBP) is used as a prior on $\\mathbf{G}$ to incorporate sparsity<br \/>\nand to allow the number of latent features to be inferred. The model's utility<br \/>\nfor modeling gene expression data is investigated using randomly generated data<br \/>\nsets based on a known sparse connectivity matrix for E. Coli, and on three<br \/>\nbiological data sets of increasing complexity.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw13_56ab9f7b0d775"},"id":"rgw13_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=47843669","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling\/links\/55554b9d08ae6fd2d821cdba\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw14_56ab9f7b0d775"},"id":"rgw14_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab9f7b0d775"},"id":"rgw5_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=47843669&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2082334634,"url":"researcher\/2082334634_Abhinav_V_Sambasivan","fullname":"Abhinav V. Sambasivan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2082335849,"url":"researcher\/2082335849_Jarvis_D_Haupt","fullname":"Jarvis D. Haupt","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Oct 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/282603175_Minimax_Lower_Bounds_for_Noisy_Matrix_Completion_Under_Sparse_Factor_Models","usePlainButton":true,"publicationUid":282603175,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/282603175_Minimax_Lower_Bounds_for_Noisy_Matrix_Completion_Under_Sparse_Factor_Models","title":"Minimax Lower Bounds for Noisy Matrix Completion Under Sparse Factor Models","displayTitleAsLink":true,"authors":[{"id":2082334634,"url":"researcher\/2082334634_Abhinav_V_Sambasivan","fullname":"Abhinav V. Sambasivan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2082335849,"url":"researcher\/2082335849_Jarvis_D_Haupt","fullname":"Jarvis D. Haupt","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/282603175_Minimax_Lower_Bounds_for_Noisy_Matrix_Completion_Under_Sparse_Factor_Models","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/282603175_Minimax_Lower_Bounds_for_Noisy_Matrix_Completion_Under_Sparse_Factor_Models\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56ab9f7b0d775"},"id":"rgw16_56ab9f7b0d775","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=282603175","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":39921049,"url":"researcher\/39921049_Kentaro_Katahira","fullname":"Kentaro Katahira","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38354789,"url":"researcher\/38354789_Narihisa_Matsumoto","fullname":"Narihisa Matsumoto","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39498179,"url":"researcher\/39498179_Yasuko_Sugase-Miyamoto","fullname":"Yasuko Sugase-Miyamoto","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":38860008,"url":"researcher\/38860008_Kazuo_Okanoya","fullname":"Kazuo Okanoya","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jul 2010","journal":"Journal of Physics Conference Series","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/230935081_Doubly_sparse_factor_models_for_unifying_feature_transformation_and_feature_selection","usePlainButton":true,"publicationUid":230935081,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/230935081_Doubly_sparse_factor_models_for_unifying_feature_transformation_and_feature_selection","title":"Doubly sparse factor models for unifying feature transformation and feature selection","displayTitleAsLink":true,"authors":[{"id":39921049,"url":"researcher\/39921049_Kentaro_Katahira","fullname":"Kentaro Katahira","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38354789,"url":"researcher\/38354789_Narihisa_Matsumoto","fullname":"Narihisa Matsumoto","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39498179,"url":"researcher\/39498179_Yasuko_Sugase-Miyamoto","fullname":"Yasuko Sugase-Miyamoto","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38860008,"url":"researcher\/38860008_Kazuo_Okanoya","fullname":"Kazuo Okanoya","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":48033604,"url":"researcher\/48033604_Masato_Okada","fullname":"Masato Okada","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of Physics Conference Series 07\/2010; 233(1):012021. DOI:10.1088\/1742-6596\/233\/1\/012021"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/230935081_Doubly_sparse_factor_models_for_unifying_feature_transformation_and_feature_selection","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/230935081_Doubly_sparse_factor_models_for_unifying_feature_transformation_and_feature_selection\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab9f7b0d775"},"id":"rgw17_56ab9f7b0d775","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=230935081","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":28812743,"url":"researcher\/28812743_Carlos_M_Carvalho","fullname":"Carlos M Carvalho","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38550383,"url":"researcher\/38550383_Jeffrey_Chang","fullname":"Jeffrey Chang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39290919,"url":"researcher\/39290919_Joseph_E_Lucas","fullname":"Joseph E Lucas","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":39391194,"url":"researcher\/39391194_Joseph_R_Nevins","fullname":"Joseph R Nevins","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":2,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Dec 2008","journal":"Journal of the American Statistical Association","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/49739891_High-Dimensional_Sparse_Factor_Modeling_Applications_in_Gene_Expression_Genomics","usePlainButton":true,"publicationUid":49739891,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.98","url":"publication\/49739891_High-Dimensional_Sparse_Factor_Modeling_Applications_in_Gene_Expression_Genomics","title":"High-Dimensional Sparse Factor Modeling: Applications in Gene Expression Genomics","displayTitleAsLink":true,"authors":[{"id":28812743,"url":"researcher\/28812743_Carlos_M_Carvalho","fullname":"Carlos M Carvalho","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38550383,"url":"researcher\/38550383_Jeffrey_Chang","fullname":"Jeffrey Chang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39290919,"url":"researcher\/39290919_Joseph_E_Lucas","fullname":"Joseph E Lucas","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39391194,"url":"researcher\/39391194_Joseph_R_Nevins","fullname":"Joseph R Nevins","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38762711,"url":"researcher\/38762711_Quanli_Wang","fullname":"Quanli Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38910148,"url":"researcher\/38910148_Mike_West","fullname":"Mike West","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of the American Statistical Association 12\/2008; 103(484):1438-1456. DOI:10.1198\/016214508000000869"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/49739891_High-Dimensional_Sparse_Factor_Modeling_Applications_in_Gene_Expression_Genomics","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/49739891_High-Dimensional_Sparse_Factor_Modeling_Applications_in_Gene_Expression_Genomics\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab9f7b0d775"},"id":"rgw18_56ab9f7b0d775","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=49739891","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw15_56ab9f7b0d775"},"id":"rgw15_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=47843669&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":47843669,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":47843669,"publicationType":"article","linkId":"55554b9d08ae6fd2d821cdba","fileName":"1011.6293.pdf","fileUrl":"profile\/David_Knowles2\/publication\/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling\/links\/55554b9d08ae6fd2d821cdba.pdf","name":"David A. Knowles","nameUrl":"profile\/David_Knowles2","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Jan 20, 2016","fileSize":"863.44 KB","widgetId":"rgw21_56ab9f7b0d775"},"id":"rgw21_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=47843669&linkId=55554b9d08ae6fd2d821cdba&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":47843669,"publicationType":"article","linkId":"0f64b09a38294e886aa279b1","fileName":"Nonparametric Bayesian sparse factor models with application to gene\nexpression modeling","fileUrl":"http:\/\/arxiv.org\/pdf\/1011.6293.pdf","name":"ArXiv","nameUrl":"http:\/\/arxiv.org\/pdf\/1011.6293.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw22_56ab9f7b0d775"},"id":"rgw22_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=47843669&linkId=0f64b09a38294e886aa279b1&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":47843669,"publicationType":"article","linkId":"00b236a90cf245659d019bc9","fileName":"Nonparametric Bayesian sparse factor models with application to gene\nexpression modeling","fileUrl":"http:\/\/arxiv.org\/pdf\/1011.6293","name":"arxiv.org","nameUrl":"http:\/\/arxiv.org\/pdf\/1011.6293","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw23_56ab9f7b0d775"},"id":"rgw23_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=47843669&linkId=00b236a90cf245659d019bc9&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw20_56ab9f7b0d775"},"id":"rgw20_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=47843669&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":3,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":6,"valueFormatted":"6","widgetId":"rgw24_56ab9f7b0d775"},"id":"rgw24_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=47843669","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw19_56ab9f7b0d775"},"id":"rgw19_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=47843669&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":47843669,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw26_56ab9f7b0d775"},"id":"rgw26_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=47843669&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":6,"valueFormatted":"6","widgetId":"rgw27_56ab9f7b0d775"},"id":"rgw27_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=47843669","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw25_56ab9f7b0d775"},"id":"rgw25_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=47843669&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Submitted to the Annals of Applied Statistics\narXiv: math.PR\/0000000\nNONPARAMETRIC BAYESIAN SPARSE FACTOR\nMODELS WITH APPLICATION TO GENE EXPRESSION\nMODELLING\nBy David Knowles\u2217\nand Zoubin Ghahramani\u2020\nUniversity of Cambridge\nA nonparametric Bayesian extension of Factor Analysis (FA) is\nproposed where observed data Y is modeled as a linear superposi-\ntion, G, of a potentially infinite number of hidden factors, X. The\nIndian Buffet Process (IBP) is used as a prior on G to incorporate\nsparsity and to allow the number of latent features to be inferred.\nThe model\u2019s utility for modeling gene expression data is investigated\nusing randomly generated datasets based on a known sparse connec-\ntivity matrix for E. Coli, and on three biological datasets of increasing\ncomplexity.\n1. Introduction.\nysis (FA) and Independent Components Analysis (ICA) are models which\nexplain observed data, yn\u2208 RD, in terms of a linear superposition of inde-\npendent hidden factors, xn\u2208 RK, so\nPrincipal Components Analysis (PCA), Factor Anal-\nyn= Gxn+ ?n\n(1)\nwhere G is the factor loading matrix and ?nis a noise vector, usually as-\nsumed to be Gaussian. These algorithms can be expressed in terms of per-\nforming inference in appropriate probabilistic models. The latent factors are\nusually considered as random variables, and the mixing matrix as a parame-\nter to estimate. In both PCA and FA the latent factors are given a standard\n(zero mean, unit variance) normal prior. In PCA the noise is assumed to\nbe isotropic, whereas in FA the noise covariance is only constrained to be\ndiagonal. A standard approach in these models is to integrate out the latent\nfactors and find the maximum likelihood estimate of the mixing matrix. In\nICA the latent factors are assumed to be heavy-tailed, so it is not usually\n\u2217Supported by Microsoft Research through the Roger Needham Scholarship at Wolfson\nCollege, University of Cambridge.\n\u2020Supported by EPSRC Grant EP\/F027400\/1\nKeywords and phrases: Nonparametric Bayes, Sparsity, Factor Analysis, Markov Chain\nMonte Carlo, Indian Buffet Process\nAMS 2000 subject classifications: Primary 62H25; secondary 62F15\n1\narXiv:1011.6293v1  [stat.AP]  29 Nov 2010"},{"page":2,"text":"2\nKNOWLES ET AL.\npossible to integrate them out. In this paper we take a fully Bayesian ap-\nproach, viewing not only the hidden factors but also the mixing coefficients\nas random variables whose posterior distribution given data we aim to infer.\nSparsity plays an important role in latent feature models, and is desir-\nable for several reasons. It gives improved predictive performance, because\nfactors irrelevant to a particular dimension are not included. Sparse models\nare more readily interpretable since a smaller number of factors are asso-\nciated with observed dimensions. In many real world situations there is an\nintuitive reason why we expect sparsity: for example, in gene regulatory\nnetworks a transcription factor will only regulate genes with specific motifs.\nIn our previous work (Knowles and Ghahramani, 2007) we investigated the\nuse of sparsity the on latent factors xn, but this formulation is not appro-\npriate in the case of modeling gene expression, where, as described above,\na transcription factor will regulate only a small set of genes, corresponding\nto sparsity in the factor loadings, G. Here we propose a novel approach to\nsparse latent factor modeling where we place sparse priors on the factor\nloading matrix, G. The Bayesian Factor Regression Model of West et al.\n(2007) is closely related to our work in this way, although the hierarchical\nsparsity prior they use is somewhat different. An alternative \u201csoft\u201d approach\nto incorporating sparsity is to put a Gamma(a,b) (usually exponential, i.e.\na = 1) prior on the precision of each element of G independently, result-\ning in the elements of G being marginally Student-t distributed a priori:\nsee Fokoue (2004), Fevotte and Godsill (2006), and Archambeau and Bach\n(2009). A LASSO-based approach to generating a sparse factor loading has\nalso been developed (Witten, Tibshirani and Hastie, 2009; Zou, Hastie and\nTibshirani, 2004). We compare these sparsity schemes empirically in the\ncontext of gene expression modeling.\nA problematic issue with this type of model is how to choose the latent\ndimensionality of the factor space, K. Model selection can be used to choose\nbetween different values of K, but generally requires significant manual in-\nput and still requires the range of K over which to search to be specified.\nZhang et al. (2004) applied Reversible Jump MCMC to PCA, which has\nmany of the advantages of our approach: a posterior distribution over the\nnumber of latent dimensions can be approximated, and the number of la-\ntent dimensions could potentially be unbounded. However, RJ MCMC is\nconsiderably more complex to implement for sparse Factor Analysis than\nour proposed framework.\nWe use the Indian Buffet Process (Griffiths and Ghahramani, 2006), which\ndefines a distribution over infinite binary matrices, to provide sparsity and\na framework for inferring the appropriate latent dimension of the dataset"},{"page":3,"text":"NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS\n3\nusing a straightforward Gibbs sampling algorithm. The Indian Buffet Pro-\ncess (IBP) allows a potentially unbounded number of latent factors, so we\ndo not have to specify a maximum number of latent dimensions a priori. We\ndenote our model \u201cNSFA\u201d for \u201cNon-parametric Sparse Factor Analysis\u201d.\nOur model is closely related to that of Rai and Daum\u00b4 e III (2008), and is a\nsimultaneous development.\n2. The Model.\nZ be a binary matrix whose (d,k)-th element represents whether observed\ndimension d includes any contribution from factor k. We then model the\nmixing matrix by\np(gdk|Zdk,\u03bbk) = ZdkN?gdk;0,\u03bb\u22121\nwhere \u03bbkis the inverse variance (precision) of the kth factor and \u03b40is a delta\nfunction (pont-mass) at 0. Distributions of this type are sometimes known\nas \u201cspike and slab\u201d distributions. We allow a potentially infinite number of\nhidden sources, so that Z has infinitely many columns, although only a finite\nnumber will have non-zero entries. This construction allows us to use the\nIBP to provide sparsity and define a generative process for the number of\nlatent factors.\nWe assume independent Gaussian noise, ?n, with diagonal covariance ma-\ntrix \u03a8. We find that for many applications assuming isotropic noise is too\nrestrictive, but this option is available for situations where there is strong\nprior belief that all observed dimensions should have the same noise vari-\nance. The latent factors, xn, are given Gaussian priors. Figure 1 shows the\ncomplete graphical model.\nWe will define our model in terms of Equation 1. Let\nk\n?+ (1 \u2212 Zdk)\u03b40(gdk)(2)\n2.1. Defining a distribution over infinite binary matrices.\nour infinite model by taking the limit of a series of finite models.\nWe now define\nStart with a finite model.\nfinite K model and taking the limit as K \u2192 \u221e. We then show how the\ninfinite case corresponds to a simple stochastic process.\nWe have D dimensions and K hidden sources. Recall that zdkof matrix\nZ tells us whether hidden source k contributes to dimension d. We assume\nthat the probability of a source k contributing to any dimension is \u03c0k, and\nthat the rows are generated independently. We find\nWe derive the distribution on Z by defining a\nP(Z|\u03c0) =\nK\n?\nk=1\nD\n?\nd=1\nP(zdk|\u03c0k) =\nK\n?\nk=1\n\u03c0mk\nk(1 \u2212 \u03c0k)D\u2212mk\n(3)"},{"page":4,"text":"4\nKNOWLES ET AL.\nG\nX\nY\nZ\n\u03bb\n\u03b1\n\u03a8\nK\nD\nN\nFig 1. Graphical model\nwhere mk=?D\nchoose the conjugate Beta(r,s) distribution for \u03c0k. For now we take r =\u03b1\nand s = 1, where \u03b1 is the strength parameter of the IBP. The model is\ndefined by\nd=1zdkis the number of dimensions to which source k con-\ntributes. The inner term of the product is a binomial distribution, so we\nK\n\u03c0k|\u03b1 \u223c Beta\nzdk|\u03c0k\u223c Bernoulli(\u03c0k)\n?\u03b1\nK,1\n?\n(4)\n(5)\nDue to the conjugacy between the binomial and beta distributions we are\nable to integrate out \u03c0 to find\nP(Z) =\nK\n?\nk=1\n\u03b1\nK\u0393(mk+\u03b1\nK)\u0393(D \u2212 mk+ 1)\n\u0393(D + 1 +\u03b1\nK)\n(6)\nwhere \u0393(.) is the Gamma function.\nTake the infinite limit.\nto order the non-zero rows of Z which allows us to take the limit K \u2192 \u221e\nand find\nGriffiths and Ghahramani (2006) define a scheme\nP(Z) =\n\u03b1K+\nh>0Kh!exp(\u2212\u03b1HD)\n?\nK+\n?\nk=1\n(D \u2212 mk)!(mk\u2212 1)!\nN!\n(7)"},{"page":5,"text":"NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS\n5\nwhere K+ is the number of active features (i.e. non-zero columns of Z),\nHD=?D\nGo to an Indian Buffet. This distribution corresponds to a simple stochas-\ntic process, the Indian Buffet Process. Consider a buffet with a seemingly\ninfinite number of dishes (hidden sources) arranged in a line. The first cus-\ntomer (observed dimension) starts at the left and samples Poisson(\u03b1) dishes.\nThe ith customer moves from left to right sampling dishes with probability\nmk\niwhere mkis the number of customers to have previously sampled dish k.\nHaving reached the end of the previously sampled dishes, he tries Poisson(\u03b1\nnew dishes. Figure 2 shows two draws from the IBP for two different values\nof \u03b1.\nj=1\n1\njis the D-th harmonic number, and Khis the number of rows\nwhose entries correspond to the binary number h.\ni)\nfactors (dishes)\ngenes (customers)\n5 101520\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n(a) \u03b1 = 4\nfactors (dishes)\ngenes (customers)\n10 203040\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n(b) \u03b1 = 8\nFig 2. Draws from the one parameter IBP for two different values of \u03b1.\nIf we apply the same ordering scheme to the matrix generated by this\nprocess as for the finite model, we recover the correct exchangeable distri-\nbution. Since the distribution is exchangeable with respect to the customers\nwe find by considering the last customer that\nP(zkt= 1|z\u2212kn) =mk,\u2212t\nD\n(8)\nwhere mk,\u2212t=?\ndimension follows a Poisson(\u03b1) distribution, and the expected number of\ns?=tzks, which is used in sampling Z. By exchangeability\nand considering the first customer, the number of active sources for each"},{"page":6,"text":"6\nKNOWLES ET AL.\nentries in Z is D\u03b1. We also see that the number of active features, K+=\n?D\n3. Related work.The Bayesian Factor Regression Model (BFRM)\nof West et al. (2007) is closely related to the finite version of our model.\nThe key difference is the use of a hierarchical sparsity prior. Each element\nof G has prior of the form\ngdk\u223c (1 \u2212 \u03c0dk)\u03b40(gdk) + \u03c0dkN?gdk;0,\u03bb\u22121\nThe finite IBP model is equivalent to setting \u03c0dk= \u03c0k\u223c Beta(\u03b1\/K,1) and\nthen integrating out \u03c0k. In BFRM a hierarchical prior is used:\nd=1Poisson(\u03b1\nd) = Poisson(\u03b1HD).\nk\n?\n\u03c0dk\u223c (1 \u2212 \u03c1k)\u03b40(\u03c0dk) + \u03c1kBeta(\u03c0dk;am,a(1 \u2212 m))\nwhere \u03c1k\u223c Beta(sr,s(1 \u2212 r)). Non-zero elements of \u03c0dkare given a diffuse\nprior favoring larger probabilities (a = 10,m = 0.75 are suggested in West\net al. (2007)), and \u03c1kis given a prior which strongly favors small values,\ncorresponding to a sparse solution (e.g. s = D,r =5\nNote that on integrating out \u03c0dk, the prior on gdkis\ngdk\u223c (1 \u2212 m\u03c1k)\u03b40(gdk) + m\u03c1kN?gdk;0,\u03bb\u22121\nThis hierarchical sparsity prior is motivated by improved interpretability\nin terms of less uncertainty in the posterior as to whether an element of G\nis non-zero. However, this comes at a cost of significantly increased compu-\ntation and reduced predictive performance, suggesting that the uncertainty\nremoved from the posterior was actually important.\nThe LASSO-based Sparse PCA (SPCA) method of Zou, Hastie and Tib-\nshirani (2004) and Witten, Tibshirani and Hastie (2009) has similar aims to\nour work in terms of providing a sparse variant of PCA to aid interpreta-\ntion of the results. However, since SPCA is not formulated as a generative\nmodel it is not necessarily clear how to choose the regularization parameters\nor dimensionality without resorting to cross-validation. In our experimental\ncomparison to SPCA we adjust the regularization constants such that each\ncomponent explains roughly the same proportion of the total variance as\nthe corresponding standard (non-sparse) principal component.\nD).\nk\n?\n4. Inference.\nsources X, which sources are active Z, the mixing matrix G, and all hyper-\nparameters. We use Gibbs sampling, but with Metropolis-Hastings (MH)\nsteps for sampling new features. We draw samples from the marginal distri-\nbution of the model parameters given the data by successively sampling the\nGiven the observed data Y, we wish to infer the hidden"},{"page":7,"text":"NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS\n7\nconditional distributions of each parameter in turn, given all other parame-\nters.\nSince we assume independent Gaussian noise, the likelihood function is\nP(Y|G,X,\u03c8) =\nN\n?\nt=1\n1\n(2\u03c0)\nD\n2|\u03c8|\n1\n2\nexp\n?\n\u22121\n2(yn\u2212 Gxn)T\u03c8\u22121(yn\u2212 Gxn)\n?\n(9)\nwhere \u03c8 is a diagonal noise covariance matrix.\nNotation.\nvariables not explicitly conditioned upon in the current state of the Markov\nchain. The r-th row and c-th column of matrix A are denoted Ar:and A:c\nrespectively.\nWe use \u2212 to denote the \u201crest\u201d of the model, i.e. the values of all\nMixture coefficients.\nual element of the IBP matrix, Zdk, determining whether factor k is active\nfor dimension d. Recall that \u03bbkis the precision (inverse covariance) of the\nfactor loadings for the k-th factor. The ratio of likelihoods can be calculated\nusing Equation 9. Integrating out the (d,k)-th element of the factor loading\nmatrix, gdk(whose prior is given by Equation 2) we obtain\n?P(Y|gdk,\u2212)N?gdk;0,\u03bb\u22121\n?\nWe first derive a Gibbs sampling step for an individ-\nP(Y|Zdk= 1,\u2212)\nP(Y|Zdk= 0,\u2212)=\nk\n?dgdk\nP(Y|gdk= 0,\u2212)\n?1\n(10)\n=\n\u03bbk\n\u03bbexp\n2\u03bb\u00b52\n?\n(11)\nwhere we have defined \u03bb = \u03c8\u22121\nmatrix of residuals\u02c6E = Y \u2212 GX evaluated with gdk= 0. The dominant\ncalculation is that for \u00b5 since the calculation for \u03bb can be cached. This\noperation is O(N) and must be calculated D \u00d7 K times, so sampling the\nIBP matrix, Z and factor loading matrix, G is order O(NDK).\nFrom the exchangeability of the IBP we can imagine that dimension d\nwas the last to be observed, so that the ratio of the priors is\ndXT\nk:Xk:+ \u03bbkand \u00b5 =\n\u03c8\u22121\nd\n\u03bbXT\nk:\u02c6Ed:with the\nP(Zdk= 1|\u2212)\nP(Zdk= 0|\u2212)=\nm\u2212d,k\nN \u2212 1 \u2212 m\u2212d,k\n(12)\nwhere m\u2212d,kis the number of dimensions for which factor k is active, ex-\ncluding the current dimension d. Multiplying Equations 11 and 12 gives the\nexpression for the ratio of posterior probabilities for Zdkbeing 1 or 0, which\nis used for sampling. If Zdkis set to 1, we sample gdk|\u2212 \u223c N?\u00b5,\u03bb\u22121?with\n\u00b5,\u03bb defined as for Equation 11."},{"page":8,"text":"8\nKNOWLES ET AL.\nfactors (dishes)\ngenes (customers)\n24681012\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\nFig 3. A diagram to illustrate the definition of \u03bad, for d = 10.\nAdding new features.\nnon-zero columns contribute to the likelihood and are held in memory. How-\never, the zero columns still need to be taken into account since the number\nof active factors can change. Let \u03badbe the number of columns of Z which\ncontain 1 only in row d, i.e. the number of features which are active only\nfor dimension d. Note that due to the form of the prior for elements of Z\ngiven in Equation 12, \u03bad= 0 for all d after a sampling sweep of Z. Figure 3\nillustrates \u03badfor a sample Z matrix.\nNew features are proposed by sampling \u03badwith a MH step. It is possible\nto integrate out either the new elements of the mixing matrix, g (a 1 \u00d7 \u03bad\nvector), or the new rows of the latent feature matrix, X?(a \u03bad\u00d7N matrix),\nbut not both. Since the latter generally has higher dimension, we choose to\nintegrate out X?and include gTas part of the proposal. Thus the proposal\nis \u03be = {\u03bad,g}, and we propose a move \u03be \u2192 \u03be\u2217with probability J(\u03be\u2217|\u03be). In\nthis case \u03be = \u2205 since as noted above \u03bad= 0 initially. The simplest proposal,\nfollowing Meeds et al. (2006), would be to use the prior on \u03be\u2217, i.e.\nZ is a matrix with infinitely many columns, but the\nJ(\u03be) = P(\u03bad|\u03b1) \u00b7 p(g|\u03bad,\u03bbk) = Poisson(\u03bad;\u03b3) \u00b7 N(g;0,\u03bb\u22121\n\u03b1\nD\u22121.\nUnfortunately, the rate constant of the Poisson prior tends to be so small\nk)\nwhere \u03b3 ="},{"page":9,"text":"NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS\n9\nthat new features are very rarely proposed, resulting in slow mixing. To\nremedy this we modify the proposal distribution for \u03badand introduce two\ntunable parameters, \u03c0 and \u03bb.\n(13)J(\u03bad) = (1 \u2212 \u03c0)Poisson(\u03bad;\u03bb\u03b3) + \u03c01(\u03bad= 1)\nThus the Poisson rate is multiplied by a factor \u03bb, and a spike at \u03bad= 1 is\nadded with mass \u03c0. The proposal is accepted with probability min(1,a\u03be\u2192\u03be\u2217)\nwhere\n(14)\na\u03be\u2192\u03be\u2217 =P(\u03be\u2217|\u2212,Y )J(\u03be|\u03be\u2217)\nP(\u03be|\u2212,Y )J(\u03be\u2217|\u03be)\nwhere\n=P(Y |\u03be\u2217,\u2212)P(\u03bad|\u03b1)p(g|\u03bad,\u03bbk)\nP(Y |\u2212)J(\u03bad)p(g|\u03bad,\u03bbk)\n= al\u00b7 ap\nal=P(Y |\u03be\u2217,\u2212)\nP(Y |\u2212)\nap=P(\u03bad|\u03b1)\nJ(\u03bad)\n(15)\n=\nPoisson(\u03bad;\u03b3)\nPoisson(\u03bad;\u03bb\u03b3)\n(16)\nNote that we take J(\u03be|\u03be\u2217) = 1 since \u03be = \u2205. To calculate the likelihood ratio,\nal, we need the collapsed likelihood under the new proposal:\nP(Yd:|\u03be\u2217,\u2212) =\nN\n?\nN\n?\nn=1\n?\nP(Ydn|\u03be\u2217,x?\nn,\u2212)P(x?\nn)dx?\n(17)\n=\nn=1\n(2\u03c0\u03c8\u22121\nd)\u22121\n2(2\u03c0)\n\u03bad\n2 |M|\u22121\n2exp\n?1\n2(mT\nnMmn\u2212 \u03c8\u22121\nd\n\u02c6E2\ndn)\n?\n(18)\nwhere we have defined M = \u03c8\u22121\nmatrix of residuals\u02c6E = Y \u2212 GX. The likelihood under the current sample\nis:\n?\nSubstituting these likelihood terms into the expression for the ratio of like-\nlihood terms, al, gives\ndggT+I\u03badand mn= M\u22121\u03c8\u22121\ndg\u02c6Ednwith the\nP(Yd:|\u03be,\u2212) =\nN\nn=1\n(2\u03c0\u03c8\u22121\nd)\u22121\n2exp\n?\n\u22121\n2\u03c8\u22121\nd\n\u02c6E2\ndn\n?\n(19)\nal= (2\u03c0)\nN\u03bad\n2 |M|\u2212N\n2 exp\n?\n1\n2\n?\nn\nmT\nnMmn\n?\n(20)\nWe found that appropriate scheduling of the sampler improved mixing,\nparticularly with respect to adding new features. The final scheme we settled\non is described in Algorithm 1."},{"page":10,"text":"10\nKNOWLES ET AL.\nIBP parameters.\nwith conjugate Gamma(e,f) prior (note that we use the inverse scale param-\neterization of the Gamma distribution). The conditional prior of Equation 7,\nacts as the likelihood term and the posterior update is as follows:\nWe can choose to sample the IBP strength parameter \u03b1,\nP(\u03b1|Z) \u221d P(Z|\u03b1)P(\u03b1) = Gamma(\u03b1;K++ e,f + HD)\nwhere K+is the number of active sources and HD=?D\nThe remaining sampling steps are standard, but are included here for\ncompleteness.\n(21)\nj=1\n1\njis the D-th\nharmonic number.\nLatent variables.\nfor each t \u2208 [1,...,N] we have\n(22)P(xn|\u2212) \u221d P(yn|xn,\u2212)P(xn) = N (xn;\u00b5n,\u039b)\nwhere we have defined \u039b = GT\u03c8\u22121G + I and \u00b5n= \u039b\u22121GT\u03c8\u22121yn. Note\nthat since \u039b does not depend on n we only need to compute and invert\nit once per iteration. Calculating \u039b is order O(K2D), and inverting it is\nO(K3). Calculating \u00b5tis order O(KD) and must be calculated for all N\nxt\u2019s, a total of O(NKD). Thus sampling X is order O(K2+ K3+ NKD).\nSampling the columns xnof the latent variable matrix X\nFactor precision.\nstrained to be equal, we have \u03bbk= \u03bb \u223c Gamma(c,d). The posterior update\nis then given by \u03bb|G \u223c Gamma(c +\nHowever, if the variances are allowed to be different for each column of\nG, we set \u03bbk\u223c Gamma(c,d), and the posterior update is given by \u03bbk|G \u223c\nGamma(c +mk\ndk). In this case we may also wish to share power\nacross factors, in which case we also sample d. Putting a Gamma prior on d\nsuch that d \u223c Gamma(c0,d0), the posterior update is d|\u03bbk\u223c Gamma(c0+\ncK,d0+?K\nNoise variance.The additive Gaussian noise can be constrained to be\nisotropic, in which case the inverse variance is given a Gamma prior: \u03c8\u22121\n\u03c8\u22121\u223c Gamma(a,b) which gives the posterior update \u03c8\u22121|\u2212 \u223c Gamma(a+\nND\nd,n\u02c6E2\nHowever, if the noise is only assumed to be independent (which we have\nfound to be more appropriate for gene expression data), then each dimen-\nsion has a separate noise variance, whose inverse is given a Gamma prior:\n\u03c8\u22121\nd\n\u223c Gamma(a,b) which gives the posterior update \u03c8\u22121\nN\nnE2\npower between dimensions by giving the hyperparameter b a hyperprior\nIf the mixture coefficient prior precisions \u03bbk are con-\n?\nkmk\n2\n,d +?\nd,kG2\ndk).\n2,d +?\ndG2\nk=1\u03bbk).\nd\n=\n2,b +?\ndn).\nd|\u2212 \u223c Gamma(a+\n2,b +?\ndn) where the matrix of residuals\u02c6E = Y \u2212 GX. We can share"},{"page":11,"text":"NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS\n11\nGamma(a0,b0) resulting in the Gibbs update b|\u2212 \u223c Gamma(a0+ aD,b0+\n?D\nd=1\u03c8\u22121\nvariances in each dimension, so we will refer to this variant as sc.\nd). This hierarchical prior results in soft coupling between the noise\nAlgorithm 1 One iteration of the NSFA sampler\nfor d = 1 to D do\nfor k = 1 to K do\nSample Zdk\nend for\nSample \u03bad\nend for\nfor n = 1 to N do\nSample X:n\nend for\nSample \u03b1,\u03c6,\u03bbg\n5. Results.We compare the following models:\n\u2022 FA - Bayesian Factor Analysis, see for example Kaufman and Press\n(1973) or Rowe and Press (1998)\n\u2022 AFA - Factor Analysis with ARD prior to determine active sources\n\u2022 FOK - The sparse Factor Analysis method of Fokoue (2004), Fevotte\nand Godsill (2006) and Archambeau and Bach (2009)\n\u2022 SPCA - The Sparse PCA method of Zou, Hastie and Tibshirani (2004)\n\u2022 BFRM - Bayesian Factor Regression Model of West et al. (2007).\n\u2022 SFA - Sparse Factor Analysis, using the finite IBP\n\u2022 NSFA - The proposed model: Nonparametric Sparse Factor Analysis\nNote that all of these models can be learned using the software package\nwe provide simply by using appropriate settings.\n5.1. Synthetic data.\nIBP itself would clearly bias towards our model, we instead use the D =\n100 gene by K = 16 factor E. Coli connectivity matrix derived in Kao\net al. (2004) from RegulonDB and current literature. We ignore whether the\nconnection is believed to be up or down regulation, resulting in a binary\nmatrix Z. We generate random datasets with N = 100 samples by drawing\nthe non-zero elements of G (corresponding to the elements of Z which are\nnon-zero), and all elements of X, from a zero mean unit variance Gaussian,\ncalculating Y = GX + E, where E is Gaussian white noise with variance\nset to give a signal to noise ratio of 10.\nSince generating a connectivity matrix Z from the"},{"page":12,"text":"12\nKNOWLES ET AL.\nFig 4. Boxplot of reconstruction errors for simulated data derived from the E. Coli con-\nnectivity matrix of Kao et al. (2004). Ten datasets were generated and the reconstruction\nerror calculated for the last ten samples from each algorithm. Numbers refer to the number\nof latent factors used, K. a1 denotes fixing \u03b1 = 1. sn denotes sharing power between noise\ndimensions.\nHere we will define the reconstruction error, Eras\nEr(G,\u02c6G) =\n1\nDK\nK\n?\nk=1\nmin\n\u02c6k\u2208{1,..,\u02c6 K}\nD\n?\nd=1\n(Gdk\u2212 Gd\u02c6k)2\nwhere\u02c6G,\u02c6K are the inferred quantities. Although we minimize over permu-\ntations, we do not minimize over rotations since, as noted in Fokoue (2004),\nthe sparsity of the prior stops the solution being rotation invariant. We av-\nerage this error over the last ten samples of the MCMC run. This error\nfunction does not penalize inferring extra spurious factors, so we will inves-\ntigate this possibility separately. The precision and recall of active elements\nof the Z achieved by each algorithm (after thresholding for the non-sparse\nalgorithms) are presented in the supplementary material (see Supplement\nA), but omitted here since the results are consistent with the reconstruction\nerror.\nThe reconstruction error for each method with different numbers of latent\nfeatures is shown in Figure 4. Ten random datasets were used and for the\nsampling methods (all but SPCA) the results were averaged over the last\nten samples out of 1000. Unsurprisingly, plain Factor Analysis (FA) performs"},{"page":13,"text":"NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS\n13\nthe worst, with increasing overfitting as the number of factors is increased.\nFor\u02c6K = 20 the variance is also very high, since the four spurious features fit\nnoise. Using an ARD prior on the features (AFA) improves the performance,\nand overfitting no longer occurs. The reconstruction error is actually less\nfor\u02c6K = 20, but this is an artifact due to the reconstruction error not\npenalizing additional spurious features in the inferred G. The Sparse PCA\n(SPCA) of Zou, Hastie and Tibshirani (2004) shows improved reconstruction\ncompared to the non-sparse methods (FA and AFA) but does not perform\nas well as the Bayesian sparse models. Sparse factor analysis (SFA), the\nfinite version of the full infinite model, performs very well. The Bayesian\nFactor Regression Model (BFRM) performs significantly better than the\nARD factor analysis (AFA), but not as well as our sparse model (SFA). It\nis interesting that for BFRM the reconstruction error decreases significantly\nwith increasing\u02c6K, suggesting that the default priors may actually encourage\ntoo much sparsity for this dataset. Fokoue\u2019s method (FOK) only performs\nmarginally better than AFA, suggesting that this \u201csoft\u201d sparsity scheme is\nnot as effective at finding the underlying sparsity in the data. Overfitting\nis also seen, with the error increasing with\u02c6K. This could potentially be\nresolved by placing an appropriate per factor ARD-like prior over the scale\nparameters of the Gamma distributions controlling the precision of elements\nof G. Finally, the Non-parametric Sparse Factor Analysis (NSFA) proposed\nhere and in Rai and Daum\u00b4 e III (2008) performs very well. With fixed \u03b1 = 1\n(a1) or inferring \u03b1 we see very similar performance. Using the soft coupling\n(sc) variant which shares power between dimensions when fitting the noise\nvariances seems to reduce the variance of the sampler, which is reasonable\nin this example since the noise was in fact isotropic.\nSince the reconstruction error does not penalize spurious factors it is im-\nportant to check that NSFA is not scoring well simply by inferring many\nadditional factors. Histograms for the number of latent features inferred for\nthe nonparametric sparse model are shown in Figure 5. This represents an\napproximate posterior over K. For fixed \u03b1 = 1 the distribution is centered\naround the true value of K = 16, with minimal bias (EK = 16.1). The vari-\nance is significant (standard deviation of 1.46), but is reasonable considering\nthe noise level (SNR=10) and that in some of the random datasets, elements\nof Z which are 1 could be masked by very small corresponding values of G.\nThis hypothesis is supported by the results of a similar experiment where\nG was set equal to Z. In this case, the sampler always converged to at least\n16 features, but would also sometimes infer spurious features from noise (re-\nsults not shown). When inferring \u03b1 some bias and skew are noticeable. The\nmean of the posterior is now at 18.3 with standard deviation 2.0, suggesting"},{"page":14,"text":"14\nKNOWLES ET AL.\n1314\nNumber of latent factors\n15161718192021\n0\n50\n100\n150\n200\n250\n300\nfreq\n14 15 16 17 18 19 20 21 22 23 24 25 26 27\nNumber of latent factors\n0\n50\n100\n150\n200\n250\nfreq\nFig 5. Histograms of the number of latent features inferred by the nonparametric sparse\nFA sampler for the last 100 samples out of 1000. Left: With \u03b1 = 1. Right: Inferring \u03b1.\nthere is little to gain from sampling \u03b1 in this data.\n5.2. Convergence.\nof new features is drawn from the prior. Figure 6 shows how the different\nproposals for \u03badeffect how quickly the sampler reaches a sensible number\nof features. If we use the prior as the proposal distribution, mixing is very\nslow, taking around 5000 iterations to converge, as shown in Figure 6(a). If\na mass of 0.1 is added at \u03bad= 1 (see Equation 13), then the sampler reaches\nthe equilibrium number of features in around 1500 iterations, as shown in\nFigure 6(b)). However, if we try to add features even faster, for example\nby setting the factor \u03bb = 50 in Equation 13, then the sampling noise is\ngreatly increased, as shown in Figure 6(c), and the computational cost also\nincreases significantly because so many spurious features are proposed only\nto be rejected.\nNSFA can suffer from slow convergence if the number\n(a) Prior.\n0\n2\niterations\/1000\n4\n6\n8\n10\n0\n20\n40\n60\n80\nnum active factors\n(b) Prior plus 0.1I(\u03ba = 1).\n0\n400\niterations\n800\n1200\n0\n20\n40\n60\n80\n100\nnum active factors\n(c) Factor \u03bb = 50.\nFig 6. The effect of different proposal distributions for the number of new features."},{"page":15,"text":"NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS\n15\n(a) Log likelihood of test data under each\nmodel based on the last 100 MCMC samples.\nThe boxplots show variation across 10 differ-\nent random splits of the data into training\nand test sets.\n01000 2000\n3000\n0\n1\n2\n3\n4\n5\n6\niterations\nactive factors\n(b) Number of active latent features\nduring a typical MCMC run of the\nNSFA model.\nFig 7. Results on E. Coli time-series dataset from Kao et al. (2004) (N = 24,D = 100,\n3000 MCMC iterations).\n5.3. Biological data: E. Coli time-series dataset.\nmance of each algorithm on the biological data where no ground truth is\navailable, we calculated the test set log likelihood under the posterior. Ten\npercent of entries from Y were removed at random, ten times, to give ten\ndatasets for inference. We do not use mean square error as a measure of\npredictive performance because of the large variation in the signal to noise\nratio across gene expression level probes.\nThe test log likelihood achieved by the various algorithms on the E. Coli\ndataset from Kao et al. (2004), including 100 genes at 24 time-points, is\nshown in Figure 7(a). On this simple dataset incorporating sparsity doesn\u2019t\nimprove predictive performance. Overfitting the number of latent factors\ndoes damage performance, although using the ARD or sparse prior alleviates\nthe problem. Based on predictive performance of the finite models, five is a\nsensible number of features for this dataset: the NSFA model infers a median\nnumber of 4 features, with some probability of there being 5, as shown in\nFigure 7(b).\nTo assess the perfor-\n5.4. Breast cancer dataset.\ndictive performance on the breast cancer dataset of West et al. (2007), in-\ncluding 226 genes across 251 individuals. We find that all the finite models\nare sensitive to the choice of the number of factors, K. The samplers were\nfound to have converged after around 1000 samples according to standard\nWe assess these algorithms in terms of pre-"},{"page":16,"text":"16\nKNOWLES ET AL.\nmultiple chain convergence measures, so 3000 MCMC iterations were used\nfor all models. The predictive log likelihood was calculated using the final\n100 MCMC samples. Figure 8(a) shows test set log likelihoods for 10 ran-\ndom divisions of the data into training and test sets. Factor analysis (FA)\nshows significant overfitting as the number of latent features is increased\nfrom 20 to 40. Using the ARD prior prevents this overfitting (AFA), giving\nimproved performance when using 20 features and only slightly reduced per-\nformance when 40 features are used. The sparse finite model (SFA) shows\nan advantage over AFA in terms of predictive performance as long as un-\nderfitting does not occur: performance is comparable when using only 10\nfeatures. However, the performance of SFA is sensitive to the choice of the\nnumber of factors, K. The performance of the sparse nonparametric model\n(NSFA) is comparable to the sparse finite model when an appropriate num-\nber of features is chosen, but avoids the time consuming model selection\nprocess. Fokoue\u2019s method (FOK) was run with K = 20 and various settings\nof the hyperparameter d which controls the overall sparsity of the solution.\nThe model\u2019s predictive performance depends strongly on the setting of this\nparameter, with results approaching the performance of the sparse models\n(SFA and NSFA) for d = 10\u22124. The performance of BFRM on this dataset\nis noticeably worse than the other sparse models.\nWe now consider the computation cost of the algorithms. As described\nin Section 4, sampling Z and G takes order O(NKD) operations per itera-\ntion, and sampling X takes O(K2+ K3+ ND). However, for the moderate\nvalues encountered for datasets 1 and 2 the main computational cost is sam-\npling the non-zero elements of G, which takes O((1\u2212s)DK) where s is the\nsparsity of the model. Figure 8(c) shows the mean CPU time per iteration\ndivided by the number of features at that iteration. Naturally, straight FA\nis the fastest, taking only around 0.025s per iteration per feature. The value\nincreases slightly with increasing K, suggesting that here the O(K2D+K3)\ncalculation and inversion of \u03bb, the precision of the conditional on X, must be\ncontributing. The computational cost of adding the ARD prior is negligible\n(AFA). The CPU time per iteration is just over double for the sparse finite\nmodel (SFA), but the cost actually decreases with increasing K, because\nthe sparsity of the solution increases to avoid overfitting. There are fewer\nnon-zero elements of G to sample per feature, so the CPU time per feature\ndecreases. The CPU time per iteration per feature for the non-parametric\nsparse model (NSFA) is somewhat higher than for the finite model because\nof the cost of the feature birth and death process. However, Figure 8(b)\nshows the absolute CPU time per iteration, where we see that the nonpara-\nmetric model is only marginally more expensive than the finite model of"},{"page":17,"text":"NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS\n17\n10\n20\nFA\n401020\nAFA\n4010152040\n1e-3   1e-4    1e-5   1e-6\nFOK\n5 1020\n\u22123800\n\u22123600\n\u22123400\n\u22123200\n\u22123000\n\u22122800\nlog likelihood of test data\nSFABFRM\nNSFA\n(a) Predictive performance: log likelihood of test (the 10%\nmissing) data under each model based on the last 100 MCMC\nsamples. Higher values indicate better performance. The box-\nplots show variation across 10 different random splits of the\ndata into training and test sets.\n1\n2\n3\n4\n5\ncpu time per iteration\n10\n20\nFA\n401020\nAFA\n40101520 40\n1e-3   1e-4    1e-5   1e-6\nFOK, K=20\n51020\nSFABFRM\nNSFA\n(b) CPU time (in seconds) per iteration, averaged across the\n3000 iteration run.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\ncpu time per iteration per feature\n10\n20\nFA\n4010 20\nAFA\n4010152040\n1e-3   1e-4    1e-5   1e-6\nFOK\n51020\nSFABFRM\nNSFA\n(c) CPU time (in seconds) per iteration divided by the number\nof features at that iteration, averaged across all iterations.\nFig 8. Results on breast cancer dataset (N = 251,D = 226, 3000 MCMC iterations)."},{"page":18,"text":"18\nKNOWLES ET AL.\n10 20\nAFA\n401020\nSFA\n40\n\u22123.15\n\u22123.1\n\u22123.05\n\u22123\n\u22122.95\n\u22122.9\n\u22122.85\nx 10\n5\nlog likelihood of test data\nNSFA\nFig 9. Test set log likelihoods on Prostate cancer dataset from Yu et al. (2004), including\n12557 genes across 171 individuals (1000 MCMC iterations).\nappropriate size\u02c6K = 15 and cheaper than choosing an unnecessarily large\nfinite model (SFA with K = 20,40). Fokoue\u2019s method (FOK) has compara-\nble computational performance to the sparse finite model, but interestingly\nhas increased cost for the optimal setting of d = 10\u22124. The parameter space\nfor FOK is continuous, making search easier but requiring a normal random\nvariable for every element of G. BFRM pays a considerable computational\ncost for both the hierarchical sparsity prior and the DP prior on X. SPCA\nwas not run on this dataset but results on the synthetic data in Section 5.1\nsuggest it is somewhat faster than the sampling methods, but not hugely\nso. The computational cost of SPCA is ND2+ mO(D2K + DK2+ D3) in\nthe N > D case (where m is the number of iterations to convergence) and\nND2+ mO(D2K + DK2) in the D > N case taking the limit \u03bb \u2192 \u221e. In\neither case an individual iteration of SPCA is more expensive than one sam-\npling iteration of NSFA (since K < D) but fewer iterations will generally be\nrequired to reach convergence of SPCA than are required to ensure mixing\nof NSFA.\n5.5. Prostate cancer dataset.\nof AFA, FOK and NSFA on the prostate cancer dataset of Yu et al. (2004),\nfor ten random splits into training and test data. The boxplots show varia-\ntion from ten random splits into training and test data. The large number\nof genes (D = 12557 across N = 171 individuals) in this dataset makes in-\nference slower, but the problem is manageable since the computational com-\nFigure 9 shows the predictive performance"},{"page":19,"text":"NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS\n19\nplexity is linear in the number of genes. Despite the large number of genes,\nthe appropriate number of latent factors, in terms of maximizing predictive\nperformance, is still small, around 10 (NSFA infers a median of 12 factors).\nThis may seem small relative to the number of genes, but it should be noted\nthat the genes included in the breast cancer and E. Coli datasets are those\ncapturing the most variability. Surprisingly, SFA actually performed slightly\nworse on this dataset than AFA. Both are highly sensitive to the number of\nlatent factors chosen. NSFA however gives better predictive log likelihoods\nthan either finite model for any fixed number of latent factors K. Running\n1000 iterations of NSFA on this dataset takes under 8 hours. BFRM and\nFOK were impractically slow to run on this dataset.\n6. Discussion.\ndatasets that sparsity can improve predictive performance, as well as provid-\ning a more easily interpretable solution. Using the IBP to provide sparsity\nis straightforward, and allows the number of latent factors to be inferred\nwithin a well defined theoretical framework. This has several advantages\nover manually choosing the number of latent factors. Choosing too few la-\ntent factors damages predictive performance, as seen for the breast cancer\ndataset. Although choosing too many latent factors can be compensated for\nby using appropriate ARD-like priors, we find this is typically more compu-\ntationally expensive than the birth and death process of the IBP. Manual\nmodel selection is an alternative but is time consuming. Finally we show that\nrunning NSFA on full gene expression datasets with 10000+ genes is feasible\nso long as the number of latent factors remains relatively small. An inter-\nesting direction for this research is how to incorporate prior knowledge, for\nexample if certain transcription factors are known to regulate specific genes.\nIncorporating this knowledge could both improve the performance of the\nmodel and improve interpretability by associating latent variables with spe-\ncific transcription factors. Another possibility is incorporating correlations\nin the Indian Buffet Process, which has been proposed for simpler mod-\nels (Courville, Eck and Bengio, 2009; Doshi-Velez and Ghahramani, 2009).\nThis would be appropriate in a gene expression setting where multiple tran-\nscription factors might be expected to share sets of regulated genes due to\ncommon motifs. Unfortunately, performing MCMC in all but the simplest\nof these models suffers from slow mixing.\nWe have seen that in both the E. Coli and breast cancer\nAcknowledgements.\nfor helpful comments.\nWe would like to thank the anonymous reviewers"},{"page":20,"text":"20\nKNOWLES ET AL.\nSUPPLEMENTARY MATERIAL\nSupplement A: Graphs of precision and recall for the synthetic\ndata experiment.\n(doi: ??????; .pdf). The precision and recall of active elements of the Z\nmatrix achieved by each algorithm (after thresholding for the non-sparse\nalgorithms) on the synethic data experiment, described in Section 5.1. The\nresults are consistent with the reconstruction error.\nReferences.\nArchambeau, C. and Bach, F. (2009). Sparse Probabilistic Projections. In Proceed-\nings of the Conference on Neural Information Processing Systems (NIPS) (D. Koller,\nD. Schuurmans, Y. Bengio and L. Bottou, eds.) 73-80. MIT Press, Vancouver,\nCanada.\nCourville, A. C., Eck, D. and Bengio, Y. (2009). An Infinite Factor Model Hierarchy\nVia a Noisy-Or Mechanism. In Advances in Neural Information Processing Systems 21.\nThe MIT Press, Cambridge, MA, USA.\nDoshi-Velez, F. and Ghahramani, Z. (2009). Correlated Nonparametric Latent Feature\nModels In Conference on Uncertainty in Artificial Intelligence.\nFevotte, C. and Godsill, S. J. (2006). A Bayesian Approach for Blind Separation of\nSparse Sources. Audio, Speech, and Language Processing, IEEE Transactions on 14\n2174-2188.\nFokoue, E. (2004). Stochastic determination of the intrinsic structure in Bayesian fac-\ntor analysis. Technical Report No. 17, Statistical and Applied Mathematical Sciences\nInstitute.\nGriffiths, T. L. and Ghahramani, Z. (2006). Infinite Latent Feature Models and the\nIndian Buffet Process. In Advances in Neural Information Processing Systems 18. The\nMIT Press, Cambridge, MA, USA.\nKao, K. C., Yang, Y.-L., Boscolo, R., Sabatti, C., Roychowdhury, V. and\nLiao, J. C. (2004). Transcriptome-based determination of multiple transcription reg-\nulator activities in Escherichia coli by using network component analysis. Proceedings\nof the National Academy of Sciences of the United States of America (PNAS) 101\n641\u2013646.\nKaufman, G. M. and Press, S. J. (1973). Bayesian factor analysis. Technical Report\nNo. 662-73, Sloan School of Management, University of Chicago.\nKnowles, D. and Ghahramani, Z. (2007). Infinite Sparse Factor Analysis and Infinite\nIndependent Components Analysis. In 7th International Conference on Independent\nComponent Analysis and Signal Separation 381-388.\nMeeds, E., Ghahramani, Z., Neal, R. and Roweis, S. (2006). Modeling Dyadic Data\nwith Binary Latent Factors. In Neural Information Processing Systems 19.\nRai, P. and Daum\u00b4 e III, H. (2008). The Infinite Hierarchical Factor Regression Model. In\nNeural Information Processing Systems.\nRowe, D. B. and Press, S. J. (1998). Gibbs Sampling and Hill Climbing in Bayesian\nFactor Analysis. Technical Report No. 255, Department of Statistics, University of\nCalifornia Riverside.\nWest, M., Chang, J., Lucas, J., Nevins, J. R., Wang, Q. and Carvalho, C. (2007).\nHigh-Dimensional Sparse Factor Modelling: Applications in Gene Expression Genomics\nTechnical Report, ISDS, Duke University."},{"page":21,"text":"NONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS\n21\nWitten, D. M., Tibshirani, R. and Hastie, T. (2009). A penalized matrix decom-\nposition, with applications to sparse principal components and canonical correlation\nanalysis. Biostatistics 10 515\u2013534.\nYu, Y. P., Landsittel, D., Jing, L., Nelson, J., Ren, B., Liu, L., McDonald, C.,\nThomas, R., Dhir, R., Finkelstein, S., Michalopoulos, G., Becich, M. and\nLuo, J.-H. (2004). Gene expression alterations in prostate cancer predicting tumor\naggression and preceding development of malignancy. Journal of Clinical Oncology 22\n2790\u20132799.\nZhang, Z., Chan, K. L., Kwok, J. T. and yan Yeung, D. (2004). Bayesian Inference\non Principal Component Analysis using Reversible Jump Markov Chain Monte Carlo.\nIn Proceedings of the 19th National Conference on Artificial Intelligence, San Jose,\nCalifornia, USA 372-377. AAAI Press.\nZou, H., Hastie, T. and Tibshirani, R. (2004). Sparse Principal Component Analysis.\nJournal of Computational and Graphical Statistics 15 2006.\nCambridge University Engineering Department\nTrumpington Street\nCambridge\nCB2 1PZ\nUK\nE-mail: dak33@cam.ac.uk\nzoubin@eng.cam.ac.uk"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/David_Knowles2\/publication\/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling\/links\/55554b9d08ae6fd2d821cdba.pdf","widgetId":"rgw28_56ab9f7b0d775"},"id":"rgw28_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=47843669&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw29_56ab9f7b0d775"},"id":"rgw29_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=47843669&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":47843669,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"55554b9d08ae6fd2d821cdba","name":"David A. Knowles","date":"May 15, 2015 ","nameLink":"profile\/David_Knowles2","filename":"1011.6293.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/David_Knowles2\/publication\/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling\/links\/55554b9d08ae6fd2d821cdba.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/David_Knowles2\/publication\/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling\/links\/55554b9d08ae6fd2d821cdba.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"c32469a8125cae5583e54089801dd5ed","showFileSizeNote":false,"fileSize":"863.44 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"55554b9d08ae6fd2d821cdba","name":"David A. Knowles","date":"May 15, 2015 ","nameLink":"profile\/David_Knowles2","filename":"1011.6293.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/David_Knowles2\/publication\/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling\/links\/55554b9d08ae6fd2d821cdba.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/David_Knowles2\/publication\/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling\/links\/55554b9d08ae6fd2d821cdba.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"c32469a8125cae5583e54089801dd5ed","showFileSizeNote":false,"fileSize":"863.44 KB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=W83LBU7Wa-HkkqtI4S9NVnY5O-drJ-qOK-N4Glaa8w6-XJ6qM5Gw55qoGzobi_-KJnO_AyaTXhzz97ZWJyUkMw.VAhg_TlnxA9tMh9LK-9unhiq5YHn6WeAO9akLXJH7BmkmGPgtENbDsgSwJGklYTfVsg9faHXosIPkN4oc2nkMw","clickOnPill":"publication.PublicationFigures.html?_sg=fvSYbPDYNZuve_qLYFQdpOB9XLWkx27p3AQyzq5d4T5oSqaBB0e1z8IVTAx3Q84xq9kFmIN1GjPJgSxBeY9fEw.zvg5KfiR7yIHLtuMlKDRViO1GP_PCV-p_lblcPR_wOJQkvyLw-Xm04-rQzxTCMZyV7Y0eOXaoEJzexCBAtupCw"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FDavid_Knowles2%2Fpublication%2F47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling%2Flinks%2F55554b9d08ae6fd2d821cdba.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=H5fifzsVszbG3Mnk9jc4rSX6A775RbP_7fHpMG63PFeoYtzdzoQHTYCLwV9-7kAQaqPFQrt9CQimZ4osrf6DzQ","urlHash":"81ed2517ecaa16133e749ce2f52cab49","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=gpX500Dxc0K3DH2nnUqIUs2L91TqgI4PNxFrBEEc88pFd0-mA_X_y9eM3NxXN0Ie0PsiSSUkimOR49JvXZZB3VXfDKAUCGL5yGZtfsMW1Wo.K6qj2IsL9AvO3leYGHrNM2A1tcSpAF06_jaba5tmyvtKq6RjKDpNO8L8kKzSVKvqlDFZZAl6NZTq_T5iflSnhg.rqGNex72gEm1jtVz1cn1LUeJC0SBvxA_ksespEGoEjHQSV-IzwP3ypISaBKLYKLnQbjeYNIZfX6wJ06EjFdBEA","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"55554b9d08ae6fd2d821cdba","trackedDownloads":{"55554b9d08ae6fd2d821cdba":{"v":false,"d":false}},"assetId":"AS:229184605782021@1431653276958","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":47843669,"commentCursorPromo":null,"widgetId":"rgw31_56ab9f7b0d775"},"id":"rgw31_56ab9f7b0d775","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FDavid_Knowles2%2Fpublication%2F47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling%2Flinks%2F55554b9d08ae6fd2d821cdba.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A229184605782021%401431653276958&publicationUid=47843669&linkId=55554b9d08ae6fd2d821cdba&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Nonparametric Bayesian sparse factor models with application to gene\nexpression modeling","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=HHaQECgLv1wY3H4P2P3arfrvQXXXsOcgV-Ip1HZSOAwLYMdcApdmgfdOh3EKxYXmR7xin27iGZnMstw6jtr7fA2Ch263yPa1e7ns11_pwz8.A743VSi4oCQbpW66VHVsKOZW13nKgB6ao7fGvtkcl4X8eubJan-CNDTZuJ_5DPPRpHYrWD9N5PRbaIpcydRLtw._qY41orBRXqZOrk9Idy9Eloc-pbc4eJ6NkHt_kanHAky1pUsrQKYvyeOcz3MtoGIpWPXvfdIcsY74CePxFsE1w","publicationUid":47843669,"trackedDownloads":{"55554b9d08ae6fd2d821cdba":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw33_56ab9f7b0d775"},"id":"rgw33_56ab9f7b0d775","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw34_56ab9f7b0d775"},"id":"rgw34_56ab9f7b0d775","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw35_56ab9f7b0d775"},"id":"rgw35_56ab9f7b0d775","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw36_56ab9f7b0d775"},"id":"rgw36_56ab9f7b0d775","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw37_56ab9f7b0d775"},"id":"rgw37_56ab9f7b0d775","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw32_56ab9f7b0d775"},"id":"rgw32_56ab9f7b0d775","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw30_56ab9f7b0d775"},"id":"rgw30_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab9f7b0d775"},"id":"rgw2_56ab9f7b0d775","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":47843669},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=47843669&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab9f7b0d775"},"id":"rgw1_56ab9f7b0d775","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"ubJMXgSpz4eL2AZBIndrrqGht3N1DM6cgMOndPPHHVvYDGHhU\/DgnNFaIe1flKWQRz7zdX7yLTad4fDdsQ4RiwsoSNhpShX3HkzzT84\/prXEUEunL7gasqGDK0H4zNDM6upSLOElu\/D0gcxfIWqcaid3r0llI8wzUa\/VNyUkQ8nJxMnc3tNnC+OM1uaJ4FTy+JwyyKElxi2mr\/2FE0dBiYXYDaH2PGswCFu+\/Y4CWeCIgrNfQOvkAGU+\/tM8VMdOB2KYO6Z9pcH7FKBYOtx9sB8MZvTDqP1vs8wzU79Li4g=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Nonparametric Bayesian sparse factor models with application to gene\nexpression modeling\" \/>\n<meta property=\"og:description\" content=\"A nonparametric Bayesian extension of Factor Analysis (FA) is proposed where\nobserved data $\\mathbf{Y}$ is modeled as a linear superposition, $\\mathbf{G}$,\nof a potentially infinite number of...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling\/links\/55554b9d08ae6fd2d821cdba\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling\" \/>\n<meta property=\"rg:id\" content=\"PB:47843669\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1214\/10-AOAS435\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Nonparametric Bayesian sparse factor models with application to gene\nexpression modeling\" \/>\n<meta name=\"citation_author\" content=\"David Knowles\" \/>\n<meta name=\"citation_author\" content=\"Zoubin Ghahramani\" \/>\n<meta name=\"citation_publication_date\" content=\"2010\/11\/29\" \/>\n<meta name=\"citation_journal_title\" content=\"The Annals of Applied Statistics\" \/>\n<meta name=\"citation_issn\" content=\"1932-6157\" \/>\n<meta name=\"citation_volume\" content=\"5\" \/>\n<meta name=\"citation_issue\" content=\"2011\" \/>\n<meta name=\"citation_doi\" content=\"10.1214\/10-AOAS435\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/David_Knowles2\/publication\/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling\/links\/55554b9d08ae6fd2d821cdba.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-340d01a6-7a15-4a59-bfe2-ea65610226db","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":400,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw38_56ab9f7b0d775"},"id":"rgw38_56ab9f7b0d775","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-340d01a6-7a15-4a59-bfe2-ea65610226db", "cfaf47938d8046114335550bd7322740ffbbaa5b");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-340d01a6-7a15-4a59-bfe2-ea65610226db", "cfaf47938d8046114335550bd7322740ffbbaa5b");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw39_56ab9f7b0d775"},"id":"rgw39_56ab9f7b0d775","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/47843669_Nonparametric_Bayesian_sparse_factor_models_with_application_to_geneexpression_modeling","requestToken":"kXK4Fgj+YrugXhHP1oX1z0PTV9cltTZsQb9fcSPerXuEWxothmHHHyMa68odpCTQEuYR12sn45QSyLMickUJwderRGI1MSKyQrrAEN8hNxpfAzaSU2gAk7Vaex1hSP\/B0C9dcGc9BSQKSOR+CP+c3YzDHx\/vEK\/GnY9xpSawR0yoyIZCgAQ+K3a0C2OKbbnQeuit6PLvC3X\/16TY8pkJVaNIs9jjePUoULW+Zm6ybbUvIJMYurk\/7DZke6K5Rh4IknPaRLDonNsmSfeXCYOmnXtrPAq9vEOOUkVumzTOfTg=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=-McXKYND0KseG3Ww2LRRZ8XfnwTFXxqZkQG0VcMERJQPhKUksKlhlu5VH1SAlcKO","encodedUrlAfterLogin":"cHVibGljYXRpb24vNDc4NDM2NjlfTm9ucGFyYW1ldHJpY19CYXllc2lhbl9zcGFyc2VfZmFjdG9yX21vZGVsc193aXRoX2FwcGxpY2F0aW9uX3RvX2dlbmVleHByZXNzaW9uX21vZGVsaW5n","signupCallToAction":"Join for free","widgetId":"rgw41_56ab9f7b0d775"},"id":"rgw41_56ab9f7b0d775","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw40_56ab9f7b0d775"},"id":"rgw40_56ab9f7b0d775","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw42_56ab9f7b0d775"},"id":"rgw42_56ab9f7b0d775","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
