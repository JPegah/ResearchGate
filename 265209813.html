<!DOCTYPE html> <html lang="en" class="" id="rgw38_56aba20b301e0"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="g1QL1cqwJaLrgO0nHVRAsyND3dGXK8FzJvC0M+Peqq3R3ZoSzDfPPLNPyu6vJxGzvuA398liWA617K11wej7KbAHN6awAioEPo56lQDoFiKd+rBXZvr/kFxZuY6U0J1Uh6fRgpO3irTReshnCvc3b0E7vNhxMQqd+VYrxcUtsM7l+EAK4aVuohFMR5+S5W9fAm3Gr2C89EQocsIF6XGQL6wQVCKrVFkWVCgfn9y7uDJ2JIabd57fcoR3ZMh+jYXCPxxZEsiuJGblBQ5+ZXXfJHZzDH6COdrDOqzbQAtKy6o="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-cdf14eca-e980-4ddc-8485-af29cd7f5eb4",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Faster Computation of Expected Hypervolume Improvement" />
<meta property="og:description" content="The expected improvement algorithm (or efficient global optimization) aims
for global continuous optimization with a limited budget of black-box function
evaluations. It is based on a statistical..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement/links/5404956f0cf23d9765a67a6c/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement" />
<meta property="rg:id" content="PB:265209813" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Faster Computation of Expected Hypervolume Improvement" />
<meta name="citation_author" content="Iris Hupkens" />
<meta name="citation_author" content="Michael Emmerich" />
<meta name="citation_author" content="AndrÃ© Deutz" />
<meta name="citation_publication_date" content="2014/08/29" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Michael_Emmerich/publication/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement/links/5404956f0cf23d9765a67a6c.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/215868066921738/styles/pow/publicliterature/FigureList.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Faster Computation of Expected Hypervolume Improvement (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Faster Computation of Expected Hypervolume Improvement on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56aba20b301e0" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56aba20b301e0" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56aba20b301e0">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Faster%20Computation%20of%20Expected%20Hypervolume%20Improvement&rft.title=ArXiv%20e-prints&rft.jtitle=ArXiv%20e-prints&rft.date=2014&rft.au=Iris%20Hupkens%2CMichael%20Emmerich%2CAndr%C3%A9%20Deutz&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Faster Computation of Expected Hypervolume Improvement</h1> <meta itemprop="headline" content="Faster Computation of Expected Hypervolume Improvement">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement/links/5404956f0cf23d9765a67a6c/smallpreview.png">  <div id="rgw7_56aba20b301e0" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56aba20b301e0" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Iris_Hupkens" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Iris Hupkens" alt="Iris Hupkens" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Iris Hupkens</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw9_56aba20b301e0" data-account-key="Iris_Hupkens">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Iris_Hupkens"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Iris Hupkens" alt="Iris Hupkens" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Iris_Hupkens" class="display-name">Iris Hupkens</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Leiden_University" title="Leiden University">Leiden University</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56aba20b301e0" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Michael_Emmerich" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272193728806937%401441907450928_m/Michael_Emmerich.png" title="Michael Emmerich" alt="Michael Emmerich" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Michael Emmerich</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw11_56aba20b301e0" data-account-key="Michael_Emmerich">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Michael_Emmerich"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272193728806937%401441907450928_l/Michael_Emmerich.png" title="Michael Emmerich" alt="Michael Emmerich" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Michael_Emmerich" class="display-name">Michael Emmerich</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Leiden_University" title="Leiden University">Leiden University</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56aba20b301e0" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Andre_Deutz" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="AndrÃ© H. Deutz" alt="AndrÃ© H. Deutz" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">AndrÃ© H. Deutz</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw13_56aba20b301e0" data-account-key="Andre_Deutz">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Andre_Deutz"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="AndrÃ© H. Deutz" alt="AndrÃ© H. Deutz" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Andre_Deutz" class="display-name">AndrÃ© H. Deutz</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Leiden_University" title="Leiden University">Leiden University</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">     ArXiv e-prints   <meta itemprop="datePublished" content="2014-08">  08/2014;               <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1408.7114" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw14_56aba20b301e0" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>The expected improvement algorithm (or efficient global optimization) aims<br />
for global continuous optimization with a limited budget of black-box function<br />
evaluations. It is based on a statistical model of the function learned from<br />
previous evaluations and an infill criterion - the expected improvement - used<br />
to find a promising point for a new evaluation. The `expected improvement'<br />
infill criterion takes into account the mean and variance of a predictive<br />
multivariate Gaussian distribution.<br />
The expected improvement algorithm has recently been generalized to<br />
multiobjective optimization. In order to measure the improvement of a Pareto<br />
front quantitatively the gain in dominated (hyper-)volume is used. The<br />
computation of the expected hypervolume improvement (EHVI) is a<br />
multidimensional integration of a step-wise defined non-linear function related<br />
to the Gaussian probability density function over an intersection of boxes.<br />
This paper provides a new algorithm for the exact computation of the expected<br />
improvement to more than two objective functions. For the bicriteria case it<br />
has a time complexity in $O(n^2)$ with $n$ denoting the number of points in the<br />
current best Pareto front approximation. It improves previously known<br />
algorithms with time complexity $O(n^3 \log n)$. For tricriteria optimization<br />
we devise an algorithm with time complexity of $O(n^3)$. Besides discussing the<br />
new time complexity bounds the speed of the new algorithm is also tested<br />
empirically on test data. It is shown that further improvements in speed can be<br />
achieved by reusing data structures built up in previous iterations. The<br />
resulting numerical algorithms can be readily used in existing implementations<br />
of hypervolume-based expected improvement algorithms.</div> </p>  </div>   </div>     <div id="rgw15_56aba20b301e0" class="figure-carousel"> <div class="carousel-hd"> Figures in this publication </div> <div class="carousel-bd"> <ul class="clearfix">  <li> <a href="/figure/265209813_fig1_Figure-5-Time-needed-to-calculate-the-expected-hypervolume-improvement-in-2-D-averaged" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 5: Time needed to calculate the expected hypervolume improvement..." data-key="265209813_fig1_Figure-5-Time-needed-to-calculate-the-expected-hypervolume-improvement-in-2-D-averaged"> <img class="fig" src="https://www.researchgate.net/profile/Michael_Emmerich/publication/265209813/figure/fig1/Figure-5-Time-needed-to-calculate-the-expected-hypervolume-improvement-in-2-D-averaged_small.png" alt="Figure 5: Time needed to calculate the expected hypervolume improvement..." title="Figure 5: Time needed to calculate the expected hypervolume improvement..."/> </a> </li>  <li> <a href="/figure/265209813_fig2_Figure-9-Logarithmic-scale-graph-of-the-convergence-of-Monte-Carlo-integration-The" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 9: Logarithmic-scale graph of the convergence of Monte Carlo..." data-key="265209813_fig2_Figure-9-Logarithmic-scale-graph-of-the-convergence-of-Monte-Carlo-integration-The"> <img class="fig" src="https://www.researchgate.net/profile/Michael_Emmerich/publication/265209813/figure/fig2/Figure-9-Logarithmic-scale-graph-of-the-convergence-of-Monte-Carlo-integration-The_small.png" alt="Figure 9: Logarithmic-scale graph of the convergence of Monte Carlo..." title="Figure 9: Logarithmic-scale graph of the convergence of Monte Carlo..."/> </a> </li>  <li> <a href="/figure/265209813_fig3_Figure-10-Two-logarithmic-scale-graphs-showing-the-convergence-of-Monte-Carlo" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 10: Two logarithmic-scale graphs showing the convergence of..." data-key="265209813_fig3_Figure-10-Two-logarithmic-scale-graphs-showing-the-convergence-of-Monte-Carlo"> <img class="fig" src="https://www.researchgate.net/profile/Michael_Emmerich/publication/265209813/figure/fig3/Figure-10-Two-logarithmic-scale-graphs-showing-the-convergence-of-Monte-Carlo_small.png" alt="Figure 10: Two logarithmic-scale graphs showing the convergence of..." title="Figure 10: Two logarithmic-scale graphs showing the convergence of..."/> </a> </li>  <li> <a href="/figure/265209813_fig4_Figure-11-Time-needed-to-calculate-the-expected-hypervolume-improvement-for-a-Pareto" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 11: Time needed to calculate the expected hypervolume..." data-key="265209813_fig4_Figure-11-Time-needed-to-calculate-the-expected-hypervolume-improvement-for-a-Pareto"> <img class="fig" src="https://www.researchgate.net/profile/Michael_Emmerich/publication/265209813/figure/fig4/Figure-11-Time-needed-to-calculate-the-expected-hypervolume-improvement-for-a-Pareto_small.png" alt="Figure 11: Time needed to calculate the expected hypervolume..." title="Figure 11: Time needed to calculate the expected hypervolume..."/> </a> </li>  <li> <a href="/figure/265209813_fig5_Figure-12-The-figure-on-the-left-shows-the-number-of-Monte-Carlo-trials-that-can-be" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 12: The figure on the left shows the number of Monte Carlo..." data-key="265209813_fig5_Figure-12-The-figure-on-the-left-shows-the-number-of-Monte-Carlo-trials-that-can-be"> <img class="fig" src="https://www.researchgate.net/profile/Michael_Emmerich/publication/265209813/figure/fig5/Figure-12-The-figure-on-the-left-shows-the-number-of-Monte-Carlo-trials-that-can-be_small.png" alt="Figure 12: The figure on the left shows the number of Monte Carlo..." title="Figure 12: The figure on the left shows the number of Monte Carlo..."/> </a> </li>  <li> <a href="/figure/265209813_fig6_Figure-13-Graph-showing-the-time-needed-to-simultaneously-calculate-the-EHVI-on-a-number" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 13: Graph showing the time needed to simultaneously calculate..." data-key="265209813_fig6_Figure-13-Graph-showing-the-time-needed-to-simultaneously-calculate-the-EHVI-on-a-number"> <img class="fig" src="https://www.researchgate.net/profile/Michael_Emmerich/publication/265209813/figure/fig6/Figure-13-Graph-showing-the-time-needed-to-simultaneously-calculate-the-EHVI-on-a-number_small.png" alt="Figure 13: Graph showing the time needed to simultaneously calculate..." title="Figure 13: Graph showing the time needed to simultaneously calculate..."/> </a> </li>  </ul> </div> </div> <div class="action-container"> <div id="rgw16_56aba20b301e0" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw29_56aba20b301e0">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw30_56aba20b301e0">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Michael_Emmerich/publication/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement/links/5404956f0cf23d9765a67a6c.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Michael_Emmerich">Michael Emmerich</a>, <span class="js-publication-date"> Sep 01, 2014 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw32_56aba20b301e0" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw33_56aba20b301e0" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw34_56aba20b301e0" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw35_56aba20b301e0" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw36_56aba20b301e0" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw37_56aba20b301e0" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw31_56aba20b301e0" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMichael_Emmerich%2Fpublication%2F265209813_Faster_Computation_of_Expected_Hypervolume_Improvement%2Flinks%2F5404956f0cf23d9765a67a6c.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw28_56aba20b301e0"  itemprop="articleBody">  <p>Page 1</p> <p>Faster Computation of Expected Hypervolume Improvement<br />Iris HupkensMichael EmmerichAndrÂ´ e Deutz<br />Leiden Institute for Advanced Computer Science,<br />Leiden University, Niels Bohrweg 1, 2333-CA Leiden<br />Tel.: +31-71-527-7094<br />Fax: +31-71-527-7001<br />emmerich AT liacs.nl<br />Abstract<br />The expected improvement algorithm (or efficient global optimization) aims for global<br />continuous optimization with a limited budget of black-box function evaluations. It is based<br />on a statistical model of the function learned from previous evaluations and an infill cri-<br />terion - the expected improvement - used to find a promising point for a new evaluation.<br />The âexpected improvementâ infill criterion takes into account the mean and variance of a<br />predictive multivariate Gaussian distribution.<br />The expected improvement algorithm has recently been generalized to multiobjective<br />optimization. In order to measure the improvement of a Pareto front quantitatively the<br />gain in dominated (hyper-)volume is used. The computation of the expected hypervolume<br />improvement (EHVI) is a multidimensional integration of a step-wise defined non-linear<br />function related to the Gaussian probability density function over an intersection of boxes.<br />This paper provides a new algorithm for the exact computation of the expected improvement<br />to more than two objective functions. For the bicriteria case it has a time complexity in<br />O(n2) with n denoting the number of points in the current best Pareto front approximation.<br />It improves previously known algorithms with time complexity O(n3logn). For tricriteria<br />optimization we devise an algorithm with time complexity of O(n3). Besides discussing the<br />new time complexity bounds the speed of the new algorithm is also tested empirically on<br />test data. It is shown that further improvements in speed can be achieved by reusing data<br />structures built up in previous iterations. The resulting numerical algorithms can be readily<br />used in existing implementations of hypervolume-based expected improvement algorithms.<br />Keywords: Multiobjective Optimization, Expected Improvement, Efficient Global Op-<br />timization, Bayesian Global Optimization, Hypervolume Indicator<br />1</p>  <p>Page 2</p> <p>1 Introduction<br />In multiobjective optimization, the goal is to find (a set of) solutions which opti-<br />mize multiple objective functions at the same time [28]. As in case of conflicting<br />objectives there is no single best solution, it is common to compute instead all<br />points of the Pareto front (PF) or an approximation to this set.<br />Sometimes the function values of solutions can only be determined through<br />costly simulations, so approximation functions are used in their place. This makes<br />it possible to evaluate the function values of the most promising individuals only,<br />instead of wasting time evaluating the function values of individuals that are un-<br />likely to result in an improvement. A common approximation method are Gaussian<br />processes (or Kriging), which yield a prediction in the form of a 1-D normal dis-<br />tribution. This predictive distribution is learned from previously evaluated points<br />and quantifies for a given new point how likely it is that certain function values will<br />be obtained. In the context of computer experiments, the statistical assumptions<br />of such metamodels were discussed in Sacks et al. [11].<br />Optimization methods for expensive function values based on Gaussian pro-<br />cesses date back to the Lithuanian school of global optimization [12]. More re-<br />cently they have been refined and gained popularity in the context of optimization<br />with expensive computer experiments [13] under names such as efficient global op-<br />timization (EGO) and expected improvement algorithm [14]. Even more recently,<br />different expected improvement formulations for multiobjective optimization have<br />been developed and were compared in Wagner et al [7]. Among these, the ex-<br />pected hypervolume improvement (EHVI) turns out to have desirable theoretical<br />properties. The EHVI was first suggested in [26] and represents the expected im-<br />provement in the hypervolume measure relative to the current approximation of<br />the Pareto front [7] given the probability distribution of possible function values.<br />The hypervolume measure itself is a common measure used to determine the qual-<br />ity of a set of solutions to a multiobjective optimization problem [8] and can be<br />applied without a priori knowledge of the Pareto front, which makes the EHVI a<br />natural quality measure to use in multiobjective surrogate-assisted optimization.<br />The calculation of the EHVI has so far been a problem. Monte Carlo integra-<br />tion can solve the issue of computing the EHVI directly, but to get an accurate<br />approximation out of Monte Carlo integration is slow. An exact calculation ap-<br />proach exists for the bi-objective case, but it is slow as well (time complexity in<br />O(n3logn)). This thesis aims to increase the speed of the exact calculation of the<br />EHVI in two dimensions, as well as provide a method of calculating it in higher<br />dimensions. Its implementation will be validated with results from Monte Carlo<br />integration. The empirical performance of directly calculating the EHVI in the<br />three-dimensional case will also be analyzed in order to show the feasibility of us-<br />ing direct calculations in place of Monte Carlo integration. The main contribution<br />of this paper is therefore to make the EHVI computation both exact and fast, so<br />2</p>  <p>Page 3</p> <p>that it can be used in Gaussian-process assisted global optimization algorithms.<br />The article is structured as follows: Section 2 introduces important definitions<br />and technical preliminaries. Section 3 summarizes the related work. Section 4<br />contains a proof that the exact calculation of the bi-objective EHVI can be done<br />in O(n2) as well as a lower bound on the worst case complexity of Î©(nlogn).<br />The proof of the upper bound is by constructing an algorithm. The implemen-<br />tation of this algorithm is then empirically compared to the naive (O(n3logn))<br />implementation. Section 5 describes the details of the new, general algorithm for<br />calculating the expected hypervolume improvement in more than two dimensions,<br />and Section 6 describes an exact method for determining the tri-objective EHVI<br />with time complexity O(n3). Section 7 describes the results of empirical tests of<br />implementations of the algorithms described in Sections 5 and 6, both to validate<br />the correctness of the implementations and to measure their performance. Finally,<br />Section 8 contains concluding remarks and an outline of promising directions for<br />future research.<br />2Preliminaries<br />Without loss of generality, we will consider maximization of m â¥ 1 objective<br />functions f1 : X â R, ..., fm : X â R. A distinction is made between the<br />decision space X of alternative solutions and the objective space Rmwhere the<br />images of points in X under f are represented. However, the attention of this study<br />will be on points and probability distributions of points in the objective space.<br />The a posteriori approach of multiobjective optimization is concerned with<br />finding (approximations to) the Pareto front, that is: the set of solutions in the<br />objective space that are not dominated in the Pareto dominance relation [28].<br />The hypervolume indicator is a quality measure for Pareto approximation sets<br />[29]. Among the performance measures being used in Pareto optimization, it has<br />some favorable properties. First of all it can be used to compute both absolute and<br />relative improvements of a Pareto front approximation without a priori knowledge<br />of the Pareto front and it is compliant with Pareto dominance [30]. Furthermore,<br />its maximization yields a set of Pareto optimal points distributed across the Pareto<br />front [31, 32, 33].<br />The hypervolume indicator of a finite set of points P â Rmwith respect to a<br />user-defined reference point r is defined as the Lebesgue measure of the hypervol-<br />ume covered by the boxes that have an element of P as their upper corner and a<br />reference point r as their lower corner. Thereby it measures the size of the dom-<br />inated space of P cut from below by a reference point. The reference point must<br />be chosen in a such a way that it is dominated by all points in P, and ideally also<br />by all points of the Pareto front.<br />The set containing the part of the objective space that is dominated by the<br />points in P will be referred to as DomSet(P). The hypervolume contribution of a<br />3</p>  <p>Page 4</p> <p>point p â P is the difference in dominated hypervolume between P \ {p} and P.<br />The hypervolume contribution of a set of points S â P is defined analogously, as<br />the difference between P \ S and P.<br />The hypervolume improvement of a point p / â P with respect to P is defined as<br />the hypervolume contribution of p with regards to P âª {p}, i.e. the increment of<br />the hypervolume indicator after p is added to P.<br />Note that in the entire article we consider a fixed reference point that is dom-<br />inated by all points in the Pareto front approximations. The choice of reference<br />point is an important issue by itself, which we do however not adress here.<br />2.1The Expected Hypervolume Improvement<br />In global optimization with expensive function evaluations it is common to pre-<br />dict function values using statistical methods such as Gaussian processes [13].<br />Such methods provide a predictive distribution of possible outcomes of the precise<br />evaluation of the vector valued objective function in form of the parameters of a<br />probability density function (PDF) over all possible outcomes. In case of Gaus-<br />sian processes or Kriging approximations the predictions are given by multivariate<br />normal distributions[26].<br />The expected hypervolume improvement (EHVI) is the expected value of the<br />hypervolume improvement of a new candidate point in X, given its predictive dis-<br />tribution function (PDF) over points in the objective space. The general formula<br />for the EHVI with respect to a mutually non-dominated set P is[26]:<br />?<br />pâRm<br />HI(p,P) Â· PDF(p)dp(1)<br />The EHVI is a generalization of the classical expected improvement (EI) criterion<br />?â<br />For a given mean and standard deviation vector of an independently distributed<br />predictive distribution, the EHVI is monotonic with respect to the mean value [7]<br />and, at least for m â¤ 2, also w.r.t. the variance[9]. It has been used as an infill<br />criterion for multiobjective EGO in multiobjective optimization in various studies<br />[3, 4, 5] but its application so far has been confined to the bi-objective case and<br />the computation of the EHVI was criticized to be computationally expensive as<br />compared to more simple generalizations of the EI[7].<br />In [9], a formula is derived for exactly calculating the EHVI for m = 2 inde-<br />pendent and identically distributed normal PDFs. The expression in [9] in general<br />does not yield the exact result for m &gt; 2, as will be shown later.<br />Given these preliminaries the general problem discussed in this article can now<br />be defined concisely:<br />y=f?max{0,y â f?}PDF(y)dy used in model-assisted single objective optimiza-<br />tion, where f?denote the function value of the currently best solution[12, 13].<br />4</p>  <p>Page 5</p> <p>Problem 1 Given a finite set of points P â Rm, a reference point r and a pre-<br />dictive independent distributed multivariate normal PDF, given by its mean value<br />Âµ â Rmand standard deviations Ï â Rm, how can the EHVI be computed and how<br />can the EHVI be computed efficiently?<br />2.2One-Dimensional Expected Improvement and its Decomposition<br />In order to calculate the EHVI, we will need to calculate many integrals that have<br />the form of a partial one-dimensional improvement. In [9], a function was derived<br />that could be used for that purpose.<br />In the following definition we recall the notion of standard normal distribution<br />and normal distribution. Moreover we introduce a useful shorthand named Ï.<br />1. The function Ï(s) = 1/â2Ïeâ1<br />of the standard normal distribution and Î¦(s) =<br />mulative probability distribution function of the standard normal distribution.<br />The general normal distribution with mean Âµ and variance Ï has as density<br />the function ÏÂµ,Ï(s) =<br />Definition 1<br />2s2,s â R is the density function<br />1<br />2<br />1 + erf<br />??<br />s<br />â2<br />??<br />is the cu-<br />1<br />Ïâ2Ïeâ1<br />2(sâÂµ<br />Ï)2,s â R. The cumulative distribution<br />function of the general normal distribution is: Î¦Âµ,Ï(s) =1<br />2<br />?<br />1 + erf<br />?<br />sâÂµ<br />Ïâ2<br />??<br />.<br />2.<br />Ï(a,b,Âµ,Ï) := Ï Â· Ï(b â Âµ<br />Ï<br />) + (a â Âµ)Î¦(b â Âµ<br />Ï<br />)(2)<br />Remark: it is easy to check that ÏÂµ,Ï(s) =1<br />ÏÏ(sâÂµ<br />Ï) and Î¦Âµ,Ï(s) = Î¦(sâÂµ<br />Ï).<br />Integrals of the form<br />Î¦(bâÂµ<br />ââ can be written as the difference of two such integrals, allowing partial expected<br />improvements over an interval [l,u) â R, l â¥ f?to be calculated. Moreover one<br />can easily see that this difference can be neatly expressed in terms of Ï:<br />?â<br />z=b(z â a)1<br />ÏÏ(zâÂµ<br />Ï) are equal to ÏÏ(bâÂµ<br />Ï<br />+ (Âµ â a)[1 â<br />Ï)]. Integrals whose upper limit is less than â and lower limit greater than<br />u<br />?<br />?<br />z=l<br />(z â f?)1<br />ÏÏ(z â Âµ<br />Ï<br />)dz<br />=<br />â<br />z=l<br />(z â f?)1<br />ÏÏ(z â Âµ<br />Ï<br />)dz â<br />â<br />?<br />z=u<br />(z â f?)1<br />ÏÏ(z â Âµ<br />Ï<br />)dz<br />= Ï(f?,l,Âµ,Ï) â Ï(f?,u,Âµ,Ï)<br />The value f?in this case is the currently best function value.<br />5</p>  <p>Page 6</p> <p>In the rest of this thesis we will use the abbreviations Ïx(s) := ÏÂµx,Ïx(s)(=<br />ÏxÏ(sâÂµx<br />variance of the normal distribution associated to the point x in the search space.<br />Analogously, we use abbreviations Ïy,Î¦yand Ïz,Î¦zfor the y and the z coordinate.<br />1<br />Ïx)) and Î¦x(s) := Î¦(s)(= Î¦(sâÂµx<br />Ïx)), where Âµx and Ïx are the mean and<br />3Related Work<br />The use of the one-dimensional expected improvement to solve engineering prob-<br />lems with expensive-to-evaluate objective functions was initially proposed by Mockus<br />et al.[12] and then later reintroduced by Jones et al. in [13]. Since then it has<br />been widely used in global optimization with expensive-to-evaluate functions. It<br />has been shown to converge to the global optimum for the single objective case<br />and a subclass of continuous functions [14].<br />Generalizing the one-dimensional expected improvement algorithm to multiob-<br />jective optimization is still a very new area of research. Besides the aforementioned<br />EHVI, first published in [26], various other solutions have been proposed:<br />â¢ Chebyshev scalarization with dynamically changing weights [15].<br />â¢ Scalarization by using the distance from the centroid of the probability dis-<br />tribution to the Pareto approximation set [16].<br />â¢ The hypervolume improvement for candidate points, calculated based on the<br />upper confidence bound of the meta-model prediction [17].<br />Moreover, in Kumano et al. [27] it was proposed to use a vector of expected<br />improvements for the single objective functions, instead of a scalar measure.<br />Expected improvement has been studied as an infill criterion in different appli-<br />cation fields, such as bioinformatics [15], mechanical engineering [17] and aerospace<br />design [16]. Also the EHVI was already applied in practice for the tuning of con-<br />trollers in sewage treatment plants [3], in mechanical engineering [4] and quantum<br />control [2]. As compared to other indicators EHVI was found to have monotonicity<br />in mean values [7] and variance[9] and yielded high accuracy optima approxima-<br />tions. However, its computation is so far limited to the biobjective case and the<br />time complexity of existing exact algorithms is still very high (O(n3logn), see[9].<br />Recently, Couckuyt et al. [6] published an algorithm that is faster based on<br />empirical tests. The complexity of this algorithm is not reported. It follows a<br />heuristic block partitioning scheme for computing the improvement contribution<br />of each cell and we conjecture its total complexity to be in Î©(n3) for two and in<br />Î©(n4) in three objectives.<br />6</p>  <p>Page 7</p> <p>4Calculating the 2-D Expected Hypervolume Improve-<br />ment<br />Firstly, an efficient exact algorithm for the computation of the EHVI in two di-<br />mensions will be discussed.<br />Let P denote a set of n mutually non-dominated points in the two-dimensional<br />plane. P is the currently best Pareto front approximation. Furthermore, let r â R2<br />denote a reference point which is dominated by every point in P. The aim is to<br />calculate the expected hypervolume improvement for a point p in the decision<br />space for which we have the mean (Âµx,Âµy) and standard deviation (Ïx,Ïy) of a<br />predictive distribution.<br />In the two-dimensional case, calculating the EHVI for p exactly can be done<br />by piecewise integration over a set of half-open rectangular interval boxes (cells)<br />formed by the horizontal and vertical lines going through the points in P and<br />through r. The final EHVI is the sum of the contributions calculated for all grid<br />cells. See Figure 1 for a visualization of the grid.<br />(0,0)<br />(1,0)<br />(2,0)<br />(3,0)<br />(0,1)<br />(1,1)<br />(2,1)<br />(3,1)<br />(0,2)<br />(1,2)(2,2)(3,2)<br />(0,3)(1,3)<br />(2,3)<br />(3,3)<br />(â,â)<br />r<br />x<br />y<br />Figure 1: An example of the interval boxes for a small population P. Checkered boxes fall in<br />the dominated hypervolume of P. Therefore their contribution to the integral will be 0, and no<br />calculation will be necessary for these boxes.<br />Individual grid cells will be denoted by C(a,b), where 0 â¤ a â¤ n and 0 â¤ b â¤ n.<br />Let Q = Pâª{(â,ry)}âª{(rx,â)}, with Qxdenoting Q sorted in order of ascending<br />x coordinate, and Qydenoting Q sorted in order of ascending y coordinate. Let<br />C be the set of grid cells representing the interval boxes. The numbers a and<br />b represent positions in the sorting order of Q, starting with 0. Then, a is the<br />position of elements of Qxand b is the position of elements of Qy. The lower left<br />corner of a cell will have the coordinates (Qx<br />the grid cell will have the coordinates (Qx<br />Note, that due to the characteristics of mutually non-dominated points in the<br />two-dimensional plane, it is not necessary to sort Q twice in order to determine<br />a.x,Qy<br />a+1.x,Qy<br />b.y). The upper right corner of<br />b+1.y).<br />7</p>  <p>Page 8</p> <p>Qxand Qy. Sorting P in order of ascending x coordinate is equivalent to sorting<br />it in order of descending y coordinate. It follows that Qx<br />When dividing the grid in the way described above, (n + 1)2interval boxes<br />are formed. However, if the upper right corner of an interval box is dominated<br />by or equal to some point in P, its contribution will be zero, and no calculation<br />will need to be done for that interval box. Note that if the upper right corner is<br />not dominated by P then the lower left corner is neither. These interval boxes<br />are represented by a grid cell C(a,b) which is within the dominated hypervolume<br />of P. The remaining cells, Cstairs, are formed by cells for which this is not the<br />case, meaning that â(C(a,b) â Cstairs,p â P) : p.x &gt; Qx<br />analogously, p.y &gt; Qy<br />Due to the definition of Q, we know that for p â P it holds that p = Qx<br />Qy<br />dominates C(a,b). From this we get the following equivalence: a â¥ nâb â C(a,b)<br />is dominated by some point p â P. Thus Cstairsconsists of all cells satisfying<br />a â¥ n â b. There are<br />on the complexity of any algorithm which iterates over these interval boxes.<br />If we call the lower corner of the cell l and the upper corner u, the contribution<br />of a grid cell to the integral is defined as follows:<br />k= Qy<br />n+1âk.<br />a.x â Qy<br />b.y â¥ p.y and,<br />b.y â Qx<br />a.x â¥ p.x.<br />k=<br />n+1âkfor some 0 &lt; k â¤ n. Furthermore k &gt; a and n+1âk &gt; b, if and only if p<br />(n+1)(n+2)<br />2<br />of such cells, resulting in a lower bound of O(n2)<br />uy<br />?<br />py=ly<br />ux<br />?<br />px=lx<br />HI(p)Ïx(px)Ïy(py)dpxdpy<br />C(a,b)<br />(â,â)<br />r<br />x<br />y<br />p<br />Sminus<br />Figure 2: Within an integration region C(a,b), the hypervolume improvement of candidate<br />points p is equal to (p.xâQy<br />represents (p.x â Qy<br />rectangle.<br />b+1.x)Â·(p.yâQx<br />b+1.x) Â· (p.y â Qx<br />a+1.y)âSminus. In this example, the yellow rectangle<br />a+1.y), and S consists of the two points within the yellow<br />Dominated cells have a contribution of 0 to the integral, and for cells which<br />are non-dominated, HI(p) can be calculated as a rectangular volume from which<br />8</p>  <p>Page 9</p> <p>a correction term is subtracted. See Figure 2 for a visual representation. The<br />integral for these cells can be calculated as follows, as was described in more detail<br />in [9]:<br />uy<br />?<br />?<br />py=ly<br />ux<br />?<br />?<br />px=lx<br />(pxâ rx)(pyâ ry) â SminusÏx(px)Ïy(py)dpxdpy<br />=<br />uy<br />py=ly<br />ux<br />px=lx<br />uy<br />?<br />py=ly<br />(pxâ vx)(pyâ vy)Ïx(px)Ïy(py)dpxdpy<br />â<br />ux<br />?<br />px=lx<br />SminusÏx(px)Ïy(py)dpxdpy<br />= (Ï(vx,lx,Âµx,Ïx) â Ï(vx,ux,Âµx,Ïx)) Â· (Ï(vy,ly,Âµy,Ïy) â Ï(vy,uy,Âµy,Ïy))<br />â SminusÂ· (Î¦x(ux) â Î¦x(lx)) Â· (Î¦y(uy) â Î¦y(ly))<br />The last step is motivated by Subsection 2.2 and the application of Fubiniâs<br />Theorem [23]. It can be seen that the formula is of the form c1âSminusÂ·c2, where<br />c1and c2are calculations which are performed in constant time with respect to n<br />for a single cell.<br />The correction term Sminusis equal to the hypervolume contribution of S â P,<br />where S consists of those points dominated by or equal to the lower corner of<br />the cell. Calculating the dominated hypervolume of a set in the two-dimensional<br />plane has a time complexity of O(nlogn). This complexity results from needing<br />to find the neighbors of each point in order to calculate its contribution to the<br />hypervolume. Sorting the set has a time complexity of O(nlogn), after which the<br />dominated hypervolume calculation itself is done in O(n) by iterating over each<br />point and performing an O(1) calculation using the points that come before and<br />after it in the sorting order. When calculating Sminus, the points for which the<br />dominated hypervolume is to be calculated come from P, which was already sorted.<br />This brings the complexity of this step down to O(n), but it can be brought down<br />to O(1) when the order of calculations is chosen carefully, giving the algorithm a<br />total complexity of O(n2).<br />The points of P dominated by or equal to the lower corner of C(a,b), which<br />define S, are those points satisfying the following inequalities:<br />p â P,Qx<br />a.x â¥ p.x,Qy<br />b.y â¥ p.y<br />Because of the sorting order and definition of Qxand Qy, S can be described<br />equivalently as follows. The set S is empty, if a = n â b (the lowest value of a for<br />9</p>  <p>Page 10</p> <p>(â,â)<br />r<br />x<br />y<br />(3,3)<br />(2,3)<br />(1,3)<br />(0,3)<br />(3,2)<br />(2,2)<br />(1,2)<br />(3,1)<br />(2,1)<br />(3,0)<br />S = {}<br />S = {}<br />S = {3}<br />3<br />2<br />1<br />S = {}<br />S = {2}<br />S = {2,3}<br />S = {}<br />S = {1}S = {1,2}<br />S = {1,2,3}<br />Figure 3: An example showing the order of iterations which allows the hypervolume contribution<br />of S to be updated in constant time.<br />which a â¥ n â b), otherwise (a &gt; n â b) S is formed by an uninterrupted range<br />with Qx<br />A row in Cstairsis a set of cells Cstairs(a,b) where b is the same. In a single row,<br />S will always be either empty or have Qx<br />one point to the range of points in P which falls between Qx<br />makes it possible to iterate over all cells in Cstairswhile adding no more than one<br />point to S per iteration. We will do this as follows:<br />We will start iterating over each row of Cstairsat its first cell, where a = nâb.<br />In this cell, S = â and Sminus= 0. For each iteration within a row after the first<br />one, we add 1 to a and add the point Qx<br />which shows the order of operations and the contents of S during each step.<br />Although the above description refers to âadding points to Sâ, we only need to<br />keep track of the first and last points of S in between algorithm iterations. When<br />a new point is added to S, Sminusincreases by the area covered by the rectangle<br />from (Qx<br />addition of a point to S, only the left neighbor of the first element of S, the last<br />element of S and the right neighbor of the last element of S are needed. Figure 4<br />shows an example of this process. This can be done in constant time in any data<br />structure which allows the neighbors of a point to be looked up in constant time:<br />whenever a is incremented, Qx<br />b is incremented, the new Qx<br />then start iterating through values of a at the beginning of the row, Qx<br />the new Qx<br />first cell in a row of Cstairs.<br />We have shown that the upper bound on the complexity of determining the<br />expected hypervolume improvement is O(n2). We can also show that the worst-<br />case complexity can be no better than O(nlogn). If the standard deviation of<br />(n+1âb)as its first element and Qx<br />aas its last element.<br />aas its last element. Adding 1 to a adds<br />(n+1âb)and Qx<br />a. This<br />ato S. For an example, refer to Figure 3,<br />(nâb).x,Qx<br />a+1.y) to (Qx<br />a.x,Qx<br />a.y). Therefore, to update Sminus after the<br />abecomes Qx<br />(nâb)becomes its left neighbor, Qx<br />a+1and Qx<br />a+1becomes Qx<br />a+2. Whenever<br />(nâ1âb), and as we will<br />abecomes<br />(nâb)in the<br />(nâb)as well because we have established earlier that Qx<br />a= Qx<br />10</p>  <p>Page 11</p> <p>a candidate pointâs predictive distribution is 0 and the mean value vector is a<br />point which dominates all points in P, then the problem of calculating its EHVI<br />reduces to calculating the hypervolume that will be dominated by pcandidateminus<br />the hypervolume dominated by P. If it was possible to solve this calculation with<br />lower complexity than O(nlogn), then it would also be possible to reduce the<br />calculation of Pâs hypervolume to the problem of calculating the EHVI of a point<br />that dominates P, and it has already been proven in [10] that the complexity of<br />calculating the hypervolume of a set of points in the 2-D plane is in Î(nlogn).<br />11</p>  <p>Page 12</p> <p>(0,0)<br />(1,0)<br />(2,0)<br />(3,0)<br />(0,1)<br />(1,1)<br />(2,1)<br />(3,1)<br />(0,2)<br />(1,2)<br />(2,2)<br />(3,2)<br />(0,3)<br />(1,3)<br />(2,3)<br />(3,3)<br />(â,â)<br />r<br />x<br />y<br />(4,0)<br />(4,1)<br />(4,2)<br />(4,3)<br />(0,4)<br />(1,4)<br />(2,4)<br />(3,4)<br />(4,4)<br />(0,0)<br />(1,0)<br />(2,0)<br />(3,0)<br />(0,1)<br />(1,1)<br />(2,1)<br />(3,1)<br />(0,2)<br />(1,2)<br />(2,2)<br />(3,2)<br />(0,3)<br />(1,3)<br />(2,3)<br />(3,3)<br />(â,â)<br />r<br />y<br />x<br />y<br />(4,0)<br />(4,1)<br />(4,2)<br />(4,3)<br />(0,4)<br />(1,4)<br />(2,4)<br />(3,4)<br />(4,4)<br />(0,0)<br />(1,0)<br />(2,0)<br />(3,0)<br />(0,1)<br />(1,1)<br />(2,1)<br />(3,1)<br />(0,2)<br />(1,2)<br />(2,2)<br />(3,2)<br />(0,3)<br />(1,3)<br />(2,3)<br />(3,3)<br />(â,â)<br />r<br />x<br />y<br />(4,0)<br />(4,1)<br />(4,2)<br />(4,3)<br />(0,4)<br />(1,4)<br />(2,4)<br />(3,4)<br />(4,4)<br />(0,0)<br />(1,0)<br />(2,0)<br />(3,0)<br />(0,1)<br />(1,1)<br />(2,1)<br />(3,1)<br />(0,2)<br />(1,2)<br />(2,2)<br />(3,2)<br />(0,3)<br />(1,3)<br />(2,3)<br />(3,3)<br />(â,â)<br />r<br />x<br />(4,0)<br />(4,1)<br />(4,2)<br />(4,3)<br />(0,4)<br />(1,4)<br />(2,4)<br />(3,4)<br />(4,4)<br />Figure 4: An example showing how Sminuschanges during each iteration within a single row.<br />The rectangular strip which is added after each iteration can be calculated with knowledge of<br />three points: the point Qx<br />lower corner, and the point Qx<br />does not change, the hypervolume covered by the older points in S stays the same and does not<br />have to be re-calculated.<br />ais its upper corner, the point Qx<br />(nâb)provides the x coordinate of its lower corner. Because Qx<br />a+1provides the y coordinate of its<br />(nâb)<br />12</p>  <p>Page 13</p> <p>4.1Empirical Performance<br />As an additional verification of the correctness of the algorithm presented above,<br />two implementations were written in C++. The first used the constant-time up-<br />date scheme, and the second did not: instead of using the constant-time update<br />scheme, Sminuswas calculated by first finding the set of points S by checking each<br />point in P to see if it was dominated, and then calling a separate function on S<br />to calculate the hypervolume of this set of points.<br />The expected hypervolume improvement calculated using these implementa-<br />tions was identical for all test problems, but their speed was not. See Figure 5 for<br />the empirical performance on a simple test where P consisted of n different points<br />on a diagonal Pareto front. From this, it appears that using the constant-time<br />update scheme becomes worthwhile for n &gt; 20, though results might vary slightly<br />depending on implementation and system details.<br />Figure 5: Time needed to calculate the expected hypervolume improvement in 2-D, averaged<br />over 10 runs. The times reported were measured on an Intel i7 quadcore CPU with 2.1 GHz<br />clockspeed, and the code was compiled using GNU under Windows with the optimization level<br />set to O3.<br />13</p>  <p>Page 14</p> <p>5Calculation of the Higher-Dimensional Expected Hyper-<br />volume Improvement<br />The algorithm given in [9] for exactly calculating the expected hypervolume im-<br />provement is not correct when the dimensions is higher than 2. This is because the<br />shape of the hypervolume improvement becomes more complex when the number<br />of dimensions increases. We will therefore derive a new formula by first decom-<br />posing the calculation into parts with less complex shapes, and then simplifying<br />the resulting formula for the sake of more convenient calculation.<br />5.1Decomposition into Cells<br />In higher dimensions, the search space can be divided into cells the same way it<br />is done in two dimensions, except instead of the boundaries being given by lines<br />going through the points in P and the reference point r, now the cells are separated<br />from each other by (m â 1)-dimensional hyperplanes (where m is the number of<br />objective functions).<br />Each cell is denoted by C(a1,a2,...,am) where a1 through am are integers<br />from 0 to |P| denoting the labeling of the cell.<br />and right upper corner u of the cell with label a1, ..., amare defined as follows:<br />Let P?= {r} âª P âª (â,...,â)Tand let sd[0],...,sd[|P| + 1] denote the d-th<br />components of the vectors in P?sorted in ascending order. Then ld= sd[ad] and<br />ud= sd[ad+ 1] for d = 1,...,m. In other words, corners of this cell complex are<br />given as the intersection points of all axis-parallel m â 1 dimensional hyperplanes<br />through points in P?.<br />The hypervolume improvement of a new point p with respect to the current<br />Pareto front approximation P is given by the function HyperVolume(A \ DomSet(P)),<br />where A is the dominated hypervolume covered by p. This is the same as calcu-<br />lating HyperVolume(A)âHyperVolume(DomSet(P) â© A). We will denote the set<br />of dimensions by D = {1,2,...,m}. We can decompose the calculation of the<br />hypervolume improvement of a point p â C(a1,a2,...,am) as follows:<br />?<br />IC:= HyperVolume(AC) â HyperVolume(DomSet(P) â© AC)<br />Then the left lower corner l<br />HI(p) =<br />CâD<br />IC, where<br />14</p>  <p>Page 15</p> <p>A1,2,3<br />A1,2<br />A2<br />A1<br />A3<br />A1,3<br />A2,3<br />f3<br />f2<br />f1<br />Figure 6: An example showing how the quantities AC for C â {1,2,3} are defined in a three-<br />dimensional objective space. Aâis hidden within the rectangular volume. The checkered volumes<br />represent the volume dominated by the points in the Pareto front approximation.<br />and ACare given by:<br />AC:=<br />ï£®<br />ï£°<br />ld<br />rd<br />ï£¯ï£¯<br />ï£«<br />ï£¬<br />ï£¬<br />ï£­<br />v1<br />v2<br />...<br />vm<br />ï£¶<br />ï£·<br />ï£·<br />ï£¸,<br />ï£«<br />ï£¬<br />ï£¬<br />ï£­<br />w1<br />w2<br />...<br />wm<br />ï£¶<br />ï£·<br />ï£·<br />ï£¸<br />ï£¹<br />ï£»<br />ï£ºï£º<br />vd=<br />?<br />?<br />if d â C<br />if d / â C<br />if d â C<br />if d / â C<br />wd=<br />pd<br />ld<br />See Figure 6 for an example in 3 dimensions.<br />In the above formula Hypervolume denotes the Lebesgue measure of Rm. Note<br />that it can happen that the dimension of AC is strictly less than m. In this<br />case Hypervolume(AC) = 0. We can make a similar remark about Hypervolume(<br />domSet(P) â© AC).<br />The values of rd and ld are constant for all points that fall within a given<br />interval box (cell): r is the reference point and is, of course, always constant,<br />while l represents the position of the lower corner of the cell. From this, it follows<br />that ICrepresents the portion of the hypervolume improvement which is constant<br />with regards to the values of pd,d / â C, and which is variable with regards to the<br />values of pd,d â C. In fact, it is linearly related to these values. This is a direct<br />consequence of the way the cell boundaries are defined:<br />Let SecCbe the cross-section of DomSet(P) â© ACwhich goes through p. This<br />cross-section is defined by a projection to the dimensions not in C (if C consists<br />15</p>  <p>Page 16</p> <p>of k dimensions, the slice will be (mâk)-dimensional as a result). The projection<br />of DomSet(P) uses only those points in P for which the function values in the<br />dimensions given by C are larger than the corresponding function values of p. We<br />shall call this selection P?. No points in P can fall between cell boundaries in any<br />dimension, so the composition of P?must be the same for all points within a cell.<br />The projection of ACto the dimensions not in C is constant for all points within<br />a cell as well, because the coordinates defining AC are independent of p in all<br />dimensions not in C. HyperVolume(SecC) is constant as a result â note that here<br />Hypervolume is the Lebesque measure of Rmâk. Because ACdoes not span across<br />cell boundaries in the dimensions in C, HyperVolume(DomSet(P) â© AC) is equal<br />to the hypervolume of SecCmultiplied by the length of ACin each dimension in<br />C, and those lengths are given by (pdâ ld) with d â C.<br />There is one quantity ICfor which C = D. This quantity IDis special because<br />it is linearly related to all values of p. ID falls entirely within the cell, and as<br />such, instead of projecting P onto a zero-dimensional space, it can simply be<br />said that HyperVolume(DomSet(P) â© AD) = HyperVolume(AD) if the cell is not<br />dominated, and HyperVolume(ADâ© DomSet(P)) = 0 if it is. Therefore, ID =<br />HyperVolume(AD) for non-dominated cells.<br />By decomposing the calculation of the hypervolume improvement, we can use<br />the sum rule to decompose the calculation of a cellâs contribution to the EHVI as<br />well.<br />?<br />p=l<br />u<br />?<br />CâD<br />ICÂ· PDF(p)dp =<br />?<br />CâD<br />u<br />?<br />p=l<br />ICÂ· PDF(p)dp<br />IC is calculated as the product of a constant and a set of values which are<br />linearly related to exactly one coordinate of p, therefore we can first factor out<br />the calculation of the constant part. The PDF consists of independent normal<br />distributions, allowing the probability distributions for dimensions not in C (in<br />which ICis constant) to be factured out as well. An integral consisting solely of<br />a normal distribution can be exactly calculated using the cumulative probability<br />distribution function Î¦ to calculate the probability that a point is within range of<br />the cell.<br />u<br />?<br />p=l<br />ICÂ· PDF(p)dp = Iconst<br />C<br />Â·<br />uC<br />?<br />pC=lC<br />?<br />câC<br />(pcâ lc) Â·<br />?<br />câC<br />ÏcdpCÂ·<br />?<br />c/ âC<br />(Î¦c(uc) â Î¦c(lc))<br />The integral that remains is a box-shaped expected improvement where each<br />dimension is independent. Fubiniâs theorem [23] states that iterated integration,<br />performed in any order, can be used to calculate a multiple integral under the<br />condition that the multiple integral is absolutely convergent. The partial integrals<br />16</p>  <p>Page 17</p> <p>making up the cellâs contribution to the EHVI all converge to finite numbers,<br />so we can safely use iterated integration. The result is a product of expected<br />improvements, which are captured in the Ï function described earlier:<br />u<br />?<br />p=l<br />ICÂ· PDF(p)dp =(3)<br />Iconst<br />C<br />Â·<br />?<br />câC<br />(Ï(lc,lc,Âµc,Ïc) â Ï(lc,uc,Âµc,Ïc)) Â·<br />?<br />c/ âC<br />(Î¦c(uc) â Î¦c(lc)) (4)<br />It is already possible to calculate the contribution of a cell to the EHVI by sum-<br />ming all these terms, but the calculation can be made a bit more efficient when<br />instead of decomposing the calculation of the hypervolume, we instead only decom-<br />pose the calculation of the dominated hypervolume. In Section 5.2, that possibility<br />will be examined in more detail by looking at the 3-D case as an example.<br />5.2Calculation of the 3-D Expected Hypervolume Improvement<br />Consider that, in the 2-D case, we are able to calculate the hypervolume by in-<br />tegrating over a box bounded by the dominated hypervolume and subtracting a<br />correction term Sminus. In higher dimensions, the correction term is not a constant,<br />but we can still make use of a modified version of this technique. The hypervolume<br />improvement HI(p) is decomposed as follows, in three dimensions:<br />HI(p) = Iâ+ Ix+ Iy+ Iz+ Ixy+ Ixz+ Iyz+ Ixyz<br />ï£«<br />Together, they form the volume of<br />ï£­<br />ï£®<br />ï£°<br />ï£«<br />ï£­<br />rx<br />ry<br />rz<br />ï£¶<br />ï£¸,<br />ï£«<br />ï£­<br />px<br />py<br />pz<br />ï£¶<br />ï£¸<br />ï£¹<br />ï£»\ DomSet(P)<br />ï£¶<br />ï£¸. Instead<br />of writing HI(p) as a sum of hypervolume improvements, we can also write it as<br />a single rectangular volume from which a dominated hypervolume is subtracted:<br />HI(p) = Vol<br />ï£«<br />ï£­<br />ï£®<br />ï£°<br />ï£«<br />ï£­<br />rx<br />ry<br />rz<br />ï£¶<br />ï£¸,<br />ï£«<br />ï£­<br />px<br />py<br />pz<br />ï£¶<br />ï£¸<br />ï£¹<br />ï£»<br />ï£¶<br />ï£¸â Vol<br />ï£«<br />ï£­DomSet(P) â©<br />ï£®<br />ï£°<br />ï£«<br />ï£­<br />rx<br />ry<br />rz<br />ï£¶<br />ï£¸,<br />ï£«<br />ï£­<br />px<br />py<br />pz<br />ï£¶<br />ï£¸<br />ï£¹<br />ï£»<br />ï£¶<br />ï£¸<br />We can then decompose the calculation of the dominated hypervolume instead<br />of the calculation of the hypervolume improvement. In the following decomposition<br />of the total subtracted dominated hypervolume Sâ, each part Sâ<br />subtracted dominated hypervolume needed to calculate IC. When p is within the<br />integration cell bounded from below by l, we get the following:<br />Cis equal to the<br />17</p>  <p>Page 18</p> <p>Sâ= Sâ<br />â+ Sâ<br />ï£«<br />x+ Sâ<br />y+ Sâ<br />z+ Sâ<br />ï£®<br />xy+ Sâ<br />rx<br />ry<br />rz<br />xz+ Sâ<br />ï£«<br />lz<br />yz<br />ï£¶<br />= Vol<br />ï£­DomSet(P) â©<br />+ (pxâ lx) Â· Area<br />+ ...<br />+ (pxâ lx) Â· (pyâ ly) Â·?Max(rz,Ïz(Ïx&gt;lx,y&gt;ly(P))) â rz<br />+ ...<br />ï£°<br />ï£«<br />ï£­<br />ï£¶<br />ï£¸,<br />ï£­<br />lx<br />ly<br />ï£¶<br />ï£¸<br />ï£¹<br />ï£»ï£¸<br />?<br />DomSet(Ïyz(Ïx&gt;lx(P))) â©<br />??ry<br />rz<br />?<br />,<br />?py<br />pz<br />???<br />?<br />By abuse of language we use (Max(rz,Ïz(Ïx&gt;lx,y&gt;ly(P))) instead of the following<br />correct notation: Max({rz} âª Ïz(Ïx&gt;lx,y&gt;ly(P)). Similar notations are also used in<br />the sequel.<br />The first thing to note is that if rzâ¥ Max(rz,Ïz(Ïx&gt;lx,y&gt;ly(P))), Sâ<br />analogous cases are true for Sâ<br />if r = v, all three quantities are 0:<br />ï£®<br />Max(rz,Ïz(Ïx&gt;lx,y&gt;ly(P)))<br />xy= 0. The<br />xzand Sâ<br />yz, allowing us to define a point v for which,<br />v =<br />ï£°<br />ï£«<br />ï£­<br />Max(rx,Ïx(Ïy&gt;ly,z&gt;lz(P)))<br />Max(ry,Ïy(Ïx&gt;lx,z&gt;lz(P)))<br />ï£¶<br />ï£¸<br />ï£¹<br />ï£»<br />The bounding box bounded by v from below and p from above contains the<br />entire volume of HI(p). This allows us to use v in place of r and rewrite our<br />initial equation in a way that reduces the number of components from 8 to 5:<br />HI(p) = Vol<br />ï£«<br />ï£«<br />ï£­<br />ï£­DomSet(P) â©<br />â (pxâ lx) Â· Area<br />ï£®<br />ï£°<br />ï£«<br />ï£­<br />vx<br />vy<br />vz<br />ï£¶<br />ï£¸,<br />ï£«<br />ï£­<br />px<br />py<br />pz<br />ï£¶<br />ï£®<br />ï£¸<br />ï£°<br />ï£¹<br />ï£«<br />ï£»<br />ï£­<br />ï£¶<br />ï£¸<br />vx<br />vy<br />vz<br />â Vol<br />ï£¶<br />ï£¸,<br />ï£«<br />ï£­<br />lx<br />ly<br />lz<br />ï£¶<br />ï£¸<br />ï£¹<br />ï£»<br />ï£¶<br />ï£¸<br />?<br />?<br />?<br />DomSet(Ïyz(Ïx&gt;lx(P))) â©<br />DomSet?Ïxz<br />DomSet(Ïxy(Ïz&gt;lz(P))) â©<br />??vy<br />??vx<br />??vx<br />vz<br />?<br />?<br />?<br />,<br />?ly<br />,<br />?lx<br />lz<br />???<br />???<br />???<br />â (pyâ ly) Â· Area<br />?Ïy&gt;ly(P)??â©<br />vz<br />?lx<br />lz<br />â (pzâ lz) Â· Area<br />vy<br />,<br />ly<br />18</p>  <p>Page 19</p> <p>The component of the EHVI integral corresponding to Vol<br />ï£«<br />ï£­<br />ï£®<br />ï£°<br />ï£«<br />ï£­<br />vx<br />vy<br />vz<br />ï£¶<br />ï£¸,<br />ï£«<br />ï£­<br />px<br />py<br />pz<br />ï£¶<br />ï£¸<br />ï£¹<br />ï£»<br />ï£¶<br />ï£¸<br />is the only component in this equation which is variable in more than one dimen-<br />sion, but since it is a rectangular volume, it is simply a product of one-dimensional<br />improvements:<br />?<br />Sâ<br />use v as a reference point. Even without examining the corresponding integral it<br />is clear that it only needs to be multiplied with the probability that a given point<br />is within the cell. The formula for calculating this correction term is:<br />?<br />Sâ<br />one coordinate of p. We will look at Sâ<br />?<br />This has to be multiplied by (pxâ lx). The expected value of Sâ<br />equal to a constant multiplied by the partial expected improvement of pxover the<br />interval [lx,ux). This is given by:<br />câ{x,y,z}<br />(Ï(vc,lc,Âµc,Ïc) â Ï(vc,uc,Âµc,Ïc))<br />âis a constant. We will keep using the notation Sâââ, although now we will<br />Sâ<br />âÂ·<br />câ{x,y,z}<br />(Î¦c(uc) â Î¦c(lc))<br />x, Sâ<br />y, and Sâ<br />zare not constants, but they are each linearly related to only<br />xas an example:<br />The constant part of Sâ<br />DomSet(Ïyz(Ïx&gt;lx(P))) â©<br />xis Area<br />??vy<br />vz<br />xis therefore<br />?<br />,<br />?ly<br />lz<br />???<br />.<br />ux<br />?<br />px=lx<br />(pxâ lx)Ïx(px)dpx= Ï(lx,lx,Âµx,Ïx) â Ï(lx,ux,Âµx,Ïx)<br />Using a new call to Ï to calculate this term is not necessary. We can use the<br />fact that Ï represents the function of a one-dimensional expected improvement<br />over a certain range bounded from below. The partial expected improvement for<br />the region below the lower cell bound l is a constant term multiplied by the chance<br />of being within the cellâs range, which is captured in the equation below:<br />Ï(vc,lc,Âµc,Ïc) â Ï(vc,uc,Âµc,Ïc) = Ï(lc,lc,Âµc,Ïc) â Ï(lc,uc,Âµc,Ïc)<br />+ (Î¦c(uc) â Î¦c(lc)) Â· (lcâ vc)<br />Both (Î¦c(uc) â Î¦c(lc)) Â· (lcâ vc) and Ï(vc,lc,Âµc,Ïc) â Ï(vc,uc,Âµc,Ïc) were cal-<br />culated earlier, so we can reuse them to easily find Ï(lc,lc,Âµc,Ïc)âÏ(lc,uc,Âµc,Ïc).<br />This means that the formula for calculating the partial expected hypervolume<br />improvement of a cell will look like the following if the cell is not dominated:<br />19</p>  <p>Page 20</p> <p>Let<br />and<br />âÏc:= Ï(vc,lc,Âµc,Ïc) â Ï(vc,uc,Âµc,Ïc), c â {x,y,z}<br />âÎ¦c:= Î¦c(uc) â Î¦c(lc), c â {x,y,z}<br />â V ol<br />ï£«<br />ï£­DomSet(P) â©<br />â (âÏzâ âÎ¦zÂ· (lzâ vz))<br />Â· Area<br />ï£®<br />ï£°<br />ï£«<br />ï£­<br />vx<br />vy<br />vz<br />ï£¶<br />ï£¸,<br />ï£«<br />ï£­<br />lx<br />ly<br />lz<br />ï£¶<br />ï£¸<br />ï£¹<br />ï£»<br />??vy<br />ï£¶<br />ï£¸Â·<br />?<br />câ{x,y,z}<br />âÎ¦c<br />?<br />DomSet(Ïyz(Ïx&gt;lx(P))) â©<br />vz<br />?<br />,<br />?ly<br />lz<br />???<br />Â·<br />?<br />câ{x,y}<br />âÎ¦c<br />â (âÏyâ âÎ¦yÂ· (lyâ vy))<br />Â· Area<br />?<br />DomSet?Ïxz<br />?Ïy&gt;ly(P)??â©<br />??vx<br />vz<br />?<br />,<br />?lx<br />lz<br />???<br />Â·<br />?<br />câ{x,z}<br />âÎ¦c<br />â (âÏxâ âÎ¦xÂ· (lxâ vx))<br />Â· Area<br />?<br />DomSet(Ïxy(Ïz&gt;lz(P))) â©<br />??vx<br />vy<br />?<br />,<br />?lx<br />ly<br />???<br />Â·<br />?<br />câ{y,z}<br />âÎ¦c<br />And it will be 0 otherwise.<br />5.3Simple Higher Dimensional Expected Hypervolume Improvement<br />Computation<br />Although we are currently decomposing our integral into different quantities in<br />order to calculate it, we can also calculate the sum of these quantities using a<br />single dominated hypervolume calculation, though this has downsides which will<br />be explored later. This subsection will give the general formula for doing so. Recall<br />how we decomposed the calculation of the hypervolume improvement in Section<br />5.1:<br />HI(p) =<br />?<br />CâD<br />IC<br />IC= HyperVolume(AC) â HyperVolume(DomSet(P) â© AC)<br />This sum can be rearranged to the following:<br />?<br />CâD<br />HyperVolume(AC) â<br />?<br />CâD<br />HyperVolume(DomSet(P) â© AC)<br />20</p>  <p>Page 21</p> <p>Since the quantities AC sum to a generalized rectangular volume, we could<br />just as readily calculate the total volume of A directly. This is what we did in<br />Section 5.2, where the correction terms HyperVolume(DomSet(P)â©AC) were still<br />calculated separately. Clearly?<br />(?<br />Hypervolume(DomSet(P) â© A) =<br />CâD<br />We initially decomposed the HI(p) in this way in order to compute the correspond-<br />ing EI-integral.<br />We have determined that each partial quantity HyperVolume(DomSet(P)â©AC)<br />depends linearly on the dimensions which its corresponding volume AC depends<br />on, and is constant in the same dimensions in which AC is constant. This is<br />true as well when HyperVolume(DomSet(P)â©A) is first calculated, and then split<br />into the various volumes representing DomSet(P) â© AC. Because of this, we can<br />calculate an m-dimensional EHVI using only a single m-dimensional hypervolume<br />calculation per cell. We need to calculate the hypervolume improvement of each<br />cellâs center of mass, Â¯ p.<br />?ud<br />Î¦d(ud) â Î¦d(ld)<br />The integral can be calculated as if it is an expected improvement where the<br />currently best solution is 0. However, we already need to compute Ï(rd,ld,Âµd,Ïd)â<br />Ï(ld,ud,Âµd,Ïd) to calculate the component of the EHVI corresponding to A, and<br />the following equation holds:<br />CâDAC = A. Moreover the ACs are mutually<br />disjoint except for common boundary points. From this we get that DomSet(P)â©<br />CâDAC) = DomSet(P) â© A (and the DomSet(P) â© ACs are mutually disjoint<br />except for the boundaries). Thus<br />?<br />Hypervolume(DomSet(P) â© AC).<br />Â¯ pd=<br />pd=ldpdÂ· Ïd(pd)dp<br />Ï(0,ld,Âµd,Ïd) â Ï(0,ud,Âµd,Ïd)<br />Î¦d(ud) â Î¦d(ld)<br />Dividing a partial expected improvement over a range [ld,ud) by the chance of<br />being in that range (given by Î¦d(ud)âÎ¦d(ld)) gives the expected improvement of<br />points which are known to lie within that range. Adding the value of rdgives the<br />expected dth coordinate of a point in the objective space bounded from below by<br />r.<br />This means that the general formula for calculating the partial expected im-<br />provement in a cell is the following if the cell is not dominated:<br />=Ï(rd,ld,Âµd,Ïd) â Ï(rd,ud,Âµd,Ïd)<br />Î¦d(ud) â Î¦d(ld)<br />+ rd<br />EI =<br />?<br />dâD<br />(Ï(rd,ld,Âµd,Ïd) â Ï(rd,ud,Âµd,Ïd)) â SâÂ·<br />?<br />dâD<br />(Î¦d(ud) â Î¦d(ld)), where<br />21</p>  <p>Page 22</p> <p>Sâ= HyperVolume<br />ï£«<br />ï£¬<br />ï£¬<br />ï£­DomSet(P) â©<br />ï£®<br />ï£°<br />ï£¯ï£¯<br />ï£«<br />ï£¬<br />ï£¬<br />ï£­<br />r1<br />r2<br />...<br />rm<br />ï£¶<br />ï£·<br />ï£·<br />ï£¸,<br />ï£«<br />ï£¬<br />ï£¬<br />ï£­<br />Â¯ p1<br />Â¯ p2<br />...<br />Â¯ pm<br />ï£¶<br />ï£·<br />ï£·<br />ï£¸<br />ï£¹<br />ï£»<br />ï£ºï£º<br />ï£¶<br />ï£·<br />ï£·<br />ï£¸and<br />Â¯ pd= rd+Ï(rd,ld,Âµd,Ïd) â Ï(rd,ud,Âµd,Ïd)<br />Î¦d(ud) â Î¦d(ld)<br />And 0 otherwise.<br />5.4 Complexity and Algorithm Details<br />Any algorithm which iterates over all grid cells described in Section 5 will have<br />a time complexity of Î©(nm). This is further increased by the complexity of the<br />calculations within each grid cell. The algorithm of Section 5.3 requires an m-<br />dimensional hypervolume to be calculated for each cell that is not dominated.<br />Calculating a 3-dimensional hypervolume can be done in O(nlogn), which re-<br />sults in a time complexity of O(n4logn). However, as will be shown in Section<br />6, constant-time calculations within each grid cell are possible with O(n3) total<br />preparation time, resulting in an algorithm of complexity O(n3). Similar O(nm)<br />algorithms are conjectured to exist for m &gt; 3.<br />One important thing to note, is that the expected hypervolume improvements<br />for multiple individuals can be calculated at the same time without having to per-<br />form the hypervolume calculations more than once when using the decomposition<br />described in Section 5.2, because the hypervolume calculations are not dependent<br />on the mean and standard deviation of the probability distribution. The algorithm<br />described in Section 5.3 does not have this advantage.<br />6O(n3)-time 3-D Expected Hypervolume Improvement Cal-<br />culations<br />In Section 4 we showed that calculating the 2-D expected hypervolume improve-<br />ment is possible with time complexity O(n2). Although the algorithm described<br />in that subsection made use of characteristics of a 2-D Pareto approximation set<br />which are not present in higher dimensions, this subsection will show that there<br />is also a way to calculate the 3-D EHVI with time complexity O(n3). In other<br />words: the calculations necessary for computing the partial expected hypervolume<br />improvement of each grid cell will be performed in constant time. The trade-off is<br />that we will need O(n2) extra memory.<br />The only calculations which have a complexity higher than constant time are<br />the dominated hypervolume calculations. If we use the simple algorithm described<br />22</p>  <p>Page 23</p> <p>in Section 5.3, we only need to perform a single 3-dimensional hypervolume calcu-<br />lation to find the correction term that we need. However, we will start out with<br />the algorithm described in Section 5.2 (without replacing r by v), because it lends<br />itself better to the re-use of old hypervolume calculations. Three sets of correction<br />terms are needed to calculate the partial expected hypervolume improvement of a<br />cell:<br />â¢ Sâ<br />ume calculation.<br />â, a constant correction term which requires a three-dimensional hypervol-<br />â¢ Sâ<br />tion. We will call the 2-D areas used in the calculation of these correction<br />terms xslice, yslice and zslice, respectively.<br />x, Sâ<br />yand Sâ<br />z, which each require a two-dimensional hypervolume calcula-<br />â¢ Sâ<br />Instead of calculating these correction terms afresh for each cell, it is possible<br />to perform all necessary hypervolume calculations in only O(n3) time total. The<br />first step is to create a data structure which allows us to see whether or not a cell<br />is dominated in O(1) time. This can simply be a two-dimensional array holding<br />the highest value of z for which the cell is dominated, which we shall call Hz. A<br />simple way to fill this array is to iterate over all points q â P in order of ascending<br />z value, setting the array value Hz(a1,a2) to z if q dominates the lower corner of<br />C(a1,a2,0). The complexity of this operation is in O(n2n+nlogn) = O(n3). This<br />only needs to be done once, so the O(n3) time complexity does not increase the<br />total asymptotic time complexity of computing the EHVI in 3-D. Figure 7 shows<br />an example.<br />xy, Sâ<br />xzand Sâ<br />yz, which requires a âone-dimensionalâ hypervolume calculation.<br />x<br />y<br />z<br />(10,2,10)<br />(7,3,8)<br />(9,6,6)<br />(4,10,4)<br />Reference point: (0,0,0)<br />y<br />x<br />1010<br />10<br />10<br />8<br />8<br />6<br />6<br />6<br />6<br />4<br />0<br />0<br />0<br />0<br />0<br />Figure 7: Example height array Hzfor a population consisting of 4 points, which is visualized<br />on the left. Cells on the outermost edge of the integration area (which stretch out to â in some<br />dimension) are always non-dominated.<br />23</p>  <p>Page 24</p> <p>Besides containing information that allows constant-time evaluation of whether<br />a cell is dominated, the value of Sâ<br />is also given by Hz(a1,a2). If we build two more height arrays Hxand Hywhere<br />we use the highest value of x and y instead of z, we can determine the results of<br />all three of the one-dimensional hypervolume calculations in constant time during<br />cell calculations.<br />Now, only the two-dimensional hypervolume calculations represented by xslice,<br />yslice and zslice, and the three-dimensional hypervolume calculation represented<br />by Sâ<br />ity, we have omitted their dependence on a particular cell from the notation until<br />now, but in order to show the relations between correction terms of different cells,<br />we will write âSâ<br />the two-dimensional hypervolumes.<br />The value of Sâ<br />following way:<br />xyfor a cell C(a1,a2,a3) that is not dominated<br />â, still have a complexity greater than constant time. For notational simplic-<br />âbelonging to C(a1,a2,a3)â as C(a1,a2,a3).Sâ<br />â, and likewise for<br />âis related to the values of xslice, yslice and zslice in the<br />â¢ C(a1,a2,a3).xslice =<br />C(a1+1,a2,a3).Sâ<br />ââC(a1,a2,a3).Sâ<br />uxâlx<br />ââC(a1,a2,a3).Sâ<br />uyâly<br />ââC(a1,a2,a3).Sâ<br />uzâlz<br />â<br />â¢ C(a1,a2,a3).yslice =<br />C(a1,a2+1,a3).Sâ<br />â<br />â¢ C(a1,a2,a3).zslice =<br />With our height array Hz, we can calculate all values of zslice for a given value<br />of a3in O(n2) time. We can also calculate all values of Sâ<br />a3in O(n2) time, provided a3 = 0 or we have both Sâ<br />where a3is one lower. The details of these calculations will be given below. If we<br />go through our cells in the right order (with a3starting at 0, incrementing it only<br />after we have performed the calculations for all cells with a given value of a3), we<br />only need to update the values of zslice and Sâ<br />for the full computation with complexity in O(n3). If we know the value of Sâ<br />all cells with a given value of a3, we can use the formulas given above to calculate<br />xslice and yslice in constant time whenever we need them, so we do not need to<br />calculate these constants in advance.<br />The details of calculating zslice using the height array are as follows. We will<br />iterate through the possible values of a1and a2in ascending order. We know that<br />zslice(a1,a2) is 0 if a1= 0 or a2= 0. If our height array shows that C(a1â1,a2â<br />1,a3) is dominated, zslice(a1,a2) is set equal to the area of the 2-D rectangle from<br />its lower corner to (rx,ry). Else, if that cell is not dominated, zslice(a1,a2) is set<br />equal to zslice(a1â1,a2)+zslice(a1,a2â1)âzslice(a1â1,a2â1). The value of<br />zslice(a1â 1,a2â 1) is removed as this is the area which is overlapping, causing<br />it to be added twice otherwise.<br />For an example, refer to Figure 8.<br />C(a1,a2,a3+1).Sâ<br />â<br />âfor a given value of<br />âand zslice for the cells<br />ân times, resulting in an algorithm<br />âfor<br />24</p>  <p>Page 25</p> <p>7 Empirical Tests and Results<br />Five different implementations of a 3-D expected hypervolume improvement cal-<br />culation algorithm were used throughout the following tests, referred to as the<br />8-term, 5-term, 2-term, slice-update and Monte Carlo schemes. The goal of com-<br />paring the exact calculation algorithms to a Monte Carlo scheme is twofold. First,<br />by computing the Expected Hypervolume Improvement in different ways, the al-<br />gorithms and their implementations will be thoroughly validated. Second, the<br />time consumption of the algorithms will be compared. This is of particular inter-<br />est because Monte Carlo schemes are often used as fast approximations to exact<br />computations.<br />â¢ The 8-term scheme is a direct implementation of the calculations described<br />in Section 5.2.<br />â¢ The 5-term scheme implements the slightly simplified calculations described<br />in Section 5.2.<br />â¢ The 2-term scheme implements the calculations described in Section 5.3.<br />â¢ The slice-update scheme implements the algorithm described in Section 6.<br />â¢ The Monte Carlo scheme uses Monte Carlo integration to give an approxima-<br />tion of the expected hypervolume improvement. Its random number generator<br />uses the Box-Muller transform [25] in combination with the Mersenne Twister<br />algorithm [24] (specifically, the 32-bit MT19937 variant from the C++ stan-<br />dard library, implemented in GCC) to generate normally distributed pseudo-<br />random numbers. Due to the nature of Monte Carlo algorithms, it is impos-<br />sible to get an exact answer out of this scheme. The expected error of Monte<br />Carlo integration is related to the number of trials m by<br />that to make the estimate ten times more accurate, a hundred times more<br />trials are required.<br />1<br />âm, which means<br />The implementations of Ï and the Gaussian cumulative distribution function<br />were identical for all schemes, except for the Monte Carlo scheme where they were<br />not used. The 2-D and 3-D hypervolume calculation functions were also identical<br />between those schemes which used them. Standard C++ library functions were<br />used for sorting and for the implementation of the Gaussian error function erf.<br />7.1Monte Carlo Verification<br />As a verification of the correctness of the algorithms, the expected hypervolume<br />improvements calculated by all schemes on several test problems were compared<br />to each other and to the value which the Monte Carlo scheme converged towards.<br />25</p>  <p>Page 26</p> <p>The graph in Figure 9 shows the results of running the algorithms on a sim-<br />ple test problem. The population consisted of three points: (1,2,3), (2,3,1) and<br />(3,1,2). The reference point was set to (0,0,0). The median vector for the Gaus-<br />sian distribution was set to (3,3,3), placing it right between cell borders, and the<br />standard deviation was set to (2,2,2). All non-Monte Carlo schemes gave exactly<br />identical answers, which was likely due to the simplicity of the test case, because<br />rounding errors in the floating-point calculations would have resulted in small dif-<br />ferences otherwise. The Monte Carlo scheme was allowed to run for 100.000.000<br />iterations.<br />Figure 10 shows the results of running the algorithm on a few more complex<br />populations. The first consists of 30 points, some of which had identical values to<br />another point in the population in one of their dimensions (creating cells of size<br />0). The second consists of 100 points with a bias towards one area of the search<br />space. The results of all non-Monte Carlo schemes on these two test problems were<br />identical to 15 and 14 digits, respectively. The double-precision floating numbers<br />which were used in the implementations are accurate to approximately the 15th<br />decimal, so the answers can safely be considered identical.<br />The convergence of the Monte Carlo integration, as well as the near-identical<br />answers generated by the different approaches towards calculating the expected<br />hypervolume improvement, both support the validity of the calculations described<br />in this thesis.<br />26</p>  <p>Page 27</p> <p>y<br />x<br />4<br />7<br />9<br />10<br />2<br />3<br />6<br />10<br />00<br />00<br />00<br />0<br />0<br />0<br />0<br />0<br />0<br />0<br />0<br />0<br />0<br />y<br />x<br />4<br />7<br />9<br />10<br />2<br />3<br />6<br />10<br />2*4 = 82*7 = 14 2*9 = 18 2*10 = 20<br />3*4 = 12 3*7 = 21 3*9 = 27<br />27+20-18<br />= 29<br />6*4 = 24 6*7 = 42 6*9 = 5454+29-27<br />= 56<br />10*4 = 4040+42-24<br />= 58<br />58+54-42<br />= 70<br />70+56-54<br />= 72<br />y<br />x<br />4<br />7<br />9<br />10<br />2<br />3<br />6<br />10<br />18*4<br />= 72<br />20*4<br />= 80<br />27*4<br />= 108<br />29*4<br />= 116<br />24*4<br />= 96<br />42*4<br />= 168<br />54*4<br />= 216<br />56*4<br />= 224<br />40*4<br />= 160<br />58*4<br />= 232<br />70*4<br />= 280<br />72*4<br />= 288<br />12*4<br />= 48<br />21*4<br />= 84<br />14*4<br />= 56<br />8*4<br />= 32<br />y<br />x<br />4<br />7<br />9<br />10<br />2<br />3<br />6<br />10<br />2*4 = 82*7 = 14 2*9 = 18 2*10 = 20<br />3*4 = 12 3*7 = 21 3*9 = 27<br />27+20-18<br />= 29<br />6*4 = 24 6*7 = 42 6*9 = 5454+29-27<br />= 56<br />0+24-0<br />= 24<br />24+42-24<br />= 42<br />42+54-42<br />= 54<br />54+56-54<br />= 56<br />y<br />x<br />4<br />7<br />9<br />10<br />2<br />3<br />6<br />10<br />72<br />+ 18*2<br />= 108<br />80<br />+ 20*2<br />= 120<br />108<br />+ 27*2<br />= 162<br />116<br />+ 29*2<br />= 174<br />96<br />+ 24*2<br />= 144<br />168<br />+ 42*2<br />= 252<br />216<br />+ 54*2<br />= 324<br />224<br />+ 56*2<br />= 336<br />160<br />+ 24*2<br />= 208<br />232<br />+ 42*2<br />= 316<br />280<br />+ 54*2<br />= 388<br />288<br />+ 56*2<br />= 400<br />48<br />+ 12*2<br />= 72<br />84<br />+ 21*2<br />= 126<br />56<br />+ 14*2<br />= 84<br />32<br />+ 8*2<br />= 48<br />y<br />x<br />4<br />7<br />9<br />10<br />2<br />3<br />6<br />10<br />2*4 = 82*7 = 14 2*9 = 18 2*10 = 20<br />3*4 = 12 3*7 = 2121+18-14<br />= 25<br />25+20-18<br />= 27<br />0+12-0<br />= 12<br />12+21-12<br />= 21<br />21+25-21<br />= 25<br />25+27-25<br />= 27<br />0+12-0<br />= 12<br />12+21-12<br />= 21<br />21+25-21<br />= 25<br />25+27-25<br />= 27<br />Figure 8: Some values of zslice and Sâfor the example shown in Figure 7, with a3= 0, 1 and<br />2, respectively. The x and y values of each cellâs lower corner are shown on the axes. The grids<br />with the values of Sâare on the left and the grids with the values of zslice are on the right.<br />27</p>  <p>Page 28</p> <p>Figure 9: Logarithmic-scale graph of the convergence of Monte Carlo integration. The answer<br />was measured every 100.000 iterations.<br />28</p>  <p>Page 29</p> <p>Figure 10: Two logarithmic-scale graphs showing the convergence of Monte Carlo integration,<br />along with visualizations of the Pareto approximation sets.<br />29</p>  <p>Page 30</p> <p>7.2 Empirical Performance<br />To test the empirical performance of the exact calculation schemes, they were<br />tested on mutually non-dominated populations of varying sizes that were gener-<br />ated by selecting n pseudo-random points which were uniformly distributed on<br />a spherical surface. The time needed for calculating the expected hypervolume<br />improvement was measured (along with all operations required to do so, such as<br />sorting the populations, but not including the time needed to generate the popula-<br />tions). The seed of the pseudorandom generator was the same for each calculation<br />scheme that was tested. Figure 11 shows the results. There is a noticeable dif-<br />ference in speed between the 8-term, 5-term and 2-term scheme, but they are in<br />the same complexity class and for any given n, their performance relative to each<br />other is roughly the same. The slice-update scheme, by contrast, performs better<br />relative to the other schemes when n increases, as would be expected due to its<br />lower complexity. Even for small n it outperforms the other algorithms.<br />Figure 11: Time needed to calculate the expected hypervolume improvement for a Pareto ap-<br />proximation set consisting of n points randomly selected on the surface of a sphere, averaged<br />over 10 runs.<br />What is interesting is that going from 8 terms to 5 terms causes a greater<br />improvement than going from 5 terms to 2 terms, even though 2-dimensional<br />hypervolume calculations are completely removed from the equation when going to<br />2 terms. No solid conclusions can be drawn from the magnitude of the differences,<br />as they might depend on the implementation details of the code and the compiler<br />30</p>  <p>Page 31</p> <p>optimizations. However, this does show that simplifying calculations can make a<br />big difference for the speed of an algorithm. A benefit of the 2-term scheme which<br />is not captured in the graph, is that it is the simplest scheme in terms of the<br />number of operations that must be implemented, so the time needed to implement<br />it will be shorter.<br />Figure 12: The figure on the left shows the number of Monte Carlo trials that can be performed<br />in a second given a spherical Pareto approximation set consisting of n points. The figure on the<br />right plots the same data as a graph of the time required for 100.000 Monte Carlo iterations,<br />compared to the time needed for the fastest non-Monte Carlo scheme.<br />Figure 13: Graph showing the time needed to simultaneously calculate the EHVI on a number of<br />candidate points using either the 5-term or slice-update schemes, for a population size of 30. The<br />expected time taken when simply calling the slice-update scheme on each candidate individual<br />separately is also plotted in this graph for purposes of comparison.<br />31</p>  <p>Page 32</p> <p>The Monte Carlo scheme is a special case, in that the time it takes to run<br />depends on the desired accuracy, and this accuracy in turn also depends on the<br />variance of the predictive distribution. When this variance is higher, the accuracy<br />will be lower. For a rough idea of its performance relative to the exact calculation<br />schemes, see Figure 12, which shows the number of Monte Carlo trials which can be<br />performed if the algorithm is allowed to run for a second. Because of the O(nlogn)<br />time complexity of each individual trial, it is less affected by n than any of the<br />exact calculation schemes. If n is large enough and the desired accuracy is low<br />enough, it might be the faster option. However, when n is reasonably small, there<br />is no advantage to using it.<br />The complexity of calculating the expected hypervolume improvement of mul-<br />tiple points by repeatedly using one of the described algorithms is of course linear<br />in the number of candidate individuals. Here, the 8-term, 5-term and slice-update<br />schemes have an advantage not shared by the Monte Carlo and 2-term schemes, in<br />that their hypervolume calculations are independent of the probability distribution<br />for which the EHVI is being calculated. This makes it possible to calculate sev-<br />eral expected hypervolume improvements on the same population with a relatively<br />small corresponding increase in calculation time, because the additional calcula-<br />tions have complexity O(n3). It is expected to be less impressive for the slice-<br />update scheme, as this already has a time complexity of O(n3), but the amount of<br />overhead that is avoided might still be noticeable. To determine the impact of this<br />advantage on the relative performance of the schemes, Figure 13 shows the results<br />of using the schemes to calculate the EHVI for a vector of probability distributions<br />instead of just one.<br />As can be seen, the time taken increases linearly in the number of individuals<br />evaluated at the same time, but the constant added on top of that is larger for the<br />5-term scheme than for the slice-update scheme. When n is 30 it is only a difference<br />equivalent to evaluating a few more candidate individuals, however. Because the<br />5-term scheme is somewhat easier to implement, it might be preferable to use it if<br />the number of candidate individuals is expected to be high in comparison to n.<br />8 Conclusion and Future Work<br />The main results realized in this thesis are as follows: A fast algorithm for cal-<br />culating the EHVI in two dimensions was proposed with runtime complexity in<br />O(n2) (previously: O(n3logn)). An empirical test shows improved speed even for<br />relatively small n (â 20). An exact calculation algorithm for calculating the EHVI<br />in more than two dimensions is provided. This generic algorithm has been detailed<br />and improved in efficiency for the important tri-objective case. It has a cubic run-<br />time complexity in O(n3), and a further efficiency gain can be obtained by batch<br />evaluation, i.e. re-using the data structures for multiple EHVI computations. The<br />algorithm are based on linear data structures and there are no large hidden con-<br />32</p>  <p>Page 33</p> <p>stants. For three dimensions it is now possible to perform more than a hundred<br />EHVI in 2.5 seconds for an approximation set size of 30. Implementations of all<br />algorithms are made available [TODO: url] and have been validated with results<br />from Monte Carlo algorithms.<br />The results open up new possibilities to construct expected improvement algo-<br />rithms for multiobjective optimization, for instance by using the fast EHVI eval-<br />uation as an infill criterion in the efficient global optimization algorithm (EGO).<br />Moreover, as the new exact EHVI computation methods have the same or better<br />runtime performance compared to the Monte Carlo algorithms used so far, they<br />can now replace these inaccurate methods.<br />As a side result a relationship between the expected improvement and the center<br />of (probability) mass over single cells was established, which might in the future<br />shed some light on the relation to alternative expected improvement formulations<br />[Keane04] and be useful for establishing theoretical results.<br />Source code and acknowledgement<br />thesis of Iris Hupkens [1] under the supervision of M. Emmerich and A. Deutz.<br />The sourcecode of all algorithms in C++ is made available on http://natcomp.<br />liacs.nl/index.php?page=code.<br />This work is based on the honors masterâs<br />References<br />[1] Iris Hupkens: Complexity Reduction and Validation of Computing the Ex-<br />pected Hypervolume Improvement, Masterâs Thesis (with honors) published<br />as LIACS, Internal Report Nr. 2013-12, August, 2013 http://www.liacs.nl/<br />assets/Masterscripties/2013-12IHupkens.pdf<br />[2] Shir, O. M., Emmerich, M., BÂ¨ ack, T., and Vrakking, M. J. (2007, September).<br />The application of evolutionary multi-criteria optimization to dynamic molecular<br />alignment. In Evolutionary Computation, 2007. CEC 2007. IEEE Congress on<br />(pp. 4108-4115). IEEE.<br />[3] Zaefferer, M., Bartz-Beielstein, T., Naujoks, B., Wagner, T., and Emmerich,<br />M. (2013, January). A Case Study on Multi-Criteria Optimization of an Event<br />Detection Software under Limited Budgets. InEvolutionary Multi-Criterion Op-<br />timization(pp. 756-770). Springer Berlin Heidelberg.<br />[4] Shimoyama, K., Sato, K., Jeong, S., and Obayashi, S. (2012, June). Comparison<br />of the criteria for updating Kriging response surface models in multi-objective<br />optimization. InEvolutionary Computation (CEC), 2012 IEEE Congress on(pp.<br />1-8). IEEE.<br />33</p>  <p>Page 34</p> <p>[5] Shimoyama, K., Jeong, S., and Obayashi, S. (2013, June). Kriging-surrogate-<br />based optimization considering expected hypervolume improvement in non-<br />constrained many-objective test problems. InEvolutionary Computation (CEC),<br />2013 IEEE Congress on(pp. 658-665). IEEE.<br />[6] Couckuyt, Ivo, Dirk Deschrijver, and Tom Dhaene. âFast calculation of mul-<br />tiobjective probability of improvement and expected improvement criteria for<br />Pareto optimization.âJournal of Global Optimization(2013): 1-20.<br />[7] Wagner, T.; Emmerich, M.; Deutz, A. and Ponweiser, W. (2010) âOn expected-<br />improvement criteria for model-based multi-objective optimizationâ, in âProc. of<br />PPSN XI Vol. 1, Springer-Verlag, Berlin, Heidelberg, pp. 718-727.<br />[8] Fleischer, M. (2003) âThe Measure of Pareto Optima Applications to Multi-<br />objective Metaheuristicsâ. Evolutionary Multi-Criterion Optimization. Second<br />International Conference, EMO 2003, pg. 519-533.<br />[9] Emmerich, M. T M; Deutz, A.H.; Klinkenberg, J.W. (2011) âHypervolume-<br />based expected improvement: Monotonicity properties and exact computation,â<br />2011 IEEE Congress on Evolutionary Computation (CEC), pp.2147-2154<br />[10] Nicola Beume, Carlos M. Fonseca, Manuel Lopez-Ibanez, Luis Paquete, and<br />Jan Vahrenhold. (2009) âOn the complexity of computing the hypervolume in-<br />dicator.â IEEE Trans. Evolutionary Computation, 13(5) pp. 1075-1082.<br />[11] Sacks, J., Welch, W. J., Mitchell, T. J., and Wynn, H. P. (1989) âDesign and<br />analysis of computer experimentsâ. Statistical science, 4(4), 409-423.<br />[12] Mockus, J., Tiesis, V., Zilinskas, A. (1978) âThe application of Bayesian meth-<br />ods for seeking the extremumâ. In: Dixon, L., Szego, G. (Eds.), Towards Global<br />Optimization, vol. 2. North Holland, New York, pp. 117129.<br />[13] Donald R. Jones, Matthias Schonlau, and William J. Welch. (1998) âEfficient<br />Global Optimization of Expensive Black-Box Functionsâ. J. of Global Optimiza-<br />tion 13, 4 (December 1998), 455-492.<br />[14] Emmanuel Vazquez and Julien Bect. (2010) âConvergence properties of the<br />expected improvement algorithm with fixed mean and covariance functionsâ.<br />Journal of Statistical Planning and Inference 140, pp. 3088-3095<br />[15] Knowles, J. (2006) âParEGO: A hybrid algorithm with on-line landscape ap-<br />proximation for expensive multiobjective optimization problemsâ. IEEE Trans-<br />actions on Evolutionary Computation. 10 (1): 50-66.<br />[16] Keane, A.J. (2006) âStatistical improvement criteria for use in multiobjective<br />design optimisationâ. AIAA Journal, 44, (4), 879-891.<br />34</p>  <p>Page 35</p> <p>[17] Wolfgang Ponweiser, Tobias Wagner, Dirk Biermann, and Markus Vincze.<br />(2008) âMultiobjective Optimization on a Limited Budget of Evaluations Using<br />Model-Assisted S-Metric Selectionâ. In Proceedings of the 10th international<br />conference on Parallel Problem Solving from Nature: PPSN X. Springer-Verlag,<br />Berlin, Heidelberg, 784-794.<br />[18] Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and T. Meyarivan. (2000)<br />âA fast and elitist multi-objective genetic algorithm: NSGA-IIâ.<br />[19] E. Zitzler, M. Laumanns and L. Thiele. (2001) âSPEA2: Improving the<br />Strength Pareto Evolutionary Algorithmâ.<br />[20] Michael Emmerich, Nicola Beume, and Boris Naujoks. (2005) âAn EMO Al-<br />gorithm Using the Hypervolume Measure as Selection Criterionâ. In 2005 Intl<br />Conference, March 2005, pages 62-76.<br />[21] Christian Igel, Nikolaus Hansen, and Stefan Roth. (2007) âCovariance Ma-<br />trix Adaptation for Multi-objective Optimizationâ. Evol. Comput. 15, 1 (March<br />2007), 1-28.<br />[22] Williams, Christopher K.I. (1998) âPrediction with Gaussian processes: From<br />linear regression to linear prediction and beyondâ. In M. I. Jordan. Learning in<br />graphical models. MIT Press. pp. 599612.<br />[23] Fubini, G. âSugli integrali multipli.â (1958) Opere scelte, Vol. 2. Cremonese,<br />pp. 243-249.<br />[24] Matsumoto, M.; Nishimura, T. (1998) âMersenne twister: a 623-dimensionally<br />equidistributed uniform pseudo-random number generatorâ. ACM Transactions<br />on Modeling and Computer Simulation 8 (1): 330<br />[25] G. E. P. Box, Mervin E. Muller. (1958) A Note on the Generation of Random<br />Normal Deviates. The Annals of Mathematical Statistics, Vol. 29, No. 2. pp.<br />610-611<br />[26] Emmerich, M. (2005). Single-and multi-objective evolutionary design opti-<br />mization assisted by gaussian random field metamodels. Dissertation, TU Dort-<br />mund, Informatik, Eldorado, http://hdl.handle.net/2003/21807.<br />[27] Kumano, T., Jeong, S., Obayashi, S., Ito, Y., Hatanaka, K., and Morino, H.<br />(2006). Multidisciplinary design optimization of wing shape with nacelle and<br />pylon. InEuropean Conference on Computational Fluid Dynamics ECCOMAS<br />CFD.<br />[28] Miettinen, K. (1999). Nonlinear Multiobjective Optimization, volume 12 of<br />International Series in Operations Research and Management Science.<br />35</p>  <p>Page 36</p> <p>[29] Zitzler, E., Thiele, L. (1998, January). Multiobjective optimization using evo-<br />lutionary algorithmsa comparative case study. In Parallel problem solving from<br />naturePPSN V (pp. 292-301). Springer Berlin Heidelberg<br />[30] Zitzler, E., Thiele, L., Laumanns, M., Fonseca, C. M., and Da Fonseca, V. G.<br />(2003). Performance assessment of multiobjective optimizers: An analysis and<br />review. Evolutionary Computation, IEEE Transactions on, 7(2), 117-132.<br />[31] Fleischer, M. (2003, January). The measure of Pareto optima applications<br />to multi-objective metaheuristics. In Evolutionary multi-criterion optimization<br />(pp. 519-533). Springer Berlin Heidelberg.<br />[32] Auger, A., Bader, J., Brockhoff, D., and Zitzler, E. (2009, January). Theory of<br />the hypervolume indicator: optimal Âµ-distributions and the choice of the refer-<br />ence point. In Proceedings of the tenth ACM SIGEVO workshop on Foundations<br />of genetic algorithms (pp. 87-102). ACM.<br />[33] Bringmann, K., and Friedrich, T. (2010, July). The maximum hypervolume<br />set yields near-optimal approximation. In Proceedings of the 12th annual con-<br />ference on Genetic and evolutionary computation (pp. 511-518). ACM.<br />[34] Laniewski-Wollk, P., Obayashi S., Jeong, S. (2010), Development of expected<br />improvement for multi-objective problems, in Proceedings of 42nd Fluid Dy-<br />namics Conference/Aerospace Numerical, Simulation Symposium (CD ROM),<br />June 2010<br />[35] Koch, P. (2013). Efficient tuning in supervised machine learning (Doctoral<br />dissertation, Leiden Institute of Advanced Computer Science (LIACS), Faculty<br />of Science, Leiden University).<br />36</p>  <a href="https://www.researchgate.net/profile/Michael_Emmerich/publication/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement/links/5404956f0cf23d9765a67a6c.pdf">Download full-text</a> </div> <div id="rgw21_56aba20b301e0" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw22_56aba20b301e0">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw23_56aba20b301e0"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Michael_Emmerich/publication/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement/links/5404956f0cf23d9765a67a6c.pdf" class="publication-viewer" title="HED14arxiv.pdf">HED14arxiv.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Michael_Emmerich">Michael Emmerich</a> &middot; Sep 1, 2014 </span>   </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw25_56aba20b301e0" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw26_56aba20b301e0">  </ul> </div> </div>   <div id="rgw17_56aba20b301e0" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw18_56aba20b301e0"> <div> <h5> <a href="publication/291386506_Piecewise-planar_reconstruction_using_two_views" class="color-inherit ga-similar-publication-title"><span class="publication-title">Piecewise-planar reconstruction using two views</span></a>  </h5>  <div class="authors"> <a href="researcher/2045184928_Michel_Antunes" class="authors ga-similar-publication-author">Michel Antunes</a>, <a href="researcher/30580412_Joao_P_Barreto" class="authors ga-similar-publication-author">JoÃ£o P. Barreto</a>, <a href="researcher/2095220730_Urbano_Nunes" class="authors ga-similar-publication-author">Urbano Nunes</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56aba20b301e0"> <div> <h5> <a href="publication/284077148_Crossover-first_differential_evolution_for_improved_global_optimization_in_non-uniform_search_landscapes" class="color-inherit ga-similar-publication-title"><span class="publication-title">Crossover-first differential evolution for improved global optimization in non-uniform search landscapes</span></a>  </h5>  <div class="authors"> <a href="researcher/2084992116_Jason_Teo" class="authors ga-similar-publication-author">Jason Teo</a>, <a href="researcher/31593222_Mohd_Hanafi_Ahmad_Hijazi" class="authors ga-similar-publication-author">Mohd Hanafi Ahmad Hijazi</a>, <a href="researcher/2085012518_Hui_Keng_Lau" class="authors ga-similar-publication-author">Hui Keng Lau</a>, <a href="researcher/2085025477_Salmah_Fattah" class="authors ga-similar-publication-author">Salmah Fattah</a>, <a href="researcher/2085079111_Aslina_Baharum" class="authors ga-similar-publication-author">Aslina Baharum</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw20_56aba20b301e0"> <div> <h5> <a href="publication/282585033_A_Novel_Tournament_Selection_Based_Differential_Evolution_Variant_for_Continuous_Optimization_Problems" class="color-inherit ga-similar-publication-title"><span class="publication-title">A Novel Tournament Selection Based Differential Evolution Variant for Continuous Optimization Problems</span></a>  </h5>  <div class="authors"> <a href="researcher/2082337042_Qamar_Abbas" class="authors ga-similar-publication-author">Qamar Abbas</a>, <a href="researcher/27090289_Jamil_Ahmad" class="authors ga-similar-publication-author">Jamil Ahmad</a>, <a href="researcher/2082332173_Hajira_Jabeen" class="authors ga-similar-publication-author">Hajira Jabeen</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw39_56aba20b301e0" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw40_56aba20b301e0">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw41_56aba20b301e0" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=ODgn0L1WvoQ6z4_8szPrWelIGMIHTZBVTwGHEuKE_wUC6yBhJmfHrP3iTVHD4GVh" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="SgHvF0prwj+U945vVKHtZnnPEjgb7vM9HFwRWefpaDM/fADlb5//caDTH4nb7IHyc16W74pzu6MhN1g6GyDoqvytd1DfTlAuR+qcT3Uteu6LuAf1KJO1jhKw1XGacN0xjC08reDeJN+RAcIvuNEUfTQLQN0+OTL2LlOHPWAUW555iO5P89Vt3DTxk8TnXeZXtAXGSFbJgwzl5LLDLoARpDJeiayKz4FWs6yUH2BWzouEsm9OjL/NtAXgUQ8GuH3B5Fk0g7oQBoSV3oqrS0BJs4D+oFqEShvF2Lf33VDA/iI="/> <input type="hidden" name="urlAfterLogin" value="publication/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjY1MjA5ODEzX0Zhc3Rlcl9Db21wdXRhdGlvbl9vZl9FeHBlY3RlZF9IeXBlcnZvbHVtZV9JbXByb3ZlbWVudA%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjY1MjA5ODEzX0Zhc3Rlcl9Db21wdXRhdGlvbl9vZl9FeHBlY3RlZF9IeXBlcnZvbHVtZV9JbXByb3ZlbWVudA%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjY1MjA5ODEzX0Zhc3Rlcl9Db21wdXRhdGlvbl9vZl9FeHBlY3RlZF9IeXBlcnZvbHVtZV9JbXByb3ZlbWVudA%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw42_56aba20b301e0"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 765;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FigureList","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Iris Hupkens","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Iris_Hupkens","institution":"Leiden University","institutionUrl":false,"widgetId":"rgw4_56aba20b301e0"},"id":"rgw4_56aba20b301e0","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=3444711","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56aba20b301e0"},"id":"rgw3_56aba20b301e0","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=265209813","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":265209813,"title":"Faster Computation of Expected Hypervolume Improvement","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"ArXiv e-prints","publicationDate":"08\/2014;","publicationDateRobot":"2014-08","article":""}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1408.7114","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Faster Computation of Expected Hypervolume Improvement"},{"key":"rft.title","value":"ArXiv e-prints"},{"key":"rft.jtitle","value":"ArXiv e-prints"},{"key":"rft.date","value":"2014"},{"key":"rft.au","value":"Iris Hupkens,Michael Emmerich,Andr\u00e9 Deutz"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56aba20b301e0"},"id":"rgw6_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=265209813","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":265209813,"peopleItems":[{"data":{"authorNameOnPublication":"Iris Hupkens","accountUrl":"profile\/Iris_Hupkens","accountKey":"Iris_Hupkens","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Iris Hupkens","profile":{"professionalInstitution":{"professionalInstitutionName":"Leiden University","professionalInstitutionUrl":"institution\/Leiden_University"}},"professionalInstitutionName":"Leiden University","professionalInstitutionUrl":"institution\/Leiden_University","url":"profile\/Iris_Hupkens","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Iris_Hupkens","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw9_56aba20b301e0"},"id":"rgw9_56aba20b301e0","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=3444711&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Leiden University","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":3,"publicationUid":265209813,"widgetId":"rgw8_56aba20b301e0"},"id":"rgw8_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=3444711&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=3&publicationUid=265209813","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Michael Emmerich","accountUrl":"profile\/Michael_Emmerich","accountKey":"Michael_Emmerich","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272193728806937%401441907450928_m\/Michael_Emmerich.png","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Michael Emmerich","profile":{"professionalInstitution":{"professionalInstitutionName":"Leiden University","professionalInstitutionUrl":"institution\/Leiden_University"}},"professionalInstitutionName":"Leiden University","professionalInstitutionUrl":"institution\/Leiden_University","url":"profile\/Michael_Emmerich","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272193728806937%401441907450928_l\/Michael_Emmerich.png","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Michael_Emmerich","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw11_56aba20b301e0"},"id":"rgw11_56aba20b301e0","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=555031&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Leiden University","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":3,"publicationUid":265209813,"widgetId":"rgw10_56aba20b301e0"},"id":"rgw10_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=555031&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=3&publicationUid=265209813","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Andr\u00e9 H. Deutz","accountUrl":"profile\/Andre_Deutz","accountKey":"Andre_Deutz","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Andr\u00e9 H. Deutz","profile":{"professionalInstitution":{"professionalInstitutionName":"Leiden University","professionalInstitutionUrl":"institution\/Leiden_University"}},"professionalInstitutionName":"Leiden University","professionalInstitutionUrl":"institution\/Leiden_University","url":"profile\/Andre_Deutz","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Andre_Deutz","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw13_56aba20b301e0"},"id":"rgw13_56aba20b301e0","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=3323517&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Leiden University","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":3,"publicationUid":265209813,"widgetId":"rgw12_56aba20b301e0"},"id":"rgw12_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=3323517&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=3&publicationUid=265209813","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56aba20b301e0"},"id":"rgw7_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=265209813&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":265209813,"abstract":"<noscript><\/noscript><div>The expected improvement algorithm (or efficient global optimization) aims<br \/>\nfor global continuous optimization with a limited budget of black-box function<br \/>\nevaluations. It is based on a statistical model of the function learned from<br \/>\nprevious evaluations and an infill criterion - the expected improvement - used<br \/>\nto find a promising point for a new evaluation. The `expected improvement'<br \/>\ninfill criterion takes into account the mean and variance of a predictive<br \/>\nmultivariate Gaussian distribution.<br \/>\nThe expected improvement algorithm has recently been generalized to<br \/>\nmultiobjective optimization. In order to measure the improvement of a Pareto<br \/>\nfront quantitatively the gain in dominated (hyper-)volume is used. The<br \/>\ncomputation of the expected hypervolume improvement (EHVI) is a<br \/>\nmultidimensional integration of a step-wise defined non-linear function related<br \/>\nto the Gaussian probability density function over an intersection of boxes.<br \/>\nThis paper provides a new algorithm for the exact computation of the expected<br \/>\nimprovement to more than two objective functions. For the bicriteria case it<br \/>\nhas a time complexity in $O(n^2)$ with $n$ denoting the number of points in the<br \/>\ncurrent best Pareto front approximation. It improves previously known<br \/>\nalgorithms with time complexity $O(n^3 \\log n)$. For tricriteria optimization<br \/>\nwe devise an algorithm with time complexity of $O(n^3)$. Besides discussing the<br \/>\nnew time complexity bounds the speed of the new algorithm is also tested<br \/>\nempirically on test data. It is shown that further improvements in speed can be<br \/>\nachieved by reusing data structures built up in previous iterations. The<br \/>\nresulting numerical algorithms can be readily used in existing implementations<br \/>\nof hypervolume-based expected improvement algorithms.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw14_56aba20b301e0"},"id":"rgw14_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=265209813","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":{"data":{"figures":[{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Emmerich\/publication\/265209813\/figure\/fig1\/Figure-5-Time-needed-to-calculate-the-expected-hypervolume-improvement-in-2-D-averaged.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Emmerich\/publication\/265209813\/figure\/fig1\/Figure-5-Time-needed-to-calculate-the-expected-hypervolume-improvement-in-2-D-averaged_small.png","figureUrl":"\/figure\/265209813_fig1_Figure-5-Time-needed-to-calculate-the-expected-hypervolume-improvement-in-2-D-averaged","selected":false,"title":"Figure 5: Time needed to calculate the expected hypervolume improvement...","key":"265209813_fig1_Figure-5-Time-needed-to-calculate-the-expected-hypervolume-improvement-in-2-D-averaged"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Emmerich\/publication\/265209813\/figure\/fig2\/Figure-9-Logarithmic-scale-graph-of-the-convergence-of-Monte-Carlo-integration-The.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Emmerich\/publication\/265209813\/figure\/fig2\/Figure-9-Logarithmic-scale-graph-of-the-convergence-of-Monte-Carlo-integration-The_small.png","figureUrl":"\/figure\/265209813_fig2_Figure-9-Logarithmic-scale-graph-of-the-convergence-of-Monte-Carlo-integration-The","selected":false,"title":"Figure 9: Logarithmic-scale graph of the convergence of Monte Carlo...","key":"265209813_fig2_Figure-9-Logarithmic-scale-graph-of-the-convergence-of-Monte-Carlo-integration-The"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Emmerich\/publication\/265209813\/figure\/fig3\/Figure-10-Two-logarithmic-scale-graphs-showing-the-convergence-of-Monte-Carlo.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Emmerich\/publication\/265209813\/figure\/fig3\/Figure-10-Two-logarithmic-scale-graphs-showing-the-convergence-of-Monte-Carlo_small.png","figureUrl":"\/figure\/265209813_fig3_Figure-10-Two-logarithmic-scale-graphs-showing-the-convergence-of-Monte-Carlo","selected":false,"title":"Figure 10: Two logarithmic-scale graphs showing the convergence of...","key":"265209813_fig3_Figure-10-Two-logarithmic-scale-graphs-showing-the-convergence-of-Monte-Carlo"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Emmerich\/publication\/265209813\/figure\/fig4\/Figure-11-Time-needed-to-calculate-the-expected-hypervolume-improvement-for-a-Pareto.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Emmerich\/publication\/265209813\/figure\/fig4\/Figure-11-Time-needed-to-calculate-the-expected-hypervolume-improvement-for-a-Pareto_small.png","figureUrl":"\/figure\/265209813_fig4_Figure-11-Time-needed-to-calculate-the-expected-hypervolume-improvement-for-a-Pareto","selected":false,"title":"Figure 11: Time needed to calculate the expected hypervolume...","key":"265209813_fig4_Figure-11-Time-needed-to-calculate-the-expected-hypervolume-improvement-for-a-Pareto"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Emmerich\/publication\/265209813\/figure\/fig5\/Figure-12-The-figure-on-the-left-shows-the-number-of-Monte-Carlo-trials-that-can-be.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Emmerich\/publication\/265209813\/figure\/fig5\/Figure-12-The-figure-on-the-left-shows-the-number-of-Monte-Carlo-trials-that-can-be_small.png","figureUrl":"\/figure\/265209813_fig5_Figure-12-The-figure-on-the-left-shows-the-number-of-Monte-Carlo-trials-that-can-be","selected":false,"title":"Figure 12: The figure on the left shows the number of Monte Carlo...","key":"265209813_fig5_Figure-12-The-figure-on-the-left-shows-the-number-of-Monte-Carlo-trials-that-can-be"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Emmerich\/publication\/265209813\/figure\/fig6\/Figure-13-Graph-showing-the-time-needed-to-simultaneously-calculate-the-EHVI-on-a-number.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Emmerich\/publication\/265209813\/figure\/fig6\/Figure-13-Graph-showing-the-time-needed-to-simultaneously-calculate-the-EHVI-on-a-number_small.png","figureUrl":"\/figure\/265209813_fig6_Figure-13-Graph-showing-the-time-needed-to-simultaneously-calculate-the-EHVI-on-a-number","selected":false,"title":"Figure 13: Graph showing the time needed to simultaneously calculate...","key":"265209813_fig6_Figure-13-Graph-showing-the-time-needed-to-simultaneously-calculate-the-EHVI-on-a-number"}],"readerDocId":"5064652","linkBehaviour":"dialog","isDialog":true,"headerText":"Figures in this publication","isNewPublicationDesign":false,"widgetId":"rgw15_56aba20b301e0"},"id":"rgw15_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/FigureList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FigureList.html?readerDocId=5064652&isDialog=1&linkBehaviour=dialog","viewClass":"views.publicliterature.FigureListView","yuiModules":["rg.views.publicliterature.FigureListView","css-pow-publicliterature-FigureList"],"stylesheets":["pow\/publicliterature\/FigureList.css"],"_isYUI":true},"previewImage":"https:\/\/i1.rgstatic.net\/publication\/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement\/links\/5404956f0cf23d9765a67a6c\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw16_56aba20b301e0"},"id":"rgw16_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56aba20b301e0"},"id":"rgw5_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=265209813&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2045184928,"url":"researcher\/2045184928_Michel_Antunes","fullname":"Michel Antunes","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":30580412,"url":"researcher\/30580412_Joao_P_Barreto","fullname":"Jo\u00e3o P. Barreto","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095220730,"url":"researcher\/2095220730_Urbano_Nunes","fullname":"Urbano Nunes","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Image and Vision Computing","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291386506_Piecewise-planar_reconstruction_using_two_views","usePlainButton":true,"publicationUid":291386506,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.59","url":"publication\/291386506_Piecewise-planar_reconstruction_using_two_views","title":"Piecewise-planar reconstruction using two views","displayTitleAsLink":true,"authors":[{"id":2045184928,"url":"researcher\/2045184928_Michel_Antunes","fullname":"Michel Antunes","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":30580412,"url":"researcher\/30580412_Joao_P_Barreto","fullname":"Jo\u00e3o P. Barreto","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095220730,"url":"researcher\/2095220730_Urbano_Nunes","fullname":"Urbano Nunes","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Image and Vision Computing 01\/2016;  DOI:10.1016\/j.imavis.2015.11.008"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291386506_Piecewise-planar_reconstruction_using_two_views","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291386506_Piecewise-planar_reconstruction_using_two_views\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56aba20b301e0"},"id":"rgw18_56aba20b301e0","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291386506","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2084992116,"url":"researcher\/2084992116_Jason_Teo","fullname":"Jason Teo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":31593222,"url":"researcher\/31593222_Mohd_Hanafi_Ahmad_Hijazi","fullname":"Mohd Hanafi Ahmad Hijazi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2085012518,"url":"researcher\/2085012518_Hui_Keng_Lau","fullname":"Hui Keng Lau","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2085025477,"url":"researcher\/2085025477_Salmah_Fattah","fullname":"Salmah Fattah","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Nov 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/284077148_Crossover-first_differential_evolution_for_improved_global_optimization_in_non-uniform_search_landscapes","usePlainButton":true,"publicationUid":284077148,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/284077148_Crossover-first_differential_evolution_for_improved_global_optimization_in_non-uniform_search_landscapes","title":"Crossover-first differential evolution for improved global optimization in non-uniform search landscapes","displayTitleAsLink":true,"authors":[{"id":2084992116,"url":"researcher\/2084992116_Jason_Teo","fullname":"Jason Teo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":31593222,"url":"researcher\/31593222_Mohd_Hanafi_Ahmad_Hijazi","fullname":"Mohd Hanafi Ahmad Hijazi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2085012518,"url":"researcher\/2085012518_Hui_Keng_Lau","fullname":"Hui Keng Lau","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2085025477,"url":"researcher\/2085025477_Salmah_Fattah","fullname":"Salmah Fattah","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2085079111,"url":"researcher\/2085079111_Aslina_Baharum","fullname":"Aslina Baharum","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["11\/2015; 3(3-4). DOI:10.1007\/s13748-015-0061-1"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/284077148_Crossover-first_differential_evolution_for_improved_global_optimization_in_non-uniform_search_landscapes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/284077148_Crossover-first_differential_evolution_for_improved_global_optimization_in_non-uniform_search_landscapes\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56aba20b301e0"},"id":"rgw19_56aba20b301e0","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=284077148","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2082337042,"url":"researcher\/2082337042_Qamar_Abbas","fullname":"Qamar Abbas","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":27090289,"url":"researcher\/27090289_Jamil_Ahmad","fullname":"Jamil Ahmad","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2082332173,"url":"researcher\/2082332173_Hajira_Jabeen","fullname":"Hajira Jabeen","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Oct 2015","journal":"Mathematical Problems in Engineering","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/282585033_A_Novel_Tournament_Selection_Based_Differential_Evolution_Variant_for_Continuous_Optimization_Problems","usePlainButton":true,"publicationUid":282585033,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"0.76","url":"publication\/282585033_A_Novel_Tournament_Selection_Based_Differential_Evolution_Variant_for_Continuous_Optimization_Problems","title":"A Novel Tournament Selection Based Differential Evolution Variant for Continuous Optimization Problems","displayTitleAsLink":true,"authors":[{"id":2082337042,"url":"researcher\/2082337042_Qamar_Abbas","fullname":"Qamar Abbas","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":27090289,"url":"researcher\/27090289_Jamil_Ahmad","fullname":"Jamil Ahmad","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2082332173,"url":"researcher\/2082332173_Hajira_Jabeen","fullname":"Hajira Jabeen","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Mathematical Problems in Engineering 10\/2015; 2015(5):1-21. DOI:10.1155\/2015\/205709"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/282585033_A_Novel_Tournament_Selection_Based_Differential_Evolution_Variant_for_Continuous_Optimization_Problems","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/282585033_A_Novel_Tournament_Selection_Based_Differential_Evolution_Variant_for_Continuous_Optimization_Problems\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56aba20b301e0"},"id":"rgw20_56aba20b301e0","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=282585033","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw17_56aba20b301e0"},"id":"rgw17_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=265209813&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":265209813,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":265209813,"publicationType":"article","linkId":"5404956f0cf23d9765a67a6c","fileName":"HED14arxiv.pdf","fileUrl":"profile\/Michael_Emmerich\/publication\/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement\/links\/5404956f0cf23d9765a67a6c.pdf","name":"Michael Emmerich","nameUrl":"profile\/Michael_Emmerich","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Sep 1, 2014","fileSize":"972.23 KB","widgetId":"rgw23_56aba20b301e0"},"id":"rgw23_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=265209813&linkId=5404956f0cf23d9765a67a6c&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw22_56aba20b301e0"},"id":"rgw22_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=265209813&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":85,"valueFormatted":"85","widgetId":"rgw24_56aba20b301e0"},"id":"rgw24_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=265209813","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw21_56aba20b301e0"},"id":"rgw21_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=265209813&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":265209813,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw26_56aba20b301e0"},"id":"rgw26_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=265209813&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":85,"valueFormatted":"85","widgetId":"rgw27_56aba20b301e0"},"id":"rgw27_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=265209813","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw25_56aba20b301e0"},"id":"rgw25_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=265209813&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Faster Computation of Expected Hypervolume Improvement\nIris HupkensMichael EmmerichAndr\u00b4 e Deutz\nLeiden Institute for Advanced Computer Science,\nLeiden University, Niels Bohrweg 1, 2333-CA Leiden\nTel.: +31-71-527-7094\nFax: +31-71-527-7001\nemmerich AT liacs.nl\nAbstract\nThe expected improvement algorithm (or efficient global optimization) aims for global\ncontinuous optimization with a limited budget of black-box function evaluations. It is based\non a statistical model of the function learned from previous evaluations and an infill cri-\nterion - the expected improvement - used to find a promising point for a new evaluation.\nThe \u2018expected improvement\u2019 infill criterion takes into account the mean and variance of a\npredictive multivariate Gaussian distribution.\nThe expected improvement algorithm has recently been generalized to multiobjective\noptimization. In order to measure the improvement of a Pareto front quantitatively the\ngain in dominated (hyper-)volume is used. The computation of the expected hypervolume\nimprovement (EHVI) is a multidimensional integration of a step-wise defined non-linear\nfunction related to the Gaussian probability density function over an intersection of boxes.\nThis paper provides a new algorithm for the exact computation of the expected improvement\nto more than two objective functions. For the bicriteria case it has a time complexity in\nO(n2) with n denoting the number of points in the current best Pareto front approximation.\nIt improves previously known algorithms with time complexity O(n3logn). For tricriteria\noptimization we devise an algorithm with time complexity of O(n3). Besides discussing the\nnew time complexity bounds the speed of the new algorithm is also tested empirically on\ntest data. It is shown that further improvements in speed can be achieved by reusing data\nstructures built up in previous iterations. The resulting numerical algorithms can be readily\nused in existing implementations of hypervolume-based expected improvement algorithms.\nKeywords: Multiobjective Optimization, Expected Improvement, Efficient Global Op-\ntimization, Bayesian Global Optimization, Hypervolume Indicator\n1"},{"page":2,"text":"1 Introduction\nIn multiobjective optimization, the goal is to find (a set of) solutions which opti-\nmize multiple objective functions at the same time [28]. As in case of conflicting\nobjectives there is no single best solution, it is common to compute instead all\npoints of the Pareto front (PF) or an approximation to this set.\nSometimes the function values of solutions can only be determined through\ncostly simulations, so approximation functions are used in their place. This makes\nit possible to evaluate the function values of the most promising individuals only,\ninstead of wasting time evaluating the function values of individuals that are un-\nlikely to result in an improvement. A common approximation method are Gaussian\nprocesses (or Kriging), which yield a prediction in the form of a 1-D normal dis-\ntribution. This predictive distribution is learned from previously evaluated points\nand quantifies for a given new point how likely it is that certain function values will\nbe obtained. In the context of computer experiments, the statistical assumptions\nof such metamodels were discussed in Sacks et al. [11].\nOptimization methods for expensive function values based on Gaussian pro-\ncesses date back to the Lithuanian school of global optimization [12]. More re-\ncently they have been refined and gained popularity in the context of optimization\nwith expensive computer experiments [13] under names such as efficient global op-\ntimization (EGO) and expected improvement algorithm [14]. Even more recently,\ndifferent expected improvement formulations for multiobjective optimization have\nbeen developed and were compared in Wagner et al [7]. Among these, the ex-\npected hypervolume improvement (EHVI) turns out to have desirable theoretical\nproperties. The EHVI was first suggested in [26] and represents the expected im-\nprovement in the hypervolume measure relative to the current approximation of\nthe Pareto front [7] given the probability distribution of possible function values.\nThe hypervolume measure itself is a common measure used to determine the qual-\nity of a set of solutions to a multiobjective optimization problem [8] and can be\napplied without a priori knowledge of the Pareto front, which makes the EHVI a\nnatural quality measure to use in multiobjective surrogate-assisted optimization.\nThe calculation of the EHVI has so far been a problem. Monte Carlo integra-\ntion can solve the issue of computing the EHVI directly, but to get an accurate\napproximation out of Monte Carlo integration is slow. An exact calculation ap-\nproach exists for the bi-objective case, but it is slow as well (time complexity in\nO(n3logn)). This thesis aims to increase the speed of the exact calculation of the\nEHVI in two dimensions, as well as provide a method of calculating it in higher\ndimensions. Its implementation will be validated with results from Monte Carlo\nintegration. The empirical performance of directly calculating the EHVI in the\nthree-dimensional case will also be analyzed in order to show the feasibility of us-\ning direct calculations in place of Monte Carlo integration. The main contribution\nof this paper is therefore to make the EHVI computation both exact and fast, so\n2"},{"page":3,"text":"that it can be used in Gaussian-process assisted global optimization algorithms.\nThe article is structured as follows: Section 2 introduces important definitions\nand technical preliminaries. Section 3 summarizes the related work. Section 4\ncontains a proof that the exact calculation of the bi-objective EHVI can be done\nin O(n2) as well as a lower bound on the worst case complexity of \u03a9(nlogn).\nThe proof of the upper bound is by constructing an algorithm. The implemen-\ntation of this algorithm is then empirically compared to the naive (O(n3logn))\nimplementation. Section 5 describes the details of the new, general algorithm for\ncalculating the expected hypervolume improvement in more than two dimensions,\nand Section 6 describes an exact method for determining the tri-objective EHVI\nwith time complexity O(n3). Section 7 describes the results of empirical tests of\nimplementations of the algorithms described in Sections 5 and 6, both to validate\nthe correctness of the implementations and to measure their performance. Finally,\nSection 8 contains concluding remarks and an outline of promising directions for\nfuture research.\n2Preliminaries\nWithout loss of generality, we will consider maximization of m \u2265 1 objective\nfunctions f1 : X \u2192 R, ..., fm : X \u2192 R. A distinction is made between the\ndecision space X of alternative solutions and the objective space Rmwhere the\nimages of points in X under f are represented. However, the attention of this study\nwill be on points and probability distributions of points in the objective space.\nThe a posteriori approach of multiobjective optimization is concerned with\nfinding (approximations to) the Pareto front, that is: the set of solutions in the\nobjective space that are not dominated in the Pareto dominance relation [28].\nThe hypervolume indicator is a quality measure for Pareto approximation sets\n[29]. Among the performance measures being used in Pareto optimization, it has\nsome favorable properties. First of all it can be used to compute both absolute and\nrelative improvements of a Pareto front approximation without a priori knowledge\nof the Pareto front and it is compliant with Pareto dominance [30]. Furthermore,\nits maximization yields a set of Pareto optimal points distributed across the Pareto\nfront [31, 32, 33].\nThe hypervolume indicator of a finite set of points P \u2282 Rmwith respect to a\nuser-defined reference point r is defined as the Lebesgue measure of the hypervol-\nume covered by the boxes that have an element of P as their upper corner and a\nreference point r as their lower corner. Thereby it measures the size of the dom-\ninated space of P cut from below by a reference point. The reference point must\nbe chosen in a such a way that it is dominated by all points in P, and ideally also\nby all points of the Pareto front.\nThe set containing the part of the objective space that is dominated by the\npoints in P will be referred to as DomSet(P). The hypervolume contribution of a\n3"},{"page":4,"text":"point p \u2208 P is the difference in dominated hypervolume between P \\ {p} and P.\nThe hypervolume contribution of a set of points S \u2286 P is defined analogously, as\nthe difference between P \\ S and P.\nThe hypervolume improvement of a point p \/ \u2208 P with respect to P is defined as\nthe hypervolume contribution of p with regards to P \u222a {p}, i.e. the increment of\nthe hypervolume indicator after p is added to P.\nNote that in the entire article we consider a fixed reference point that is dom-\ninated by all points in the Pareto front approximations. The choice of reference\npoint is an important issue by itself, which we do however not adress here.\n2.1The Expected Hypervolume Improvement\nIn global optimization with expensive function evaluations it is common to pre-\ndict function values using statistical methods such as Gaussian processes [13].\nSuch methods provide a predictive distribution of possible outcomes of the precise\nevaluation of the vector valued objective function in form of the parameters of a\nprobability density function (PDF) over all possible outcomes. In case of Gaus-\nsian processes or Kriging approximations the predictions are given by multivariate\nnormal distributions[26].\nThe expected hypervolume improvement (EHVI) is the expected value of the\nhypervolume improvement of a new candidate point in X, given its predictive dis-\ntribution function (PDF) over points in the objective space. The general formula\nfor the EHVI with respect to a mutually non-dominated set P is[26]:\n?\np\u2208Rm\nHI(p,P) \u00b7 PDF(p)dp(1)\nThe EHVI is a generalization of the classical expected improvement (EI) criterion\n?\u221e\nFor a given mean and standard deviation vector of an independently distributed\npredictive distribution, the EHVI is monotonic with respect to the mean value [7]\nand, at least for m \u2264 2, also w.r.t. the variance[9]. It has been used as an infill\ncriterion for multiobjective EGO in multiobjective optimization in various studies\n[3, 4, 5] but its application so far has been confined to the bi-objective case and\nthe computation of the EHVI was criticized to be computationally expensive as\ncompared to more simple generalizations of the EI[7].\nIn [9], a formula is derived for exactly calculating the EHVI for m = 2 inde-\npendent and identically distributed normal PDFs. The expression in [9] in general\ndoes not yield the exact result for m > 2, as will be shown later.\nGiven these preliminaries the general problem discussed in this article can now\nbe defined concisely:\ny=f?max{0,y \u2212 f?}PDF(y)dy used in model-assisted single objective optimiza-\ntion, where f?denote the function value of the currently best solution[12, 13].\n4"},{"page":5,"text":"Problem 1 Given a finite set of points P \u2282 Rm, a reference point r and a pre-\ndictive independent distributed multivariate normal PDF, given by its mean value\n\u00b5 \u2208 Rmand standard deviations \u03c3 \u2208 Rm, how can the EHVI be computed and how\ncan the EHVI be computed efficiently?\n2.2One-Dimensional Expected Improvement and its Decomposition\nIn order to calculate the EHVI, we will need to calculate many integrals that have\nthe form of a partial one-dimensional improvement. In [9], a function was derived\nthat could be used for that purpose.\nIn the following definition we recall the notion of standard normal distribution\nand normal distribution. Moreover we introduce a useful shorthand named \u03c8.\n1. The function \u03c6(s) = 1\/\u221a2\u03c0e\u22121\nof the standard normal distribution and \u03a6(s) =\nmulative probability distribution function of the standard normal distribution.\nThe general normal distribution with mean \u00b5 and variance \u03c3 has as density\nthe function \u03c6\u00b5,\u03c3(s) =\nDefinition 1\n2s2,s \u2208 R is the density function\n1\n2\n1 + erf\n??\ns\n\u221a2\n??\nis the cu-\n1\n\u03c3\u221a2\u03c0e\u22121\n2(s\u2212\u00b5\n\u03c3)2,s \u2208 R. The cumulative distribution\nfunction of the general normal distribution is: \u03a6\u00b5,\u03c3(s) =1\n2\n?\n1 + erf\n?\ns\u2212\u00b5\n\u03c3\u221a2\n??\n.\n2.\n\u03c8(a,b,\u00b5,\u03c3) := \u03c3 \u00b7 \u03c6(b \u2212 \u00b5\n\u03c3\n) + (a \u2212 \u00b5)\u03a6(b \u2212 \u00b5\n\u03c3\n)(2)\nRemark: it is easy to check that \u03c6\u00b5,\u03c3(s) =1\n\u03c3\u03c6(s\u2212\u00b5\n\u03c3) and \u03a6\u00b5,\u03c3(s) = \u03a6(s\u2212\u00b5\n\u03c3).\nIntegrals of the form\n\u03a6(b\u2212\u00b5\n\u2212\u221e can be written as the difference of two such integrals, allowing partial expected\nimprovements over an interval [l,u) \u2282 R, l \u2265 f?to be calculated. Moreover one\ncan easily see that this difference can be neatly expressed in terms of \u03c8:\n?\u221e\nz=b(z \u2212 a)1\n\u03c3\u03c6(z\u2212\u00b5\n\u03c3) are equal to \u03c3\u03c6(b\u2212\u00b5\n\u03c3\n+ (\u00b5 \u2212 a)[1 \u2212\n\u03c3)]. Integrals whose upper limit is less than \u221e and lower limit greater than\nu\n?\n?\nz=l\n(z \u2212 f?)1\n\u03c3\u03c6(z \u2212 \u00b5\n\u03c3\n)dz\n=\n\u221e\nz=l\n(z \u2212 f?)1\n\u03c3\u03c6(z \u2212 \u00b5\n\u03c3\n)dz \u2212\n\u221e\n?\nz=u\n(z \u2212 f?)1\n\u03c3\u03c6(z \u2212 \u00b5\n\u03c3\n)dz\n= \u03c8(f?,l,\u00b5,\u03c3) \u2212 \u03c8(f?,u,\u00b5,\u03c3)\nThe value f?in this case is the currently best function value.\n5"},{"page":6,"text":"In the rest of this thesis we will use the abbreviations \u03c6x(s) := \u03c6\u00b5x,\u03c3x(s)(=\n\u03c3x\u03c6(s\u2212\u00b5x\nvariance of the normal distribution associated to the point x in the search space.\nAnalogously, we use abbreviations \u03c6y,\u03a6yand \u03c6z,\u03a6zfor the y and the z coordinate.\n1\n\u03c3x)) and \u03a6x(s) := \u03a6(s)(= \u03a6(s\u2212\u00b5x\n\u03c3x)), where \u00b5x and \u03c3x are the mean and\n3Related Work\nThe use of the one-dimensional expected improvement to solve engineering prob-\nlems with expensive-to-evaluate objective functions was initially proposed by Mockus\net al.[12] and then later reintroduced by Jones et al. in [13]. Since then it has\nbeen widely used in global optimization with expensive-to-evaluate functions. It\nhas been shown to converge to the global optimum for the single objective case\nand a subclass of continuous functions [14].\nGeneralizing the one-dimensional expected improvement algorithm to multiob-\njective optimization is still a very new area of research. Besides the aforementioned\nEHVI, first published in [26], various other solutions have been proposed:\n\u2022 Chebyshev scalarization with dynamically changing weights [15].\n\u2022 Scalarization by using the distance from the centroid of the probability dis-\ntribution to the Pareto approximation set [16].\n\u2022 The hypervolume improvement for candidate points, calculated based on the\nupper confidence bound of the meta-model prediction [17].\nMoreover, in Kumano et al. [27] it was proposed to use a vector of expected\nimprovements for the single objective functions, instead of a scalar measure.\nExpected improvement has been studied as an infill criterion in different appli-\ncation fields, such as bioinformatics [15], mechanical engineering [17] and aerospace\ndesign [16]. Also the EHVI was already applied in practice for the tuning of con-\ntrollers in sewage treatment plants [3], in mechanical engineering [4] and quantum\ncontrol [2]. As compared to other indicators EHVI was found to have monotonicity\nin mean values [7] and variance[9] and yielded high accuracy optima approxima-\ntions. However, its computation is so far limited to the biobjective case and the\ntime complexity of existing exact algorithms is still very high (O(n3logn), see[9].\nRecently, Couckuyt et al. [6] published an algorithm that is faster based on\nempirical tests. The complexity of this algorithm is not reported. It follows a\nheuristic block partitioning scheme for computing the improvement contribution\nof each cell and we conjecture its total complexity to be in \u03a9(n3) for two and in\n\u03a9(n4) in three objectives.\n6"},{"page":7,"text":"4Calculating the 2-D Expected Hypervolume Improve-\nment\nFirstly, an efficient exact algorithm for the computation of the EHVI in two di-\nmensions will be discussed.\nLet P denote a set of n mutually non-dominated points in the two-dimensional\nplane. P is the currently best Pareto front approximation. Furthermore, let r \u2208 R2\ndenote a reference point which is dominated by every point in P. The aim is to\ncalculate the expected hypervolume improvement for a point p in the decision\nspace for which we have the mean (\u00b5x,\u00b5y) and standard deviation (\u03c3x,\u03c3y) of a\npredictive distribution.\nIn the two-dimensional case, calculating the EHVI for p exactly can be done\nby piecewise integration over a set of half-open rectangular interval boxes (cells)\nformed by the horizontal and vertical lines going through the points in P and\nthrough r. The final EHVI is the sum of the contributions calculated for all grid\ncells. See Figure 1 for a visualization of the grid.\n(0,0)\n(1,0)\n(2,0)\n(3,0)\n(0,1)\n(1,1)\n(2,1)\n(3,1)\n(0,2)\n(1,2)(2,2)(3,2)\n(0,3)(1,3)\n(2,3)\n(3,3)\n(\u221e,\u221e)\nr\nx\ny\nFigure 1: An example of the interval boxes for a small population P. Checkered boxes fall in\nthe dominated hypervolume of P. Therefore their contribution to the integral will be 0, and no\ncalculation will be necessary for these boxes.\nIndividual grid cells will be denoted by C(a,b), where 0 \u2264 a \u2264 n and 0 \u2264 b \u2264 n.\nLet Q = P\u222a{(\u221e,ry)}\u222a{(rx,\u221e)}, with Qxdenoting Q sorted in order of ascending\nx coordinate, and Qydenoting Q sorted in order of ascending y coordinate. Let\nC be the set of grid cells representing the interval boxes. The numbers a and\nb represent positions in the sorting order of Q, starting with 0. Then, a is the\nposition of elements of Qxand b is the position of elements of Qy. The lower left\ncorner of a cell will have the coordinates (Qx\nthe grid cell will have the coordinates (Qx\nNote, that due to the characteristics of mutually non-dominated points in the\ntwo-dimensional plane, it is not necessary to sort Q twice in order to determine\na.x,Qy\na+1.x,Qy\nb.y). The upper right corner of\nb+1.y).\n7"},{"page":8,"text":"Qxand Qy. Sorting P in order of ascending x coordinate is equivalent to sorting\nit in order of descending y coordinate. It follows that Qx\nWhen dividing the grid in the way described above, (n + 1)2interval boxes\nare formed. However, if the upper right corner of an interval box is dominated\nby or equal to some point in P, its contribution will be zero, and no calculation\nwill need to be done for that interval box. Note that if the upper right corner is\nnot dominated by P then the lower left corner is neither. These interval boxes\nare represented by a grid cell C(a,b) which is within the dominated hypervolume\nof P. The remaining cells, Cstairs, are formed by cells for which this is not the\ncase, meaning that \u2200(C(a,b) \u2208 Cstairs,p \u2208 P) : p.x > Qx\nanalogously, p.y > Qy\nDue to the definition of Q, we know that for p \u2208 P it holds that p = Qx\nQy\ndominates C(a,b). From this we get the following equivalence: a \u2265 n\u2212b \u21d4 C(a,b)\nis dominated by some point p \u2208 P. Thus Cstairsconsists of all cells satisfying\na \u2265 n \u2212 b. There are\non the complexity of any algorithm which iterates over these interval boxes.\nIf we call the lower corner of the cell l and the upper corner u, the contribution\nof a grid cell to the integral is defined as follows:\nk= Qy\nn+1\u2212k.\na.x \u21d2 Qy\nb.y \u2265 p.y and,\nb.y \u21d2 Qx\na.x \u2265 p.x.\nk=\nn+1\u2212kfor some 0 < k \u2264 n. Furthermore k > a and n+1\u2212k > b, if and only if p\n(n+1)(n+2)\n2\nof such cells, resulting in a lower bound of O(n2)\nuy\n?\npy=ly\nux\n?\npx=lx\nHI(p)\u03c6x(px)\u03c6y(py)dpxdpy\nC(a,b)\n(\u221e,\u221e)\nr\nx\ny\np\nSminus\nFigure 2: Within an integration region C(a,b), the hypervolume improvement of candidate\npoints p is equal to (p.x\u2212Qy\nrepresents (p.x \u2212 Qy\nrectangle.\nb+1.x)\u00b7(p.y\u2212Qx\nb+1.x) \u00b7 (p.y \u2212 Qx\na+1.y)\u2212Sminus. In this example, the yellow rectangle\na+1.y), and S consists of the two points within the yellow\nDominated cells have a contribution of 0 to the integral, and for cells which\nare non-dominated, HI(p) can be calculated as a rectangular volume from which\n8"},{"page":9,"text":"a correction term is subtracted. See Figure 2 for a visual representation. The\nintegral for these cells can be calculated as follows, as was described in more detail\nin [9]:\nuy\n?\n?\npy=ly\nux\n?\n?\npx=lx\n(px\u2212 rx)(py\u2212 ry) \u2212 Sminus\u03c6x(px)\u03c6y(py)dpxdpy\n=\nuy\npy=ly\nux\npx=lx\nuy\n?\npy=ly\n(px\u2212 vx)(py\u2212 vy)\u03c6x(px)\u03c6y(py)dpxdpy\n\u2212\nux\n?\npx=lx\nSminus\u03c6x(px)\u03c6y(py)dpxdpy\n= (\u03c8(vx,lx,\u00b5x,\u03c3x) \u2212 \u03c8(vx,ux,\u00b5x,\u03c3x)) \u00b7 (\u03c8(vy,ly,\u00b5y,\u03c3y) \u2212 \u03c8(vy,uy,\u00b5y,\u03c3y))\n\u2212 Sminus\u00b7 (\u03a6x(ux) \u2212 \u03a6x(lx)) \u00b7 (\u03a6y(uy) \u2212 \u03a6y(ly))\nThe last step is motivated by Subsection 2.2 and the application of Fubini\u2019s\nTheorem [23]. It can be seen that the formula is of the form c1\u2212Sminus\u00b7c2, where\nc1and c2are calculations which are performed in constant time with respect to n\nfor a single cell.\nThe correction term Sminusis equal to the hypervolume contribution of S \u2286 P,\nwhere S consists of those points dominated by or equal to the lower corner of\nthe cell. Calculating the dominated hypervolume of a set in the two-dimensional\nplane has a time complexity of O(nlogn). This complexity results from needing\nto find the neighbors of each point in order to calculate its contribution to the\nhypervolume. Sorting the set has a time complexity of O(nlogn), after which the\ndominated hypervolume calculation itself is done in O(n) by iterating over each\npoint and performing an O(1) calculation using the points that come before and\nafter it in the sorting order. When calculating Sminus, the points for which the\ndominated hypervolume is to be calculated come from P, which was already sorted.\nThis brings the complexity of this step down to O(n), but it can be brought down\nto O(1) when the order of calculations is chosen carefully, giving the algorithm a\ntotal complexity of O(n2).\nThe points of P dominated by or equal to the lower corner of C(a,b), which\ndefine S, are those points satisfying the following inequalities:\np \u2208 P,Qx\na.x \u2265 p.x,Qy\nb.y \u2265 p.y\nBecause of the sorting order and definition of Qxand Qy, S can be described\nequivalently as follows. The set S is empty, if a = n \u2212 b (the lowest value of a for\n9"},{"page":10,"text":"(\u221e,\u221e)\nr\nx\ny\n(3,3)\n(2,3)\n(1,3)\n(0,3)\n(3,2)\n(2,2)\n(1,2)\n(3,1)\n(2,1)\n(3,0)\nS = {}\nS = {}\nS = {3}\n3\n2\n1\nS = {}\nS = {2}\nS = {2,3}\nS = {}\nS = {1}S = {1,2}\nS = {1,2,3}\nFigure 3: An example showing the order of iterations which allows the hypervolume contribution\nof S to be updated in constant time.\nwhich a \u2265 n \u2212 b), otherwise (a > n \u2212 b) S is formed by an uninterrupted range\nwith Qx\nA row in Cstairsis a set of cells Cstairs(a,b) where b is the same. In a single row,\nS will always be either empty or have Qx\none point to the range of points in P which falls between Qx\nmakes it possible to iterate over all cells in Cstairswhile adding no more than one\npoint to S per iteration. We will do this as follows:\nWe will start iterating over each row of Cstairsat its first cell, where a = n\u2212b.\nIn this cell, S = \u2205 and Sminus= 0. For each iteration within a row after the first\none, we add 1 to a and add the point Qx\nwhich shows the order of operations and the contents of S during each step.\nAlthough the above description refers to \u2018adding points to S\u2019, we only need to\nkeep track of the first and last points of S in between algorithm iterations. When\na new point is added to S, Sminusincreases by the area covered by the rectangle\nfrom (Qx\naddition of a point to S, only the left neighbor of the first element of S, the last\nelement of S and the right neighbor of the last element of S are needed. Figure 4\nshows an example of this process. This can be done in constant time in any data\nstructure which allows the neighbors of a point to be looked up in constant time:\nwhenever a is incremented, Qx\nb is incremented, the new Qx\nthen start iterating through values of a at the beginning of the row, Qx\nthe new Qx\nfirst cell in a row of Cstairs.\nWe have shown that the upper bound on the complexity of determining the\nexpected hypervolume improvement is O(n2). We can also show that the worst-\ncase complexity can be no better than O(nlogn). If the standard deviation of\n(n+1\u2212b)as its first element and Qx\naas its last element.\naas its last element. Adding 1 to a adds\n(n+1\u2212b)and Qx\na. This\nato S. For an example, refer to Figure 3,\n(n\u2212b).x,Qx\na+1.y) to (Qx\na.x,Qx\na.y). Therefore, to update Sminus after the\nabecomes Qx\n(n\u2212b)becomes its left neighbor, Qx\na+1and Qx\na+1becomes Qx\na+2. Whenever\n(n\u22121\u2212b), and as we will\nabecomes\n(n\u2212b)in the\n(n\u2212b)as well because we have established earlier that Qx\na= Qx\n10"},{"page":11,"text":"a candidate point\u2019s predictive distribution is 0 and the mean value vector is a\npoint which dominates all points in P, then the problem of calculating its EHVI\nreduces to calculating the hypervolume that will be dominated by pcandidateminus\nthe hypervolume dominated by P. If it was possible to solve this calculation with\nlower complexity than O(nlogn), then it would also be possible to reduce the\ncalculation of P\u2019s hypervolume to the problem of calculating the EHVI of a point\nthat dominates P, and it has already been proven in [10] that the complexity of\ncalculating the hypervolume of a set of points in the 2-D plane is in \u0398(nlogn).\n11"},{"page":12,"text":"(0,0)\n(1,0)\n(2,0)\n(3,0)\n(0,1)\n(1,1)\n(2,1)\n(3,1)\n(0,2)\n(1,2)\n(2,2)\n(3,2)\n(0,3)\n(1,3)\n(2,3)\n(3,3)\n(\u221e,\u221e)\nr\nx\ny\n(4,0)\n(4,1)\n(4,2)\n(4,3)\n(0,4)\n(1,4)\n(2,4)\n(3,4)\n(4,4)\n(0,0)\n(1,0)\n(2,0)\n(3,0)\n(0,1)\n(1,1)\n(2,1)\n(3,1)\n(0,2)\n(1,2)\n(2,2)\n(3,2)\n(0,3)\n(1,3)\n(2,3)\n(3,3)\n(\u221e,\u221e)\nr\ny\nx\ny\n(4,0)\n(4,1)\n(4,2)\n(4,3)\n(0,4)\n(1,4)\n(2,4)\n(3,4)\n(4,4)\n(0,0)\n(1,0)\n(2,0)\n(3,0)\n(0,1)\n(1,1)\n(2,1)\n(3,1)\n(0,2)\n(1,2)\n(2,2)\n(3,2)\n(0,3)\n(1,3)\n(2,3)\n(3,3)\n(\u221e,\u221e)\nr\nx\ny\n(4,0)\n(4,1)\n(4,2)\n(4,3)\n(0,4)\n(1,4)\n(2,4)\n(3,4)\n(4,4)\n(0,0)\n(1,0)\n(2,0)\n(3,0)\n(0,1)\n(1,1)\n(2,1)\n(3,1)\n(0,2)\n(1,2)\n(2,2)\n(3,2)\n(0,3)\n(1,3)\n(2,3)\n(3,3)\n(\u221e,\u221e)\nr\nx\n(4,0)\n(4,1)\n(4,2)\n(4,3)\n(0,4)\n(1,4)\n(2,4)\n(3,4)\n(4,4)\nFigure 4: An example showing how Sminuschanges during each iteration within a single row.\nThe rectangular strip which is added after each iteration can be calculated with knowledge of\nthree points: the point Qx\nlower corner, and the point Qx\ndoes not change, the hypervolume covered by the older points in S stays the same and does not\nhave to be re-calculated.\nais its upper corner, the point Qx\n(n\u2212b)provides the x coordinate of its lower corner. Because Qx\na+1provides the y coordinate of its\n(n\u2212b)\n12"},{"page":13,"text":"4.1Empirical Performance\nAs an additional verification of the correctness of the algorithm presented above,\ntwo implementations were written in C++. The first used the constant-time up-\ndate scheme, and the second did not: instead of using the constant-time update\nscheme, Sminuswas calculated by first finding the set of points S by checking each\npoint in P to see if it was dominated, and then calling a separate function on S\nto calculate the hypervolume of this set of points.\nThe expected hypervolume improvement calculated using these implementa-\ntions was identical for all test problems, but their speed was not. See Figure 5 for\nthe empirical performance on a simple test where P consisted of n different points\non a diagonal Pareto front. From this, it appears that using the constant-time\nupdate scheme becomes worthwhile for n > 20, though results might vary slightly\ndepending on implementation and system details.\nFigure 5: Time needed to calculate the expected hypervolume improvement in 2-D, averaged\nover 10 runs. The times reported were measured on an Intel i7 quadcore CPU with 2.1 GHz\nclockspeed, and the code was compiled using GNU under Windows with the optimization level\nset to O3.\n13"},{"page":14,"text":"5Calculation of the Higher-Dimensional Expected Hyper-\nvolume Improvement\nThe algorithm given in [9] for exactly calculating the expected hypervolume im-\nprovement is not correct when the dimensions is higher than 2. This is because the\nshape of the hypervolume improvement becomes more complex when the number\nof dimensions increases. We will therefore derive a new formula by first decom-\nposing the calculation into parts with less complex shapes, and then simplifying\nthe resulting formula for the sake of more convenient calculation.\n5.1Decomposition into Cells\nIn higher dimensions, the search space can be divided into cells the same way it\nis done in two dimensions, except instead of the boundaries being given by lines\ngoing through the points in P and the reference point r, now the cells are separated\nfrom each other by (m \u2212 1)-dimensional hyperplanes (where m is the number of\nobjective functions).\nEach cell is denoted by C(a1,a2,...,am) where a1 through am are integers\nfrom 0 to |P| denoting the labeling of the cell.\nand right upper corner u of the cell with label a1, ..., amare defined as follows:\nLet P?= {r} \u222a P \u222a (\u221e,...,\u221e)Tand let sd[0],...,sd[|P| + 1] denote the d-th\ncomponents of the vectors in P?sorted in ascending order. Then ld= sd[ad] and\nud= sd[ad+ 1] for d = 1,...,m. In other words, corners of this cell complex are\ngiven as the intersection points of all axis-parallel m \u2212 1 dimensional hyperplanes\nthrough points in P?.\nThe hypervolume improvement of a new point p with respect to the current\nPareto front approximation P is given by the function HyperVolume(A \\ DomSet(P)),\nwhere A is the dominated hypervolume covered by p. This is the same as calcu-\nlating HyperVolume(A)\u2212HyperVolume(DomSet(P) \u2229 A). We will denote the set\nof dimensions by D = {1,2,...,m}. We can decompose the calculation of the\nhypervolume improvement of a point p \u2208 C(a1,a2,...,am) as follows:\n?\nIC:= HyperVolume(AC) \u2212 HyperVolume(DomSet(P) \u2229 AC)\nThen the left lower corner l\nHI(p) =\nC\u2286D\nIC, where\n14"},{"page":15,"text":"A1,2,3\nA1,2\nA2\nA1\nA3\nA1,3\nA2,3\nf3\nf2\nf1\nFigure 6: An example showing how the quantities AC for C \u2286 {1,2,3} are defined in a three-\ndimensional objective space. A\u2205is hidden within the rectangular volume. The checkered volumes\nrepresent the volume dominated by the points in the Pareto front approximation.\nand ACare given by:\nAC:=\n\uf8ee\n\uf8f0\nld\nrd\n\uf8ef\uf8ef\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\nv1\nv2\n...\nvm\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8,\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\nw1\nw2\n...\nwm\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8\n\uf8f9\n\uf8fb\n\uf8fa\uf8fa\nvd=\n?\n?\nif d \u2208 C\nif d \/ \u2208 C\nif d \u2208 C\nif d \/ \u2208 C\nwd=\npd\nld\nSee Figure 6 for an example in 3 dimensions.\nIn the above formula Hypervolume denotes the Lebesgue measure of Rm. Note\nthat it can happen that the dimension of AC is strictly less than m. In this\ncase Hypervolume(AC) = 0. We can make a similar remark about Hypervolume(\ndomSet(P) \u2229 AC).\nThe values of rd and ld are constant for all points that fall within a given\ninterval box (cell): r is the reference point and is, of course, always constant,\nwhile l represents the position of the lower corner of the cell. From this, it follows\nthat ICrepresents the portion of the hypervolume improvement which is constant\nwith regards to the values of pd,d \/ \u2208 C, and which is variable with regards to the\nvalues of pd,d \u2208 C. In fact, it is linearly related to these values. This is a direct\nconsequence of the way the cell boundaries are defined:\nLet SecCbe the cross-section of DomSet(P) \u2229 ACwhich goes through p. This\ncross-section is defined by a projection to the dimensions not in C (if C consists\n15"},{"page":16,"text":"of k dimensions, the slice will be (m\u2212k)-dimensional as a result). The projection\nof DomSet(P) uses only those points in P for which the function values in the\ndimensions given by C are larger than the corresponding function values of p. We\nshall call this selection P?. No points in P can fall between cell boundaries in any\ndimension, so the composition of P?must be the same for all points within a cell.\nThe projection of ACto the dimensions not in C is constant for all points within\na cell as well, because the coordinates defining AC are independent of p in all\ndimensions not in C. HyperVolume(SecC) is constant as a result \u2013 note that here\nHypervolume is the Lebesque measure of Rm\u2212k. Because ACdoes not span across\ncell boundaries in the dimensions in C, HyperVolume(DomSet(P) \u2229 AC) is equal\nto the hypervolume of SecCmultiplied by the length of ACin each dimension in\nC, and those lengths are given by (pd\u2212 ld) with d \u2208 C.\nThere is one quantity ICfor which C = D. This quantity IDis special because\nit is linearly related to all values of p. ID falls entirely within the cell, and as\nsuch, instead of projecting P onto a zero-dimensional space, it can simply be\nsaid that HyperVolume(DomSet(P) \u2229 AD) = HyperVolume(AD) if the cell is not\ndominated, and HyperVolume(AD\u2229 DomSet(P)) = 0 if it is. Therefore, ID =\nHyperVolume(AD) for non-dominated cells.\nBy decomposing the calculation of the hypervolume improvement, we can use\nthe sum rule to decompose the calculation of a cell\u2019s contribution to the EHVI as\nwell.\n?\np=l\nu\n?\nC\u2286D\nIC\u00b7 PDF(p)dp =\n?\nC\u2286D\nu\n?\np=l\nIC\u00b7 PDF(p)dp\nIC is calculated as the product of a constant and a set of values which are\nlinearly related to exactly one coordinate of p, therefore we can first factor out\nthe calculation of the constant part. The PDF consists of independent normal\ndistributions, allowing the probability distributions for dimensions not in C (in\nwhich ICis constant) to be factured out as well. An integral consisting solely of\na normal distribution can be exactly calculated using the cumulative probability\ndistribution function \u03a6 to calculate the probability that a point is within range of\nthe cell.\nu\n?\np=l\nIC\u00b7 PDF(p)dp = Iconst\nC\n\u00b7\nuC\n?\npC=lC\n?\nc\u2208C\n(pc\u2212 lc) \u00b7\n?\nc\u2208C\n\u03c6cdpC\u00b7\n?\nc\/ \u2208C\n(\u03a6c(uc) \u2212 \u03a6c(lc))\nThe integral that remains is a box-shaped expected improvement where each\ndimension is independent. Fubini\u2019s theorem [23] states that iterated integration,\nperformed in any order, can be used to calculate a multiple integral under the\ncondition that the multiple integral is absolutely convergent. The partial integrals\n16"},{"page":17,"text":"making up the cell\u2019s contribution to the EHVI all converge to finite numbers,\nso we can safely use iterated integration. The result is a product of expected\nimprovements, which are captured in the \u03c8 function described earlier:\nu\n?\np=l\nIC\u00b7 PDF(p)dp =(3)\nIconst\nC\n\u00b7\n?\nc\u2208C\n(\u03c8(lc,lc,\u00b5c,\u03c3c) \u2212 \u03c8(lc,uc,\u00b5c,\u03c3c)) \u00b7\n?\nc\/ \u2208C\n(\u03a6c(uc) \u2212 \u03a6c(lc)) (4)\nIt is already possible to calculate the contribution of a cell to the EHVI by sum-\nming all these terms, but the calculation can be made a bit more efficient when\ninstead of decomposing the calculation of the hypervolume, we instead only decom-\npose the calculation of the dominated hypervolume. In Section 5.2, that possibility\nwill be examined in more detail by looking at the 3-D case as an example.\n5.2Calculation of the 3-D Expected Hypervolume Improvement\nConsider that, in the 2-D case, we are able to calculate the hypervolume by in-\ntegrating over a box bounded by the dominated hypervolume and subtracting a\ncorrection term Sminus. In higher dimensions, the correction term is not a constant,\nbut we can still make use of a modified version of this technique. The hypervolume\nimprovement HI(p) is decomposed as follows, in three dimensions:\nHI(p) = I\u2205+ Ix+ Iy+ Iz+ Ixy+ Ixz+ Iyz+ Ixyz\n\uf8eb\nTogether, they form the volume of\n\uf8ed\n\uf8ee\n\uf8f0\n\uf8eb\n\uf8ed\nrx\nry\nrz\n\uf8f6\n\uf8f8,\n\uf8eb\n\uf8ed\npx\npy\npz\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\\ DomSet(P)\n\uf8f6\n\uf8f8. Instead\nof writing HI(p) as a sum of hypervolume improvements, we can also write it as\na single rectangular volume from which a dominated hypervolume is subtracted:\nHI(p) = Vol\n\uf8eb\n\uf8ed\n\uf8ee\n\uf8f0\n\uf8eb\n\uf8ed\nrx\nry\nrz\n\uf8f6\n\uf8f8,\n\uf8eb\n\uf8ed\npx\npy\npz\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n\uf8f6\n\uf8f8\u2212 Vol\n\uf8eb\n\uf8edDomSet(P) \u2229\n\uf8ee\n\uf8f0\n\uf8eb\n\uf8ed\nrx\nry\nrz\n\uf8f6\n\uf8f8,\n\uf8eb\n\uf8ed\npx\npy\npz\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n\uf8f6\n\uf8f8\nWe can then decompose the calculation of the dominated hypervolume instead\nof the calculation of the hypervolume improvement. In the following decomposition\nof the total subtracted dominated hypervolume S\u2212, each part S\u2212\nsubtracted dominated hypervolume needed to calculate IC. When p is within the\nintegration cell bounded from below by l, we get the following:\nCis equal to the\n17"},{"page":18,"text":"S\u2212= S\u2212\n\u2205+ S\u2212\n\uf8eb\nx+ S\u2212\ny+ S\u2212\nz+ S\u2212\n\uf8ee\nxy+ S\u2212\nrx\nry\nrz\nxz+ S\u2212\n\uf8eb\nlz\nyz\n\uf8f6\n= Vol\n\uf8edDomSet(P) \u2229\n+ (px\u2212 lx) \u00b7 Area\n+ ...\n+ (px\u2212 lx) \u00b7 (py\u2212 ly) \u00b7?Max(rz,\u03c0z(\u03c3x>lx,y>ly(P))) \u2212 rz\n+ ...\n\uf8f0\n\uf8eb\n\uf8ed\n\uf8f6\n\uf8f8,\n\uf8ed\nlx\nly\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\uf8f8\n?\nDomSet(\u03c0yz(\u03c3x>lx(P))) \u2229\n??ry\nrz\n?\n,\n?py\npz\n???\n?\nBy abuse of language we use (Max(rz,\u03c0z(\u03c3x>lx,y>ly(P))) instead of the following\ncorrect notation: Max({rz} \u222a \u03c0z(\u03c3x>lx,y>ly(P)). Similar notations are also used in\nthe sequel.\nThe first thing to note is that if rz\u2265 Max(rz,\u03c0z(\u03c3x>lx,y>ly(P))), S\u2212\nanalogous cases are true for S\u2212\nif r = v, all three quantities are 0:\n\uf8ee\nMax(rz,\u03c0z(\u03c3x>lx,y>ly(P)))\nxy= 0. The\nxzand S\u2212\nyz, allowing us to define a point v for which,\nv =\n\uf8f0\n\uf8eb\n\uf8ed\nMax(rx,\u03c0x(\u03c3y>ly,z>lz(P)))\nMax(ry,\u03c0y(\u03c3x>lx,z>lz(P)))\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\nThe bounding box bounded by v from below and p from above contains the\nentire volume of HI(p). This allows us to use v in place of r and rewrite our\ninitial equation in a way that reduces the number of components from 8 to 5:\nHI(p) = Vol\n\uf8eb\n\uf8eb\n\uf8ed\n\uf8edDomSet(P) \u2229\n\u2212 (px\u2212 lx) \u00b7 Area\n\uf8ee\n\uf8f0\n\uf8eb\n\uf8ed\nvx\nvy\nvz\n\uf8f6\n\uf8f8,\n\uf8eb\n\uf8ed\npx\npy\npz\n\uf8f6\n\uf8ee\n\uf8f8\n\uf8f0\n\uf8f9\n\uf8eb\n\uf8fb\n\uf8ed\n\uf8f6\n\uf8f8\nvx\nvy\nvz\n\u2212 Vol\n\uf8f6\n\uf8f8,\n\uf8eb\n\uf8ed\nlx\nly\nlz\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n\uf8f6\n\uf8f8\n?\n?\n?\nDomSet(\u03c0yz(\u03c3x>lx(P))) \u2229\nDomSet?\u03c0xz\nDomSet(\u03c0xy(\u03c3z>lz(P))) \u2229\n??vy\n??vx\n??vx\nvz\n?\n?\n?\n,\n?ly\n,\n?lx\nlz\n???\n???\n???\n\u2212 (py\u2212 ly) \u00b7 Area\n?\u03c3y>ly(P)??\u2229\nvz\n?lx\nlz\n\u2212 (pz\u2212 lz) \u00b7 Area\nvy\n,\nly\n18"},{"page":19,"text":"The component of the EHVI integral corresponding to Vol\n\uf8eb\n\uf8ed\n\uf8ee\n\uf8f0\n\uf8eb\n\uf8ed\nvx\nvy\nvz\n\uf8f6\n\uf8f8,\n\uf8eb\n\uf8ed\npx\npy\npz\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n\uf8f6\n\uf8f8\nis the only component in this equation which is variable in more than one dimen-\nsion, but since it is a rectangular volume, it is simply a product of one-dimensional\nimprovements:\n?\nS\u2212\nuse v as a reference point. Even without examining the corresponding integral it\nis clear that it only needs to be multiplied with the probability that a given point\nis within the cell. The formula for calculating this correction term is:\n?\nS\u2212\none coordinate of p. We will look at S\u2212\n?\nThis has to be multiplied by (px\u2212 lx). The expected value of S\u2212\nequal to a constant multiplied by the partial expected improvement of pxover the\ninterval [lx,ux). This is given by:\nc\u2208{x,y,z}\n(\u03c8(vc,lc,\u00b5c,\u03c3c) \u2212 \u03c8(vc,uc,\u00b5c,\u03c3c))\n\u2205is a constant. We will keep using the notation S\u2217\u2217\u2217, although now we will\nS\u2212\n\u2205\u00b7\nc\u2208{x,y,z}\n(\u03a6c(uc) \u2212 \u03a6c(lc))\nx, S\u2212\ny, and S\u2212\nzare not constants, but they are each linearly related to only\nxas an example:\nThe constant part of S\u2212\nDomSet(\u03c0yz(\u03c3x>lx(P))) \u2229\nxis Area\n??vy\nvz\nxis therefore\n?\n,\n?ly\nlz\n???\n.\nux\n?\npx=lx\n(px\u2212 lx)\u03c6x(px)dpx= \u03c8(lx,lx,\u00b5x,\u03c3x) \u2212 \u03c8(lx,ux,\u00b5x,\u03c3x)\nUsing a new call to \u03c8 to calculate this term is not necessary. We can use the\nfact that \u03c8 represents the function of a one-dimensional expected improvement\nover a certain range bounded from below. The partial expected improvement for\nthe region below the lower cell bound l is a constant term multiplied by the chance\nof being within the cell\u2019s range, which is captured in the equation below:\n\u03c8(vc,lc,\u00b5c,\u03c3c) \u2212 \u03c8(vc,uc,\u00b5c,\u03c3c) = \u03c8(lc,lc,\u00b5c,\u03c3c) \u2212 \u03c8(lc,uc,\u00b5c,\u03c3c)\n+ (\u03a6c(uc) \u2212 \u03a6c(lc)) \u00b7 (lc\u2212 vc)\nBoth (\u03a6c(uc) \u2212 \u03a6c(lc)) \u00b7 (lc\u2212 vc) and \u03c8(vc,lc,\u00b5c,\u03c3c) \u2212 \u03c8(vc,uc,\u00b5c,\u03c3c) were cal-\nculated earlier, so we can reuse them to easily find \u03c8(lc,lc,\u00b5c,\u03c3c)\u2212\u03c8(lc,uc,\u00b5c,\u03c3c).\nThis means that the formula for calculating the partial expected hypervolume\nimprovement of a cell will look like the following if the cell is not dominated:\n19"},{"page":20,"text":"Let\nand\n\u2206\u03c8c:= \u03c8(vc,lc,\u00b5c,\u03c3c) \u2212 \u03c8(vc,uc,\u00b5c,\u03c3c), c \u2208 {x,y,z}\n\u2206\u03a6c:= \u03a6c(uc) \u2212 \u03a6c(lc), c \u2208 {x,y,z}\n\u2212 V ol\n\uf8eb\n\uf8edDomSet(P) \u2229\n\u2212 (\u2206\u03c8z\u2212 \u2206\u03a6z\u00b7 (lz\u2212 vz))\n\u00b7 Area\n\uf8ee\n\uf8f0\n\uf8eb\n\uf8ed\nvx\nvy\nvz\n\uf8f6\n\uf8f8,\n\uf8eb\n\uf8ed\nlx\nly\nlz\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n??vy\n\uf8f6\n\uf8f8\u00b7\n?\nc\u2208{x,y,z}\n\u2206\u03a6c\n?\nDomSet(\u03c0yz(\u03c3x>lx(P))) \u2229\nvz\n?\n,\n?ly\nlz\n???\n\u00b7\n?\nc\u2208{x,y}\n\u2206\u03a6c\n\u2212 (\u2206\u03c8y\u2212 \u2206\u03a6y\u00b7 (ly\u2212 vy))\n\u00b7 Area\n?\nDomSet?\u03c0xz\n?\u03c3y>ly(P)??\u2229\n??vx\nvz\n?\n,\n?lx\nlz\n???\n\u00b7\n?\nc\u2208{x,z}\n\u2206\u03a6c\n\u2212 (\u2206\u03c8x\u2212 \u2206\u03a6x\u00b7 (lx\u2212 vx))\n\u00b7 Area\n?\nDomSet(\u03c0xy(\u03c3z>lz(P))) \u2229\n??vx\nvy\n?\n,\n?lx\nly\n???\n\u00b7\n?\nc\u2208{y,z}\n\u2206\u03a6c\nAnd it will be 0 otherwise.\n5.3Simple Higher Dimensional Expected Hypervolume Improvement\nComputation\nAlthough we are currently decomposing our integral into different quantities in\norder to calculate it, we can also calculate the sum of these quantities using a\nsingle dominated hypervolume calculation, though this has downsides which will\nbe explored later. This subsection will give the general formula for doing so. Recall\nhow we decomposed the calculation of the hypervolume improvement in Section\n5.1:\nHI(p) =\n?\nC\u2286D\nIC\nIC= HyperVolume(AC) \u2212 HyperVolume(DomSet(P) \u2229 AC)\nThis sum can be rearranged to the following:\n?\nC\u2286D\nHyperVolume(AC) \u2212\n?\nC\u2286D\nHyperVolume(DomSet(P) \u2229 AC)\n20"},{"page":21,"text":"Since the quantities AC sum to a generalized rectangular volume, we could\njust as readily calculate the total volume of A directly. This is what we did in\nSection 5.2, where the correction terms HyperVolume(DomSet(P)\u2229AC) were still\ncalculated separately. Clearly?\n(?\nHypervolume(DomSet(P) \u2229 A) =\nC\u2286D\nWe initially decomposed the HI(p) in this way in order to compute the correspond-\ning EI-integral.\nWe have determined that each partial quantity HyperVolume(DomSet(P)\u2229AC)\ndepends linearly on the dimensions which its corresponding volume AC depends\non, and is constant in the same dimensions in which AC is constant. This is\ntrue as well when HyperVolume(DomSet(P)\u2229A) is first calculated, and then split\ninto the various volumes representing DomSet(P) \u2229 AC. Because of this, we can\ncalculate an m-dimensional EHVI using only a single m-dimensional hypervolume\ncalculation per cell. We need to calculate the hypervolume improvement of each\ncell\u2019s center of mass, \u00af p.\n?ud\n\u03a6d(ud) \u2212 \u03a6d(ld)\nThe integral can be calculated as if it is an expected improvement where the\ncurrently best solution is 0. However, we already need to compute \u03c8(rd,ld,\u00b5d,\u03c3d)\u2212\n\u03c8(ld,ud,\u00b5d,\u03c3d) to calculate the component of the EHVI corresponding to A, and\nthe following equation holds:\nC\u2286DAC = A. Moreover the ACs are mutually\ndisjoint except for common boundary points. From this we get that DomSet(P)\u2229\nC\u2286DAC) = DomSet(P) \u2229 A (and the DomSet(P) \u2229 ACs are mutually disjoint\nexcept for the boundaries). Thus\n?\nHypervolume(DomSet(P) \u2229 AC).\n\u00af pd=\npd=ldpd\u00b7 \u03c6d(pd)dp\n\u03c8(0,ld,\u00b5d,\u03c3d) \u2212 \u03c8(0,ud,\u00b5d,\u03c3d)\n\u03a6d(ud) \u2212 \u03a6d(ld)\nDividing a partial expected improvement over a range [ld,ud) by the chance of\nbeing in that range (given by \u03a6d(ud)\u2212\u03a6d(ld)) gives the expected improvement of\npoints which are known to lie within that range. Adding the value of rdgives the\nexpected dth coordinate of a point in the objective space bounded from below by\nr.\nThis means that the general formula for calculating the partial expected im-\nprovement in a cell is the following if the cell is not dominated:\n=\u03c8(rd,ld,\u00b5d,\u03c3d) \u2212 \u03c8(rd,ud,\u00b5d,\u03c3d)\n\u03a6d(ud) \u2212 \u03a6d(ld)\n+ rd\nEI =\n?\nd\u2208D\n(\u03c8(rd,ld,\u00b5d,\u03c3d) \u2212 \u03c8(rd,ud,\u00b5d,\u03c3d)) \u2212 S\u2212\u00b7\n?\nd\u2208D\n(\u03a6d(ud) \u2212 \u03a6d(ld)), where\n21"},{"page":22,"text":"S\u2212= HyperVolume\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8edDomSet(P) \u2229\n\uf8ee\n\uf8f0\n\uf8ef\uf8ef\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\nr1\nr2\n...\nrm\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8,\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n\u00af p1\n\u00af p2\n...\n\u00af pm\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8\n\uf8f9\n\uf8fb\n\uf8fa\uf8fa\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8and\n\u00af pd= rd+\u03c8(rd,ld,\u00b5d,\u03c3d) \u2212 \u03c8(rd,ud,\u00b5d,\u03c3d)\n\u03a6d(ud) \u2212 \u03a6d(ld)\nAnd 0 otherwise.\n5.4 Complexity and Algorithm Details\nAny algorithm which iterates over all grid cells described in Section 5 will have\na time complexity of \u03a9(nm). This is further increased by the complexity of the\ncalculations within each grid cell. The algorithm of Section 5.3 requires an m-\ndimensional hypervolume to be calculated for each cell that is not dominated.\nCalculating a 3-dimensional hypervolume can be done in O(nlogn), which re-\nsults in a time complexity of O(n4logn). However, as will be shown in Section\n6, constant-time calculations within each grid cell are possible with O(n3) total\npreparation time, resulting in an algorithm of complexity O(n3). Similar O(nm)\nalgorithms are conjectured to exist for m > 3.\nOne important thing to note, is that the expected hypervolume improvements\nfor multiple individuals can be calculated at the same time without having to per-\nform the hypervolume calculations more than once when using the decomposition\ndescribed in Section 5.2, because the hypervolume calculations are not dependent\non the mean and standard deviation of the probability distribution. The algorithm\ndescribed in Section 5.3 does not have this advantage.\n6O(n3)-time 3-D Expected Hypervolume Improvement Cal-\nculations\nIn Section 4 we showed that calculating the 2-D expected hypervolume improve-\nment is possible with time complexity O(n2). Although the algorithm described\nin that subsection made use of characteristics of a 2-D Pareto approximation set\nwhich are not present in higher dimensions, this subsection will show that there\nis also a way to calculate the 3-D EHVI with time complexity O(n3). In other\nwords: the calculations necessary for computing the partial expected hypervolume\nimprovement of each grid cell will be performed in constant time. The trade-off is\nthat we will need O(n2) extra memory.\nThe only calculations which have a complexity higher than constant time are\nthe dominated hypervolume calculations. If we use the simple algorithm described\n22"},{"page":23,"text":"in Section 5.3, we only need to perform a single 3-dimensional hypervolume calcu-\nlation to find the correction term that we need. However, we will start out with\nthe algorithm described in Section 5.2 (without replacing r by v), because it lends\nitself better to the re-use of old hypervolume calculations. Three sets of correction\nterms are needed to calculate the partial expected hypervolume improvement of a\ncell:\n\u2022 S\u2212\nume calculation.\n\u2205, a constant correction term which requires a three-dimensional hypervol-\n\u2022 S\u2212\ntion. We will call the 2-D areas used in the calculation of these correction\nterms xslice, yslice and zslice, respectively.\nx, S\u2212\nyand S\u2212\nz, which each require a two-dimensional hypervolume calcula-\n\u2022 S\u2212\nInstead of calculating these correction terms afresh for each cell, it is possible\nto perform all necessary hypervolume calculations in only O(n3) time total. The\nfirst step is to create a data structure which allows us to see whether or not a cell\nis dominated in O(1) time. This can simply be a two-dimensional array holding\nthe highest value of z for which the cell is dominated, which we shall call Hz. A\nsimple way to fill this array is to iterate over all points q \u2208 P in order of ascending\nz value, setting the array value Hz(a1,a2) to z if q dominates the lower corner of\nC(a1,a2,0). The complexity of this operation is in O(n2n+nlogn) = O(n3). This\nonly needs to be done once, so the O(n3) time complexity does not increase the\ntotal asymptotic time complexity of computing the EHVI in 3-D. Figure 7 shows\nan example.\nxy, S\u2212\nxzand S\u2212\nyz, which requires a \u2018one-dimensional\u2019 hypervolume calculation.\nx\ny\nz\n(10,2,10)\n(7,3,8)\n(9,6,6)\n(4,10,4)\nReference point: (0,0,0)\ny\nx\n1010\n10\n10\n8\n8\n6\n6\n6\n6\n4\n0\n0\n0\n0\n0\nFigure 7: Example height array Hzfor a population consisting of 4 points, which is visualized\non the left. Cells on the outermost edge of the integration area (which stretch out to \u221e in some\ndimension) are always non-dominated.\n23"},{"page":24,"text":"Besides containing information that allows constant-time evaluation of whether\na cell is dominated, the value of S\u2212\nis also given by Hz(a1,a2). If we build two more height arrays Hxand Hywhere\nwe use the highest value of x and y instead of z, we can determine the results of\nall three of the one-dimensional hypervolume calculations in constant time during\ncell calculations.\nNow, only the two-dimensional hypervolume calculations represented by xslice,\nyslice and zslice, and the three-dimensional hypervolume calculation represented\nby S\u2212\nity, we have omitted their dependence on a particular cell from the notation until\nnow, but in order to show the relations between correction terms of different cells,\nwe will write \u2018S\u2212\nthe two-dimensional hypervolumes.\nThe value of S\u2212\nfollowing way:\nxyfor a cell C(a1,a2,a3) that is not dominated\n\u2205, still have a complexity greater than constant time. For notational simplic-\n\u2205belonging to C(a1,a2,a3)\u2019 as C(a1,a2,a3).S\u2212\n\u2205, and likewise for\n\u2205is related to the values of xslice, yslice and zslice in the\n\u2022 C(a1,a2,a3).xslice =\nC(a1+1,a2,a3).S\u2212\n\u2205\u2212C(a1,a2,a3).S\u2212\nux\u2212lx\n\u2205\u2212C(a1,a2,a3).S\u2212\nuy\u2212ly\n\u2205\u2212C(a1,a2,a3).S\u2212\nuz\u2212lz\n\u2205\n\u2022 C(a1,a2,a3).yslice =\nC(a1,a2+1,a3).S\u2212\n\u2205\n\u2022 C(a1,a2,a3).zslice =\nWith our height array Hz, we can calculate all values of zslice for a given value\nof a3in O(n2) time. We can also calculate all values of S\u2212\na3in O(n2) time, provided a3 = 0 or we have both S\u2212\nwhere a3is one lower. The details of these calculations will be given below. If we\ngo through our cells in the right order (with a3starting at 0, incrementing it only\nafter we have performed the calculations for all cells with a given value of a3), we\nonly need to update the values of zslice and S\u2212\nfor the full computation with complexity in O(n3). If we know the value of S\u2212\nall cells with a given value of a3, we can use the formulas given above to calculate\nxslice and yslice in constant time whenever we need them, so we do not need to\ncalculate these constants in advance.\nThe details of calculating zslice using the height array are as follows. We will\niterate through the possible values of a1and a2in ascending order. We know that\nzslice(a1,a2) is 0 if a1= 0 or a2= 0. If our height array shows that C(a1\u22121,a2\u2212\n1,a3) is dominated, zslice(a1,a2) is set equal to the area of the 2-D rectangle from\nits lower corner to (rx,ry). Else, if that cell is not dominated, zslice(a1,a2) is set\nequal to zslice(a1\u22121,a2)+zslice(a1,a2\u22121)\u2212zslice(a1\u22121,a2\u22121). The value of\nzslice(a1\u2212 1,a2\u2212 1) is removed as this is the area which is overlapping, causing\nit to be added twice otherwise.\nFor an example, refer to Figure 8.\nC(a1,a2,a3+1).S\u2212\n\u2205\n\u2205for a given value of\n\u2205and zslice for the cells\n\u2205n times, resulting in an algorithm\n\u2205for\n24"},{"page":25,"text":"7 Empirical Tests and Results\nFive different implementations of a 3-D expected hypervolume improvement cal-\nculation algorithm were used throughout the following tests, referred to as the\n8-term, 5-term, 2-term, slice-update and Monte Carlo schemes. The goal of com-\nparing the exact calculation algorithms to a Monte Carlo scheme is twofold. First,\nby computing the Expected Hypervolume Improvement in different ways, the al-\ngorithms and their implementations will be thoroughly validated. Second, the\ntime consumption of the algorithms will be compared. This is of particular inter-\nest because Monte Carlo schemes are often used as fast approximations to exact\ncomputations.\n\u2022 The 8-term scheme is a direct implementation of the calculations described\nin Section 5.2.\n\u2022 The 5-term scheme implements the slightly simplified calculations described\nin Section 5.2.\n\u2022 The 2-term scheme implements the calculations described in Section 5.3.\n\u2022 The slice-update scheme implements the algorithm described in Section 6.\n\u2022 The Monte Carlo scheme uses Monte Carlo integration to give an approxima-\ntion of the expected hypervolume improvement. Its random number generator\nuses the Box-Muller transform [25] in combination with the Mersenne Twister\nalgorithm [24] (specifically, the 32-bit MT19937 variant from the C++ stan-\ndard library, implemented in GCC) to generate normally distributed pseudo-\nrandom numbers. Due to the nature of Monte Carlo algorithms, it is impos-\nsible to get an exact answer out of this scheme. The expected error of Monte\nCarlo integration is related to the number of trials m by\nthat to make the estimate ten times more accurate, a hundred times more\ntrials are required.\n1\n\u221am, which means\nThe implementations of \u03c8 and the Gaussian cumulative distribution function\nwere identical for all schemes, except for the Monte Carlo scheme where they were\nnot used. The 2-D and 3-D hypervolume calculation functions were also identical\nbetween those schemes which used them. Standard C++ library functions were\nused for sorting and for the implementation of the Gaussian error function erf.\n7.1Monte Carlo Verification\nAs a verification of the correctness of the algorithms, the expected hypervolume\nimprovements calculated by all schemes on several test problems were compared\nto each other and to the value which the Monte Carlo scheme converged towards.\n25"},{"page":26,"text":"The graph in Figure 9 shows the results of running the algorithms on a sim-\nple test problem. The population consisted of three points: (1,2,3), (2,3,1) and\n(3,1,2). The reference point was set to (0,0,0). The median vector for the Gaus-\nsian distribution was set to (3,3,3), placing it right between cell borders, and the\nstandard deviation was set to (2,2,2). All non-Monte Carlo schemes gave exactly\nidentical answers, which was likely due to the simplicity of the test case, because\nrounding errors in the floating-point calculations would have resulted in small dif-\nferences otherwise. The Monte Carlo scheme was allowed to run for 100.000.000\niterations.\nFigure 10 shows the results of running the algorithm on a few more complex\npopulations. The first consists of 30 points, some of which had identical values to\nanother point in the population in one of their dimensions (creating cells of size\n0). The second consists of 100 points with a bias towards one area of the search\nspace. The results of all non-Monte Carlo schemes on these two test problems were\nidentical to 15 and 14 digits, respectively. The double-precision floating numbers\nwhich were used in the implementations are accurate to approximately the 15th\ndecimal, so the answers can safely be considered identical.\nThe convergence of the Monte Carlo integration, as well as the near-identical\nanswers generated by the different approaches towards calculating the expected\nhypervolume improvement, both support the validity of the calculations described\nin this thesis.\n26"},{"page":27,"text":"y\nx\n4\n7\n9\n10\n2\n3\n6\n10\n00\n00\n00\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\ny\nx\n4\n7\n9\n10\n2\n3\n6\n10\n2*4 = 82*7 = 14 2*9 = 18 2*10 = 20\n3*4 = 12 3*7 = 21 3*9 = 27\n27+20-18\n= 29\n6*4 = 24 6*7 = 42 6*9 = 5454+29-27\n= 56\n10*4 = 4040+42-24\n= 58\n58+54-42\n= 70\n70+56-54\n= 72\ny\nx\n4\n7\n9\n10\n2\n3\n6\n10\n18*4\n= 72\n20*4\n= 80\n27*4\n= 108\n29*4\n= 116\n24*4\n= 96\n42*4\n= 168\n54*4\n= 216\n56*4\n= 224\n40*4\n= 160\n58*4\n= 232\n70*4\n= 280\n72*4\n= 288\n12*4\n= 48\n21*4\n= 84\n14*4\n= 56\n8*4\n= 32\ny\nx\n4\n7\n9\n10\n2\n3\n6\n10\n2*4 = 82*7 = 14 2*9 = 18 2*10 = 20\n3*4 = 12 3*7 = 21 3*9 = 27\n27+20-18\n= 29\n6*4 = 24 6*7 = 42 6*9 = 5454+29-27\n= 56\n0+24-0\n= 24\n24+42-24\n= 42\n42+54-42\n= 54\n54+56-54\n= 56\ny\nx\n4\n7\n9\n10\n2\n3\n6\n10\n72\n+ 18*2\n= 108\n80\n+ 20*2\n= 120\n108\n+ 27*2\n= 162\n116\n+ 29*2\n= 174\n96\n+ 24*2\n= 144\n168\n+ 42*2\n= 252\n216\n+ 54*2\n= 324\n224\n+ 56*2\n= 336\n160\n+ 24*2\n= 208\n232\n+ 42*2\n= 316\n280\n+ 54*2\n= 388\n288\n+ 56*2\n= 400\n48\n+ 12*2\n= 72\n84\n+ 21*2\n= 126\n56\n+ 14*2\n= 84\n32\n+ 8*2\n= 48\ny\nx\n4\n7\n9\n10\n2\n3\n6\n10\n2*4 = 82*7 = 14 2*9 = 18 2*10 = 20\n3*4 = 12 3*7 = 2121+18-14\n= 25\n25+20-18\n= 27\n0+12-0\n= 12\n12+21-12\n= 21\n21+25-21\n= 25\n25+27-25\n= 27\n0+12-0\n= 12\n12+21-12\n= 21\n21+25-21\n= 25\n25+27-25\n= 27\nFigure 8: Some values of zslice and S\u2212for the example shown in Figure 7, with a3= 0, 1 and\n2, respectively. The x and y values of each cell\u2019s lower corner are shown on the axes. The grids\nwith the values of S\u2212are on the left and the grids with the values of zslice are on the right.\n27"},{"page":28,"text":"Figure 9: Logarithmic-scale graph of the convergence of Monte Carlo integration. The answer\nwas measured every 100.000 iterations.\n28"},{"page":29,"text":"Figure 10: Two logarithmic-scale graphs showing the convergence of Monte Carlo integration,\nalong with visualizations of the Pareto approximation sets.\n29"},{"page":30,"text":"7.2 Empirical Performance\nTo test the empirical performance of the exact calculation schemes, they were\ntested on mutually non-dominated populations of varying sizes that were gener-\nated by selecting n pseudo-random points which were uniformly distributed on\na spherical surface. The time needed for calculating the expected hypervolume\nimprovement was measured (along with all operations required to do so, such as\nsorting the populations, but not including the time needed to generate the popula-\ntions). The seed of the pseudorandom generator was the same for each calculation\nscheme that was tested. Figure 11 shows the results. There is a noticeable dif-\nference in speed between the 8-term, 5-term and 2-term scheme, but they are in\nthe same complexity class and for any given n, their performance relative to each\nother is roughly the same. The slice-update scheme, by contrast, performs better\nrelative to the other schemes when n increases, as would be expected due to its\nlower complexity. Even for small n it outperforms the other algorithms.\nFigure 11: Time needed to calculate the expected hypervolume improvement for a Pareto ap-\nproximation set consisting of n points randomly selected on the surface of a sphere, averaged\nover 10 runs.\nWhat is interesting is that going from 8 terms to 5 terms causes a greater\nimprovement than going from 5 terms to 2 terms, even though 2-dimensional\nhypervolume calculations are completely removed from the equation when going to\n2 terms. No solid conclusions can be drawn from the magnitude of the differences,\nas they might depend on the implementation details of the code and the compiler\n30"},{"page":31,"text":"optimizations. However, this does show that simplifying calculations can make a\nbig difference for the speed of an algorithm. A benefit of the 2-term scheme which\nis not captured in the graph, is that it is the simplest scheme in terms of the\nnumber of operations that must be implemented, so the time needed to implement\nit will be shorter.\nFigure 12: The figure on the left shows the number of Monte Carlo trials that can be performed\nin a second given a spherical Pareto approximation set consisting of n points. The figure on the\nright plots the same data as a graph of the time required for 100.000 Monte Carlo iterations,\ncompared to the time needed for the fastest non-Monte Carlo scheme.\nFigure 13: Graph showing the time needed to simultaneously calculate the EHVI on a number of\ncandidate points using either the 5-term or slice-update schemes, for a population size of 30. The\nexpected time taken when simply calling the slice-update scheme on each candidate individual\nseparately is also plotted in this graph for purposes of comparison.\n31"},{"page":32,"text":"The Monte Carlo scheme is a special case, in that the time it takes to run\ndepends on the desired accuracy, and this accuracy in turn also depends on the\nvariance of the predictive distribution. When this variance is higher, the accuracy\nwill be lower. For a rough idea of its performance relative to the exact calculation\nschemes, see Figure 12, which shows the number of Monte Carlo trials which can be\nperformed if the algorithm is allowed to run for a second. Because of the O(nlogn)\ntime complexity of each individual trial, it is less affected by n than any of the\nexact calculation schemes. If n is large enough and the desired accuracy is low\nenough, it might be the faster option. However, when n is reasonably small, there\nis no advantage to using it.\nThe complexity of calculating the expected hypervolume improvement of mul-\ntiple points by repeatedly using one of the described algorithms is of course linear\nin the number of candidate individuals. Here, the 8-term, 5-term and slice-update\nschemes have an advantage not shared by the Monte Carlo and 2-term schemes, in\nthat their hypervolume calculations are independent of the probability distribution\nfor which the EHVI is being calculated. This makes it possible to calculate sev-\neral expected hypervolume improvements on the same population with a relatively\nsmall corresponding increase in calculation time, because the additional calcula-\ntions have complexity O(n3). It is expected to be less impressive for the slice-\nupdate scheme, as this already has a time complexity of O(n3), but the amount of\noverhead that is avoided might still be noticeable. To determine the impact of this\nadvantage on the relative performance of the schemes, Figure 13 shows the results\nof using the schemes to calculate the EHVI for a vector of probability distributions\ninstead of just one.\nAs can be seen, the time taken increases linearly in the number of individuals\nevaluated at the same time, but the constant added on top of that is larger for the\n5-term scheme than for the slice-update scheme. When n is 30 it is only a difference\nequivalent to evaluating a few more candidate individuals, however. Because the\n5-term scheme is somewhat easier to implement, it might be preferable to use it if\nthe number of candidate individuals is expected to be high in comparison to n.\n8 Conclusion and Future Work\nThe main results realized in this thesis are as follows: A fast algorithm for cal-\nculating the EHVI in two dimensions was proposed with runtime complexity in\nO(n2) (previously: O(n3logn)). An empirical test shows improved speed even for\nrelatively small n (\u2248 20). An exact calculation algorithm for calculating the EHVI\nin more than two dimensions is provided. This generic algorithm has been detailed\nand improved in efficiency for the important tri-objective case. It has a cubic run-\ntime complexity in O(n3), and a further efficiency gain can be obtained by batch\nevaluation, i.e. re-using the data structures for multiple EHVI computations. The\nalgorithm are based on linear data structures and there are no large hidden con-\n32"},{"page":33,"text":"stants. For three dimensions it is now possible to perform more than a hundred\nEHVI in 2.5 seconds for an approximation set size of 30. Implementations of all\nalgorithms are made available [TODO: url] and have been validated with results\nfrom Monte Carlo algorithms.\nThe results open up new possibilities to construct expected improvement algo-\nrithms for multiobjective optimization, for instance by using the fast EHVI eval-\nuation as an infill criterion in the efficient global optimization algorithm (EGO).\nMoreover, as the new exact EHVI computation methods have the same or better\nruntime performance compared to the Monte Carlo algorithms used so far, they\ncan now replace these inaccurate methods.\nAs a side result a relationship between the expected improvement and the center\nof (probability) mass over single cells was established, which might in the future\nshed some light on the relation to alternative expected improvement formulations\n[Keane04] and be useful for establishing theoretical results.\nSource code and acknowledgement\nthesis of Iris Hupkens [1] under the supervision of M. Emmerich and A. Deutz.\nThe sourcecode of all algorithms in C++ is made available on http:\/\/natcomp.\nliacs.nl\/index.php?page=code.\nThis work is based on the honors master\u2019s\nReferences\n[1] Iris Hupkens: Complexity Reduction and Validation of Computing the Ex-\npected Hypervolume Improvement, Master\u2019s Thesis (with honors) published\nas LIACS, Internal Report Nr. 2013-12, August, 2013 http:\/\/www.liacs.nl\/\nassets\/Masterscripties\/2013-12IHupkens.pdf\n[2] Shir, O. M., Emmerich, M., B\u00a8 ack, T., and Vrakking, M. J. (2007, September).\nThe application of evolutionary multi-criteria optimization to dynamic molecular\nalignment. In Evolutionary Computation, 2007. CEC 2007. IEEE Congress on\n(pp. 4108-4115). IEEE.\n[3] Zaefferer, M., Bartz-Beielstein, T., Naujoks, B., Wagner, T., and Emmerich,\nM. (2013, January). A Case Study on Multi-Criteria Optimization of an Event\nDetection Software under Limited Budgets. InEvolutionary Multi-Criterion Op-\ntimization(pp. 756-770). Springer Berlin Heidelberg.\n[4] Shimoyama, K., Sato, K., Jeong, S., and Obayashi, S. (2012, June). Comparison\nof the criteria for updating Kriging response surface models in multi-objective\noptimization. InEvolutionary Computation (CEC), 2012 IEEE Congress on(pp.\n1-8). IEEE.\n33"},{"page":34,"text":"[5] Shimoyama, K., Jeong, S., and Obayashi, S. (2013, June). Kriging-surrogate-\nbased optimization considering expected hypervolume improvement in non-\nconstrained many-objective test problems. InEvolutionary Computation (CEC),\n2013 IEEE Congress on(pp. 658-665). IEEE.\n[6] Couckuyt, Ivo, Dirk Deschrijver, and Tom Dhaene. \u201dFast calculation of mul-\ntiobjective probability of improvement and expected improvement criteria for\nPareto optimization.\u201dJournal of Global Optimization(2013): 1-20.\n[7] Wagner, T.; Emmerich, M.; Deutz, A. and Ponweiser, W. (2010) \u201cOn expected-\nimprovement criteria for model-based multi-objective optimization\u201d, in \u2018Proc. of\nPPSN XI Vol. 1, Springer-Verlag, Berlin, Heidelberg, pp. 718-727.\n[8] Fleischer, M. (2003) \u201cThe Measure of Pareto Optima Applications to Multi-\nobjective Metaheuristics\u201d. Evolutionary Multi-Criterion Optimization. Second\nInternational Conference, EMO 2003, pg. 519-533.\n[9] Emmerich, M. T M; Deutz, A.H.; Klinkenberg, J.W. (2011) \u201cHypervolume-\nbased expected improvement: Monotonicity properties and exact computation,\u201d\n2011 IEEE Congress on Evolutionary Computation (CEC), pp.2147-2154\n[10] Nicola Beume, Carlos M. Fonseca, Manuel Lopez-Ibanez, Luis Paquete, and\nJan Vahrenhold. (2009) \u201cOn the complexity of computing the hypervolume in-\ndicator.\u201d IEEE Trans. Evolutionary Computation, 13(5) pp. 1075-1082.\n[11] Sacks, J., Welch, W. J., Mitchell, T. J., and Wynn, H. P. (1989) \u201cDesign and\nanalysis of computer experiments\u201d. Statistical science, 4(4), 409-423.\n[12] Mockus, J., Tiesis, V., Zilinskas, A. (1978) \u201cThe application of Bayesian meth-\nods for seeking the extremum\u201d. In: Dixon, L., Szego, G. (Eds.), Towards Global\nOptimization, vol. 2. North Holland, New York, pp. 117129.\n[13] Donald R. Jones, Matthias Schonlau, and William J. Welch. (1998) \u201cEfficient\nGlobal Optimization of Expensive Black-Box Functions\u201d. J. of Global Optimiza-\ntion 13, 4 (December 1998), 455-492.\n[14] Emmanuel Vazquez and Julien Bect. (2010) \u201cConvergence properties of the\nexpected improvement algorithm with fixed mean and covariance functions\u201d.\nJournal of Statistical Planning and Inference 140, pp. 3088-3095\n[15] Knowles, J. (2006) \u201cParEGO: A hybrid algorithm with on-line landscape ap-\nproximation for expensive multiobjective optimization problems\u201d. IEEE Trans-\nactions on Evolutionary Computation. 10 (1): 50-66.\n[16] Keane, A.J. (2006) \u201cStatistical improvement criteria for use in multiobjective\ndesign optimisation\u201d. AIAA Journal, 44, (4), 879-891.\n34"},{"page":35,"text":"[17] Wolfgang Ponweiser, Tobias Wagner, Dirk Biermann, and Markus Vincze.\n(2008) \u201cMultiobjective Optimization on a Limited Budget of Evaluations Using\nModel-Assisted S-Metric Selection\u201d. In Proceedings of the 10th international\nconference on Parallel Problem Solving from Nature: PPSN X. Springer-Verlag,\nBerlin, Heidelberg, 784-794.\n[18] Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and T. Meyarivan. (2000)\n\u201cA fast and elitist multi-objective genetic algorithm: NSGA-II\u201d.\n[19] E. Zitzler, M. Laumanns and L. Thiele. (2001) \u201cSPEA2: Improving the\nStrength Pareto Evolutionary Algorithm\u201d.\n[20] Michael Emmerich, Nicola Beume, and Boris Naujoks. (2005) \u201cAn EMO Al-\ngorithm Using the Hypervolume Measure as Selection Criterion\u201d. In 2005 Intl\nConference, March 2005, pages 62-76.\n[21] Christian Igel, Nikolaus Hansen, and Stefan Roth. (2007) \u201cCovariance Ma-\ntrix Adaptation for Multi-objective Optimization\u201d. Evol. Comput. 15, 1 (March\n2007), 1-28.\n[22] Williams, Christopher K.I. (1998) \u201cPrediction with Gaussian processes: From\nlinear regression to linear prediction and beyond\u201d. In M. I. Jordan. Learning in\ngraphical models. MIT Press. pp. 599612.\n[23] Fubini, G. \u201dSugli integrali multipli.\u201d (1958) Opere scelte, Vol. 2. Cremonese,\npp. 243-249.\n[24] Matsumoto, M.; Nishimura, T. (1998) \u201cMersenne twister: a 623-dimensionally\nequidistributed uniform pseudo-random number generator\u201d. ACM Transactions\non Modeling and Computer Simulation 8 (1): 330\n[25] G. E. P. Box, Mervin E. Muller. (1958) A Note on the Generation of Random\nNormal Deviates. The Annals of Mathematical Statistics, Vol. 29, No. 2. pp.\n610-611\n[26] Emmerich, M. (2005). Single-and multi-objective evolutionary design opti-\nmization assisted by gaussian random field metamodels. Dissertation, TU Dort-\nmund, Informatik, Eldorado, http:\/\/hdl.handle.net\/2003\/21807.\n[27] Kumano, T., Jeong, S., Obayashi, S., Ito, Y., Hatanaka, K., and Morino, H.\n(2006). Multidisciplinary design optimization of wing shape with nacelle and\npylon. InEuropean Conference on Computational Fluid Dynamics ECCOMAS\nCFD.\n[28] Miettinen, K. (1999). Nonlinear Multiobjective Optimization, volume 12 of\nInternational Series in Operations Research and Management Science.\n35"},{"page":36,"text":"[29] Zitzler, E., Thiele, L. (1998, January). Multiobjective optimization using evo-\nlutionary algorithmsa comparative case study. In Parallel problem solving from\nnaturePPSN V (pp. 292-301). Springer Berlin Heidelberg\n[30] Zitzler, E., Thiele, L., Laumanns, M., Fonseca, C. M., and Da Fonseca, V. G.\n(2003). Performance assessment of multiobjective optimizers: An analysis and\nreview. Evolutionary Computation, IEEE Transactions on, 7(2), 117-132.\n[31] Fleischer, M. (2003, January). The measure of Pareto optima applications\nto multi-objective metaheuristics. In Evolutionary multi-criterion optimization\n(pp. 519-533). Springer Berlin Heidelberg.\n[32] Auger, A., Bader, J., Brockhoff, D., and Zitzler, E. (2009, January). Theory of\nthe hypervolume indicator: optimal \u00b5-distributions and the choice of the refer-\nence point. In Proceedings of the tenth ACM SIGEVO workshop on Foundations\nof genetic algorithms (pp. 87-102). ACM.\n[33] Bringmann, K., and Friedrich, T. (2010, July). The maximum hypervolume\nset yields near-optimal approximation. In Proceedings of the 12th annual con-\nference on Genetic and evolutionary computation (pp. 511-518). ACM.\n[34] Laniewski-Wollk, P., Obayashi S., Jeong, S. (2010), Development of expected\nimprovement for multi-objective problems, in Proceedings of 42nd Fluid Dy-\nnamics Conference\/Aerospace Numerical, Simulation Symposium (CD ROM),\nJune 2010\n[35] Koch, P. (2013). Efficient tuning in supervised machine learning (Doctoral\ndissertation, Leiden Institute of Advanced Computer Science (LIACS), Faculty\nof Science, Leiden University).\n36"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Emmerich\/publication\/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement\/links\/5404956f0cf23d9765a67a6c.pdf","widgetId":"rgw28_56aba20b301e0"},"id":"rgw28_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=265209813&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw29_56aba20b301e0"},"id":"rgw29_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=265209813&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":265209813,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"5404956f0cf23d9765a67a6c","name":"Michael Emmerich","date":"Sep 01, 2014 ","nameLink":"profile\/Michael_Emmerich","filename":"HED14arxiv.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Michael_Emmerich\/publication\/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement\/links\/5404956f0cf23d9765a67a6c.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Emmerich\/publication\/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement\/links\/5404956f0cf23d9765a67a6c.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"2ab89bbb1747e1a9f47b72256efa7094","showFileSizeNote":false,"fileSize":"972.23 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"5404956f0cf23d9765a67a6c","name":"Michael Emmerich","date":"Sep 01, 2014 ","nameLink":"profile\/Michael_Emmerich","filename":"HED14arxiv.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Michael_Emmerich\/publication\/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement\/links\/5404956f0cf23d9765a67a6c.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Emmerich\/publication\/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement\/links\/5404956f0cf23d9765a67a6c.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"2ab89bbb1747e1a9f47b72256efa7094","showFileSizeNote":false,"fileSize":"972.23 KB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[{"props":{"position":"float","orientation":"portrait","coords":"pag:13:rect:89.29,567.34,416.69,44.71","ordinal":"5"},"assetId":"AS:295914846605312@1447563005239"},{"props":{"position":"float","orientation":"portrait","coords":"pag:28:rect:89.29,510.91,416.69,20.80","ordinal":"9"},"assetId":"AS:295914846605313@1447563005320"},{"props":{"position":"float","orientation":"portrait","coords":"pag:29:rect:89.29,589.88,416.70,20.80","ordinal":"0"},"assetId":"AS:295914846605314@1447563005481"},{"props":{"position":"float","orientation":"portrait","coords":"pag:30:rect:89.29,580.72,416.70,32.76","ordinal":"1"},"assetId":"AS:295914846605315@1447563005649"},{"props":{"position":"float","orientation":"portrait","coords":"pag:31:rect:89.29,304.12,416.69,44.71","ordinal":"2"},"assetId":"AS:295914846605317@1447563005749"},{"props":{"position":"float","orientation":"portrait","coords":"pag:31:rect:89.29,599.29,416.69,44.71","ordinal":"3"},"assetId":"AS:295914846605318@1447563005966"}],"figureAssetIds":["AS:295914846605312@1447563005239","AS:295914846605313@1447563005320","AS:295914846605314@1447563005481","AS:295914846605315@1447563005649","AS:295914846605317@1447563005749","AS:295914846605318@1447563005966"],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=eXqn7mkxnIx8RwQlJZhyhCqT3aOzQABfDMd15t_F1r1MZaGg-xYO7eEBTnNU2N--DGGJX_X5wglC_hYy10vpXA.ylYiRrLyZoeYb57HdQIjmp3YJ03pccqnK-8rnmu0KpuL0u_kWjWLU_MgEdZeWIeST5oKM8-a90P2rmcM0uI_tw","clickOnPill":"publication.PublicationFigures.html?_sg=EXZS3LRUh1Yia7Gf9EOld4qXlAoZ0-leHXNXO01S9ORZjQhyQ-HTdZ3MvWAPmTKRvxfymMyltqObxflNlywXSQ.oi_eK1n26M_4nKTaiKjxiUz37SYyXVb9Fgpzw1T9ZY8H_BORQmMf_04tumaQEGUCA1hV4bJ4oSpmie_Yv93HXw"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMichael_Emmerich%2Fpublication%2F265209813_Faster_Computation_of_Expected_Hypervolume_Improvement%2Flinks%2F5404956f0cf23d9765a67a6c.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=rD5kEsToadR_NzjyuvDulUsFVDqWI2W0T6zvdGMDf1gxbwpqy9U37Vr7ps-Ya7PrezyLuhVVW3o4Yg0NmBSCxw","urlHash":"beab34b2a481275dd23ca22d5da68e3d","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=lQxyAEAb3Pln6ZLfUCNAh0oOdFcK4DX3CdcDWEBa9uAV98MUBD3_ccwxvcKMp2i3XrcNjM8wt0ZuC_5za_KnFSaAb9CAfg25ITOWjMq_yxo.q7EPIwXQvvEM1Ig2VVD8YRs1vhZzua8KJfEIf-dIHosdzwnAfcZB0UGyN7XUX1ymePgV3l68KHZu6Aoc2KyKdQ.iFmvJ8OjtMNjmOZoMPvVUNRsmMKx2R37vd8ACtbQzUuTf2jUY9ZHXOkCuytqtm1tqBnMCc2sgL1ZfUI-Zc8pZA","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"5404956f0cf23d9765a67a6c","trackedDownloads":{"5404956f0cf23d9765a67a6c":{"v":false,"d":false}},"assetId":"AS:136630014844929@1409586542501","readerDocId":"5064652","assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":265209813,"commentCursorPromo":null,"widgetId":"rgw31_56aba20b301e0"},"id":"rgw31_56aba20b301e0","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMichael_Emmerich%2Fpublication%2F265209813_Faster_Computation_of_Expected_Hypervolume_Improvement%2Flinks%2F5404956f0cf23d9765a67a6c.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A136630014844929%401409586542501&publicationUid=265209813&linkId=5404956f0cf23d9765a67a6c&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Faster Computation of Expected Hypervolume Improvement","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=6PRd4DgvMxVjbcZjuTjeY47JdySAxycR9by67Vq8DD3I1BvbPIy8g3W6JjfaHK5F04evAITxUBHkMussi9SUo_mxWh-aMWZ0PUhQV7Y9LMs.wP1UXTIcpIF1bvl9bb01TMobyM4-IXvr5dgPad90TrLcAOZzt6QfM1duTnz159IhKvW4hygHH2TnGMm8fCtHog.1OZBiP1MKx0U--j9l3o2NN6grYB-Mll5X618UcGI86csVJuAZN_20-dWrQnXh1eG4_cfnvDljVnMpl24QVf1tA","publicationUid":265209813,"trackedDownloads":{"5404956f0cf23d9765a67a6c":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw33_56aba20b301e0"},"id":"rgw33_56aba20b301e0","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw34_56aba20b301e0"},"id":"rgw34_56aba20b301e0","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw35_56aba20b301e0"},"id":"rgw35_56aba20b301e0","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw36_56aba20b301e0"},"id":"rgw36_56aba20b301e0","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw37_56aba20b301e0"},"id":"rgw37_56aba20b301e0","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw32_56aba20b301e0"},"id":"rgw32_56aba20b301e0","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw30_56aba20b301e0"},"id":"rgw30_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56aba20b301e0"},"id":"rgw2_56aba20b301e0","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":265209813},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=265209813&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56aba20b301e0"},"id":"rgw1_56aba20b301e0","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"g1QL1cqwJaLrgO0nHVRAsyND3dGXK8FzJvC0M+Peqq3R3ZoSzDfPPLNPyu6vJxGzvuA398liWA617K11wej7KbAHN6awAioEPo56lQDoFiKd+rBXZvr\/kFxZuY6U0J1Uh6fRgpO3irTReshnCvc3b0E7vNhxMQqd+VYrxcUtsM7l+EAK4aVuohFMR5+S5W9fAm3Gr2C89EQocsIF6XGQL6wQVCKrVFkWVCgfn9y7uDJ2JIabd57fcoR3ZMh+jYXCPxxZEsiuJGblBQ5+ZXXfJHZzDH6COdrDOqzbQAtKy6o=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Faster Computation of Expected Hypervolume Improvement\" \/>\n<meta property=\"og:description\" content=\"The expected improvement algorithm (or efficient global optimization) aims\nfor global continuous optimization with a limited budget of black-box function\nevaluations. It is based on a statistical...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement\/links\/5404956f0cf23d9765a67a6c\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement\" \/>\n<meta property=\"rg:id\" content=\"PB:265209813\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Faster Computation of Expected Hypervolume Improvement\" \/>\n<meta name=\"citation_author\" content=\"Iris Hupkens\" \/>\n<meta name=\"citation_author\" content=\"Michael Emmerich\" \/>\n<meta name=\"citation_author\" content=\"Andr\u00e9 Deutz\" \/>\n<meta name=\"citation_publication_date\" content=\"2014\/08\/29\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Michael_Emmerich\/publication\/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement\/links\/5404956f0cf23d9765a67a6c.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/215868066921738\/styles\/pow\/publicliterature\/FigureList.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-cdf14eca-e980-4ddc-8485-af29cd7f5eb4","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":749,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw38_56aba20b301e0"},"id":"rgw38_56aba20b301e0","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-cdf14eca-e980-4ddc-8485-af29cd7f5eb4", "9ec244616c919972b195e1870c13257c9ebfccd2");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-cdf14eca-e980-4ddc-8485-af29cd7f5eb4", "9ec244616c919972b195e1870c13257c9ebfccd2");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw39_56aba20b301e0"},"id":"rgw39_56aba20b301e0","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/265209813_Faster_Computation_of_Expected_Hypervolume_Improvement","requestToken":"SgHvF0prwj+U945vVKHtZnnPEjgb7vM9HFwRWefpaDM\/fADlb5\/\/caDTH4nb7IHyc16W74pzu6MhN1g6GyDoqvytd1DfTlAuR+qcT3Uteu6LuAf1KJO1jhKw1XGacN0xjC08reDeJN+RAcIvuNEUfTQLQN0+OTL2LlOHPWAUW555iO5P89Vt3DTxk8TnXeZXtAXGSFbJgwzl5LLDLoARpDJeiayKz4FWs6yUH2BWzouEsm9OjL\/NtAXgUQ8GuH3B5Fk0g7oQBoSV3oqrS0BJs4D+oFqEShvF2Lf33VDA\/iI=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=ODgn0L1WvoQ6z4_8szPrWelIGMIHTZBVTwGHEuKE_wUC6yBhJmfHrP3iTVHD4GVh","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjY1MjA5ODEzX0Zhc3Rlcl9Db21wdXRhdGlvbl9vZl9FeHBlY3RlZF9IeXBlcnZvbHVtZV9JbXByb3ZlbWVudA%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw41_56aba20b301e0"},"id":"rgw41_56aba20b301e0","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw40_56aba20b301e0"},"id":"rgw40_56aba20b301e0","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw42_56aba20b301e0"},"id":"rgw42_56aba20b301e0","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
