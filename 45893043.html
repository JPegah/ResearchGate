<!DOCTYPE html> <html lang="en" class="" id="rgw28_56aba00261acf"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="8B5KhQOSpQf5KWAkRqZTyH4LXxK7qT8X7xTS5pY2MWuwasofyhOhJe/mM5KShxs/7hxEK1I5m+PREBNCHZnUCozWCejCbqQRQgSdSECp2M27wP6UQo2noQmOYjjTp9uVTvY8tlbjMl31ILjkp8NMFq2Gf8uK4BJE54IymvTPFWWQLP/JTVtb6XBxEOfumku1DUWAdWbaEIqoqV7zwYCSRRF/oPpZzBcw2MMVVF8LMG4Cp7KYjSrCRI7P2oGf+YTMStqf/0Z+Df9IMLI1mnx2RpvbMHP/9f+22JlT6KsIrs8="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-d6176f05-3a1a-4e5c-9872-e6e5ade54a1b",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/45893043_Elliptical_Slice_Sampling" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Elliptical Slice Sampling." />
<meta property="og:description" content="Many probabilistic models introduce strong dependencies between variables using a latent multivariate Gaussian distribution or a Gaussian process. We present a new Markov chain Monte Carlo..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/45893043_Elliptical_Slice_Sampling/links/0f64778138294e886aa1aa25/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/45893043_Elliptical_Slice_Sampling" />
<meta property="rg:id" content="PB:45893043" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Elliptical Slice Sampling." />
<meta name="citation_author" content="Iain Murray" />
<meta name="citation_author" content="Ryan Prescott Adams" />
<meta name="citation_author" content="David J. C. MacKay" />
<meta name="citation_publication_date" content="2009/12/31" />
<meta name="citation_volume" content="1001" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/45893043_Elliptical_Slice_Sampling" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/45893043_Elliptical_Slice_Sampling" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Elliptical Slice Sampling.</title>
<meta name="description" content="Elliptical Slice Sampling. on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56aba00261acf" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56aba00261acf" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56aba00261acf">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Elliptical%20Slice%20Sampling.&rft.title=arXiv%20preprint%20arXiv&rft.jtitle=arXiv%20preprint%20arXiv&rft.volume=1001&rft.date=2009&rft.au=Iain%20Murray%2CRyan%20Prescott%20Adams%2CDavid%20J.%20C.%20MacKay&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Elliptical Slice Sampling.</h1> <meta itemprop="headline" content="Elliptical Slice Sampling.">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/45893043_Elliptical_Slice_Sampling/links/0f64778138294e886aa1aa25/smallpreview.png">  <div id="rgw7_56aba00261acf" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56aba00261acf"> <a href="researcher/45470150_Iain_Murray" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Iain Murray" alt="Iain Murray" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Iain Murray</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw9_56aba00261acf">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/45470150_Iain_Murray"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Iain Murray" alt="Iain Murray" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/45470150_Iain_Murray" class="display-name">Iain Murray</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56aba00261acf"> <a href="researcher/32349423_Ryan_Prescott_Adams" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Ryan Prescott Adams" alt="Ryan Prescott Adams" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Ryan Prescott Adams</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw11_56aba00261acf">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/32349423_Ryan_Prescott_Adams"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Ryan Prescott Adams" alt="Ryan Prescott Adams" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/32349423_Ryan_Prescott_Adams" class="display-name">Ryan Prescott Adams</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56aba00261acf" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/David_Mackay" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272707275194376%401442029889420_m" title="David Mackay" alt="David Mackay" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">David Mackay</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw13_56aba00261acf" data-account-key="David_Mackay">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/David_Mackay"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272707275194376%401442029889420_l" title="David Mackay" alt="David Mackay" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/David_Mackay" class="display-name">David Mackay</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_of_Cambridge" title="University of Cambridge">University of Cambridge</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">     arXiv preprint arXiv   <meta itemprop="datePublished" content="2009-12">  12/2009;  1001.             <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1001.0175" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw14_56aba00261acf" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>Many probabilistic models introduce strong dependencies between variables using a latent multivariate Gaussian distribution or a Gaussian process. We present a new Markov chain Monte Carlo algorithm for performing inference in models with multivariate Gaussian priors. Its key properties are: 1) it has simple, generic code applicable to many models, 2) it has no free parameters, 3) it works well for a variety of Gaussian process based models. These properties make our method ideal for use while model building, removing the need to spend time deriving and tuning updates for more complex algorithms. Comment: 8 pages, 6 figures, appearing in AISTATS 2010 (JMLR: W&amp;CP volume 6). Differences from first submission: some minor edits in response to feedback.</div> </p>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw27_56aba00261acf">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw26_56aba00261acf"  itemprop="articleBody">  <p>Page 1</p> <p>Elliptical slice sampling<br />Iain Murray<br />University of Toronto<br />Ryan Prescott Adams<br />University of Toronto<br />David J.C. MacKay<br />University of Cambridge<br />Abstract<br />Many probabilistic models introduce strong<br />dependencies between variables using a latent<br />multivariate Gaussian distribution or a Gaus-<br />sian process. We present a new Markov chain<br />Monte Carlo algorithm for performing infer-<br />ence in models with multivariate Gaussian<br />priors. Its key properties are: 1) it has simple,<br />generic code applicable to many models, 2) it<br />has no free parameters, 3) it works well for<br />a variety of Gaussian process based models.<br />These properties make our method ideal for<br />use while model building, removing the need<br />to spend time deriving and tuning updates<br />for more complex algorithms.<br />1Introduction<br />The multivariate Gaussian distribution is commonly<br />used to specify a priori beliefs about dependencies<br />between latent variables in probabilistic models. The<br />parameters of such a Gaussian may be specified directly,<br />as in graphical models and Markov random fields, or<br />implicitly as the marginals of a Gaussian process (GP).<br />Gaussian processes may be used to express concepts of<br />spatial or temporal coherence, or may more generally<br />be used to construct Bayesian kernel methods for non-<br />parametric regression and classification. Rasmussen<br />and Williams (2006) provide a recent review of GPs.<br />Inferences can only be calculated in closed form for<br />the simplest Gaussian latent variable models. Recent<br />work shows that posterior marginals can sometimes be<br />well approximated with deterministic methods (Kuss<br />and Rasmussen, 2005; Rue et al., 2009). Markov chain<br />Monte Carlo (MCMC) methods represent joint pos-<br />terior distributions with samples (e.g. Neal, 1993).<br />MCMC can be slower but applies more generally.<br />Appearing in Proceedings of the 13thInternational Con-<br />ference on Artificial Intelligence and Statistics (AISTATS)<br />2010, Chia Laguna Resort, Sardinia, Italy. Volume 6 of<br />JMLR: W&amp;CP 6. Copyright 2010 by the authors.<br />In some circumstances MCMC provides good results<br />with minimal model-specific implementation. Gibbs<br />sampling, in particular, is frequently used to sample<br />from probabilistic models in a straightforward way, up-<br />dating one variable at a time. In models with strong<br />dependencies among variables, including many with<br />Gaussian priors, Gibbs sampling is known to perform<br />poorly. Several authors have previously addressed the<br />issue of sampling from models containing strongly cor-<br />related Gaussians, notably the recent work of Titsias<br />et al. (2009). In this paper we provide a technique<br />called elliptical slice sampling that is simpler and of-<br />ten faster than other methods, while also removing the<br />need for preliminary tuning runs. Our method provides<br />a drop-in replacement for MCMC samplers of Gaussian<br />models that are currently using Gibbs or Metropolis–<br />Hastings and we demonstrate empirical success against<br />competing methods with several different GP-based<br />likelihood models.<br />2Elliptical slice sampling<br />Our objective is to sample from a posterior distri-<br />bution over latent variables that is proportional to<br />the product of a multivariate Gaussian prior and a<br />likelihood function that ties the latent variables to the<br />observed data. We will use f to indicate the vector<br />of latent variables that we wish to sample and denote<br />a zero-mean Gaussian distribution with covariance Σ by<br />N(f;0,Σ) ≡ |2πΣ|−1/2exp?−1<br />We also use f ∼ N(0,Σ) to state that f is drawn from<br />a distribution with the density in (1).<br />with non-zero means can simply be shifted to have<br />zero-mean with a change of variables; an example<br />will be given in Section 3.3. We use L(f) = p(data|f)<br />to denote the likelihood function so that our target<br />distribution for the MCMC sampler is<br />p?(f) =1<br />ZN(f;0,Σ)L(f),<br />where Z is the normalization constant, or the marginal<br />likelihood, of the model.<br />2f?Σ−1f?.(1)<br />Gaussians<br />(2)<br />Our starting point is a Metropolis–Hastings method<br />introduced by Neal (1999). Given an initial state f, a<br />arXiv:1001.0175v2  [stat.CO]  19 Mar 2010</p>  <p>Page 2</p> <p>Elliptical slice sampling<br />new state<br />f?=<br />?<br />1 − ?2f + ?ν,ν ∼ N(0,Σ) (3)<br />is proposed, where ? ∈ [−1,1] is a step-size parameter.<br />The proposal is a sample from the prior for ?=1 and<br />more conservative for values closer to zero. The move<br />is accepted with probability<br />p(accept) = min(1, L(f?)/L(f)), (4)<br />otherwise the next state in the chain is a copy of f.<br />Neal reported that for some Gaussian process classifiers<br />the Metropolis–Hastings method was many times faster<br />than Gibbs sampling. The method is also simpler to<br />implement and can immediately be applied to a much<br />wider variety of models with Gaussian priors.<br />A drawback, identified by Neal (1999), is that the<br />step-size ? needs to be chosen appropriately for the<br />Markov chain to mix efficiently. This may require<br />preliminary runs. Usually parameters of the covariance<br />Σ and likelihood function L are also inferred from data.<br />Different step-size parameters may be needed as the<br />model parameters are updated. It would be desirable<br />to automatically search over the step-size parameter,<br />while maintaining a valid algorithm.<br />For a fixed auxiliary random draw, ν, the locus of<br />possible proposals by varying ? ∈ [−1,1] in (3) is half<br />of an ellipse.<br />f?= ν sinθ + f cosθ, (5)<br />defining a full ellipse passing through the current state f<br />and the auxiliary draw ν. For a fixed θ there is an<br />equivalent ? that gives the same proposal distribution<br />in the original algorithm. However, if we can search<br />over the step-size, the full ellipse gives a richer choice<br />of updates for a given ν.<br />2.1 Sampling an alternative model<br />‘Slice sampling’ (Neal, 2003) provides a way to sample<br />along a line with an adaptive step-size. Proposals are<br />drawn from an interval or ‘bracket’ which, if too large,<br />is shrunk automatically until an acceptable point is<br />found. There are also ways to automatically enlarge<br />small initial brackets. Naively applying these adaptive<br />algorithms to select the value of ? in (3) or θ in (5)<br />does not result in a Markov chain transition operator<br />with the correct stationary distribution. The locus of<br />states is defined using the current position f, which<br />upsets the reversibility and correctness of the update.<br />We would like to construct a valid Markov chain tran-<br />sition operator on the ellipse of states that uses slice<br />sampling’s existing ability to adaptively pick step sizes.<br />Input: current state f, a routine that samples from<br />N(0,Σ), log-likelihood function logL.<br />Output: a new state f?. When f is drawn from p?(f)∝<br />N(f;0,Σ)L(f), the marginal distribution of f?is also p?.<br />1. Sample from p(ν0,ν1,θ|(ν0sinθ+ν1cosθ=f)):<br />θ ∼ Uniform[0,2π]<br />ν ∼ N(0,Σ)<br />ν0 ← f sinθ + ν cosθ<br />ν1 ← f cosθ − ν sinθ<br />2. Update θ ∈ [0,2π] using slice sampling (Neal, 2003)<br />on:<br />p?(θ|ν0,ν1) ∝ L(ν0sinθ + ν1cosθ)<br />3. return f?= ν0sinθ + ν1cosθ<br />Figure 1: Intuition behind elliptical slice sampling. This<br />is a valid algorithm, but will be adapted (Figure 2).<br />We will first intuitively construct a valid method by<br />positing an augmented probabilistic model in which<br />the step-size is a variable. Standard slice sampling algo-<br />rithms then apply to that model. We will then adjust<br />the algorithm for our particular setting to provide a<br />second, slightly tidier algorithm.<br />Our augmented probabilistic model replaces the origi-<br />nal latent variable with prior f ∼ N(0,Σ) with<br />ν0∼ N(0,Σ)<br />ν1∼ N(0,Σ)<br />θ ∼ Uniform[0,2π]<br />f = ν0sinθ + ν1cosθ.<br />(6)<br />The marginal distribution over the original latent vari-<br />able f is still N(0,Σ), so the distribution over data<br />is identical. However, we can now sample from the<br />posterior over the new latent variables:<br />p?(ν0,ν1,θ) ∝ N(ν0;0,Σ)N(ν1;0,Σ)L(f(ν0,ν1,θ)),<br />and use the values of f deterministically derived from<br />these samples. Our first approach applies two Monte<br />Carlo transition operators that leave the new latent<br />posterior invariant.<br />Operator 1: jointly resample the latents ν0,ν1,θ<br />given the constraint that f(ν0,ν1,θ) is unchanged. Be-<br />cause the effective variable of interest doesn’t change,<br />the likelihood does not affect this conditional distribu-<br />tion, so the update is generic and easy to implement.<br />Operator 2: use a standard slice sampling algorithm<br />to update the step-size θ given the other variables.<br />The resulting algorithm is given in Figure 1. The<br />auxiliary model construction makes the link to slice<br />sampling explicit, which makes it easy to understand<br />the validity of the approach. However, the algorithm</p>  <p>Page 3</p> <p>Murray, Adams, MacKay<br />can be neater and the [0,2π] range for slice sampling<br />is unnatural on an ellipse. The algorithm that we will<br />present in detail results from eliminating ν0and ν1<br />and a different way of setting slice sampling’s initial<br />proposal range. The precise connection will be given<br />in Section 2.4. A more direct, technical proof that<br />the equilibrium distribution of the Markov chain is the<br />target distribution is presented in Section 2.3.<br />Elliptical slice sampling, our proposed algorithm is<br />given in Figure 2, which includes the details of the slice<br />sampler. An example run is illustrated in Figure 3(a–d).<br />Even for high-dimensional problems, the states consid-<br />ered within one update lie in a two-dimensional plane.<br />In high dimensions f and ν are likely to have similar<br />lengths and be an angle of π/2 apart. Therefore the<br />ellipse will typically be fairly close to a circle, although<br />this is not required for the validity of the algorithm.<br />As intended, our slice sampling approach selects a<br />new location on the randomly generated ellipse in (5).<br />There are no rejections: the new state f?is never equal<br />to the current state f unless that is the only state on<br />the ellipse with non-zero likelihood. The algorithm<br />proposes the angle θ from a bracket [θmin,θmax] which<br />is shrunk exponentially quickly until an acceptable<br />state is found. Thus the step size is effectively adapted<br />on each iteration for the current ν and Σ.<br />2.2Computational cost<br />Drawing ν costs O(N3), for N-dimensional f and gen-<br />eral Σ. The usual implementation of a Gaussian sam-<br />pler would involve caching a (Cholesky) decomposition<br />of Σ, such that draws on subsequent iterations cost<br />O(N2). For some problems with special structure draw-<br />ing samples from the Gaussian prior can be cheaper.<br />In many models the Gaussian prior distribution cap-<br />tures dependencies: the observations are independent<br />conditioned on f. In these cases, computing L(f) will<br />cost O(N) computation. As a result, drawing the ν<br />random variate will be the dominant cost of the update<br />in many high-dimensional problems. In these cases<br />the extra cost of elliptical slice sampling over Neal’s<br />Metropolis–Hastings algorithm will be small.<br />As a minor performance improvement, our implementa-<br />tion optionally accepts the log-likelihood of the initial<br />state, if known from a previous update, so that it<br />doesn’t need to be recomputed in step 2.<br />2.3 Validity<br />Elliptical slice sampling considers settings of an angle<br />variable, θ. Figure 2 presented the algorithm as it<br />would be used: there is no need to index or remember<br />the visited angles. For the purposes of analysis we<br />Input: current state f, a routine that samples from<br />N(0,Σ), log-likelihood function logL.<br />Output: a new state f?. When f is drawn from p?(f)∝<br />N(f;0,Σ)L(f), the marginal distribution of f?is also p?.<br />1. Choose ellipse: ν ∼ N(0,Σ)<br />2. Log-likelihood threshold:<br />u ∼ Uniform[0,1]<br />logy ← logL(f) + logu<br />3. Draw an initial proposal, also defining a bracket:<br />θ ∼ Uniform[0,2π]<br />[θmin,θmax] ← [θ−2π, θ]<br />4. f?← f cosθ + ν sinθ<br />5. if logL(f?) &gt; logy then:<br />6. Accept: return f?<br />7. else:<br />Shrink the bracket and try a new point:<br />8. if θ &lt; 0 then: θmin←θ else: θmax←θ<br />9.θ ∼ Uniform[θmin,θmax]<br />10.GoTo 4.<br />Figure 2: The elliptical slice sampling algorithm.<br />(a) (b)<br />(c)(d)<br />(e)<br />Figure 3: (a) The algorithm receives f=<br />draws auxiliary variate ν= , defining an ellipse centred at<br />the origin ( ). Step 2: a likelihood threshold defines the<br />‘slice’ (). Step 3: an initial proposal<br />case not on the slice. (b) The first proposal defined both<br />edges of the [θmin,θmax] bracket; the second proposal ( )<br />is also drawn from the whole range. (c) One edge of the<br />bracket () is moved to the last rejected point such that<br />is still included. Proposals are made with this shrinking<br />rule until one lands on the slice. (d) The proposal here ( )<br />is on the slice and is returned as f?. (e) Shows the reverse<br />configuration discussed in Section 2.3:<br />which with auxiliary ν?= defines the same ellipse. The<br />brackets and first three proposals ( ) are the same. The<br />final proposal ( ) is accepted, a move back to f.<br />as input. Step 1<br />is drawn, in this<br />is the input f?,</p>  <p>Page 4</p> <p>Elliptical slice sampling<br />will denote the ordered sequence of angles considered<br />during the algorithm by {θk} with k=1..K.<br />We first identify the joint distribution over a state<br />drawn from the target distribution (2) and the other<br />random quantities generated by the algorithm:<br />p(f,y,ν,{θk}) = p?(f)p(y|f)p(ν)p({θk}|f,ν,y)<br />=1<br />ZN(f;0,Σ)N(ν;0,Σ)p({θk}|f,ν,y),<br />where the vertical level y was drawn uniformly<br />in [0,L(f)], that is, p(y |f)=1/L(f). The final term,<br />p({θk}|f,ν,y), is a distribution over a random-sized set<br />of angles, defined by the stopping rule of the algorithm.<br />(7)<br />Given the random variables in (7) the algorithm de-<br />terministically computes positions, {fk}, accepting the<br />first one that satisfies a likelihood constraint. More<br />generally each angle specifies a rotation of the two<br />a priori Gaussian variables:<br />νk= ν cosθk− f sinθk<br />fk= ν sinθk+ f cosθk,k = 1..K.<br />(8)<br />For any choice of θkthis deterministic transformation<br />has unit Jacobian. Any such rotation also leaves the<br />joint prior probability invariant,<br />N(νk;0,Σ)N(fk;0,Σ) = N(ν;0,Σ)N(f;0,Σ)<br />for all k, which can easily be verified by substituting<br />values into the Gaussian form (1).<br />(9)<br />It is often useful to consider how an MCMC algorithm<br />could make a reverse transition from the final state f?<br />back to the initial state f. The final state f?=fKwas<br />the result of a rotation by θKin (8). Given an initial<br />state of f?=fK, the algorithm could generate ν?=νK<br />in step 1. Then a rotation of −θKwould return back<br />to the original (f,ν) pair. Moreover, the same ellipse<br />of states is accessible and rotations of θk−θK will<br />reproduce any intermediate fk&lt;Klocations visited by<br />the initial run of the algorithm.<br />In fact, the algorithm is reversible:<br />p(f,y,ν,{θk}) = p(f?,y,ν?,{θ?<br />the equilibrium probability of a forwards draw (7) is<br />the same as the probability of starting at f?, drawing<br />the same y (possible because L(f?)&gt;y), ν?=νKand<br />?<br />−θK<br />k}), (10)<br />angles, θ?<br />k=<br />θk− θK<br />k &lt; K<br />k = K,<br />(11)<br />resulting in the original state f being returned. The<br />reverse configuration corresponding to the result of a<br />forwards run in Figure 3(d) is illustrated in Figure 3(e).<br />Substituting (9) into (7) shows that ensuring that the<br />forward and reverse angles are equally probable,<br />p({θk}|f,ν,y) = p({θ?<br />results in the reversible property (10).<br />k}|f?,ν?,y), (12)<br />The algorithm does satisfy (12): The probability of the<br />first angle is always1/2π. If more angles were considered<br />before an acceptable state was found, these angles were<br />drawn with probabilities 1/(θmax− θmin). Whenever<br />the bracket was shrunk in step 8, the side to shrink must<br />have been chosen such that fKremained selectable as it<br />was selected later. The reverse transition uses the same<br />intermediate proposals, making the same rejections<br />with the same likelihood threshold, y. Because the<br />algorithm explicitly includes the initial state, which in<br />reverse is fKat θ?=0, the reverse transition involves<br />the same set of shrinking decisions as the forwards<br />transitions. As the same brackets are sampled, the<br />1/(θmax−θmin) probabilities for drawing angles are the<br />same for the forwards and reverse transitions.<br />The reversibility of the transition operator (10) implies<br />that the target posterior distribution (2) is a station-<br />ary distribution of the Markov chain. Drawing f from<br />the stationary distribution and running the algorithm<br />draws a sample from the joint auxiliary distribution (7).<br />The deterministic transformations in (8) and (11) have<br />unit Jacobian, so the probability density of obtaining<br />a joint draw corresponding to (f?,y,ν?,{θ?<br />to the probability given by (7) for the original vari-<br />ables. The reversible property in (10) shows that this<br />is the same probability as generating the variables by<br />first generating f?from the target distribution and gen-<br />erating the remaining quantities using the algorithm.<br />Therefore, the marginal probability of f?is given by<br />the target posterior (2).<br />k}) is equal<br />Given the first angle, the distribution over the first pro-<br />posed move is N(f cosθ, Σsin2θ). Therefore, there is a<br />non-zero probability of transitioning to any region that<br />has non-zero probability under the posterior. This is<br />enough to ensure that, formally, the chain is irreducible<br />and aperiodic (Tierney, 1994). Therefore, the Markov<br />chain has a unique stationary distribution and repeated<br />applications of elliptical slice sampling to an arbitrary<br />starting point will asymptotically lead to points drawn<br />from the target posterior distribution (2).<br />2.4Slice sampling variants<br />There is some amount of choice in how the slice sampler<br />on the ellipse could be set up. Other methods for<br />proposing angles could have been used, as long as they<br />satisfied the reversible condition in (12). The particular<br />algorithm proposed in Figure 2 is appealing because it<br />is simple and has no free parameters.</p>  <p>Page 5</p> <p>Murray, Adams, MacKay<br />The algorithm must choose the initial edges of the<br />bracket [θmin,θmax] randomly. It would be aesthet-<br />ically pleasing to place the edges of the bracket at<br />the opposite side of the ellipse to the current position,<br />at ±π. However this deterministic bracket placement<br />would not be reversible and gives an invalid algorithm.<br />The edge of a randomly-chosen bracket could lie on<br />the ‘slice’, the acceptable region of states. Our recom-<br />mended elliptical slice sampling algorithm, Figure 2,<br />would accept this point. The initially-presented algo-<br />rithm, Figure 1, effectively randomly places the end-<br />points of the bracket but without checking this location<br />for acceptability. Apart from this small change, it can<br />be shown that the algorithms are equivalent.<br />In typical problems the slice will not cover the whole<br />ellipse. For example, if f is a representative sample<br />from a posterior, often −f will not be. Increasing the<br />probability of proposing points close to the current<br />state may increase efficiency. One way to do this would<br />be to shrink the bracket more aggressively (Skilling<br />and MacKay, 2003). Another would be to derive a<br />model from the auxiliary variable model (6), but with<br />a non-uniform distribution on θ. Another way would<br />be to randomly position an initial bracket of width less<br />than 2π —the code that we provide optionally allows<br />this. However, as explained in section 2.2, for high-<br />dimensional problems such tuning will often only give<br />small improvements. For smaller problems we have<br />seen it possible to improve the cpu-time efficiency of<br />the algorithm by around two times.<br />Another possible line of research is methods for biasing<br />proposals away from the current state. For example<br />the ‘over-relaxed’ methods discussed by Neal (2003)<br />have a bias towards the opposite side of the slice from<br />the current position. In our context it may be desirable<br />to encourage moves close to θ=π/2, as these moves are<br />independent of the previous position. These proposals<br />are only likely to be useful when the likelihood terms are<br />very weak, however. In the limit of sampling from the<br />prior due to a constant likelihood, the algorithm already<br />samples reasonably efficiently. To see this, consider<br />the distribution over the outcome after N iterations<br />initialized at f0:<br />fN= f0<br />N<br />?<br />n=1<br />cosθn+<br />N<br />?<br />m=1<br />νmsinθm<br />N<br />?<br />n=m+1<br />cosθn,<br />where νnand θnare values drawn at iteration n. Only<br />one angle is drawn per iteration when sampling from<br />the prior, because the first proposal is always accepted.<br />The only dependence on the initial state is the first<br />term, the coefficient of which shrinks towards zero<br />exponentially quickly.<br />2.5Limitations<br />A common modeling situation is that an unknown<br />constant offset, c ∼ N(0,σ2<br />entire latent vector f. The resulting variable, g=f+c, is<br />still Gaussian distributed, with the constant σ2<br />to every element of the covariance matrix. Neal (1999)<br />identified that this sort of covariance will not tend<br />to produce useful auxiliary draws ν. An iteration of<br />the Markov chain can only add a nearly-constant shift<br />to the current state. Indeed, covariances with large<br />constant terms are generally problematic as they tend<br />to be poorly conditioned. Instead, large offsets should<br />be modeled and sampled as separate variables.<br />m), has been added to the<br />madded<br />No algorithm can sample effectively from arbitrary dis-<br />tributions. As any distribution can be factored as in<br />(2), there exist likelihoods L(f) for which elliptical slice<br />sampling is not effective. Many Gaussian process appli-<br />cations have strong prior smoothness constraints and<br />relatively weak likelihood constraints. This important<br />regime is where we focus our empirical comparison.<br />3 Related work<br />Elliptical slice sampling builds on a Metropolis–<br />Hastings (M–H) update proposed by Neal (1999). Neal<br />reported that the original update performed moderately<br />better than using a more obvious M–H proposal,<br />f?= f + ?ν,ν ∼ N(0,Σ), (13)<br />and much better than Gibbs sampling for Gaussian<br />process classification. Neal also proposed using Hy-<br />brid/Hamiltonian Monte Carlo (Duane et al., 1987;<br />Neal, 1993), which can be very effective, but requires<br />tuning and the implementation of gradients. We now<br />consider some other alternatives that have similar re-<br />quirements to elliptical slice sampling.<br />3.1‘Conventional’ slice sampling<br />Elliptical slice sampling builds on the family of methods<br />introduced by Neal (2003). Several of the existing<br />slice sampling methods would also be easy to apply:<br />they only require point-wise evaluation of the posterior<br />up to a constant. These methods do have step-size<br />parameters, but unlike simple Metropolis methods,<br />typically the performance of slice samplers does not<br />crucially rely on carefully setting free parameters.<br />The most popular generic slice samplers use simple<br />univariate updates, although applying these directly<br />to f would suffer the same slow convergence problems<br />as Gibbs sampling. While Agarwal and Gelfand (2005)<br />have applied slice sampling for sampling parameters<br />in Gaussian spatial process models, they assumed a</p>  <p>Page 6</p> <p>Elliptical slice sampling<br />linear-Gaussian observation model. For non-Gaussian<br />data it was suggested that “there seems to be little role<br />for slice sampling.”<br />Elliptical slice sampling changes all of the variables in<br />f at once, although there are potentially better ways of<br />achieving this. An extensive search space of possibilities<br />includes the suggestions for multivariate updates made<br />by Neal (2003).<br />One simple possible slice sampling update performs a<br />univariate update along a random line traced out by<br />varying ? in (13). As the M–H method based on the<br />line worked less well than that based on an ellipse, one<br />might also expect a line-based slice sampler to perform<br />less well. Intuitively, in high dimensions much of the<br />mass of a Gaussian distribution is in a thin ellipsoidal<br />shell. A straight line will more rapidly escape this shell<br />than an ellipse passing through two points within it.<br />3.2 Control variables<br />Titsias et al. (2009) introduced a sampling method<br />inspired by sparse Gaussian process approximations.<br />M control variables fc are introduced such that the<br />joint prior p(f,fc) is Gaussian, and that f still has<br />marginal prior N(0,Σ). For Gaussian process models a<br />parametric family of joint covariances was defined, and<br />the model is optimized so that the control variables<br />are informative about the original variables: p(f | fc)<br />is made to be highly peaked. The optimization is a<br />pre-processing step that occurs before sampling begins.<br />The idea is that the small number of control variables fc<br />will be less strongly coupled than the original variables,<br />and so can be moved individually more easily than the<br />components of f. A proposal involves resampling one<br />control variable from the conditional prior and then<br />resampling f from p(f |fc). This move is accepted or<br />rejected with the Metropolis–Hastings rule.<br />Although the method is inspired by an approximation<br />used for large datasets, the accept/reject step uses the<br />full model. After O(N3) pre-processing it costs O(N2)<br />to evaluate a proposed change to the N-dimensional<br />vector f. One ‘iteration’ in the paper consisted of an<br />update for each control variable and so costs O(MN2)<br />— roughly M elliptical slice sampling updates. The<br />control method uses fewer likelihood evaluations per<br />iteration, although has some different minor costs asso-<br />ciated with book-keeping of the control variables.<br />3.3Local updates<br />In some applications it may make sense to update only<br />a subset of the latent variables at a time. This might<br />help for computational reasons given the O(N2) scaling<br />for drawing samples of subsets of size N. Titsias et al.<br />(2009) also identified suitable subsets for local updates<br />and then investigated sampling proposals from the<br />conditional Gaussian prior.<br />In fact, local updates can be combined with any tran-<br />sition operator for models with Gaussian priors. If<br />fAis a subset of variables to update and fB are the<br />remaining variables, we can write the prior as:<br />?fA<br />and the conditional prior is:<br />fB<br />?<br />∼ N<br />?<br />0,<br />?ΣA,A<br />ΣA,B<br />ΣB,B<br />ΣB,A<br />??<br />(14)<br />p(fA|fB) = N(fA; m,S), where<br />m = ΣA,BΣ−1<br />B,BfB, and S = ΣA,A−ΣA,BΣ−1<br />B,BΣB,A.<br />A change of variables g=fA−m allows us to express the<br />conditional posterior as: p?(g) ∝ N(g; 0, S)L??g+m<br />ternative, to update g (and thus fA). Updating groups<br />of variables according to their conditional distributions<br />is a standard way of sampling from a joint distribution.<br />fB<br />??.<br />We can then apply elliptical slice sampling, or any al-<br />4 Experiments<br />We performed an empirical comparison on three Gaus-<br />sian process based probabilistic modeling tasks. Only<br />a brief description of the models and methods can<br />be given here. Full code to reproduce the results is<br />provided as supplementary material.<br />4.1Models<br />Each of the models associates a dimension of the latent<br />variable, fn, with an ‘input’ or ‘feature’ vector xn. The<br />models in our experiments construct the covariance<br />from the inputs using the most common method,<br />?D<br />the squared-exponential or “Gaussian” covariance.<br />This covariance has “lengthscale” parameter ? and<br />an overall “signal variance” σ2<br />be more appropriate in many modeling situations, but<br />our algorithm would apply unchanged.<br />Σij = σ2<br />fexp?−1<br />2<br />d=1(xd,i− xd,j)2/?2?, (15)<br />f. Other covariances may<br />Gaussian regression: given observations y of the<br />latent variables with Gaussian noise of variance σ2<br />Lr(f) =?<br />the posterior is Gaussian and so fully tractable. We<br />use this as a simple test that the method is working<br />correctly. Differences in performance on this task will<br />also give some indication of performance with a simple<br />log-concave likelihood function.<br />n,<br />nN(yn;fn,σ2<br />n), (16)</p>  <p>Page 7</p> <p>Murray, Adams, MacKay<br />We generated ten synthetic datasets with input fea-<br />ture dimensions from one to ten. Each dataset was<br />of size N =200, with inputs {xn}N<br />from a D-dimensional unit hypercube and function<br />values drawn from a Gaussian prior, f ∼N(0,Σ), using<br />covariance (15) with lengthscale ?=1 and unit signal<br />variance, σ2<br />added to generate the observations.<br />n=1drawn uniformly<br />f= 1. Noise with variance σ2<br />n= 0.32was<br />Gaussian process classification: a well-explored<br />application of Gaussian processes with a non-Gaussian<br />noise model is binary classification:<br />Lc(f) =?<br />where yn∈ {−1,+1} are the label data and σ(a) is a<br />sigmoidal function: 1/(1+e−a) for the logistic classifier;<br />a cumulative Gaussian for the probit classifier.<br />nσ (ynfn), (17)<br />We ran tests on the USPS classification problem<br />as set up by Kuss and Rasmussen (2005).<br />used logσf=3.5, log?=2.5 and the logistic likelihood.<br />We<br />Log Gaussian Cox process: an inhomogeneous Pois-<br />son process with a non-parametric rate can be con-<br />structed by using a shifted draw from a Gaussian pro-<br />cess as the log-intensity function. Approximate infer-<br />ence can be performed by discretizing the space into<br />bins and assuming that the log-intensity is uniform in<br />each bin (Møller et al., 1998). Each bin contributes a<br />Poisson likelihood:<br />Lp(f) =<br />?<br />n<br />λnynexp(−λn)<br />yn!<br />,λn=efn+m, (18)<br />where the model explains the yn counts in bin n as<br />drawn from a Poisson distribution with mean λn. The<br />offset to the log mean, m, is the mean log-intensity of<br />the Poisson process plus the log of the bin size.<br />We perform inference for a Cox process model of the<br />dates of mining disasters taken from a standard data<br />set for testing point processes (Jarrett, 1979). The<br />191 events were placed into 811 bins of 50 days each.<br />The Gaussian process parameters were fixed to σ2<br />and ?=13516 days (a third of the range of the dataset).<br />The offset m in (18) was set to m=log(191/811), to<br />match the empirical mean rate.<br />f=1<br />4.2Results<br />A trace of the samples’ log-likelihoods, Figure 4, shows<br />that elliptical slice sampling and control variables sam-<br />pling have different behavior. The methods make differ-<br />ent types of moves and only control variables sampling<br />contains rejections. Using long runs of either method<br />to estimate expectations under the target distribution<br />is valid. However, sticking in a state due to many<br />rejections can give a poor estimator as can always mak-<br />0 5001000<br />−45<br />−40<br />−35<br /># Control variable moves<br />log L<br />Control Variables<br />0 5001000<br />−45<br />−40<br />−35<br /># Iterations<br />Elliptical slice sampling<br />Figure 4: Traces of log-likelihoods for the 1-dimensional GP<br />regression experiment. Both lines are made with 333 points<br />plotted after each sweep through M =3 control variables<br />and after every 3 iterations of elliptical slice sampling.<br />0 5000 10000<br />−140<br />−120<br />−100<br />−80<br />−60<br />−40<br /># Control variable moves<br />log L<br />Control Variables<br />0 500010000<br />−140<br />−120<br />−100<br />−80<br />−60<br />−40<br /># Iterations<br />Elliptical slice sampling<br />Figure 5: As in Figure 4 but for 10-dimensional regression<br />and plotting every M =78 iterations. (Control variables<br />didn’t move on this run.)<br />ing small moves. It can be difficult to judge overall<br />sampling quality from trace plots alone.<br />As a quantitative measure of quality we estimated the<br />“effective number of samples” from log-likelihood traces<br />using R-CODA (Cowles et al., 2006). Figure 6 shows<br />these results along with computer time taken. The step<br />size for Neal’s Metropolis method was chosen using a<br />grid search to maximize performance. Times are for the<br />provided implementations under Matlab v7.8 on a sin-<br />gle 64bit, 3GHz Intel Xeon CPU. Comparing runtimes<br />is always problematic, due to implementation-specific<br />details. Our numbers of effective samples are primarily<br />plotted for the same number of O(N2) updates with<br />the understanding that some correction based loosely<br />on runtime should be applied.<br />The control variables approach was particularly recom-<br />mended for Gaussian processes with low-dimensional<br />input spaces.On our particular low-dimensional<br />synthetic regression problems using control variables<br />clearly outperforms all the other methods. On the<br />model of mining disasters, control variable sampling<br />has comparable performance to elliptical slice sampling<br />with about 50% less run time. On higher-dimensional<br />problems more control variables are required; then<br />other methods cost less. Control variables failed to sam-<br />ple in high-dimensions (Figure 5). On the USPS classifi-<br />cation problem control variables ran exceedingly slowly<br />and we were unable to obtain any meaningful results.<br />Elliptical slice sampling obtained more effective samples<br />than Neal’s M–H method with the best possible step<br />size, although at the cost of increased run time. On the<br />problems involving real data, elliptical slice sampling</p>  <p>Page 8</p> <p>Elliptical slice sampling<br />Figure 6: Number of effective samples from 105iterations after 104burn in, with time and likelihood evaluations required.<br />The means and standard deviations for 100 runs are shown (divide the “error bars” by 10 to get standard errors on the<br />mean, which are small). Each iteration involves one O(N2) operation (e.g. one ν draw or updating one control variable).<br />Each group of bars in the top two rows has been rescaled for readability: the numbers beneath each group show the<br />number of effective samples or CPU time in seconds for elliptical slice sampling, which always has bars of height 1.<br />was better overall whereas M–H has more effective<br />samples per unit time (in our implementation) on the<br />synthetic problems. The performance differences aren’t<br />huge; either method would work well enough.<br />Elliptical slice sampling takes less time than slice sam-<br />pling along a straight line (line sampling involves addi-<br />tional prior evaluations) and usually performs better.<br />5 Discussion<br />The slice samplers use many more likelihood evaluations<br />than the other methods.<br />our code can take a step-size parameter to reduce the<br />number of likelihood evaluations (Section 2.4). On<br />these problems the time for likelihood computations<br />isn’t completely negligible: speedups of around ×2 may<br />be possible by tuning elliptical slice sampling. Our<br />default position is that ease-of-use and human time is<br />important and that the advantage of having no free<br />parameters should often be taken in exchange for a<br />factor of two in runtime.<br />This is partly by choice:<br />We fixed the parameters of Σ and L in our experi-<br />ments to simplify the comparison. Fixing the model<br />potentially favors the methods that have adjustable<br />parameters. In problems were Σ and L change dra-<br />matically, a single step-size or optimized set of control<br />variables could work very poorly.<br />Elliptical slice sampling is a simple generic algorithm<br />with no tweak parameters. It performs similarly to the<br />best possible performance of a related M–H scheme,<br />and could be applied to a wide variety of applications<br />in both low and high dimensions.<br />Acknowledgements<br />Thanks to Michalis Titsias for code, Sinead Williamson<br />and Katherine Heller for a helpful discussion, and to<br />Radford Neal, Sam Roweis, Christophe Andrieu and<br />the reviewers for useful suggestions. RPA is funded by<br />the Canadian Institute for Advanced Research.<br />References<br />D. K. Agarwal and A. E. Gelfand.<br />simulation based fitting of spatial data models. Statistics<br />and Computing, 15(1):61–69, 2005.<br />M. K. Cowles, N. Best, K. Vines, and M. Plummer. R-<br />CODA 0.10-5, 2006. http://www-fis.iarc.fr/coda/.<br />S. Duane, A. D. Kennedy, B. J. Pendleton, and D. Roweth.<br />Hybrid Monte Carlo. Physics Letters B, 195(2):216–222,<br />September 1987.<br />R. G. Jarrett. A note on the intervals between coal-mining<br />disasters. Biometrika, 66(1):191–193, 1979.<br />M. Kuss and C. E. Rasmussen. Assessing approximate infer-<br />ence for binary Gaussian process classification. Journal<br />of Machine Learning Research, 6:1679–1704, 2005.<br />J. Møller, A. R. Syversveen, and R. P. Waagepetersen.<br />Log Gaussian Cox processes. Scandinavian Journal of<br />Statistics, 25(3):451–482, 1998.<br />R. M. Neal. Probabilistic inference using Markov chain<br />Monte Carlo methods. Technical Report CRG-TR-93-1,<br />Dept. of Computer Science, University of Toronto, 1993.<br />R. M. Neal. Regression and classification using Gaussian<br />process priors. In J. M. Bernardo et al., editors, Bayesian<br />Statistics 6, pages 475–501. OU Press, 1999.<br />R. M. Neal. Slice sampling. Annals of Statistics, 31(3):<br />705–767, 2003.<br />C. E. Rasmussen and C. K. I. Williams. Gaussian Processes<br />for machine learning. MIT Press, 2006.<br />H. Rue, S. Martino, and N. Chopin. Approximate Bayesian<br />inference for latent Gaussian models by using integrated<br />nested Laplace approximations. Journal of the Royal<br />Statistical Society: Series B, 71(2):319–392, 2009.<br />J. Skilling and D. J. C. MacKay. Slice sampling — a binary<br />implementation. Annals of Statistics, 31(3), 2003.<br />L. Tierney. Markov chains for exploring posterior distribu-<br />tions. The Annals of Statistics, 22(4):1701–1728, 1994.<br />M. Titsias, N. D. Lawrence, and M. Rattray. Efficient<br />sampling for Gaussian process inference using control<br />variables. In Advances in Neural Information Processing<br />Systems 21, pages 1681–1688, 2009.<br />Slice sampling for</p>   </div> <div id="rgw19_56aba00261acf" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw20_56aba00261acf">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw21_56aba00261acf"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://arxiv.org/pdf/1001.0175.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Elliptical Slice Sampling.">Elliptical Slice Sampling.</a> </div>  <div class="details">   Available from <a href="http://arxiv.org/pdf/1001.0175.pdf" target="_blank" rel="nofollow">ArXiv</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw23_56aba00261acf" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw24_56aba00261acf">  </ul> </div> </div>   <div id="rgw15_56aba00261acf" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw16_56aba00261acf"> <div> <h5> <a href="publication/291340346_Temperature-Dependent_Emission_Kinetics_of_Colloidal_Semiconductor_Nanoplatelets_Strongly_Modified_by_Stacking" class="color-inherit ga-similar-publication-title"><span class="publication-title">Temperature-Dependent Emission Kinetics of Colloidal Semiconductor Nanoplatelets Strongly Modified by Stacking</span></a>  </h5>  <div class="authors"> <a href="researcher/2059777116_Onur_Erdem" class="authors ga-similar-publication-author">Onur Erdem</a>, <a href="researcher/31320601_Murat_Olutas" class="authors ga-similar-publication-author">Murat Olutas</a>, <a href="researcher/60001925_Burak_Guzelturk" class="authors ga-similar-publication-author">Burak Guzelturk</a>, <a href="researcher/2008924001_Yusuf_Kelestemur" class="authors ga-similar-publication-author">Yusuf Kelestemur</a>, <a href="researcher/38633698_Hilmi_Volkan_Demir" class="authors ga-similar-publication-author">Hilmi Volkan Demir</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56aba00261acf"> <div> <h5> <a href="publication/282897577_A_Retinex_model_based_on_Absorbing_Markov_Chains" class="color-inherit ga-similar-publication-title"><span class="publication-title">A Retinex model based on Absorbing Markov Chains</span></a>  </h5>  <div class="authors"> <a href="researcher/12000465_Gabriele_Gianini" class="authors ga-similar-publication-author">Gabriele Gianini</a>, <a href="researcher/12409286_Alessandro_Rizzi" class="authors ga-similar-publication-author">Alessandro Rizzi</a>, <a href="researcher/6725194_Ernesto_Damiani" class="authors ga-similar-publication-author">Ernesto Damiani</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56aba00261acf"> <div> <h5> <a href="publication/283656157_Wind_Power_Forecast_Error_Probabilistic_Model_Using_Markov_Chains" class="color-inherit ga-similar-publication-title"><span class="publication-title">Wind Power Forecast Error Probabilistic Model Using Markov Chains</span></a>  </h5>  <div class="authors"> <a href="researcher/2084342649_S_Martin_Martinez" class="authors ga-similar-publication-author">S. Martín Martinez</a>, <a href="researcher/2084370777_A_Honrubia_Escribano" class="authors ga-similar-publication-author">A. Honrubia Escribano</a>, <a href="researcher/2084353056_M_Canas_Carreton" class="authors ga-similar-publication-author">M. Cañas Carretón</a>, <a href="researcher/2084376137_V_Guerrero_Mestre" class="authors ga-similar-publication-author">V. Guerrero Mestre</a>, <a href="researcher/2084375230_E_Gomez_Lazaro" class="authors ga-similar-publication-author">E. Gómez Lázaro</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw29_56aba00261acf" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw30_56aba00261acf">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw31_56aba00261acf" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=KsAUFGTqpVyMtV-O2R85PXTzgcY3e8qNhyQPGGjsx-MzJ0_NZSQvkG4vBdO6gHNz" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="9dW0n1jDgA4+fXqXdn9uFVsobghe7vv920BUmyljh1Vqt0pVwrNbLlHGTIFN+Y+qPsgrXrFp+CBTGdrFcGfDprKzwkTt3nZhry+clDeqjyXKk/ZvGvzwIQ9Q52ZZmhZJLnE76vv4NKuTaRLpuvglEcCyXMb6O/OWhog9pXZzN7Sdvv6ynSvmh95kB8fxsmj34+CNj/IwWJNAuD/NfB0wAaeih7GDv91k0PjbAtQt6ErXoesdzAudNm/NKFr7ds8f3L9h04Dk1uDWiiJVdhrcVT3ekVhoeeXjVKDoRcnXCC0="/> <input type="hidden" name="urlAfterLogin" value="publication/45893043_Elliptical_Slice_Sampling"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vNDU4OTMwNDNfRWxsaXB0aWNhbF9TbGljZV9TYW1wbGluZw%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vNDU4OTMwNDNfRWxsaXB0aWNhbF9TbGljZV9TYW1wbGluZw%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vNDU4OTMwNDNfRWxsaXB0aWNhbF9TbGljZV9TYW1wbGluZw%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw32_56aba00261acf"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 622;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"David Mackay","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272707275194376%401442029889420_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/David_Mackay","institution":"University of Cambridge","institutionUrl":false,"widgetId":"rgw4_56aba00261acf"},"id":"rgw4_56aba00261acf","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=2865457","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56aba00261acf"},"id":"rgw3_56aba00261acf","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=45893043","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":45893043,"title":"Elliptical Slice Sampling.","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"arXiv preprint arXiv","publicationDate":"12\/2009;","publicationDateRobot":"2009-12","article":"1001."}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1001.0175","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Elliptical Slice Sampling."},{"key":"rft.title","value":"arXiv preprint arXiv"},{"key":"rft.jtitle","value":"arXiv preprint arXiv"},{"key":"rft.volume","value":"1001"},{"key":"rft.date","value":"2009"},{"key":"rft.au","value":"Iain Murray,Ryan Prescott Adams,David J. C. MacKay"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56aba00261acf"},"id":"rgw6_56aba00261acf","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=45893043","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":45893043,"peopleItems":[{"data":{"authorUrl":"researcher\/45470150_Iain_Murray","authorNameOnPublication":"Iain Murray","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Iain Murray","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/45470150_Iain_Murray","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw9_56aba00261acf"},"id":"rgw9_56aba00261acf","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=45470150&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw8_56aba00261acf"},"id":"rgw8_56aba00261acf","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=45470150&authorNameOnPublication=Iain%20Murray","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/32349423_Ryan_Prescott_Adams","authorNameOnPublication":"Ryan Prescott Adams","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Ryan Prescott Adams","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/32349423_Ryan_Prescott_Adams","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw11_56aba00261acf"},"id":"rgw11_56aba00261acf","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=32349423&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw10_56aba00261acf"},"id":"rgw10_56aba00261acf","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=32349423&authorNameOnPublication=Ryan%20Prescott%20Adams","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"David Mackay","accountUrl":"profile\/David_Mackay","accountKey":"David_Mackay","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272707275194376%401442029889420_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"David Mackay","profile":{"professionalInstitution":{"professionalInstitutionName":"University of Cambridge","professionalInstitutionUrl":"institution\/University_of_Cambridge"}},"professionalInstitutionName":"University of Cambridge","professionalInstitutionUrl":"institution\/University_of_Cambridge","url":"profile\/David_Mackay","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272707275194376%401442029889420_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"David_Mackay","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw13_56aba00261acf"},"id":"rgw13_56aba00261acf","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=2865457&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University of Cambridge","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":1,"publicationUid":45893043,"widgetId":"rgw12_56aba00261acf"},"id":"rgw12_56aba00261acf","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=2865457&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=1&publicationUid=45893043","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56aba00261acf"},"id":"rgw7_56aba00261acf","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=45893043&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":45893043,"abstract":"<noscript><\/noscript><div>Many probabilistic models introduce strong dependencies between variables using a latent multivariate Gaussian distribution or a Gaussian process. We present a new Markov chain Monte Carlo algorithm for performing inference in models with multivariate Gaussian priors. Its key properties are: 1) it has simple, generic code applicable to many models, 2) it has no free parameters, 3) it works well for a variety of Gaussian process based models. These properties make our method ideal for use while model building, removing the need to spend time deriving and tuning updates for more complex algorithms. Comment: 8 pages, 6 figures, appearing in AISTATS 2010 (JMLR: W&amp;CP volume 6). Differences from first submission: some minor edits in response to feedback.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw14_56aba00261acf"},"id":"rgw14_56aba00261acf","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=45893043","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/45893043_Elliptical_Slice_Sampling\/links\/0f64778138294e886aa1aa25\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw5_56aba00261acf"},"id":"rgw5_56aba00261acf","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=45893043&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2059777116,"url":"researcher\/2059777116_Onur_Erdem","fullname":"Onur Erdem","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":31320601,"url":"researcher\/31320601_Murat_Olutas","fullname":"Murat Olutas","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":60001925,"url":"researcher\/60001925_Burak_Guzelturk","fullname":"Burak Guzelturk","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2008924001,"url":"researcher\/2008924001_Yusuf_Kelestemur","fullname":"Yusuf Kelestemur","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Journal of Physical Chemistry Letters","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291340346_Temperature-Dependent_Emission_Kinetics_of_Colloidal_Semiconductor_Nanoplatelets_Strongly_Modified_by_Stacking","usePlainButton":true,"publicationUid":291340346,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"7.46","url":"publication\/291340346_Temperature-Dependent_Emission_Kinetics_of_Colloidal_Semiconductor_Nanoplatelets_Strongly_Modified_by_Stacking","title":"Temperature-Dependent Emission Kinetics of Colloidal Semiconductor Nanoplatelets Strongly Modified by Stacking","displayTitleAsLink":true,"authors":[{"id":2059777116,"url":"researcher\/2059777116_Onur_Erdem","fullname":"Onur Erdem","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":31320601,"url":"researcher\/31320601_Murat_Olutas","fullname":"Murat Olutas","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":60001925,"url":"researcher\/60001925_Burak_Guzelturk","fullname":"Burak Guzelturk","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2008924001,"url":"researcher\/2008924001_Yusuf_Kelestemur","fullname":"Yusuf Kelestemur","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38633698,"url":"researcher\/38633698_Hilmi_Volkan_Demir","fullname":"Hilmi Volkan Demir","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of Physical Chemistry Letters 01\/2016;  DOI:10.1021\/acs.jpclett.5b02763"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291340346_Temperature-Dependent_Emission_Kinetics_of_Colloidal_Semiconductor_Nanoplatelets_Strongly_Modified_by_Stacking","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291340346_Temperature-Dependent_Emission_Kinetics_of_Colloidal_Semiconductor_Nanoplatelets_Strongly_Modified_by_Stacking\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56aba00261acf"},"id":"rgw16_56aba00261acf","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291340346","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":12000465,"url":"researcher\/12000465_Gabriele_Gianini","fullname":"Gabriele Gianini","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":12409286,"url":"researcher\/12409286_Alessandro_Rizzi","fullname":"Alessandro Rizzi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":6725194,"url":"researcher\/6725194_Ernesto_Damiani","fullname":"Ernesto Damiani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Information Sciences","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/282897577_A_Retinex_model_based_on_Absorbing_Markov_Chains","usePlainButton":true,"publicationUid":282897577,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"4.04","url":"publication\/282897577_A_Retinex_model_based_on_Absorbing_Markov_Chains","title":"A Retinex model based on Absorbing Markov Chains","displayTitleAsLink":true,"authors":[{"id":12000465,"url":"researcher\/12000465_Gabriele_Gianini","fullname":"Gabriele Gianini","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":12409286,"url":"researcher\/12409286_Alessandro_Rizzi","fullname":"Alessandro Rizzi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":6725194,"url":"researcher\/6725194_Ernesto_Damiani","fullname":"Ernesto Damiani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Information Sciences 01\/2016; 327:149-174. DOI:10.1016\/j.ins.2015.08.015"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/282897577_A_Retinex_model_based_on_Absorbing_Markov_Chains","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/282897577_A_Retinex_model_based_on_Absorbing_Markov_Chains\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56aba00261acf"},"id":"rgw17_56aba00261acf","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=282897577","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2084342649,"url":"researcher\/2084342649_S_Martin_Martinez","fullname":"S. Mart\u00edn Martinez","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084370777,"url":"researcher\/2084370777_A_Honrubia_Escribano","fullname":"A. Honrubia Escribano","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084353056,"url":"researcher\/2084353056_M_Canas_Carreton","fullname":"M. Ca\u00f1as Carret\u00f3n","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2084376137,"url":"researcher\/2084376137_V_Guerrero_Mestre","fullname":"V. Guerrero Mestre","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283656157_Wind_Power_Forecast_Error_Probabilistic_Model_Using_Markov_Chains","usePlainButton":true,"publicationUid":283656157,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/283656157_Wind_Power_Forecast_Error_Probabilistic_Model_Using_Markov_Chains","title":"Wind Power Forecast Error Probabilistic Model Using Markov Chains","displayTitleAsLink":true,"authors":[{"id":2084342649,"url":"researcher\/2084342649_S_Martin_Martinez","fullname":"S. Mart\u00edn Martinez","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084370777,"url":"researcher\/2084370777_A_Honrubia_Escribano","fullname":"A. Honrubia Escribano","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084353056,"url":"researcher\/2084353056_M_Canas_Carreton","fullname":"M. Ca\u00f1as Carret\u00f3n","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084376137,"url":"researcher\/2084376137_V_Guerrero_Mestre","fullname":"V. Guerrero Mestre","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084375230,"url":"researcher\/2084375230_E_Gomez_Lazaro","fullname":"E. G\u00f3mez L\u00e1zaro","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283656157_Wind_Power_Forecast_Error_Probabilistic_Model_Using_Markov_Chains","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283656157_Wind_Power_Forecast_Error_Probabilistic_Model_Using_Markov_Chains\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56aba00261acf"},"id":"rgw18_56aba00261acf","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=283656157","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw15_56aba00261acf"},"id":"rgw15_56aba00261acf","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=45893043&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":45893043,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":45893043,"publicationType":"article","linkId":"0f64778138294e886aa1aa25","fileName":"Elliptical Slice Sampling.","fileUrl":"http:\/\/arxiv.org\/pdf\/1001.0175.pdf","name":"ArXiv","nameUrl":"http:\/\/arxiv.org\/pdf\/1001.0175.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw21_56aba00261acf"},"id":"rgw21_56aba00261acf","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=45893043&linkId=0f64778138294e886aa1aa25&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw20_56aba00261acf"},"id":"rgw20_56aba00261acf","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=45893043&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":4,"valueFormatted":"4","widgetId":"rgw22_56aba00261acf"},"id":"rgw22_56aba00261acf","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=45893043","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw19_56aba00261acf"},"id":"rgw19_56aba00261acf","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=45893043&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":45893043,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw24_56aba00261acf"},"id":"rgw24_56aba00261acf","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=45893043&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":4,"valueFormatted":"4","widgetId":"rgw25_56aba00261acf"},"id":"rgw25_56aba00261acf","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=45893043","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw23_56aba00261acf"},"id":"rgw23_56aba00261acf","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=45893043&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Elliptical slice sampling\nIain Murray\nUniversity of Toronto\nRyan Prescott Adams\nUniversity of Toronto\nDavid J.C. MacKay\nUniversity of Cambridge\nAbstract\nMany probabilistic models introduce strong\ndependencies between variables using a latent\nmultivariate Gaussian distribution or a Gaus-\nsian process. We present a new Markov chain\nMonte Carlo algorithm for performing infer-\nence in models with multivariate Gaussian\npriors. Its key properties are: 1) it has simple,\ngeneric code applicable to many models, 2) it\nhas no free parameters, 3) it works well for\na variety of Gaussian process based models.\nThese properties make our method ideal for\nuse while model building, removing the need\nto spend time deriving and tuning updates\nfor more complex algorithms.\n1Introduction\nThe multivariate Gaussian distribution is commonly\nused to specify a priori beliefs about dependencies\nbetween latent variables in probabilistic models. The\nparameters of such a Gaussian may be specified directly,\nas in graphical models and Markov random fields, or\nimplicitly as the marginals of a Gaussian process (GP).\nGaussian processes may be used to express concepts of\nspatial or temporal coherence, or may more generally\nbe used to construct Bayesian kernel methods for non-\nparametric regression and classification. Rasmussen\nand Williams (2006) provide a recent review of GPs.\nInferences can only be calculated in closed form for\nthe simplest Gaussian latent variable models. Recent\nwork shows that posterior marginals can sometimes be\nwell approximated with deterministic methods (Kuss\nand Rasmussen, 2005; Rue et al., 2009). Markov chain\nMonte Carlo (MCMC) methods represent joint pos-\nterior distributions with samples (e.g. Neal, 1993).\nMCMC can be slower but applies more generally.\nAppearing in Proceedings of the 13thInternational Con-\nference on Artificial Intelligence and Statistics (AISTATS)\n2010, Chia Laguna Resort, Sardinia, Italy. Volume 6 of\nJMLR: W&CP 6. Copyright 2010 by the authors.\nIn some circumstances MCMC provides good results\nwith minimal model-specific implementation. Gibbs\nsampling, in particular, is frequently used to sample\nfrom probabilistic models in a straightforward way, up-\ndating one variable at a time. In models with strong\ndependencies among variables, including many with\nGaussian priors, Gibbs sampling is known to perform\npoorly. Several authors have previously addressed the\nissue of sampling from models containing strongly cor-\nrelated Gaussians, notably the recent work of Titsias\net al. (2009). In this paper we provide a technique\ncalled elliptical slice sampling that is simpler and of-\nten faster than other methods, while also removing the\nneed for preliminary tuning runs. Our method provides\na drop-in replacement for MCMC samplers of Gaussian\nmodels that are currently using Gibbs or Metropolis\u2013\nHastings and we demonstrate empirical success against\ncompeting methods with several different GP-based\nlikelihood models.\n2Elliptical slice sampling\nOur objective is to sample from a posterior distri-\nbution over latent variables that is proportional to\nthe product of a multivariate Gaussian prior and a\nlikelihood function that ties the latent variables to the\nobserved data. We will use f to indicate the vector\nof latent variables that we wish to sample and denote\na zero-mean Gaussian distribution with covariance \u03a3 by\nN(f;0,\u03a3) \u2261 |2\u03c0\u03a3|\u22121\/2exp?\u22121\nWe also use f \u223c N(0,\u03a3) to state that f is drawn from\na distribution with the density in (1).\nwith non-zero means can simply be shifted to have\nzero-mean with a change of variables; an example\nwill be given in Section 3.3. We use L(f) = p(data|f)\nto denote the likelihood function so that our target\ndistribution for the MCMC sampler is\np?(f) =1\nZN(f;0,\u03a3)L(f),\nwhere Z is the normalization constant, or the marginal\nlikelihood, of the model.\n2f?\u03a3\u22121f?.(1)\nGaussians\n(2)\nOur starting point is a Metropolis\u2013Hastings method\nintroduced by Neal (1999). Given an initial state f, a\narXiv:1001.0175v2  [stat.CO]  19 Mar 2010"},{"page":2,"text":"Elliptical slice sampling\nnew state\nf?=\n?\n1 \u2212 ?2f + ?\u03bd,\u03bd \u223c N(0,\u03a3) (3)\nis proposed, where ? \u2208 [\u22121,1] is a step-size parameter.\nThe proposal is a sample from the prior for ?=1 and\nmore conservative for values closer to zero. The move\nis accepted with probability\np(accept) = min(1, L(f?)\/L(f)), (4)\notherwise the next state in the chain is a copy of f.\nNeal reported that for some Gaussian process classifiers\nthe Metropolis\u2013Hastings method was many times faster\nthan Gibbs sampling. The method is also simpler to\nimplement and can immediately be applied to a much\nwider variety of models with Gaussian priors.\nA drawback, identified by Neal (1999), is that the\nstep-size ? needs to be chosen appropriately for the\nMarkov chain to mix efficiently. This may require\npreliminary runs. Usually parameters of the covariance\n\u03a3 and likelihood function L are also inferred from data.\nDifferent step-size parameters may be needed as the\nmodel parameters are updated. It would be desirable\nto automatically search over the step-size parameter,\nwhile maintaining a valid algorithm.\nFor a fixed auxiliary random draw, \u03bd, the locus of\npossible proposals by varying ? \u2208 [\u22121,1] in (3) is half\nof an ellipse.\nf?= \u03bd sin\u03b8 + f cos\u03b8, (5)\ndefining a full ellipse passing through the current state f\nand the auxiliary draw \u03bd. For a fixed \u03b8 there is an\nequivalent ? that gives the same proposal distribution\nin the original algorithm. However, if we can search\nover the step-size, the full ellipse gives a richer choice\nof updates for a given \u03bd.\n2.1 Sampling an alternative model\n\u2018Slice sampling\u2019 (Neal, 2003) provides a way to sample\nalong a line with an adaptive step-size. Proposals are\ndrawn from an interval or \u2018bracket\u2019 which, if too large,\nis shrunk automatically until an acceptable point is\nfound. There are also ways to automatically enlarge\nsmall initial brackets. Naively applying these adaptive\nalgorithms to select the value of ? in (3) or \u03b8 in (5)\ndoes not result in a Markov chain transition operator\nwith the correct stationary distribution. The locus of\nstates is defined using the current position f, which\nupsets the reversibility and correctness of the update.\nWe would like to construct a valid Markov chain tran-\nsition operator on the ellipse of states that uses slice\nsampling\u2019s existing ability to adaptively pick step sizes.\nInput: current state f, a routine that samples from\nN(0,\u03a3), log-likelihood function logL.\nOutput: a new state f?. When f is drawn from p?(f)\u221d\nN(f;0,\u03a3)L(f), the marginal distribution of f?is also p?.\n1. Sample from p(\u03bd0,\u03bd1,\u03b8|(\u03bd0sin\u03b8+\u03bd1cos\u03b8=f)):\n\u03b8 \u223c Uniform[0,2\u03c0]\n\u03bd \u223c N(0,\u03a3)\n\u03bd0 \u2190 f sin\u03b8 + \u03bd cos\u03b8\n\u03bd1 \u2190 f cos\u03b8 \u2212 \u03bd sin\u03b8\n2. Update \u03b8 \u2208 [0,2\u03c0] using slice sampling (Neal, 2003)\non:\np?(\u03b8|\u03bd0,\u03bd1) \u221d L(\u03bd0sin\u03b8 + \u03bd1cos\u03b8)\n3. return f?= \u03bd0sin\u03b8 + \u03bd1cos\u03b8\nFigure 1: Intuition behind elliptical slice sampling. This\nis a valid algorithm, but will be adapted (Figure 2).\nWe will first intuitively construct a valid method by\npositing an augmented probabilistic model in which\nthe step-size is a variable. Standard slice sampling algo-\nrithms then apply to that model. We will then adjust\nthe algorithm for our particular setting to provide a\nsecond, slightly tidier algorithm.\nOur augmented probabilistic model replaces the origi-\nnal latent variable with prior f \u223c N(0,\u03a3) with\n\u03bd0\u223c N(0,\u03a3)\n\u03bd1\u223c N(0,\u03a3)\n\u03b8 \u223c Uniform[0,2\u03c0]\nf = \u03bd0sin\u03b8 + \u03bd1cos\u03b8.\n(6)\nThe marginal distribution over the original latent vari-\nable f is still N(0,\u03a3), so the distribution over data\nis identical. However, we can now sample from the\nposterior over the new latent variables:\np?(\u03bd0,\u03bd1,\u03b8) \u221d N(\u03bd0;0,\u03a3)N(\u03bd1;0,\u03a3)L(f(\u03bd0,\u03bd1,\u03b8)),\nand use the values of f deterministically derived from\nthese samples. Our first approach applies two Monte\nCarlo transition operators that leave the new latent\nposterior invariant.\nOperator 1: jointly resample the latents \u03bd0,\u03bd1,\u03b8\ngiven the constraint that f(\u03bd0,\u03bd1,\u03b8) is unchanged. Be-\ncause the effective variable of interest doesn\u2019t change,\nthe likelihood does not affect this conditional distribu-\ntion, so the update is generic and easy to implement.\nOperator 2: use a standard slice sampling algorithm\nto update the step-size \u03b8 given the other variables.\nThe resulting algorithm is given in Figure 1. The\nauxiliary model construction makes the link to slice\nsampling explicit, which makes it easy to understand\nthe validity of the approach. However, the algorithm"},{"page":3,"text":"Murray, Adams, MacKay\ncan be neater and the [0,2\u03c0] range for slice sampling\nis unnatural on an ellipse. The algorithm that we will\npresent in detail results from eliminating \u03bd0and \u03bd1\nand a different way of setting slice sampling\u2019s initial\nproposal range. The precise connection will be given\nin Section 2.4. A more direct, technical proof that\nthe equilibrium distribution of the Markov chain is the\ntarget distribution is presented in Section 2.3.\nElliptical slice sampling, our proposed algorithm is\ngiven in Figure 2, which includes the details of the slice\nsampler. An example run is illustrated in Figure 3(a\u2013d).\nEven for high-dimensional problems, the states consid-\nered within one update lie in a two-dimensional plane.\nIn high dimensions f and \u03bd are likely to have similar\nlengths and be an angle of \u03c0\/2 apart. Therefore the\nellipse will typically be fairly close to a circle, although\nthis is not required for the validity of the algorithm.\nAs intended, our slice sampling approach selects a\nnew location on the randomly generated ellipse in (5).\nThere are no rejections: the new state f?is never equal\nto the current state f unless that is the only state on\nthe ellipse with non-zero likelihood. The algorithm\nproposes the angle \u03b8 from a bracket [\u03b8min,\u03b8max] which\nis shrunk exponentially quickly until an acceptable\nstate is found. Thus the step size is effectively adapted\non each iteration for the current \u03bd and \u03a3.\n2.2Computational cost\nDrawing \u03bd costs O(N3), for N-dimensional f and gen-\neral \u03a3. The usual implementation of a Gaussian sam-\npler would involve caching a (Cholesky) decomposition\nof \u03a3, such that draws on subsequent iterations cost\nO(N2). For some problems with special structure draw-\ning samples from the Gaussian prior can be cheaper.\nIn many models the Gaussian prior distribution cap-\ntures dependencies: the observations are independent\nconditioned on f. In these cases, computing L(f) will\ncost O(N) computation. As a result, drawing the \u03bd\nrandom variate will be the dominant cost of the update\nin many high-dimensional problems. In these cases\nthe extra cost of elliptical slice sampling over Neal\u2019s\nMetropolis\u2013Hastings algorithm will be small.\nAs a minor performance improvement, our implementa-\ntion optionally accepts the log-likelihood of the initial\nstate, if known from a previous update, so that it\ndoesn\u2019t need to be recomputed in step 2.\n2.3 Validity\nElliptical slice sampling considers settings of an angle\nvariable, \u03b8. Figure 2 presented the algorithm as it\nwould be used: there is no need to index or remember\nthe visited angles. For the purposes of analysis we\nInput: current state f, a routine that samples from\nN(0,\u03a3), log-likelihood function logL.\nOutput: a new state f?. When f is drawn from p?(f)\u221d\nN(f;0,\u03a3)L(f), the marginal distribution of f?is also p?.\n1. Choose ellipse: \u03bd \u223c N(0,\u03a3)\n2. Log-likelihood threshold:\nu \u223c Uniform[0,1]\nlogy \u2190 logL(f) + logu\n3. Draw an initial proposal, also defining a bracket:\n\u03b8 \u223c Uniform[0,2\u03c0]\n[\u03b8min,\u03b8max] \u2190 [\u03b8\u22122\u03c0, \u03b8]\n4. f?\u2190 f cos\u03b8 + \u03bd sin\u03b8\n5. if logL(f?) > logy then:\n6. Accept: return f?\n7. else:\nShrink the bracket and try a new point:\n8. if \u03b8 < 0 then: \u03b8min\u2190\u03b8 else: \u03b8max\u2190\u03b8\n9.\u03b8 \u223c Uniform[\u03b8min,\u03b8max]\n10.GoTo 4.\nFigure 2: The elliptical slice sampling algorithm.\n(a) (b)\n(c)(d)\n(e)\nFigure 3: (a) The algorithm receives f=\ndraws auxiliary variate \u03bd= , defining an ellipse centred at\nthe origin ( ). Step 2: a likelihood threshold defines the\n\u2018slice\u2019 (). Step 3: an initial proposal\ncase not on the slice. (b) The first proposal defined both\nedges of the [\u03b8min,\u03b8max] bracket; the second proposal ( )\nis also drawn from the whole range. (c) One edge of the\nbracket () is moved to the last rejected point such that\nis still included. Proposals are made with this shrinking\nrule until one lands on the slice. (d) The proposal here ( )\nis on the slice and is returned as f?. (e) Shows the reverse\nconfiguration discussed in Section 2.3:\nwhich with auxiliary \u03bd?= defines the same ellipse. The\nbrackets and first three proposals ( ) are the same. The\nfinal proposal ( ) is accepted, a move back to f.\nas input. Step 1\nis drawn, in this\nis the input f?,"},{"page":4,"text":"Elliptical slice sampling\nwill denote the ordered sequence of angles considered\nduring the algorithm by {\u03b8k} with k=1..K.\nWe first identify the joint distribution over a state\ndrawn from the target distribution (2) and the other\nrandom quantities generated by the algorithm:\np(f,y,\u03bd,{\u03b8k}) = p?(f)p(y|f)p(\u03bd)p({\u03b8k}|f,\u03bd,y)\n=1\nZN(f;0,\u03a3)N(\u03bd;0,\u03a3)p({\u03b8k}|f,\u03bd,y),\nwhere the vertical level y was drawn uniformly\nin [0,L(f)], that is, p(y |f)=1\/L(f). The final term,\np({\u03b8k}|f,\u03bd,y), is a distribution over a random-sized set\nof angles, defined by the stopping rule of the algorithm.\n(7)\nGiven the random variables in (7) the algorithm de-\nterministically computes positions, {fk}, accepting the\nfirst one that satisfies a likelihood constraint. More\ngenerally each angle specifies a rotation of the two\na priori Gaussian variables:\n\u03bdk= \u03bd cos\u03b8k\u2212 f sin\u03b8k\nfk= \u03bd sin\u03b8k+ f cos\u03b8k,k = 1..K.\n(8)\nFor any choice of \u03b8kthis deterministic transformation\nhas unit Jacobian. Any such rotation also leaves the\njoint prior probability invariant,\nN(\u03bdk;0,\u03a3)N(fk;0,\u03a3) = N(\u03bd;0,\u03a3)N(f;0,\u03a3)\nfor all k, which can easily be verified by substituting\nvalues into the Gaussian form (1).\n(9)\nIt is often useful to consider how an MCMC algorithm\ncould make a reverse transition from the final state f?\nback to the initial state f. The final state f?=fKwas\nthe result of a rotation by \u03b8Kin (8). Given an initial\nstate of f?=fK, the algorithm could generate \u03bd?=\u03bdK\nin step 1. Then a rotation of \u2212\u03b8Kwould return back\nto the original (f,\u03bd) pair. Moreover, the same ellipse\nof states is accessible and rotations of \u03b8k\u2212\u03b8K will\nreproduce any intermediate fk<Klocations visited by\nthe initial run of the algorithm.\nIn fact, the algorithm is reversible:\np(f,y,\u03bd,{\u03b8k}) = p(f?,y,\u03bd?,{\u03b8?\nthe equilibrium probability of a forwards draw (7) is\nthe same as the probability of starting at f?, drawing\nthe same y (possible because L(f?)>y), \u03bd?=\u03bdKand\n?\n\u2212\u03b8K\nk}), (10)\nangles, \u03b8?\nk=\n\u03b8k\u2212 \u03b8K\nk < K\nk = K,\n(11)\nresulting in the original state f being returned. The\nreverse configuration corresponding to the result of a\nforwards run in Figure 3(d) is illustrated in Figure 3(e).\nSubstituting (9) into (7) shows that ensuring that the\nforward and reverse angles are equally probable,\np({\u03b8k}|f,\u03bd,y) = p({\u03b8?\nresults in the reversible property (10).\nk}|f?,\u03bd?,y), (12)\nThe algorithm does satisfy (12): The probability of the\nfirst angle is always1\/2\u03c0. If more angles were considered\nbefore an acceptable state was found, these angles were\ndrawn with probabilities 1\/(\u03b8max\u2212 \u03b8min). Whenever\nthe bracket was shrunk in step 8, the side to shrink must\nhave been chosen such that fKremained selectable as it\nwas selected later. The reverse transition uses the same\nintermediate proposals, making the same rejections\nwith the same likelihood threshold, y. Because the\nalgorithm explicitly includes the initial state, which in\nreverse is fKat \u03b8?=0, the reverse transition involves\nthe same set of shrinking decisions as the forwards\ntransitions. As the same brackets are sampled, the\n1\/(\u03b8max\u2212\u03b8min) probabilities for drawing angles are the\nsame for the forwards and reverse transitions.\nThe reversibility of the transition operator (10) implies\nthat the target posterior distribution (2) is a station-\nary distribution of the Markov chain. Drawing f from\nthe stationary distribution and running the algorithm\ndraws a sample from the joint auxiliary distribution (7).\nThe deterministic transformations in (8) and (11) have\nunit Jacobian, so the probability density of obtaining\na joint draw corresponding to (f?,y,\u03bd?,{\u03b8?\nto the probability given by (7) for the original vari-\nables. The reversible property in (10) shows that this\nis the same probability as generating the variables by\nfirst generating f?from the target distribution and gen-\nerating the remaining quantities using the algorithm.\nTherefore, the marginal probability of f?is given by\nthe target posterior (2).\nk}) is equal\nGiven the first angle, the distribution over the first pro-\nposed move is N(f cos\u03b8, \u03a3sin2\u03b8). Therefore, there is a\nnon-zero probability of transitioning to any region that\nhas non-zero probability under the posterior. This is\nenough to ensure that, formally, the chain is irreducible\nand aperiodic (Tierney, 1994). Therefore, the Markov\nchain has a unique stationary distribution and repeated\napplications of elliptical slice sampling to an arbitrary\nstarting point will asymptotically lead to points drawn\nfrom the target posterior distribution (2).\n2.4Slice sampling variants\nThere is some amount of choice in how the slice sampler\non the ellipse could be set up. Other methods for\nproposing angles could have been used, as long as they\nsatisfied the reversible condition in (12). The particular\nalgorithm proposed in Figure 2 is appealing because it\nis simple and has no free parameters."},{"page":5,"text":"Murray, Adams, MacKay\nThe algorithm must choose the initial edges of the\nbracket [\u03b8min,\u03b8max] randomly. It would be aesthet-\nically pleasing to place the edges of the bracket at\nthe opposite side of the ellipse to the current position,\nat \u00b1\u03c0. However this deterministic bracket placement\nwould not be reversible and gives an invalid algorithm.\nThe edge of a randomly-chosen bracket could lie on\nthe \u2018slice\u2019, the acceptable region of states. Our recom-\nmended elliptical slice sampling algorithm, Figure 2,\nwould accept this point. The initially-presented algo-\nrithm, Figure 1, effectively randomly places the end-\npoints of the bracket but without checking this location\nfor acceptability. Apart from this small change, it can\nbe shown that the algorithms are equivalent.\nIn typical problems the slice will not cover the whole\nellipse. For example, if f is a representative sample\nfrom a posterior, often \u2212f will not be. Increasing the\nprobability of proposing points close to the current\nstate may increase efficiency. One way to do this would\nbe to shrink the bracket more aggressively (Skilling\nand MacKay, 2003). Another would be to derive a\nmodel from the auxiliary variable model (6), but with\na non-uniform distribution on \u03b8. Another way would\nbe to randomly position an initial bracket of width less\nthan 2\u03c0 \u2014the code that we provide optionally allows\nthis. However, as explained in section 2.2, for high-\ndimensional problems such tuning will often only give\nsmall improvements. For smaller problems we have\nseen it possible to improve the cpu-time efficiency of\nthe algorithm by around two times.\nAnother possible line of research is methods for biasing\nproposals away from the current state. For example\nthe \u2018over-relaxed\u2019 methods discussed by Neal (2003)\nhave a bias towards the opposite side of the slice from\nthe current position. In our context it may be desirable\nto encourage moves close to \u03b8=\u03c0\/2, as these moves are\nindependent of the previous position. These proposals\nare only likely to be useful when the likelihood terms are\nvery weak, however. In the limit of sampling from the\nprior due to a constant likelihood, the algorithm already\nsamples reasonably efficiently. To see this, consider\nthe distribution over the outcome after N iterations\ninitialized at f0:\nfN= f0\nN\n?\nn=1\ncos\u03b8n+\nN\n?\nm=1\n\u03bdmsin\u03b8m\nN\n?\nn=m+1\ncos\u03b8n,\nwhere \u03bdnand \u03b8nare values drawn at iteration n. Only\none angle is drawn per iteration when sampling from\nthe prior, because the first proposal is always accepted.\nThe only dependence on the initial state is the first\nterm, the coefficient of which shrinks towards zero\nexponentially quickly.\n2.5Limitations\nA common modeling situation is that an unknown\nconstant offset, c \u223c N(0,\u03c32\nentire latent vector f. The resulting variable, g=f+c, is\nstill Gaussian distributed, with the constant \u03c32\nto every element of the covariance matrix. Neal (1999)\nidentified that this sort of covariance will not tend\nto produce useful auxiliary draws \u03bd. An iteration of\nthe Markov chain can only add a nearly-constant shift\nto the current state. Indeed, covariances with large\nconstant terms are generally problematic as they tend\nto be poorly conditioned. Instead, large offsets should\nbe modeled and sampled as separate variables.\nm), has been added to the\nmadded\nNo algorithm can sample effectively from arbitrary dis-\ntributions. As any distribution can be factored as in\n(2), there exist likelihoods L(f) for which elliptical slice\nsampling is not effective. Many Gaussian process appli-\ncations have strong prior smoothness constraints and\nrelatively weak likelihood constraints. This important\nregime is where we focus our empirical comparison.\n3 Related work\nElliptical slice sampling builds on a Metropolis\u2013\nHastings (M\u2013H) update proposed by Neal (1999). Neal\nreported that the original update performed moderately\nbetter than using a more obvious M\u2013H proposal,\nf?= f + ?\u03bd,\u03bd \u223c N(0,\u03a3), (13)\nand much better than Gibbs sampling for Gaussian\nprocess classification. Neal also proposed using Hy-\nbrid\/Hamiltonian Monte Carlo (Duane et al., 1987;\nNeal, 1993), which can be very effective, but requires\ntuning and the implementation of gradients. We now\nconsider some other alternatives that have similar re-\nquirements to elliptical slice sampling.\n3.1\u2018Conventional\u2019 slice sampling\nElliptical slice sampling builds on the family of methods\nintroduced by Neal (2003). Several of the existing\nslice sampling methods would also be easy to apply:\nthey only require point-wise evaluation of the posterior\nup to a constant. These methods do have step-size\nparameters, but unlike simple Metropolis methods,\ntypically the performance of slice samplers does not\ncrucially rely on carefully setting free parameters.\nThe most popular generic slice samplers use simple\nunivariate updates, although applying these directly\nto f would suffer the same slow convergence problems\nas Gibbs sampling. While Agarwal and Gelfand (2005)\nhave applied slice sampling for sampling parameters\nin Gaussian spatial process models, they assumed a"},{"page":6,"text":"Elliptical slice sampling\nlinear-Gaussian observation model. For non-Gaussian\ndata it was suggested that \u201cthere seems to be little role\nfor slice sampling.\u201d\nElliptical slice sampling changes all of the variables in\nf at once, although there are potentially better ways of\nachieving this. An extensive search space of possibilities\nincludes the suggestions for multivariate updates made\nby Neal (2003).\nOne simple possible slice sampling update performs a\nunivariate update along a random line traced out by\nvarying ? in (13). As the M\u2013H method based on the\nline worked less well than that based on an ellipse, one\nmight also expect a line-based slice sampler to perform\nless well. Intuitively, in high dimensions much of the\nmass of a Gaussian distribution is in a thin ellipsoidal\nshell. A straight line will more rapidly escape this shell\nthan an ellipse passing through two points within it.\n3.2 Control variables\nTitsias et al. (2009) introduced a sampling method\ninspired by sparse Gaussian process approximations.\nM control variables fc are introduced such that the\njoint prior p(f,fc) is Gaussian, and that f still has\nmarginal prior N(0,\u03a3). For Gaussian process models a\nparametric family of joint covariances was defined, and\nthe model is optimized so that the control variables\nare informative about the original variables: p(f | fc)\nis made to be highly peaked. The optimization is a\npre-processing step that occurs before sampling begins.\nThe idea is that the small number of control variables fc\nwill be less strongly coupled than the original variables,\nand so can be moved individually more easily than the\ncomponents of f. A proposal involves resampling one\ncontrol variable from the conditional prior and then\nresampling f from p(f |fc). This move is accepted or\nrejected with the Metropolis\u2013Hastings rule.\nAlthough the method is inspired by an approximation\nused for large datasets, the accept\/reject step uses the\nfull model. After O(N3) pre-processing it costs O(N2)\nto evaluate a proposed change to the N-dimensional\nvector f. One \u2018iteration\u2019 in the paper consisted of an\nupdate for each control variable and so costs O(MN2)\n\u2014 roughly M elliptical slice sampling updates. The\ncontrol method uses fewer likelihood evaluations per\niteration, although has some different minor costs asso-\nciated with book-keeping of the control variables.\n3.3Local updates\nIn some applications it may make sense to update only\na subset of the latent variables at a time. This might\nhelp for computational reasons given the O(N2) scaling\nfor drawing samples of subsets of size N. Titsias et al.\n(2009) also identified suitable subsets for local updates\nand then investigated sampling proposals from the\nconditional Gaussian prior.\nIn fact, local updates can be combined with any tran-\nsition operator for models with Gaussian priors. If\nfAis a subset of variables to update and fB are the\nremaining variables, we can write the prior as:\n?fA\nand the conditional prior is:\nfB\n?\n\u223c N\n?\n0,\n?\u03a3A,A\n\u03a3A,B\n\u03a3B,B\n\u03a3B,A\n??\n(14)\np(fA|fB) = N(fA; m,S), where\nm = \u03a3A,B\u03a3\u22121\nB,BfB, and S = \u03a3A,A\u2212\u03a3A,B\u03a3\u22121\nB,B\u03a3B,A.\nA change of variables g=fA\u2212m allows us to express the\nconditional posterior as: p?(g) \u221d N(g; 0, S)L??g+m\nternative, to update g (and thus fA). Updating groups\nof variables according to their conditional distributions\nis a standard way of sampling from a joint distribution.\nfB\n??.\nWe can then apply elliptical slice sampling, or any al-\n4 Experiments\nWe performed an empirical comparison on three Gaus-\nsian process based probabilistic modeling tasks. Only\na brief description of the models and methods can\nbe given here. Full code to reproduce the results is\nprovided as supplementary material.\n4.1Models\nEach of the models associates a dimension of the latent\nvariable, fn, with an \u2018input\u2019 or \u2018feature\u2019 vector xn. The\nmodels in our experiments construct the covariance\nfrom the inputs using the most common method,\n?D\nthe squared-exponential or \u201cGaussian\u201d covariance.\nThis covariance has \u201clengthscale\u201d parameter ? and\nan overall \u201csignal variance\u201d \u03c32\nbe more appropriate in many modeling situations, but\nour algorithm would apply unchanged.\n\u03a3ij = \u03c32\nfexp?\u22121\n2\nd=1(xd,i\u2212 xd,j)2\/?2?, (15)\nf. Other covariances may\nGaussian regression: given observations y of the\nlatent variables with Gaussian noise of variance \u03c32\nLr(f) =?\nthe posterior is Gaussian and so fully tractable. We\nuse this as a simple test that the method is working\ncorrectly. Differences in performance on this task will\nalso give some indication of performance with a simple\nlog-concave likelihood function.\nn,\nnN(yn;fn,\u03c32\nn), (16)"},{"page":7,"text":"Murray, Adams, MacKay\nWe generated ten synthetic datasets with input fea-\nture dimensions from one to ten. Each dataset was\nof size N =200, with inputs {xn}N\nfrom a D-dimensional unit hypercube and function\nvalues drawn from a Gaussian prior, f \u223cN(0,\u03a3), using\ncovariance (15) with lengthscale ?=1 and unit signal\nvariance, \u03c32\nadded to generate the observations.\nn=1drawn uniformly\nf= 1. Noise with variance \u03c32\nn= 0.32was\nGaussian process classification: a well-explored\napplication of Gaussian processes with a non-Gaussian\nnoise model is binary classification:\nLc(f) =?\nwhere yn\u2208 {\u22121,+1} are the label data and \u03c3(a) is a\nsigmoidal function: 1\/(1+e\u2212a) for the logistic classifier;\na cumulative Gaussian for the probit classifier.\nn\u03c3 (ynfn), (17)\nWe ran tests on the USPS classification problem\nas set up by Kuss and Rasmussen (2005).\nused log\u03c3f=3.5, log?=2.5 and the logistic likelihood.\nWe\nLog Gaussian Cox process: an inhomogeneous Pois-\nson process with a non-parametric rate can be con-\nstructed by using a shifted draw from a Gaussian pro-\ncess as the log-intensity function. Approximate infer-\nence can be performed by discretizing the space into\nbins and assuming that the log-intensity is uniform in\neach bin (M\u00f8ller et al., 1998). Each bin contributes a\nPoisson likelihood:\nLp(f) =\n?\nn\n\u03bbnynexp(\u2212\u03bbn)\nyn!\n,\u03bbn=efn+m, (18)\nwhere the model explains the yn counts in bin n as\ndrawn from a Poisson distribution with mean \u03bbn. The\noffset to the log mean, m, is the mean log-intensity of\nthe Poisson process plus the log of the bin size.\nWe perform inference for a Cox process model of the\ndates of mining disasters taken from a standard data\nset for testing point processes (Jarrett, 1979). The\n191 events were placed into 811 bins of 50 days each.\nThe Gaussian process parameters were fixed to \u03c32\nand ?=13516 days (a third of the range of the dataset).\nThe offset m in (18) was set to m=log(191\/811), to\nmatch the empirical mean rate.\nf=1\n4.2Results\nA trace of the samples\u2019 log-likelihoods, Figure 4, shows\nthat elliptical slice sampling and control variables sam-\npling have different behavior. The methods make differ-\nent types of moves and only control variables sampling\ncontains rejections. Using long runs of either method\nto estimate expectations under the target distribution\nis valid. However, sticking in a state due to many\nrejections can give a poor estimator as can always mak-\n0 5001000\n\u221245\n\u221240\n\u221235\n# Control variable moves\nlog L\nControl Variables\n0 5001000\n\u221245\n\u221240\n\u221235\n# Iterations\nElliptical slice sampling\nFigure 4: Traces of log-likelihoods for the 1-dimensional GP\nregression experiment. Both lines are made with 333 points\nplotted after each sweep through M =3 control variables\nand after every 3 iterations of elliptical slice sampling.\n0 5000 10000\n\u2212140\n\u2212120\n\u2212100\n\u221280\n\u221260\n\u221240\n# Control variable moves\nlog L\nControl Variables\n0 500010000\n\u2212140\n\u2212120\n\u2212100\n\u221280\n\u221260\n\u221240\n# Iterations\nElliptical slice sampling\nFigure 5: As in Figure 4 but for 10-dimensional regression\nand plotting every M =78 iterations. (Control variables\ndidn\u2019t move on this run.)\ning small moves. It can be difficult to judge overall\nsampling quality from trace plots alone.\nAs a quantitative measure of quality we estimated the\n\u201ceffective number of samples\u201d from log-likelihood traces\nusing R-CODA (Cowles et al., 2006). Figure 6 shows\nthese results along with computer time taken. The step\nsize for Neal\u2019s Metropolis method was chosen using a\ngrid search to maximize performance. Times are for the\nprovided implementations under Matlab v7.8 on a sin-\ngle 64bit, 3GHz Intel Xeon CPU. Comparing runtimes\nis always problematic, due to implementation-specific\ndetails. Our numbers of effective samples are primarily\nplotted for the same number of O(N2) updates with\nthe understanding that some correction based loosely\non runtime should be applied.\nThe control variables approach was particularly recom-\nmended for Gaussian processes with low-dimensional\ninput spaces.On our particular low-dimensional\nsynthetic regression problems using control variables\nclearly outperforms all the other methods. On the\nmodel of mining disasters, control variable sampling\nhas comparable performance to elliptical slice sampling\nwith about 50% less run time. On higher-dimensional\nproblems more control variables are required; then\nother methods cost less. Control variables failed to sam-\nple in high-dimensions (Figure 5). On the USPS classifi-\ncation problem control variables ran exceedingly slowly\nand we were unable to obtain any meaningful results.\nElliptical slice sampling obtained more effective samples\nthan Neal\u2019s M\u2013H method with the best possible step\nsize, although at the cost of increased run time. On the\nproblems involving real data, elliptical slice sampling"},{"page":8,"text":"Elliptical slice sampling\nFigure 6: Number of effective samples from 105iterations after 104burn in, with time and likelihood evaluations required.\nThe means and standard deviations for 100 runs are shown (divide the \u201cerror bars\u201d by 10 to get standard errors on the\nmean, which are small). Each iteration involves one O(N2) operation (e.g. one \u03bd draw or updating one control variable).\nEach group of bars in the top two rows has been rescaled for readability: the numbers beneath each group show the\nnumber of effective samples or CPU time in seconds for elliptical slice sampling, which always has bars of height 1.\nwas better overall whereas M\u2013H has more effective\nsamples per unit time (in our implementation) on the\nsynthetic problems. The performance differences aren\u2019t\nhuge; either method would work well enough.\nElliptical slice sampling takes less time than slice sam-\npling along a straight line (line sampling involves addi-\ntional prior evaluations) and usually performs better.\n5 Discussion\nThe slice samplers use many more likelihood evaluations\nthan the other methods.\nour code can take a step-size parameter to reduce the\nnumber of likelihood evaluations (Section 2.4). On\nthese problems the time for likelihood computations\nisn\u2019t completely negligible: speedups of around \u00d72 may\nbe possible by tuning elliptical slice sampling. Our\ndefault position is that ease-of-use and human time is\nimportant and that the advantage of having no free\nparameters should often be taken in exchange for a\nfactor of two in runtime.\nThis is partly by choice:\nWe fixed the parameters of \u03a3 and L in our experi-\nments to simplify the comparison. Fixing the model\npotentially favors the methods that have adjustable\nparameters. In problems were \u03a3 and L change dra-\nmatically, a single step-size or optimized set of control\nvariables could work very poorly.\nElliptical slice sampling is a simple generic algorithm\nwith no tweak parameters. It performs similarly to the\nbest possible performance of a related M\u2013H scheme,\nand could be applied to a wide variety of applications\nin both low and high dimensions.\nAcknowledgements\nThanks to Michalis Titsias for code, Sinead Williamson\nand Katherine Heller for a helpful discussion, and to\nRadford Neal, Sam Roweis, Christophe Andrieu and\nthe reviewers for useful suggestions. RPA is funded by\nthe Canadian Institute for Advanced Research.\nReferences\nD. K. Agarwal and A. E. Gelfand.\nsimulation based fitting of spatial data models. Statistics\nand Computing, 15(1):61\u201369, 2005.\nM. K. Cowles, N. Best, K. Vines, and M. Plummer. R-\nCODA 0.10-5, 2006. http:\/\/www-fis.iarc.fr\/coda\/.\nS. Duane, A. D. Kennedy, B. J. Pendleton, and D. Roweth.\nHybrid Monte Carlo. Physics Letters B, 195(2):216\u2013222,\nSeptember 1987.\nR. G. Jarrett. A note on the intervals between coal-mining\ndisasters. Biometrika, 66(1):191\u2013193, 1979.\nM. Kuss and C. E. Rasmussen. Assessing approximate infer-\nence for binary Gaussian process classification. Journal\nof Machine Learning Research, 6:1679\u20131704, 2005.\nJ. M\u00f8ller, A. R. Syversveen, and R. P. Waagepetersen.\nLog Gaussian Cox processes. Scandinavian Journal of\nStatistics, 25(3):451\u2013482, 1998.\nR. M. Neal. Probabilistic inference using Markov chain\nMonte Carlo methods. Technical Report CRG-TR-93-1,\nDept. of Computer Science, University of Toronto, 1993.\nR. M. Neal. Regression and classification using Gaussian\nprocess priors. In J. M. Bernardo et al., editors, Bayesian\nStatistics 6, pages 475\u2013501. OU Press, 1999.\nR. M. Neal. Slice sampling. Annals of Statistics, 31(3):\n705\u2013767, 2003.\nC. E. Rasmussen and C. K. I. Williams. Gaussian Processes\nfor machine learning. MIT Press, 2006.\nH. Rue, S. Martino, and N. Chopin. Approximate Bayesian\ninference for latent Gaussian models by using integrated\nnested Laplace approximations. Journal of the Royal\nStatistical Society: Series B, 71(2):319\u2013392, 2009.\nJ. Skilling and D. J. C. MacKay. Slice sampling \u2014 a binary\nimplementation. Annals of Statistics, 31(3), 2003.\nL. Tierney. Markov chains for exploring posterior distribu-\ntions. The Annals of Statistics, 22(4):1701\u20131728, 1994.\nM. Titsias, N. D. Lawrence, and M. Rattray. Efficient\nsampling for Gaussian process inference using control\nvariables. In Advances in Neural Information Processing\nSystems 21, pages 1681\u20131688, 2009.\nSlice sampling for"}],"widgetId":"rgw26_56aba00261acf"},"id":"rgw26_56aba00261acf","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=45893043&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw27_56aba00261acf"},"id":"rgw27_56aba00261acf","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=45893043&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":45893043,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/45893043_Elliptical_Slice_Sampling","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56aba00261acf"},"id":"rgw2_56aba00261acf","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":45893043},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=45893043&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56aba00261acf"},"id":"rgw1_56aba00261acf","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"8B5KhQOSpQf5KWAkRqZTyH4LXxK7qT8X7xTS5pY2MWuwasofyhOhJe\/mM5KShxs\/7hxEK1I5m+PREBNCHZnUCozWCejCbqQRQgSdSECp2M27wP6UQo2noQmOYjjTp9uVTvY8tlbjMl31ILjkp8NMFq2Gf8uK4BJE54IymvTPFWWQLP\/JTVtb6XBxEOfumku1DUWAdWbaEIqoqV7zwYCSRRF\/oPpZzBcw2MMVVF8LMG4Cp7KYjSrCRI7P2oGf+YTMStqf\/0Z+Df9IMLI1mnx2RpvbMHP\/9f+22JlT6KsIrs8=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/45893043_Elliptical_Slice_Sampling\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Elliptical Slice Sampling.\" \/>\n<meta property=\"og:description\" content=\"Many probabilistic models introduce strong dependencies between variables using a latent multivariate Gaussian distribution or a Gaussian process. We present a new Markov chain Monte Carlo...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/45893043_Elliptical_Slice_Sampling\/links\/0f64778138294e886aa1aa25\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/45893043_Elliptical_Slice_Sampling\" \/>\n<meta property=\"rg:id\" content=\"PB:45893043\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Elliptical Slice Sampling.\" \/>\n<meta name=\"citation_author\" content=\"Iain Murray\" \/>\n<meta name=\"citation_author\" content=\"Ryan Prescott Adams\" \/>\n<meta name=\"citation_author\" content=\"David J. C. MacKay\" \/>\n<meta name=\"citation_publication_date\" content=\"2009\/12\/31\" \/>\n<meta name=\"citation_volume\" content=\"1001\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/45893043_Elliptical_Slice_Sampling\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/45893043_Elliptical_Slice_Sampling\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-d6176f05-3a1a-4e5c-9872-e6e5ade54a1b","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":604,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw28_56aba00261acf"},"id":"rgw28_56aba00261acf","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-d6176f05-3a1a-4e5c-9872-e6e5ade54a1b", "95cbc25b41b409d89e42a141098f61d00c662ae5");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-d6176f05-3a1a-4e5c-9872-e6e5ade54a1b", "95cbc25b41b409d89e42a141098f61d00c662ae5");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw29_56aba00261acf"},"id":"rgw29_56aba00261acf","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/45893043_Elliptical_Slice_Sampling","requestToken":"9dW0n1jDgA4+fXqXdn9uFVsobghe7vv920BUmyljh1Vqt0pVwrNbLlHGTIFN+Y+qPsgrXrFp+CBTGdrFcGfDprKzwkTt3nZhry+clDeqjyXKk\/ZvGvzwIQ9Q52ZZmhZJLnE76vv4NKuTaRLpuvglEcCyXMb6O\/OWhog9pXZzN7Sdvv6ynSvmh95kB8fxsmj34+CNj\/IwWJNAuD\/NfB0wAaeih7GDv91k0PjbAtQt6ErXoesdzAudNm\/NKFr7ds8f3L9h04Dk1uDWiiJVdhrcVT3ekVhoeeXjVKDoRcnXCC0=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=KsAUFGTqpVyMtV-O2R85PXTzgcY3e8qNhyQPGGjsx-MzJ0_NZSQvkG4vBdO6gHNz","encodedUrlAfterLogin":"cHVibGljYXRpb24vNDU4OTMwNDNfRWxsaXB0aWNhbF9TbGljZV9TYW1wbGluZw%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw31_56aba00261acf"},"id":"rgw31_56aba00261acf","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw30_56aba00261acf"},"id":"rgw30_56aba00261acf","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw32_56aba00261acf"},"id":"rgw32_56aba00261acf","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
