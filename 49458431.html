<!DOCTYPE html> <html lang="en" class="" id="rgw30_56aba15bec488"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="NtazlyJSqaHwzYcAPMGIBlLqewcS3KTvpzsCGVZWyJ1UH4hyqLyYrwaE1pGzOzTLOMs6vr0m5qOREwqU4NmnpGJOMajOpGq01YuxHD0JtnlMX60xEZ+zWUNPwHcr7iHy8tyz6xg1EAFxRKhXlWiIEGuAUYzP8A4D9UPOkwLz652+ACCjbLcX+AqE1VchGBKe2u83udCOPWxZazhjeR5udQgbpWYHnx5SKR/2J3b6qJgWacxvryB+tnXL1Z8fxF189OHpshtGBt7YjYHDc+k3gjeMrK3UatntiytsWhZeuVA="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-46c68acc-9e67-421f-877d-3c90e127a9be",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/49458431_A_tutorial_on_adaptive_MCMC" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="A tutorial on adaptive MCMC" />
<meta property="og:description" content="We review adaptive Markov chain Monte Carlo algorithms (MCMC) as a mean to optimise their performance. Using simple toy examples we review their theoretical underpinnings, and in particular show..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/49458431_A_tutorial_on_adaptive_MCMC/links/0ffc9e5f0cf255165fc9f05f/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/49458431_A_tutorial_on_adaptive_MCMC" />
<meta property="rg:id" content="PB:49458431" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1007/s11222-008-9110-y" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="A tutorial on adaptive MCMC" />
<meta name="citation_author" content="Christophe Andrieu" />
<meta name="citation_author" content="Johannes Thoms" />
<meta name="citation_publication_date" content="2008/12/01" />
<meta name="citation_journal_title" content="Statistics and Computing" />
<meta name="citation_issn" content="0960-3174" />
<meta name="citation_volume" content="18" />
<meta name="citation_issue" content="4" />
<meta name="citation_doi" content="10.1007/s11222-008-9110-y" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/49458431_A_tutorial_on_adaptive_MCMC" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/49458431_A_tutorial_on_adaptive_MCMC" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>A tutorial on adaptive MCMC</title>
<meta name="description" content="A tutorial on adaptive MCMC on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56aba15bec488" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56aba15bec488" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw8_56aba15bec488">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1007%2Fs11222-008-9110-y&rft.atitle=A%20tutorial%20on%20adaptive%20MCMC&rft.title=Statistics%20and%20Computing&rft.jtitle=Statistics%20and%20Computing&rft.volume=18&rft.issue=4&rft.date=2008&rft.issn=0960-3174&rft.au=Christophe%20Andrieu%2CJohannes%20Thoms&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">A tutorial on adaptive MCMC</h1> <meta itemprop="headline" content="A tutorial on adaptive MCMC">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/49458431_A_tutorial_on_adaptive_MCMC/links/0ffc9e5f0cf255165fc9f05f/smallpreview.png">  <div id="rgw11_56aba15bec488" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw12_56aba15bec488"> <a href="researcher/3226552_Christophe_Andrieu" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Christophe Andrieu" alt="Christophe Andrieu" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Christophe Andrieu</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw13_56aba15bec488">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/3226552_Christophe_Andrieu"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Christophe Andrieu" alt="Christophe Andrieu" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/3226552_Christophe_Andrieu" class="display-name">Christophe Andrieu</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw14_56aba15bec488"> <a href="researcher/55344272_Johannes_Thoms" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Johannes Thoms" alt="Johannes Thoms" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Johannes Thoms</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw15_56aba15bec488">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/55344272_Johannes_Thoms"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Johannes Thoms" alt="Johannes Thoms" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/55344272_Johannes_Thoms" class="display-name">Johannes Thoms</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/0960-3174_Statistics_and_Computing"><span itemprop="name">Statistics and Computing</span></a> </span>    (Impact Factor: 1.62).     <meta itemprop="datePublished" content="2008-12">  12/2008;  18(4).    DOI:&nbsp;10.1007/s11222-008-9110-y           <div class="pub-source"> Source: <a href="http://infoscience.epfl.ch/record/160389" rel="nofollow">OAI</a> </div>  </div> <div id="rgw16_56aba15bec488" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>We review adaptive Markov chain Monte Carlo algorithms (MCMC) as a mean to optimise their performance. Using simple toy examples we review their theoretical underpinnings, and in particular show why adaptive MCMC algorithms might fail when some fundamental properties are not satisfied. This leads to guidelines concerning the design of correct algorithms. We then review criteria and the useful framework of stochastic approximation, which allows one to systematically optimise generally used criteria, but also analyse the properties of adaptive MCMC algorithms. We then propose a series of novel adaptive algorithms which prove to be robust and reliable in practice. These algorithms are applied to artificial and high dimensional scenarios, but also to the classic mine disaster dataset inference problem.</div> </p>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw29_56aba15bec488">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw28_56aba15bec488"  itemprop="articleBody">  <p>Page 1</p> <p>Stat Comput (2008) 18: 343–373<br />DOI 10.1007/s11222-008-9110-y<br />A tutorial on adaptive MCMC<br />Christophe Andrieu ·Johannes Thoms<br />Received: 23 January 2008 / Accepted: 19 November 2008 / Published online: 3 December 2008<br />© Springer Science+Business Media, LLC 2008<br />Abstract We review adaptive Markov chain Monte Carlo<br />algorithms (MCMC) as a mean to optimise their perfor-<br />mance. Using simple toy examples we review their theo-<br />retical underpinnings, and in particular show why adaptive<br />MCMC algorithmsmightfail when some fundamentalprop-<br />erties are not satisfied. This leads to guidelines concern-<br />ing the design of correct algorithms. We then review cri-<br />teria and the useful framework of stochastic approximation,<br />which allows one to systematically optimise generally used<br />criteria, but also analyse the properties of adaptive MCMC<br />algorithms. We then propose a series of novel adaptive al-<br />gorithms which prove to be robust and reliable in practice.<br />These algorithms are applied to artificial and high dimen-<br />sional scenarios, but also to the classic mine disaster dataset<br />inference problem.<br />Keywords MCMC · Adaptive MCMC · Controlled<br />Markov chain · Stochastic approximation<br />1 Introduction<br />Markov chain Monte Carlo (MCMC) is a general strategy<br />for generating samples {Xi, i = 0,1,...} from complex<br />high-dimensional distributions, say π defined on a space<br />C. Andrieu (?)<br />School of Mathematics, University of Bristol,<br />Bristol BS8 1TW, UK<br />e-mail: c.andrieu@bristol.ac.uk<br />url: http://www.stats.bris.ac.uk/~maxca<br />J. Thoms<br />Chairs of Statistics, École Polytechnique Fédérale de Lausanne,<br />1015 Lausanne, Switzerland<br />X ⊂ Rnx(assumed for simplicity to have a density with re-<br />spect to the Lebesgue measure, also denoted π), from which<br />integrals of the type<br />?<br />for some π-integrable functions X → Rnfcan be approxi-<br />mated using the estimator<br />I (f) :=<br />X<br />f (x)π (x)dx,<br />ˆIN(f) :=1<br />N<br />N<br />?<br />i=1<br />f (Xi),<br />(1)<br />provided that the Markov chain generated with, say, transi-<br />tion P is ergodic i.e. it is guaranteed to eventually produce<br />samples {Xi} distributed according to π. Throughout this<br />review we will refer, in broad terms, to the consistency of<br />such estimates and the convergence of the distribution of Xi<br />to π as π-ergodicity. The main building block of this class<br />of algorithms is the Metropolis-Hastings (MH) algorithm. It<br />requires the definition of a family of proposal distributions<br />{q(x,·),x ∈ X} whose role is to generate possible transitions<br />for the Markov chain, say from X to Y, which are then ac-<br />cepted or rejected according to the probability<br />?<br />The simplicity and universality of this algorithm are both<br />its strength and weakness. Indeed, the choice of the pro-<br />posal distribution is crucial: the statistical properties of the<br />Markov chain heavily depend upon this choice, an inade-<br />quate choice resulting in possibly poor performance of the<br />Monte Carlo estimators. For example, in the toy case where<br />nx= 1 and the normal symmetric random walk Metropo-<br />lis algorithm (N-SRWM) is used to produce transitions, the<br />α(X,Y) = min1,π (Y)q (Y,X)<br />π (X)q (X,Y)<br />?<br />.</p>  <p>Page 2</p> <p>344 Stat Comput (2008) 18: 343–373<br />density of the proposal distribution is of the form<br />?−1<br />where θ2is the variance of the proposed increments, hence<br />defining a Markov transition probability Pθ. The variance<br />of the corresponding estimatorˆIθ<br />as small as possible for the purpose of efficiency, is well<br />known to be typically unsatisfactory for values of θ2that<br />are either “too small or too large” in comparison to optimal<br />or suboptimal value(s). In more realistic scenarios, MCMC<br />algorithms are in general combinations of several MH up-<br />dates {Pk,θ, k = 1,...,n, θ ∈ ?} for some set ?, with each<br />having its own parametrised proposal distribution qk,θ for<br />k = 1,...,n and sharing π as common invariant distribu-<br />tion. These transition probabilities are usually designed in<br />order to capture various features of the target distribution π<br />and in general chosen to complement one another. Such a<br />combination can for example take the form of a mixture of<br />different strategies, i.e.<br />qθ(x,y) =<br />1<br />√2πθ2exp<br />2θ2(y −x)2<br />?<br />,<br />N(f), which we wish to be<br />Pθ(x,dy) =<br />n<br />?<br />k=1<br />wk(θ)Pk,θ(x,dy),<br />(2)<br />where for any θ ∈ ?,?n<br />ucts of transition matrices in the discrete case) such as<br />k=1wk(θ) = 1, wk(θ) ≥ 0, but can<br />also, for example, take the form of combinations (i.e. prod-<br />Pθ(x,dy) = P1,θP2,θ···Pn,θ(x,dy).<br />Both examples are particular cases of the class of Markov<br />transition probabilities Pθ on which we shall focus in<br />this paper: they are characterised by the fact that they<br />(a) belong to a family of parametrised transition proba-<br />bilities {Pθ,θ ∈ ?} (for some problem dependent set ?,<br />? = (0,+∞) in the toy example above) (b) for all θ ∈ ? π<br />is an invariant distribution for Pθ, which is assumed to be<br />ergodic (c) the performance of Pθ, for example the variance<br />ofˆIθ<br />Our aim in this paper is to review the theoretical under-<br />pinnings and recent methodological advances in the area<br />of computer algorithms that aim to “optimise” such para-<br />metrised MCMC transition probabilities in order to lead<br />to computationally efficient and reliable procedures. As we<br />shall see wealso suggest new algorithms.One shouldnoteat<br />this point that in some situations of interest, such as temper-<br />ing type algorithms (Geyer and Thompson 1995), property<br />(b) above might be violated and instead the invariant distrib-<br />ution of Pθmight depend on θ ∈ ? (although only a non θ-<br />dependent feature of this distribution πθmight be of interest<br />to us for practical purposes). We will not consider this case<br />in depth here, but simply note that most of the arguments<br />and ideas presented hereafter generally carry on to this<br />N(f) above, is sensitive to the choice of θ.<br />slightly more complex scenario e.g. (Benveniste et al. 1990;<br />Atchadé and Rosenthal 2005).<br />The choice of a criterion to optimise is clearly the first<br />decision that needs to be made in practice. We discuss this<br />issue in Sect. 4.1 where we point out that most sensible opti-<br />mality or suboptimality criteria can be expressed in terms of<br />expectations with respect to the steady state-distributions of<br />Markov chains generated by Pθfor θ ∈ ? fixed, and make<br />new suggestions in Sect. 5 which are subsequently illus-<br />trated on examples in Sect. 6. We will denote by θ∗a generic<br />optimal value for our criteria, which is always assumed to<br />exist hereafter.<br />In order to optimise such criteria, or even simply find<br />suboptimal values for θ, one could suggest to sequentially<br />run a standard MCMC algorithm with transition Pθ for a<br />set of values of θ (either predefined or defined sequentially)<br />and compute the criterion of interest (or its derivative etc.)<br />once we have evidence that equilibrium has been reached.<br />This can naturally be wasteful and we will rather focus here<br />on a technique which belongs to the well known class of<br />processes called controlled Markov chains (Borkar 1990)<br />in the engineering literature, which we will refer to as con-<br />trolled MCMC (Andrieu and Robert 2001), due to their nat-<br />ural filiation. More precisely we will assume that the algo-<br />rithm proceeds as follows. Given a family of transition prob-<br />abilities {Pθ,θ ∈ ?} defined on X such that for any θ ∈ ?,<br />πPθ= π (meaning that if Xi∼ π, then Xi+1∼ π,Xi+2∼<br />π,...) and given a family of (possibly random) mappings<br />{θi: ? × Xi+1→ ?,i = 1,...}, which encodes what is<br />meant by optimality by the user, the most general form of<br />a controlled MCMC proceeds as follows:<br />Algorithm 1 Controlled Markov chain Monte Carlo<br />• Sample initial values θ0,X0∈ ?× X.<br />• Iteration i + 1 (i ≥ 0), given θi= θi(θ0,X0,...,Xi) from<br />iteration i<br />1. Sample Xi+1|(θ0,X0,...,Xi) ∼ Pθi(Xi,·).<br />2. Compute θi+1= θi+1(θ0,X0,...,Xi+1).<br />In Sect. 4.2 we will focus our results to particular map-<br />pings well suited to our purpose of computationally efficient<br />sequential updating of {θi} for MCMC algorithms, which<br />rely on the Robbins-Monro update and more generally on<br />the stochastic approximation framework (Benveniste et al.<br />1990). However, before embarking on the description of<br />practical procedures to optimise MCMC transition probabil-<br />ities we will first investigate, using mostly elementary un-<br />dergraduate level tools, some of the theoretical ergodicity<br />properties of controlled MCMC algorithms.<br />Indeed, as we shall see, despite the assumption that for<br />any θ ∈ ?, πPθ= π, adaptation in the context of MCMC</p>  <p>Page 3</p> <p>Stat Comput (2008) 18: 343–373345<br />using the controlled approach leads to complications. In<br />fact, this type of adaptation can easily perturb the ergodicity<br />properties of MCMC algorithms. In particular algorithms of<br />this type will in most cases lead to the loss of π as an invari-<br />ant distribution of the process {Xi}, which intuitively should<br />be the minimum requirement to produce samples from π<br />and lead to consistent estimators. Note also that when not<br />carefully designed such controlled MCMC can lead to tran-<br />sient processes or processes such thatˆIN(f) is not consis-<br />tent. Studying the convergence properties of such processes<br />naturally raises the question of the relevance of such devel-<br />opments in the present context. Indeed it is often argued that<br />one might simply stop adaptation once we have enough evi-<br />dence that {θi} has reached a satisfactory optimal or subop-<br />timal value of θ and then simply use samples produced by a<br />standard MCMC algorithm using such a fixed good value˜θ.<br />No new theory should then be required. While apparently<br />valid, this remark ignores the fact that most criteria of in-<br />terest depend explicitly on features of π, which can only<br />be evaluated with... MCMC algorithms. For example, as<br />mentioned above most known and useful criteria can be for-<br />mulated as expectations with respect to distributions which<br />usually explicitly involve π.<br />Optimising such criteria, or finding suboptimal values<br />of θ∗, thus requires one to be able to sample—perhaps ap-<br />proximately or asymptotically—from π, which in the con-<br />text of controlled MCMC requires one to ensure that the<br />process described above can, in principle, achieve this aim.<br />This, in our opinion, motivates and justifies the need for<br />such theoretical developments as they establish whether or<br />not controlled MCMC can, again in principle, optimise such<br />π-dependent criteria. Note that convergence of {θi} should<br />itself not be overlooked since, in light of our earlier discus-<br />sion of the univariate N-SRWM, optimisation of {Pθ} is our<br />primary goal and should be part of our theoretical develop-<br />ments. Note that users wary of the perturbation to ergodic-<br />ity brought by adaptation might naturally choose to “freeze”<br />{θi} to a value θτ beyond an iteration τ and consider only<br />samples produced by the induced Markov chain for their in-<br />ference problem. A stopping rule is described in Sect. 4.2.2.<br />In fact, as we shall see it is possible to run the two proce-<br />dures simultaneously.<br />Finally, whereas optimising an MCMC algorithm seems<br />a legitimate thing to do, one might wonder if it is compu-<br />tationally worth adapting. This is a very difficult question<br />for which there is probably no straight answer. The view we<br />adopt here is that such optimisation schemes are very useful<br />tools to design or help the design of efficient MCMC algo-<br />rithmswhich,whileleadingtosomeadditionalcomputation,<br />have the potential to spare the MCMC user significant im-<br />plementation time.<br />The paper is organised as follows. In Sect. 2 we provide<br />toy examples that illustrate the difficulties introduced by the<br />adaptation of MCMC algorithms. In Sect. 3 we discuss why<br />one might expect vanishing adaptation to lead to processes<br />such that {Xi} can be used in order to estimate expectation<br />with respect to π. This section might be skipped on a first<br />reading. In Sect. 4 we first discuss various natural criteria<br />which are motivated by theory, but to some extent simplified<br />in order to lead to useful and implementable algorithms. We<br />then go on to describe how the standard framework of sto-<br />chastic approximation, of which the Robbins-Monro recur-<br />sion is the cornerstone, provides us with a systematic frame-<br />work to design families of mappings {θi} in a recursive man-<br />ner and understand their properties. In Sect. 5 we present a<br />series of novel adaptive algorithms which circumvent some<br />of the caveats of existing procedures. These algorithms are<br />applied to various examples in Sect. 6.<br />2 The trouble with adaptation<br />In this section we first illustrate the loss of π-ergodicity of<br />controlledMCMCwiththehelpoftwosimpletoyexamples.<br />The level of technicality required for these two examples<br />is that of a basic undergraduate course on Markov chains.<br />Despite their simplicity, these examples suggest that vanish-<br />ing adaptation (a term made more precise later) might pre-<br />serve asymptotic π-ergodicity. We then finish this section<br />by formulating more precisely the fundamental difference<br />between standard MCMC algorithms and their controlled<br />counterparts which affects the invariant distribution of the<br />algorithm. This requires the introduction of some additional<br />notation used in Sect. 4 and a basic understanding of expec-<br />tations to justify vanishing adaptation, but does not signifi-<br />cantly raise the level of technicality.<br />Consider the following toy example, suggested in An-<br />drieu and Moulines (2006), where X = {1,2} and π =<br />(1/2,1/2) (it is understood here that for such a case we will<br />abuse notation and use π for the vector of values of π and<br />Pθfor the transition matrix) and where the family of transi-<br />tion probabilities under consideration is of the form, for any<br />θ ∈ ? := (0,1)<br />?Pθ(Xi= 1,Xi+1= 1)<br />?<br />It is clear that for any θ ∈ ?, π is a left eigenvector of Pθ<br />with eigenvalue 1,<br />Pθ=<br />Pθ(Xi= 1,Xi+1= 2)<br />Pθ(Xi= 2,Xi+1= 2)<br />Pθ(Xi= 2,Xi+1= 1)<br />θ<br />1−θ<br />1−θ<br />?<br />=<br />θ<br />?<br />.<br />(3)<br />πPθ= π,<br />i.e. π is an invariant distribution of Pθ. For any θ ∈ ? the<br />Markov chain is obviously irreducible and aperiodic, and<br />by standard theory is therefore ergodic, i.e. for any starting</p>  <p>Page 4</p> <p>346 Stat Comput (2008) 18: 343–373<br />probability distribution μ,<br />lim<br />i→∞μPi<br />(with Pi<br />function f<br />θ= π<br />θthe i-th power of Pθ), and for any finite real valued<br />lim<br />N→∞<br />1<br />N<br />N<br />?<br />i=1<br />f(Xi) = Eπ(f(X)),<br />almost surely, where for any probability distribution ν, Eν<br />represents the expectation operator with respect to ν. Now<br />assume that θ is adapted to the current state in order to sam-<br />ple the next state of the chain, and assume for now that<br />this adaptation is a time invariant function of the previous<br />state of the MC. More precisely assume that for any i ≥ 1<br />the transition from Xi to Xi+1is parametrised by θ(Xi),<br />where θ : X → ?. The remarkable property, specific to this<br />purely pedagogical example, is that {Xi} is still in this case<br />a time homogeneous Markov chain with transition proba-<br />bility<br />ˇ P(Xi= a,Xi+1= b) := Pθ(a)(Xi= a,Xi+1= b)<br />for a,b ∈ X, resulting in the time homogeneous transition<br />matrix<br />?<br />Naturally the symmetry of Pθ above is lost and one can<br />check that the invariant distribution ofˇ P is<br />?<br />ˇ P :=<br />θ(1)<br />1−θ(2)<br />1−θ(1)<br />θ(2)<br />?<br />.<br />(4)<br />ˇ π =<br />1−θ(2)<br />2−θ(1)−θ(2),<br />1−θ(1)<br />2−θ(1)−θ(2)<br />?<br />?= π,<br />in general. For θ(1),θ(2) ∈ ? the time homogeneous<br />Markov chain will be ergodic, but will fail to converge to<br />π as soon as θ(1) ?= θ(2), that is as soon as there is depen-<br />dence on the current state. As we shall see, the principle of<br />vanishing adaptation consists of the present toy example of<br />making both θ(1) and θ(2) time dependent (deterministi-<br />cally for simplicity here), denoted θi(1) and θi(2) at itera-<br />tion i, and ensure that as i → ∞, |θi(1) − θi(2)| vanishes.<br />Indeed, while {θi(1)} and {θi(2)} are allowed to evolve for-<br />ever (and maybe not converge) the corresponding transition<br />probabilities {ˇ Pi:= Pθi(Xi)} haveinvariantdistributions {ˇ πi}<br />convergent to π. We might hence expect one to recover π-<br />ergodicity. In fact in the present case standard theory for<br />non-homogeneous Markov chains can be used in order to<br />find conditions on {θi} that ensure ergodicity, but we do not<br />pursue this in depth here.<br />It could be argued, and this is sometimes suggested, that<br />the problem with the example above is that in order to pre-<br />serve π as a marginal distribution, θ should not depend on<br />Xifor the transition to Xi+1, but on X0,...,Xi−1only. For<br />simplicityassumethatthedependenceison Xi−1only.Then<br />it is sometimes argued that since<br />?<br />π(Xi= 1)<br />π(Xi= 2)<br />?T<br />×<br />?<br />?Pθ(Xi−1)(Xi= 1,Xi+1= 1)<br />?T?<br />=?π(Xi+1= 1), π(Xi+1= 2)?,<br />then Xi+1,Xi+2,... are all marginally distributed accord-<br />ing to π. Although this calculation is correct, the underly-<br />ing reasoning is naturally incorrect in general. This can be<br />checked in two ways. First through a counterexample which<br />only requires elementary arguments. Indeed in the situation<br />just outlined, the law of Xi+1 given θ0,X0,...,Xi−1,Xi<br />is Pθ(Xi−1)(Xi,Xi+1∈ ·), from which we deduce that Zi=<br />(Zi(1),Zi(2)) = (Xi,Xi−1) isatimehomogeneousMarkov<br />chain with transition<br />Pθ(Xi−1)(Xi= 1,Xi+1= 2)<br />Pθ(Xi−1)(Xi= 2,Xi+1= 2)<br />Pθ(Xi−1)(Xi= 2,Xi+1= 1)<br />?<br />=<br />π(Xi= 1)<br />π(Xi= 2)<br />θ(Xi−1)<br />1−θ(Xi−1)<br />1−θ(Xi−1)<br />θ(Xi−1)<br />?<br />Pθ(Zi(2))(Zi(1),Zi+1(1)) I{Zi+1(2) = Zi(1)},<br />where for a set A, IA denotes its indicator function. De-<br />noting the states¯1 := (1,1),¯2 := (1,2),¯3 := (2,1) and<br />¯4 := (2,2), the transition matrix of the time homogeneous<br />Markov chain is<br />⎡<br />⎢<br />ˇ P =<br />⎢<br />⎣<br />θ(1)<br />θ(2)<br />0<br />0<br />0<br />0<br />1−θ(1)<br />1−θ(2)<br />0<br />0<br />0<br />0<br />1−θ(1)<br />1−θ(2)<br />θ(1)<br />θ(2)<br />⎤<br />⎥<br />⎥<br />⎦<br />and it can be directly checked that the marginal invariant<br />distribution of Zi(1) is<br />ˇ π =<br />?<br />×<br />2+<br />?<br />θ(2)<br />1−θ(1)+<br />1+θ(2)−θ(1)<br />1−θ(1)<br />θ(1)<br />1−θ(2)<br />1+θ(1)−θ(2)<br />1−θ(2)<br />?−1<br />?<br />?= (1/2, 1/2),<br />in general. The second and more informative approach con-<br />sists of considering the actual distribution of the process<br />generated by a controlled MCMC. Let us denoteˇE∗ the<br />expectation for the process started at some arbitrary θ,x ∈<br />? × X. This operator is particularly useful to describe the<br />expectation of ψ(Xi,Xi+1,...) for any i ≥ 1 and any func-<br />tion ψ : Xkψ→ R,ˇE∗(ψ(Xi,Xi+1,...,Xi+kψ−1)). More<br />precisely it allows one to clearly express the dependence of<br />θi(θ0,X0,...,Xi) on the past θ0,X0,...,Xiof the process.<br />Indeed for any f : X → R, using the tower property of<br />expectations and the definition of controlled MCMC given</p>  <p>Page 5</p> <p>Stat Comput (2008) 18: 343–373347<br />in the introduction, we find that<br />ˇE∗(f(Xi+1)) =ˇE∗<br />?ˇE∗(f(Xi+1)|θ0,X0,...,Xi)<br />??<br />?<br />?<br />=ˇE∗<br />XPθi(X0,...,Xi)(Xi,dx)f(x),<br />(5)<br />which is another way of saying that the distribution of<br />Xi+1 is that of a random variable sampled, conditional<br />upon θ0,X0,...,Xi, according to the random transition<br />Pθi(X0,...,Xi)(Xi,Xi+1 ∈ ·), where the pair θi(θ0,X0,<br />...,Xi), Xi is randomly drawn from a distribution com-<br />pletely determined by the possible histories θ0,X0,...,Xi.<br />In the case where X is a finite discrete set, writing this<br />relation concisely as the familiar product of a row vector<br />and a transition matrix as above would require one to de-<br />termine the (possibly very large) set of values for the pair<br />θi(θ0,X0,...,Xi), Xi(say Wi), the vector representing the<br />probability distribution of all these pairs as well as the tran-<br />sition matrix from Wito X. The introduction of the expec-<br />tation allows one to bypass these conceptual and notational<br />difficulties. We will hereafter denote<br />?<br />and whenever possible will drop unnecessary arguments i.e.<br />arguments of ϕ which do not affect its values.<br />The possibly complex dependence on θi(θ0,X0,...,Xi),<br />Xiof the transition of the process to Xi+1needs to be con-<br />trasted with the case of standard MCMC algorithms. Indeed,<br />in this situation the randomness of the transition probability<br />only stems from Xi. This turns out to be a major advantage<br />when it comes to invariant distributions. Let us assume that<br />for some i ≥ 1ˇE∗(g(Xi)) = Eπ(g(X)) for all π-integrable<br />functions g. Then according to the identity in (5), for any<br />given θ ∈ ? and θi= θ for all i ≥ 0 a standard MCMC al-<br />gorithm has the well known and fundamental property<br />ϕ(θ0,X0,...,Xi) :=<br />X<br />Pθi(θ0,X0,...,Xi)(Xi,dx)f(x),<br />ˇE∗(f(Xi+1)) =ˇE∗(ϕ(θ,Xi))<br />= Eπ(ϕ(θ,X))<br />=<br />?<br />X×X<br />π(dx)Pθ(x,dy)f(y) = Eπ(f(X)),<br />where the second equality stems from the assumption<br />ˇE∗(g(Xi)) = Eπ(g(X)) and the last equality is obtained by<br />the assumed invariance of π for Pθ for any θ ∈ ?. Now<br />we turn to the controlled MCMC process and focus for<br />simplicity on the case θi(θ0,X0,...,Xi) = θ(Xi−1), cor-<br />responding to our counterexample. Assume that for some<br />i ≥ 1 Xi is marginally distributed according to π, i.e. for<br />any g : X → R,ˇE∗(g(Xi)) = Eπ(g(X)), then we would like<br />to check ifˇE∗(g(Xj)) = Eπ(g(X)) for all j ≥ i. However<br />using the tower property of expectations in order to exploit<br />the propertyˇE∗(g(Xi)) = Eπ(g(X)),<br />ˇE∗(f(Xi+1)) =ˇE∗(ϕ(Xi−1,Xi))<br />=ˇE∗<br />?ˇE∗(ϕ(Xi−1,X)|X)<br />Now it would be tempting to use the stationarity assumption<br />in the last expression,<br />?<br />= Eπ(f(X)).<br />This is however not possible due to the presence of the<br />conditional expectationˇE∗(·|X) (which crucially depends<br />on X) and conclude that in general<br />?ˇE∗(ϕ(Xi−1,Xi)|Xi)<br />?<br />.<br />= Eπ<br />?<br />Eπ(ϕ(Xi−1,X)) =<br />X×X<br />π(dx)Pθ(Xi−1)(x,dy)f(y)<br />ˇE∗(f(Xi+1))<br />?=ˇE∗<br />?<br />Eπ<br />??<br />X<br />Pθ(θ0,X0,Xi−1)(X,dxi+1)f(xi+1)<br />??<br />.<br />The misconception that this inequality might be an equality<br />is at the root of the incorrect reasoning outlined earlier. This<br />problem naturally extends to more general situations.<br />Vanishing adaptation seems, intuitively, to offer the pos-<br />sibility to circumvent the problem of the loss of π as in-<br />variant distribution. However, as illustrated by the follow-<br />ing toy example, vanishing adaptation might come with<br />its own shortcomings. Consider a (deterministic) sequence<br />{θi} ⊂ (−1,1)Nand for simplicity first consider the non-<br />homogeneous, and non-adaptive, Markov chain {Xi} with<br />transition Pθiat iteration i ≥ 1, where Pθ is given by (3),<br />and initial distribution (μ,1 − μ) for μ ∈ [0,1]. One can<br />easily check that for any n ≥ 1 the product of matrices<br />Pθ1×···×Pθnhas the simple expression<br />Pθ1×···×Pθn<br />=1<br />2<br />i=1(2θi−1)<br />As a result one deduces that the distribution of Xnis<br />?1+?n<br />i=1(2θi−1)<br />1−?n<br />i=1(2θi−1)<br />i=1(2θi−1)<br />1−?n<br />1+?n<br />?<br />.<br />1<br />2<br />Now if θi→ 0 (resp. θi→ 1) and?∞<br />of {θi} is “too fast” , then limn→∞<br />as a consequence, whenever μ ?= 1/2, the distribution of Xn<br />does not converge to π = (1/2,1/2). Similar developments<br />are possible for the toy adaptive MCMC algorithm given by<br />?1+(2μ−1)?n<br />?∞<br />i=1(2θi−1)<br />1−(2μ−1)?n<br />i=1(2θi−1)?.<br />i=1θi&lt; +∞ (resp.<br />i=1(1 − θi) &lt; +∞ ), that is convergence to either 0 or 1<br />?n<br />i=1(2θi− 1) ?= 0 and</p>  <p>Page 6</p> <p>348 Stat Comput (2008) 18: 343–373<br />the transition matrix ˇ P in (4), at the expense of extra tech-<br />nical complications, and lead to the same conclusions. This<br />toy example points to potential difficulties encountered by<br />controlled MCMC algorithms that exploit vanishing adapta-<br />tion: whereas π-ergodicity of Pθis ensured for any θ ∈ ?,<br />this property might be lost if the sequence {θi} wanders to-<br />wards “bad” values of θ for which convergence to equilib-<br />rium of the corresponding fixed parameter Markov chains<br />Pθmight take an arbitrarily long time.<br />This point is detailed in the next section, but we first turn<br />to a discussion concerning the possibility of using vanishing<br />adaptation in order to circumvent the loss of invariance of π<br />by controlled MCMC.<br />3 Vanishing adaptation and convergence<br />As suggested in the previous section, vanishing adaptation,<br />that is ensuring that θidepends less and less on recently vis-<br />ited states of the chain {Xi} might be a way of designing<br />controlled MCMC algorithms which produce samples as-<br />ymptotically distributed according to π. In this section we<br />provide the basic arguments and principles that underpin the<br />validity of controlled MCMC with vanishing adaptation. We<br />however do not provide directly applicable technical con-<br />ditions here that ensure the validity of such algorithms -<br />more details can be found in Holden (1998), Atchadé and<br />Rosenthal (2005), Andrieu and Moulines (2006), Roberts<br />and Rosenthal (2006), Bai et al. (2008) and Atchadé and<br />Fort (2008). The interest of dedicating some attention to this<br />point here is twofold. First it provides useful guidelines as<br />to what the desirable properties of a valid controlled MCMC<br />algorithm should be, and hence help design efficient algo-<br />rithms. Secondly it points to some difficulties with the ex-<br />isting theory which is not able to fully explain the observed<br />stabilitypropertiesofnumerouscontrolledalgorithms,afact<br />sometimes overlooked.<br />3.1 Principle of the analysis<br />Existing approaches to prove that ergodicity might be pre-<br />served under vanishing adaptation all rely on the same prin-<br />ciple, which we detail in this section. The differences be-<br />tween the various existing contributions lies primarily in the<br />assumptions, which are discussed in the text. With the nota-<br />tion introduced in the previous section, we are interested in<br />the behaviour of the difference<br />|ˇE∗(f(Xi))−Eπ(f(X))|<br />as i → ∞ for any f : X → R. Although general functions<br />can be considered (Atchadé and Rosenthal 2005; Andrieu<br />and Moulines 2006) and (Atchadé and Fort 2008), we will<br />here assume for simplicity of exposition that |f| ≤ 1. The<br />study of this term is carried out by comparing the process<br />of interest to a process which coincides with {Xk} up to<br />some time ki&lt; i but becomes a time homogeneous Markov<br />chainwith“frozen”transitionprobability Pθkifromthistime<br />instant onwards. (We hereafter use the following standard<br />notation Pk[f](x) = Pkf(x) for any f : X → Rnfand<br />x ∈ X defined recursively as P0f(x) = f(x), Pf(x) :=?<br />the finite discrete case this corresponds to considering pow-<br />ers Pkof the transition matrix P and right multiplying with<br />a vector f.) Denoting Pi−ki<br />θki<br />after i − ki iterations of the “frozen” time homogeneous<br />Markov transition probability Pθkiinitialised with Xkiat<br />time kiand conditional upon θ0,X0,X1,...,Xki, this trans-<br />lates into the fundamental decomposition<br />X<br />P(x,dy)f(y) and Pk+1f(x) = P[Pkf](x) for k ≥ 1. In<br />f(Xki) the expectation of f<br />ˇE∗(f(Xi))−Eπ(f(X))<br />=ˇE∗<br />+ˇE∗<br />?<br />Pi−ki<br />θki<br />?<br />f(Xki)−π(f)<br />f(Xi)−Pi−ki<br />?<br />θki<br />f(Xki)<br />?<br />,<br />(6)<br />where the second term corresponds to the aforementioned<br />comparison and the first term is a simple remainder term.<br />Perhaps not surprisingly the convergence to zero of the<br />first term, provided that i − ki→ ∞ as i → ∞, depends<br />on the ergodicity of the non-adaptive MCMC chain with<br />fixed parameter θ ∈ ?, i.e. requires at least that for any<br />θ,x ∈ ?×X, limk→∞|Pk<br />both θkiand Xkiare random and possibly time dependent,<br />this type of simple convergence is not sufficient to ensure<br />convergence of this term. One could suggest the following<br />uniform convergence condition<br />θf(x)−π(f)| = 0.Howeversince<br />lim<br />k→∞<br />sup<br />θ,x∈?×X<br />|Pk<br />θf(x)−Eπ(f(X))| = 0,<br />(7)<br />which although mathematically convenient is unrealistic in<br />most scenarios of interest. The first toy example of Sect. 2<br />provides us with such a simple counterexample. Indeed, at<br />least intuitively, convergence of this Markov chain to equi-<br />librium can be made arbitrarily slow for values of θ ∈ (0,1)<br />arbitrarily close to either 0 or 1. This negative property un-<br />fortunately carries on to more realistic scenarios. For ex-<br />ample the normal symmetric random walk Metropolis al-<br />gorithm described in Sect. 1 can in most situations of in-<br />terest be made arbitrarily slow as the variance θ2is made<br />arbitrarily small or large. This turns out to be a fundamental<br />difficulty of the “chicken and egg” type in the study of the<br />stability of such processes, which is sometimes overlooked.<br />Indeed in order to ensure ergodicity, {θi} should stay away<br />from poor values of the parameter θ ∈ ?, but proving the<br />stability of {θi} might often require establishing the ergod-<br />icity of the chain {Xi}; see Andrieu and Moulines (2006)<br />and Andrieu and Tadi´ c (2007) where alternative conditions</p>  <p>Page 7</p> <p>Stat Comput (2008) 18: 343–373349<br />are also suggested. We will come back to this point af-<br />ter examining the second term of the decomposition above.<br />Note that “locally uniform” such conditions (i.e. where ?<br />in (7) is replaced by some subsets K ⊂ ? and the rate<br />of convergence might be slower) are however satisfied by<br />many algorithms—this property is exploited in Andrieu and<br />Moulines (2006), Andrieu and Tadi´ c (2007), Atchadé and<br />Fort (2008) and Bai et al. (2008) although this is not explicit<br />in the latter.<br />The second term in the decompositioncan be analysedby<br />“interpolating” the true process and its Markovian approxi-<br />mation using the following telescoping sum<br />?<br />=<br />j=ki<br />ˇE∗(f(Xi))−ˇE∗<br />i−1<br />?<br />which can be easily understood as follows. Each term<br />of the sum is the difference of the expectations of (a) a<br />process that adapts up to time j + 1 &gt; kiand then freezes<br />and “becomes Markovian” with transition probability Pθki<br />given the history θ0,X0,X1,...,Xj+1 (and hence θki=<br />θki(θ0,X0,X1,...,Xki)) between time j + 1 and time i<br />(b) and likewise for the second term, albeit between time j<br />and i. Hence the two terms involved only differ in that at<br />time j the first term updates the chain with θj while the<br />second term uses θki, which can be concisely expressed as<br />follows (thinking about the difference between two products<br />of matrices in the discrete case might be helpful),<br />?<br />=ˇE∗<br />=ˇE∗<br />Pi−ki<br />θki<br />f(Xki)<br />?<br />?<br />ˇE∗<br />?<br />Pi−j−1<br />θki<br />f(Xj+1)<br />−ˇE∗<br />?<br />Pi−j<br />θki<br />f(Xj)<br />?<br />,<br />ˇE∗<br />Pi−j−1<br />θki<br />f(Xj+1)<br />PθjPi−j−1<br />θki<br />??<br />?<br />f(Xj)<br />?<br />−ˇE∗<br />?<br />Pi−j<br />θki<br />f(Xj)<br />?<br />?<br />?<br />f(Xj)<br />?<br />?<br />−ˇE∗<br />Pi−j<br />θki<br />?<br />Pθj−Pθki<br />Pi−j−1<br />θki<br />f(Xj).<br />The role of vanishing adaptation should now be appar-<br />ent. Provided that the transition probability Pθ is suffi-<br />ciently smooth in θ and that the variations of {θi} vanish<br />as i → +∞ (in some unspecified sense at this point) then<br />we might expect<br />i−1<br />?<br />to vanish if the number of terms in this sum does not<br />grow too rapidly. However as noticed when analysing the<br />first term of the fundamental decomposition above, simple<br />continuity cannot be expected to be sufficient in general<br />since θki,θki+1,...,θi−1are random and time dependent.<br />By analogy with the analysis above one could assume some<br />j=ki<br />ˇE∗<br />??<br />Pθj−Pθki<br />?<br />Pi−j−1<br />θki<br />f(Xj)<br />?<br />form of uniform continuity in order to eliminate the vari-<br />ability of θjand θkiin the expression above. More precisely,<br />denoting for any δ &gt; 0<br />?(δ) := sup<br />|g|≤1<br />sup<br />x∈X,{θ,θ?∈?:|θ−θ?|≤δ}<br />|Pθg(x)−Pθ?g(x)|,<br />one could assume,<br />lim<br />δ→0?(δ) = 0.<br />Provided that the sequence {θi} is such that its increments<br />are bounded i.e. such that |θi− θi−1| ≤ γifor a determin-<br />istic sequence {γi} ∈ [0,∞)N(which is possible since the<br />updates of {θi} are chosen by the user) then the second term<br />of (6) can be bounded by<br />i−1<br />?<br />which can usually be easily dealt with; e.g. when some uni-<br />form Lipschitz continuity is assumed and ?(δ) = Cδ for<br />some constant C &gt; 0 (Andrieu and Moulines 2006). Unfor-<br />tunately, although mathematicallyconvenient,this condition<br />is not satisfied in numerous situations of interest, due in par-<br />ticular to the required uniformity in θ,θ?∈ ?. Other appar-<br />ently weaker conditions have been suggested, but share the<br />same practical underlying difficulties such as<br />j=ki<br />j<br />?<br />k=ki+1<br />?γk,<br />lim<br />i→∞sup<br />|g|≤1<br />ˇE∗<br />?|Pθig(Xi)−Pθi−1g(Xi)|?= 0<br />suggested in Benveniste et al. (1990, p. 236) and the slightly<br />stronger condition of the type<br />lim<br />i→∞<br />sup<br />x∈X,|g|≤1<br />ˇE∗<br />?|Pθig(x)−Pθi−1g(x)|?= 0,<br />in Roberts and Rosenthal (2006).<br />3.2 Discussion<br />The simple discussion above is interesting in two respects.<br />On the one hand, using basic arguments, it points to the pri-<br />mary conditions under which one might expect controlled<br />MCMC algorithms to be π-ergodic: “expected ergodicity<br />and continuity of the transition probabilities”. On the other<br />hand it also points to the difficulty of proposing verifiable<br />conditions to ensure that the aforementioned primary condi-<br />tions are satisfied. The problem stems from the fact that it is<br />required to prove that the algorithm is not unlucky enough<br />to tune θ to poor values, leading to possibly unstable algo-<br />rithms.Theuniformconditionssuggestedearliercircumvent<br />this problem, since they suggest that there are no such arbi-<br />trarily “bad” values. This is unfortunately not the case for</p>  <p>Page 8</p> <p>350 Stat Comput (2008) 18: 343–373<br />numerous algorithms of practical interest. However it is of-<br />ten the case that such uniformity holds for subsets K ⊂ ?.<br />For example in the case of the first toy exampleof Sect. 2 the<br />sets defined as K?:= [?,1 − ?] for any ? ∈ (0,1) are such<br />that for any ? ∈ (0,1) the Markov chains with parameters<br />θ ∈ K?are geometrically convergent with a rate of at least<br />|1−2?|, independent of θ ∈ K?. A simple practical solution<br />thus consists of identifying such subsets K and constrain<br />{θi} to these sets by design. This naturally requires some un-<br />derstanding of the problem at hand, which might be difficult<br />in practice, and does not reflect the fact that stability is ob-<br />served in practice without the need to resort to such “fixed”<br />truncation strategies. A general approach for adaptive trun-<br />cation is developed and analysed in Andrieu et al. (2005)<br />and Andrieu and Moulines (2006). It takes advantage of the<br />fact that uniform ergodicity and continuity can be shown for<br />families of subsets of {Ki⊂ ?}, such that {Ki⊂ ?} is a<br />covering of ? such that Ki⊂ Ki+1for i ≥ 0. The strategy<br />then consists of adapting the truncation of the algorithm on<br />the fly in order to ensure that {θi} does not wander too fast<br />towards inappropriate values in ? or at its boundary, hence<br />ensuring that ergodicity can kick in and stabilise the trajec-<br />tories of {θi}, i.e. ensure that there is a random, but finite,<br />k such that {θi} ⊂ Kk with probability 1. The procedure<br />has the advantage of not requiring much knowledge about<br />what constitutes good or bad values for θ, while allowing<br />for the incorporation of prior information and ultimately en-<br />suring stability. It can in fact be shown, under conditions<br />satisfied by some classes of algorithms, that the number k of<br />reprojections needed for stabilisation is a random variable<br />with probability distribution whose tails decay superexpo-<br />nentially. While this approach is general and comes with a<br />general and applicable theory, it might be computationally<br />wasteful in some situations and more crucially does not re-<br />flect the fact that numerous algorithm naturally present sta-<br />bility properties. Another possibility consists of considering<br />mixtures of adaptive and non-adaptive MCMC proposal dis-<br />tributions, the non adaptive components ensuring stability<br />e.g. (Roberts and Rosenthal 2007): again while this type of<br />strategy generally ensures that the theory works, it poses the<br />problem of the practical choice of the non-adaptive compo-<br />nent, and might not always result in efficient strategies. In<br />addition, as for the strategies discussed earlier, this type of<br />approach fails to explain the observed behaviour of some<br />adaptive algorithms.<br />In a number of situations of interest it is possible to show<br />that the parameter θ stays away from its forbidden values<br />with probability one (Andrieu and Tadi´ c 2007). The ap-<br />proach establishes a form of recurrence via the existence of<br />composite drift functions for the joint chain {θi,Xi}, which<br />in turn ensures an infinite number of visits of {θi} to some<br />sets K for which the uniform properties above hold. This<br />can be shown to result in stability under fairly general con-<br />ditions. In addition the approach provides one with some<br />insight into what makes an algorithm stable or not, and sug-<br />gests numerous ways of designing updates for {θi} which<br />will ensure stability and sometimes even accelerate conver-<br />gence. Some examples are discussed in Sect. 4.2.2. In Saks-<br />man and Vihola (2008) the authors address the same prob-<br />lem by proving that provided that {θi} is constrained not to<br />drift too fast to bad values, then π-ergodicity of {Xi} is pre-<br />served. The underlying ideas are related to a general strategy<br />of stabilisation developed for the stochastic approximation<br />procedure, see Andradóttir (1995).<br />Finally we end this section with a practical implication<br />of the developments above, related to the rate of conver-<br />gence of controlled MCMC algorithms. Assume for exam-<br />ple the existence of K ⊂ ?, C ∈ (0,∞), ρ ∈ (0,1) and<br />{γi} ∈ [0,∞)Nsuch that for all i ≥ 1, θ,x ∈ K × X and any<br />f : X → [−1,1]<br />|Pi<br />and for any θ,θ?∈ K, x ∈ X and any f : X → [−1,1],<br />|Pθf(x)−Pθ?f(x)| ≤ C|θ −θ?|,<br />and such that for all i ≥ 1, |θi− θi−1| ≤ γi, where {γi}<br />satisfies a realistic assumption of slow decay (Andrieu and<br />Moulines2006)(satisfiedforexampleforγi= 1/iα,α &gt; 0).<br />These conditions are far from restrictive and can be shown<br />to hold for the symmetric random walk Metropolis (SRWM)<br />for some distributions π, the independent Metropolis-<br />Hastings (IMH) algorithm, mixtures of such transitions etc.<br />(Andrieu and Moulines 2006). Less restrictive conditions<br />are possible, but lead to slower rates of convergence. Then,<br />using a more precise form of the decomposition in (6) (An-<br />drieu and Moulines 2006, Proposition 4), one can show that<br />there exists a constant C?∈ (0,∞) such that for all i ≥ 1<br />and |f| ≤ 1,<br />???ˇE∗<br />be infinity). The result simply tells us that while the adapted<br />parameter does not leave K, convergence towards π occurs<br />at a rate of at least {γi}, and as pointed out in Andrieu and<br />Moulines (2006) does not require convergence of {θi}. This<br />might appear to be a negative result. However it can be<br />proved, (Andrieu 2004) and (Andrieu and Moulines 2006),<br />that there exist constants A(γ,K) and B(γ,K) such that for<br />any N ≥ 1,<br />?<br />N<br />i=1<br />≤A(γ,K)<br />N<br />θf(x)−Eπ(f)| ≤ Cρi,<br />(8)<br />(9)<br />?(f(Xi)−Eπ(f))I{σ ≥ i}???? ≤ C?γi,<br />(10)<br />where σ is the first time at which {θi} leaves K (which can<br />?<br />?<br />?ˇE∗<br />??????<br />1<br />N<br />?<br />f(Xi)−Eπ(f)<br />?????<br />2<br />I{σ ≥ n}<br />?<br />√N<br />+B(γ,K)<br />?N<br />k=1γk<br />.<br />(11)</p>  <p>Page 9</p> <p>Stat Comput (2008) 18: 343–373 351<br />The first term corresponds to the Monte Carlo fluctuations<br />while the second term is the price to pay for adaptation. As-<br />suming that γi= i−αfor α ∈ (0,1), then<br />?N<br />N<br />which suggests no loss in terms of rate of convergence for<br />α ≥ 1/2. More general and precise results can be found in<br />Andrieu and Moulines (2006, Proposition 6), including a<br />central limit theorem (Theorem 9) which shows the asymp-<br />totic optimality of adaptive MCMC algorithms when con-<br />vergence of {θi} is ensured . Weaker rates of convergence<br />than (8) lead to a significant loss of rate of convergence,<br />which is also observed in practice.<br />k=1γk<br />∼<br />1<br />1−αN−α,<br />4 Vanishing adaptation: a framework for consistent<br />adaptive MCMC algorithms<br />In the previous section we have given arguments that sug-<br />gest that vanishing adaptation for MCMC algorithms might<br />lead to algorithms from which expectations with respect to<br />a distribution of interest π can be consistently estimated.<br />However neither criteria nor ways of updating the parameter<br />θ were described. The main aim of this section is to point<br />out the central role played by stochastic approximation and<br />the Robbins-Monro recursion (Robbins and Monro 1951) in<br />the context of vanishing or non-vanishing adaptation. While<br />a complete treatment of the theoretical aspects of such con-<br />trolled MCMC algorithms is far beyond the scope of this<br />review, our main goal is to describe the principles underpin-<br />ning this approach that have a practical impact and to show<br />the intricate link between criteria and algorithms. Indeed, as<br />we shall see, while the stochastic approximation framework<br />can be used in order to optimise a given criterion, it can also<br />help understand the expected behaviour of an updating al-<br />gorithm proposed without resorting to grand theory, but by<br />simply resorting to common sense.<br />4.1 Criteria to optimise MCMC algorithms and a general<br />form<br />Since our main aim is that of optimising MCMC transi-<br />tion probabilities, the first step towards the implementation<br />of such a procedure naturally consists of defining what is<br />meant by optimality, or suboptimality. This can be achieved<br />through the definition of a cost function, which could for ex-<br />ample express some measure of the statistical performance<br />of the Markov chain in its stationary regime e.g. favour neg-<br />ative correlation between Xi and Xi+lfor some lag l and<br />i = 0,1,.... In what follows we will use the convention that<br />an optimum value θ∗corresponds to a root of the equation<br />h(θ) = 0 for some function h(θ) closely related to the afore-<br />mentioned cost function.<br />Since the main use of MCMC algorithms is to compute<br />averages of the formˆIθ<br />Eπ(f(X)),in situationswhereacentrallimittheoremholds,<br />i.e. in scenarios such that for any θ ∈ ?<br />√N (ˆIθ<br />N(f) given in (1) in order to estimate<br />N(f)−Eπ(f(X))) →DN(0,σ2<br />it might seem natural to attempt to optimise the constant<br />σ2<br />θ(f). This however poses several problems. The first prob-<br />lem is computational. Indeed, for a given θ ∈ ? and f :<br />X :→ [−1,1] (for simplicity) and when it exists, σ2<br />be shown to have the following expression<br />θ(f)),<br />θ(f) can<br />σ2<br />θ(f) = Eπ(¯ f2(X0))+2<br />+∞<br />?<br />k=1<br />Eθ(¯ f(X0)¯ f(Xk))<br />= Eπ(¯ f2(X0))+2Eθ<br />?+∞<br />k=1<br />?<br />¯ f(X0)¯ f(Xk)<br />?<br />,<br />(12)<br />with ¯ f(x) := f(x) − Eπ(f(X)) and Eθthe expectation<br />associated to the Markov chain with transition probability<br />Pθ and such that X0∼ π. This quantity is difficult to esti-<br />mate and optimise (since for all θ ∈ ? it is the expectation<br />of a non-trivial function with respect to an infinite set of<br />random variables) although some solutions exist (Vladislav<br />Tadi´ c, personal communication, see also Richard Everitt’s<br />Ph.D. thesis) and truncation of the infinite sum is also possi-<br />ble (Andrieu and Robert 2001; Pasarica and Gelman 2003),<br />allowing for example for the recursive estimation of the<br />gradient of σ2<br />θ(f) with respect to θ. In Pasarica and Gel-<br />man (2003), maximising the expected mean square jump<br />distance is suggested, i.e. here in the scalar case and with<br />¯Xi= Xi−Eπ(X) for i = 0,1,<br />Eθ?<br />= 2<br />(X0−X1)2?<br />= Eθ??¯X0−¯X1<br />Eπ<br />?2?<br />−Eθ?¯X0¯X1<br />?<br />?<br />¯X2?<br />??<br />(13)<br />which amounts to minimising the term corresponding to<br />k = 1 in (12) for the function f(x) = x for all x ∈ X. An-<br />other difficulty is that the criterion depends on a specific<br />function f, and optimality for a function f might not re-<br />sult in optimality for another function g. Finally it can be<br />argued that although optimising this quantity is an asymp-<br />totically desirable criterion, at least for a given function, this<br />criterion can in some scenarios lead to MCMC samplers that<br />are slow to reach equilibrium (Besag and Green 1993).<br />Despite the difficulties pointed out earlier, the criterion<br />above should not be totally discarded, but instead of try-<br />ing to optimise it directly and perfectly, suboptimal op-<br />timisation through proxies that are amenable to simple<br />computation and efficient estimation might be preferable.</p>  <p>Page 10</p> <p>352Stat Comput (2008) 18: 343–373<br />Such a simple criterion, which is at least completely sup-<br />ported by theory in some scenarios (Roberts et al. 1997;<br />Sherlock and Roberts 2006; Roberts and Rosenthal 1998;<br />Bédard 2006) and proves to be more universal in practice,<br />is the expected acceptance probability of the MH algorithm<br />for random walk Metropolis algorithms or Langevin based<br />MH updates. The expected acceptance probability is more<br />formally defined as the jump rate of a MH update in the sta-<br />tionary regime<br />¯ αθ:=<br />?<br />X2min<br />?<br />1,π (y)qθ(y,x)<br />π (x)qθ(x,y)<br />?<br />?<br />π (x)qθ(x,y)dxdy<br />= Eπ⊗qθ<br />?<br />min1,π (Y)qθ(Y,X)<br />π (X)qθ(X,Y)<br />??<br />.<br />(14)<br />This criterion has several advantages. The first one is com-<br />putational, since it is much simpler an expectation of a much<br />simpler function than σ2<br />θ(f). In such cases it has the double<br />advantage of being independent of any function f and to<br />provide a good compromise for σ2<br />A less obvious advantage of this criterion, which we illus-<br />trate later on in Sect. 5, is that where some form of smooth-<br />ness of the target density is present it can be beneficial in<br />the initial stages of the algorithm in order to ensure that the<br />adaptive algorithm actually starts exploring the target distri-<br />bution in order to “learn” some of its features.<br />The aforementioned theoretical results tell us that opti-<br />mality of σ2<br />θ(f) (in terms of θ) or proxy quantities related<br />to this quantity (truncation, asymptotics in the dimension)<br />is reached for a specific value of the expected acceptance<br />probability ¯ αθ, denoted α∗hereafter: 0.234 for the random<br />walk Metropolis algorithm for some specific target distribu-<br />tions and likewise 0.574 for Langevin diffusion based MH<br />updates (Roberts and Rosenthal 1998).<br />In some situations, Gelman et al. (1995) have shown that<br />the “optimal” covariance matrix for a multivariate random<br />walk Metropolis algorithm with proposal N(0,?) is ? :=<br />(2.382/nx)?π, where ?π is the covariance matrix of the<br />target distribution π<br />θ(f) for all functions f.<br />?π= Eπ<br />?XXT?−Eπ(X)ET<br />The covariance is unknown in general situations and re-<br />quires the numerical computation of the pair<br />π(X).<br />?Eπ(X),Eπ<br />As pointed out in Andrieu and Moulines (2006, Sect. 7), this<br />can also be interpreted as minimising the Kullback-Leibler<br />divergence<br />?XXT??= Eπ<br />?(X, XXT)?.<br />(15)<br />?<br />X<br />π(x)log<br />π(x)<br />N(x;μ,?)dx = Eπ<br />?<br />log<br />π(X)<br />N(X;μ,?)<br />?<br />,<br />which suggests generalisations consisting of minimising<br />Eπ<br />?<br />logπ(X)<br />qθ(X)<br />?<br />,<br />(16)<br />in general, for some parametric family of probability dis-<br />tributions {qθ,θ ∈ ?}. Section 7 of Andrieu and Moulines<br />(2006) is dedicated to the development or an on-line EM<br />algorithm and a theoretical analysis of an adaptive indepen-<br />dent MH algorithm where qθis a general mixture of distri-<br />butions belonging to the exponential family. We will come<br />back to this strategy in Sect. 5.2.2 where we show that this<br />procedure can also be used in order to cluster the state-space<br />X and hence define locally adaptive algorithms.<br />Before turning to ways of optimising criteria of the type<br />described above, we first detail a fundamental fact shared by<br />all the criteria described above and others, which will allow<br />us to describe a general procedure for the control of MCMC<br />algorithms. The shared characteristic is naturally that all the<br />criteria developed here take the form of an expectation with<br />respect to some probability distribution dependenton θ ∈ ?.<br />In fact as we shall see optimality can often be formulated as<br />the problem of finding the root(s) of an equation of the type<br />h(θ) := Eθ(H(θ,X0,Y1,X1,...)) = 0(17)<br />(remember that {Yi} is the sequence of proposed samples)<br />for some function ?×XN:→ Rnhfor some nh∈ N, with in<br />many situations nh= nθ (but not always). The case of the<br />coerced acceptance probability corresponds to<br />H(θ,X0,Y1,X1,...) = min<br />?<br />1,π (Y1)qθ(Y1,X0)<br />π (X0)qθ(X0,Y1)<br />?<br />−α∗,<br />which according to (14) results in the problem of finding the<br />zero(s) of h(θ) = ¯ αθ− α∗. The moment matching situation<br />corresponds to<br />H(θ,X) = (X,XXT)−(μ,?)<br />for which it is sought to find the zeros of h(θ) = (μπ,?π)−<br />(μ,?) i.e. simply (μπ,?π) (naturally assuming that the<br />two quantities exist). It might not be clear at this point how<br />optimising the remaining criteria above might amount to<br />finding the zeros of a function of the form (17). However,<br />under smoothness assumptions, it is possible to consider the<br />gradients of those criteria (note however that one might con-<br />sider other methods than gradient based approaches in order<br />to perform optimisation).In the case of the Kullback-Leibler<br />divergence,andassumingthatdifferentiationandintegration<br />can be swapped, the criterion can be expressed as<br />Eπ<br />?<br />∇θlogπ(X)<br />qθ(X)<br />?<br />= 0 (18)</p>  <p>Page 11</p> <p>Stat Comput (2008) 18: 343–373 353<br />that is<br />H(θ,X) = ∇θlogπ(X)<br />qθ(X)<br />and in the more subtle case of the first order autocovariance<br />minimisation one can invoke a standard score function argu-<br />ment and find the zeros of (in the scalar case for simplicity)<br />?∇θPθ(X0,X1)<br />Similarly, under smoothness assumptions, one can differen-<br />tiate σ2<br />of the form (17). Note that when qθis a mixture of distrib-<br />ution belonging to the exponential family, then it is possible<br />to find the zeros (assumed here to exist) of (18) using an<br />on-line EM algorithm (Andrieu and Moulines 2006).<br />Note that all the criteria described above are “steady<br />state” criteria and explicitly involve π, but that other cri-<br />teria such as the minimisation of return times to a given set<br />C ⊂ X (Andrieu and Doucet 2003), namely<br />?∞<br />i=1<br />with λ a probability measure concentrated on C, do not en-<br />ter this category. Such criteria seem however difficult to op-<br />timise in practice and we do not pursue this.<br />∇θEθ?¯X0¯X1<br />?= Eθ<br />Pθ(X0,X1)<br />X0X1<br />?<br />= 0.<br />θ(f) and obtain a theoretical expression for ∇θσ2<br />θ(f)<br />τ =ˇEθ<br />λ<br />?<br />I{Xi/ ∈ C}<br />?<br />4.2 The stochastic approximation framework<br />We dedicate here a section to the Robbins-Monro update,<br />which although not the only possibility to optimise criteria<br />of the type (17) appears naturally in most known adaptive<br />algorithms and provides us with a nice framework naturally<br />connected to the literature on controlled Markov chains in<br />the engineering literature. The reason for its ubiquity stems<br />from the trivial identity: θi+1= θi+ θi+1− θi. This turns<br />out to be a particularly fruitful point of view in the present<br />context. More precisely, it is well suited to sequential up-<br />dating of {θi} and makes explicit the central role played by<br />the updating rule defining the increments {θi+1− θi}. In<br />light of our earlier discussion {θi+1− θi} should be van-<br />ishing, and when convergence is of interest their cumula-<br />tive sums should also vanish (in some probabilistic sense) in<br />the vicinity of optimal values θ∗. Naturally, although con-<br />venient, this general framework should not prevent us from<br />thinking “outside of the box”.<br />4.2.1 Motivating example<br />Consider the case where X = R and a symmetric ran-<br />dom walk Metropolis (SRWM) algorithm with normal in-<br />crement distribution N(z;0,exp(θ)), resulting in a tran-<br />sition probability PNSRW<br />tions (Roberts et al. 1997) the expected acceptance prob-<br />ability should be in a range close to ¯ α∗= 0.44. We will<br />assume for simplicity that ¯ αθ in (14) is a non-increasing<br />function of θ (which is often observed to be true, but<br />difficult to check rigourously in practice and can further-<br />more be shown not to hold in some situations, Hastie<br />2005). In such situations one can suggest the following<br />intuitive algorithm. For an estimate θi∈ ? obtained af-<br />ter i × L iterations of the controlled MCMC algorithm,<br />one can simulate L iterations of the transition probability<br />PNSRW<br />θi<br />and estimate the expected acceptance probability<br />for such a value of the parameter for the i-th block of sam-<br />ples {XiL+1,YiL+1,...,XiL+L,YiL+L,k = 1,...,L} (ini-<br />tialised with Xi)<br />?<br />and update θiaccording to the following rule, motivated by<br />our monotonicity assumption on ¯ αθ: if ˆ αθi&gt; ¯ α∗then θiis<br />probably (ˆ αθiis only an estimator) too small and should be<br />increased while if ˆ αθi&lt; ¯ α∗then θi should be decreased.<br />There is some flexibility concerning the amount by which<br />θishould be altered and depends either on the criterion one<br />wishes to optimise or more heuristic considerations. How-<br />ever, as detailed later, this choice will have a direct influ-<br />ence on the criterion effectively optimised and in light of<br />the discussion of Sect. 3 concerning diminishing adaptation,<br />this amount of change should diminish as i → ∞ in order<br />to either ensure that π-ergodicity of {Xi} is ensured or that<br />“approximate convergence” of {θi} is ensured. The intuitive<br />description given above can suggest the following updating<br />rules (see also Gilks et al. 1998, Andrieu and Robert 2001,<br />Atchadé and Rosenthal 2005 for similar rules)<br />θ<br />. We know that in some situa-<br />ˆ αθi=1<br />L<br />?L<br />k=1min1,<br />π(YiL+k)<br />π(XiL+k−1)<br />?<br />θi+1= θi+γi+1<br />?I?ˆ αθi− ¯ α∗&gt; 0?−I?ˆ αθi− ¯ α∗≤ 0??<br />(19)<br />or<br />θi+1= θi+γi+1<br />where {γi} ⊂ (0,+∞)Nis a sequence of possibly stochas-<br />tic stepsizes which ensures that the variations of {θi} van-<br />ish. The standard approach consists of choosing the se-<br />quence {γi} deterministic and non-increasing, but it is also<br />possible to choose {γi} random e.g. such that it takes val-<br />ues in {δ,0} for some δ &gt; 0 and such that P(γi= δ) = pi<br />where {pi} ⊂ [0,1]Nis a deterministic and non-increasing<br />sequence (Roberts and Rosenthal 2007), although it is not<br />always clear what the advantage of introducing such an ad-<br />ditional level of randomness is. A more interesting choice in<br />practice consists of choosing {γi} adaptively, see Sect. 4.2.2,<br />?ˆ αθi− ¯ α∗?,<br />(20)</p>  <p>Page 12</p> <p>354 Stat Comput (2008) 18: 343–373<br />but for simplicity of exposition we focus here on the deter-<br />ministic case.<br />We will come back to the first updating rule later on, and<br />now discuss the second rule which as we shall see aims to<br />set (14) equal to ¯ α∗. Notice first that if L → ∞ and the un-<br />derlying Markov chain is ergodic, then ˆ αθ→ ¯ αθand the re-<br />cursion becomes deterministic<br />θi+1= θi+γi+1<br />?¯ αθi− ¯ α∗?<br />(21)<br />and is akin to a standard gradient algorithm, which will con-<br />verge under standard conditions. Motivated by this asymp-<br />totic result, one can rewrite the finite L recursion (20) as<br />follows<br />θi+1= θi+γi+1<br />?¯ αθi− ¯ α∗?+γi+1<br />?ˆ αθi− ¯ αθi<br />?.<br />(22)<br />Assuming for simplicity that there exists θ∗∈<br />terior of ?, such that ¯ αθ∗ = ¯ α∗and that ˆ αθiis unbiased,<br />at least as i → ∞. Then, since |θi+1− θi| ≤ γi+1→ 0 as<br />i → ∞, and provided that ˆ αθ− ¯ αθ is smooth in terms of<br />θ ∈ ? the sequence of noise terms {ˆ αθi− ¯ αθi} is expected<br />to average out to zero (i.e. statistically, positive increments<br />are compensated by negative increments) and we expect the<br />trajectory of (22) to oscillate about the trajectory of (21),<br />with the oscillations vanishing as i → ∞. This is the main<br />idea at the core of the systematic analysis of such recursions<br />which, as illustrated below, has an interest even for practi-<br />tioners. Indeed, by identifying the underlying deterministic<br />recursion which is approximated in practice, it allows one to<br />understand and predict the behaviour of algorithms, even in<br />situations where the recursion is heuristically designed and<br />the underlying criterion not explicit. Equation (20) suggests<br />that stationary points of the recursion should be such that<br />¯ αθ∗ = ¯ α∗. The stationary points of the alternative recursion<br />(19) are given in the next subsection.<br />In general most of the recursions of interest can be recast<br />as follows,<br />◦<br />?, the in-<br />θi+1= θi+γi+1Hi+1(θi,X0,...,Yi,Xi,Yi+1,Xi+1) (23)<br />where Hi+1(θ,X0,...,Yi,Xi,Yi+1,Xi+1) takes its values<br />in ?. Typically in practice {Hi+1} is a time invariant se-<br />quence of mappings which in effect only depends on a fixed<br />and finite number of arguments through time invariant sub-<br />sets of {Yi,Xi} (e.g. the last L of them at iteration i, as<br />above). For simplicity we will denote this mapping H and<br />include all the variables θi,X0,...,Yi,Xi,Yi+1,Xi+1as an<br />argument, although the dependence will effectively be on<br />a subgroup. Considering sequences {Hi(θ,X0,...,Yi,Xi,<br />Yi+1,Xi+1)} with a varying numbers of arguments is possi-<br />ble (and needed when trying to optimise (12) directly), but<br />at the expense of additional notation and assumptions.<br />4.2.2 Why bother with stochastic approximation?<br />In this subsection we point to numerous reasons why the<br />standard framework of stochastic approximation can be use-<br />ful in order to think about controlled MCMC algorithms:<br />as we shall see motivations range from theoretical to practi-<br />cal or implementational, and might help shed some lights on<br />possibly heuristically developed strategies. Again, although<br />this framework is very useful and allows for a systematic ap-<br />proach to the development and understanding of controlled<br />MCMC algorithms, and despite the fact that this framework<br />encompasses most known procedures, it should however not<br />prevent us from thinking differently.<br />A standardized framework for programming and analysis<br />Apart from the fact that the standard form (23) allows for<br />systematic ways of coding the recursions, in particular the<br />creationof“objects”,theapproachallowsforanunderstand-<br />ing of the expected behaviour of the recursion using sim-<br />ple mathematical arguments as well as the development of a<br />wealthofveryusefulvariations,madepossiblebytheunder-<br />standing of the fundamental underlying nature of the recur-<br />sions. As suggested above with a simple example (21)–(22)<br />the recursion (23) can always be rewritten as<br />θi+1= θi+γi+1h(θi)+γi+1ξi+1,<br />where h(θ) is the expectation in steady state for a fixed θ ∈<br />? of H(θ,X0,...,Yi,Xi,Yi+1,Xi+1), i.e.<br />h(θ) := Eθ(H (θ,X0,...,Yi,Xi,Yi+1,Xi+1))<br />and ξi+1:= H(θi,X0,...,Yi,Xi,Yi+1,Xi+1) − h(θi) is<br />usually referred to as the “noise”. The recursion (24) can<br />therefore be thought of as being a noisy gradient algorithm.<br />Intuitively, if we rearrange the terms in (24)<br />(24)<br />θi+1−θi<br />γi+1<br />we understand that provided that the noise increments ξi<br />“cancel out on average”, then a properly rescaled continu-<br />ous interpolation of the recursion θ0,θ1,... should behave<br />more or less like the solutions θ(t) of the ordinary differen-<br />tial equation<br />= h(θi)+ξi+1,<br />˙θ (t) = h(θ (t)),<br />whose stationary points are precisely such that h(θ) = 0.<br />The general theory of stochastic approximation consists of<br />establishing that the stationary points of (24) are related<br />to the stationary points of (25) and that convergence oc-<br />curs provided that some conditions concerning {γi},h(θ)<br />and {ξi} are satisfied. While this general theory is rather in-<br />volved, it nevertheless provides us with a useful recipe to<br />(25)</p>  <p>Page 13</p> <p>Stat Comput (2008) 18: 343–373 355<br />try to predict and understand some heuristically developed<br />algorithms. For example it is not clear what criterion is actu-<br />ally optimised when using the updating rule (19). However<br />the “mean field” approach described above can be used to<br />compute<br />h(θ) = Eθ(H (θ,X0,...,Yi,Xi,Yi+1,Xi+1))<br />= Eθ?I?ˆ αθ− ¯ α∗&gt; 0?−I?ˆ αθ− ¯ α∗≤ 0??<br />= Pθ?ˆ αθ− ¯ α∗&gt; 0?−Pθ?ˆ αθ− ¯ α∗≤ 0?.<br />Its zeros (the possible stationary points of the recursion) are<br />such that Pθ(ˆ αθ− ¯ α∗&gt; 0) = Pθ(ˆ αθ− ¯ α∗≤ 0) = 1/2, i.e.<br />the stationary points θ∗are such that ¯ α∗is the median of the<br />distribution of ˆ αθ in steady-state, which seems reasonable<br />when this median is not too different from ¯ αθ∗ given our ini-<br />tial objective. In addition this straightforward analysis also<br />tells us that the algorithm will have the desired gradient like<br />behaviour when Pθ(ˆ αθ− ¯ α∗&gt; 0) is a non-increasing func-<br />tion of θ. Other examples of the usefulness of the framework<br />to design and understand such recursions are given later in<br />Sect. 5, in particular Sect. 5.2.2.<br />In addition to allowing for an easy characterisation of<br />possible stationary points of the recursion (and hence of the<br />“ideal” optimal values θ∗) the decomposition (24) points to<br />the role played by the deterministic quantity h(θ) to ensure<br />that the sequence {θi} actually drifts towards optimal values<br />θ∗, which is the least one can ask from such a recursion,<br />and the fact that the noise sequence {ξi} should also aver-<br />age out to zero for convergence purposes. This latter point<br />is in general very much related to the ergodicity properties<br />of {Xi}, which justifies the study of ergodicity even in situ-<br />ations where it is only planned to use the optimised MCMC<br />algorithm with a fixed and suboptimal parameter˜θ obtained<br />after optimisation. This in turn points to the intrinsic dif-<br />ficulty of ensuring and proving such ergodicity properties<br />before {θi} wanders towards “bad” values, as explained in<br />Sect. 2. Recent progress in Andrieu and Tadi´ c (2007), rely-<br />ing on precise estimates of the dependence in θ of standard<br />drift functions for the analysis of Markov chains allows one<br />to establish that {θi} stays away from such “bad” values, en-<br />suring in turn ergodicity and a drift of {θi} towards the set of<br />valuesofinterest θ∗.SimilarresultsareobtainedinSaksman<br />and Vihola (2008), albeit using totally different techniques.<br />Finally note that the developments above stay valid in the<br />situation where {γi} is set to a constant, say γ. In such sit-<br />uations it is possible to study the distribution of θiaround<br />a deterministic trajectory underlying the ordinary differen-<br />tial equation, but it should be pointed out that in such sit-<br />uations {Xi} is not π-stationary, and one can at most hope<br />for πγ-stationarity for a probability distribution πγsuch that<br />πγ→ π in a certain sense as γ → 0.<br />The connection between stochastic approximation and<br />the work of Haario et al. (2001) and the underlying gener-<br />ality was realised in Andrieu and Robert (2001), although<br />it is mentioned in particular cases in Geyer and Thomp-<br />son (1995) and Ramponi (1998), the latter reference being<br />probably the first rigourous analysis of the stability and con-<br />vergence properties of a particular implementation of con-<br />trolled MCMC for tempering type algorithms.<br />A principled stopping rule<br />though ergodicity is intrinsically related to the sequence {θi}<br />approaching the zeroes of h(θ) and hence taking “good val-<br />ues”, one might be more confident in using samples pro-<br />duced by a standard MCMC algorithm that would use an<br />optimal or suboptimal value of θ. This naturally raises the<br />question of the stopping rule to be used. In the ubiquitous<br />case of the Robbins-Monro updating rule, and given the<br />clear interpretation in terms of the root finding of h(θ), one<br />can suggest monitoring the average of the field<br />As pointed out earlier, and al-<br />1<br />n<br />n<br />?<br />i=1<br />H(θi,Xi+1)<br />and stop, for example, when its magnitude is less than a pre-<br />set threshold ? for a number m of consecutive iterations.<br />More principled statistical rules relying on the CLT can also<br />be suggested, but we do not expand on this here.<br />Boundedness and convergence<br />godicity properties of Pθ can lead to some difficulties in<br />practice. Indeed these ergodicity properties are rarely uni-<br />form in θ ∈ ? and tend to degrade substantially for some<br />values, typically on the boundary ∂? of ?. For example<br />for the toy example of Sect. 2, both values ∂? = {0,1}<br />are problematic. For θ = 0 aperiodicity is lost whereas<br />for θ = 1 irreducibility is lost. This can result in impor-<br />tant problems in practice since π-ergodicity can be lost as<br />pointed out in Sect. 2 through the aforementioned toy ex-<br />ample when the sequence {θi} converges to ∂? too quickly.<br />In fact, as pointed out to us by Y.F. Atchadé, an example<br />in Winkler (2003) shows that even in the situation where<br />θi(1) = θi(2) = 1− 1/i, the sequence {n−1?n<br />problem of possible loss of ergodicity of Pθand its implica-<br />tions for controlled Markov chains has long been identified,<br />but is often ignored in the current MCMC related literature.<br />For example a normal symmetric random walk Metropolis<br />(N-SRWM) algorithm loses ergodicity as its variance (or co-<br />variance matrix) becomes either too large or too small and<br />an algorithm with poor ergodicity properties does not learn<br />features of the target distribution π. In the case of a random<br />scan MH within Gibbs algorithm as given in (2), it is pos-<br />sible to progressively lose irreducibility whenever a weight<br />drifts towards 0. Several cures are possible. The first and ob-<br />vious one consists of truncating ? in order to ensure the ex-<br />istence of some uniform ergodicity properties of the family<br />The dependence of the er-<br />i=1Xi− 3/2}<br />does not vanish (in the mean square sense) as i → ∞. This</p>  <p>Page 14</p> <p>356 Stat Comput (2008) 18: 343–373<br />of transitions {Pθ}. While this presumes that one knows by<br />how much one can truncate ? without affecting the ergod-<br />icity properties of {Pθ} significantly, this is not a completely<br />satisfactory solution since stability is actually observed in<br />numerous situations.<br />In Andrieu and Tadi´ c (2007), using explicit dependence<br />of the parameters of well known drift conditions for MCMC<br />algorithms on the tuning parameter θ, general conditions<br />on the transition probability Pθ and the updating function<br />H(θ,x) that ensure boundedness of {θi} are derived. As a<br />result π-ergodicity of {Xi}. and convergence to optimal or<br />suboptimal values of θ are automatically satisfied without<br />the need ro resort to fixed or adaptive truncations for ex-<br />ample One aspect of interest of the results is that they sug-<br />gest some ways of designing fully adaptive and stable algo-<br />rithms.<br />For example by noting that the zeroes of h(θ) are also the<br />zeroes of h(θ)/(1 + |θ|α) for example, one can modify the<br />standard recursion in order to stabilise the update, resulting<br />in the alternative updating rule<br />θi+1= θi+γi+1H(θi,Xi+1)/(1+|θi|α).<br />One can also add regularisation terms to the recursion.<br />For example, assuming for example that we learn optimal<br />weights for a mixture of transition probabilities as in (2), the<br />recursion<br />wk<br />i+1= wk<br />(with wi= (w1<br />to<br />i+γi+1Hk(wi,Xi+1)<br />i,w2<br />i,...,wn<br />i)) can be for example modified<br />wk<br />i+1= wk<br />i+γi+1Hk(wi,Xi+1)<br />?<br />+γ1+λ<br />i+1<br />α +(wk<br />?n<br />i)−β<br />j=1α +(wj<br />i)−β−wk<br />i<br />?<br />for some α,β,λ &gt; 0. Note that since the sum over k of the<br />fields is 0, the weights still sum to 1 after the update and<br />also that due to the boundedness of the additional term it<br />vanishes as i → ∞. Finally in Andrieu et al. (2005) and An-<br />drieu and Moulines (2006), following Chen et al. (1988), an<br />algorithm with adaptive truncation boundaries is suggested<br />and a general theory developedthat ensures that both bound-<br />edness and convergence of {θi} is ensured. Although requir-<br />ing an intricate theory, the conditions under which bounded-<br />nessandconvergenceholdcoveravastnumberofsituations,<br />beyond the situations treated in Andrieu and Tadi´ c (2007).<br />In Saksman and Vihola (2008) a different approach to prove<br />stability is used, and consists of proving that provided that<br />{θi} does not drift too fast to bad values, then the algorithm<br />preserves ergodicity. In fact the analysis performed by the<br />authorscanbedirectlyusedtostudythegeneralstabilisation<br />strategy of Andradóttir (1995) (see also reference therein)<br />for stochastic approximation.<br />Finally, under more restrictive conditions, detailed in<br />Benveniste et al. (1990) and Andrieu and Atchadé (2007,<br />Theorem 3.1), which include the uniqueness of θ∗such that<br />h(θ∗) = 0 and conditions (8)–(9) for θ ∈ K ⊂ ?, it is pos-<br />sible to show that for a deterministic sequence {γi}, there<br />exists a finite constant C such that for all i ≥ 1,<br />ˇE∗<br />?<br />|θi−θ∗|2I{σ ≥ i}<br />?<br />≤ Cγi,<br />where σ is the first exit time from K, meaning that while<br />θi remains in K (where locally uniform conditions of the<br />type (8)–(9) hold), then the rate of convergence towards θ∗<br />is given by {γi}.<br />Automatic choice of the stepsizes<br />imation procedure requires the choice of a stepsize se-<br />quence {γi}. A standard choice consists of choosing a deter-<br />ministic sequence satisfying?∞<br />that any point of ? can eventually be reached, while the sec-<br />ond condition ensures that the noise is contained and does<br />not prevent convergence. Such conditions are satisfied by<br />sequences of the type γi= C/iαfor α ∈ ((1+λ)−1,1]. We<br />tend in practice to favour values closer to the lower bound<br />in order to increase convergence of the algorithm towards a<br />neighbourhood of θ∗. This is at the expense of an increased<br />variance of {θi} around θ∗however.<br />A very attractive approach which can be useful in prac-<br />tice, and for which some theory is available, consists of<br />adapting {γi} in light of the current realisation of the<br />algorithm—this proves very useful in some situations see<br />Andrieu and Jasra (2008). The technique was first described<br />in Kesten (1958) and relies on the remark that, for exam-<br />ple, an alternating sign for {ˆ αθi− ¯ α∗} in (22) is an indica-<br />tion that {θi} is oscillating around (a) solution(s), whereas<br />a constant sign suggests that {θi} is, roughly speaking,<br />still far from the solution(s). In the former case the step-<br />size should be decreased, whereas in the later it should, at<br />least, be kept constant. More precisely consider a function<br />γ : [0,+∞) → [0,+∞).Thestandardscenariocorrespond-<br />ing to a predetermined deterministic schedule consists of<br />taking {γi= γ(i)}. The strategy suggested by Kesten (1958)<br />and further generalised to the multivariate case in Delyon<br />and Juditsky (1993) suggests to consider for i ≥ 2 the fol-<br />lowing sequence of stepsizes<br />The stochastic approx-<br />i=1γi= ∞ and?∞<br />i=1γ1+λ<br />i<br />&lt; ∞ for some λ &gt; 0.The former conditionsomehowensure<br />γi= γ<br />?i−1<br />k=1<br />?<br />I{?H(θk−1,Xk),H(θk,Xk+1)? ≤ 0}<br />?<br />where ?u,v? is the inner product between vector u and v.<br />Numerous generalisations are possible in order to take into</p>  <p>Page 15</p> <p>Stat Comput (2008) 18: 343–373 357<br />account the magnitudes of {H(θi,Xi+1)} in the choice of<br />{γi} (Plakhov and Cruz 2004) (and references therein),<br />?i−1<br />k=1<br />for some function φ : R → [0,+∞). Numerous generalisa-<br />tions of these ideas are naturally possible and we have found<br />that in numerous situations a componentwise choice of step-<br />sizecanleadtomajoracceleration(AndrieuandJasra2008),<br />i.e. consider for example for j = 1,...,nθ<br />?i−1<br />k=1<br />where Hj(θ,X) is the j-th component of H(θ,X), but care<br />must be taken to ensure that important properties of θ (such<br />as positivity if it is a covariance matrix) are preserved. Fi-<br />nally note that this idea needs to be handled with care in the<br />unlikely situations where (here in the scalar case for sim-<br />plicity) h(θ) ≥ 0 as well as H(θ,x) for all θ,x ∈ ?×X and<br />the solution to our problem is on the boundary of ?.<br />γi= γ<br />?<br />φ(?H(θk−1,Xk),H(θk,Xk+1)?)<br />?<br />γj<br />i= γ<br />?<br />I??Hj(θk−1,Xk),Hj(θk,Xk+1)?≤ 0?<br />?<br />4.2.3 Some variations<br />The class of algorithms considered earlier essentially rely<br />on an underlying time homogeneous Markov chain Monte<br />Carlo algorithm with target distribution π. It is however<br />possible to consider non-homogeneous versions of the al-<br />gorithms developed above. More precisely one can suggest<br />defining a sequence {πi,i ≥ 1} of probability distributions<br />on X such that πi→ π in some sense, e.g. total variation<br />distance, and select associated MCMC transition probabil-<br />ities {Pi,θ} such that for any i ≥ 1 and θ ∈ ? πiPi,θ= πi.<br />Then the controlled MCMC algorithm defined earlier can<br />use Pi+1,θiat iteration i+1 instead of Pθi. This opens up the<br />possibility for example to use tempering ideas, i.e. choose<br />πi(x) ∝ πβi(x) for βi∈ (0,1), allowing for the accumula-<br />tion of useful information concerning the distribution of in-<br />terest π, while exploring “simpler” distributions. This type<br />of strategy can be useful in order to explore multimodal dis-<br />tributions.<br />Another possibility, particularly suitable to two stage<br />strategies where adaptation is stopped, consists of remov-<br />ing the vanishing character of adaptation. In the context of<br />stochastic approximation this means for example that the se-<br />quence {γi} can be set to a constant small value γ. As a re-<br />sult, in light of the examples of the first section, one expects<br />that under some stability assumptions the chain {Xi} will<br />produce samples asymptotically distributed according to an<br />approximation πγ of π (such that πγ→ π in some sense)<br />and optimise an approximate criterion corresponding to the<br />standard criterion where π is replaced by πγ. This strategy<br />can offer some robustness properties.<br />5 Some adaptive MCMC procedures<br />In this section we present combinations of strategies, some<br />of them original,1which build on the principles developed<br />in previous sections. Note that in order to keep notation sim-<br />ple and ensure readability we present here the simplest ver-<br />sions of the algorithms but that additional features described<br />in Sect. 4.2.2, such as the modification of the mean field to<br />favour stability, the automatic choice of the stepsize (com-<br />ponentwise or not) or Rao-Blackwellisation etc., can easily<br />be incorporated.<br />5.1 Compound criteria, transient and starting to learn<br />As pointed out earlier desirable asymptotic criteria and asso-<br />ciated optimisation procedures can easily be defined. How-<br />ever it can be observed in practice that the algorithm can<br />be slow to adapt, in particular in situations where the ini-<br />tial guess of the parameter θ is particularly bad, resulting<br />for example in a large rejection probability. More generally<br />the MH algorithm has this particular rather negative char-<br />acteristic that if not well tuned it will not explore the target<br />distribution and hence will be unable to gather information<br />about it, resulting in a poor learning of the target distribu-<br />tion, and hence algorithms that adapt and behave badly. We<br />describe in this section some strategies that circumvent this<br />problem in practice.<br />We focus here on the symmetric increments random-<br />walk MH algorithm (hereafter SRWM), in which q(x,y) =<br />q(x −y) for some symmetric probability density q on Rnx,<br />referredtoastheincrementdistribution.Thetransitionprob-<br />ability of the Metropolis algorithm is then given for x,A ∈<br />X×B(X) by<br />PSRWM<br />q<br />(x,A)<br />?<br />?<br />x ∈ X,A ∈ B(X),<br />where α(x,y) := 1 ∧ π(y)/π(x). A classical choice for the<br />proposal distribution is<br />q(z) = N(z;0,?),<br />N(z;μ,?) is the density of a multivariate Gaussian with<br />mean μ and covariance matrix ?. We will later on refer to<br />this algorithm as the N-SRWM. It is well known that ei-<br />ther too small or too large a covariance matrix will result<br />in highly positively correlated Markov chains, and therefore<br />estimatorsˆI?<br />=<br />A−x<br />+I(x ∈ A)<br />α(x,x +z)q(z)dz<br />X−x<br />(1−α(x,x +z))q(z)dz,<br />(26)<br />where<br />n(f) with a large variance. In Gelman et al.<br />1First presented at the workshop Adapski’08, 6–8 January 2008,<br />Bormio, Italy.</p>  <p>Page 16</p> <p>358Stat Comput (2008) 18: 343–373<br />(1995) it is shown that the “optimal” covariance matrix (un-<br />der restrictive technical conditions not given here) for the N-<br />SRWM is (2.382/nx)?π, where ?π is the true covariance<br />matrix of the target distribution. In Haario et al. (2001) (see<br />also Haario et al. 1999) the authors have proposed to “learn<br />?π on the fly”, whenever this quantity exists. It should be<br />pointed out here that in situations where this quantity is not<br />well defined, one should resort to “robust” type estimates in<br />order to capture the dependence structure of the target distri-<br />bution; we do not consider this here. Denoting PSRWM<br />transition probability of the N-SRWM with proposal distri-<br />bution N(0,λ?) for some λ &gt; 0. With λ = 2.382/nx, the<br />algorithm in Haario et al. (2001) can be summarised as fol-<br />lows,<br />μi,?i<br />the<br />Algorithm 2 AM algorithm<br />• Initialise X0,μ0and ?0.<br />• At iteration i +1, given Xi,μiand ?i<br />1. Sample Xi+1∼ PSRWM<br />2. Update<br />μi,?i<br />(Xi,·).<br />μi+1= μi+γi+1(Xi+1−μi),<br />?i+1= ?i+γi+1((Xi+1−μi)(Xi+1−μi)T−?i).<br />(27)<br />This algorithm has been extensively studied in Andrieu<br />and Moulines (2006), Atchadé and Fort (2008), Bai et al.<br />(2008) and Andrieu and Tadi´ c (2007). We now detail some<br />simple improvements on this algorithm.<br />5.1.1 Rao-Blackwellisation and square root algorithms<br />Following (Ceperley et al. 1977) and (Frenkel 2006), we<br />note that, conditional upon the previous state Xiof the chain<br />and the proposed transition Yi+1, the vector f(Xi+1) (for<br />any function f : X → Rnf) can be expressed as<br />f(Xi+1) := I{Ui+1≤ α(Xi,Yi+1)}f(Yi+1)<br />+I{Ui+1&gt; α(Xi,Yi+1)}f(Xi),<br />(28)<br />where Ui+1∼ U(0,1). The expectation of f(Xi+1) with re-<br />spect to Ui+1conditional upon Xiand Yi+1leads to<br />f(Xi+1) := α(Xi,Yi+1)f(Yi+1)<br />+(1−α(Xi,Yi+1))f(Xi).<br />Forexample¯Xi+1:=α(Xi,Yi+1)Yi+1+(1−α(Xi,Yi+1))Xi<br />is the “average location” of state Xi+1which follows Xi<br />(29)<br />given Yi+1. This can be incorporated in the following “Rao-<br />Blackwellised” AM recursions<br />?α(Xi,Yi+1)(Yi+1−μi)<br />?α(Xi,Yi+1)(Yi+1−μi)(Yi+1−μi)T<br />+(1−α(Xi,Yi+1))(Xi−μi)(Xi−μi)T−?i<br />Using, for simplicity, the short notation (29) a Rao-Black-<br />wellised AM algorithm can be described as follows:<br />μi+1= μi+γi+1<br />+(1−α(Xi,Yi+1))(Xi−μi)?,<br />?i+1= ?i+γi+1<br />?.<br />Algorithm 3 Rao-Blackwellised AM algorithm<br />• Initialise X0,μ0and ?0.<br />• At iteration i +1, given Xi,μiand ?i<br />1. Sample Yi+1∼ N(Xi,?i) and set Xi+1= Yi+1with<br />probability α(Xi,Yi+1), otherwise Xi+1= Xi.<br />2. Update<br />μi+1= μi+γi+1(¯Xi+1−μi),<br />?i+1= ?i+γi+1[(Xi+1−μi)(Xi+1−μi)T−?i].<br />(30)<br />Note that it is not clear that this scheme is always advan-<br />tageous in terms of asymptotic variance of the estimators,<br />as shown in Delmas and Jourdain (2007), but this modifica-<br />tion of the algorithm might be beneficial during its transient<br />wheneverthe acceptanceprobabilityis not too low naturally.<br />It is worth pointing out that for computational efficiency<br />and stability one can directly update the Choleski decompo-<br />sition of ?i, using the classical rank 1 update formula<br />?1/2<br />i+1= (1−γi+1)1/2?1/2<br />?<br />i<br />+<br />1+<br />γi+1<br />1−γi+1??−1/2<br />??−1/2<br />i<br />(Xi+1−μi)?2<br />(Xi+1−μi)?2−1<br />i<br />×(1−γi+1)1/2(Xi+1−μi)(Xi+1−μi)T?−T/2<br />where AT/2is a shorthand notation for (A1/2)Twhenever<br />this quantity is well defined. This expression can be simpli-<br />fied through an expansion (requiring γi+1? 1) and modi-<br />fied to enforce a lower triangular form as follows<br />i<br />?1/2<br />i+1= ?1/2<br />i<br />+γi+1?1/2<br />?−1/2<br />i<br />i<br />L<br />×<br />?<br />(Xi+1−μi)(Xi+1−μi)T?−T/2<br />i<br />−I<br />?<br />,<br />where L(A) is the lower triangular part of matrix A. Note<br />again the familiar stochastic approximation form of the re-</p>  <p>Page 17</p> <p>Stat Comput (2008) 18: 343–373 359<br />cursion, whose mean field is<br />?<br />and whose zeros (together with those of the recursion on<br />the mean) are precisely any square root of ?π. The operator<br />ensures that the recursion is constrained to lower triangu-<br />lar matrices. Note that this is only required if one wishes to<br />save memory. Rank r updates can also be used when the co-<br />variance matrix is updated every r iterations only. In what<br />follows, whenever covariance matrices are updated, recur-<br />sions of this type can be used although we will not make<br />this explicit for notational simplicity.<br />L<br />?−1/2??π+(μ−μπ)(μ−μπ)T??−T/2−I<br />?<br />,<br />5.1.2 Compound criterion: global approach<br />As pointed out earlier, in the case of the N-SRWM algo-<br />rithm the scaling of the proposal distribution is well under-<br />stood in specific scenarios and intuitively meaningful for a<br />larger class of target distributions. A good rule of thumb is<br />to choose λ = (2.382/nx)?π, where ?π is the covariance<br />matrix of π. We have shown above that following (Haario<br />et al. 2001) one can in principle estimate ?πfrom the past<br />of the chain. However the difficulties that lead to the de-<br />sire to develop adaptive algorithms in the first place, includ-<br />ing the very poor exploration of the target distribution of<br />π, also hinder learning about the target distribution in the<br />initial stages of an adaptive MCMC algorithm when our ini-<br />tial value for the estimator of ?πis a poor guess. Again if<br />λ?i is either too large in some directions or too small in<br />all directions the algorithm has either a very small or a very<br />large acceptance probability, which results in a very slow<br />learning of ?πsince the exploration of the target’s support<br />is too localised. This is a fundamental problem in practice,<br />which has motivated the use of delayed rejection for exam-<br />ple (Haario et al. 2003), and for which we present here an<br />alternative solution which relies on the notion of composite<br />criterion.<br />While theory suggests a scaling of λ = 2.382/nxwe pro-<br />pose here to adapt this parameter in order to coerce the<br />acceptance probability to a preset and sensible value (e.g.<br />0.234), at least in the initial stages of the algorithm. Indeed,<br />while this adaptation is likely not to be useful in the long-<br />run, this proves very useful in the early stages of the algo-<br />rithm (we provide a detailed illustration in Sect. 6.3) where<br />the pathological behaviour described above can be detected<br />through monitoring of the acceptance probability, and cor-<br />rected.<br />As a consequence in what follows the proposal distri-<br />bution of the adaptive N-SRWM algorithm we consider<br />is qθ(z) = N(z;0,λ?) where here θ := (λ,μ,?). As-<br />suming that for any fixed covariance matrix ? the corre-<br />sponding expected acceptance probability ¯ αλ(see (14)) is<br />a non-increasing function of λ, one can naturally suggest<br />the recursion logλi+1= logλi+ γi+1[α(Xi,Yi+1) − ¯ α∗],<br />which following the discussion of Sect. 4 is nothing but a<br />standard Robbins-Monro recursion. Now when the covari-<br />ance matrix ?π needs to be estimated, one can suggest<br />the following “compound criterion” or “multicriteria” algo-<br />rithm:<br />Algorithm 4 AM algorithm with global adaptive scaling<br />• Initialise X0,μ0and ?0.<br />• At iteration i +1, given Xi,μi,?iand λi<br />1. Sample Yi+1∼ N(Xi,λi?i) and set Xi+1= Yi+1<br />with probability α(Xi,Yi+1), otherwise Xi+1= Xi.<br />2. Update<br />log(λi+1) = log(λi)+γi+1[α(Xi,Yi+1)− ¯ α∗],<br />μi+1= μi+γi+1(Xi+1−μi),<br />?i+1= ?i+γi+1[(Xi+1−μi)(Xi+1−μi)T−?i].<br />(31)<br />Again the interest of the algorithm is as follows: when-<br />ever our initial guess ?0is either two large or two small,<br />this will be reflected in either a large or small acceptance<br />probability, meaning that learning of ?πis likely to be slow<br />for a fixed scaling parameter. However this measure of per-<br />formance of the algorithm can be exploited as illustrated<br />above: if α(Xi,Yi+1) − ¯ α∗&lt; 0 for most transition attempts<br />then λi should be decreased, while if on the other hand<br />α(Xi,Yi+1) − ¯ α∗≥ 0 for most transition attempts, then λi<br />should be increased. As a result one might expect a more<br />rapid exploration of the target distribution following a poor<br />initialisation. Although this strategy can improve the perfor-<br />mance of the standard AM algorithm in practice, we show<br />in the next section that it is perfectible.<br />5.1.3 Compound criterion: local approach<br />As we shall now see, the global approach described in the<br />previous subsection might be improved further. There are<br />two reasons for this. First it should be clear that adjusting<br />the global scaling factor ignores the fact that the scaling of<br />λi?i might be correct in some directions, but incorrect in<br />others. In addition, in order to be efficient, such bold up-<br />dates require in general some good understanding of the de-<br />pendence structure of the target distribution, in the form of<br />a reasonable estimate of ?π, which is not available in the<br />initial stages of the algorithm. These problems tend to be<br />amplified in scenarios involving a large dimension nxof the<br />space X since innocuous approximations in low dimensions<br />tend to accumulate in larger cases. Inspired by Haario et<br />al. (2005), we suggest the following componentwise update</p>  <p>Page 18</p> <p>360Stat Comput (2008) 18: 343–373<br />strategy which consists of a mixture of timid moves whose<br />role is to attempt simpler transitions better able to initiate<br />the exploration of π. Note, however, that in contrast with<br />(Haario et al. 2005) our algorithm uses the notion of com-<br />pound criterion, which in our experience significantly im-<br />proves performance. With ekthe vector with zeroes every-<br />where but for a 1 on its k-th row and a sensible ¯ α∗∗∈ (0,1)<br />e.g. 0.44:<br />Algorithm 5 Componentwise AM with componentwise<br />adaptive scaling<br />• Initialise X0,μ0,?0and λ1<br />• At iteration i +1, given μi,?iand λ1<br />1. Choose a component k ∼ U{1,...,nx}.<br />2. Sample Yi+1 ∼ Xi + ekN(0,λk<br />Xi+1= Yi+1 with probability α(Xi,Yi+1), otherwise<br />Xi+1= Xi.<br />3. Update<br />0,...,λnx<br />0.<br />i,...,λnx<br />i<br />i[?i]k,k) and set<br />log(λk<br />i+1) = log(λk<br />μi+1= μi+γi+1(Xi+1−μi),<br />?i+1= ?i+γi+1[(Xi+1−μi)(Xi+1−μi)T−?i]<br />and λj<br />ifor j ?= k.<br />i)+γi+1[α(Xi,Yi+1)− ¯ α∗∗],<br />(32)<br />i+1= λj<br />One might question the apparently redundant use of both<br />a scaling λk<br />iand the marginal variance [?i]k,kin the pro-<br />posal distributions above, and one might choose to com-<br />bine both quantities into a single scaling factor. However the<br />present formulation allows for a natural combination (i.e. a<br />mixture or composition) of the recursion above and varia-<br />tions of the standard AM algorithm (Algorithm 2) such as<br />Algorithm 4. Such combinations allow one to circumvent<br />the shortcomings of bold moves, which require extensive<br />understanding of the structure of π, in the early iterations of<br />the algorithm. The timid moves allow the procedure to start<br />gathering information about π which might then be used by<br />more sophisticated and more global updates.<br />We now turn to yet another version of the AM algorithm<br />(Algorithm 2) which can be understood as being a version<br />of Algorithm 4 which exploits the local scalings computed<br />by Algorithm 5 instead of a single global scaling factor. It<br />consists of replacing the proposal distribution N(Xi,λi?i)<br />in Algorithm 4 with N(Xi, ?1/2<br />?<br />As we now show, such an update can be combined with Al-<br />gorithm 5 into a single update. For a vector V we will de-<br />note V(k) its k-th component and ekthe vector with zeroes<br />everywhere but for a 1 on its k-th row. We have,<br />i<br />?i?1/2<br />i<br />), where<br />?i:= diag<br />λ1<br />i,...,λnx<br />i<br />?<br />.<br />Algorithm6GlobalAMwithcomponentwiseadaptivescal-<br />ing<br />• Initialise X0,μi,?iand λ1<br />• Iteration i +1<br />1. Given μi,?i<br />N(0,?1/2<br />probability α(Xi,Xi+Zi+1), otherwise Xi+1= Xi.<br />2. Update for k = 1,...,nx<br />log(λk<br />i)+γi+1[α(Xi,Xi+Zi+1(k)ek)<br />− ¯ α∗∗],<br />μi+1= μi+γi+1(Xi+1−μi),<br />?i+1= ?i+γi+1[(Xi+1−μi)(Xi+1−μi)T−?i].<br />i,...,λnx<br />i.<br />and λ1<br />) and set Xi+1= Xi+ Zi+1 with<br />i,...,λnx<br />i, sample Zi+1 ∼<br />i<br />?i?1/2<br />i<br />i+1) = log(λk<br />(33)<br />It is naturally possible to include an update for a global<br />scaling parameter, but we do not pursue this here. This al-<br />gorithm exploits the fact that a proposed sample Xi+ Zi+1<br />provides us with information about scalings in various di-<br />rections through the “virtual” componentwise updates with<br />increments {Zi+1(k)ek} and their corresponding directional<br />acceptance probabilities. This strategy naturally requires<br />nx+ 1 evaluations of π, which is equivalent to one update<br />according to Algorithm 4 and nxupdates according to Al-<br />gorithm 5.<br />5.2 Fitting mixtures, clustering and localisation<br />As pointed out in Andrieu and Moulines (2006, Sect. 7) the<br />moment matching criterion corresponding to the recursion<br />(27) can be understood as minimising the Kullback-Leibler<br />divergence<br />?<br />where ˘ qθ(x) = N(x;μ,?) (but using qθ(z) = N(z;0,λ?)<br />as a proposal distribution for the increments of a N-SRWM<br />update). This remark leads to the following considerations,<br />of varying importance.<br />The first remark is that ˘ qθcould be used as the proposal<br />distribution of an independent MH (IMH) update, as in An-<br />drieu and Moulines (2006) or Giordani and Kohn (2006).<br />Although this might be a sensible choice when ˘ qθ(x) is a<br />good approximation of π, this might fail when θ is not close<br />to θ∗(in the transient for example) or simply because the<br />chosen parametric form is not sufficiently rich. In addition<br />such a bad behaviour is generally exacerbated by large di-<br />mensions as illustrated by the following toy example.<br />KLθ(π,qθ) := Eπ<br />logπ(X)<br />˘ qθ(X)<br />?<br />(34)<br />Example 1 The target distribution is π(x) = N(x;0,I)<br />with x ∈ Rnxand proposal distribution q(x) = N(x;ε ×</p>  <p>Page 19</p> <p>Stat Comput (2008) 18: 343–373 361<br />e,I) for some ε &gt; 0 with e = (1,1,1,...)T. The importance<br />sampling weight entering the acceptance ratio of an IMH<br />algorithm is<br />?1<br />?<br />π(x)<br />q(x)= exp<br />2ε2nx−εeTx<br />?<br />= exp<br />−1<br />2ε2nx−εn1/2<br />x n−1/2<br />x<br />nx<br />?<br />i=1<br />(x(i)−ε)<br />?<br />,<br />which is not bounded, hence preventing geometric ergod-<br />icity. The distribution of n−1/2<br />x<br />N(0,1), which results in a variance for the weights of<br />?<br />This is known to result in poorly performing importance<br />sampling algorithms, but will also have an impact on the<br />convergence of IMH algorithms which will get stuck in<br />states x with arbitrarily large weights x as nxincreases, with<br />non negligible probability.<br />?nx<br />i=1(x(i) − ε) is precisely<br />exp<br />ε2nx<br />?<br />−1.<br />IMH updates hence fall in the category of “very bold”<br />updates which require significant knowledge of the structure<br />of π and do not usually form the base for reliable adaptive<br />MCMC algorithms.<br />The second remark, which turns out to be of more inter-<br />est, is that one can consider other parametric forms for ˘ qθ,<br />and use such approximations of π to design proposal distri-<br />butions for random walk type algorithms, which are likely<br />to perform better given their robustness. It is suggested in<br />Andrieu and Moulines (2006, Sect. 7) to consider mixtures,<br />finite or infinite, of distributions belonging to the exponen-<br />tialfamily(seealsoCappéetal.2007forasimilarideainthe<br />context of importance sampling/population Monte Carlo).<br />This has the advantage of leading to an elegant optimisa-<br />tion algorithm which relies on an on-line version of the EM<br />algorithm and results in a marginal additional computational<br />overhead.<br />In this section we first detail two particular cases of this<br />procedure: mixture of normal distributions and Student t-<br />distributions.<br />5.2.1 Updates for fitting mixtures in the exponential family<br />We first briefly review how, given samples {Xi}, it is possi-<br />ble to iteratively fit a mixture<br />˘ qθ(x) =<br />n<br />?<br />k=1<br />wkN(x;μk,?k),<br />(35)<br />with θ = (w,μ,?) with w = (w1,w2,...,wn), in order<br />to minimise (34). For the purpose of describing the al-<br />gorithm it is convenient to introduce the missing data z<br />such that ˘ qθ(x,z = k) := wkN(x;μk,?k) and hence for<br />k ∈ {1,...,n}<br />˘ qθ(k|x) =wkN(x;μk,?k)<br />˘ qθ(x)<br />wkN(x;μk,?k)<br />?n<br />Nowforany k ∈ {1,...,n} and i ≥ 0 therecursionsare,with<br />˘ qθi(Zi+1= k|x) :=wk<br />˘ qθi(x)<br />=<br />l=1wlN(x;μl,?l).<br />iN(x;μk<br />i,?k<br />i)<br />,<br />μk<br />i+1= μk<br />?k<br />i+γi+1˘ qθi(Zi+1= k|Xi+1)(Xi+1−μk<br />i+1=?k<br />×[(Xi+1−μk<br />wk<br />i),<br />i+γi+1˘ qθi(Zi+1= k|Xi+1)<br />i)(Xi+1−μk<br />i+1= wk<br />Note that the standard EM framework suggests various ac-<br />celeration techniques, which we do not consider here for<br />brevity.<br />It is also possible to consider a mixture of multivariate<br />Student-tdistributions,whichisamixedcontinuous/discrete<br />mixture of normals. More precisely consider the case where<br />i)T−?k<br />i],<br />i).<br />(36)<br />i+γi+1(˘ qθi(Zi+1= k|Xi+1)−wk<br />˘ qθ(x) =<br />n<br />?<br />k=1<br />wkTν(x;μk,?k)<br />where<br />Tν(x;μ,?)<br />=<br />?(ν+nx<br />2<br />) |?|−1/2<br />(πν)<br />1<br />2nx?(ν<br />2)(1+1<br />ν(x −μ)T?−1(x −μ))<br />1<br />2(ν+nx).<br />We consider here for simplicity the case “one ν for all” since<br />we are not interested in a very precise fit of the target distri-<br />bution. Note that as ν → ∞ the mixture converges to a mix-<br />ture of normal distributions which coincides with that de-<br />scribed above. The on-line EM algorithm relies on the stan-<br />dard fact that ˘ qθ(x) can be seen as the marginal distribution<br />of<br />?−u<br />×(ν/2)ν/2<br />˘ qθ(k,u,x) =<br />wkunx<br />√|2π?k|exp<br />?(ν/2)uν/2−1exp<br />2(x −μk)T?−1<br />?<br />k(x −μk)<br />?<br />−ν<br />2u<br />?<br />I{u ≥ 0}.<br />We denote<br />˘ qθi(Zi+1= k|Xi+1) :=wk<br />iTν(x;μk<br />˘ qθi(x)<br />i,?k<br />i)</p>  <p>Page 20</p> <p>362Stat Comput (2008) 18: 343–373<br />and introduce the conditional expectation of U given X<br />and Z<br />¯ u(k,X) := Eθ[U|k,X] =<br />ν +nx<br />ν +(X −μk)T?−1<br />k(X −μk).<br />The required recursions are<br />μk<br />i+1= μk<br />i+γi+1¯ u(k,Xi+1)˘ qθi(Zi+1= k|Xi+1)<br />×(Xi+1−μk<br />i+1=?k<br />×[(Xi+1−μk<br />wk<br />?<br />This later choice is closely related to the “fast K-mean” al-<br />gorithm used in Giordani and Kohn (2006) (although the<br />algorithm developed there is not on-line, whereas the al-<br />gorithm developed here is computationally very efficient)<br />which is beneficial in the initial stages of the algorithm in<br />order to start the learning process. In practice we suggest<br />thatwhenfittinga mixtureof normaldistributions,therecur-<br />sions for the Studentt-distributions be used witha parameter<br />νi→ ∞ with the iterations.<br />i),<br />?k<br />i+γi+1¯ u(k,Xi+1)˘ qθi(Zi+1= k|Xi+1)<br />i)(Xi+1−μk<br />i+1= wk<br />¯ uk<br />i)T−?k<br />i],<br />i),<br />i+γi+1(˘ qθi(Zi+1= k|Xi+1)−wk<br />i+γi+1<br />i+1= ¯ uk<br />¯ u(k,Xi+1)˘ qθi(Zi+1= k|Xi+1)− ¯ uk<br />i<br />?<br />.<br />5.2.2 Localised random walk Metropolis updates<br />The Metropolis-Hastings algorithm in its simplest form of-<br />fers the possibility for local adaptation given the possible<br />dependence of its family of proposal distributions {q(x,·),<br />x ∈ X} on the current state of the Markov chain. Obvious<br />examples include the Langevin algorithm or self-targeting<br />schemes (Stramer and Tweedie 1999). This dependence is<br />exploited further in Green (1995) where the weights of a<br />mixture of MH updates are allowed to depend on the current<br />state x of the Markov chain, hence offering the possibility to<br />select a particular update depending on the region currently<br />visited by, say, state Xi= x.<br />We now describe an original use of the information about<br />π contained in the approximation ˘ qθ(x) of π which al-<br />lows for some localisation of the adaptation in the spirit<br />of a suggestion in Andrieu and Robert (2001) concerned<br />with Voronoi tesselations (for which the Linde-Buzo-Gray,<br />an EM like algorithm, could be used here). We however re-<br />strict here the presentation to that of an algorithm for which<br />˘ qθ(x) is a mixture of normal distributions—other cases are<br />straightforward extensions. Note that another form of locali-<br />sations has been suggested in Roberts and Rosenthal (2006),<br />which is more in line with the ideas of Stramer and Tweedie<br />(1999), and can lead to interesting algorithms.<br />The algorithm we suggest here is a mixture of N-SRWM<br />algorithms—one should bear in mind that such an algorithm<br />will in general be a component of a much larger mixture<br />or part of a composition of updates. The interest of our ap-<br />proach is that following (Green 1995) we allow the weights<br />of the mixture to depend on the current state of the chain.<br />More precisely one can suggest for example using<br />Pθ(x,dy) =<br />n<br />?<br />k=1<br />˘ qθ(k|x)PNSRWM<br />θ,k<br />(x,dy)<br />where<br />PNSRW<br />θ,k<br />proposal distribution, here in the normal case, N(y;x,<br />λk?k). Note that other choices than the weights ˘ qθ(k|x)<br />can be chosen in order to ensure, in particular in the early<br />stages of the algorithm, that all components are being used.<br />In the case of a mixture of normals one can for example<br />suggest using the conditional distribution ˘ qθ(k|x) for a mix-<br />ture of Student t-distributions with a parameter νi→ ∞<br />as i → ∞. The choice of λkis made adaptive in order to<br />achieve a preset acceptance probability, according to (31).<br />The motivations for this algorithm are twofold: (a) first the<br />weight ˘ qθ(k|x),orafunctionofthisquantity,favoursassoci-<br />ation of x to relevant components of the mixture of distribu-<br />tions ˘ qθ(x), that is for example the local linear dependencies<br />present among the components of x through the covariance<br />matrices ?1:n(b) secondly ˘ qθ(x) can be used in order to<br />cluster states of X and associate local criteria to each clus-<br />ter (here a local expected acceptance probability but other<br />choices are possible) which in turn can be locally adapted<br />using a rule of our choice. Note the advantage of this al-<br />gorithm in terms of innovation (or exploration in machine<br />learning speak) over a simple IMH algorithm that would try<br />to sample and learn from its own samples.<br />The algorithm can be summarised with the following<br />pseudo-code in the case where a mixture of normal distri-<br />butions is used in order to map the state-space.<br />θ = (μ1:n,?1:n,w1:k,λ1:k)<br />(x,dy) is a random walk Metropolis algorithm with<br />and the transition<br />Algorithm 7 Localised N-SRWM algorithm<br />• Initialise X0,μ1:n<br />• Iteration i +1, given Xi,μ1:n<br />1. Zi+1∼ ˘ qθi(Z = k|Xi), Yi+1∼ N(Xi,λZi+1<br />andset<br />Xi+1<br />min{1,π(Yi+1)˘ qθi(k|Yi+1)<br />2. Update μ1:n<br />i<br />and λ1:n<br />0,?1:n<br />0,w1:n<br />0<br />and λ1:n<br />i,?1:n<br />0.<br />,w1:n<br />ii<br />and λ1:n<br />i<br />i<br />?Zi+1<br />i<br />)<br />=<br />Yi+1<br />with probability<br />π(Xi)˘ qθi(k|Xi)}, otherwise Xi+1= Xi.<br />i,?1:n<br />i<br />i+1, according to (36) and (37).<br />,w1:n<br />and λ1:k<br />i<br />to μ1:n<br />i+1,?1:n<br />i+1,w1:n<br />i+1<br />The localised nature of the algorithm, in the spirit of the<br />state dependent mixtures of updates of Green (1995), re-<br />quires some care. Firstly note the form of the acceptance</p>  <p>Page 21</p> <p>Stat Comput (2008) 18: 343–373363<br />probability required in order to ensure that the underlying<br />“fixed θ” transition probability is in detailed balance with π<br />?<br />Secondly, updating of the parameters requires some atten-<br />tion. Indeed, given that component k is chosen, we wish<br />to adjust the conditional expected acceptance probability of<br />thiscomponentin orderto reach an expectedacceptancerate<br />¯ α∗.Inmathematicaltermswewishtosetthefollowingmean<br />field h(θ) with components hk(θ) to zero,<br />αk(x,y) := min1,π(y)˘ qθ(k|y)<br />π(x)˘ qθ(k|x)<br />?<br />.<br />hk(θ) :=<br />?<br />?<br />X2<br />π(x)˘ qθ(k|x)N(y;x,λk?k)<br />?<br />?<br />Xπ(x)˘ qθ(k|x)dx<br />αk(x,y)dxdy − ¯ α∗<br />=<br />X2π(x)˘ qθ(k|x)N(y;x,λk?k)(αk(x,y)− ¯ α∗)dxdy<br />Xπ(x)˘ qθ(k|x)dx<br />where the fraction on the first line is the density of the condi-<br />tionalsteadystatedistributionPθ(X ∼ π,Y ∼ N(X,λk?k)|<br />Z = k). Finding the zeros of hk(θ) hence amounts to finding<br />the zeros of the top of the last fraction, which can be written<br />as an expectation<br />,<br />n<br />?<br />m=1<br />?<br />X2π(x)˘ qθ(m|x)I{m = k}<br />×N(y;x,λk?k)(αk(x,y)− ¯ α∗)dxdy<br />n<br />?<br />×I{m = k}(αk(x,y)− ¯ α∗)dxdy.<br />=<br />m=1<br />?<br />X2π(x)˘ qθ(m|x)N(y;x,λm?m)<br />The second form of hk(θ) above (and since the denominator<br />does not (in general) affect the zeros of hk(θ)) suggests the<br />following recursions to update {λk<br />ponents’ running expected acceptance probabilities {αk<br />i} and compute the com-<br />i}<br />log(λk<br />i+1) =log(λk<br />i)+γi+1I{Zi+1= k}<br />×?αk(Xi,Yi+1)− ¯ α∗<br />i+γi+1I{Zi+1= k}<br />?,<br />αk<br />i+1= αk<br />?<br />αk(Xi,Yi+1)−αk<br />i<br />?<br />.<br />(37)<br />Naturally we do not address here the choice of the number<br />n of components of the mixture. Although the use of simple<br />information criteria could be advocated in order to choose n,<br />even in a crude way, we believe that although feasible this<br />might lead to additional complications at this stage. We here<br />simply argue that choosing n &gt; 1 should in general be ben-<br />eficial compared to the use of a plain N-SRWM for which<br />n = 1. Alternatively one can suggest the possibility of fitting<br />simultaneously several mixtures with each its own number<br />of components.<br />5.3 Block sampling and principal directions<br />For large dimensional problems, updating the whole vector<br />X in one block might lead to a poorly performing algorithm<br />whichfails to explorethedistribution π of interest.It is stan-<br />dard in practice to attempt to update subblocks of X con-<br />ditional upon the corresponding complementary subblock,<br />which in practice facilitates the design of better proposal<br />distributions. The choice of such subblocks is however in<br />practice crucial while far from obvious in numerous situa-<br />tions. Indeed it is well known and easy to understand that<br />variables that are highly dependent components of X (under<br />π) should in practice be updated simultaneously as it can<br />otherwise lead to algorithms that are slow to converge, and<br />produce samples with poor statistical properties. Identifying<br />such subblocks of dependent components can be very dif-<br />ficult in practice, and it is natural to ask if it is possible to<br />automatise this task in practice.<br />A possible suggestion is to consider an MCMC update<br />that takes the form of a mixture of MCMC updates<br />Pθ(x,dy) =<br />n<br />?<br />k=1<br />ωk(θ)Pk,θ(x,dy),<br />(38)<br />where for any θ ∈ ?,<br />{Pi,θ,i = 1,...,n} is a family of “partial” updates which<br />correspond to all the possible partitions of vector X. Then<br />one can suggest updating the weights {ωk(θ)} according to<br />some criterion. This is of course not realistic in practice as<br />soon as the dimension nxof X is even moderate, and can<br />lead to very poorly mixing algorithms since intuitively all<br />the possible transitions should be tried in order to assess<br />their efficiency. This might be inefficient as we expect only<br />a restricted number of these transitions to be of real interest.<br />Instead we suggest here a simple alternative which re-<br />lies on principal component analysis and a natural and well<br />known generalisation able to handle multi-modal distribu-<br />tions. Our algorithms rely on the recursive diagonalisation<br />of either the estimates {?i} of the covariance matrix ?πor<br />the covariance matrices {?k<br />mate the target distribution π, e.g. using a mixture of normal<br />or Student t-distributions. We will focus here on the former<br />scenario for simplicity, the extension to the mixture of dis-<br />tributions case is straightforward.<br />?n<br />k=1ωk(θ) = 1, ωk(θ) ≥ 0 and<br />i,k = 1,...,n} used to approxi-<br />5.3.1 Updates description<br />Formally this update is of the form (38) where Pk,θ(x,dy)<br />is a one-dimensional random walk Metropolis update along<br />eigenvector k of the covariance matrix ?π, with a scaling<br />factor ?(k) which ensures a predetermined acceptance prob-<br />ability. We will describe below the recursive estimation of<br />the first m (≤ nx) eigenvectors of ?π, which we assume</p>  <p>Page 22</p> <p>364Stat Comput (2008) 18: 343–373<br />formthecolumnsofan nx×m matrix W (thecolumnsbeing<br />denoted w(l), l = 1,...,m) and the corresponding eigenval-<br />ues ρ(l). We denote hereafter ¯ ρ(l) := ρ(l)/?m<br />bitrary distribution on the first m positive integers. The up-<br />date at iteration i proceeds as follows:<br />p=1ρ(p) the<br />normalised eigenvalues and let d(1),...,d(m) denote an ar-<br />Algorithm 8 Principal components Metropolis update<br />• At iteration i +1, given Xiand (?i,ρi,Wi)<br />1. Sample an update direction l ∼ (d(1),d(2),...,<br />d(m)).<br />2. Sample Zi+1∼ N(0,?i(l)ρi(l)), set Yi+1= Xi+1+<br />Zi+1w(l).<br />3. Set Xi+1= Yi+1 with probability min{1,π(Yi+1)/<br />π(Xi)}, otherwise Xi+1= Xi.<br />4. Update (?i,ρi,Wi) to (?i+1,ρi+1,Wi+1) in light of<br />Xi+1.<br />In words, at every iteration one of the available princi-<br />pal direction l is randomly selected, here according to the<br />probability d(1),d(2),...,d(m) (d(j) = ¯ ρ(j) being a pos-<br />sibility) but other choices are possible, and a “univariate”<br />random walk update in the direction w(l) is then attempted<br />with an increment drawn from N(0,?i(l)ρi(l)), where ?(l)<br />is a directional scaling factor adjusted to ensure that updates<br />in direction l have a preset acceptance probability—it uses<br />anupdateofthetype(22). Thisenablesfinerscalinginevery<br />principal direction. Note that this update might correspond<br />to a reducible algorithm when m &lt; nx, but that this should<br />not be a difficulty when used in combination with other up-<br />dates.<br />Wenowturntothedescriptionofanon-linealgorithmfor<br />the computation of the m first eigenvectors of the covariance<br />matrix of samples {Xi}. The algorithm relies on an on-line<br />EM algorithm for the popular probabilistic PCA (PPCA) al-<br />gorithm.<br />5.3.2 Online PCA recursion<br />The basis for PPCA was laid by Tipping and Bishop (1999)<br />who endowed the problem with a linear Gaussian model.<br />Even though the possibility of using an EM-algorithm is<br />mentioned, it is Roweis (1997) who extends the formalism<br />more specifically to the application of such a scheme. The<br />approach suffers however from a rotational ambiguity in the<br />latent variable space, since the returned vector set is a linear<br />superposition of the principal components, inducing a need<br />for post-processing. This drawback is overcome by Ahn and<br />Oh (2003) through the introduction of a constrained EM-<br />algorithm that corresponds to using several coupled models,<br />rather than a single model. These papers assume that all ob-<br />servations are present initially, whereas the adaptive algo-<br />rithm presented in this project needs to determine the prin-<br />cipal eigenvectors on-line. Ghasemi and Sousa (2005) refor-<br />mulate the constrained EM in order to achieve this. Roweis<br />(1997) mentions an on-line version, but it was not further<br />explored here because of the inherent rotational ambiguity.<br />The structure that is employed in PPCA is closely related<br />to factor analysis. This linear model is founded on the as-<br />sumption that the d-dimensional data can be explained by<br />a m-dimensional unobservable variable Z and an additive<br />noise ?,<br />Xi= WZi+?,<br />where W is a nx× m real valued matrix of factor loadings,<br />Zi<br />∼ N(0,Im) and ? ∼ N(0,R), where usually R = σ2Inx<br />for some σ2&gt; 0. It can be shown that the ML estimator of<br />W contains the eigenvectors and the latent variables struc-<br />ture suggests the use of an EM-algorithm. It is possible to<br />alter this problem in order to exactly remove the rotational<br />ambiguity, leading to the following recursion (for σ2= 0)<br />Wi+1= ?i+1WiL(WT<br />?<br />where ?iis an estimate of the covariance matrix of π at it-<br />eration i of the algorithm, while for a square matrix L(A)<br />(resp. U(A)) is the lower (resp. upper) part of A. Note the<br />computationally interesting feature of this recursion where<br />the inversion of triangular, rather than full, matrices is re-<br />quired. Roweis (1997) also provides an EM-algorithm for<br />PPCA without taking the zero-error limit, in what is called<br />Sensible PCA.<br />(39)<br />iid<br />iWi)−1<br />×UL(WT<br />iWi)−1WT<br />i?i+1WiL(WT<br />iWi)−1?−1<br />5.4 Discrete valued random vectors: the Gaussian copula<br />approach<br />So far we have implicitly assumed that π has a density with<br />respect to the Lebesgue measure and de facto excluded the<br />case where π is the probability distribution of a discrete<br />valued vector. This problem has been largely overlooked in<br />the literature, with however the exception of Nott and Kohn<br />(2005). We here briefly describe a strategy proposed in An-<br />drieu and Moffa (2008) which, as we shall see, offers the<br />possibility to exploit the tools developed for the purely con-<br />tinuous case in earlier sections. It differs from the work pre-<br />sented so far in this paper in that, as we shall see, the distri-<br />bution π or interest is embedded in an extended probability<br />model, which needs to be adapted.<br />Inordertosimplifypresentationwewillfocusonthecase<br />where X = {0,1}nx, the generalisation to scenarios involving<br />a larger number of discrete states or a mixture of continuous</p>  <p>Page 23</p> <p>Stat Comput (2008) 18: 343–373 365<br />and discrete valued random variables being straightforward.<br />Note that this simple scenario is of interest in the context<br />of variable selection, but also in the context of inference in<br />Ising models. The strategy consists of embedding the dis-<br />crete valued problem into a continuous framework by means<br />of an auxiliary variable z taking its values in Z := Rnx. More<br />precisely, consider the following distribution<br />˜ πμ(x,z)<br />:= π(x)<br />nx<br />?<br />i=1<br />N(z(i);μ(i),?(i,i))<br />??(i,i)(μ(i))x(i)(1−??(i,i)(μ(i)))1−x(i)<br />×I{z ∈ Ix},<br />where ?σ2(u) is the cumulative distribution function of the<br />univariate centered normal distribution with variance σ2,<br />μ,? ∈ Rnx× Rnx×nxand Ix:= Ix(1)× Ix(2)× ··· × Ix(nx)<br />with I0:= (−∞,0] and I1:= (0,+∞). Note that the eval-<br />uation of ?σ2(u) is routine, and that whenever π(x) can<br />be evaluated pointwise up to a normalising constant so can<br />˜ πμ(x,z), therefore allowing the use of standard sampling al-<br />gorithms. One can notice that marginally ˜ πμ(x) = π(x) but<br />also that<br />˜ πμ(x(i)|z) ∝ I{z(i) ∈ Ix(i)},<br />a type of deterministic relationship between z and x. These<br />properties suggest that the problem of sampling from π(x)<br />can be replaced by that of effectively sampling the continu-<br />ous component z ∼ ˜ πμ(z) followed by the determination of<br />the unique x satisfying I{z ∈ Ix} = 1.<br />Naturally not all choices of μ ∈ Rnxwill lead to efficient<br />sampling algorithms for a given distribution π and we note<br />in addition that the component z does not capture the de-<br />pendence structure of π(x). We shall see now how adap-<br />tive procedures can be of great help here. Consider the fol-<br />lowing distribution and denote θ := {μ, ˘ μ,?,˘?} for some<br />˘ μ,˘? ∈ Rnx×Rnx×nx<br />˘ qθ(x,z) := N(z; ˘ μ,˘?) I{z ∈ Ix},<br />whose marginal ˘ qθ(x) is often used to model the distribu-<br />tion of multivariate discrete valued random vectors e.g. in<br />the context of multivariate probit regression models. A par-<br />ticular strength of the model is that the dependence struc-<br />ture of ˘ qθ(x) is parametrised by the pair ˘ μ,˘? and that<br />sampling from ˘ qθ(x) is straightforward. However ˘ qθ(x) is<br />usually intractable, precluding its direct use to approximate<br />π(x). A natural suggestion here is simply to work on the<br />extended space X×Z and approximate ˜ πθ(x,z) := ˜ πμ(x,z)<br />with ˘ qθ(x,z). For example, with the natural choice μ = ˘ μ<br />and ? =˘? one could suggest minimising the following<br />Kullback-Leibler divergence<br />?<br />X×Z<br />˜ πθ(x,z)log˜ πθ(x,z)<br />˘ qθ(x,z)dxdz.<br />Given the structure of ˜ πθ(x,z), it is clear that the result-<br />ing ˘ qθ(x,z) is meant to “learn” both the marginals and<br />the dependence structure of π(x). Assuming that this can<br />be achieved, even approximately, ˘ qθ(x,z) and its parame-<br />ters can be used in multiple ways in order to sample from<br />˜ πθ(x,z). Following the ideas developed in earlier sections,<br />one could suggest to use ˘ qθ(x,z) as a proposal distribution<br />in an IMH algorithm targeting ˜ πθ(x,z). Indeed sampling<br />from ˘ qθ(x,z) is straightforward since it only requires one<br />to sample from Z ∼ N(μ,?) and to determine X such that<br />I{Z ∈ IX} = 1. However, as argued earlier, using the IMH<br />sampler is not always a good idea, and one could instead<br />suggest a more robust random walk Metropolis type algo-<br />rithm. For example, for some λ &gt; 0 and θ, we have<br />Algorithm 9 The Gaussian copula SRWM<br />• At iteration i +1, given Xi<br />1. Sample Z = Zi+W with W ∼ N(0,λ˘?).<br />2. Determine X such that I{Z ∈ IX} = 1.<br />3. Set (Xi+1,Zi+1) = (X,Z) with probability<br />?<br />otherwise (Xi+1,Zi+1) = (Xi,Zi).<br />min1,<br />˜ πθ(X,Z)<br />˜ πθ(Xi,Zi)<br />?<br />,<br />The problem of effectively determining ˘? can be ad-<br />dressed by using an adaptive algorithm, and in particular by<br />using recursions of the type (31) or (32) in the case of an up-<br />date component by component for example. More generally<br />all the strategies developed earlier in this paper for the con-<br />tinuous case can be adapted to the discrete setup (Andrieu<br />and Moffa 2008). Note however that the target distribution<br />now depends on θ and that a slight modification of the con-<br />vergence theory outlined earlier is required in this scenario.<br />6 Examples of applications<br />6.1 Erratic normal distribution<br />In this section we first demonstrate the practical interest of<br />the idea of compound criteria developed in Sect. 5.1 which<br />aims to accelerate the learning of features of the target dis-<br />tribution by the algorithm. Following Roberts and Rosen-<br />thal (2006) we consider a normal distribution N(0,?π=<br />MMT) defined on X = Rnxwith M a nx× nxmatrix with<br />i.i.d. entries sharing the distribution N(0,1)—we focus here<br />onthecase nx= 50.Thealgorithmweuseconsistsofa mix-<br />ture of Algorithms 4–5 and 8. Comparison with the stan-<br />dard AM algorithm is provided in Figs. 1–3 for a realisa-<br />tion of each of the algorithm and for the same number of</p>  <p>Page 24</p> <p>366 Stat Comput (2008) 18: 343–373<br />Fig. 1 Comparison of the<br />expected acceptance probability<br />of the standard AM algorithm<br />(bottom) and the corresponding<br />global update used by the<br />multi-criteria algorithm as a<br />function of the iterations<br />Fig. 2 Comparison of the R<br />coefficient for the standard AM<br />algorithm (top) and the<br />multi-criteria algorithm<br />(bottom) as a function of the<br />iterations<br />Fig. 3 Comparison of the 50<br />ordered estimated eigenvalues<br />after 50,000 iterations. Top:<br />truth. Middle: multi-criteria<br />algorithm. Bottom: standard AM<br />algorithm<br />evaluations of π. The coefficient R ≥ 1 is precisely defined<br />in Roberts and Rosenthal (2006). It is a measure of mis-<br />match between ?π and any arbitrary covariance matrix ?<br />related to the asymptotic performance of the N-SRWM, the<br />value R = 1 corresponding to optimality. Although poten-<br />tially useful the comparison of the eigenvalues alone might<br />be misleading without a comparison of the quality of the<br />eigenvectors—the R coefficient does this.<br />The gains are clear in terms of the number of evaluations<br />of the target density, whose computational cost will in gen-<br />eral dominate that of the additional recursions needed for<br />adaptation.<br />6.2 The banana example<br />The banana distribution, introduced in Haario et al. (1999)<br />and Haario et al. (2001) is a popular example to test adap-<br />tive algorithms since it presents the advantage of analytical<br />tractability of numerous characteristics, while allowing for a<br />Table 1 Summaries (mean+/−std): Norm of the first moment’s es-<br />timator, based on 100 runs. Different banana-shaped Gaussian distrib-<br />utions are used and compared to the results of the adaptive Metropolis<br />sampler (AM) presented by Haario et al. (1999). Since the target is<br />centered, the norm’s correct value is zero<br />nx<br />Norm?Eπ[X]?<br />π1= B0.03<br />Multi-criteria<br />π2= B0.1<br />Multi-criteriaAMAM<br />2<br />4<br />8<br />1.13±0.74<br />1.33±0.79<br />1.17±0.67<br />1.10±0.67<br />1.27±0.77<br />1.31±0.72<br />2.80±1.47<br />5.20±5.69<br />4.99±3.99<br />2.62±1.61<br />5.13±12.85<br />4.85±4.20<br />non-linear dependency between its components. Formally it<br />is the distribution of a normally distributed multivariate nor-<br />mal random X ∼ N(0,?) for nx≥ 2 which undergoes the<br />transformation<br />[X1,X2+b(X2<br />1−100),X3,...,Xnx],</p>  <p>Page 25</p> <p>Stat Comput (2008) 18: 343–373 367<br />Table 2 Empirical quantiles of adaptive MCMC output based on 25 runs of length 80,000 (burn-in: 60,000 lags), Banana-shaped target B0.03in<br />nxdimensions and an adaptive mixture of three Gaussian distributions, used as proposal, maximum deviation per dimension in red<br />nx<br />Banana-shaped target π1= B0.03<br />Quantile (in %)<br />1020 3040 50 6070 8090<br />2<br />5<br />7<br />9<br />9.60±0.60<br />9.55±0.75<br />9.79±0.81<br />9.69±1.12<br />10.27±1.14<br />19.54±0.74<br />19.33±1.08<br />19.49±1.28<br />19.58±1.87<br />20.46±1.84<br />29.29±1.07<br />29.05±1.36<br />29.43±1.56<br />29.57±2.33<br />30.81±2.23<br />39.52±1.34<br />39.17±1.67<br />39.46±2.09<br />39.47±2.54<br />40.77±2.39<br />49.63±1.58<br />49.32±1.98<br />49.51±2.27<br />49.47±2.60<br />50.83±2.31<br />59.78±1.85<br />59.42±2.18<br />59.58±2.18<br />59.56±2.43<br />60.95±2.00<br />70.14±1.87<br />69.37±1.97<br />69.66±2.04<br />69.71±1.95<br />70.95±1.81<br />80.38±1.65<br />79.65±1.80<br />79.87±1.62<br />79.47±1.62<br />80.86±1.54<br />90.22±1.24<br />89.93±1.25<br />90.15±1.18<br />90.22±1.22<br />90.41±1.0315<br />Table 3 Empirical quantiles of adaptive MCMC output based on 25 runs of length 80,000 (burn-in: 60,000 lags), Banana-shaped target B0.1in nx<br />dimensions and an adaptive mixture of three Gaussian distributions, used as proposal<br />nx<br />Banana-shaped target π2= B0.1<br />Quantile (in %)<br />1020 3040 5060 7080 90<br />2<br />5<br />7<br />9.60±0.60<br />9.55±0.75<br />9.79±0.81<br />19.54±0.74<br />19.33±1.08<br />19.49±1.28<br />29.29±1.07<br />29.05±1.36<br />29.43±1.56<br />39.52±1.34<br />39.17±1.67<br />39.46±2.09<br />49.63±1.58<br />49.32±1.98<br />49.51±2.27<br />59.78±1.85<br />59.42±2.18<br />59.58±2.18<br />70.14±1.87<br />69.37±1.97<br />69.66±2.04<br />80.38±1.65<br />79.65±1.80<br />79.87±1.62<br />90.22±1.24<br />89.93±1.25<br />90.15±1.18<br />and we denote hereafter Bb(?) the distribution of this ran-<br />dom vector, and simply Bbwhen ? is the identity matrix,<br />except for the top left element which is 100. We compare<br />the performance of a mixture of updates based on Algo-<br />rithm 7 which uses for each of the mixture component ei-<br />ther Algorithm 6 or Algorithm 8 with that of the AM al-<br />gorithm (Haario et al. 1999), for 10,000 iterations for B0.03<br />and 20,000 iterations for B0.1and n = 3 components for the<br />fitted mixture. The results are summarised in Table 1 seem<br />comparable,althoughouralgorithmseemstobemorerobust<br />in the difficult situation where π = B0.1.<br />We further tested the ability of the algorithm to properly<br />sample from the target distribution by comparing empirical<br />and exact quantiles. The results and methodology are sum-<br />marised in Tables 2 and 3.<br />The fitted mixture makes it possible to estimate the nor-<br />malising constant of the target distribution π, using the so-<br />called “harmonic mean” estimator, which relies on the iden-<br />tity<br />?<br />X<br />˘ qθ(x)<br />˜ π(x)π(x)dx =<br />where ˜ π(x) is proportional to π(x), but unormalised. Note<br />that ˘ qθ(x) provides us with a potentially reasonable instru-<br />mental distribution since it is adapted to fit π and might have<br />thinner tails than π. This is estimator is notoriously known<br />to be unstable wheneverthe variance of ˘ qθ(x)/˜ π(x) under π<br />is large and the suggested approach might in some situations<br />1<br />?<br />X˜ π(x)dx=: 1/Z,<br />(40)<br />Table 4 Harmonic mean estimator of the normalizing constant of<br />centered spherical Gaussian distributions and banana-shaped distrib-<br />utions F0.03(X), obtained by applying to a Gaussian N(0,S) with<br />diag(S) = [100,1,...,1] in d dimensions; Z is the partition function’s<br />analytical value<br />nx<br />π1= N(0,Inx)<br />ˆZ<br />π2= B0.03<br />ˆZZZ<br />2<br />5<br />7<br />6.27±0.01<br />97.53±0.23<br />601.72±2.57<br />6.28<br />98.96<br />621.77<br />68.9±15.5<br />1013.7±178.8<br />6204.6±1337.6<br />62.831<br />989.577<br />6217.696<br />remedythis.In the case of thebananashapeddistributionwe<br />choose ˜ π(x) such that<br />Z = det(2π?)1/2.<br />We present results for both B0.03and N(0,Inx), based on<br />50 runs, in Table 4.<br />For each of them the chain was run with three adaptive<br />Gaussian components for 100,000 iterations. The estimator<br />ˆZ was calculated according with the harmonic mean estima-<br />torafteraburn-inperiodof80,000iterations.Theestimation<br />of the Gaussian target’s partition function is very accurate.<br />Insevendimensions Z issomewhatunderestimatedsuggest-<br />ing that the chain was not run long enough to reach its sta-<br />tionary regime. The second target’s non-linearity leads to a<br />significant deterioration of the simulation results. While the<br />sample mean is close to the partition function’s true value</p>  <p>Page 26</p> <p>368Stat Comput (2008) 18: 343–373<br />Fig. 4 Top: Trace of the<br />positions s1,s2for k = 2<br />corresponding to iterations<br />1,...,40,000. Bottom:<br />Histograms of the dates for<br />k = 2 after 200,000 iterations<br />(with the first 10,000 samples<br />discarded)<br />the sample deviation is very large. A possible explanation is<br />that the chain has to be run much longer in this setting to<br />ensure convergence of the harmonic mean estimator.<br />A possible use of this result is that of the estimation of<br />posterior model probabilities.<br />6.3 Mine disaster data<br />The dataset of this classic example consists of the recorded<br />dates (in days) {y(i)} at which mine disaster have occurred<br />over a period covering 1851–1962. The data is modelled<br />as a Poisson process with intensity x(t) modelled as a step<br />function consisting of k +1 plateaux with starting positions<br />s(0) = 0 &lt; s(1) &lt; s(2) &lt; ··· &lt; s(k + 1) = T and heights<br />h(0),h(1),...,h(k) that is<br />x(t) =<br />k+1<br />?<br />i=1<br />h(i −1)I{s(i −1) ≤ t &lt; s(i)}.<br />The unknowns are therefore k,s := {s(i)} and h := {h(i)}.<br />With the priors of Green (1995) the log-posterior distribu-<br />tion, logπ(x), is the sum of the three following terms with<br />−?+klog(?)−log(?(k +1))+log?(2(k +1))<br />−(2k +1)logT,<br />(k +1)(αlogβ −log?(α))+(α −1)<br />k+1<br />?<br />i=1<br />log(h(i −1))<br />−β<br />k+1<br />?<br />i=1<br />h(i −1)+<br />k+1<br />?<br />i=1<br />log(s(i)−s(i −1)),<br />k+1<br />?<br />i=1<br />logh(i −1)<br />n<br />?<br />j=1<br />I{s(i −1) ≤ y(j) &lt; s(i)}<br />−<br />k+1<br />?<br />i=1<br />h(i −1)(s(i)−s(i −1)).<br />In our numerical experiments we took α = 1.0, β = 200 and<br />? = 3,whichisinlinewithGreen(1995)andHastie(2005),<br />and simply provided our adaptive algorithm, a combination<br />of the components described in Sect. 5, i.e. a mixture of Al-<br />gorithms 4–6 and 8, with the log-posterior above. We report<br />here the results obtained using one normal component, and<br />did not observe any significant different with 2 or 3 com-<br />ponents. One of the difficulty with the posterior distribution<br />of interest is that it involves very different scales and var-<br />ious dependence patterns between the parameters. We ran<br />the algorithm for fixed k = 1,2,3,4,5,6. In all scenarios<br />the components of h and s were initialised at 1000 and the<br />initial value for the estimate of the covariance matrix of π<br />was set to 10×I2k+1. In order to comment on the behaviour<br />of the adaptive procedure, we primarily focus on the case<br />k = 2 in order to maintain the legibility of the various fig-<br />ures. In Figs. 4–6 and 7 we present the traces and in relevant<br />cases histograms for {si}, {hi}, {λi} (the scaling coefficient<br />of the global RWM update), the corresponding running ex-<br />pected acceptance probabilities, {(λ1<br />ing coefficients of the componentwise RWM updates) and<br />their corresponding running expected acceptance probabili-<br />ties.Inthiscasethealgorithmwasranfor200,000iterations.<br />The reported robust behaviour of the algorithm is typical of<br />what we have systematically observed for all the realisations<br />i,...,λ2k+1<br />i<br />)} (the scal-</p>  <p>Page 27</p> <p>Stat Comput (2008) 18: 343–373 369<br />Fig. 5 Top: Trace of the<br />intensities h0,h1,h2for k = 2<br />corresponding to iterations<br />1,...,40,000. Bottom:<br />Histogram of the intensities for<br />k = 2 after 200,000 iterations<br />(with the first 10,000 samples<br />discarded)<br />Fig. 6 Top: Trace of the<br />“global” RWM update’s log(λ).<br />Bottom: Running expected<br />acceptance probability of the<br />“global” RWM update<br />of the algorithm that we have run. Despite poor initialisa-<br />tions of s,h and the parameters of the algorithm (observe<br />in particular the high rejection rate during the first 10,000<br />iterations particularly visible in Fig. 5) the procedure man-<br />ages to rapidly recover. The histograms show that our results<br />are in accordance with the results found in Hastie (2005).<br />The behaviour of {λi} and {(λ1<br />sponding running expected acceptance probabilities demon-<br />strate both the interest of adapting these parameters in the<br />initial phase of the algorithm, and the notion of bold and<br />i,...,λ2k+1<br />i<br />)} and their corre-<br />timid moves: small acceptance probabilities prompt the use<br />of smaller scaling factors in order to improve exploration<br />and timid moves seem to improve their performance faster<br />than bold moves (whose expected acceptance probabilities<br />is multiplied by 5 in the course of the first 200,000 itera-<br />tions). Naturally we observed that not all the parameters of<br />the algorithm seem to have converged, or stabilised around<br />fixed values, whereas the histograms for s and h seem to<br />be in agreement with previously reported results (e.g. Hastie<br />2005). The observed performance of the algorithm is in our</p>  <p>Page 28</p> <p>370 Stat Comput (2008) 18: 343–373<br />Fig. 7 k = 2: Top: Trace of the<br />“local” RWM updates’ log(λ)’s.<br />Bottom: Running expected<br />acceptance probability of the<br />“local” RWM updates<br />Fig. 8 Top: Trace of the<br />positions s1,s2,s3for k = 3<br />corresponding to iterations<br />1,...,40,000. Bottom: Trace of<br />the intensities h0,h1,h2,h3for<br />k = 3 corresponding to<br />iterations 1,...,40,000<br />view illustrative of three crucial points discussed earlier in<br />the paper:<br />1. Vanishing adaptation does not require convergence to en-<br />sure that ergodic averages are asymptotically correct,<br />2. but at the same time the study of the convergence prop-<br />erties of {θi} is fundamental since it ensures that this se-<br />quence is guaranteed to eventually approach the optimal<br />values defined by our criteria. It is indeed the conver-<br />gence properties of {θi} which explain both the observed<br />good behaviour of {λi} and {(λ1<br />and 7. As a result, and provided that we are ready to run<br />the algorithm longer then one can expect to be able to<br />obtain “better” values for the tuning parameter.<br />3. The user might decide to use this run as a preliminary<br />run to determine a satisfactory tuning parameter θ which<br />can then be used in a standard non-adaptive MCMC al-<br />gorithm, for which none of the ergodicity problems dis-<br />cussedearlierexist.Effectively,ifthisisthechoicemade,<br />this preliminary run is simply an optimisation procedure,<br />i,...,λ2k+1<br />i<br />)} in Figs. 6</p>  <p>Page 29</p> <p>Stat Comput (2008) 18: 343–373371<br />Fig. 9 k = 3: Top: Trace of the<br />“local” RWM updates’ log(λ)’s.<br />Bottom: Running expected<br />acceptance probability of the<br />“local” RWM updates<br />which however requires the use of samples at least ap-<br />proximately distributed according to the posterior distri-<br />bution π, therefore justifying the study of the ergodicity<br />properties of such algorithms.<br />We report the corresponding results for the case k = 3 in<br />Figs. 8–9. Due to the positivity constraints the harmonic<br />mean estimator in (40) cannot be mathematically exact. De-<br />spite finding results similar to those of Green (2003) and<br />Hastie (2005) we cannot in this approach as a reliable one.<br />Acknowledgements<br />associate editor for their great patience. They would like to thank the<br />reviewers, David Hastie and Arnaud Doucet for very useful comments<br />which have helped to improve the manuscript.<br />The authors are very grateful to the editor and<br />References<br />Ahn, J.-H., Oh, J.-H.: A constrained EM algorithm for principal com-<br />ponent analysis. Neural Comput. 15, 57–65 (2003)<br />Andradóttir, S.: A stochastic approximation algorithm with varying<br />bounds. Oper. Res. 43(6), 1037–1048 (1995)<br />Andrieu, C.: Discussion of Haario, H., Laine, M., Lehtinen, M., Saks-<br />man, E.: Markov chain Monte Carlo methods for high dimen-<br />sional inversion in remote sensing (December 2003). J. R. Stat.<br />Soc. Ser. B 66(3), 497–813 (2004)<br />Andrieu, C., Atchadé, Y.F.: On the efficiency of adaptive MCMC algo-<br />rithms. Electron. Commun. Probab. 12, 336–349 (2007)<br />Andrieu, C., Doucet, A.: Discussion of Brooks, S.P., Giudici, P.,<br />Roberts, G.O.: Efficient construction of reversible jump Markov<br />chain Monte Carlo proposal distributions. Part 1. J. R. Stat. Soc.<br />B 65, 3–55 (2003)<br />Andrieu, C., Jasra, A.: Efficient and principled implementation of the<br />tempering procedure. Tech. Rep. University of Bristol (2008)<br />Andrieu, C., Moffa, G.: A Gaussian copula approach for adaptation in<br />discrete scenarios (2008, in preparation)<br />Andrieu, C., Moulines, É.: On the ergodicity properties of some adap-<br />tive MCMC algorithms. Ann. Appl. Probab. 16(3), 1462–1505<br />(2006)<br />Andrieu, C., Robert, C.P.: Controlled MCMC for optimal sampling.<br />Tech. Rep. 0125, Cahiers de Mathématiques du Ceremade, Uni-<br />versité Paris-Dauphine (2001)<br />Andrieu, C., Tadi´ c, V.B.: The boundedness issue for controlled MCMC<br />algorithms. Tech. Rep. University of Bristol (2007)<br />Andrieu, C., Moulines, É., Priouret, P.: Stability of stochastic ap-<br />proximation under verifiable conditions. SIAM J. Control Optim.<br />44(1), 283–312 (2005)<br />Atchadé, Y.F.: An adaptive version for the Metropolis adjusted<br />Langevin algorithm with a truncated drift. Methodol. Comput.<br />Appl. Probab. 8, 235–254 (2006)<br />Atchadé, Y.F., Fort, G.: Limit Theorems for some adaptive MCMC al-<br />gorithms with subgeometric kernels. Tech. Rep. (2008)<br />Atchadé, Y.F., Liu, J.S.: The Wang-Landau algorithm in general state<br />spaces: applications and convergence analysis. Technical report<br />Univ. of Michigan (2004)<br />Atchadé, Y.F., Rosenthal, J.S.: On adaptive Markov chain Monte Carlo<br />algorithms. Bernoulli 11, 815–828 (2005)<br />Bai, Y., Roberts, G.O., Rosenthal, J.S.: On the Containment Condition<br />for Adaptive Markov Chain Monte Carlo Algorithms. Tech. Rep.<br />University of Toronto (2008)<br />Bédard, M.: Optimal acceptance rates for metropolis algorithms: mov-<br />ing beyond 0.234. Tech. Rep. University of Montréal (2006)<br />Bédard, M.: Weak convergence of metropolis algorithms for non-i.i.d.<br />target distributions. Ann. Appl. Probab. 17, 1222–1244 (2007)<br />Bennet, J.E., Racine-Poon, A., Wakefield, J.C.: MCMC for nonlinear<br />hierarchical models. In: MCMC in Practice. Chapman &amp; Hall,<br />London (1996)<br />Benveniste, A., Métivier, M., Priouret, P.: Adaptive Algorithms and<br />Stochastic Approximations. Springer, Berlin (1990)<br />Besag, J., Green, P.J.: Spatial statistics and Bayesian computation. J. R.<br />Stat. Soc. Ser. B Stat. Methodol. 55, 25–37 (1993)<br />Borkar, V.S.: Topics in Controlled Markov Chains. Longman, Harlow<br />(1990)<br />Browne, W.J., Draper, D.: Implementation and performance issues in<br />the Bayesian and likelihood fitting of multilevel models. Comput.<br />Stat. 15, 391–420 (2000)</p>  <p>Page 30</p> <p>372 Stat Comput (2008) 18: 343–373<br />Cappé, O., Douc, R., Gullin, A., Marin, J.-M., Robert, C.P.: Adap-<br />tive Importance Sampling in General Mixture Classes. Preprint<br />(2007)<br />Ceperley, D., Chester, G.V., Kalos, M.H.: Monte Carlo simulation of a<br />many fermion study. Phys. Rev. B 16(7), 3081–3099 (1977)<br />Chauveau, D., Vandekerkhove, P.: Improving convergence of the<br />Hastings-Metropolis algorithm with an adaptive proposal. Scand.<br />J. Statist. 29(1), 13–29 (2001)<br />Chen, H.F., Guo, L., Gao, A.J.: Convergence and robustness of the<br />Robbins-Monro algorithm truncated at randomly varying bounds.<br />Stoch. Process. Their Appl. 27(2), 217–231 (1988)<br />Chib, S., Greenberg, E., Winkelmann, R.: Posterior simulation and<br />Bayes factors in panel count data models. J. Econ. 86, 33–54<br />(1998)<br />de Freitas, N., Højen-Sørensen, P., Jordan, M., Russell, S.: Variational<br />MCMC. In: Proceedings of the 17th Conference in Uncertainty in<br />ArtificialIntelligence,pp.120–127.MorganKaufman,SanMateo<br />(2001). ISBN:1-55860-800-1<br />Delmas, J.-F., Jourdain, B.: Does waste-recycling really improve<br />Metropolis-Hastings Monte Carlo algorithm? Tech. Rep. Cer-<br />mics, ENPC (2007)<br />Delyon, B.: General results on the convergence of stochastic algo-<br />rithms. IEEE Trans. Automat. Control 41(9), 1245–1256 (1996)<br />Delyon, B., Juditsky, A.: Accelerated stochastic approximation. SIAM<br />J. Optim. 3(4), 868–881 (1993)<br />Douglas, C.: Simple adaptive algorithms for cholesky, LDLT, QR,<br />and eigenvalue decompositions of autocorrelation matrices for<br />sensor array data. In: Signals, Systems and Computers, 2001,<br />Conference Record of the Thirty-Fifth Asilomar Conference, vol.<br />21, pp. 1134–1138 (2001)<br />Erland, S.: On Adaptivity and Eigen-Decompositions of Markov<br />Chains. Ph.D. thesis Norwegian University of Science and Tech-<br />nology (2003)<br />Frenkel, D.: Waste-recycling Monte Carlo. In: Computer Simulations<br />In Condensed Matter: from Materials to Chemical Biology. Lec-<br />ture Notes in Physics, vol. 703, pp. 127–138. Springer, Berlin<br />(2006)<br />Gåsemyr, J.: On an adaptive Metropolis-Hastings algorithm with in-<br />dependent proposal distribution. Scand. J. Stat. 30(1), 159–173<br />(2003). ISSN 0303-6898<br />Gåsemyr, J., Natvig, B., Nygård, C.S.: An application of adaptive inde-<br />pendent chain Metropolis–Hastings algorithms in Bayesian haz-<br />ard rate estimation. Methodol. Comput. Appl. Probab. 6(3), 293–<br />302(10) (2004)<br />Gelfand, A.E., Sahu, S.K.:On Markov chain Monte Carlo acceleration.<br />J. Comput. Graph. Stat. 3(3), 261–276 (1994)<br />Gelman,A.,Roberts,G.,Gilks,W.:EfficientMetropolisjumpingrules.<br />In: Bayesian Statistics, vol. 5. Oxford University Press, New York<br />(1995)<br />Geyer, C.J., Thompson, E.A.: Annealing Markov chain Monte Carlo<br />with applications to ancestral inference. J. Am. Stat. Assoc. 90,<br />909–920 (1995)<br />Ghasemi, A., Sousa, E.S.: An EM-based subspace tracker for wire-<br />less communication applications. In: Vehicular Technology Con-<br />ference. VTC-2005-Fall. IEEE 62nd, pp. 1787–1790 (2005)<br />Gilks, W.R., Roberts, G.O., George, E.I.: Adaptive direction sampling.<br />The Statistician 43, 179–189 (1994)<br />Gilks, W.R., Roberts, G.O., Sahu, S.K.: Adaptive Markov chain Monte<br />Carlo through regeneration. J. Am. Stat. Assoc. 93, 1045–1054<br />(1998)<br />Giordani, P., Kohn, R.: Efficient Bayesian inference for multiple<br />change-point and mixture innovation models. Sveriges Riksbank<br />Working Paper No. 196 (2006)<br />Green, P.J.: Reversible jump Markov chain Monte Carlo computa-<br />tion and Bayesian model determination. Biometrika 82, 711–732<br />(1995)<br />Green, P.J.: Trans-dimensional Markov chain Monte Carlo. In: Green,<br />P.J., Hjort, N.L., Richardson, S. (eds.) Highly Structured Stochas-<br />tic Systems. Oxford Statistical Science Series, vol. 27, pp. 179–<br />198. Oxford University Press, London (2003)<br />Green, P.J., Mira, A.: Delayed rejection in reversible jump Metropolis-<br />Hastings. Biometrica 88(3) (2001)<br />Haario, H., Saksman, E., Tamminen, J.: Adaptive proposal distribution<br />for random walk Metropolis algorithm. Comput. Stat. 14(3), 375–<br />395 (1999)<br />Haario, H., Saksman, E., Tamminen, J.: An adaptive Metropolis algo-<br />rithm. Bernoulli 7(2), 223–242 (2001)<br />Haario, H., Laine, M., Mira, A., Saksman, E.: DRAM: Efficient adap-<br />tive MCMC (2003)<br />Haario, H., Laine, M., Lehtinen, M., Saksman, E.: Markov chain<br />Monte Carlo methods for high dimensional inversion in remote<br />sensing. J. R. Stat. Soc. Ser. B 66(3), 591–607 (2004)<br />Haario, H., Saksman, E., Tamminen, J.: Componentwise adaptation for<br />high dimensional MCMC. Comput. Stat. 20, 265–274 (2005)<br />Hastie, D.I.: Towards automatic reversible jump Markov chain Monte<br />Carlo. Ph.D. thesis Bristol University, March 2005<br />Holden, L.: Adaptive chains. Tech. Rep. Norwegian Computing Center<br />(1998)<br />Holden, L. et al.: History matching using adaptive chains. Tech. Report<br />Norwegian Computing Center (2002)<br />Kesten, H.: Accelerated stochastic approximation. Ann. Math. Stat.<br />29(1), 41–59 (1958)<br />Kim, S., Shephard, N., Chib, S.: Stochastic volatility: likelihood infer-<br />ence and comparison with ARCH models. Rev. Econ. Stud. 65,<br />361–393 (1998)<br />Laskey, K.B., Myers, J.: Population Markov chain Monte Carlo. Mach.<br />Learn. 50(1–2), 175–196 (2003)<br />Liu, J., Liang, F., Wong, W.H.: The use of multiple-try method and<br />local optimization in Metropolis sampling. J. Am. Stat. Assoc.<br />95, 121–134 (2000)<br />Mykland, P., Tierney, L., Yu, B.: Regeneration in Markov chain sam-<br />plers. J. Am. Stat. Assoc. 90, 233–241 (1995)<br />Nott, D.J., Kohn, R.: Adaptive sampling for Bayesian variable selec-<br />tion. Biometrika 92(4), 747–763 (2005)<br />Pasarica, C., Gelman, A.: Adaptively scaling the Metropolis algorithm<br />using the average squared jumped distance. Tech. Rep. Depart-<br />ment of Statistics, Columbia University (2003)<br />Plakhov, A., Cruz, P.: A stochastic approximation algorithm with step-<br />size adaptation. J. Math. Sci. 120(1), 964–973 (2004)<br />Ramponi, A.: Stochastic adaptive selection of weights in the simulated<br />tempering algorithm. J. Ital. Stat. Soc. 7(1), 27–55 (1998)<br />Robbins, H., Monro, S.: A stochastic approximation method. Ann.<br />Math. Stat. 22, 400–407 (1951)<br />Robert, C.P., Casella, G.: Monte Carlo Statistical Methods. Springer,<br />Berlin (1999)<br />Roberts, G.O., Rosenthal, J.: Optimal scaling of discrete approxima-<br />tion to Langevin diffusion. J. R. Stat. Soc. B 60, 255–268 (1998)<br />Roberts, G.O., Rosenthal, J.S.: Examples of adaptive MCMC. Techni-<br />cal Report University of Toronto (2006)<br />Roberts, G.O., Rosenthal, J.S.: Coupling and ergodicity of adaptive<br />MCMC. J. Appl. Probab. 44(2), 458–475 (2007)<br />Roberts, G.O., Gelman, A., Gilks, W.: Weak convergence and opti-<br />mal scaling of random walk Metropolis algorithms. Ann. Appl.<br />Probab. 7, 110–120 (1997)<br />Roweis, S.: EM algorithms for PCA and SPCA. Neural Inf. Process.<br />Syst. 10, 626–632 (1997)<br />Sahu, S.K., Zhigljavsky, A.A.: Adaptation for self regenera-<br />tive MCMC. Available from http://www.maths.soton.ac.uk/staff/<br />Sahu/research/papers/self.html<br />Saksman, E., Vihola, M.: On the ergodicity of the adaptive Metropolis<br />algorithm on unbounded domains (2008). arXiv:0806.2933</p>  <p>Page 31</p> <p>Stat Comput (2008) 18: 343–373373<br />Sherlock, C., Roberts, G.O.: Optimal scaling of the random walk<br />Metropolis on elliptically symmetric unimodal targets. Tech. Rep.<br />University of Lancaster (2006)<br />Sims, C.A.: Adaptive Metropolis-Hastings algorithm or Monte Carlo<br />kernel estimation. Tech. report Princeton University (1998)<br />Spall, J.C.: Adaptive stochastic approximation by the simultane-<br />ous perturbation method. IEEE Trans. Automat. Control 45(10),<br />1839–1853 (2000)<br />Stramer, O., Tweedie, R.L.: Langevin-type models II: self-targeting<br />candidates for MCMC algorithms. Methodol. Comput. Appl.<br />Probab. 1(3), 307–328 (1999)<br />Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction.<br />MIT Press, Cambridge (1998)<br />Tierney, L., Mira, A.: Some adaptive Monte Carlo methods for<br />Bayesian inference. Stat. Med. 18, 2507–2515 (1999)<br />Tipping, M.E., Bishop, C.M.: Probabilistic principal component analy-<br />sis. J. R. Stat. Soc. Ser. B Stat. Methodol. 61, 611–622 (1999)<br />Winkler,G.:ImageAnalysis,RandomFieldsandMarkovChainMonte<br />Carlo Methods: A Mathematical Introduction. Stochastic Mod-<br />elling and Applied Probability. Springer, Berlin (2003)</p>   </div> <div id="rgw21_56aba15bec488" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw22_56aba15bec488">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw23_56aba15bec488"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://www4.ncsu.edu/~rsmith/MA797V_S12/Andrieu08_AdaptiveMCMC_Tutorial.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="A tutorial on adaptive MCMC">A tutorial on adaptive MCMC</a> </div>  <div class="details">   Available from <a href="http://www4.ncsu.edu/~rsmith/MA797V_S12/Andrieu08_AdaptiveMCMC_Tutorial.pdf" target="_blank" rel="nofollow">ncsu.edu</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw25_56aba15bec488" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw26_56aba15bec488">  </ul> </div> </div>   <div id="rgw17_56aba15bec488" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw18_56aba15bec488"> <div> <h5> <a href="publication/267179538_Adaptive_MCMC_Methods_Lower_Bounds_on_the_Convergence_Time_Conclusions_Lower_Bounds_on_the_Convergence_Time_of_Adaptive_MCMC_Methods" class="color-inherit ga-similar-publication-title"><span class="publication-title">Adaptive MCMC Methods Lower Bounds on the Convergence Time Conclusions Lower Bounds on the Convergence Time of Adaptive MCMC Methods</span></a>  </h5>  <div class="authors"> <a href="researcher/2056565725_Dawn_Woodard" class="authors ga-similar-publication-author">Dawn Woodard</a>, <a href="researcher/81516769_Scott_Schmidler" class="authors ga-similar-publication-author">Scott Schmidler</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56aba15bec488"> <div> <h5> <a href="publication/265179939_Some_Results_on_the_Ergodicity_of_Adaptive_MCMC_Algorithms" class="color-inherit ga-similar-publication-title"><span class="publication-title">Some Results on the Ergodicity of Adaptive MCMC Algorithms</span></a>  </h5>  <div class="authors"> <a href="researcher/2053418592_Omar_Khalil" class="authors ga-similar-publication-author">Omar Khalil</a>, <a href="researcher/2053428522_Jeffrey_Rosenthal" class="authors ga-similar-publication-author">Jeffrey Rosenthal</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw20_56aba15bec488"> <div> <h5> <a href="publication/261204587_Implementing_manifold_learning_in_adaptive_MCMC_for_tracking_vehicle_under_disturbances" class="color-inherit ga-similar-publication-title"><span class="publication-title">Implementing manifold learning in adaptive MCMC for tracking vehicle under disturbances</span></a>  </h5>  <div class="authors"> <a href="researcher/70430180_Wei_Yeang_Kow" class="authors ga-similar-publication-author">Wei Yeang Kow</a>, <a href="researcher/74496921_Yit_Kwong_Chin" class="authors ga-similar-publication-author">Yit Kwong Chin</a>, <a href="researcher/69691704_Wei_Leong_Khong" class="authors ga-similar-publication-author">Wei Leong Khong</a>, <a href="researcher/2046157478_Hui_Keng_Lau" class="authors ga-similar-publication-author">Hui Keng Lau</a>, <a href="researcher/2045824192_KTK_Teo" class="authors ga-similar-publication-author">K.T.K. Teo</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw31_56aba15bec488" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw32_56aba15bec488">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw33_56aba15bec488" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=sypKKYGrX2IT6C_1eg62sCPnlrmisGyjHU95TEwFkgkIraouK_JdmkK-SUgcmuiX" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="3R8jr6Hr1+1UtZqpE6jbiqeeyB6YdjMcbiA/yDCWjS6yoVFZUVpXpFCzEv710whFVd4zttgXM40Gwj8do8OPXHXqD4nUHVYfnmnL1CLNlN1ir21oFZvH8cr9hfU1NXGUz2Dh6xcXT34nQjac5d4E1vwnzr3CM60qEGBprs+xyt7Cpb4IV6tZQ0TbXDxzk8OaIxEOqitZgE9taRFG4dywD+t7lqgpHsctE7JAyPgMU0RIo+tktTg/BeGPfZoSvZ6PleEXvix9WFQ0oszww6qr1EHa3eHa30thHxkZAGHyXhY="/> <input type="hidden" name="urlAfterLogin" value="publication/49458431_A_tutorial_on_adaptive_MCMC"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vNDk0NTg0MzFfQV90dXRvcmlhbF9vbl9hZGFwdGl2ZV9NQ01D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vNDk0NTg0MzFfQV90dXRvcmlhbF9vbl9hZGFwdGl2ZV9NQ01D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vNDk0NTg0MzFfQV90dXRvcmlhbF9vbl9hZGFwdGl2ZV9NQ01D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw34_56aba15bec488"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 540;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"profileUrl":"researcher\/3226552_Christophe_Andrieu","fullname":"Christophe Andrieu","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2549355721578\/images\/template\/default\/profile\/profile_default_m.png","profileStats":[{"data":{"impactPoints":"161.37","widgetId":"rgw5_56aba15bec488"},"id":"rgw5_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorImpactPoints.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorImpactPoints.html?authorUid=3226552","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationCount":153,"widgetId":"rgw6_56aba15bec488"},"id":"rgw6_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorPublicationCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorPublicationCount.html?authorUid=3226552","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"followerCount":4,"widgetId":"rgw7_56aba15bec488"},"id":"rgw7_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorFollowerCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorFollowerCount.html?authorUid=3226552","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw4_56aba15bec488"},"id":"rgw4_56aba15bec488","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorBadge.html?authorUid=3226552","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw3_56aba15bec488"},"id":"rgw3_56aba15bec488","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=49458431","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":49458431,"title":"A tutorial on adaptive MCMC","journalTitle":"Statistics and Computing","journalDetailsTooltip":{"data":{"journalTitle":"Statistics and Computing","journalAbbrev":"STAT COMPUT","publisher":"Springer Verlag","issn":"0960-3174","impactFactor":"1.62","fiveYearImpactFactor":"1.92","citedHalfLife":">10.0","immediacyIndex":"0.27","eigenFactor":"0.01","articleInfluence":"1.66","widgetId":"rgw9_56aba15bec488"},"id":"rgw9_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=0960-3174","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Article","details":{"doi":"10.1007\/s11222-008-9110-y","journalInfos":{"journal":"","publicationDate":"12\/2008;","publicationDateRobot":"2008-12","article":"18(4).","journalTitle":"Statistics and Computing","journalUrl":"journal\/0960-3174_Statistics_and_Computing","impactFactor":1.62}},"source":{"sourceUrl":"http:\/\/infoscience.epfl.ch\/record\/160389","sourceName":"OAI"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1007\/s11222-008-9110-y"},{"key":"rft.atitle","value":"A tutorial on adaptive MCMC"},{"key":"rft.title","value":"Statistics and Computing"},{"key":"rft.jtitle","value":"Statistics and Computing"},{"key":"rft.volume","value":"18"},{"key":"rft.issue","value":"4"},{"key":"rft.date","value":"2008"},{"key":"rft.issn","value":"0960-3174"},{"key":"rft.au","value":"Christophe Andrieu,Johannes Thoms"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw10_56aba15bec488"},"id":"rgw10_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=49458431","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":49458431,"peopleItems":[{"data":{"authorUrl":"researcher\/3226552_Christophe_Andrieu","authorNameOnPublication":"Christophe Andrieu","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Christophe Andrieu","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/3226552_Christophe_Andrieu","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw13_56aba15bec488"},"id":"rgw13_56aba15bec488","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=3226552&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw12_56aba15bec488"},"id":"rgw12_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=3226552&authorNameOnPublication=Christophe%20Andrieu","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/55344272_Johannes_Thoms","authorNameOnPublication":"Johannes Thoms","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Johannes Thoms","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/55344272_Johannes_Thoms","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw15_56aba15bec488"},"id":"rgw15_56aba15bec488","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=55344272&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw14_56aba15bec488"},"id":"rgw14_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=55344272&authorNameOnPublication=Johannes%20Thoms","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw11_56aba15bec488"},"id":"rgw11_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=49458431&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":49458431,"abstract":"<noscript><\/noscript><div>We review adaptive Markov chain Monte Carlo algorithms (MCMC) as a mean to optimise their performance. Using simple toy examples we review their theoretical underpinnings, and in particular show why adaptive MCMC algorithms might fail when some fundamental properties are not satisfied. This leads to guidelines concerning the design of correct algorithms. We then review criteria and the useful framework of stochastic approximation, which allows one to systematically optimise generally used criteria, but also analyse the properties of adaptive MCMC algorithms. We then propose a series of novel adaptive algorithms which prove to be robust and reliable in practice. These algorithms are applied to artificial and high dimensional scenarios, but also to the classic mine disaster dataset inference problem.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw16_56aba15bec488"},"id":"rgw16_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=49458431","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/49458431_A_tutorial_on_adaptive_MCMC\/links\/0ffc9e5f0cf255165fc9f05f\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw8_56aba15bec488"},"id":"rgw8_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=49458431&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2056565725,"url":"researcher\/2056565725_Dawn_Woodard","fullname":"Dawn Woodard","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":81516769,"url":"researcher\/81516769_Scott_Schmidler","fullname":"Scott Schmidler","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/267179538_Adaptive_MCMC_Methods_Lower_Bounds_on_the_Convergence_Time_Conclusions_Lower_Bounds_on_the_Convergence_Time_of_Adaptive_MCMC_Methods","usePlainButton":true,"publicationUid":267179538,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/267179538_Adaptive_MCMC_Methods_Lower_Bounds_on_the_Convergence_Time_Conclusions_Lower_Bounds_on_the_Convergence_Time_of_Adaptive_MCMC_Methods","title":"Adaptive MCMC Methods Lower Bounds on the Convergence Time Conclusions Lower Bounds on the Convergence Time of Adaptive MCMC Methods","displayTitleAsLink":true,"authors":[{"id":2056565725,"url":"researcher\/2056565725_Dawn_Woodard","fullname":"Dawn Woodard","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":81516769,"url":"researcher\/81516769_Scott_Schmidler","fullname":"Scott Schmidler","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/267179538_Adaptive_MCMC_Methods_Lower_Bounds_on_the_Convergence_Time_Conclusions_Lower_Bounds_on_the_Convergence_Time_of_Adaptive_MCMC_Methods","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/267179538_Adaptive_MCMC_Methods_Lower_Bounds_on_the_Convergence_Time_Conclusions_Lower_Bounds_on_the_Convergence_Time_of_Adaptive_MCMC_Methods\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56aba15bec488"},"id":"rgw18_56aba15bec488","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=267179538","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2053418592,"url":"researcher\/2053418592_Omar_Khalil","fullname":"Omar Khalil","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2053428522,"url":"researcher\/2053428522_Jeffrey_Rosenthal","fullname":"Jeffrey Rosenthal","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"May 2012","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/265179939_Some_Results_on_the_Ergodicity_of_Adaptive_MCMC_Algorithms","usePlainButton":true,"publicationUid":265179939,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/265179939_Some_Results_on_the_Ergodicity_of_Adaptive_MCMC_Algorithms","title":"Some Results on the Ergodicity of Adaptive MCMC Algorithms","displayTitleAsLink":true,"authors":[{"id":2053418592,"url":"researcher\/2053418592_Omar_Khalil","fullname":"Omar Khalil","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2053428522,"url":"researcher\/2053428522_Jeffrey_Rosenthal","fullname":"Jeffrey Rosenthal","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/265179939_Some_Results_on_the_Ergodicity_of_Adaptive_MCMC_Algorithms","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/265179939_Some_Results_on_the_Ergodicity_of_Adaptive_MCMC_Algorithms\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56aba15bec488"},"id":"rgw19_56aba15bec488","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=265179939","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":70430180,"url":"researcher\/70430180_Wei_Yeang_Kow","fullname":"Wei Yeang Kow","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":74496921,"url":"researcher\/74496921_Yit_Kwong_Chin","fullname":"Yit Kwong Chin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69691704,"url":"researcher\/69691704_Wei_Leong_Khong","fullname":"Wei Leong Khong","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2046157478,"url":"researcher\/2046157478_Hui_Keng_Lau","fullname":"Hui Keng Lau","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Conference Paper","publicationDate":"Jan 2012","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/261204587_Implementing_manifold_learning_in_adaptive_MCMC_for_tracking_vehicle_under_disturbances","usePlainButton":true,"publicationUid":261204587,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/261204587_Implementing_manifold_learning_in_adaptive_MCMC_for_tracking_vehicle_under_disturbances","title":"Implementing manifold learning in adaptive MCMC for tracking vehicle under disturbances","displayTitleAsLink":true,"authors":[{"id":70430180,"url":"researcher\/70430180_Wei_Yeang_Kow","fullname":"Wei Yeang Kow","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":74496921,"url":"researcher\/74496921_Yit_Kwong_Chin","fullname":"Yit Kwong Chin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69691704,"url":"researcher\/69691704_Wei_Leong_Khong","fullname":"Wei Leong Khong","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2046157478,"url":"researcher\/2046157478_Hui_Keng_Lau","fullname":"Hui Keng Lau","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2045824192,"url":"researcher\/2045824192_KTK_Teo","fullname":"K.T.K. Teo","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Control System, Computing and Engineering (ICCSCE), 2012 IEEE International Conference on; 01\/2012"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/261204587_Implementing_manifold_learning_in_adaptive_MCMC_for_tracking_vehicle_under_disturbances","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/261204587_Implementing_manifold_learning_in_adaptive_MCMC_for_tracking_vehicle_under_disturbances\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56aba15bec488"},"id":"rgw20_56aba15bec488","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=261204587","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw17_56aba15bec488"},"id":"rgw17_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=49458431&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":49458431,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":49458431,"publicationType":"article","linkId":"0ffc9e5f0cf255165fc9f05f","fileName":"A tutorial on adaptive MCMC","fileUrl":"http:\/\/www4.ncsu.edu\/~rsmith\/MA797V_S12\/Andrieu08_AdaptiveMCMC_Tutorial.pdf","name":"ncsu.edu","nameUrl":"http:\/\/www4.ncsu.edu\/~rsmith\/MA797V_S12\/Andrieu08_AdaptiveMCMC_Tutorial.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw23_56aba15bec488"},"id":"rgw23_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=49458431&linkId=0ffc9e5f0cf255165fc9f05f&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw22_56aba15bec488"},"id":"rgw22_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=49458431&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":23,"valueFormatted":"23","widgetId":"rgw24_56aba15bec488"},"id":"rgw24_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=49458431","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw21_56aba15bec488"},"id":"rgw21_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=49458431&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":49458431,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw26_56aba15bec488"},"id":"rgw26_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=49458431&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":23,"valueFormatted":"23","widgetId":"rgw27_56aba15bec488"},"id":"rgw27_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=49458431","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw25_56aba15bec488"},"id":"rgw25_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=49458431&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Stat Comput (2008) 18: 343\u2013373\nDOI 10.1007\/s11222-008-9110-y\nA tutorial on adaptive MCMC\nChristophe Andrieu \u00b7Johannes Thoms\nReceived: 23 January 2008 \/ Accepted: 19 November 2008 \/ Published online: 3 December 2008\n\u00a9 Springer Science+Business Media, LLC 2008\nAbstract We review adaptive Markov chain Monte Carlo\nalgorithms (MCMC) as a mean to optimise their perfor-\nmance. Using simple toy examples we review their theo-\nretical underpinnings, and in particular show why adaptive\nMCMC algorithmsmightfail when some fundamentalprop-\nerties are not satisfied. This leads to guidelines concern-\ning the design of correct algorithms. We then review cri-\nteria and the useful framework of stochastic approximation,\nwhich allows one to systematically optimise generally used\ncriteria, but also analyse the properties of adaptive MCMC\nalgorithms. We then propose a series of novel adaptive al-\ngorithms which prove to be robust and reliable in practice.\nThese algorithms are applied to artificial and high dimen-\nsional scenarios, but also to the classic mine disaster dataset\ninference problem.\nKeywords MCMC \u00b7 Adaptive MCMC \u00b7 Controlled\nMarkov chain \u00b7 Stochastic approximation\n1 Introduction\nMarkov chain Monte Carlo (MCMC) is a general strategy\nfor generating samples {Xi, i = 0,1,...} from complex\nhigh-dimensional distributions, say \u03c0 defined on a space\nC. Andrieu (?)\nSchool of Mathematics, University of Bristol,\nBristol BS8 1TW, UK\ne-mail: c.andrieu@bristol.ac.uk\nurl: http:\/\/www.stats.bris.ac.uk\/~maxca\nJ. Thoms\nChairs of Statistics, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne,\n1015 Lausanne, Switzerland\nX \u2282 Rnx(assumed for simplicity to have a density with re-\nspect to the Lebesgue measure, also denoted \u03c0), from which\nintegrals of the type\n?\nfor some \u03c0-integrable functions X \u2192 Rnfcan be approxi-\nmated using the estimator\nI (f) :=\nX\nf (x)\u03c0 (x)dx,\n\u02c6IN(f) :=1\nN\nN\n?\ni=1\nf (Xi),\n(1)\nprovided that the Markov chain generated with, say, transi-\ntion P is ergodic i.e. it is guaranteed to eventually produce\nsamples {Xi} distributed according to \u03c0. Throughout this\nreview we will refer, in broad terms, to the consistency of\nsuch estimates and the convergence of the distribution of Xi\nto \u03c0 as \u03c0-ergodicity. The main building block of this class\nof algorithms is the Metropolis-Hastings (MH) algorithm. It\nrequires the definition of a family of proposal distributions\n{q(x,\u00b7),x \u2208 X} whose role is to generate possible transitions\nfor the Markov chain, say from X to Y, which are then ac-\ncepted or rejected according to the probability\n?\nThe simplicity and universality of this algorithm are both\nits strength and weakness. Indeed, the choice of the pro-\nposal distribution is crucial: the statistical properties of the\nMarkov chain heavily depend upon this choice, an inade-\nquate choice resulting in possibly poor performance of the\nMonte Carlo estimators. For example, in the toy case where\nnx= 1 and the normal symmetric random walk Metropo-\nlis algorithm (N-SRWM) is used to produce transitions, the\n\u03b1(X,Y) = min1,\u03c0 (Y)q (Y,X)\n\u03c0 (X)q (X,Y)\n?\n."},{"page":2,"text":"344 Stat Comput (2008) 18: 343\u2013373\ndensity of the proposal distribution is of the form\n?\u22121\nwhere \u03b82is the variance of the proposed increments, hence\ndefining a Markov transition probability P\u03b8. The variance\nof the corresponding estimator\u02c6I\u03b8\nas small as possible for the purpose of efficiency, is well\nknown to be typically unsatisfactory for values of \u03b82that\nare either \u201ctoo small or too large\u201d in comparison to optimal\nor suboptimal value(s). In more realistic scenarios, MCMC\nalgorithms are in general combinations of several MH up-\ndates {Pk,\u03b8, k = 1,...,n, \u03b8 \u2208 ?} for some set ?, with each\nhaving its own parametrised proposal distribution qk,\u03b8 for\nk = 1,...,n and sharing \u03c0 as common invariant distribu-\ntion. These transition probabilities are usually designed in\norder to capture various features of the target distribution \u03c0\nand in general chosen to complement one another. Such a\ncombination can for example take the form of a mixture of\ndifferent strategies, i.e.\nq\u03b8(x,y) =\n1\n\u221a2\u03c0\u03b82exp\n2\u03b82(y \u2212x)2\n?\n,\nN(f), which we wish to be\nP\u03b8(x,dy) =\nn\n?\nk=1\nwk(\u03b8)Pk,\u03b8(x,dy),\n(2)\nwhere for any \u03b8 \u2208 ?,?n\nucts of transition matrices in the discrete case) such as\nk=1wk(\u03b8) = 1, wk(\u03b8) \u2265 0, but can\nalso, for example, take the form of combinations (i.e. prod-\nP\u03b8(x,dy) = P1,\u03b8P2,\u03b8\u00b7\u00b7\u00b7Pn,\u03b8(x,dy).\nBoth examples are particular cases of the class of Markov\ntransition probabilities P\u03b8 on which we shall focus in\nthis paper: they are characterised by the fact that they\n(a) belong to a family of parametrised transition proba-\nbilities {P\u03b8,\u03b8 \u2208 ?} (for some problem dependent set ?,\n? = (0,+\u221e) in the toy example above) (b) for all \u03b8 \u2208 ? \u03c0\nis an invariant distribution for P\u03b8, which is assumed to be\nergodic (c) the performance of P\u03b8, for example the variance\nof\u02c6I\u03b8\nOur aim in this paper is to review the theoretical under-\npinnings and recent methodological advances in the area\nof computer algorithms that aim to \u201coptimise\u201d such para-\nmetrised MCMC transition probabilities in order to lead\nto computationally efficient and reliable procedures. As we\nshall see wealso suggest new algorithms.One shouldnoteat\nthis point that in some situations of interest, such as temper-\ning type algorithms (Geyer and Thompson 1995), property\n(b) above might be violated and instead the invariant distrib-\nution of P\u03b8might depend on \u03b8 \u2208 ? (although only a non \u03b8-\ndependent feature of this distribution \u03c0\u03b8might be of interest\nto us for practical purposes). We will not consider this case\nin depth here, but simply note that most of the arguments\nand ideas presented hereafter generally carry on to this\nN(f) above, is sensitive to the choice of \u03b8.\nslightly more complex scenario e.g. (Benveniste et al. 1990;\nAtchad\u00e9 and Rosenthal 2005).\nThe choice of a criterion to optimise is clearly the first\ndecision that needs to be made in practice. We discuss this\nissue in Sect. 4.1 where we point out that most sensible opti-\nmality or suboptimality criteria can be expressed in terms of\nexpectations with respect to the steady state-distributions of\nMarkov chains generated by P\u03b8for \u03b8 \u2208 ? fixed, and make\nnew suggestions in Sect. 5 which are subsequently illus-\ntrated on examples in Sect. 6. We will denote by \u03b8\u2217a generic\noptimal value for our criteria, which is always assumed to\nexist hereafter.\nIn order to optimise such criteria, or even simply find\nsuboptimal values for \u03b8, one could suggest to sequentially\nrun a standard MCMC algorithm with transition P\u03b8 for a\nset of values of \u03b8 (either predefined or defined sequentially)\nand compute the criterion of interest (or its derivative etc.)\nonce we have evidence that equilibrium has been reached.\nThis can naturally be wasteful and we will rather focus here\non a technique which belongs to the well known class of\nprocesses called controlled Markov chains (Borkar 1990)\nin the engineering literature, which we will refer to as con-\ntrolled MCMC (Andrieu and Robert 2001), due to their nat-\nural filiation. More precisely we will assume that the algo-\nrithm proceeds as follows. Given a family of transition prob-\nabilities {P\u03b8,\u03b8 \u2208 ?} defined on X such that for any \u03b8 \u2208 ?,\n\u03c0P\u03b8= \u03c0 (meaning that if Xi\u223c \u03c0, then Xi+1\u223c \u03c0,Xi+2\u223c\n\u03c0,...) and given a family of (possibly random) mappings\n{\u03b8i: ? \u00d7 Xi+1\u2192 ?,i = 1,...}, which encodes what is\nmeant by optimality by the user, the most general form of\na controlled MCMC proceeds as follows:\nAlgorithm 1 Controlled Markov chain Monte Carlo\n\u2022 Sample initial values \u03b80,X0\u2208 ?\u00d7 X.\n\u2022 Iteration i + 1 (i \u2265 0), given \u03b8i= \u03b8i(\u03b80,X0,...,Xi) from\niteration i\n1. Sample Xi+1|(\u03b80,X0,...,Xi) \u223c P\u03b8i(Xi,\u00b7).\n2. Compute \u03b8i+1= \u03b8i+1(\u03b80,X0,...,Xi+1).\nIn Sect. 4.2 we will focus our results to particular map-\npings well suited to our purpose of computationally efficient\nsequential updating of {\u03b8i} for MCMC algorithms, which\nrely on the Robbins-Monro update and more generally on\nthe stochastic approximation framework (Benveniste et al.\n1990). However, before embarking on the description of\npractical procedures to optimise MCMC transition probabil-\nities we will first investigate, using mostly elementary un-\ndergraduate level tools, some of the theoretical ergodicity\nproperties of controlled MCMC algorithms.\nIndeed, as we shall see, despite the assumption that for\nany \u03b8 \u2208 ?, \u03c0P\u03b8= \u03c0, adaptation in the context of MCMC"},{"page":3,"text":"Stat Comput (2008) 18: 343\u2013373345\nusing the controlled approach leads to complications. In\nfact, this type of adaptation can easily perturb the ergodicity\nproperties of MCMC algorithms. In particular algorithms of\nthis type will in most cases lead to the loss of \u03c0 as an invari-\nant distribution of the process {Xi}, which intuitively should\nbe the minimum requirement to produce samples from \u03c0\nand lead to consistent estimators. Note also that when not\ncarefully designed such controlled MCMC can lead to tran-\nsient processes or processes such that\u02c6IN(f) is not consis-\ntent. Studying the convergence properties of such processes\nnaturally raises the question of the relevance of such devel-\nopments in the present context. Indeed it is often argued that\none might simply stop adaptation once we have enough evi-\ndence that {\u03b8i} has reached a satisfactory optimal or subop-\ntimal value of \u03b8 and then simply use samples produced by a\nstandard MCMC algorithm using such a fixed good value\u02dc\u03b8.\nNo new theory should then be required. While apparently\nvalid, this remark ignores the fact that most criteria of in-\nterest depend explicitly on features of \u03c0, which can only\nbe evaluated with... MCMC algorithms. For example, as\nmentioned above most known and useful criteria can be for-\nmulated as expectations with respect to distributions which\nusually explicitly involve \u03c0.\nOptimising such criteria, or finding suboptimal values\nof \u03b8\u2217, thus requires one to be able to sample\u2014perhaps ap-\nproximately or asymptotically\u2014from \u03c0, which in the con-\ntext of controlled MCMC requires one to ensure that the\nprocess described above can, in principle, achieve this aim.\nThis, in our opinion, motivates and justifies the need for\nsuch theoretical developments as they establish whether or\nnot controlled MCMC can, again in principle, optimise such\n\u03c0-dependent criteria. Note that convergence of {\u03b8i} should\nitself not be overlooked since, in light of our earlier discus-\nsion of the univariate N-SRWM, optimisation of {P\u03b8} is our\nprimary goal and should be part of our theoretical develop-\nments. Note that users wary of the perturbation to ergodic-\nity brought by adaptation might naturally choose to \u201cfreeze\u201d\n{\u03b8i} to a value \u03b8\u03c4 beyond an iteration \u03c4 and consider only\nsamples produced by the induced Markov chain for their in-\nference problem. A stopping rule is described in Sect. 4.2.2.\nIn fact, as we shall see it is possible to run the two proce-\ndures simultaneously.\nFinally, whereas optimising an MCMC algorithm seems\na legitimate thing to do, one might wonder if it is compu-\ntationally worth adapting. This is a very difficult question\nfor which there is probably no straight answer. The view we\nadopt here is that such optimisation schemes are very useful\ntools to design or help the design of efficient MCMC algo-\nrithmswhich,whileleadingtosomeadditionalcomputation,\nhave the potential to spare the MCMC user significant im-\nplementation time.\nThe paper is organised as follows. In Sect. 2 we provide\ntoy examples that illustrate the difficulties introduced by the\nadaptation of MCMC algorithms. In Sect. 3 we discuss why\none might expect vanishing adaptation to lead to processes\nsuch that {Xi} can be used in order to estimate expectation\nwith respect to \u03c0. This section might be skipped on a first\nreading. In Sect. 4 we first discuss various natural criteria\nwhich are motivated by theory, but to some extent simplified\nin order to lead to useful and implementable algorithms. We\nthen go on to describe how the standard framework of sto-\nchastic approximation, of which the Robbins-Monro recur-\nsion is the cornerstone, provides us with a systematic frame-\nwork to design families of mappings {\u03b8i} in a recursive man-\nner and understand their properties. In Sect. 5 we present a\nseries of novel adaptive algorithms which circumvent some\nof the caveats of existing procedures. These algorithms are\napplied to various examples in Sect. 6.\n2 The trouble with adaptation\nIn this section we first illustrate the loss of \u03c0-ergodicity of\ncontrolledMCMCwiththehelpoftwosimpletoyexamples.\nThe level of technicality required for these two examples\nis that of a basic undergraduate course on Markov chains.\nDespite their simplicity, these examples suggest that vanish-\ning adaptation (a term made more precise later) might pre-\nserve asymptotic \u03c0-ergodicity. We then finish this section\nby formulating more precisely the fundamental difference\nbetween standard MCMC algorithms and their controlled\ncounterparts which affects the invariant distribution of the\nalgorithm. This requires the introduction of some additional\nnotation used in Sect. 4 and a basic understanding of expec-\ntations to justify vanishing adaptation, but does not signifi-\ncantly raise the level of technicality.\nConsider the following toy example, suggested in An-\ndrieu and Moulines (2006), where X = {1,2} and \u03c0 =\n(1\/2,1\/2) (it is understood here that for such a case we will\nabuse notation and use \u03c0 for the vector of values of \u03c0 and\nP\u03b8for the transition matrix) and where the family of transi-\ntion probabilities under consideration is of the form, for any\n\u03b8 \u2208 ? := (0,1)\n?P\u03b8(Xi= 1,Xi+1= 1)\n?\nIt is clear that for any \u03b8 \u2208 ?, \u03c0 is a left eigenvector of P\u03b8\nwith eigenvalue 1,\nP\u03b8=\nP\u03b8(Xi= 1,Xi+1= 2)\nP\u03b8(Xi= 2,Xi+1= 2)\nP\u03b8(Xi= 2,Xi+1= 1)\n\u03b8\n1\u2212\u03b8\n1\u2212\u03b8\n?\n=\n\u03b8\n?\n.\n(3)\n\u03c0P\u03b8= \u03c0,\ni.e. \u03c0 is an invariant distribution of P\u03b8. For any \u03b8 \u2208 ? the\nMarkov chain is obviously irreducible and aperiodic, and\nby standard theory is therefore ergodic, i.e. for any starting"},{"page":4,"text":"346 Stat Comput (2008) 18: 343\u2013373\nprobability distribution \u03bc,\nlim\ni\u2192\u221e\u03bcPi\n(with Pi\nfunction f\n\u03b8= \u03c0\n\u03b8the i-th power of P\u03b8), and for any finite real valued\nlim\nN\u2192\u221e\n1\nN\nN\n?\ni=1\nf(Xi) = E\u03c0(f(X)),\nalmost surely, where for any probability distribution \u03bd, E\u03bd\nrepresents the expectation operator with respect to \u03bd. Now\nassume that \u03b8 is adapted to the current state in order to sam-\nple the next state of the chain, and assume for now that\nthis adaptation is a time invariant function of the previous\nstate of the MC. More precisely assume that for any i \u2265 1\nthe transition from Xi to Xi+1is parametrised by \u03b8(Xi),\nwhere \u03b8 : X \u2192 ?. The remarkable property, specific to this\npurely pedagogical example, is that {Xi} is still in this case\na time homogeneous Markov chain with transition proba-\nbility\n\u02c7 P(Xi= a,Xi+1= b) := P\u03b8(a)(Xi= a,Xi+1= b)\nfor a,b \u2208 X, resulting in the time homogeneous transition\nmatrix\n?\nNaturally the symmetry of P\u03b8 above is lost and one can\ncheck that the invariant distribution of\u02c7 P is\n?\n\u02c7 P :=\n\u03b8(1)\n1\u2212\u03b8(2)\n1\u2212\u03b8(1)\n\u03b8(2)\n?\n.\n(4)\n\u02c7 \u03c0 =\n1\u2212\u03b8(2)\n2\u2212\u03b8(1)\u2212\u03b8(2),\n1\u2212\u03b8(1)\n2\u2212\u03b8(1)\u2212\u03b8(2)\n?\n?= \u03c0,\nin general. For \u03b8(1),\u03b8(2) \u2208 ? the time homogeneous\nMarkov chain will be ergodic, but will fail to converge to\n\u03c0 as soon as \u03b8(1) ?= \u03b8(2), that is as soon as there is depen-\ndence on the current state. As we shall see, the principle of\nvanishing adaptation consists of the present toy example of\nmaking both \u03b8(1) and \u03b8(2) time dependent (deterministi-\ncally for simplicity here), denoted \u03b8i(1) and \u03b8i(2) at itera-\ntion i, and ensure that as i \u2192 \u221e, |\u03b8i(1) \u2212 \u03b8i(2)| vanishes.\nIndeed, while {\u03b8i(1)} and {\u03b8i(2)} are allowed to evolve for-\never (and maybe not converge) the corresponding transition\nprobabilities {\u02c7 Pi:= P\u03b8i(Xi)} haveinvariantdistributions {\u02c7 \u03c0i}\nconvergent to \u03c0. We might hence expect one to recover \u03c0-\nergodicity. In fact in the present case standard theory for\nnon-homogeneous Markov chains can be used in order to\nfind conditions on {\u03b8i} that ensure ergodicity, but we do not\npursue this in depth here.\nIt could be argued, and this is sometimes suggested, that\nthe problem with the example above is that in order to pre-\nserve \u03c0 as a marginal distribution, \u03b8 should not depend on\nXifor the transition to Xi+1, but on X0,...,Xi\u22121only. For\nsimplicityassumethatthedependenceison Xi\u22121only.Then\nit is sometimes argued that since\n?\n\u03c0(Xi= 1)\n\u03c0(Xi= 2)\n?T\n\u00d7\n?\n?P\u03b8(Xi\u22121)(Xi= 1,Xi+1= 1)\n?T?\n=?\u03c0(Xi+1= 1), \u03c0(Xi+1= 2)?,\nthen Xi+1,Xi+2,... are all marginally distributed accord-\ning to \u03c0. Although this calculation is correct, the underly-\ning reasoning is naturally incorrect in general. This can be\nchecked in two ways. First through a counterexample which\nonly requires elementary arguments. Indeed in the situation\njust outlined, the law of Xi+1 given \u03b80,X0,...,Xi\u22121,Xi\nis P\u03b8(Xi\u22121)(Xi,Xi+1\u2208 \u00b7), from which we deduce that Zi=\n(Zi(1),Zi(2)) = (Xi,Xi\u22121) isatimehomogeneousMarkov\nchain with transition\nP\u03b8(Xi\u22121)(Xi= 1,Xi+1= 2)\nP\u03b8(Xi\u22121)(Xi= 2,Xi+1= 2)\nP\u03b8(Xi\u22121)(Xi= 2,Xi+1= 1)\n?\n=\n\u03c0(Xi= 1)\n\u03c0(Xi= 2)\n\u03b8(Xi\u22121)\n1\u2212\u03b8(Xi\u22121)\n1\u2212\u03b8(Xi\u22121)\n\u03b8(Xi\u22121)\n?\nP\u03b8(Zi(2))(Zi(1),Zi+1(1)) I{Zi+1(2) = Zi(1)},\nwhere for a set A, IA denotes its indicator function. De-\nnoting the states\u00af1 := (1,1),\u00af2 := (1,2),\u00af3 := (2,1) and\n\u00af4 := (2,2), the transition matrix of the time homogeneous\nMarkov chain is\n\u23a1\n\u23a2\n\u02c7 P =\n\u23a2\n\u23a3\n\u03b8(1)\n\u03b8(2)\n0\n0\n0\n0\n1\u2212\u03b8(1)\n1\u2212\u03b8(2)\n0\n0\n0\n0\n1\u2212\u03b8(1)\n1\u2212\u03b8(2)\n\u03b8(1)\n\u03b8(2)\n\u23a4\n\u23a5\n\u23a5\n\u23a6\nand it can be directly checked that the marginal invariant\ndistribution of Zi(1) is\n\u02c7 \u03c0 =\n?\n\u00d7\n2+\n?\n\u03b8(2)\n1\u2212\u03b8(1)+\n1+\u03b8(2)\u2212\u03b8(1)\n1\u2212\u03b8(1)\n\u03b8(1)\n1\u2212\u03b8(2)\n1+\u03b8(1)\u2212\u03b8(2)\n1\u2212\u03b8(2)\n?\u22121\n?\n?= (1\/2, 1\/2),\nin general. The second and more informative approach con-\nsists of considering the actual distribution of the process\ngenerated by a controlled MCMC. Let us denote\u02c7E\u2217 the\nexpectation for the process started at some arbitrary \u03b8,x \u2208\n? \u00d7 X. This operator is particularly useful to describe the\nexpectation of \u03c8(Xi,Xi+1,...) for any i \u2265 1 and any func-\ntion \u03c8 : Xk\u03c8\u2192 R,\u02c7E\u2217(\u03c8(Xi,Xi+1,...,Xi+k\u03c8\u22121)). More\nprecisely it allows one to clearly express the dependence of\n\u03b8i(\u03b80,X0,...,Xi) on the past \u03b80,X0,...,Xiof the process.\nIndeed for any f : X \u2192 R, using the tower property of\nexpectations and the definition of controlled MCMC given"},{"page":5,"text":"Stat Comput (2008) 18: 343\u2013373347\nin the introduction, we find that\n\u02c7E\u2217(f(Xi+1)) =\u02c7E\u2217\n?\u02c7E\u2217(f(Xi+1)|\u03b80,X0,...,Xi)\n??\n?\n?\n=\u02c7E\u2217\nXP\u03b8i(X0,...,Xi)(Xi,dx)f(x),\n(5)\nwhich is another way of saying that the distribution of\nXi+1 is that of a random variable sampled, conditional\nupon \u03b80,X0,...,Xi, according to the random transition\nP\u03b8i(X0,...,Xi)(Xi,Xi+1 \u2208 \u00b7), where the pair \u03b8i(\u03b80,X0,\n...,Xi), Xi is randomly drawn from a distribution com-\npletely determined by the possible histories \u03b80,X0,...,Xi.\nIn the case where X is a finite discrete set, writing this\nrelation concisely as the familiar product of a row vector\nand a transition matrix as above would require one to de-\ntermine the (possibly very large) set of values for the pair\n\u03b8i(\u03b80,X0,...,Xi), Xi(say Wi), the vector representing the\nprobability distribution of all these pairs as well as the tran-\nsition matrix from Wito X. The introduction of the expec-\ntation allows one to bypass these conceptual and notational\ndifficulties. We will hereafter denote\n?\nand whenever possible will drop unnecessary arguments i.e.\narguments of \u03d5 which do not affect its values.\nThe possibly complex dependence on \u03b8i(\u03b80,X0,...,Xi),\nXiof the transition of the process to Xi+1needs to be con-\ntrasted with the case of standard MCMC algorithms. Indeed,\nin this situation the randomness of the transition probability\nonly stems from Xi. This turns out to be a major advantage\nwhen it comes to invariant distributions. Let us assume that\nfor some i \u2265 1\u02c7E\u2217(g(Xi)) = E\u03c0(g(X)) for all \u03c0-integrable\nfunctions g. Then according to the identity in (5), for any\ngiven \u03b8 \u2208 ? and \u03b8i= \u03b8 for all i \u2265 0 a standard MCMC al-\ngorithm has the well known and fundamental property\n\u03d5(\u03b80,X0,...,Xi) :=\nX\nP\u03b8i(\u03b80,X0,...,Xi)(Xi,dx)f(x),\n\u02c7E\u2217(f(Xi+1)) =\u02c7E\u2217(\u03d5(\u03b8,Xi))\n= E\u03c0(\u03d5(\u03b8,X))\n=\n?\nX\u00d7X\n\u03c0(dx)P\u03b8(x,dy)f(y) = E\u03c0(f(X)),\nwhere the second equality stems from the assumption\n\u02c7E\u2217(g(Xi)) = E\u03c0(g(X)) and the last equality is obtained by\nthe assumed invariance of \u03c0 for P\u03b8 for any \u03b8 \u2208 ?. Now\nwe turn to the controlled MCMC process and focus for\nsimplicity on the case \u03b8i(\u03b80,X0,...,Xi) = \u03b8(Xi\u22121), cor-\nresponding to our counterexample. Assume that for some\ni \u2265 1 Xi is marginally distributed according to \u03c0, i.e. for\nany g : X \u2192 R,\u02c7E\u2217(g(Xi)) = E\u03c0(g(X)), then we would like\nto check if\u02c7E\u2217(g(Xj)) = E\u03c0(g(X)) for all j \u2265 i. However\nusing the tower property of expectations in order to exploit\nthe property\u02c7E\u2217(g(Xi)) = E\u03c0(g(X)),\n\u02c7E\u2217(f(Xi+1)) =\u02c7E\u2217(\u03d5(Xi\u22121,Xi))\n=\u02c7E\u2217\n?\u02c7E\u2217(\u03d5(Xi\u22121,X)|X)\nNow it would be tempting to use the stationarity assumption\nin the last expression,\n?\n= E\u03c0(f(X)).\nThis is however not possible due to the presence of the\nconditional expectation\u02c7E\u2217(\u00b7|X) (which crucially depends\non X) and conclude that in general\n?\u02c7E\u2217(\u03d5(Xi\u22121,Xi)|Xi)\n?\n.\n= E\u03c0\n?\nE\u03c0(\u03d5(Xi\u22121,X)) =\nX\u00d7X\n\u03c0(dx)P\u03b8(Xi\u22121)(x,dy)f(y)\n\u02c7E\u2217(f(Xi+1))\n?=\u02c7E\u2217\n?\nE\u03c0\n??\nX\nP\u03b8(\u03b80,X0,Xi\u22121)(X,dxi+1)f(xi+1)\n??\n.\nThe misconception that this inequality might be an equality\nis at the root of the incorrect reasoning outlined earlier. This\nproblem naturally extends to more general situations.\nVanishing adaptation seems, intuitively, to offer the pos-\nsibility to circumvent the problem of the loss of \u03c0 as in-\nvariant distribution. However, as illustrated by the follow-\ning toy example, vanishing adaptation might come with\nits own shortcomings. Consider a (deterministic) sequence\n{\u03b8i} \u2282 (\u22121,1)Nand for simplicity first consider the non-\nhomogeneous, and non-adaptive, Markov chain {Xi} with\ntransition P\u03b8iat iteration i \u2265 1, where P\u03b8 is given by (3),\nand initial distribution (\u03bc,1 \u2212 \u03bc) for \u03bc \u2208 [0,1]. One can\neasily check that for any n \u2265 1 the product of matrices\nP\u03b81\u00d7\u00b7\u00b7\u00b7\u00d7P\u03b8nhas the simple expression\nP\u03b81\u00d7\u00b7\u00b7\u00b7\u00d7P\u03b8n\n=1\n2\ni=1(2\u03b8i\u22121)\nAs a result one deduces that the distribution of Xnis\n?1+?n\ni=1(2\u03b8i\u22121)\n1\u2212?n\ni=1(2\u03b8i\u22121)\ni=1(2\u03b8i\u22121)\n1\u2212?n\n1+?n\n?\n.\n1\n2\nNow if \u03b8i\u2192 0 (resp. \u03b8i\u2192 1) and?\u221e\nof {\u03b8i} is \u201ctoo fast\u201d , then limn\u2192\u221e\nas a consequence, whenever \u03bc ?= 1\/2, the distribution of Xn\ndoes not converge to \u03c0 = (1\/2,1\/2). Similar developments\nare possible for the toy adaptive MCMC algorithm given by\n?1+(2\u03bc\u22121)?n\n?\u221e\ni=1(2\u03b8i\u22121)\n1\u2212(2\u03bc\u22121)?n\ni=1(2\u03b8i\u22121)?.\ni=1\u03b8i< +\u221e (resp.\ni=1(1 \u2212 \u03b8i) < +\u221e ), that is convergence to either 0 or 1\n?n\ni=1(2\u03b8i\u2212 1) ?= 0 and"},{"page":6,"text":"348 Stat Comput (2008) 18: 343\u2013373\nthe transition matrix \u02c7 P in (4), at the expense of extra tech-\nnical complications, and lead to the same conclusions. This\ntoy example points to potential difficulties encountered by\ncontrolled MCMC algorithms that exploit vanishing adapta-\ntion: whereas \u03c0-ergodicity of P\u03b8is ensured for any \u03b8 \u2208 ?,\nthis property might be lost if the sequence {\u03b8i} wanders to-\nwards \u201cbad\u201d values of \u03b8 for which convergence to equilib-\nrium of the corresponding fixed parameter Markov chains\nP\u03b8might take an arbitrarily long time.\nThis point is detailed in the next section, but we first turn\nto a discussion concerning the possibility of using vanishing\nadaptation in order to circumvent the loss of invariance of \u03c0\nby controlled MCMC.\n3 Vanishing adaptation and convergence\nAs suggested in the previous section, vanishing adaptation,\nthat is ensuring that \u03b8idepends less and less on recently vis-\nited states of the chain {Xi} might be a way of designing\ncontrolled MCMC algorithms which produce samples as-\nymptotically distributed according to \u03c0. In this section we\nprovide the basic arguments and principles that underpin the\nvalidity of controlled MCMC with vanishing adaptation. We\nhowever do not provide directly applicable technical con-\nditions here that ensure the validity of such algorithms -\nmore details can be found in Holden (1998), Atchad\u00e9 and\nRosenthal (2005), Andrieu and Moulines (2006), Roberts\nand Rosenthal (2006), Bai et al. (2008) and Atchad\u00e9 and\nFort (2008). The interest of dedicating some attention to this\npoint here is twofold. First it provides useful guidelines as\nto what the desirable properties of a valid controlled MCMC\nalgorithm should be, and hence help design efficient algo-\nrithms. Secondly it points to some difficulties with the ex-\nisting theory which is not able to fully explain the observed\nstabilitypropertiesofnumerouscontrolledalgorithms,afact\nsometimes overlooked.\n3.1 Principle of the analysis\nExisting approaches to prove that ergodicity might be pre-\nserved under vanishing adaptation all rely on the same prin-\nciple, which we detail in this section. The differences be-\ntween the various existing contributions lies primarily in the\nassumptions, which are discussed in the text. With the nota-\ntion introduced in the previous section, we are interested in\nthe behaviour of the difference\n|\u02c7E\u2217(f(Xi))\u2212E\u03c0(f(X))|\nas i \u2192 \u221e for any f : X \u2192 R. Although general functions\ncan be considered (Atchad\u00e9 and Rosenthal 2005; Andrieu\nand Moulines 2006) and (Atchad\u00e9 and Fort 2008), we will\nhere assume for simplicity of exposition that |f| \u2264 1. The\nstudy of this term is carried out by comparing the process\nof interest to a process which coincides with {Xk} up to\nsome time ki< i but becomes a time homogeneous Markov\nchainwith\u201cfrozen\u201dtransitionprobability P\u03b8kifromthistime\ninstant onwards. (We hereafter use the following standard\nnotation Pk[f](x) = Pkf(x) for any f : X \u2192 Rnfand\nx \u2208 X defined recursively as P0f(x) = f(x), Pf(x) :=?\nthe finite discrete case this corresponds to considering pow-\ners Pkof the transition matrix P and right multiplying with\na vector f.) Denoting Pi\u2212ki\n\u03b8ki\nafter i \u2212 ki iterations of the \u201cfrozen\u201d time homogeneous\nMarkov transition probability P\u03b8kiinitialised with Xkiat\ntime kiand conditional upon \u03b80,X0,X1,...,Xki, this trans-\nlates into the fundamental decomposition\nX\nP(x,dy)f(y) and Pk+1f(x) = P[Pkf](x) for k \u2265 1. In\nf(Xki) the expectation of f\n\u02c7E\u2217(f(Xi))\u2212E\u03c0(f(X))\n=\u02c7E\u2217\n+\u02c7E\u2217\n?\nPi\u2212ki\n\u03b8ki\n?\nf(Xki)\u2212\u03c0(f)\nf(Xi)\u2212Pi\u2212ki\n?\n\u03b8ki\nf(Xki)\n?\n,\n(6)\nwhere the second term corresponds to the aforementioned\ncomparison and the first term is a simple remainder term.\nPerhaps not surprisingly the convergence to zero of the\nfirst term, provided that i \u2212 ki\u2192 \u221e as i \u2192 \u221e, depends\non the ergodicity of the non-adaptive MCMC chain with\nfixed parameter \u03b8 \u2208 ?, i.e. requires at least that for any\n\u03b8,x \u2208 ?\u00d7X, limk\u2192\u221e|Pk\nboth \u03b8kiand Xkiare random and possibly time dependent,\nthis type of simple convergence is not sufficient to ensure\nconvergence of this term. One could suggest the following\nuniform convergence condition\n\u03b8f(x)\u2212\u03c0(f)| = 0.Howeversince\nlim\nk\u2192\u221e\nsup\n\u03b8,x\u2208?\u00d7X\n|Pk\n\u03b8f(x)\u2212E\u03c0(f(X))| = 0,\n(7)\nwhich although mathematically convenient is unrealistic in\nmost scenarios of interest. The first toy example of Sect. 2\nprovides us with such a simple counterexample. Indeed, at\nleast intuitively, convergence of this Markov chain to equi-\nlibrium can be made arbitrarily slow for values of \u03b8 \u2208 (0,1)\narbitrarily close to either 0 or 1. This negative property un-\nfortunately carries on to more realistic scenarios. For ex-\nample the normal symmetric random walk Metropolis al-\ngorithm described in Sect. 1 can in most situations of in-\nterest be made arbitrarily slow as the variance \u03b82is made\narbitrarily small or large. This turns out to be a fundamental\ndifficulty of the \u201cchicken and egg\u201d type in the study of the\nstability of such processes, which is sometimes overlooked.\nIndeed in order to ensure ergodicity, {\u03b8i} should stay away\nfrom poor values of the parameter \u03b8 \u2208 ?, but proving the\nstability of {\u03b8i} might often require establishing the ergod-\nicity of the chain {Xi}; see Andrieu and Moulines (2006)\nand Andrieu and Tadi\u00b4 c (2007) where alternative conditions"},{"page":7,"text":"Stat Comput (2008) 18: 343\u2013373349\nare also suggested. We will come back to this point af-\nter examining the second term of the decomposition above.\nNote that \u201clocally uniform\u201d such conditions (i.e. where ?\nin (7) is replaced by some subsets K \u2282 ? and the rate\nof convergence might be slower) are however satisfied by\nmany algorithms\u2014this property is exploited in Andrieu and\nMoulines (2006), Andrieu and Tadi\u00b4 c (2007), Atchad\u00e9 and\nFort (2008) and Bai et al. (2008) although this is not explicit\nin the latter.\nThe second term in the decompositioncan be analysedby\n\u201cinterpolating\u201d the true process and its Markovian approxi-\nmation using the following telescoping sum\n?\n=\nj=ki\n\u02c7E\u2217(f(Xi))\u2212\u02c7E\u2217\ni\u22121\n?\nwhich can be easily understood as follows. Each term\nof the sum is the difference of the expectations of (a) a\nprocess that adapts up to time j + 1 > kiand then freezes\nand \u201cbecomes Markovian\u201d with transition probability P\u03b8ki\ngiven the history \u03b80,X0,X1,...,Xj+1 (and hence \u03b8ki=\n\u03b8ki(\u03b80,X0,X1,...,Xki)) between time j + 1 and time i\n(b) and likewise for the second term, albeit between time j\nand i. Hence the two terms involved only differ in that at\ntime j the first term updates the chain with \u03b8j while the\nsecond term uses \u03b8ki, which can be concisely expressed as\nfollows (thinking about the difference between two products\nof matrices in the discrete case might be helpful),\n?\n=\u02c7E\u2217\n=\u02c7E\u2217\nPi\u2212ki\n\u03b8ki\nf(Xki)\n?\n?\n\u02c7E\u2217\n?\nPi\u2212j\u22121\n\u03b8ki\nf(Xj+1)\n\u2212\u02c7E\u2217\n?\nPi\u2212j\n\u03b8ki\nf(Xj)\n?\n,\n\u02c7E\u2217\nPi\u2212j\u22121\n\u03b8ki\nf(Xj+1)\nP\u03b8jPi\u2212j\u22121\n\u03b8ki\n??\n?\nf(Xj)\n?\n\u2212\u02c7E\u2217\n?\nPi\u2212j\n\u03b8ki\nf(Xj)\n?\n?\n?\nf(Xj)\n?\n?\n\u2212\u02c7E\u2217\nPi\u2212j\n\u03b8ki\n?\nP\u03b8j\u2212P\u03b8ki\nPi\u2212j\u22121\n\u03b8ki\nf(Xj).\nThe role of vanishing adaptation should now be appar-\nent. Provided that the transition probability P\u03b8 is suffi-\nciently smooth in \u03b8 and that the variations of {\u03b8i} vanish\nas i \u2192 +\u221e (in some unspecified sense at this point) then\nwe might expect\ni\u22121\n?\nto vanish if the number of terms in this sum does not\ngrow too rapidly. However as noticed when analysing the\nfirst term of the fundamental decomposition above, simple\ncontinuity cannot be expected to be sufficient in general\nsince \u03b8ki,\u03b8ki+1,...,\u03b8i\u22121are random and time dependent.\nBy analogy with the analysis above one could assume some\nj=ki\n\u02c7E\u2217\n??\nP\u03b8j\u2212P\u03b8ki\n?\nPi\u2212j\u22121\n\u03b8ki\nf(Xj)\n?\nform of uniform continuity in order to eliminate the vari-\nability of \u03b8jand \u03b8kiin the expression above. More precisely,\ndenoting for any \u03b4 > 0\n?(\u03b4) := sup\n|g|\u22641\nsup\nx\u2208X,{\u03b8,\u03b8?\u2208?:|\u03b8\u2212\u03b8?|\u2264\u03b4}\n|P\u03b8g(x)\u2212P\u03b8?g(x)|,\none could assume,\nlim\n\u03b4\u21920?(\u03b4) = 0.\nProvided that the sequence {\u03b8i} is such that its increments\nare bounded i.e. such that |\u03b8i\u2212 \u03b8i\u22121| \u2264 \u03b3ifor a determin-\nistic sequence {\u03b3i} \u2208 [0,\u221e)N(which is possible since the\nupdates of {\u03b8i} are chosen by the user) then the second term\nof (6) can be bounded by\ni\u22121\n?\nwhich can usually be easily dealt with; e.g. when some uni-\nform Lipschitz continuity is assumed and ?(\u03b4) = C\u03b4 for\nsome constant C > 0 (Andrieu and Moulines 2006). Unfor-\ntunately, although mathematicallyconvenient,this condition\nis not satisfied in numerous situations of interest, due in par-\nticular to the required uniformity in \u03b8,\u03b8?\u2208 ?. Other appar-\nently weaker conditions have been suggested, but share the\nsame practical underlying difficulties such as\nj=ki\nj\n?\nk=ki+1\n?\u03b3k,\nlim\ni\u2192\u221esup\n|g|\u22641\n\u02c7E\u2217\n?|P\u03b8ig(Xi)\u2212P\u03b8i\u22121g(Xi)|?= 0\nsuggested in Benveniste et al. (1990, p. 236) and the slightly\nstronger condition of the type\nlim\ni\u2192\u221e\nsup\nx\u2208X,|g|\u22641\n\u02c7E\u2217\n?|P\u03b8ig(x)\u2212P\u03b8i\u22121g(x)|?= 0,\nin Roberts and Rosenthal (2006).\n3.2 Discussion\nThe simple discussion above is interesting in two respects.\nOn the one hand, using basic arguments, it points to the pri-\nmary conditions under which one might expect controlled\nMCMC algorithms to be \u03c0-ergodic: \u201cexpected ergodicity\nand continuity of the transition probabilities\u201d. On the other\nhand it also points to the difficulty of proposing verifiable\nconditions to ensure that the aforementioned primary condi-\ntions are satisfied. The problem stems from the fact that it is\nrequired to prove that the algorithm is not unlucky enough\nto tune \u03b8 to poor values, leading to possibly unstable algo-\nrithms.Theuniformconditionssuggestedearliercircumvent\nthis problem, since they suggest that there are no such arbi-\ntrarily \u201cbad\u201d values. This is unfortunately not the case for"},{"page":8,"text":"350 Stat Comput (2008) 18: 343\u2013373\nnumerous algorithms of practical interest. However it is of-\nten the case that such uniformity holds for subsets K \u2282 ?.\nFor example in the case of the first toy exampleof Sect. 2 the\nsets defined as K?:= [?,1 \u2212 ?] for any ? \u2208 (0,1) are such\nthat for any ? \u2208 (0,1) the Markov chains with parameters\n\u03b8 \u2208 K?are geometrically convergent with a rate of at least\n|1\u22122?|, independent of \u03b8 \u2208 K?. A simple practical solution\nthus consists of identifying such subsets K and constrain\n{\u03b8i} to these sets by design. This naturally requires some un-\nderstanding of the problem at hand, which might be difficult\nin practice, and does not reflect the fact that stability is ob-\nserved in practice without the need to resort to such \u201cfixed\u201d\ntruncation strategies. A general approach for adaptive trun-\ncation is developed and analysed in Andrieu et al. (2005)\nand Andrieu and Moulines (2006). It takes advantage of the\nfact that uniform ergodicity and continuity can be shown for\nfamilies of subsets of {Ki\u2282 ?}, such that {Ki\u2282 ?} is a\ncovering of ? such that Ki\u2282 Ki+1for i \u2265 0. The strategy\nthen consists of adapting the truncation of the algorithm on\nthe fly in order to ensure that {\u03b8i} does not wander too fast\ntowards inappropriate values in ? or at its boundary, hence\nensuring that ergodicity can kick in and stabilise the trajec-\ntories of {\u03b8i}, i.e. ensure that there is a random, but finite,\nk such that {\u03b8i} \u2282 Kk with probability 1. The procedure\nhas the advantage of not requiring much knowledge about\nwhat constitutes good or bad values for \u03b8, while allowing\nfor the incorporation of prior information and ultimately en-\nsuring stability. It can in fact be shown, under conditions\nsatisfied by some classes of algorithms, that the number k of\nreprojections needed for stabilisation is a random variable\nwith probability distribution whose tails decay superexpo-\nnentially. While this approach is general and comes with a\ngeneral and applicable theory, it might be computationally\nwasteful in some situations and more crucially does not re-\nflect the fact that numerous algorithm naturally present sta-\nbility properties. Another possibility consists of considering\nmixtures of adaptive and non-adaptive MCMC proposal dis-\ntributions, the non adaptive components ensuring stability\ne.g. (Roberts and Rosenthal 2007): again while this type of\nstrategy generally ensures that the theory works, it poses the\nproblem of the practical choice of the non-adaptive compo-\nnent, and might not always result in efficient strategies. In\naddition, as for the strategies discussed earlier, this type of\napproach fails to explain the observed behaviour of some\nadaptive algorithms.\nIn a number of situations of interest it is possible to show\nthat the parameter \u03b8 stays away from its forbidden values\nwith probability one (Andrieu and Tadi\u00b4 c 2007). The ap-\nproach establishes a form of recurrence via the existence of\ncomposite drift functions for the joint chain {\u03b8i,Xi}, which\nin turn ensures an infinite number of visits of {\u03b8i} to some\nsets K for which the uniform properties above hold. This\ncan be shown to result in stability under fairly general con-\nditions. In addition the approach provides one with some\ninsight into what makes an algorithm stable or not, and sug-\ngests numerous ways of designing updates for {\u03b8i} which\nwill ensure stability and sometimes even accelerate conver-\ngence. Some examples are discussed in Sect. 4.2.2. In Saks-\nman and Vihola (2008) the authors address the same prob-\nlem by proving that provided that {\u03b8i} is constrained not to\ndrift too fast to bad values, then \u03c0-ergodicity of {Xi} is pre-\nserved. The underlying ideas are related to a general strategy\nof stabilisation developed for the stochastic approximation\nprocedure, see Andrad\u00f3ttir (1995).\nFinally we end this section with a practical implication\nof the developments above, related to the rate of conver-\ngence of controlled MCMC algorithms. Assume for exam-\nple the existence of K \u2282 ?, C \u2208 (0,\u221e), \u03c1 \u2208 (0,1) and\n{\u03b3i} \u2208 [0,\u221e)Nsuch that for all i \u2265 1, \u03b8,x \u2208 K \u00d7 X and any\nf : X \u2192 [\u22121,1]\n|Pi\nand for any \u03b8,\u03b8?\u2208 K, x \u2208 X and any f : X \u2192 [\u22121,1],\n|P\u03b8f(x)\u2212P\u03b8?f(x)| \u2264 C|\u03b8 \u2212\u03b8?|,\nand such that for all i \u2265 1, |\u03b8i\u2212 \u03b8i\u22121| \u2264 \u03b3i, where {\u03b3i}\nsatisfies a realistic assumption of slow decay (Andrieu and\nMoulines2006)(satisfiedforexamplefor\u03b3i= 1\/i\u03b1,\u03b1 > 0).\nThese conditions are far from restrictive and can be shown\nto hold for the symmetric random walk Metropolis (SRWM)\nfor some distributions \u03c0, the independent Metropolis-\nHastings (IMH) algorithm, mixtures of such transitions etc.\n(Andrieu and Moulines 2006). Less restrictive conditions\nare possible, but lead to slower rates of convergence. Then,\nusing a more precise form of the decomposition in (6) (An-\ndrieu and Moulines 2006, Proposition 4), one can show that\nthere exists a constant C?\u2208 (0,\u221e) such that for all i \u2265 1\nand |f| \u2264 1,\n???\u02c7E\u2217\nbe infinity). The result simply tells us that while the adapted\nparameter does not leave K, convergence towards \u03c0 occurs\nat a rate of at least {\u03b3i}, and as pointed out in Andrieu and\nMoulines (2006) does not require convergence of {\u03b8i}. This\nmight appear to be a negative result. However it can be\nproved, (Andrieu 2004) and (Andrieu and Moulines 2006),\nthat there exist constants A(\u03b3,K) and B(\u03b3,K) such that for\nany N \u2265 1,\n?\nN\ni=1\n\u2264A(\u03b3,K)\nN\n\u03b8f(x)\u2212E\u03c0(f)| \u2264 C\u03c1i,\n(8)\n(9)\n?(f(Xi)\u2212E\u03c0(f))I{\u03c3 \u2265 i}???? \u2264 C?\u03b3i,\n(10)\nwhere \u03c3 is the first time at which {\u03b8i} leaves K (which can\n?\n?\n?\u02c7E\u2217\n??????\n1\nN\n?\nf(Xi)\u2212E\u03c0(f)\n?????\n2\nI{\u03c3 \u2265 n}\n?\n\u221aN\n+B(\u03b3,K)\n?N\nk=1\u03b3k\n.\n(11)"},{"page":9,"text":"Stat Comput (2008) 18: 343\u2013373 351\nThe first term corresponds to the Monte Carlo fluctuations\nwhile the second term is the price to pay for adaptation. As-\nsuming that \u03b3i= i\u2212\u03b1for \u03b1 \u2208 (0,1), then\n?N\nN\nwhich suggests no loss in terms of rate of convergence for\n\u03b1 \u2265 1\/2. More general and precise results can be found in\nAndrieu and Moulines (2006, Proposition 6), including a\ncentral limit theorem (Theorem 9) which shows the asymp-\ntotic optimality of adaptive MCMC algorithms when con-\nvergence of {\u03b8i} is ensured . Weaker rates of convergence\nthan (8) lead to a significant loss of rate of convergence,\nwhich is also observed in practice.\nk=1\u03b3k\n\u223c\n1\n1\u2212\u03b1N\u2212\u03b1,\n4 Vanishing adaptation: a framework for consistent\nadaptive MCMC algorithms\nIn the previous section we have given arguments that sug-\ngest that vanishing adaptation for MCMC algorithms might\nlead to algorithms from which expectations with respect to\na distribution of interest \u03c0 can be consistently estimated.\nHowever neither criteria nor ways of updating the parameter\n\u03b8 were described. The main aim of this section is to point\nout the central role played by stochastic approximation and\nthe Robbins-Monro recursion (Robbins and Monro 1951) in\nthe context of vanishing or non-vanishing adaptation. While\na complete treatment of the theoretical aspects of such con-\ntrolled MCMC algorithms is far beyond the scope of this\nreview, our main goal is to describe the principles underpin-\nning this approach that have a practical impact and to show\nthe intricate link between criteria and algorithms. Indeed, as\nwe shall see, while the stochastic approximation framework\ncan be used in order to optimise a given criterion, it can also\nhelp understand the expected behaviour of an updating al-\ngorithm proposed without resorting to grand theory, but by\nsimply resorting to common sense.\n4.1 Criteria to optimise MCMC algorithms and a general\nform\nSince our main aim is that of optimising MCMC transi-\ntion probabilities, the first step towards the implementation\nof such a procedure naturally consists of defining what is\nmeant by optimality, or suboptimality. This can be achieved\nthrough the definition of a cost function, which could for ex-\nample express some measure of the statistical performance\nof the Markov chain in its stationary regime e.g. favour neg-\native correlation between Xi and Xi+lfor some lag l and\ni = 0,1,.... In what follows we will use the convention that\nan optimum value \u03b8\u2217corresponds to a root of the equation\nh(\u03b8) = 0 for some function h(\u03b8) closely related to the afore-\nmentioned cost function.\nSince the main use of MCMC algorithms is to compute\naverages of the form\u02c6I\u03b8\nE\u03c0(f(X)),in situationswhereacentrallimittheoremholds,\ni.e. in scenarios such that for any \u03b8 \u2208 ?\n\u221aN (\u02c6I\u03b8\nN(f) given in (1) in order to estimate\nN(f)\u2212E\u03c0(f(X))) \u2192DN(0,\u03c32\nit might seem natural to attempt to optimise the constant\n\u03c32\n\u03b8(f). This however poses several problems. The first prob-\nlem is computational. Indeed, for a given \u03b8 \u2208 ? and f :\nX :\u2192 [\u22121,1] (for simplicity) and when it exists, \u03c32\nbe shown to have the following expression\n\u03b8(f)),\n\u03b8(f) can\n\u03c32\n\u03b8(f) = E\u03c0(\u00af f2(X0))+2\n+\u221e\n?\nk=1\nE\u03b8(\u00af f(X0)\u00af f(Xk))\n= E\u03c0(\u00af f2(X0))+2E\u03b8\n?+\u221e\nk=1\n?\n\u00af f(X0)\u00af f(Xk)\n?\n,\n(12)\nwith \u00af f(x) := f(x) \u2212 E\u03c0(f(X)) and E\u03b8the expectation\nassociated to the Markov chain with transition probability\nP\u03b8 and such that X0\u223c \u03c0. This quantity is difficult to esti-\nmate and optimise (since for all \u03b8 \u2208 ? it is the expectation\nof a non-trivial function with respect to an infinite set of\nrandom variables) although some solutions exist (Vladislav\nTadi\u00b4 c, personal communication, see also Richard Everitt\u2019s\nPh.D. thesis) and truncation of the infinite sum is also possi-\nble (Andrieu and Robert 2001; Pasarica and Gelman 2003),\nallowing for example for the recursive estimation of the\ngradient of \u03c32\n\u03b8(f) with respect to \u03b8. In Pasarica and Gel-\nman (2003), maximising the expected mean square jump\ndistance is suggested, i.e. here in the scalar case and with\n\u00afXi= Xi\u2212E\u03c0(X) for i = 0,1,\nE\u03b8?\n= 2\n(X0\u2212X1)2?\n= E\u03b8??\u00afX0\u2212\u00afX1\nE\u03c0\n?2?\n\u2212E\u03b8?\u00afX0\u00afX1\n?\n?\n\u00afX2?\n??\n(13)\nwhich amounts to minimising the term corresponding to\nk = 1 in (12) for the function f(x) = x for all x \u2208 X. An-\nother difficulty is that the criterion depends on a specific\nfunction f, and optimality for a function f might not re-\nsult in optimality for another function g. Finally it can be\nargued that although optimising this quantity is an asymp-\ntotically desirable criterion, at least for a given function, this\ncriterion can in some scenarios lead to MCMC samplers that\nare slow to reach equilibrium (Besag and Green 1993).\nDespite the difficulties pointed out earlier, the criterion\nabove should not be totally discarded, but instead of try-\ning to optimise it directly and perfectly, suboptimal op-\ntimisation through proxies that are amenable to simple\ncomputation and efficient estimation might be preferable."},{"page":10,"text":"352Stat Comput (2008) 18: 343\u2013373\nSuch a simple criterion, which is at least completely sup-\nported by theory in some scenarios (Roberts et al. 1997;\nSherlock and Roberts 2006; Roberts and Rosenthal 1998;\nB\u00e9dard 2006) and proves to be more universal in practice,\nis the expected acceptance probability of the MH algorithm\nfor random walk Metropolis algorithms or Langevin based\nMH updates. The expected acceptance probability is more\nformally defined as the jump rate of a MH update in the sta-\ntionary regime\n\u00af \u03b1\u03b8:=\n?\nX2min\n?\n1,\u03c0 (y)q\u03b8(y,x)\n\u03c0 (x)q\u03b8(x,y)\n?\n?\n\u03c0 (x)q\u03b8(x,y)dxdy\n= E\u03c0\u2297q\u03b8\n?\nmin1,\u03c0 (Y)q\u03b8(Y,X)\n\u03c0 (X)q\u03b8(X,Y)\n??\n.\n(14)\nThis criterion has several advantages. The first one is com-\nputational, since it is much simpler an expectation of a much\nsimpler function than \u03c32\n\u03b8(f). In such cases it has the double\nadvantage of being independent of any function f and to\nprovide a good compromise for \u03c32\nA less obvious advantage of this criterion, which we illus-\ntrate later on in Sect. 5, is that where some form of smooth-\nness of the target density is present it can be beneficial in\nthe initial stages of the algorithm in order to ensure that the\nadaptive algorithm actually starts exploring the target distri-\nbution in order to \u201clearn\u201d some of its features.\nThe aforementioned theoretical results tell us that opti-\nmality of \u03c32\n\u03b8(f) (in terms of \u03b8) or proxy quantities related\nto this quantity (truncation, asymptotics in the dimension)\nis reached for a specific value of the expected acceptance\nprobability \u00af \u03b1\u03b8, denoted \u03b1\u2217hereafter: 0.234 for the random\nwalk Metropolis algorithm for some specific target distribu-\ntions and likewise 0.574 for Langevin diffusion based MH\nupdates (Roberts and Rosenthal 1998).\nIn some situations, Gelman et al. (1995) have shown that\nthe \u201coptimal\u201d covariance matrix for a multivariate random\nwalk Metropolis algorithm with proposal N(0,?) is ? :=\n(2.382\/nx)?\u03c0, where ?\u03c0 is the covariance matrix of the\ntarget distribution \u03c0\n\u03b8(f) for all functions f.\n?\u03c0= E\u03c0\n?XXT?\u2212E\u03c0(X)ET\nThe covariance is unknown in general situations and re-\nquires the numerical computation of the pair\n\u03c0(X).\n?E\u03c0(X),E\u03c0\nAs pointed out in Andrieu and Moulines (2006, Sect. 7), this\ncan also be interpreted as minimising the Kullback-Leibler\ndivergence\n?XXT??= E\u03c0\n?(X, XXT)?.\n(15)\n?\nX\n\u03c0(x)log\n\u03c0(x)\nN(x;\u03bc,?)dx = E\u03c0\n?\nlog\n\u03c0(X)\nN(X;\u03bc,?)\n?\n,\nwhich suggests generalisations consisting of minimising\nE\u03c0\n?\nlog\u03c0(X)\nq\u03b8(X)\n?\n,\n(16)\nin general, for some parametric family of probability dis-\ntributions {q\u03b8,\u03b8 \u2208 ?}. Section 7 of Andrieu and Moulines\n(2006) is dedicated to the development or an on-line EM\nalgorithm and a theoretical analysis of an adaptive indepen-\ndent MH algorithm where q\u03b8is a general mixture of distri-\nbutions belonging to the exponential family. We will come\nback to this strategy in Sect. 5.2.2 where we show that this\nprocedure can also be used in order to cluster the state-space\nX and hence define locally adaptive algorithms.\nBefore turning to ways of optimising criteria of the type\ndescribed above, we first detail a fundamental fact shared by\nall the criteria described above and others, which will allow\nus to describe a general procedure for the control of MCMC\nalgorithms. The shared characteristic is naturally that all the\ncriteria developed here take the form of an expectation with\nrespect to some probability distribution dependenton \u03b8 \u2208 ?.\nIn fact as we shall see optimality can often be formulated as\nthe problem of finding the root(s) of an equation of the type\nh(\u03b8) := E\u03b8(H(\u03b8,X0,Y1,X1,...)) = 0(17)\n(remember that {Yi} is the sequence of proposed samples)\nfor some function ?\u00d7XN:\u2192 Rnhfor some nh\u2208 N, with in\nmany situations nh= n\u03b8 (but not always). The case of the\ncoerced acceptance probability corresponds to\nH(\u03b8,X0,Y1,X1,...) = min\n?\n1,\u03c0 (Y1)q\u03b8(Y1,X0)\n\u03c0 (X0)q\u03b8(X0,Y1)\n?\n\u2212\u03b1\u2217,\nwhich according to (14) results in the problem of finding the\nzero(s) of h(\u03b8) = \u00af \u03b1\u03b8\u2212 \u03b1\u2217. The moment matching situation\ncorresponds to\nH(\u03b8,X) = (X,XXT)\u2212(\u03bc,?)\nfor which it is sought to find the zeros of h(\u03b8) = (\u03bc\u03c0,?\u03c0)\u2212\n(\u03bc,?) i.e. simply (\u03bc\u03c0,?\u03c0) (naturally assuming that the\ntwo quantities exist). It might not be clear at this point how\noptimising the remaining criteria above might amount to\nfinding the zeros of a function of the form (17). However,\nunder smoothness assumptions, it is possible to consider the\ngradients of those criteria (note however that one might con-\nsider other methods than gradient based approaches in order\nto perform optimisation).In the case of the Kullback-Leibler\ndivergence,andassumingthatdifferentiationandintegration\ncan be swapped, the criterion can be expressed as\nE\u03c0\n?\n\u2207\u03b8log\u03c0(X)\nq\u03b8(X)\n?\n= 0 (18)"},{"page":11,"text":"Stat Comput (2008) 18: 343\u2013373 353\nthat is\nH(\u03b8,X) = \u2207\u03b8log\u03c0(X)\nq\u03b8(X)\nand in the more subtle case of the first order autocovariance\nminimisation one can invoke a standard score function argu-\nment and find the zeros of (in the scalar case for simplicity)\n?\u2207\u03b8P\u03b8(X0,X1)\nSimilarly, under smoothness assumptions, one can differen-\ntiate \u03c32\nof the form (17). Note that when q\u03b8is a mixture of distrib-\nution belonging to the exponential family, then it is possible\nto find the zeros (assumed here to exist) of (18) using an\non-line EM algorithm (Andrieu and Moulines 2006).\nNote that all the criteria described above are \u201csteady\nstate\u201d criteria and explicitly involve \u03c0, but that other cri-\nteria such as the minimisation of return times to a given set\nC \u2282 X (Andrieu and Doucet 2003), namely\n?\u221e\ni=1\nwith \u03bb a probability measure concentrated on C, do not en-\nter this category. Such criteria seem however difficult to op-\ntimise in practice and we do not pursue this.\n\u2207\u03b8E\u03b8?\u00afX0\u00afX1\n?= E\u03b8\nP\u03b8(X0,X1)\nX0X1\n?\n= 0.\n\u03b8(f) and obtain a theoretical expression for \u2207\u03b8\u03c32\n\u03b8(f)\n\u03c4 =\u02c7E\u03b8\n\u03bb\n?\nI{Xi\/ \u2208 C}\n?\n4.2 The stochastic approximation framework\nWe dedicate here a section to the Robbins-Monro update,\nwhich although not the only possibility to optimise criteria\nof the type (17) appears naturally in most known adaptive\nalgorithms and provides us with a nice framework naturally\nconnected to the literature on controlled Markov chains in\nthe engineering literature. The reason for its ubiquity stems\nfrom the trivial identity: \u03b8i+1= \u03b8i+ \u03b8i+1\u2212 \u03b8i. This turns\nout to be a particularly fruitful point of view in the present\ncontext. More precisely, it is well suited to sequential up-\ndating of {\u03b8i} and makes explicit the central role played by\nthe updating rule defining the increments {\u03b8i+1\u2212 \u03b8i}. In\nlight of our earlier discussion {\u03b8i+1\u2212 \u03b8i} should be van-\nishing, and when convergence is of interest their cumula-\ntive sums should also vanish (in some probabilistic sense) in\nthe vicinity of optimal values \u03b8\u2217. Naturally, although con-\nvenient, this general framework should not prevent us from\nthinking \u201coutside of the box\u201d.\n4.2.1 Motivating example\nConsider the case where X = R and a symmetric ran-\ndom walk Metropolis (SRWM) algorithm with normal in-\ncrement distribution N(z;0,exp(\u03b8)), resulting in a tran-\nsition probability PNSRW\ntions (Roberts et al. 1997) the expected acceptance prob-\nability should be in a range close to \u00af \u03b1\u2217= 0.44. We will\nassume for simplicity that \u00af \u03b1\u03b8 in (14) is a non-increasing\nfunction of \u03b8 (which is often observed to be true, but\ndifficult to check rigourously in practice and can further-\nmore be shown not to hold in some situations, Hastie\n2005). In such situations one can suggest the following\nintuitive algorithm. For an estimate \u03b8i\u2208 ? obtained af-\nter i \u00d7 L iterations of the controlled MCMC algorithm,\none can simulate L iterations of the transition probability\nPNSRW\n\u03b8i\nand estimate the expected acceptance probability\nfor such a value of the parameter for the i-th block of sam-\nples {XiL+1,YiL+1,...,XiL+L,YiL+L,k = 1,...,L} (ini-\ntialised with Xi)\n?\nand update \u03b8iaccording to the following rule, motivated by\nour monotonicity assumption on \u00af \u03b1\u03b8: if \u02c6 \u03b1\u03b8i> \u00af \u03b1\u2217then \u03b8iis\nprobably (\u02c6 \u03b1\u03b8iis only an estimator) too small and should be\nincreased while if \u02c6 \u03b1\u03b8i< \u00af \u03b1\u2217then \u03b8i should be decreased.\nThere is some flexibility concerning the amount by which\n\u03b8ishould be altered and depends either on the criterion one\nwishes to optimise or more heuristic considerations. How-\never, as detailed later, this choice will have a direct influ-\nence on the criterion effectively optimised and in light of\nthe discussion of Sect. 3 concerning diminishing adaptation,\nthis amount of change should diminish as i \u2192 \u221e in order\nto either ensure that \u03c0-ergodicity of {Xi} is ensured or that\n\u201capproximate convergence\u201d of {\u03b8i} is ensured. The intuitive\ndescription given above can suggest the following updating\nrules (see also Gilks et al. 1998, Andrieu and Robert 2001,\nAtchad\u00e9 and Rosenthal 2005 for similar rules)\n\u03b8\n. We know that in some situa-\n\u02c6 \u03b1\u03b8i=1\nL\n?L\nk=1min1,\n\u03c0(YiL+k)\n\u03c0(XiL+k\u22121)\n?\n\u03b8i+1= \u03b8i+\u03b3i+1\n?I?\u02c6 \u03b1\u03b8i\u2212 \u00af \u03b1\u2217> 0?\u2212I?\u02c6 \u03b1\u03b8i\u2212 \u00af \u03b1\u2217\u2264 0??\n(19)\nor\n\u03b8i+1= \u03b8i+\u03b3i+1\nwhere {\u03b3i} \u2282 (0,+\u221e)Nis a sequence of possibly stochas-\ntic stepsizes which ensures that the variations of {\u03b8i} van-\nish. The standard approach consists of choosing the se-\nquence {\u03b3i} deterministic and non-increasing, but it is also\npossible to choose {\u03b3i} random e.g. such that it takes val-\nues in {\u03b4,0} for some \u03b4 > 0 and such that P(\u03b3i= \u03b4) = pi\nwhere {pi} \u2282 [0,1]Nis a deterministic and non-increasing\nsequence (Roberts and Rosenthal 2007), although it is not\nalways clear what the advantage of introducing such an ad-\nditional level of randomness is. A more interesting choice in\npractice consists of choosing {\u03b3i} adaptively, see Sect. 4.2.2,\n?\u02c6 \u03b1\u03b8i\u2212 \u00af \u03b1\u2217?,\n(20)"},{"page":12,"text":"354 Stat Comput (2008) 18: 343\u2013373\nbut for simplicity of exposition we focus here on the deter-\nministic case.\nWe will come back to the first updating rule later on, and\nnow discuss the second rule which as we shall see aims to\nset (14) equal to \u00af \u03b1\u2217. Notice first that if L \u2192 \u221e and the un-\nderlying Markov chain is ergodic, then \u02c6 \u03b1\u03b8\u2192 \u00af \u03b1\u03b8and the re-\ncursion becomes deterministic\n\u03b8i+1= \u03b8i+\u03b3i+1\n?\u00af \u03b1\u03b8i\u2212 \u00af \u03b1\u2217?\n(21)\nand is akin to a standard gradient algorithm, which will con-\nverge under standard conditions. Motivated by this asymp-\ntotic result, one can rewrite the finite L recursion (20) as\nfollows\n\u03b8i+1= \u03b8i+\u03b3i+1\n?\u00af \u03b1\u03b8i\u2212 \u00af \u03b1\u2217?+\u03b3i+1\n?\u02c6 \u03b1\u03b8i\u2212 \u00af \u03b1\u03b8i\n?.\n(22)\nAssuming for simplicity that there exists \u03b8\u2217\u2208\nterior of ?, such that \u00af \u03b1\u03b8\u2217 = \u00af \u03b1\u2217and that \u02c6 \u03b1\u03b8iis unbiased,\nat least as i \u2192 \u221e. Then, since |\u03b8i+1\u2212 \u03b8i| \u2264 \u03b3i+1\u2192 0 as\ni \u2192 \u221e, and provided that \u02c6 \u03b1\u03b8\u2212 \u00af \u03b1\u03b8 is smooth in terms of\n\u03b8 \u2208 ? the sequence of noise terms {\u02c6 \u03b1\u03b8i\u2212 \u00af \u03b1\u03b8i} is expected\nto average out to zero (i.e. statistically, positive increments\nare compensated by negative increments) and we expect the\ntrajectory of (22) to oscillate about the trajectory of (21),\nwith the oscillations vanishing as i \u2192 \u221e. This is the main\nidea at the core of the systematic analysis of such recursions\nwhich, as illustrated below, has an interest even for practi-\ntioners. Indeed, by identifying the underlying deterministic\nrecursion which is approximated in practice, it allows one to\nunderstand and predict the behaviour of algorithms, even in\nsituations where the recursion is heuristically designed and\nthe underlying criterion not explicit. Equation (20) suggests\nthat stationary points of the recursion should be such that\n\u00af \u03b1\u03b8\u2217 = \u00af \u03b1\u2217. The stationary points of the alternative recursion\n(19) are given in the next subsection.\nIn general most of the recursions of interest can be recast\nas follows,\n\u25e6\n?, the in-\n\u03b8i+1= \u03b8i+\u03b3i+1Hi+1(\u03b8i,X0,...,Yi,Xi,Yi+1,Xi+1) (23)\nwhere Hi+1(\u03b8,X0,...,Yi,Xi,Yi+1,Xi+1) takes its values\nin ?. Typically in practice {Hi+1} is a time invariant se-\nquence of mappings which in effect only depends on a fixed\nand finite number of arguments through time invariant sub-\nsets of {Yi,Xi} (e.g. the last L of them at iteration i, as\nabove). For simplicity we will denote this mapping H and\ninclude all the variables \u03b8i,X0,...,Yi,Xi,Yi+1,Xi+1as an\nargument, although the dependence will effectively be on\na subgroup. Considering sequences {Hi(\u03b8,X0,...,Yi,Xi,\nYi+1,Xi+1)} with a varying numbers of arguments is possi-\nble (and needed when trying to optimise (12) directly), but\nat the expense of additional notation and assumptions.\n4.2.2 Why bother with stochastic approximation?\nIn this subsection we point to numerous reasons why the\nstandard framework of stochastic approximation can be use-\nful in order to think about controlled MCMC algorithms:\nas we shall see motivations range from theoretical to practi-\ncal or implementational, and might help shed some lights on\npossibly heuristically developed strategies. Again, although\nthis framework is very useful and allows for a systematic ap-\nproach to the development and understanding of controlled\nMCMC algorithms, and despite the fact that this framework\nencompasses most known procedures, it should however not\nprevent us from thinking differently.\nA standardized framework for programming and analysis\nApart from the fact that the standard form (23) allows for\nsystematic ways of coding the recursions, in particular the\ncreationof\u201cobjects\u201d,theapproachallowsforanunderstand-\ning of the expected behaviour of the recursion using sim-\nple mathematical arguments as well as the development of a\nwealthofveryusefulvariations,madepossiblebytheunder-\nstanding of the fundamental underlying nature of the recur-\nsions. As suggested above with a simple example (21)\u2013(22)\nthe recursion (23) can always be rewritten as\n\u03b8i+1= \u03b8i+\u03b3i+1h(\u03b8i)+\u03b3i+1\u03bei+1,\nwhere h(\u03b8) is the expectation in steady state for a fixed \u03b8 \u2208\n? of H(\u03b8,X0,...,Yi,Xi,Yi+1,Xi+1), i.e.\nh(\u03b8) := E\u03b8(H (\u03b8,X0,...,Yi,Xi,Yi+1,Xi+1))\nand \u03bei+1:= H(\u03b8i,X0,...,Yi,Xi,Yi+1,Xi+1) \u2212 h(\u03b8i) is\nusually referred to as the \u201cnoise\u201d. The recursion (24) can\ntherefore be thought of as being a noisy gradient algorithm.\nIntuitively, if we rearrange the terms in (24)\n(24)\n\u03b8i+1\u2212\u03b8i\n\u03b3i+1\nwe understand that provided that the noise increments \u03bei\n\u201ccancel out on average\u201d, then a properly rescaled continu-\nous interpolation of the recursion \u03b80,\u03b81,... should behave\nmore or less like the solutions \u03b8(t) of the ordinary differen-\ntial equation\n= h(\u03b8i)+\u03bei+1,\n\u02d9\u03b8 (t) = h(\u03b8 (t)),\nwhose stationary points are precisely such that h(\u03b8) = 0.\nThe general theory of stochastic approximation consists of\nestablishing that the stationary points of (24) are related\nto the stationary points of (25) and that convergence oc-\ncurs provided that some conditions concerning {\u03b3i},h(\u03b8)\nand {\u03bei} are satisfied. While this general theory is rather in-\nvolved, it nevertheless provides us with a useful recipe to\n(25)"},{"page":13,"text":"Stat Comput (2008) 18: 343\u2013373 355\ntry to predict and understand some heuristically developed\nalgorithms. For example it is not clear what criterion is actu-\nally optimised when using the updating rule (19). However\nthe \u201cmean field\u201d approach described above can be used to\ncompute\nh(\u03b8) = E\u03b8(H (\u03b8,X0,...,Yi,Xi,Yi+1,Xi+1))\n= E\u03b8?I?\u02c6 \u03b1\u03b8\u2212 \u00af \u03b1\u2217> 0?\u2212I?\u02c6 \u03b1\u03b8\u2212 \u00af \u03b1\u2217\u2264 0??\n= P\u03b8?\u02c6 \u03b1\u03b8\u2212 \u00af \u03b1\u2217> 0?\u2212P\u03b8?\u02c6 \u03b1\u03b8\u2212 \u00af \u03b1\u2217\u2264 0?.\nIts zeros (the possible stationary points of the recursion) are\nsuch that P\u03b8(\u02c6 \u03b1\u03b8\u2212 \u00af \u03b1\u2217> 0) = P\u03b8(\u02c6 \u03b1\u03b8\u2212 \u00af \u03b1\u2217\u2264 0) = 1\/2, i.e.\nthe stationary points \u03b8\u2217are such that \u00af \u03b1\u2217is the median of the\ndistribution of \u02c6 \u03b1\u03b8 in steady-state, which seems reasonable\nwhen this median is not too different from \u00af \u03b1\u03b8\u2217 given our ini-\ntial objective. In addition this straightforward analysis also\ntells us that the algorithm will have the desired gradient like\nbehaviour when P\u03b8(\u02c6 \u03b1\u03b8\u2212 \u00af \u03b1\u2217> 0) is a non-increasing func-\ntion of \u03b8. Other examples of the usefulness of the framework\nto design and understand such recursions are given later in\nSect. 5, in particular Sect. 5.2.2.\nIn addition to allowing for an easy characterisation of\npossible stationary points of the recursion (and hence of the\n\u201cideal\u201d optimal values \u03b8\u2217) the decomposition (24) points to\nthe role played by the deterministic quantity h(\u03b8) to ensure\nthat the sequence {\u03b8i} actually drifts towards optimal values\n\u03b8\u2217, which is the least one can ask from such a recursion,\nand the fact that the noise sequence {\u03bei} should also aver-\nage out to zero for convergence purposes. This latter point\nis in general very much related to the ergodicity properties\nof {Xi}, which justifies the study of ergodicity even in situ-\nations where it is only planned to use the optimised MCMC\nalgorithm with a fixed and suboptimal parameter\u02dc\u03b8 obtained\nafter optimisation. This in turn points to the intrinsic dif-\nficulty of ensuring and proving such ergodicity properties\nbefore {\u03b8i} wanders towards \u201cbad\u201d values, as explained in\nSect. 2. Recent progress in Andrieu and Tadi\u00b4 c (2007), rely-\ning on precise estimates of the dependence in \u03b8 of standard\ndrift functions for the analysis of Markov chains allows one\nto establish that {\u03b8i} stays away from such \u201cbad\u201d values, en-\nsuring in turn ergodicity and a drift of {\u03b8i} towards the set of\nvaluesofinterest \u03b8\u2217.SimilarresultsareobtainedinSaksman\nand Vihola (2008), albeit using totally different techniques.\nFinally note that the developments above stay valid in the\nsituation where {\u03b3i} is set to a constant, say \u03b3. In such sit-\nuations it is possible to study the distribution of \u03b8iaround\na deterministic trajectory underlying the ordinary differen-\ntial equation, but it should be pointed out that in such sit-\nuations {Xi} is not \u03c0-stationary, and one can at most hope\nfor \u03c0\u03b3-stationarity for a probability distribution \u03c0\u03b3such that\n\u03c0\u03b3\u2192 \u03c0 in a certain sense as \u03b3 \u2192 0.\nThe connection between stochastic approximation and\nthe work of Haario et al. (2001) and the underlying gener-\nality was realised in Andrieu and Robert (2001), although\nit is mentioned in particular cases in Geyer and Thomp-\nson (1995) and Ramponi (1998), the latter reference being\nprobably the first rigourous analysis of the stability and con-\nvergence properties of a particular implementation of con-\ntrolled MCMC for tempering type algorithms.\nA principled stopping rule\nthough ergodicity is intrinsically related to the sequence {\u03b8i}\napproaching the zeroes of h(\u03b8) and hence taking \u201cgood val-\nues\u201d, one might be more confident in using samples pro-\nduced by a standard MCMC algorithm that would use an\noptimal or suboptimal value of \u03b8. This naturally raises the\nquestion of the stopping rule to be used. In the ubiquitous\ncase of the Robbins-Monro updating rule, and given the\nclear interpretation in terms of the root finding of h(\u03b8), one\ncan suggest monitoring the average of the field\nAs pointed out earlier, and al-\n1\nn\nn\n?\ni=1\nH(\u03b8i,Xi+1)\nand stop, for example, when its magnitude is less than a pre-\nset threshold ? for a number m of consecutive iterations.\nMore principled statistical rules relying on the CLT can also\nbe suggested, but we do not expand on this here.\nBoundedness and convergence\ngodicity properties of P\u03b8 can lead to some difficulties in\npractice. Indeed these ergodicity properties are rarely uni-\nform in \u03b8 \u2208 ? and tend to degrade substantially for some\nvalues, typically on the boundary \u2202? of ?. For example\nfor the toy example of Sect. 2, both values \u2202? = {0,1}\nare problematic. For \u03b8 = 0 aperiodicity is lost whereas\nfor \u03b8 = 1 irreducibility is lost. This can result in impor-\ntant problems in practice since \u03c0-ergodicity can be lost as\npointed out in Sect. 2 through the aforementioned toy ex-\nample when the sequence {\u03b8i} converges to \u2202? too quickly.\nIn fact, as pointed out to us by Y.F. Atchad\u00e9, an example\nin Winkler (2003) shows that even in the situation where\n\u03b8i(1) = \u03b8i(2) = 1\u2212 1\/i, the sequence {n\u22121?n\nproblem of possible loss of ergodicity of P\u03b8and its implica-\ntions for controlled Markov chains has long been identified,\nbut is often ignored in the current MCMC related literature.\nFor example a normal symmetric random walk Metropolis\n(N-SRWM) algorithm loses ergodicity as its variance (or co-\nvariance matrix) becomes either too large or too small and\nan algorithm with poor ergodicity properties does not learn\nfeatures of the target distribution \u03c0. In the case of a random\nscan MH within Gibbs algorithm as given in (2), it is pos-\nsible to progressively lose irreducibility whenever a weight\ndrifts towards 0. Several cures are possible. The first and ob-\nvious one consists of truncating ? in order to ensure the ex-\nistence of some uniform ergodicity properties of the family\nThe dependence of the er-\ni=1Xi\u2212 3\/2}\ndoes not vanish (in the mean square sense) as i \u2192 \u221e. This"},{"page":14,"text":"356 Stat Comput (2008) 18: 343\u2013373\nof transitions {P\u03b8}. While this presumes that one knows by\nhow much one can truncate ? without affecting the ergod-\nicity properties of {P\u03b8} significantly, this is not a completely\nsatisfactory solution since stability is actually observed in\nnumerous situations.\nIn Andrieu and Tadi\u00b4 c (2007), using explicit dependence\nof the parameters of well known drift conditions for MCMC\nalgorithms on the tuning parameter \u03b8, general conditions\non the transition probability P\u03b8 and the updating function\nH(\u03b8,x) that ensure boundedness of {\u03b8i} are derived. As a\nresult \u03c0-ergodicity of {Xi}. and convergence to optimal or\nsuboptimal values of \u03b8 are automatically satisfied without\nthe need ro resort to fixed or adaptive truncations for ex-\nample One aspect of interest of the results is that they sug-\ngest some ways of designing fully adaptive and stable algo-\nrithms.\nFor example by noting that the zeroes of h(\u03b8) are also the\nzeroes of h(\u03b8)\/(1 + |\u03b8|\u03b1) for example, one can modify the\nstandard recursion in order to stabilise the update, resulting\nin the alternative updating rule\n\u03b8i+1= \u03b8i+\u03b3i+1H(\u03b8i,Xi+1)\/(1+|\u03b8i|\u03b1).\nOne can also add regularisation terms to the recursion.\nFor example, assuming for example that we learn optimal\nweights for a mixture of transition probabilities as in (2), the\nrecursion\nwk\ni+1= wk\n(with wi= (w1\nto\ni+\u03b3i+1Hk(wi,Xi+1)\ni,w2\ni,...,wn\ni)) can be for example modified\nwk\ni+1= wk\ni+\u03b3i+1Hk(wi,Xi+1)\n?\n+\u03b31+\u03bb\ni+1\n\u03b1 +(wk\n?n\ni)\u2212\u03b2\nj=1\u03b1 +(wj\ni)\u2212\u03b2\u2212wk\ni\n?\nfor some \u03b1,\u03b2,\u03bb > 0. Note that since the sum over k of the\nfields is 0, the weights still sum to 1 after the update and\nalso that due to the boundedness of the additional term it\nvanishes as i \u2192 \u221e. Finally in Andrieu et al. (2005) and An-\ndrieu and Moulines (2006), following Chen et al. (1988), an\nalgorithm with adaptive truncation boundaries is suggested\nand a general theory developedthat ensures that both bound-\nedness and convergence of {\u03b8i} is ensured. Although requir-\ning an intricate theory, the conditions under which bounded-\nnessandconvergenceholdcoveravastnumberofsituations,\nbeyond the situations treated in Andrieu and Tadi\u00b4 c (2007).\nIn Saksman and Vihola (2008) a different approach to prove\nstability is used, and consists of proving that provided that\n{\u03b8i} does not drift too fast to bad values, then the algorithm\npreserves ergodicity. In fact the analysis performed by the\nauthorscanbedirectlyusedtostudythegeneralstabilisation\nstrategy of Andrad\u00f3ttir (1995) (see also reference therein)\nfor stochastic approximation.\nFinally, under more restrictive conditions, detailed in\nBenveniste et al. (1990) and Andrieu and Atchad\u00e9 (2007,\nTheorem 3.1), which include the uniqueness of \u03b8\u2217such that\nh(\u03b8\u2217) = 0 and conditions (8)\u2013(9) for \u03b8 \u2208 K \u2282 ?, it is pos-\nsible to show that for a deterministic sequence {\u03b3i}, there\nexists a finite constant C such that for all i \u2265 1,\n\u02c7E\u2217\n?\n|\u03b8i\u2212\u03b8\u2217|2I{\u03c3 \u2265 i}\n?\n\u2264 C\u03b3i,\nwhere \u03c3 is the first exit time from K, meaning that while\n\u03b8i remains in K (where locally uniform conditions of the\ntype (8)\u2013(9) hold), then the rate of convergence towards \u03b8\u2217\nis given by {\u03b3i}.\nAutomatic choice of the stepsizes\nimation procedure requires the choice of a stepsize se-\nquence {\u03b3i}. A standard choice consists of choosing a deter-\nministic sequence satisfying?\u221e\nthat any point of ? can eventually be reached, while the sec-\nond condition ensures that the noise is contained and does\nnot prevent convergence. Such conditions are satisfied by\nsequences of the type \u03b3i= C\/i\u03b1for \u03b1 \u2208 ((1+\u03bb)\u22121,1]. We\ntend in practice to favour values closer to the lower bound\nin order to increase convergence of the algorithm towards a\nneighbourhood of \u03b8\u2217. This is at the expense of an increased\nvariance of {\u03b8i} around \u03b8\u2217however.\nA very attractive approach which can be useful in prac-\ntice, and for which some theory is available, consists of\nadapting {\u03b3i} in light of the current realisation of the\nalgorithm\u2014this proves very useful in some situations see\nAndrieu and Jasra (2008). The technique was first described\nin Kesten (1958) and relies on the remark that, for exam-\nple, an alternating sign for {\u02c6 \u03b1\u03b8i\u2212 \u00af \u03b1\u2217} in (22) is an indica-\ntion that {\u03b8i} is oscillating around (a) solution(s), whereas\na constant sign suggests that {\u03b8i} is, roughly speaking,\nstill far from the solution(s). In the former case the step-\nsize should be decreased, whereas in the later it should, at\nleast, be kept constant. More precisely consider a function\n\u03b3 : [0,+\u221e) \u2192 [0,+\u221e).Thestandardscenariocorrespond-\ning to a predetermined deterministic schedule consists of\ntaking {\u03b3i= \u03b3(i)}. The strategy suggested by Kesten (1958)\nand further generalised to the multivariate case in Delyon\nand Juditsky (1993) suggests to consider for i \u2265 2 the fol-\nlowing sequence of stepsizes\nThe stochastic approx-\ni=1\u03b3i= \u221e and?\u221e\ni=1\u03b31+\u03bb\ni\n< \u221e for some \u03bb > 0.The former conditionsomehowensure\n\u03b3i= \u03b3\n?i\u22121\nk=1\n?\nI{?H(\u03b8k\u22121,Xk),H(\u03b8k,Xk+1)? \u2264 0}\n?\nwhere ?u,v? is the inner product between vector u and v.\nNumerous generalisations are possible in order to take into"},{"page":15,"text":"Stat Comput (2008) 18: 343\u2013373 357\naccount the magnitudes of {H(\u03b8i,Xi+1)} in the choice of\n{\u03b3i} (Plakhov and Cruz 2004) (and references therein),\n?i\u22121\nk=1\nfor some function \u03c6 : R \u2192 [0,+\u221e). Numerous generalisa-\ntions of these ideas are naturally possible and we have found\nthat in numerous situations a componentwise choice of step-\nsizecanleadtomajoracceleration(AndrieuandJasra2008),\ni.e. consider for example for j = 1,...,n\u03b8\n?i\u22121\nk=1\nwhere Hj(\u03b8,X) is the j-th component of H(\u03b8,X), but care\nmust be taken to ensure that important properties of \u03b8 (such\nas positivity if it is a covariance matrix) are preserved. Fi-\nnally note that this idea needs to be handled with care in the\nunlikely situations where (here in the scalar case for sim-\nplicity) h(\u03b8) \u2265 0 as well as H(\u03b8,x) for all \u03b8,x \u2208 ?\u00d7X and\nthe solution to our problem is on the boundary of ?.\n\u03b3i= \u03b3\n?\n\u03c6(?H(\u03b8k\u22121,Xk),H(\u03b8k,Xk+1)?)\n?\n\u03b3j\ni= \u03b3\n?\nI??Hj(\u03b8k\u22121,Xk),Hj(\u03b8k,Xk+1)?\u2264 0?\n?\n4.2.3 Some variations\nThe class of algorithms considered earlier essentially rely\non an underlying time homogeneous Markov chain Monte\nCarlo algorithm with target distribution \u03c0. It is however\npossible to consider non-homogeneous versions of the al-\ngorithms developed above. More precisely one can suggest\ndefining a sequence {\u03c0i,i \u2265 1} of probability distributions\non X such that \u03c0i\u2192 \u03c0 in some sense, e.g. total variation\ndistance, and select associated MCMC transition probabil-\nities {Pi,\u03b8} such that for any i \u2265 1 and \u03b8 \u2208 ? \u03c0iPi,\u03b8= \u03c0i.\nThen the controlled MCMC algorithm defined earlier can\nuse Pi+1,\u03b8iat iteration i+1 instead of P\u03b8i. This opens up the\npossibility for example to use tempering ideas, i.e. choose\n\u03c0i(x) \u221d \u03c0\u03b2i(x) for \u03b2i\u2208 (0,1), allowing for the accumula-\ntion of useful information concerning the distribution of in-\nterest \u03c0, while exploring \u201csimpler\u201d distributions. This type\nof strategy can be useful in order to explore multimodal dis-\ntributions.\nAnother possibility, particularly suitable to two stage\nstrategies where adaptation is stopped, consists of remov-\ning the vanishing character of adaptation. In the context of\nstochastic approximation this means for example that the se-\nquence {\u03b3i} can be set to a constant small value \u03b3. As a re-\nsult, in light of the examples of the first section, one expects\nthat under some stability assumptions the chain {Xi} will\nproduce samples asymptotically distributed according to an\napproximation \u03c0\u03b3 of \u03c0 (such that \u03c0\u03b3\u2192 \u03c0 in some sense)\nand optimise an approximate criterion corresponding to the\nstandard criterion where \u03c0 is replaced by \u03c0\u03b3. This strategy\ncan offer some robustness properties.\n5 Some adaptive MCMC procedures\nIn this section we present combinations of strategies, some\nof them original,1which build on the principles developed\nin previous sections. Note that in order to keep notation sim-\nple and ensure readability we present here the simplest ver-\nsions of the algorithms but that additional features described\nin Sect. 4.2.2, such as the modification of the mean field to\nfavour stability, the automatic choice of the stepsize (com-\nponentwise or not) or Rao-Blackwellisation etc., can easily\nbe incorporated.\n5.1 Compound criteria, transient and starting to learn\nAs pointed out earlier desirable asymptotic criteria and asso-\nciated optimisation procedures can easily be defined. How-\never it can be observed in practice that the algorithm can\nbe slow to adapt, in particular in situations where the ini-\ntial guess of the parameter \u03b8 is particularly bad, resulting\nfor example in a large rejection probability. More generally\nthe MH algorithm has this particular rather negative char-\nacteristic that if not well tuned it will not explore the target\ndistribution and hence will be unable to gather information\nabout it, resulting in a poor learning of the target distribu-\ntion, and hence algorithms that adapt and behave badly. We\ndescribe in this section some strategies that circumvent this\nproblem in practice.\nWe focus here on the symmetric increments random-\nwalk MH algorithm (hereafter SRWM), in which q(x,y) =\nq(x \u2212y) for some symmetric probability density q on Rnx,\nreferredtoastheincrementdistribution.Thetransitionprob-\nability of the Metropolis algorithm is then given for x,A \u2208\nX\u00d7B(X) by\nPSRWM\nq\n(x,A)\n?\n?\nx \u2208 X,A \u2208 B(X),\nwhere \u03b1(x,y) := 1 \u2227 \u03c0(y)\/\u03c0(x). A classical choice for the\nproposal distribution is\nq(z) = N(z;0,?),\nN(z;\u03bc,?) is the density of a multivariate Gaussian with\nmean \u03bc and covariance matrix ?. We will later on refer to\nthis algorithm as the N-SRWM. It is well known that ei-\nther too small or too large a covariance matrix will result\nin highly positively correlated Markov chains, and therefore\nestimators\u02c6I?\n=\nA\u2212x\n+I(x \u2208 A)\n\u03b1(x,x +z)q(z)dz\nX\u2212x\n(1\u2212\u03b1(x,x +z))q(z)dz,\n(26)\nwhere\nn(f) with a large variance. In Gelman et al.\n1First presented at the workshop Adapski\u201908, 6\u20138 January 2008,\nBormio, Italy."},{"page":16,"text":"358Stat Comput (2008) 18: 343\u2013373\n(1995) it is shown that the \u201coptimal\u201d covariance matrix (un-\nder restrictive technical conditions not given here) for the N-\nSRWM is (2.382\/nx)?\u03c0, where ?\u03c0 is the true covariance\nmatrix of the target distribution. In Haario et al. (2001) (see\nalso Haario et al. 1999) the authors have proposed to \u201clearn\n?\u03c0 on the fly\u201d, whenever this quantity exists. It should be\npointed out here that in situations where this quantity is not\nwell defined, one should resort to \u201crobust\u201d type estimates in\norder to capture the dependence structure of the target distri-\nbution; we do not consider this here. Denoting PSRWM\ntransition probability of the N-SRWM with proposal distri-\nbution N(0,\u03bb?) for some \u03bb > 0. With \u03bb = 2.382\/nx, the\nalgorithm in Haario et al. (2001) can be summarised as fol-\nlows,\n\u03bci,?i\nthe\nAlgorithm 2 AM algorithm\n\u2022 Initialise X0,\u03bc0and ?0.\n\u2022 At iteration i +1, given Xi,\u03bciand ?i\n1. Sample Xi+1\u223c PSRWM\n2. Update\n\u03bci,?i\n(Xi,\u00b7).\n\u03bci+1= \u03bci+\u03b3i+1(Xi+1\u2212\u03bci),\n?i+1= ?i+\u03b3i+1((Xi+1\u2212\u03bci)(Xi+1\u2212\u03bci)T\u2212?i).\n(27)\nThis algorithm has been extensively studied in Andrieu\nand Moulines (2006), Atchad\u00e9 and Fort (2008), Bai et al.\n(2008) and Andrieu and Tadi\u00b4 c (2007). We now detail some\nsimple improvements on this algorithm.\n5.1.1 Rao-Blackwellisation and square root algorithms\nFollowing (Ceperley et al. 1977) and (Frenkel 2006), we\nnote that, conditional upon the previous state Xiof the chain\nand the proposed transition Yi+1, the vector f(Xi+1) (for\nany function f : X \u2192 Rnf) can be expressed as\nf(Xi+1) := I{Ui+1\u2264 \u03b1(Xi,Yi+1)}f(Yi+1)\n+I{Ui+1> \u03b1(Xi,Yi+1)}f(Xi),\n(28)\nwhere Ui+1\u223c U(0,1). The expectation of f(Xi+1) with re-\nspect to Ui+1conditional upon Xiand Yi+1leads to\nf(Xi+1) := \u03b1(Xi,Yi+1)f(Yi+1)\n+(1\u2212\u03b1(Xi,Yi+1))f(Xi).\nForexample\u00afXi+1:=\u03b1(Xi,Yi+1)Yi+1+(1\u2212\u03b1(Xi,Yi+1))Xi\nis the \u201caverage location\u201d of state Xi+1which follows Xi\n(29)\ngiven Yi+1. This can be incorporated in the following \u201cRao-\nBlackwellised\u201d AM recursions\n?\u03b1(Xi,Yi+1)(Yi+1\u2212\u03bci)\n?\u03b1(Xi,Yi+1)(Yi+1\u2212\u03bci)(Yi+1\u2212\u03bci)T\n+(1\u2212\u03b1(Xi,Yi+1))(Xi\u2212\u03bci)(Xi\u2212\u03bci)T\u2212?i\nUsing, for simplicity, the short notation (29) a Rao-Black-\nwellised AM algorithm can be described as follows:\n\u03bci+1= \u03bci+\u03b3i+1\n+(1\u2212\u03b1(Xi,Yi+1))(Xi\u2212\u03bci)?,\n?i+1= ?i+\u03b3i+1\n?.\nAlgorithm 3 Rao-Blackwellised AM algorithm\n\u2022 Initialise X0,\u03bc0and ?0.\n\u2022 At iteration i +1, given Xi,\u03bciand ?i\n1. Sample Yi+1\u223c N(Xi,?i) and set Xi+1= Yi+1with\nprobability \u03b1(Xi,Yi+1), otherwise Xi+1= Xi.\n2. Update\n\u03bci+1= \u03bci+\u03b3i+1(\u00afXi+1\u2212\u03bci),\n?i+1= ?i+\u03b3i+1[(Xi+1\u2212\u03bci)(Xi+1\u2212\u03bci)T\u2212?i].\n(30)\nNote that it is not clear that this scheme is always advan-\ntageous in terms of asymptotic variance of the estimators,\nas shown in Delmas and Jourdain (2007), but this modifica-\ntion of the algorithm might be beneficial during its transient\nwheneverthe acceptanceprobabilityis not too low naturally.\nIt is worth pointing out that for computational efficiency\nand stability one can directly update the Choleski decompo-\nsition of ?i, using the classical rank 1 update formula\n?1\/2\ni+1= (1\u2212\u03b3i+1)1\/2?1\/2\n?\ni\n+\n1+\n\u03b3i+1\n1\u2212\u03b3i+1??\u22121\/2\n??\u22121\/2\ni\n(Xi+1\u2212\u03bci)?2\n(Xi+1\u2212\u03bci)?2\u22121\ni\n\u00d7(1\u2212\u03b3i+1)1\/2(Xi+1\u2212\u03bci)(Xi+1\u2212\u03bci)T?\u2212T\/2\nwhere AT\/2is a shorthand notation for (A1\/2)Twhenever\nthis quantity is well defined. This expression can be simpli-\nfied through an expansion (requiring \u03b3i+1? 1) and modi-\nfied to enforce a lower triangular form as follows\ni\n?1\/2\ni+1= ?1\/2\ni\n+\u03b3i+1?1\/2\n?\u22121\/2\ni\ni\nL\n\u00d7\n?\n(Xi+1\u2212\u03bci)(Xi+1\u2212\u03bci)T?\u2212T\/2\ni\n\u2212I\n?\n,\nwhere L(A) is the lower triangular part of matrix A. Note\nagain the familiar stochastic approximation form of the re-"},{"page":17,"text":"Stat Comput (2008) 18: 343\u2013373 359\ncursion, whose mean field is\n?\nand whose zeros (together with those of the recursion on\nthe mean) are precisely any square root of ?\u03c0. The operator\nensures that the recursion is constrained to lower triangu-\nlar matrices. Note that this is only required if one wishes to\nsave memory. Rank r updates can also be used when the co-\nvariance matrix is updated every r iterations only. In what\nfollows, whenever covariance matrices are updated, recur-\nsions of this type can be used although we will not make\nthis explicit for notational simplicity.\nL\n?\u22121\/2??\u03c0+(\u03bc\u2212\u03bc\u03c0)(\u03bc\u2212\u03bc\u03c0)T??\u2212T\/2\u2212I\n?\n,\n5.1.2 Compound criterion: global approach\nAs pointed out earlier, in the case of the N-SRWM algo-\nrithm the scaling of the proposal distribution is well under-\nstood in specific scenarios and intuitively meaningful for a\nlarger class of target distributions. A good rule of thumb is\nto choose \u03bb = (2.382\/nx)?\u03c0, where ?\u03c0 is the covariance\nmatrix of \u03c0. We have shown above that following (Haario\net al. 2001) one can in principle estimate ?\u03c0from the past\nof the chain. However the difficulties that lead to the de-\nsire to develop adaptive algorithms in the first place, includ-\ning the very poor exploration of the target distribution of\n\u03c0, also hinder learning about the target distribution in the\ninitial stages of an adaptive MCMC algorithm when our ini-\ntial value for the estimator of ?\u03c0is a poor guess. Again if\n\u03bb?i is either too large in some directions or too small in\nall directions the algorithm has either a very small or a very\nlarge acceptance probability, which results in a very slow\nlearning of ?\u03c0since the exploration of the target\u2019s support\nis too localised. This is a fundamental problem in practice,\nwhich has motivated the use of delayed rejection for exam-\nple (Haario et al. 2003), and for which we present here an\nalternative solution which relies on the notion of composite\ncriterion.\nWhile theory suggests a scaling of \u03bb = 2.382\/nxwe pro-\npose here to adapt this parameter in order to coerce the\nacceptance probability to a preset and sensible value (e.g.\n0.234), at least in the initial stages of the algorithm. Indeed,\nwhile this adaptation is likely not to be useful in the long-\nrun, this proves very useful in the early stages of the algo-\nrithm (we provide a detailed illustration in Sect. 6.3) where\nthe pathological behaviour described above can be detected\nthrough monitoring of the acceptance probability, and cor-\nrected.\nAs a consequence in what follows the proposal distri-\nbution of the adaptive N-SRWM algorithm we consider\nis q\u03b8(z) = N(z;0,\u03bb?) where here \u03b8 := (\u03bb,\u03bc,?). As-\nsuming that for any fixed covariance matrix ? the corre-\nsponding expected acceptance probability \u00af \u03b1\u03bb(see (14)) is\na non-increasing function of \u03bb, one can naturally suggest\nthe recursion log\u03bbi+1= log\u03bbi+ \u03b3i+1[\u03b1(Xi,Yi+1) \u2212 \u00af \u03b1\u2217],\nwhich following the discussion of Sect. 4 is nothing but a\nstandard Robbins-Monro recursion. Now when the covari-\nance matrix ?\u03c0 needs to be estimated, one can suggest\nthe following \u201ccompound criterion\u201d or \u201cmulticriteria\u201d algo-\nrithm:\nAlgorithm 4 AM algorithm with global adaptive scaling\n\u2022 Initialise X0,\u03bc0and ?0.\n\u2022 At iteration i +1, given Xi,\u03bci,?iand \u03bbi\n1. Sample Yi+1\u223c N(Xi,\u03bbi?i) and set Xi+1= Yi+1\nwith probability \u03b1(Xi,Yi+1), otherwise Xi+1= Xi.\n2. Update\nlog(\u03bbi+1) = log(\u03bbi)+\u03b3i+1[\u03b1(Xi,Yi+1)\u2212 \u00af \u03b1\u2217],\n\u03bci+1= \u03bci+\u03b3i+1(Xi+1\u2212\u03bci),\n?i+1= ?i+\u03b3i+1[(Xi+1\u2212\u03bci)(Xi+1\u2212\u03bci)T\u2212?i].\n(31)\nAgain the interest of the algorithm is as follows: when-\never our initial guess ?0is either two large or two small,\nthis will be reflected in either a large or small acceptance\nprobability, meaning that learning of ?\u03c0is likely to be slow\nfor a fixed scaling parameter. However this measure of per-\nformance of the algorithm can be exploited as illustrated\nabove: if \u03b1(Xi,Yi+1) \u2212 \u00af \u03b1\u2217< 0 for most transition attempts\nthen \u03bbi should be decreased, while if on the other hand\n\u03b1(Xi,Yi+1) \u2212 \u00af \u03b1\u2217\u2265 0 for most transition attempts, then \u03bbi\nshould be increased. As a result one might expect a more\nrapid exploration of the target distribution following a poor\ninitialisation. Although this strategy can improve the perfor-\nmance of the standard AM algorithm in practice, we show\nin the next section that it is perfectible.\n5.1.3 Compound criterion: local approach\nAs we shall now see, the global approach described in the\nprevious subsection might be improved further. There are\ntwo reasons for this. First it should be clear that adjusting\nthe global scaling factor ignores the fact that the scaling of\n\u03bbi?i might be correct in some directions, but incorrect in\nothers. In addition, in order to be efficient, such bold up-\ndates require in general some good understanding of the de-\npendence structure of the target distribution, in the form of\na reasonable estimate of ?\u03c0, which is not available in the\ninitial stages of the algorithm. These problems tend to be\namplified in scenarios involving a large dimension nxof the\nspace X since innocuous approximations in low dimensions\ntend to accumulate in larger cases. Inspired by Haario et\nal. (2005), we suggest the following componentwise update"},{"page":18,"text":"360Stat Comput (2008) 18: 343\u2013373\nstrategy which consists of a mixture of timid moves whose\nrole is to attempt simpler transitions better able to initiate\nthe exploration of \u03c0. Note, however, that in contrast with\n(Haario et al. 2005) our algorithm uses the notion of com-\npound criterion, which in our experience significantly im-\nproves performance. With ekthe vector with zeroes every-\nwhere but for a 1 on its k-th row and a sensible \u00af \u03b1\u2217\u2217\u2208 (0,1)\ne.g. 0.44:\nAlgorithm 5 Componentwise AM with componentwise\nadaptive scaling\n\u2022 Initialise X0,\u03bc0,?0and \u03bb1\n\u2022 At iteration i +1, given \u03bci,?iand \u03bb1\n1. Choose a component k \u223c U{1,...,nx}.\n2. Sample Yi+1 \u223c Xi + ekN(0,\u03bbk\nXi+1= Yi+1 with probability \u03b1(Xi,Yi+1), otherwise\nXi+1= Xi.\n3. Update\n0,...,\u03bbnx\n0.\ni,...,\u03bbnx\ni\ni[?i]k,k) and set\nlog(\u03bbk\ni+1) = log(\u03bbk\n\u03bci+1= \u03bci+\u03b3i+1(Xi+1\u2212\u03bci),\n?i+1= ?i+\u03b3i+1[(Xi+1\u2212\u03bci)(Xi+1\u2212\u03bci)T\u2212?i]\nand \u03bbj\nifor j ?= k.\ni)+\u03b3i+1[\u03b1(Xi,Yi+1)\u2212 \u00af \u03b1\u2217\u2217],\n(32)\ni+1= \u03bbj\nOne might question the apparently redundant use of both\na scaling \u03bbk\niand the marginal variance [?i]k,kin the pro-\nposal distributions above, and one might choose to com-\nbine both quantities into a single scaling factor. However the\npresent formulation allows for a natural combination (i.e. a\nmixture or composition) of the recursion above and varia-\ntions of the standard AM algorithm (Algorithm 2) such as\nAlgorithm 4. Such combinations allow one to circumvent\nthe shortcomings of bold moves, which require extensive\nunderstanding of the structure of \u03c0, in the early iterations of\nthe algorithm. The timid moves allow the procedure to start\ngathering information about \u03c0 which might then be used by\nmore sophisticated and more global updates.\nWe now turn to yet another version of the AM algorithm\n(Algorithm 2) which can be understood as being a version\nof Algorithm 4 which exploits the local scalings computed\nby Algorithm 5 instead of a single global scaling factor. It\nconsists of replacing the proposal distribution N(Xi,\u03bbi?i)\nin Algorithm 4 with N(Xi, ?1\/2\n?\nAs we now show, such an update can be combined with Al-\ngorithm 5 into a single update. For a vector V we will de-\nnote V(k) its k-th component and ekthe vector with zeroes\neverywhere but for a 1 on its k-th row. We have,\ni\n?i?1\/2\ni\n), where\n?i:= diag\n\u03bb1\ni,...,\u03bbnx\ni\n?\n.\nAlgorithm6GlobalAMwithcomponentwiseadaptivescal-\ning\n\u2022 Initialise X0,\u03bci,?iand \u03bb1\n\u2022 Iteration i +1\n1. Given \u03bci,?i\nN(0,?1\/2\nprobability \u03b1(Xi,Xi+Zi+1), otherwise Xi+1= Xi.\n2. Update for k = 1,...,nx\nlog(\u03bbk\ni)+\u03b3i+1[\u03b1(Xi,Xi+Zi+1(k)ek)\n\u2212 \u00af \u03b1\u2217\u2217],\n\u03bci+1= \u03bci+\u03b3i+1(Xi+1\u2212\u03bci),\n?i+1= ?i+\u03b3i+1[(Xi+1\u2212\u03bci)(Xi+1\u2212\u03bci)T\u2212?i].\ni,...,\u03bbnx\ni.\nand \u03bb1\n) and set Xi+1= Xi+ Zi+1 with\ni,...,\u03bbnx\ni, sample Zi+1 \u223c\ni\n?i?1\/2\ni\ni+1) = log(\u03bbk\n(33)\nIt is naturally possible to include an update for a global\nscaling parameter, but we do not pursue this here. This al-\ngorithm exploits the fact that a proposed sample Xi+ Zi+1\nprovides us with information about scalings in various di-\nrections through the \u201cvirtual\u201d componentwise updates with\nincrements {Zi+1(k)ek} and their corresponding directional\nacceptance probabilities. This strategy naturally requires\nnx+ 1 evaluations of \u03c0, which is equivalent to one update\naccording to Algorithm 4 and nxupdates according to Al-\ngorithm 5.\n5.2 Fitting mixtures, clustering and localisation\nAs pointed out in Andrieu and Moulines (2006, Sect. 7) the\nmoment matching criterion corresponding to the recursion\n(27) can be understood as minimising the Kullback-Leibler\ndivergence\n?\nwhere \u02d8 q\u03b8(x) = N(x;\u03bc,?) (but using q\u03b8(z) = N(z;0,\u03bb?)\nas a proposal distribution for the increments of a N-SRWM\nupdate). This remark leads to the following considerations,\nof varying importance.\nThe first remark is that \u02d8 q\u03b8could be used as the proposal\ndistribution of an independent MH (IMH) update, as in An-\ndrieu and Moulines (2006) or Giordani and Kohn (2006).\nAlthough this might be a sensible choice when \u02d8 q\u03b8(x) is a\ngood approximation of \u03c0, this might fail when \u03b8 is not close\nto \u03b8\u2217(in the transient for example) or simply because the\nchosen parametric form is not sufficiently rich. In addition\nsuch a bad behaviour is generally exacerbated by large di-\nmensions as illustrated by the following toy example.\nKL\u03b8(\u03c0,q\u03b8) := E\u03c0\nlog\u03c0(X)\n\u02d8 q\u03b8(X)\n?\n(34)\nExample 1 The target distribution is \u03c0(x) = N(x;0,I)\nwith x \u2208 Rnxand proposal distribution q(x) = N(x;\u03b5 \u00d7"},{"page":19,"text":"Stat Comput (2008) 18: 343\u2013373 361\ne,I) for some \u03b5 > 0 with e = (1,1,1,...)T. The importance\nsampling weight entering the acceptance ratio of an IMH\nalgorithm is\n?1\n?\n\u03c0(x)\nq(x)= exp\n2\u03b52nx\u2212\u03b5eTx\n?\n= exp\n\u22121\n2\u03b52nx\u2212\u03b5n1\/2\nx n\u22121\/2\nx\nnx\n?\ni=1\n(x(i)\u2212\u03b5)\n?\n,\nwhich is not bounded, hence preventing geometric ergod-\nicity. The distribution of n\u22121\/2\nx\nN(0,1), which results in a variance for the weights of\n?\nThis is known to result in poorly performing importance\nsampling algorithms, but will also have an impact on the\nconvergence of IMH algorithms which will get stuck in\nstates x with arbitrarily large weights x as nxincreases, with\nnon negligible probability.\n?nx\ni=1(x(i) \u2212 \u03b5) is precisely\nexp\n\u03b52nx\n?\n\u22121.\nIMH updates hence fall in the category of \u201cvery bold\u201d\nupdates which require significant knowledge of the structure\nof \u03c0 and do not usually form the base for reliable adaptive\nMCMC algorithms.\nThe second remark, which turns out to be of more inter-\nest, is that one can consider other parametric forms for \u02d8 q\u03b8,\nand use such approximations of \u03c0 to design proposal distri-\nbutions for random walk type algorithms, which are likely\nto perform better given their robustness. It is suggested in\nAndrieu and Moulines (2006, Sect. 7) to consider mixtures,\nfinite or infinite, of distributions belonging to the exponen-\ntialfamily(seealsoCapp\u00e9etal.2007forasimilarideainthe\ncontext of importance sampling\/population Monte Carlo).\nThis has the advantage of leading to an elegant optimisa-\ntion algorithm which relies on an on-line version of the EM\nalgorithm and results in a marginal additional computational\noverhead.\nIn this section we first detail two particular cases of this\nprocedure: mixture of normal distributions and Student t-\ndistributions.\n5.2.1 Updates for fitting mixtures in the exponential family\nWe first briefly review how, given samples {Xi}, it is possi-\nble to iteratively fit a mixture\n\u02d8 q\u03b8(x) =\nn\n?\nk=1\nwkN(x;\u03bck,?k),\n(35)\nwith \u03b8 = (w,\u03bc,?) with w = (w1,w2,...,wn), in order\nto minimise (34). For the purpose of describing the al-\ngorithm it is convenient to introduce the missing data z\nsuch that \u02d8 q\u03b8(x,z = k) := wkN(x;\u03bck,?k) and hence for\nk \u2208 {1,...,n}\n\u02d8 q\u03b8(k|x) =wkN(x;\u03bck,?k)\n\u02d8 q\u03b8(x)\nwkN(x;\u03bck,?k)\n?n\nNowforany k \u2208 {1,...,n} and i \u2265 0 therecursionsare,with\n\u02d8 q\u03b8i(Zi+1= k|x) :=wk\n\u02d8 q\u03b8i(x)\n=\nl=1wlN(x;\u03bcl,?l).\niN(x;\u03bck\ni,?k\ni)\n,\n\u03bck\ni+1= \u03bck\n?k\ni+\u03b3i+1\u02d8 q\u03b8i(Zi+1= k|Xi+1)(Xi+1\u2212\u03bck\ni+1=?k\n\u00d7[(Xi+1\u2212\u03bck\nwk\ni),\ni+\u03b3i+1\u02d8 q\u03b8i(Zi+1= k|Xi+1)\ni)(Xi+1\u2212\u03bck\ni+1= wk\nNote that the standard EM framework suggests various ac-\nceleration techniques, which we do not consider here for\nbrevity.\nIt is also possible to consider a mixture of multivariate\nStudent-tdistributions,whichisamixedcontinuous\/discrete\nmixture of normals. More precisely consider the case where\ni)T\u2212?k\ni],\ni).\n(36)\ni+\u03b3i+1(\u02d8 q\u03b8i(Zi+1= k|Xi+1)\u2212wk\n\u02d8 q\u03b8(x) =\nn\n?\nk=1\nwkT\u03bd(x;\u03bck,?k)\nwhere\nT\u03bd(x;\u03bc,?)\n=\n?(\u03bd+nx\n2\n) |?|\u22121\/2\n(\u03c0\u03bd)\n1\n2nx?(\u03bd\n2)(1+1\n\u03bd(x \u2212\u03bc)T?\u22121(x \u2212\u03bc))\n1\n2(\u03bd+nx).\nWe consider here for simplicity the case \u201cone \u03bd for all\u201d since\nwe are not interested in a very precise fit of the target distri-\nbution. Note that as \u03bd \u2192 \u221e the mixture converges to a mix-\nture of normal distributions which coincides with that de-\nscribed above. The on-line EM algorithm relies on the stan-\ndard fact that \u02d8 q\u03b8(x) can be seen as the marginal distribution\nof\n?\u2212u\n\u00d7(\u03bd\/2)\u03bd\/2\n\u02d8 q\u03b8(k,u,x) =\nwkunx\n\u221a|2\u03c0?k|exp\n?(\u03bd\/2)u\u03bd\/2\u22121exp\n2(x \u2212\u03bck)T?\u22121\n?\nk(x \u2212\u03bck)\n?\n\u2212\u03bd\n2u\n?\nI{u \u2265 0}.\nWe denote\n\u02d8 q\u03b8i(Zi+1= k|Xi+1) :=wk\niT\u03bd(x;\u03bck\n\u02d8 q\u03b8i(x)\ni,?k\ni)"},{"page":20,"text":"362Stat Comput (2008) 18: 343\u2013373\nand introduce the conditional expectation of U given X\nand Z\n\u00af u(k,X) := E\u03b8[U|k,X] =\n\u03bd +nx\n\u03bd +(X \u2212\u03bck)T?\u22121\nk(X \u2212\u03bck).\nThe required recursions are\n\u03bck\ni+1= \u03bck\ni+\u03b3i+1\u00af u(k,Xi+1)\u02d8 q\u03b8i(Zi+1= k|Xi+1)\n\u00d7(Xi+1\u2212\u03bck\ni+1=?k\n\u00d7[(Xi+1\u2212\u03bck\nwk\n?\nThis later choice is closely related to the \u201cfast K-mean\u201d al-\ngorithm used in Giordani and Kohn (2006) (although the\nalgorithm developed there is not on-line, whereas the al-\ngorithm developed here is computationally very efficient)\nwhich is beneficial in the initial stages of the algorithm in\norder to start the learning process. In practice we suggest\nthatwhenfittinga mixtureof normaldistributions,therecur-\nsions for the Studentt-distributions be used witha parameter\n\u03bdi\u2192 \u221e with the iterations.\ni),\n?k\ni+\u03b3i+1\u00af u(k,Xi+1)\u02d8 q\u03b8i(Zi+1= k|Xi+1)\ni)(Xi+1\u2212\u03bck\ni+1= wk\n\u00af uk\ni)T\u2212?k\ni],\ni),\ni+\u03b3i+1(\u02d8 q\u03b8i(Zi+1= k|Xi+1)\u2212wk\ni+\u03b3i+1\ni+1= \u00af uk\n\u00af u(k,Xi+1)\u02d8 q\u03b8i(Zi+1= k|Xi+1)\u2212 \u00af uk\ni\n?\n.\n5.2.2 Localised random walk Metropolis updates\nThe Metropolis-Hastings algorithm in its simplest form of-\nfers the possibility for local adaptation given the possible\ndependence of its family of proposal distributions {q(x,\u00b7),\nx \u2208 X} on the current state of the Markov chain. Obvious\nexamples include the Langevin algorithm or self-targeting\nschemes (Stramer and Tweedie 1999). This dependence is\nexploited further in Green (1995) where the weights of a\nmixture of MH updates are allowed to depend on the current\nstate x of the Markov chain, hence offering the possibility to\nselect a particular update depending on the region currently\nvisited by, say, state Xi= x.\nWe now describe an original use of the information about\n\u03c0 contained in the approximation \u02d8 q\u03b8(x) of \u03c0 which al-\nlows for some localisation of the adaptation in the spirit\nof a suggestion in Andrieu and Robert (2001) concerned\nwith Voronoi tesselations (for which the Linde-Buzo-Gray,\nan EM like algorithm, could be used here). We however re-\nstrict here the presentation to that of an algorithm for which\n\u02d8 q\u03b8(x) is a mixture of normal distributions\u2014other cases are\nstraightforward extensions. Note that another form of locali-\nsations has been suggested in Roberts and Rosenthal (2006),\nwhich is more in line with the ideas of Stramer and Tweedie\n(1999), and can lead to interesting algorithms.\nThe algorithm we suggest here is a mixture of N-SRWM\nalgorithms\u2014one should bear in mind that such an algorithm\nwill in general be a component of a much larger mixture\nor part of a composition of updates. The interest of our ap-\nproach is that following (Green 1995) we allow the weights\nof the mixture to depend on the current state of the chain.\nMore precisely one can suggest for example using\nP\u03b8(x,dy) =\nn\n?\nk=1\n\u02d8 q\u03b8(k|x)PNSRWM\n\u03b8,k\n(x,dy)\nwhere\nPNSRW\n\u03b8,k\nproposal distribution, here in the normal case, N(y;x,\n\u03bbk?k). Note that other choices than the weights \u02d8 q\u03b8(k|x)\ncan be chosen in order to ensure, in particular in the early\nstages of the algorithm, that all components are being used.\nIn the case of a mixture of normals one can for example\nsuggest using the conditional distribution \u02d8 q\u03b8(k|x) for a mix-\nture of Student t-distributions with a parameter \u03bdi\u2192 \u221e\nas i \u2192 \u221e. The choice of \u03bbkis made adaptive in order to\nachieve a preset acceptance probability, according to (31).\nThe motivations for this algorithm are twofold: (a) first the\nweight \u02d8 q\u03b8(k|x),orafunctionofthisquantity,favoursassoci-\nation of x to relevant components of the mixture of distribu-\ntions \u02d8 q\u03b8(x), that is for example the local linear dependencies\npresent among the components of x through the covariance\nmatrices ?1:n(b) secondly \u02d8 q\u03b8(x) can be used in order to\ncluster states of X and associate local criteria to each clus-\nter (here a local expected acceptance probability but other\nchoices are possible) which in turn can be locally adapted\nusing a rule of our choice. Note the advantage of this al-\ngorithm in terms of innovation (or exploration in machine\nlearning speak) over a simple IMH algorithm that would try\nto sample and learn from its own samples.\nThe algorithm can be summarised with the following\npseudo-code in the case where a mixture of normal distri-\nbutions is used in order to map the state-space.\n\u03b8 = (\u03bc1:n,?1:n,w1:k,\u03bb1:k)\n(x,dy) is a random walk Metropolis algorithm with\nand the transition\nAlgorithm 7 Localised N-SRWM algorithm\n\u2022 Initialise X0,\u03bc1:n\n\u2022 Iteration i +1, given Xi,\u03bc1:n\n1. Zi+1\u223c \u02d8 q\u03b8i(Z = k|Xi), Yi+1\u223c N(Xi,\u03bbZi+1\nandset\nXi+1\nmin{1,\u03c0(Yi+1)\u02d8 q\u03b8i(k|Yi+1)\n2. Update \u03bc1:n\ni\nand \u03bb1:n\n0,?1:n\n0,w1:n\n0\nand \u03bb1:n\ni,?1:n\n0.\n,w1:n\nii\nand \u03bb1:n\ni\ni\n?Zi+1\ni\n)\n=\nYi+1\nwith probability\n\u03c0(Xi)\u02d8 q\u03b8i(k|Xi)}, otherwise Xi+1= Xi.\ni,?1:n\ni\ni+1, according to (36) and (37).\n,w1:n\nand \u03bb1:k\ni\nto \u03bc1:n\ni+1,?1:n\ni+1,w1:n\ni+1\nThe localised nature of the algorithm, in the spirit of the\nstate dependent mixtures of updates of Green (1995), re-\nquires some care. Firstly note the form of the acceptance"},{"page":21,"text":"Stat Comput (2008) 18: 343\u2013373363\nprobability required in order to ensure that the underlying\n\u201cfixed \u03b8\u201d transition probability is in detailed balance with \u03c0\n?\nSecondly, updating of the parameters requires some atten-\ntion. Indeed, given that component k is chosen, we wish\nto adjust the conditional expected acceptance probability of\nthiscomponentin orderto reach an expectedacceptancerate\n\u00af \u03b1\u2217.Inmathematicaltermswewishtosetthefollowingmean\nfield h(\u03b8) with components hk(\u03b8) to zero,\n\u03b1k(x,y) := min1,\u03c0(y)\u02d8 q\u03b8(k|y)\n\u03c0(x)\u02d8 q\u03b8(k|x)\n?\n.\nhk(\u03b8) :=\n?\n?\nX2\n\u03c0(x)\u02d8 q\u03b8(k|x)N(y;x,\u03bbk?k)\n?\n?\nX\u03c0(x)\u02d8 q\u03b8(k|x)dx\n\u03b1k(x,y)dxdy \u2212 \u00af \u03b1\u2217\n=\nX2\u03c0(x)\u02d8 q\u03b8(k|x)N(y;x,\u03bbk?k)(\u03b1k(x,y)\u2212 \u00af \u03b1\u2217)dxdy\nX\u03c0(x)\u02d8 q\u03b8(k|x)dx\nwhere the fraction on the first line is the density of the condi-\ntionalsteadystatedistributionP\u03b8(X \u223c \u03c0,Y \u223c N(X,\u03bbk?k)|\nZ = k). Finding the zeros of hk(\u03b8) hence amounts to finding\nthe zeros of the top of the last fraction, which can be written\nas an expectation\n,\nn\n?\nm=1\n?\nX2\u03c0(x)\u02d8 q\u03b8(m|x)I{m = k}\n\u00d7N(y;x,\u03bbk?k)(\u03b1k(x,y)\u2212 \u00af \u03b1\u2217)dxdy\nn\n?\n\u00d7I{m = k}(\u03b1k(x,y)\u2212 \u00af \u03b1\u2217)dxdy.\n=\nm=1\n?\nX2\u03c0(x)\u02d8 q\u03b8(m|x)N(y;x,\u03bbm?m)\nThe second form of hk(\u03b8) above (and since the denominator\ndoes not (in general) affect the zeros of hk(\u03b8)) suggests the\nfollowing recursions to update {\u03bbk\nponents\u2019 running expected acceptance probabilities {\u03b1k\ni} and compute the com-\ni}\nlog(\u03bbk\ni+1) =log(\u03bbk\ni)+\u03b3i+1I{Zi+1= k}\n\u00d7?\u03b1k(Xi,Yi+1)\u2212 \u00af \u03b1\u2217\ni+\u03b3i+1I{Zi+1= k}\n?,\n\u03b1k\ni+1= \u03b1k\n?\n\u03b1k(Xi,Yi+1)\u2212\u03b1k\ni\n?\n.\n(37)\nNaturally we do not address here the choice of the number\nn of components of the mixture. Although the use of simple\ninformation criteria could be advocated in order to choose n,\neven in a crude way, we believe that although feasible this\nmight lead to additional complications at this stage. We here\nsimply argue that choosing n > 1 should in general be ben-\neficial compared to the use of a plain N-SRWM for which\nn = 1. Alternatively one can suggest the possibility of fitting\nsimultaneously several mixtures with each its own number\nof components.\n5.3 Block sampling and principal directions\nFor large dimensional problems, updating the whole vector\nX in one block might lead to a poorly performing algorithm\nwhichfails to explorethedistribution \u03c0 of interest.It is stan-\ndard in practice to attempt to update subblocks of X con-\nditional upon the corresponding complementary subblock,\nwhich in practice facilitates the design of better proposal\ndistributions. The choice of such subblocks is however in\npractice crucial while far from obvious in numerous situa-\ntions. Indeed it is well known and easy to understand that\nvariables that are highly dependent components of X (under\n\u03c0) should in practice be updated simultaneously as it can\notherwise lead to algorithms that are slow to converge, and\nproduce samples with poor statistical properties. Identifying\nsuch subblocks of dependent components can be very dif-\nficult in practice, and it is natural to ask if it is possible to\nautomatise this task in practice.\nA possible suggestion is to consider an MCMC update\nthat takes the form of a mixture of MCMC updates\nP\u03b8(x,dy) =\nn\n?\nk=1\n\u03c9k(\u03b8)Pk,\u03b8(x,dy),\n(38)\nwhere for any \u03b8 \u2208 ?,\n{Pi,\u03b8,i = 1,...,n} is a family of \u201cpartial\u201d updates which\ncorrespond to all the possible partitions of vector X. Then\none can suggest updating the weights {\u03c9k(\u03b8)} according to\nsome criterion. This is of course not realistic in practice as\nsoon as the dimension nxof X is even moderate, and can\nlead to very poorly mixing algorithms since intuitively all\nthe possible transitions should be tried in order to assess\ntheir efficiency. This might be inefficient as we expect only\na restricted number of these transitions to be of real interest.\nInstead we suggest here a simple alternative which re-\nlies on principal component analysis and a natural and well\nknown generalisation able to handle multi-modal distribu-\ntions. Our algorithms rely on the recursive diagonalisation\nof either the estimates {?i} of the covariance matrix ?\u03c0or\nthe covariance matrices {?k\nmate the target distribution \u03c0, e.g. using a mixture of normal\nor Student t-distributions. We will focus here on the former\nscenario for simplicity, the extension to the mixture of dis-\ntributions case is straightforward.\n?n\nk=1\u03c9k(\u03b8) = 1, \u03c9k(\u03b8) \u2265 0 and\ni,k = 1,...,n} used to approxi-\n5.3.1 Updates description\nFormally this update is of the form (38) where Pk,\u03b8(x,dy)\nis a one-dimensional random walk Metropolis update along\neigenvector k of the covariance matrix ?\u03c0, with a scaling\nfactor ?(k) which ensures a predetermined acceptance prob-\nability. We will describe below the recursive estimation of\nthe first m (\u2264 nx) eigenvectors of ?\u03c0, which we assume"},{"page":22,"text":"364Stat Comput (2008) 18: 343\u2013373\nformthecolumnsofan nx\u00d7m matrix W (thecolumnsbeing\ndenoted w(l), l = 1,...,m) and the corresponding eigenval-\nues \u03c1(l). We denote hereafter \u00af \u03c1(l) := \u03c1(l)\/?m\nbitrary distribution on the first m positive integers. The up-\ndate at iteration i proceeds as follows:\np=1\u03c1(p) the\nnormalised eigenvalues and let d(1),...,d(m) denote an ar-\nAlgorithm 8 Principal components Metropolis update\n\u2022 At iteration i +1, given Xiand (?i,\u03c1i,Wi)\n1. Sample an update direction l \u223c (d(1),d(2),...,\nd(m)).\n2. Sample Zi+1\u223c N(0,?i(l)\u03c1i(l)), set Yi+1= Xi+1+\nZi+1w(l).\n3. Set Xi+1= Yi+1 with probability min{1,\u03c0(Yi+1)\/\n\u03c0(Xi)}, otherwise Xi+1= Xi.\n4. Update (?i,\u03c1i,Wi) to (?i+1,\u03c1i+1,Wi+1) in light of\nXi+1.\nIn words, at every iteration one of the available princi-\npal direction l is randomly selected, here according to the\nprobability d(1),d(2),...,d(m) (d(j) = \u00af \u03c1(j) being a pos-\nsibility) but other choices are possible, and a \u201cunivariate\u201d\nrandom walk update in the direction w(l) is then attempted\nwith an increment drawn from N(0,?i(l)\u03c1i(l)), where ?(l)\nis a directional scaling factor adjusted to ensure that updates\nin direction l have a preset acceptance probability\u2014it uses\nanupdateofthetype(22). Thisenablesfinerscalinginevery\nprincipal direction. Note that this update might correspond\nto a reducible algorithm when m < nx, but that this should\nnot be a difficulty when used in combination with other up-\ndates.\nWenowturntothedescriptionofanon-linealgorithmfor\nthe computation of the m first eigenvectors of the covariance\nmatrix of samples {Xi}. The algorithm relies on an on-line\nEM algorithm for the popular probabilistic PCA (PPCA) al-\ngorithm.\n5.3.2 Online PCA recursion\nThe basis for PPCA was laid by Tipping and Bishop (1999)\nwho endowed the problem with a linear Gaussian model.\nEven though the possibility of using an EM-algorithm is\nmentioned, it is Roweis (1997) who extends the formalism\nmore specifically to the application of such a scheme. The\napproach suffers however from a rotational ambiguity in the\nlatent variable space, since the returned vector set is a linear\nsuperposition of the principal components, inducing a need\nfor post-processing. This drawback is overcome by Ahn and\nOh (2003) through the introduction of a constrained EM-\nalgorithm that corresponds to using several coupled models,\nrather than a single model. These papers assume that all ob-\nservations are present initially, whereas the adaptive algo-\nrithm presented in this project needs to determine the prin-\ncipal eigenvectors on-line. Ghasemi and Sousa (2005) refor-\nmulate the constrained EM in order to achieve this. Roweis\n(1997) mentions an on-line version, but it was not further\nexplored here because of the inherent rotational ambiguity.\nThe structure that is employed in PPCA is closely related\nto factor analysis. This linear model is founded on the as-\nsumption that the d-dimensional data can be explained by\na m-dimensional unobservable variable Z and an additive\nnoise ?,\nXi= WZi+?,\nwhere W is a nx\u00d7 m real valued matrix of factor loadings,\nZi\n\u223c N(0,Im) and ? \u223c N(0,R), where usually R = \u03c32Inx\nfor some \u03c32> 0. It can be shown that the ML estimator of\nW contains the eigenvectors and the latent variables struc-\nture suggests the use of an EM-algorithm. It is possible to\nalter this problem in order to exactly remove the rotational\nambiguity, leading to the following recursion (for \u03c32= 0)\nWi+1= ?i+1WiL(WT\n?\nwhere ?iis an estimate of the covariance matrix of \u03c0 at it-\neration i of the algorithm, while for a square matrix L(A)\n(resp. U(A)) is the lower (resp. upper) part of A. Note the\ncomputationally interesting feature of this recursion where\nthe inversion of triangular, rather than full, matrices is re-\nquired. Roweis (1997) also provides an EM-algorithm for\nPPCA without taking the zero-error limit, in what is called\nSensible PCA.\n(39)\niid\niWi)\u22121\n\u00d7UL(WT\niWi)\u22121WT\ni?i+1WiL(WT\niWi)\u22121?\u22121\n5.4 Discrete valued random vectors: the Gaussian copula\napproach\nSo far we have implicitly assumed that \u03c0 has a density with\nrespect to the Lebesgue measure and de facto excluded the\ncase where \u03c0 is the probability distribution of a discrete\nvalued vector. This problem has been largely overlooked in\nthe literature, with however the exception of Nott and Kohn\n(2005). We here briefly describe a strategy proposed in An-\ndrieu and Moffa (2008) which, as we shall see, offers the\npossibility to exploit the tools developed for the purely con-\ntinuous case in earlier sections. It differs from the work pre-\nsented so far in this paper in that, as we shall see, the distri-\nbution \u03c0 or interest is embedded in an extended probability\nmodel, which needs to be adapted.\nInordertosimplifypresentationwewillfocusonthecase\nwhere X = {0,1}nx, the generalisation to scenarios involving\na larger number of discrete states or a mixture of continuous"},{"page":23,"text":"Stat Comput (2008) 18: 343\u2013373 365\nand discrete valued random variables being straightforward.\nNote that this simple scenario is of interest in the context\nof variable selection, but also in the context of inference in\nIsing models. The strategy consists of embedding the dis-\ncrete valued problem into a continuous framework by means\nof an auxiliary variable z taking its values in Z := Rnx. More\nprecisely, consider the following distribution\n\u02dc \u03c0\u03bc(x,z)\n:= \u03c0(x)\nnx\n?\ni=1\nN(z(i);\u03bc(i),?(i,i))\n??(i,i)(\u03bc(i))x(i)(1\u2212??(i,i)(\u03bc(i)))1\u2212x(i)\n\u00d7I{z \u2208 Ix},\nwhere ?\u03c32(u) is the cumulative distribution function of the\nunivariate centered normal distribution with variance \u03c32,\n\u03bc,? \u2208 Rnx\u00d7 Rnx\u00d7nxand Ix:= Ix(1)\u00d7 Ix(2)\u00d7 \u00b7\u00b7\u00b7 \u00d7 Ix(nx)\nwith I0:= (\u2212\u221e,0] and I1:= (0,+\u221e). Note that the eval-\nuation of ?\u03c32(u) is routine, and that whenever \u03c0(x) can\nbe evaluated pointwise up to a normalising constant so can\n\u02dc \u03c0\u03bc(x,z), therefore allowing the use of standard sampling al-\ngorithms. One can notice that marginally \u02dc \u03c0\u03bc(x) = \u03c0(x) but\nalso that\n\u02dc \u03c0\u03bc(x(i)|z) \u221d I{z(i) \u2208 Ix(i)},\na type of deterministic relationship between z and x. These\nproperties suggest that the problem of sampling from \u03c0(x)\ncan be replaced by that of effectively sampling the continu-\nous component z \u223c \u02dc \u03c0\u03bc(z) followed by the determination of\nthe unique x satisfying I{z \u2208 Ix} = 1.\nNaturally not all choices of \u03bc \u2208 Rnxwill lead to efficient\nsampling algorithms for a given distribution \u03c0 and we note\nin addition that the component z does not capture the de-\npendence structure of \u03c0(x). We shall see now how adap-\ntive procedures can be of great help here. Consider the fol-\nlowing distribution and denote \u03b8 := {\u03bc, \u02d8 \u03bc,?,\u02d8?} for some\n\u02d8 \u03bc,\u02d8? \u2208 Rnx\u00d7Rnx\u00d7nx\n\u02d8 q\u03b8(x,z) := N(z; \u02d8 \u03bc,\u02d8?) I{z \u2208 Ix},\nwhose marginal \u02d8 q\u03b8(x) is often used to model the distribu-\ntion of multivariate discrete valued random vectors e.g. in\nthe context of multivariate probit regression models. A par-\nticular strength of the model is that the dependence struc-\nture of \u02d8 q\u03b8(x) is parametrised by the pair \u02d8 \u03bc,\u02d8? and that\nsampling from \u02d8 q\u03b8(x) is straightforward. However \u02d8 q\u03b8(x) is\nusually intractable, precluding its direct use to approximate\n\u03c0(x). A natural suggestion here is simply to work on the\nextended space X\u00d7Z and approximate \u02dc \u03c0\u03b8(x,z) := \u02dc \u03c0\u03bc(x,z)\nwith \u02d8 q\u03b8(x,z). For example, with the natural choice \u03bc = \u02d8 \u03bc\nand ? =\u02d8? one could suggest minimising the following\nKullback-Leibler divergence\n?\nX\u00d7Z\n\u02dc \u03c0\u03b8(x,z)log\u02dc \u03c0\u03b8(x,z)\n\u02d8 q\u03b8(x,z)dxdz.\nGiven the structure of \u02dc \u03c0\u03b8(x,z), it is clear that the result-\ning \u02d8 q\u03b8(x,z) is meant to \u201clearn\u201d both the marginals and\nthe dependence structure of \u03c0(x). Assuming that this can\nbe achieved, even approximately, \u02d8 q\u03b8(x,z) and its parame-\nters can be used in multiple ways in order to sample from\n\u02dc \u03c0\u03b8(x,z). Following the ideas developed in earlier sections,\none could suggest to use \u02d8 q\u03b8(x,z) as a proposal distribution\nin an IMH algorithm targeting \u02dc \u03c0\u03b8(x,z). Indeed sampling\nfrom \u02d8 q\u03b8(x,z) is straightforward since it only requires one\nto sample from Z \u223c N(\u03bc,?) and to determine X such that\nI{Z \u2208 IX} = 1. However, as argued earlier, using the IMH\nsampler is not always a good idea, and one could instead\nsuggest a more robust random walk Metropolis type algo-\nrithm. For example, for some \u03bb > 0 and \u03b8, we have\nAlgorithm 9 The Gaussian copula SRWM\n\u2022 At iteration i +1, given Xi\n1. Sample Z = Zi+W with W \u223c N(0,\u03bb\u02d8?).\n2. Determine X such that I{Z \u2208 IX} = 1.\n3. Set (Xi+1,Zi+1) = (X,Z) with probability\n?\notherwise (Xi+1,Zi+1) = (Xi,Zi).\nmin1,\n\u02dc \u03c0\u03b8(X,Z)\n\u02dc \u03c0\u03b8(Xi,Zi)\n?\n,\nThe problem of effectively determining \u02d8? can be ad-\ndressed by using an adaptive algorithm, and in particular by\nusing recursions of the type (31) or (32) in the case of an up-\ndate component by component for example. More generally\nall the strategies developed earlier in this paper for the con-\ntinuous case can be adapted to the discrete setup (Andrieu\nand Moffa 2008). Note however that the target distribution\nnow depends on \u03b8 and that a slight modification of the con-\nvergence theory outlined earlier is required in this scenario.\n6 Examples of applications\n6.1 Erratic normal distribution\nIn this section we first demonstrate the practical interest of\nthe idea of compound criteria developed in Sect. 5.1 which\naims to accelerate the learning of features of the target dis-\ntribution by the algorithm. Following Roberts and Rosen-\nthal (2006) we consider a normal distribution N(0,?\u03c0=\nMMT) defined on X = Rnxwith M a nx\u00d7 nxmatrix with\ni.i.d. entries sharing the distribution N(0,1)\u2014we focus here\nonthecase nx= 50.Thealgorithmweuseconsistsofa mix-\nture of Algorithms 4\u20135 and 8. Comparison with the stan-\ndard AM algorithm is provided in Figs. 1\u20133 for a realisa-\ntion of each of the algorithm and for the same number of"},{"page":24,"text":"366 Stat Comput (2008) 18: 343\u2013373\nFig. 1 Comparison of the\nexpected acceptance probability\nof the standard AM algorithm\n(bottom) and the corresponding\nglobal update used by the\nmulti-criteria algorithm as a\nfunction of the iterations\nFig. 2 Comparison of the R\ncoefficient for the standard AM\nalgorithm (top) and the\nmulti-criteria algorithm\n(bottom) as a function of the\niterations\nFig. 3 Comparison of the 50\nordered estimated eigenvalues\nafter 50,000 iterations. Top:\ntruth. Middle: multi-criteria\nalgorithm. Bottom: standard AM\nalgorithm\nevaluations of \u03c0. The coefficient R \u2265 1 is precisely defined\nin Roberts and Rosenthal (2006). It is a measure of mis-\nmatch between ?\u03c0 and any arbitrary covariance matrix ?\nrelated to the asymptotic performance of the N-SRWM, the\nvalue R = 1 corresponding to optimality. Although poten-\ntially useful the comparison of the eigenvalues alone might\nbe misleading without a comparison of the quality of the\neigenvectors\u2014the R coefficient does this.\nThe gains are clear in terms of the number of evaluations\nof the target density, whose computational cost will in gen-\neral dominate that of the additional recursions needed for\nadaptation.\n6.2 The banana example\nThe banana distribution, introduced in Haario et al. (1999)\nand Haario et al. (2001) is a popular example to test adap-\ntive algorithms since it presents the advantage of analytical\ntractability of numerous characteristics, while allowing for a\nTable 1 Summaries (mean+\/\u2212std): Norm of the first moment\u2019s es-\ntimator, based on 100 runs. Different banana-shaped Gaussian distrib-\nutions are used and compared to the results of the adaptive Metropolis\nsampler (AM) presented by Haario et al. (1999). Since the target is\ncentered, the norm\u2019s correct value is zero\nnx\nNorm?E\u03c0[X]?\n\u03c01= B0.03\nMulti-criteria\n\u03c02= B0.1\nMulti-criteriaAMAM\n2\n4\n8\n1.13\u00b10.74\n1.33\u00b10.79\n1.17\u00b10.67\n1.10\u00b10.67\n1.27\u00b10.77\n1.31\u00b10.72\n2.80\u00b11.47\n5.20\u00b15.69\n4.99\u00b13.99\n2.62\u00b11.61\n5.13\u00b112.85\n4.85\u00b14.20\nnon-linear dependency between its components. Formally it\nis the distribution of a normally distributed multivariate nor-\nmal random X \u223c N(0,?) for nx\u2265 2 which undergoes the\ntransformation\n[X1,X2+b(X2\n1\u2212100),X3,...,Xnx],"},{"page":25,"text":"Stat Comput (2008) 18: 343\u2013373 367\nTable 2 Empirical quantiles of adaptive MCMC output based on 25 runs of length 80,000 (burn-in: 60,000 lags), Banana-shaped target B0.03in\nnxdimensions and an adaptive mixture of three Gaussian distributions, used as proposal, maximum deviation per dimension in red\nnx\nBanana-shaped target \u03c01= B0.03\nQuantile (in %)\n1020 3040 50 6070 8090\n2\n5\n7\n9\n9.60\u00b10.60\n9.55\u00b10.75\n9.79\u00b10.81\n9.69\u00b11.12\n10.27\u00b11.14\n19.54\u00b10.74\n19.33\u00b11.08\n19.49\u00b11.28\n19.58\u00b11.87\n20.46\u00b11.84\n29.29\u00b11.07\n29.05\u00b11.36\n29.43\u00b11.56\n29.57\u00b12.33\n30.81\u00b12.23\n39.52\u00b11.34\n39.17\u00b11.67\n39.46\u00b12.09\n39.47\u00b12.54\n40.77\u00b12.39\n49.63\u00b11.58\n49.32\u00b11.98\n49.51\u00b12.27\n49.47\u00b12.60\n50.83\u00b12.31\n59.78\u00b11.85\n59.42\u00b12.18\n59.58\u00b12.18\n59.56\u00b12.43\n60.95\u00b12.00\n70.14\u00b11.87\n69.37\u00b11.97\n69.66\u00b12.04\n69.71\u00b11.95\n70.95\u00b11.81\n80.38\u00b11.65\n79.65\u00b11.80\n79.87\u00b11.62\n79.47\u00b11.62\n80.86\u00b11.54\n90.22\u00b11.24\n89.93\u00b11.25\n90.15\u00b11.18\n90.22\u00b11.22\n90.41\u00b11.0315\nTable 3 Empirical quantiles of adaptive MCMC output based on 25 runs of length 80,000 (burn-in: 60,000 lags), Banana-shaped target B0.1in nx\ndimensions and an adaptive mixture of three Gaussian distributions, used as proposal\nnx\nBanana-shaped target \u03c02= B0.1\nQuantile (in %)\n1020 3040 5060 7080 90\n2\n5\n7\n9.60\u00b10.60\n9.55\u00b10.75\n9.79\u00b10.81\n19.54\u00b10.74\n19.33\u00b11.08\n19.49\u00b11.28\n29.29\u00b11.07\n29.05\u00b11.36\n29.43\u00b11.56\n39.52\u00b11.34\n39.17\u00b11.67\n39.46\u00b12.09\n49.63\u00b11.58\n49.32\u00b11.98\n49.51\u00b12.27\n59.78\u00b11.85\n59.42\u00b12.18\n59.58\u00b12.18\n70.14\u00b11.87\n69.37\u00b11.97\n69.66\u00b12.04\n80.38\u00b11.65\n79.65\u00b11.80\n79.87\u00b11.62\n90.22\u00b11.24\n89.93\u00b11.25\n90.15\u00b11.18\nand we denote hereafter Bb(?) the distribution of this ran-\ndom vector, and simply Bbwhen ? is the identity matrix,\nexcept for the top left element which is 100. We compare\nthe performance of a mixture of updates based on Algo-\nrithm 7 which uses for each of the mixture component ei-\nther Algorithm 6 or Algorithm 8 with that of the AM al-\ngorithm (Haario et al. 1999), for 10,000 iterations for B0.03\nand 20,000 iterations for B0.1and n = 3 components for the\nfitted mixture. The results are summarised in Table 1 seem\ncomparable,althoughouralgorithmseemstobemorerobust\nin the difficult situation where \u03c0 = B0.1.\nWe further tested the ability of the algorithm to properly\nsample from the target distribution by comparing empirical\nand exact quantiles. The results and methodology are sum-\nmarised in Tables 2 and 3.\nThe fitted mixture makes it possible to estimate the nor-\nmalising constant of the target distribution \u03c0, using the so-\ncalled \u201charmonic mean\u201d estimator, which relies on the iden-\ntity\n?\nX\n\u02d8 q\u03b8(x)\n\u02dc \u03c0(x)\u03c0(x)dx =\nwhere \u02dc \u03c0(x) is proportional to \u03c0(x), but unormalised. Note\nthat \u02d8 q\u03b8(x) provides us with a potentially reasonable instru-\nmental distribution since it is adapted to fit \u03c0 and might have\nthinner tails than \u03c0. This is estimator is notoriously known\nto be unstable wheneverthe variance of \u02d8 q\u03b8(x)\/\u02dc \u03c0(x) under \u03c0\nis large and the suggested approach might in some situations\n1\n?\nX\u02dc \u03c0(x)dx=: 1\/Z,\n(40)\nTable 4 Harmonic mean estimator of the normalizing constant of\ncentered spherical Gaussian distributions and banana-shaped distrib-\nutions F0.03(X), obtained by applying to a Gaussian N(0,S) with\ndiag(S) = [100,1,...,1] in d dimensions; Z is the partition function\u2019s\nanalytical value\nnx\n\u03c01= N(0,Inx)\n\u02c6Z\n\u03c02= B0.03\n\u02c6ZZZ\n2\n5\n7\n6.27\u00b10.01\n97.53\u00b10.23\n601.72\u00b12.57\n6.28\n98.96\n621.77\n68.9\u00b115.5\n1013.7\u00b1178.8\n6204.6\u00b11337.6\n62.831\n989.577\n6217.696\nremedythis.In the case of thebananashapeddistributionwe\nchoose \u02dc \u03c0(x) such that\nZ = det(2\u03c0?)1\/2.\nWe present results for both B0.03and N(0,Inx), based on\n50 runs, in Table 4.\nFor each of them the chain was run with three adaptive\nGaussian components for 100,000 iterations. The estimator\n\u02c6Z was calculated according with the harmonic mean estima-\ntorafteraburn-inperiodof80,000iterations.Theestimation\nof the Gaussian target\u2019s partition function is very accurate.\nInsevendimensions Z issomewhatunderestimatedsuggest-\ning that the chain was not run long enough to reach its sta-\ntionary regime. The second target\u2019s non-linearity leads to a\nsignificant deterioration of the simulation results. While the\nsample mean is close to the partition function\u2019s true value"},{"page":26,"text":"368Stat Comput (2008) 18: 343\u2013373\nFig. 4 Top: Trace of the\npositions s1,s2for k = 2\ncorresponding to iterations\n1,...,40,000. Bottom:\nHistograms of the dates for\nk = 2 after 200,000 iterations\n(with the first 10,000 samples\ndiscarded)\nthe sample deviation is very large. A possible explanation is\nthat the chain has to be run much longer in this setting to\nensure convergence of the harmonic mean estimator.\nA possible use of this result is that of the estimation of\nposterior model probabilities.\n6.3 Mine disaster data\nThe dataset of this classic example consists of the recorded\ndates (in days) {y(i)} at which mine disaster have occurred\nover a period covering 1851\u20131962. The data is modelled\nas a Poisson process with intensity x(t) modelled as a step\nfunction consisting of k +1 plateaux with starting positions\ns(0) = 0 < s(1) < s(2) < \u00b7\u00b7\u00b7 < s(k + 1) = T and heights\nh(0),h(1),...,h(k) that is\nx(t) =\nk+1\n?\ni=1\nh(i \u22121)I{s(i \u22121) \u2264 t < s(i)}.\nThe unknowns are therefore k,s := {s(i)} and h := {h(i)}.\nWith the priors of Green (1995) the log-posterior distribu-\ntion, log\u03c0(x), is the sum of the three following terms with\n\u2212?+klog(?)\u2212log(?(k +1))+log?(2(k +1))\n\u2212(2k +1)logT,\n(k +1)(\u03b1log\u03b2 \u2212log?(\u03b1))+(\u03b1 \u22121)\nk+1\n?\ni=1\nlog(h(i \u22121))\n\u2212\u03b2\nk+1\n?\ni=1\nh(i \u22121)+\nk+1\n?\ni=1\nlog(s(i)\u2212s(i \u22121)),\nk+1\n?\ni=1\nlogh(i \u22121)\nn\n?\nj=1\nI{s(i \u22121) \u2264 y(j) < s(i)}\n\u2212\nk+1\n?\ni=1\nh(i \u22121)(s(i)\u2212s(i \u22121)).\nIn our numerical experiments we took \u03b1 = 1.0, \u03b2 = 200 and\n? = 3,whichisinlinewithGreen(1995)andHastie(2005),\nand simply provided our adaptive algorithm, a combination\nof the components described in Sect. 5, i.e. a mixture of Al-\ngorithms 4\u20136 and 8, with the log-posterior above. We report\nhere the results obtained using one normal component, and\ndid not observe any significant different with 2 or 3 com-\nponents. One of the difficulty with the posterior distribution\nof interest is that it involves very different scales and var-\nious dependence patterns between the parameters. We ran\nthe algorithm for fixed k = 1,2,3,4,5,6. In all scenarios\nthe components of h and s were initialised at 1000 and the\ninitial value for the estimate of the covariance matrix of \u03c0\nwas set to 10\u00d7I2k+1. In order to comment on the behaviour\nof the adaptive procedure, we primarily focus on the case\nk = 2 in order to maintain the legibility of the various fig-\nures. In Figs. 4\u20136 and 7 we present the traces and in relevant\ncases histograms for {si}, {hi}, {\u03bbi} (the scaling coefficient\nof the global RWM update), the corresponding running ex-\npected acceptance probabilities, {(\u03bb1\ning coefficients of the componentwise RWM updates) and\ntheir corresponding running expected acceptance probabili-\nties.Inthiscasethealgorithmwasranfor200,000iterations.\nThe reported robust behaviour of the algorithm is typical of\nwhat we have systematically observed for all the realisations\ni,...,\u03bb2k+1\ni\n)} (the scal-"},{"page":27,"text":"Stat Comput (2008) 18: 343\u2013373 369\nFig. 5 Top: Trace of the\nintensities h0,h1,h2for k = 2\ncorresponding to iterations\n1,...,40,000. Bottom:\nHistogram of the intensities for\nk = 2 after 200,000 iterations\n(with the first 10,000 samples\ndiscarded)\nFig. 6 Top: Trace of the\n\u201cglobal\u201d RWM update\u2019s log(\u03bb).\nBottom: Running expected\nacceptance probability of the\n\u201cglobal\u201d RWM update\nof the algorithm that we have run. Despite poor initialisa-\ntions of s,h and the parameters of the algorithm (observe\nin particular the high rejection rate during the first 10,000\niterations particularly visible in Fig. 5) the procedure man-\nages to rapidly recover. The histograms show that our results\nare in accordance with the results found in Hastie (2005).\nThe behaviour of {\u03bbi} and {(\u03bb1\nsponding running expected acceptance probabilities demon-\nstrate both the interest of adapting these parameters in the\ninitial phase of the algorithm, and the notion of bold and\ni,...,\u03bb2k+1\ni\n)} and their corre-\ntimid moves: small acceptance probabilities prompt the use\nof smaller scaling factors in order to improve exploration\nand timid moves seem to improve their performance faster\nthan bold moves (whose expected acceptance probabilities\nis multiplied by 5 in the course of the first 200,000 itera-\ntions). Naturally we observed that not all the parameters of\nthe algorithm seem to have converged, or stabilised around\nfixed values, whereas the histograms for s and h seem to\nbe in agreement with previously reported results (e.g. Hastie\n2005). The observed performance of the algorithm is in our"},{"page":28,"text":"370 Stat Comput (2008) 18: 343\u2013373\nFig. 7 k = 2: Top: Trace of the\n\u201clocal\u201d RWM updates\u2019 log(\u03bb)\u2019s.\nBottom: Running expected\nacceptance probability of the\n\u201clocal\u201d RWM updates\nFig. 8 Top: Trace of the\npositions s1,s2,s3for k = 3\ncorresponding to iterations\n1,...,40,000. Bottom: Trace of\nthe intensities h0,h1,h2,h3for\nk = 3 corresponding to\niterations 1,...,40,000\nview illustrative of three crucial points discussed earlier in\nthe paper:\n1. Vanishing adaptation does not require convergence to en-\nsure that ergodic averages are asymptotically correct,\n2. but at the same time the study of the convergence prop-\nerties of {\u03b8i} is fundamental since it ensures that this se-\nquence is guaranteed to eventually approach the optimal\nvalues defined by our criteria. It is indeed the conver-\ngence properties of {\u03b8i} which explain both the observed\ngood behaviour of {\u03bbi} and {(\u03bb1\nand 7. As a result, and provided that we are ready to run\nthe algorithm longer then one can expect to be able to\nobtain \u201cbetter\u201d values for the tuning parameter.\n3. The user might decide to use this run as a preliminary\nrun to determine a satisfactory tuning parameter \u03b8 which\ncan then be used in a standard non-adaptive MCMC al-\ngorithm, for which none of the ergodicity problems dis-\ncussedearlierexist.Effectively,ifthisisthechoicemade,\nthis preliminary run is simply an optimisation procedure,\ni,...,\u03bb2k+1\ni\n)} in Figs. 6"},{"page":29,"text":"Stat Comput (2008) 18: 343\u2013373371\nFig. 9 k = 3: Top: Trace of the\n\u201clocal\u201d RWM updates\u2019 log(\u03bb)\u2019s.\nBottom: Running expected\nacceptance probability of the\n\u201clocal\u201d RWM updates\nwhich however requires the use of samples at least ap-\nproximately distributed according to the posterior distri-\nbution \u03c0, therefore justifying the study of the ergodicity\nproperties of such algorithms.\nWe report the corresponding results for the case k = 3 in\nFigs. 8\u20139. Due to the positivity constraints the harmonic\nmean estimator in (40) cannot be mathematically exact. De-\nspite finding results similar to those of Green (2003) and\nHastie (2005) we cannot in this approach as a reliable one.\nAcknowledgements\nassociate editor for their great patience. They would like to thank the\nreviewers, David Hastie and Arnaud Doucet for very useful comments\nwhich have helped to improve the manuscript.\nThe authors are very grateful to the editor and\nReferences\nAhn, J.-H., Oh, J.-H.: A constrained EM algorithm for principal com-\nponent analysis. Neural Comput. 15, 57\u201365 (2003)\nAndrad\u00f3ttir, S.: A stochastic approximation algorithm with varying\nbounds. Oper. Res. 43(6), 1037\u20131048 (1995)\nAndrieu, C.: Discussion of Haario, H., Laine, M., Lehtinen, M., Saks-\nman, E.: Markov chain Monte Carlo methods for high dimen-\nsional inversion in remote sensing (December 2003). J. R. Stat.\nSoc. Ser. B 66(3), 497\u2013813 (2004)\nAndrieu, C., Atchad\u00e9, Y.F.: On the efficiency of adaptive MCMC algo-\nrithms. Electron. Commun. Probab. 12, 336\u2013349 (2007)\nAndrieu, C., Doucet, A.: Discussion of Brooks, S.P., Giudici, P.,\nRoberts, G.O.: Efficient construction of reversible jump Markov\nchain Monte Carlo proposal distributions. Part 1. J. R. Stat. Soc.\nB 65, 3\u201355 (2003)\nAndrieu, C., Jasra, A.: Efficient and principled implementation of the\ntempering procedure. Tech. Rep. University of Bristol (2008)\nAndrieu, C., Moffa, G.: A Gaussian copula approach for adaptation in\ndiscrete scenarios (2008, in preparation)\nAndrieu, C., Moulines, \u00c9.: On the ergodicity properties of some adap-\ntive MCMC algorithms. Ann. Appl. Probab. 16(3), 1462\u20131505\n(2006)\nAndrieu, C., Robert, C.P.: Controlled MCMC for optimal sampling.\nTech. Rep. 0125, Cahiers de Math\u00e9matiques du Ceremade, Uni-\nversit\u00e9 Paris-Dauphine (2001)\nAndrieu, C., Tadi\u00b4 c, V.B.: The boundedness issue for controlled MCMC\nalgorithms. Tech. Rep. University of Bristol (2007)\nAndrieu, C., Moulines, \u00c9., Priouret, P.: Stability of stochastic ap-\nproximation under verifiable conditions. SIAM J. Control Optim.\n44(1), 283\u2013312 (2005)\nAtchad\u00e9, Y.F.: An adaptive version for the Metropolis adjusted\nLangevin algorithm with a truncated drift. Methodol. Comput.\nAppl. Probab. 8, 235\u2013254 (2006)\nAtchad\u00e9, Y.F., Fort, G.: Limit Theorems for some adaptive MCMC al-\ngorithms with subgeometric kernels. Tech. Rep. (2008)\nAtchad\u00e9, Y.F., Liu, J.S.: The Wang-Landau algorithm in general state\nspaces: applications and convergence analysis. Technical report\nUniv. of Michigan (2004)\nAtchad\u00e9, Y.F., Rosenthal, J.S.: On adaptive Markov chain Monte Carlo\nalgorithms. Bernoulli 11, 815\u2013828 (2005)\nBai, Y., Roberts, G.O., Rosenthal, J.S.: On the Containment Condition\nfor Adaptive Markov Chain Monte Carlo Algorithms. Tech. Rep.\nUniversity of Toronto (2008)\nB\u00e9dard, M.: Optimal acceptance rates for metropolis algorithms: mov-\ning beyond 0.234. Tech. Rep. University of Montr\u00e9al (2006)\nB\u00e9dard, M.: Weak convergence of metropolis algorithms for non-i.i.d.\ntarget distributions. Ann. Appl. Probab. 17, 1222\u20131244 (2007)\nBennet, J.E., Racine-Poon, A., Wakefield, J.C.: MCMC for nonlinear\nhierarchical models. In: MCMC in Practice. Chapman & Hall,\nLondon (1996)\nBenveniste, A., M\u00e9tivier, M., Priouret, P.: Adaptive Algorithms and\nStochastic Approximations. Springer, Berlin (1990)\nBesag, J., Green, P.J.: Spatial statistics and Bayesian computation. J. R.\nStat. Soc. Ser. B Stat. Methodol. 55, 25\u201337 (1993)\nBorkar, V.S.: Topics in Controlled Markov Chains. Longman, Harlow\n(1990)\nBrowne, W.J., Draper, D.: Implementation and performance issues in\nthe Bayesian and likelihood fitting of multilevel models. Comput.\nStat. 15, 391\u2013420 (2000)"},{"page":30,"text":"372 Stat Comput (2008) 18: 343\u2013373\nCapp\u00e9, O., Douc, R., Gullin, A., Marin, J.-M., Robert, C.P.: Adap-\ntive Importance Sampling in General Mixture Classes. Preprint\n(2007)\nCeperley, D., Chester, G.V., Kalos, M.H.: Monte Carlo simulation of a\nmany fermion study. Phys. Rev. B 16(7), 3081\u20133099 (1977)\nChauveau, D., Vandekerkhove, P.: Improving convergence of the\nHastings-Metropolis algorithm with an adaptive proposal. Scand.\nJ. Statist. 29(1), 13\u201329 (2001)\nChen, H.F., Guo, L., Gao, A.J.: Convergence and robustness of the\nRobbins-Monro algorithm truncated at randomly varying bounds.\nStoch. Process. Their Appl. 27(2), 217\u2013231 (1988)\nChib, S., Greenberg, E., Winkelmann, R.: Posterior simulation and\nBayes factors in panel count data models. J. Econ. 86, 33\u201354\n(1998)\nde Freitas, N., H\u00f8jen-S\u00f8rensen, P., Jordan, M., Russell, S.: Variational\nMCMC. In: Proceedings of the 17th Conference in Uncertainty in\nArtificialIntelligence,pp.120\u2013127.MorganKaufman,SanMateo\n(2001). ISBN:1-55860-800-1\nDelmas, J.-F., Jourdain, B.: Does waste-recycling really improve\nMetropolis-Hastings Monte Carlo algorithm? Tech. Rep. Cer-\nmics, ENPC (2007)\nDelyon, B.: General results on the convergence of stochastic algo-\nrithms. IEEE Trans. Automat. Control 41(9), 1245\u20131256 (1996)\nDelyon, B., Juditsky, A.: Accelerated stochastic approximation. SIAM\nJ. Optim. 3(4), 868\u2013881 (1993)\nDouglas, C.: Simple adaptive algorithms for cholesky, LDLT, QR,\nand eigenvalue decompositions of autocorrelation matrices for\nsensor array data. In: Signals, Systems and Computers, 2001,\nConference Record of the Thirty-Fifth Asilomar Conference, vol.\n21, pp. 1134\u20131138 (2001)\nErland, S.: On Adaptivity and Eigen-Decompositions of Markov\nChains. Ph.D. thesis Norwegian University of Science and Tech-\nnology (2003)\nFrenkel, D.: Waste-recycling Monte Carlo. In: Computer Simulations\nIn Condensed Matter: from Materials to Chemical Biology. Lec-\nture Notes in Physics, vol. 703, pp. 127\u2013138. Springer, Berlin\n(2006)\nG\u00e5semyr, J.: On an adaptive Metropolis-Hastings algorithm with in-\ndependent proposal distribution. Scand. J. Stat. 30(1), 159\u2013173\n(2003). ISSN 0303-6898\nG\u00e5semyr, J., Natvig, B., Nyg\u00e5rd, C.S.: An application of adaptive inde-\npendent chain Metropolis\u2013Hastings algorithms in Bayesian haz-\nard rate estimation. Methodol. Comput. Appl. Probab. 6(3), 293\u2013\n302(10) (2004)\nGelfand, A.E., Sahu, S.K.:On Markov chain Monte Carlo acceleration.\nJ. Comput. Graph. Stat. 3(3), 261\u2013276 (1994)\nGelman,A.,Roberts,G.,Gilks,W.:EfficientMetropolisjumpingrules.\nIn: Bayesian Statistics, vol. 5. Oxford University Press, New York\n(1995)\nGeyer, C.J., Thompson, E.A.: Annealing Markov chain Monte Carlo\nwith applications to ancestral inference. J. Am. Stat. Assoc. 90,\n909\u2013920 (1995)\nGhasemi, A., Sousa, E.S.: An EM-based subspace tracker for wire-\nless communication applications. In: Vehicular Technology Con-\nference. VTC-2005-Fall. IEEE 62nd, pp. 1787\u20131790 (2005)\nGilks, W.R., Roberts, G.O., George, E.I.: Adaptive direction sampling.\nThe Statistician 43, 179\u2013189 (1994)\nGilks, W.R., Roberts, G.O., Sahu, S.K.: Adaptive Markov chain Monte\nCarlo through regeneration. J. Am. Stat. Assoc. 93, 1045\u20131054\n(1998)\nGiordani, P., Kohn, R.: Efficient Bayesian inference for multiple\nchange-point and mixture innovation models. Sveriges Riksbank\nWorking Paper No. 196 (2006)\nGreen, P.J.: Reversible jump Markov chain Monte Carlo computa-\ntion and Bayesian model determination. Biometrika 82, 711\u2013732\n(1995)\nGreen, P.J.: Trans-dimensional Markov chain Monte Carlo. In: Green,\nP.J., Hjort, N.L., Richardson, S. (eds.) Highly Structured Stochas-\ntic Systems. Oxford Statistical Science Series, vol. 27, pp. 179\u2013\n198. Oxford University Press, London (2003)\nGreen, P.J., Mira, A.: Delayed rejection in reversible jump Metropolis-\nHastings. Biometrica 88(3) (2001)\nHaario, H., Saksman, E., Tamminen, J.: Adaptive proposal distribution\nfor random walk Metropolis algorithm. Comput. Stat. 14(3), 375\u2013\n395 (1999)\nHaario, H., Saksman, E., Tamminen, J.: An adaptive Metropolis algo-\nrithm. Bernoulli 7(2), 223\u2013242 (2001)\nHaario, H., Laine, M., Mira, A., Saksman, E.: DRAM: Efficient adap-\ntive MCMC (2003)\nHaario, H., Laine, M., Lehtinen, M., Saksman, E.: Markov chain\nMonte Carlo methods for high dimensional inversion in remote\nsensing. J. R. Stat. Soc. Ser. B 66(3), 591\u2013607 (2004)\nHaario, H., Saksman, E., Tamminen, J.: Componentwise adaptation for\nhigh dimensional MCMC. Comput. Stat. 20, 265\u2013274 (2005)\nHastie, D.I.: Towards automatic reversible jump Markov chain Monte\nCarlo. Ph.D. thesis Bristol University, March 2005\nHolden, L.: Adaptive chains. Tech. Rep. Norwegian Computing Center\n(1998)\nHolden, L. et al.: History matching using adaptive chains. Tech. Report\nNorwegian Computing Center (2002)\nKesten, H.: Accelerated stochastic approximation. Ann. Math. Stat.\n29(1), 41\u201359 (1958)\nKim, S., Shephard, N., Chib, S.: Stochastic volatility: likelihood infer-\nence and comparison with ARCH models. Rev. Econ. Stud. 65,\n361\u2013393 (1998)\nLaskey, K.B., Myers, J.: Population Markov chain Monte Carlo. Mach.\nLearn. 50(1\u20132), 175\u2013196 (2003)\nLiu, J., Liang, F., Wong, W.H.: The use of multiple-try method and\nlocal optimization in Metropolis sampling. J. Am. Stat. Assoc.\n95, 121\u2013134 (2000)\nMykland, P., Tierney, L., Yu, B.: Regeneration in Markov chain sam-\nplers. J. Am. Stat. Assoc. 90, 233\u2013241 (1995)\nNott, D.J., Kohn, R.: Adaptive sampling for Bayesian variable selec-\ntion. Biometrika 92(4), 747\u2013763 (2005)\nPasarica, C., Gelman, A.: Adaptively scaling the Metropolis algorithm\nusing the average squared jumped distance. Tech. Rep. Depart-\nment of Statistics, Columbia University (2003)\nPlakhov, A., Cruz, P.: A stochastic approximation algorithm with step-\nsize adaptation. J. Math. Sci. 120(1), 964\u2013973 (2004)\nRamponi, A.: Stochastic adaptive selection of weights in the simulated\ntempering algorithm. J. Ital. Stat. Soc. 7(1), 27\u201355 (1998)\nRobbins, H., Monro, S.: A stochastic approximation method. Ann.\nMath. Stat. 22, 400\u2013407 (1951)\nRobert, C.P., Casella, G.: Monte Carlo Statistical Methods. Springer,\nBerlin (1999)\nRoberts, G.O., Rosenthal, J.: Optimal scaling of discrete approxima-\ntion to Langevin diffusion. J. R. Stat. Soc. B 60, 255\u2013268 (1998)\nRoberts, G.O., Rosenthal, J.S.: Examples of adaptive MCMC. Techni-\ncal Report University of Toronto (2006)\nRoberts, G.O., Rosenthal, J.S.: Coupling and ergodicity of adaptive\nMCMC. J. Appl. Probab. 44(2), 458\u2013475 (2007)\nRoberts, G.O., Gelman, A., Gilks, W.: Weak convergence and opti-\nmal scaling of random walk Metropolis algorithms. Ann. Appl.\nProbab. 7, 110\u2013120 (1997)\nRoweis, S.: EM algorithms for PCA and SPCA. Neural Inf. Process.\nSyst. 10, 626\u2013632 (1997)\nSahu, S.K., Zhigljavsky, A.A.: Adaptation for self regenera-\ntive MCMC. Available from http:\/\/www.maths.soton.ac.uk\/staff\/\nSahu\/research\/papers\/self.html\nSaksman, E., Vihola, M.: On the ergodicity of the adaptive Metropolis\nalgorithm on unbounded domains (2008). arXiv:0806.2933"},{"page":31,"text":"Stat Comput (2008) 18: 343\u2013373373\nSherlock, C., Roberts, G.O.: Optimal scaling of the random walk\nMetropolis on elliptically symmetric unimodal targets. Tech. Rep.\nUniversity of Lancaster (2006)\nSims, C.A.: Adaptive Metropolis-Hastings algorithm or Monte Carlo\nkernel estimation. Tech. report Princeton University (1998)\nSpall, J.C.: Adaptive stochastic approximation by the simultane-\nous perturbation method. IEEE Trans. Automat. Control 45(10),\n1839\u20131853 (2000)\nStramer, O., Tweedie, R.L.: Langevin-type models II: self-targeting\ncandidates for MCMC algorithms. Methodol. Comput. Appl.\nProbab. 1(3), 307\u2013328 (1999)\nSutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction.\nMIT Press, Cambridge (1998)\nTierney, L., Mira, A.: Some adaptive Monte Carlo methods for\nBayesian inference. Stat. Med. 18, 2507\u20132515 (1999)\nTipping, M.E., Bishop, C.M.: Probabilistic principal component analy-\nsis. J. R. Stat. Soc. Ser. B Stat. Methodol. 61, 611\u2013622 (1999)\nWinkler,G.:ImageAnalysis,RandomFieldsandMarkovChainMonte\nCarlo Methods: A Mathematical Introduction. Stochastic Mod-\nelling and Applied Probability. Springer, Berlin (2003)"}],"widgetId":"rgw28_56aba15bec488"},"id":"rgw28_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=49458431&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw29_56aba15bec488"},"id":"rgw29_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=49458431&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":49458431,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/49458431_A_tutorial_on_adaptive_MCMC","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56aba15bec488"},"id":"rgw2_56aba15bec488","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":49458431},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=49458431&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56aba15bec488"},"id":"rgw1_56aba15bec488","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"NtazlyJSqaHwzYcAPMGIBlLqewcS3KTvpzsCGVZWyJ1UH4hyqLyYrwaE1pGzOzTLOMs6vr0m5qOREwqU4NmnpGJOMajOpGq01YuxHD0JtnlMX60xEZ+zWUNPwHcr7iHy8tyz6xg1EAFxRKhXlWiIEGuAUYzP8A4D9UPOkwLz652+ACCjbLcX+AqE1VchGBKe2u83udCOPWxZazhjeR5udQgbpWYHnx5SKR\/2J3b6qJgWacxvryB+tnXL1Z8fxF189OHpshtGBt7YjYHDc+k3gjeMrK3UatntiytsWhZeuVA=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/49458431_A_tutorial_on_adaptive_MCMC\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"A tutorial on adaptive MCMC\" \/>\n<meta property=\"og:description\" content=\"We review adaptive Markov chain Monte Carlo algorithms (MCMC) as a mean to optimise their performance. Using simple toy examples we review their theoretical underpinnings, and in particular show...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/49458431_A_tutorial_on_adaptive_MCMC\/links\/0ffc9e5f0cf255165fc9f05f\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/49458431_A_tutorial_on_adaptive_MCMC\" \/>\n<meta property=\"rg:id\" content=\"PB:49458431\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1007\/s11222-008-9110-y\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"A tutorial on adaptive MCMC\" \/>\n<meta name=\"citation_author\" content=\"Christophe Andrieu\" \/>\n<meta name=\"citation_author\" content=\"Johannes Thoms\" \/>\n<meta name=\"citation_publication_date\" content=\"2008\/12\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"Statistics and Computing\" \/>\n<meta name=\"citation_issn\" content=\"0960-3174\" \/>\n<meta name=\"citation_volume\" content=\"18\" \/>\n<meta name=\"citation_issue\" content=\"4\" \/>\n<meta name=\"citation_doi\" content=\"10.1007\/s11222-008-9110-y\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/49458431_A_tutorial_on_adaptive_MCMC\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/49458431_A_tutorial_on_adaptive_MCMC\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-46c68acc-9e67-421f-877d-3c90e127a9be","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":524,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw30_56aba15bec488"},"id":"rgw30_56aba15bec488","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-46c68acc-9e67-421f-877d-3c90e127a9be", "2033c7bce59375d8134740e033eaad974e975aba");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-46c68acc-9e67-421f-877d-3c90e127a9be", "2033c7bce59375d8134740e033eaad974e975aba");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw31_56aba15bec488"},"id":"rgw31_56aba15bec488","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/49458431_A_tutorial_on_adaptive_MCMC","requestToken":"3R8jr6Hr1+1UtZqpE6jbiqeeyB6YdjMcbiA\/yDCWjS6yoVFZUVpXpFCzEv710whFVd4zttgXM40Gwj8do8OPXHXqD4nUHVYfnmnL1CLNlN1ir21oFZvH8cr9hfU1NXGUz2Dh6xcXT34nQjac5d4E1vwnzr3CM60qEGBprs+xyt7Cpb4IV6tZQ0TbXDxzk8OaIxEOqitZgE9taRFG4dywD+t7lqgpHsctE7JAyPgMU0RIo+tktTg\/BeGPfZoSvZ6PleEXvix9WFQ0oszww6qr1EHa3eHa30thHxkZAGHyXhY=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=sypKKYGrX2IT6C_1eg62sCPnlrmisGyjHU95TEwFkgkIraouK_JdmkK-SUgcmuiX","encodedUrlAfterLogin":"cHVibGljYXRpb24vNDk0NTg0MzFfQV90dXRvcmlhbF9vbl9hZGFwdGl2ZV9NQ01D","signupCallToAction":"Join for free","widgetId":"rgw33_56aba15bec488"},"id":"rgw33_56aba15bec488","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw32_56aba15bec488"},"id":"rgw32_56aba15bec488","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw34_56aba15bec488"},"id":"rgw34_56aba15bec488","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
