<!DOCTYPE html> <html lang="en" class="" id="rgw40_56ab1dc5c8ffa"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="JSEwHQZSHr6k4FCiDt2C39Nvz+rAvdDIShrgfo968QfgUU12NJuFa5VVEMmq8qz+hC1n0Q3HngoNaZhCDTIiItpB2/upGm01TME1qmuF3eq/febihN9gkeBFcqnlsxQ4lSluSmlA/TrgCsppcdze0I9qHsssggIBXnJ7sCf+HTHzuM0H0fwXJQSP6DhgH4s5Utwez2OR65jaT0bk1+ZO+efO0xElPQKz4Ck6vxpRIU7s07wUG988zHxHhT23q8ImQP/cfxZY62Ci3Zk8MVEjn/DygwA844iNl6jb0haqajE="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-ac28c13a-a67f-4225-9464-51b21ced042d",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/6690142_Bayesian_Gaussian_Process_Classification_with_the_EM-EP_Algorithm" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Bayesian Gaussian Process Classification with the EM-EP Algorithm" />
<meta property="og:description" content="Gaussian process classifiers (GPCs) are Bayesian probabilistic kernel classifiers. In GPCs, the probability of belonging to a certain class at an input location is monotonically related to the..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/6690142_Bayesian_Gaussian_Process_Classification_with_the_EM-EP_Algorithm/links/0e5fb5a5f0c41c4932e99405/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/6690142_Bayesian_Gaussian_Process_Classification_with_the_EM-EP_Algorithm" />
<meta property="rg:id" content="PB:6690142" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1109/TPAMI.2006.238" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Bayesian Gaussian Process Classification with the EM-EP Algorithm" />
<meta name="citation_author" content="Hyun-Chul Kim" />
<meta name="citation_author" content="Zoubin Ghahramani" />
<meta name="citation_pmid" content="17108369" />
<meta name="citation_publication_date" content="2007/01/01" />
<meta name="citation_journal_title" content="IEEE Transactions on Pattern Analysis and Machine Intelligence" />
<meta name="citation_issn" content="0162-8828" />
<meta name="citation_volume" content="28" />
<meta name="citation_issue" content="12" />
<meta name="citation_firstpage" content="1948" />
<meta name="citation_lastpage" content="59" />
<meta name="citation_doi" content="10.1109/TPAMI.2006.238" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/6690142_Bayesian_Gaussian_Process_Classification_with_the_EM-EP_Algorithm" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/6690142_Bayesian_Gaussian_Process_Classification_with_the_EM-EP_Algorithm" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Bayesian Gaussian Process Classification with the EM-EP Algorithm</title>
<meta name="description" content="Bayesian Gaussian Process Classification with the EM-EP Algorithm on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1dc5c8ffa" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1dc5c8ffa" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw7_56ab1dc5c8ffa">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1109%2FTPAMI.2006.238&rft.atitle=Bayesian%20Gaussian%20Process%20Classification%20with%20the%20EM-EP%20Algorithm&rft.title=IEEE%20transactions%20on%20pattern%20analysis%20and%20machine%20intelligence&rft.jtitle=IEEE%20transactions%20on%20pattern%20analysis%20and%20machine%20intelligence&rft.volume=28&rft.issue=12&rft.date=2007&rft.pages=1948-59&rft.issn=0162-8828&rft.au=Hyun-Chul%20Kim%2CZoubin%20Ghahramani&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Bayesian Gaussian Process Classification with the EM-EP Algorithm</h1> <meta itemprop="headline" content="Bayesian Gaussian Process Classification with the EM-EP Algorithm">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/6690142_Bayesian_Gaussian_Process_Classification_with_the_EM-EP_Algorithm/links/0e5fb5a5f0c41c4932e99405/smallpreview.png">  <div id="rgw10_56ab1dc5c8ffa" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw11_56ab1dc5c8ffa"> <a href="researcher/8942347_Hyun-Chul_Kim" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Hyun-Chul Kim" alt="Hyun-Chul Kim" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Hyun-Chul Kim</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56ab1dc5c8ffa">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/8942347_Hyun-Chul_Kim"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Hyun-Chul Kim" alt="Hyun-Chul Kim" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/8942347_Hyun-Chul_Kim" class="display-name">Hyun-Chul Kim</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw13_56ab1dc5c8ffa"> <a href="researcher/8159937_Zoubin_Ghahramani" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Zoubin Ghahramani</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw14_56ab1dc5c8ffa">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/8159937_Zoubin_Ghahramani"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/8159937_Zoubin_Ghahramani" class="display-name">Zoubin Ghahramani</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">  <div> Department of Industrial and Management Engineering, Pohang University of Science and Technology, Nam-gu, Pohang, Republic of China.  </div>      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/0162-8828_IEEE_Transactions_on_Pattern_Analysis_and_Machine_Intelligence"><span itemprop="name">IEEE Transactions on Pattern Analysis and Machine Intelligence</span></a> </span>    (Impact Factor: 5.78).     <meta itemprop="datePublished" content="2007-01">  01/2007;  28(12):1948-59.    DOI:&nbsp;10.1109/TPAMI.2006.238           <div class="pub-source"> Source: <a href="http://www.ncbi.nlm.nih.gov/pubmed/17108369" rel="nofollow">PubMed</a> </div>  </div> <div id="rgw15_56ab1dc5c8ffa" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>Gaussian process classifiers (GPCs) are Bayesian probabilistic kernel classifiers. In GPCs, the probability of belonging to a certain class at an input location is monotonically related to the value of some latent function at that location. Starting from a Gaussian process prior over this latent function, data are used to infer both the posterior over the latent function and the values of hyperparameters to determine various aspects of the function. Recently, the expectation propagation (EP) approach has been proposed to infer the posterior over the latent function. Based on this work, we present an approximate EM algorithm, the EM-EP algorithm, to learn both the latent function and the hyperparameters. This algorithm is found to converge in practice and provides an efficient Bayesian framework for learning hyperparameters of the kernel. A multiclass extension of the EM-EP algorithm for GPCs is also derived. In the experimental results, the EM-EP algorithms are as good or better than other methods for GPCs or Support Vector Machines (SVMs) with cross-validation.</div> </p>  </div>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw28_56ab1dc5c8ffa">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw27_56ab1dc5c8ffa"  itemprop="articleBody">  <p>Page 1</p> <p>Bayesian Gaussian Process Classification<br />with the EM-EP Algorithm<br />Hyun-Chul Kim and Zoubin Ghahramani, Member, IEEE<br />Abstract—Gaussian process classifiers (GPCs) are Bayesian probabilistic kernel classifiers. In GPCs, the probability of belonging to a<br />certain class at an input location is monotonically related to the value of some latent function at that location. Starting from a Gaussian<br />process prior over this latent function, data are used to infer both the posterior over the latent function and the values of<br />hyperparameters to determine various aspects of the function. Recently, the expectation propagation (EP) approach has been<br />proposed to infer the posterior over the latent function. Based on this work, we present an approximate EM algorithm, the EM-EP<br />algorithm, to learn both the latent function and the hyperparameters. This algorithm is found to converge in practice and provides an<br />efficient Bayesian framework for learning hyperparameters of the kernel. A multiclass extension of the EM-EP algorithm for GPCs is<br />also derived. In the experimental results, the EM-EP algorithms are as good or better than other methods for GPCs or Support Vector<br />Machines (SVMs) with cross-validation.<br />Index Terms—Gaussian process classification, Bayesian methods, kernel methods, expectation propagation, EM-EP algorithm.<br />Ç<br />1<br />K<br />kernel classifiers are the support vector machine (SVM),<br />Bayes point machine (BPM), and Gaussian process classifier<br />(GPC). The SVM was proposed as a classifier maximizing<br />the margin, which is the smallest distance between data<br />points and the class boundary [1]. SVMs have been a<br />popular tool and have resulted in many successful applica-<br />tions. The BPM is a kernel classifier whose goal is to<br />approximate Bayes-optimal classification by finding the<br />center of the mass of version space, which is the set of<br />hyperplanes in feature space that separate the data [2]. It<br />was also shown that SVMs can be viewed as a form of Bayes<br />point machine which tries to find the center of the largest<br />ball to fit in version space. In contrast with the above two<br />classifiers, GPCs are Bayesian kernel classifiers derived<br />from Gaussian process priors over functions which were<br />developed originally for regression [3], [4], [5].<br />Gaussian processes for regression [5], [6], [7], [8] assume<br />that the target function has a Gaussian process prior. This<br />means that the density of any collection of target function<br />values is modeled as a multivariate Gaussian density.<br />Usually, the mean of this Gaussian is assumed to be zero<br />and the covariance between the targets at two different<br />points is a decreasing function of their distance in input<br />space. This decreasing function is controlled by a small set<br />INTRODUCTION<br />ERNEL classifiers have recently received much attention<br />from the machine learning community. Some popular<br />of hyperparameters that capture interpretable properties of<br />the function, such as the length scale of autocorrelation, the<br />overall scale of the function, and the amount of noise. The<br />posterior distributions of these hyperparameters given the<br />data can be inferred in a Bayesian way via Markov Chain<br />Monte Carlo (MCMC) methods [5], [7] or they can be<br />selected by maximizing the marginal likelihood (also<br />known as the evidence) [8]. A Bayesian treatment of<br />multilayer perceptrons for regression has been shown to<br />converge to a Gaussian process as the number of hidden<br />nodes approaches to infinity, if the prior on input-to-hidden<br />weights and hidden unit biases are independent and<br />identically distributed [9]. Empirically, Gaussian processes<br />have been shown to be an excellent method for nonlinear<br />regression [10].<br />In GPCs, the target values are discrete class labels while<br />the target values in GP regression are continuous real<br />values. It is not appropriate to assume that the target<br />function with discrete outputs has a Gaussian process prior.<br />We assume that there is some latent function whose value at<br />a certain input location is monotonically related to the<br />probability of belonging to a certain class at that location<br />and that the latent function rather than the target function<br />has a Gaussian process prior. We can use a Gaussian<br />process as a prior of the latent function, and for multiclass<br />classification, one can use multiple GPs or a multivariate<br />GP. Since only the class labels are observed in GPCs, we<br />need to integrate not only over hyperparameters but also<br />over latent values of these functions at the data points.<br />Williams and Barber [3] used a Laplace approximation to<br />integrate over the latent values and Hybrid Monte Carlo<br />(HMC) to integrate over the hyperparameters. Neal [5] used<br />Gibbs sampling to integrate over latent values and used<br />HMC to integrate over hyperparameters. Gibbs and Mackay<br />[4] used a variational approximation method to integrate<br />over latent values and determined hyperparameters by<br />maximizing the marginal likelihood. Opper and Winther<br />1948 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,VOL. 28, NO. 12,DECEMBER 2006<br />. H.-C. Kim is with the Department of Industrial and Management<br />Engineering, Pohang University of Science and Technology, San 31<br />Hyoja-Dong, Nam-gu, Pohang, 790-784, Republic of Korea.<br />E-mail: grass@postech.ac.kr.<br />. Z. Ghahramani is with the Department of Engineering, University of<br />Cambridge, Trumpington Street, Cambridge CB2 1PZ, UK.<br />E-mail: zoubin@eng.cam.ac.uk.<br />Manuscript received 2 Nov. 2005; revised 3 Apr. 2006; accepted 12 Apr. 2006;<br />published online 12 Oct. 2006.<br />Recommended for acceptance by M. Figueiredo.<br />For information on obtaining reprints of this article, please send e-mail to:<br />tpami@computer.org, and reference IEEECS Log Number TPAMI-0595-1105.<br />0162-8828/06/$20.00 ? 2006 IEEE Published by the IEEE Computer Society</p>  <p>Page 2</p> <p>[11] used the TAP approach originally proposed in<br />statistical physics of disordered systems to integrate over<br />latent values.<br />It turns out that the TAP approach for GPCs is equivalent<br />to the Expectation Propagation (EP) algorithm for approx-<br />imate inference in Bayes point kernel machines [12]. EP has<br />been shown to give results which are superior to Laplace’s<br />method and are very similar to MCMC methods both in<br />terms of predictive distributions and marginal likelihood<br />estimates [13]. In these previous papers, the focus has been<br />on approximate inference rather than determining hyper-<br />parameters. Two potential methods for determining the<br />hyperparameters have been proposed in [14]. The first<br />method, which they called mean field method I, is to<br />maximize the variational lower bound of the evidence<br />under the assumption that the densities of latent values are<br />independent and Gaussian. The second method, which they<br />called mean field method II, is to maximize the evidence<br />approximated by Fourier transformation of the likelihood<br />and a saddle point approximation. An EM algorithm for<br />learning the kernel length scales using an L1 prior has also<br />been proposed [15].<br />In this paper, we propose and investigate a conceptually<br />simple EM-like algorithm to learn the hyperparameters<br />which we call EM-EP.1In the E-step, we use EP to estimate<br />the joint density of latent values under the assumption that<br />the joint density is multivariate Gaussian. This multivariate<br />approximation is better than factorized approximations<br />such as the mean field method I. In the M-step, we<br />maximize with respect to the hyperparameters the varia-<br />tional lower bound on the marginal likelihood given by<br />using the density of latent values obtained from the E-step.<br />These two steps are repeated until convergence. The idea of<br />using the variational lower bound for model selection in<br />GPC was suggested in [17]. Here, we use a slightly different<br />formulation for GPC and provide experimental results.<br />Another emphasis of this paper is examining the role of the<br />different hyperparameters and comparing these algorithms<br />with several variants of SVMs. We also propose an<br />extension of the EM-EP algorithm for multiclass classifica-<br />tion. Finally, although improving computational complexity<br />of GPC learning through sparsification methods is an<br />important research topic ([18], [19], [20]), we will not<br />address this problem in this paper.<br />The paper is organized as follows: Section 2 introduces<br />Gaussian process classification. In Section 3, we introduce<br />the EP method for GPCs, derive the EM-EP algorithm,<br />and show the experimental results. In Section 4, we<br />derive the EP method and the EM-EP algorithm for<br />multiclass GPCs and show experimental results. In<br />Section 5, we discuss our approach and related work.<br />Software implementing the EM-EP algorithm is available<br />at http://home.postech.ac.kr/~grass/software/.<br />2GAUSSIAN PROCESS CLASSIFICATION<br />Let us assume that we have a data set D of data points xi<br />with binary class labels yi2 f?1;þ1g:<br />D ¼ fðxi;yiÞji ¼ 1;2;...;ng;<br />X ¼ fxiji ¼ 1;2;...;ng;<br />y ¼ fyiji ¼ 1;2;...;ng:<br />Given this data set, the classification problem is to output<br />the correct class label for a new data point. To represent our<br />uncertainty over class labels, one may want a method that<br />outputs probabilities over the different labels for each new<br />data point.<br />We assume that the probability over class labels as a<br />functionofxdependsonthevalueofsomelatentreal-valued<br />function fðxÞ. That is, for binary classification, given the<br />value of fðxÞ the probability of class label is independent of<br />all other quantities: pðy ¼ þ1jx;fðxÞ;DÞ ¼ pðy ¼ þ1jfðxÞÞ.<br />The probability of observing y ¼ þ1 is assumed to be a<br />monotonically increasing function of fðxÞ. This can take<br />several forms, for example for fi¼ fðxiÞ:<br />1<br />1þexpð?yifiÞ<br />?ðyifiÞ<br />HðyifiÞ<br />? þ ð1 ? 2?ÞHðyifiÞ<br />where HðzÞ ¼ 1 iff z &gt; 0, ?ðzÞ is the cumulative normal<br />function ?ðzÞ ¼Rz<br />pðyijfiÞ ¼<br />sigmoid<br />cumulative normal<br />threshold<br />noisy threshold;<br />8<br />&gt;<br />&gt;<br />&gt;<br />&gt;<br />:<br />&lt;<br />ð1Þ<br />?1<br />1ffiffiffiffi<br />2?<br />p<br />expð?x2<br />2Þdx, and 0 ? ? &lt; 0:5.<br />We put a Gaussian process (see Fig. 1) prior on this<br />function, meaning that any number of points evaluated<br />from the function have a multivariate Gaussian density (see<br />[7] for a review of GPs). Assume that this GP prior is<br />parameterized by ? which we will call the hyperpara-<br />meters. We can write the probability of interest given ? as:<br />Z<br />The second part of (2) is obtained by further integration<br />over f ¼ ½f1;f2???fn?, the values of the latent function at the<br />data points.<br />pð~ yj~ x;D;?Þ ¼<br />pð~ yj~f;?Þpð~fjD; ~ x;?Þ d~f:<br />ð2Þ<br />KIM AND GHAHRAMANI: BAYESIAN GAUSSIAN PROCESS CLASSIFICATION WITH THE EM-EP ALGORITHM1949<br />1. An earlier version of this paper focusing on the EM-EP algorithm for<br />binary classification was presented at a workshop [16].<br />Fig. 1. In Gaussian processes for classification the class label is related<br />to a latent function. (a) Examples of one-dimensional and (b) two-<br />dimensional data showing a latent function sampled from a Gaussian<br />process prior and the corresponding class label under the threshold<br />model.</p>  <p>Page 3</p> <p>pð~fjD; ~ x;?Þ ¼<br />Z<br />Z<br />pðf;~fjD; ~ x;?Þ df<br />¼<br />pð~fj~ x;f;X;?ÞpðfjD;?Þdf;<br />ð3Þ<br />where<br />pðfjD;?Þ / pðyjf;X;?ÞpðfjX;?Þ ¼<br />Y<br />n<br />i¼1<br />pðyijfi;?Þ<br /> !<br />pðfjX;?Þ:<br />The first term is the probability of each observed class label<br />given the latent function value, which can be of one of the<br />forms in (1), while the second term is the GP prior over<br />functions evaluated at the data. Writing the dependence of f<br />on x implicitly, the GP prior over functions can be written<br />ð4Þ<br />pðfjX;?Þ ¼<br />1<br />ð2?ÞN=2jC?j1=2exp ?1<br />2ðf ? ? ?Þ&gt;C?1<br />?ðf ? ? ?Þ<br />??<br />;<br />ð5Þ<br />where the mean ? ? is usually assumed to be zero ? ? ¼~0 and<br />each term of a covariance matrix Cijis a function of xiand<br />xj, i.e., cðxi;xjÞ. This covariance function is the kernel which<br />defines how data points generalize to nearby data points.<br />The covariance function is parameterized by the hyper-<br />parameters ?, which we can learn from the data. We will<br />describe the particular covariance functions used in this<br />paper later on. The Gaussian process classifier can be<br />represented using the graphical model shown in Fig. 2.<br />In general, the class probability at a test point would be<br />obtained by integrating over the hyperparameters weighted<br />by their posterior probability<br />Z<br />This integral is costly and there are usually many fewer<br />hyperparameters than data points. Therefore, in this paper,<br />rather than integrating over the hyperparameters, we fit<br />them by maximizing the marginal likelihood as ^? ¼<br />argmax?pðDj?Þ and predict using these best fit hyperpara-<br />meters: pð~ yj~ x;D;^?Þ. The marginal likelihood and pðfjD;?Þ<br />in (4) are both intractable due to the nonlinearities in (1). We<br />use the Expectation-Propagation (EP) algorithm to approx-<br />imate both.<br />pð~ yj~ x;DÞ ¼<br />pð~ yj~ x;D;?Þpð?jDÞ d?:<br />ð6Þ<br />3<br />3.1<br />The Expectation Propagation (EP) algorithm is an approx-<br />imate Bayesian inference method [12]. We briefly review EP<br />in its general form before describing its application to GPCs.<br />Consider a Bayesian inference problem where the<br />posterior over some latent value (or parameter) f is<br />proportional to the prior times likelihood terms for an<br />i.i.d. data set<br />THE EM-EP ALGORITHM<br />Expectation Propagation<br />pðfjy1;...;ynÞ / pðfÞ<br />Y<br />n<br />i¼1<br />pðyijfÞ ¼<br />Y<br />n<br />i¼0<br />tiðfÞ;<br />ð7Þ<br />where t0ðfÞ ¼ pðfÞ and tiðfÞ ¼ pðyijfÞ for i ¼ 1;...;n. Notice<br />that, dropping some variables being conditioned on, (4) is of<br />this form. We approximate this distribution with a product<br />of simple terms<br />qðfÞ /~t0ðfÞ<br />Y<br />n<br />i¼1<br />~tiðfÞ;<br />ð8Þ<br />where each term (and therefore q) is assumed to be in the<br />exponential family. EP iterates the following procedure<br />over i until convergence:<br />Remove the ith term from qðfÞ: qniðfÞ ¼Qn<br />3.<br />Find a new~tiðfÞ ¼ tðfÞ such that it minimizes the<br />Kullback-Leibler divergence2from qniðfÞtiðfÞ to<br />qniðfÞtðfÞ:<br />?<br />¼ argmin<br />1.<br />2.<br />j6¼i~tjðfÞ.<br />Multiply by the true ith factor: qniðfÞtiðfÞ ¼ qðfÞtiðfÞ<br />~tiðfÞ.<br />~tnew<br />i<br />ðfÞ ¼ argmin<br />tðfÞKL qniðfÞtiðfÞ<br />?<br />????<br />????qniðfÞtðfÞ<br />?<br />tðfÞKL<br />qðfÞ<br />~toldiðfÞpðyijfÞ<br />????<br />????<br />qðfÞ<br />~toldiðfÞtðfÞ<br />?<br />:<br />ð9Þ<br />Since q is in the exponential family, this minimiza-<br />tion is solved by matching moments of the approxi-<br />mated distribution.<br />The algorithm is not guaranteed to converge although it<br />did in practice in all our examples. Assumed Density<br />Filtering (ADF)3is a special online form of EP where only<br />one pass through the data is performed (i ¼ 1;...n). EP can<br />be seen as an extension of ADF to batch situations.<br />EP has been applied to several Bayesian learning<br />problems and its excellent performance has been demon-<br />strated on other problems. Minka showed that EP is better<br />than Laplace’s method and the variational Bayes method in<br />terms of accuracy and computational cost for simple<br />Bayesian learning problems such as the clutter problem<br />and mixture weights learning problem [12]. It has been<br />shown that EP provides better accuracy than variational<br />methods at a comparable cost for the generative aspect<br />model [21]. EP was also applied to the signal detection<br />problem in flat-fading channels which can be formulated as<br />1950 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,VOL. 28,NO. 12,DECEMBER 2006<br />2. The Kullback-Leibler divergence between two distributions pðfÞ and<br />qðfÞ is defined as: KLðpðfÞjjqðfÞÞ ¼RpðfÞlogpðfÞ<br />intelligence as different names such as online Bayesian learning, moment<br />matching, and weak marginalization.<br />qðfÞdf.<br />3. ADF appears in several fields such as control, statistics, and artificial<br />Fig. 2. Graphical model for GPCs with n training data points and one test<br />data point. xi and yi are observed, ~ x is given, ~ y is what should be<br />predicted, fi and~f are latent and jointly Gaussian, hence have the<br />undirected edges.</p>  <p>Page 4</p> <p>an estimation problem in a hybrid dynamic system with<br />both continuous and discrete variables [22]. In this problem,<br />EP provides a much lower computational cost than Monte<br />Carlo filter and smoothers. Tree-structured EP showed<br />better accuracy and convergence than normal belief<br />propagation, and a lower cost than variational trees or<br />double-loop algorithms [23]. A Bayes point machine with<br />EP which allows only hard decision boundaries showed<br />better performance than a hard-margin support vector<br />machine in most cases [12]. EP is not guaranteed to<br />converge but in practice it converges in many cases. Its<br />generalized version which is convergent but slower has<br />been proposed [24].<br />3.2<br />We describe EP for GPC referring to [11], [12], [20]. The<br />form of the likelihood we use in the GPC is<br />EP for Gaussian Process Classification<br />pðyijfiÞ ¼ ? þ ð1 ? 2?ÞHðyifiÞ;<br />ð10Þ<br />where HðxÞ ¼ 1 if x &gt; 0, and otherwise 0. The hyperpara-<br />meter, ? in (10) models labeling error outliers. The<br />EP algorithm approximates the posterior<br />pðfjD;?Þ ¼pðfjX;?Þpðyjf;?Þ<br />pðyjX;?Þ<br />ð11Þ<br />as a Gaussian having the form<br />qðfÞ ¼ Nðh;AÞ;<br />ð12Þ<br />where the GP prior pðfjX;?Þ ? Nð0;CÞ has the covariance<br />matrix C with elements Cij defined by the covariance<br />function<br />(<br />þ ?1þ ?2?ði;jÞ;<br />where xm<br />iis the mth element of xi,<br />?<br />and?ðxm<br />more reasonable distance measure than ðxm<br />discrete data.<br />The hyperparameter ?0specifies the overall vertical scale<br />of variation of the latent values, ?1 the overall bias of the<br />latent values from zero mean, ?2the latent noise variance,<br />and lm the (inverse) lengthscale for feature dimension m.<br />The cumulative normal density likelihood term in (1) is<br />equivalent to using the threshold function in (10) with ? ¼ 0<br />and nonzero latent noise ?2.<br />EP tries to approximate the posterior (11) which can be<br />written as:<br />pðfjD;?Þ ¼pðfjX;?ÞQn<br />pðyijfÞ ¼ tiðfÞ in (10) is approximated by<br />~tiðfÞ ¼ siexp ?1<br />Cij¼ cðxi;xjÞ ¼ ?0exp ?1<br />2<br />X<br />d<br />m¼1<br />lmdmðxm<br />i;xm<br />jÞ<br />)<br />ð13Þ<br />dmðxm<br />i;xm<br />jÞ ¼<br />ðxm<br />1 ? ?ðxm<br />i? xm<br />jÞ2<br />if xmis continous;<br />if xmis discrete;<br />i;xm<br />jÞ<br />ð14Þ<br />i;xm<br />jÞisaKroneckerdeltafunction.1 ? ?ðxm<br />i;xm<br />jÞ2for<br />jÞisa<br />i? xm<br />i¼1pðyijfÞ<br />pðyjX;?Þ<br />:<br />ð15Þ<br />2viðfi? miÞ2<br />??<br />:<br />ð16Þ<br />From this initial setting, we can derive EP for GPC by<br />applying the general idea described above. The details of<br />the derivation are in [25]. The resulting EP procedure is<br />virtually identical to the one derived for BPMs in [12]. We<br />define the following notation:4<br />? ? ¼ diagðv1;...;vnÞ;hi¼ E½fi?;hni<br />?i¼ V ar½fi?;?ni<br />where hni<br />i<br />and fni<br />i<br />are values obtained from a whole set<br />except for xi. The EP algorithm is as follows: After the<br />initialization<br />i¼ E½fni<br />i?;<br />i¼ Var½fni<br />i?;<br />ð17Þ<br />vi¼ 1;mi¼ 0;si¼ 1;hi¼ 0;?ni<br />the following process is performed until all ðmi;vi;siÞ<br />converge: Loop i ¼ 1;2;...;n:<br />1.<br />Remove the approximate density ~ti (for ith data<br />point)fromtheposteriorqðfÞtogetan“old”posterior<br />qniðfÞ, and get a marginal qniðfiÞ ¼ Nðhni<br />hni<br />i¼ Cii;<br />ð18Þ<br />i;?ni<br />iÞ:<br />i¼ hiþ ?ni<br />iv?1<br />iðhi? miÞ;?ni<br />i¼ ð1=Aii? 1=viÞ?1:<br />ð19Þ<br />2.<br />Find qnewðfiÞ ? Nðhi;?iÞ which minimizes KL diver-<br />gence from qniðfiÞtiðfiÞ to qnewðfiÞ:<br />z ¼yihni<br />?ni<br />i<br />yiffiffiffiffiffiffi<br />where ?ðzÞ is a cumulative normal density function<br />defined in (1).<br />Get a new~tiðfiÞ ¼ siexpð?1<br />1<br />?ihi? 1<br />ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi<br />iffiffiffiffiffiffi<br />q<br />q<br />;Zi¼ ? þ ð1 ? 2?Þ?ðzÞ;<br />?i¼<br />?ni<br />i<br />ð1 ? 2?ÞNðz;0;1Þ<br />Zi<br />;hi¼ hni<br />iþ ?ni<br />i?i;<br />ð20Þ<br />3.<br />2viðfi? miÞ2Þ:<br />?<br />?ni<br />vi¼ ?ni<br />i<br />?<br />q<br />;mi¼ hiþ vi?i;<br /> <br />si¼ Zi<br />1 þ v?1<br />i?ni<br />i<br />exp<br />i?i<br />2hi<br />!<br />:<br />ð21Þ<br />4.<br />Obtain a new qðfÞ ? Nðh;AÞ using a new~tiðfiÞ:<br />A ¼ ðC?1þ ? ??1Þ?1;h ¼ A? ??1m:<br />ð22Þ<br />The approximate evidence Zð?Þð? pðyjX;?ÞÞ is as<br />follows:<br />Zð?Þ ¼t0ðfÞQn<br />where r ¼P<br />done in Oðn2Þ by the Woodbury formula also known as the<br />matrix inversion lemma [26]. Our approximated posterior<br />i¼1~tiðfiÞ<br />qðfÞ<br />m2<br />i<br />¼<br />j? ?j1=2<br />jC þ ? ?j1=2expð?r=2Þ<br />mimj<br />vivj. One iteration of the above<br />Y<br />n<br />i¼1<br />si; ð23Þ<br />i<br />vi?P<br />ijAij<br />EP algorithm can be executed in Oðn3Þ because (22) can be<br />KIM AND GHAHRAMANI: BAYESIAN GAUSSIAN PROCESS CLASSIFICATION WITH THE EM-EP ALGORITHM1951<br />4. diagðv1;...;vnÞ means a diagonal matrix whose diagonal elements are<br />v1;...;vn. Similarly for diagðvÞ.</p>  <p>Page 5</p> <p>over latent values is qðfÞ ¼ Nðh;AÞ. According to [11], it<br />can also be written as qðfÞ ¼ NðC? ?;AÞ. The approximate<br />evidence in (23) can be used to measure the quality of fit of<br />kernels or their hyperparameters to the data for model<br />selection. However, it is difficult to obtain an updating rule<br />from (23). In the following section, we derive the algorithm<br />to find the hyperparameters automatically, based not on<br />(23) but a variational lower bound of the evidence.<br />We will demonstrate how to find the hyperparameters<br />soon, but for the moment, we’d like to concentrate on how<br />we predict the class probabilities at a new point, ~ x. To begin<br />with, we obtain the density for a latent value~f correspond-<br />ing to ~ x from (3). We obtain, using the approximation in (12),<br />Z<br />? NðkTðCÞ?1h;? ? kTð? ? þ CÞ?1kÞ;<br />where k ¼ ½cð~ x;x1Þ;cð~ x;x2Þ;...;cð~ x;xnÞ? and ? ¼ cð~ x; ~ xÞ.<br />Probability of ~ x being class ~ y is obtained from (2) as follows:<br />Z<br />0<br />@<br />if we assume the test data point does not have labeling<br />errors and<br />0<br />@<br />if we assume the test data point might also have labeling<br />errors.<br />Strict classification of a new data point ~ x can be done<br />according to<br />pð~fjD; ~ x;?Þ ¼<br />pð~fj~ x;f;?ÞpðfjD;?Þ df<br />ð24Þ<br />pð~ yj~ x;D;?Þ ¼<br />pð~ yj~f;?Þpð~fjD; ~ x;?Þ d~f<br />¼ ?<br />~ ykTC?1h<br />ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi<br />? ? kTð? ? þ CÞ?1kÞ<br />q<br />B<br />1<br />A;<br />C<br />ð25Þ<br />pð~ yj~ x;D;?Þ ¼ ? þ ð1 ? 2?Þ?<br />~ ykTC?1h<br />ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi<br />? ? kTð? ? þ CÞ?1kÞ<br />q<br />B<br />1<br />A; ð26Þ<br />C<br />argmax<br />~ y<br />pð~ yj~ x;D;?Þ ¼ sgnðE½~f?Þ ¼ sgnðkTC?1hÞ:<br />Equivalently, since h ¼ C? ?, classification can be conducted<br />according to sgnðPn<br />More information about EP for GPC can be found on the<br />Computer Society Digital Library at http://computer.org/<br />tpami/archives.htm.<br />i¼1?icðxi; ~ xÞÞ which is the expression<br />found in [11].<br />3.3<br />In the last section, we have presented the EP algorithm for<br />Gaussian process classification. It supplies the posterior<br />over latent functions, the predictive class probability for<br />new data points, and the approximate evidence. One<br />important component missing from the paper so far is an<br />algorithm to estimate the hyperparameters of the covar-<br />iance function.<br />We address the problem of estimating hyperparameters<br />of the covariance function in the framework of Gaussian<br />process regression with incomplete target values. This idea<br />makes it possible to apply an EM-like algorithm. In the<br />E-step, we infer the approximate (Gaussian) density for<br />latent function values qðfÞ using EP. In the M-step, using<br />The EM-EP Algorithm<br />qðfÞ obtained in the E-step, we maximize a lower bound on<br />pðyjX;?Þ as a function of ?. The E-step and M-step are<br />alternated until convergence.<br />.<br />E-step. EP iterations are performed given the<br />hyperparameters. pðfjDÞ is approximated as a<br />Gaussian density qðfÞ:<br />qðfÞ ¼ Nðh;AÞ ¼ NðC? ?;AÞ:<br />ð27Þ<br />.<br />M-step. Given qðfÞ obtained from the E-step, find the<br />covariance function hyperparameters and the label-<br />ing error hyperparameter which maximize a lower<br />bound of the log evidence logpðyjX;?Þ. We define<br />y ¼ ½y1;y2;...;yn?&gt;, let ? represent all of the hyper-<br />parameters of the model: ?;?0;?1;?2;l1;l2;...;lp, and<br />let ?covrepresent all those in ? except for ?. Then, we<br />obtain the evidence<br />Z<br />Since the above integral is intractable, we use an<br />approximation technique. We take a lower bound F<br />for the log evidence by Jensen’s inequality, as<br />follows:<br />Z<br />Z<br />pðyjX;?Þ ¼<br />pðyjf;?ÞpðfjX;?covÞ df:<br />ð28Þ<br />logpðyjX;?Þ ¼ logpðyjf;?ÞpðfjX;?covÞdf<br />ð29Þ<br />?<br />qðfÞlogpðyjf;?ÞpðfjX;?covÞ<br />qðfÞ<br />df ¼ F:<br />ð30Þ<br />Using the E-step result (27) and the fact that<br />pðfjX;?covÞ ¼ Nð0;C?Þ and~C ¼ CdiagðyÞ, we ob-<br />tain the following gradient update rule with respect<br />to a covariance hyperparameter ?ð2 ?covÞ:5<br />@F<br />@?¼1<br />þ1<br />2? ?&gt;@C<br />2tr C?1@C<br />@?? ? ?1<br />?<br />2tr C?1@C<br />@?<br />??<br />@?C?1A<br />?<br />:<br />ð31Þ<br />The detailed derivation is in Appendix A.<br />3.4A Property of the EM-EP Algorithm<br />It turns out that the gradient of the lower bound of the<br />evidence pðyjX;?Þ in the M-step of the EM-EP algorithm is<br />in the same direction as the gradient of the approximate<br />evidence obtained by EP when we deal with only the<br />hyperparameters ? ?covin the prior density. The proof is as<br />follows:<br />Theorem 1. In the M-step of the EM-EP algorithm, the gradient<br />of the lower bound F of pðyjX;?Þ under qðfÞ with respect to<br />the hyperparameters ? ?cov6of the covariance function is in the<br />same direction as the gradient of approximate evidence Zð? ?covÞ<br />(? pðyjX;? ?covÞ) in (23).<br />1952 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,VOL. 28,NO. 12,DECEMBER 2006<br />5.@C<br />6. ? ?cov¼ ½?0;?1;?2;l1;l2;...;ld?.<br />@?is an elementwise differentiation of C.</p>  <p>Page 6</p> <p>Proof. The gradient of F with respect to ? ?covis expressed:<br />Z<br />df ¼ r? ?cov<br />r? ?covF ¼ r? ?cov<br />qðfÞlogpðyjf;?ÞpðfjX;? ?covÞ<br />qðfÞ<br />Z<br />qðfÞlogpðfjX;? ?covÞ df:<br />ð32Þ<br />The gradient of Zð? ?covÞ with respect to ? ?covis expressed<br />using (23):<br />Z<br />df ¼ r? ?cov<br />r? ?covlogZð? ?covÞ ¼ r? ?cov<br />qðfÞlogZð? ?covÞ<br />Z<br />qðfÞlog<br />Qn<br />i¼1~tiðfÞt0ðfj? ?covÞ<br />qðfÞ<br />df<br />ð33Þ<br />¼ r? ?cov<br />Z<br />qðfÞlogt0ðfj? ?covÞ df:<br />ð34Þ<br />From (32),(34)andthe factthat t0ðfj? ?covÞ ¼ pðfjX;? ?covÞ,<br />we obtain the following equation:<br />r? ?covF ¼ r? ?covlogZð? ?covÞ ¼<br />1<br />Zð? ?covÞr? ?covZð? ?covÞ:<br />u t<br />According to Theorem 1, when we use the EM-EP<br />algorithm with only covariance hyperparmeters, the M-step<br />uses the same direction as the gradient of the approximate<br />evidence Zð? ?covÞ. On the other hand, when we use the EM-<br />EP algorithm with some hyperparameters related to the<br />likelihood, the M-step does not use the same direction as<br />the gradient of the approximate evidence Zð? ?covÞ.<br />Even though the theoretical justification for the EM-EP<br />algorithm is harder, in practice generally better inference<br />(E-step) should lead to better (hyperparameter) learning.<br />Some examples have shown that the approximate evidence<br />from EP agrees very well with the one from an MCMC<br />method [13]. The EM-EP algorithm is more likely to learn<br />the hyperparameter which is a maximum of the approx-<br />imate evidence by MCMC method, when its M-step uses<br />the gradient of the approximate evidence (Theorem 1).<br />3.5Experimental Results<br />To demonstrate the EM-EP procedure, we start with<br />hyperparameter learning in synthetic data sets. We then<br />use binary-class real world data sets to compare the<br />proposed algorithm with SVMs and other classification<br />methods. In the M-step, we used the conjugate gradient<br />method with line searches.7<br />hyperparameters are optimized in log transformed spaces<br />so as to avoid constrained optimization.<br />All covariance function<br />3.5.1 Synthetic Data Sets<br />First, we show with a simple intuitive example that the EM-<br />EP algorithm learns the hyperparameters better than<br />Laplace’s method and the variational method. We have<br />sampled a latent function in a two-dimensional input space<br />from a Gaussian process prior with inverse lengthscales 0.5<br />and 2.0 in the two dimensions. We then sampled 200 data<br />points randomly from a uniform(-10,10) distribution and<br />usedthesignofthelatentfunctiontodefinetheclasslabelsof<br />the points. Using this data, we learned the hyperparameters<br />of a GPC with Laplace’s method [3], the variational method<br />[4],andtheEM-EPalgorithm.Weperformedthisexperiment<br />10 times for different latent functions. Table 1 shows the<br />means and standard deviation of the lengthscale hyperpara-<br />meters learned by the three methods. All methods seem to<br />underestimate the lengthscale parameters in (13), which<br />corresponds to assuming functions with longer lengthscales<br />(i.e., more slowly varying). This may indicate underfitting<br />due to limited data. The EM-EP algorithm shows the best<br />results, which are fairly close to the true value.<br />To show the usefulness of lengthscale hyperparameters,<br />we generated a simple data set with six features distributed<br />as follows: x1;x2;x3? Nðy;1Þ and x4;x5;x6? Nð0;1Þ,<br />where y 2 f?1g is the class label. That is, x1;x2;x3 are<br />relevant features while x4;x5;x6 are irrelevant to the<br />classification problem. We generated 300 data samples for<br />a training set and 10,000 data samples for a test set. We tried<br />the EM-EP algorithm with a single lengthscale hyperpara-<br />meter for all dimensions, or with multiple lengthscale<br />hyperparameters. As would be hoped, we saw that the<br />lengthscale hyperparameters for the irrelevant features<br />(x4;x5;x6) decreased to near zero. The approximate log<br />evidence and classification error are shown in Table 2. The<br />result show that GPC with multiple lengthscale hyperpara-<br />meters was significantly better than one with a single<br />lengthscale, as measured both by classification error rates as<br />well as approximate log evidence logZð? ?covÞ.<br />3.5.2 Real-World Data Sets<br />We applied the proposed algorithm to several real-world<br />data sets. The detailed information for the real-world data<br />sets we used is in Table 3. Thyroid, Heart disease, and<br />Ionosphere data sets were obtained from the UCI Machine<br />KIM AND GHAHRAMANI: BAYESIAN GAUSSIAN PROCESS CLASSIFICATION WITH THE EM-EP ALGORITHM1953<br />TABLE 1<br />Comparison of GPCs with Laplace’s Method, the Variational Method, and the EM-EP Algorithm<br />7. The optimization procedure is described in Appendix B in [10] and the<br />code is available from http://www.kyb.tuebingen.mpg.de/bs/people/<br />carl/code/minimize/.</p>  <p>Page 7</p> <p>Learning Repository,8Crabs, and Pima data sets were<br />obtained from the PRNN site,9and Boston Housing data set<br />were obtained from the R software site.10The Thyroid data<br />set originally had three classes: “normal,” “hyper,” and<br />“hypo,” but we created a binary classification problem by<br />grouping hyper and hypo into “not normal.” The Pima data<br />set has a training set of 200 and two kinds of test sets, but<br />we used only the training set as a whole set for experiments.<br />The Boston Housing data set has 506 data points and<br />20 variables. It has a pair of duplicated variables, one of<br />which is wrong and the other is a corrected one for one<br />attribute, and has another pair of duplicated variables<br />which are town name and town number. We made a<br />binary-class data set by assigning a class label according to<br />whether housing price is greater than USD25000 or not. So,<br />we actually have 17 variables for our classification problem.<br />Table 4 and Table 5 show the classification error rates of<br />various methods. Each data set was divided into 10 folds.<br />Each fold was subsequently used as a test set, while the<br />other nine folds were used as a training set. The numbers in<br />Table 4 and Table 5 are the means of those 10 error rates and<br />standard errors on the means. We tried three versions of the<br />EM-EP Gaussian Process Classifier: GPC-EP(s,soft) used a<br />single lengthscale hyperparameter for all feature dimen-<br />sions, while GPC-EP(m,soft) used a different lengthscale<br />hyperparameter for each feature dimension.11Finally, GPC-<br />EP(s,hard) was a GPC with a single lengthscale hyperpara-<br />meter where the decision boundary was “hard” in the sense<br />that the latent function noise parameter ?2 was fixed to<br />1954IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 28,NO. 12, DECEMBER 2006<br />TABLE 3<br />Detailed Information on the Real-World Data Sets<br />(The items “dis.,” “con.,” “classes,” and “data points” mean the number<br />of discrete variables, continuous variables, classes, and data points in<br />each data set.)<br />TABLE 4<br />Classification Error Rates of Various Methods<br />for Real-World Data Sets (I)<br />TABLE 5<br />Classification Error Rates of Various Methods<br />for Real-World Data Sets (II)<br />TABLE 2<br />Comparison of GPCs with a Single Lengthscale<br />Hyperparameter and with Multiple Ones<br />8. Available from http://www.ics.uci.edu/~mlearn/MPRepository.<br />html.<br />9. Available from http://www.stats.ox.ac.uk/pub/PRNN.<br />10. Available from http://www.maths.lth.se/help/R/.R/library/<br />spdep/html/boston.html.<br />11. The initial values of hyperparameters for GPC-EP(s,soft) for the<br />first fold were as follows: ?0<br />and those for subsequent folds are the results for the former fold. For<br />GPC-EP(m,soft), the initial values of the hyperparameters for every<br />fold were the results learned for the same fold in GPC-EP(s,soft).<br />0¼ 1, ?0<br />1¼ 0:0001, ?0<br />2¼ 0:001, l0<br />m¼ 0:05;8m,</p>  <p>Page 8</p> <p>zero or a very small number. In all GPC models, ? (cf (1))<br />was set to zero.12<br />We also tried other methods for GPC: GPC-VL(m,soft)<br />used a variational lower bound, and GPC-VU(m,soft) used<br />a variational upper bound to infer latent values [4]. GPC-<br />L(m,soft) used Laplace’s method to infer latent values [3].<br />All use an optimization scheme for hyperparameters. All<br />use a multiple lengthscale hyperparameter, but, in the cases<br />of GPC-VL(m,soft) and GPC-VU(m,soft), they used a<br />lengthscale hyperparameter of type<br />Even though they do not have latent value noise hyper-<br />parameter ?2, all have a soft decision boundary, because<br />they use a sigmoid function as a likelihood and they have a<br />signal variance hyperparameter ?0.<br />We compared our results to several variants of SVMs.13<br />We wanted to distinguish the effect of the kernel choice<br />from the effect of the different loss functions and noise<br />model in SVMs vs GPCs. Thus, in SVM-EP(s,soft), the<br />kernel, (i.e., covariance function) was set to be the same,<br />with the same hyperparameters as the corresponding GPC-<br />EP(s,soft) trained using EM-EP except for the latent noise<br />variance ?2. Instead, the penalty parameter C allowing<br />training errors (i.e., penalizing the SVM slack variables) was<br />selected by five-fold cross-validation.14<br />We also applied both hard and soft-margin SVMs with a<br />Gaussian kernel with a single lengthscale hyperparameter<br />(without ?0, ?1, and ?2) selected by 5-fold cross-validation.15<br />For hard-margin SVM, SVM-CV(hard), we only needed to<br />perform a two-level grid search for l. For soft-margin SVMs,<br />SVM-CV(soft), we also had to determine the penalty<br />parameter C, so we performed a two-level grid search over<br />a two-dimensional parameter space ðC;lÞ.16Finally, for<br />comparison to baseline methods, we also examined the<br />performance of One Nearest Neighbor (1-NN), k Nearest<br />Neighbor (k-NN)17and linear discriminant analysis (LDA).<br />The experimental results (in Table 4 and Table 5) for the<br />three versions of EM-EP applied to GPC models provide<br />interesting insights. GPC with latent function noise (GPC-<br />EP(s,soft)), i.e., which explicitly allows soft boundaries, is<br />better than or as good as the harder version (GPC-<br />EP(s,hard)). This shows that allowing ambiguity at the<br />boundary is important. For these size data sets, the model<br />with multiple lengthscale hyperparameters (GPC-EP(m,-<br />soft)) did not always outperform the single lengthscale<br />model (GPC-EP(s,soft)). However, multiple lengthscales<br />did seem to be essential in learning the Crabs data set,<br />where its error rate was less than half the nearest<br />competitor, and the multiple lengthscale model usually<br />performed among the top methods. For higher-dimensional<br />data sets, fitting too many lengthscale hyperparameters can<br />1<br />minstead of type lm.<br />?2<br />clearly lead to the danger of overfitting, and it might be<br />advisable to do Bayesian averaging over these parameters.<br />The experimental results for the three variants of SVMs<br />are also enlightening. The SVM with the same hyperpara-<br />meters as GPC trained by EM-EP (SVM-EP(s,soft)) is worse<br />than (Heart disease, Crabs, and Pima) or comparable to or<br />slightly better than (Thyroid, Ionosphere, and Boston<br />Housing) the corresponding GPC (GPC-EP(s,soft)). Hard-<br />margin SVM with cross-validation is worse than GPC with<br />a hard decision boundary on four out of six data sets and is<br />slightly better than (or almost as good as) that on the other<br />data sets. In all data sets, GPC with a soft decision boundary<br />(GPC-EP(m,soft) or GPC-EP(s,soft)) is better than or as good<br />as soft-margin SVM (SVM-CV(soft)) with cross-validation.<br />In all cases, the EM-EP procedure seems to perform better than<br />cross-validation, even when it comes to fitting the SVM kernel<br />hyperparameters. Moreover, cross-validation would be com-<br />putationally prohibitive for models with many hyperpara-<br />meters, such as the multiple lengthscale models.<br />The experiment results for GPCs with other approxima-<br />tion methods than EP are interesting. GPC-EP (m,soft) is<br />better than GPC-VL(m,soft), GPC-VU(m,soft), and GPC-<br />L(m,soft) on four out of six data sets and is slightly worse<br />than (or almost as good as) the best of them on the other<br />data sets. EP seems to work better than the variational<br />approximation method and Laplace approximation method<br />in Gaussian process classification.<br />GPC and SVM both have a time complexity of Oðn3Þ, but<br />SVM is usually faster since it uses a sparse scheme.<br />SVM-CV methods are very slow because of the need to<br />solve many quadratic programs during cross validation.<br />More information about the experiment results of the<br />GPCs can be found on the Computer Society Digital Library<br />at http://computer.org/tpami/archives.htm.<br />4MULTICLASS CLASSIFICATION<br />In previous sections, we dealt with binary-class Gaussian<br />process classification. Here, we consider a multiclass<br />extension of Gaussian process classification.<br />4.1<br />If the data has J classes, each data point has latent functions<br />The Traditional Multiclass GPC Formulation<br />f1ð?Þ;f2ð?Þ;...;fJð?Þ:<br />For a data set D ¼ fðxi;yiÞji ¼ 1;2;...;ng, where<br />yi2 f1;...;Jg;<br />latent function values are represented as<br />f ¼ ½f1<br />where J is the number of classes, n is the number of data<br />points, and fj<br />point related to class j.<br />In the literature, the GP prior for multiclass classification<br />has usually been chosen to have only intraclass correlations<br />[3], [27]. The covariance matrix C for the prior of latent<br />values is defined as<br />1;f1<br />2;...;f1<br />n;f2<br />1;f2<br />2;...;f2<br />n;...;fJ<br />1;fJ<br />2;...;fJ<br />n?T;<br />ð36Þ<br />iis fjðxiÞ, a latent function value of ith data<br />KIM AND GHAHRAMANI: BAYESIAN GAUSSIAN PROCESS CLASSIFICATION WITH THE EM-EP ALGORITHM 1955<br />12. An outlier robust classification algorithm with ? updated was<br />proposed in [25].<br />13. Using the MATLAB Support Vector Machine Toolbox available from<br />http://theoval.sys.uea.ac.uk/~gcc/svm/toolbox with modified kernel<br />functions.<br />14. First,we dida coarse<br />0:5;1;1:5;2;2:5;3g to obtain C1. Then, we did a finer grid search over<br />fCjlog10C ¼ ?0:4 þ logC1;?0:3 þ logC1;...;0:4 þ logC1g.<br />15. Similarly to the selection of C, we did a two-level grid search over<br />fljlog10l ¼ ?3;?2:5;?2;?1:5;?1;?0:5;0g a n d<br />?0:4 þ log10l1; ?0:3 þ logl1;...;0:4 þ logl1g.<br />16. The same grids as above for parameters C;l were used.<br />17. k was selected by five-fold cross validation.<br />grid searchover<br />fCjlog10C ¼ 0,<br />fljlog10l ¼fljlog10l ¼</p>  <p>Page 9</p> <p>C ¼<br />Cf1<br />0<br />0<br />0 ...<br />...<br />...<br />0<br />0<br />Cf2<br />0CfJ<br />2<br />4<br />3<br />5;<br />ð37Þ<br />where Cfj is a covariance matrix of latent values related to<br />class j. The covariance function for covariance matrix C will<br />be defined as<br />Covðfj<br />i;fl<br />kÞ ¼ ?ðj;lÞcðxi;xkÞ:<br />ð38Þ<br />Since we can assume that the mean is zero, the prior for<br />the latent function values f is pðfÞ ¼ Nð0;CÞ.<br />The likelihood term pðyijfiÞ, where fi¼ ½f1<br />yi2 f1;...;Jg; is defined by using a softmax function as<br />follows:<br />i;f2<br />i...fJ<br />i?, and<br />pðyijfiÞ ¼<br />expðfyi<br />P<br />iÞ<br />jexpðfj<br />iÞ:<br />ð39Þ<br />The graphical model for this version of multiclass GPC is<br />shown in Fig. 3.<br />Now that we have the prior and likelihood for latent<br />values f, we can get the posterior of f by Bayes’ theorem:<br />pðfjD;?Þ ¼pðyjfÞpðfjX;?Þ<br />pðDj?Þ<br />/<br />Y<br />i<br />expðfyi<br />P<br />iÞ<br />jexpðfj<br />iÞpðfjX;?Þ:<br />ð40Þ<br />The class probability for the new data point pð~ yj~ xÞ can be<br />obtained in the same way as in the binary case.<br />We review the multiclass GPCs with the traditional GPC<br />formulation. In [3], the latent function was inferred by<br />Laplace’s method. They used the Hybrid Monte Carlo<br />method to integrate over the hyperparamters. In [27], the<br />latent function was inferred by variational methods. They<br />maximized the lower bound or the upper bound of the<br />evidence to determine the hyperparameters. In [5], the<br />Markov Chain Monte Carlo method was used both to<br />estimate the posterior of latent function values and to<br />integrate over the hyperparameters. Recently, a sparse<br />approximation method for multiclass classification in the<br />traditional GPC formulation has been proposed in [28]. It<br />uses EP with greedy active set selection of a training set<br />based on information-theoretic criteria.<br />4.2A New Multiclass GPC Formulation<br />We now introduce a different representation for multi-<br />class classification using a new type of latent functions<br />gyi;jð?Þ, which are differences between fyið?Þ and fjð?Þ.<br />This makes it possible to straightforwardly extend the<br />EP algorithm for binary-class classification to the multi-<br />class case. Using the notation gyi;j<br />gyi;j<br />i<br />¼ fyi<br />Covðgyi;j<br />i<br />¼ gyi;jðxiÞ, where<br />i? fj<br />i ;gyk;l<br />i, we get, using (38):<br />kÞ ¼ E½gyi;j<br />i gyk;l<br />k? ¼ E½ðfyi<br />i? fj<br />iÞðfyk<br />k? fl<br />kÞ?ð41Þ<br />¼ E½fyi<br />ifyk<br />k??E½fyi<br />ifl<br />k? ? E½fj<br />ifyk<br />k? þ E½fj<br />ifl<br />k? ð42Þ<br />¼ ?ðyi;ykÞ? ?ðyi;lÞ ? ?ðyk;jÞ þ ?ðj;lÞðÞcðxi;xkÞ:<br />ð43Þ<br />This makes up the prior pðgjXÞ.<br />Using gyi;j<br />i<br />¼ fyi<br />i? fj<br />expðfyi<br />PJ<br />1 þP<br />i, we can rewrite:<br />pðyijfiÞ ¼<br />iÞ<br />j¼1expðfj<br />iÞ¼<br />expðfyi<br />PJ<br />i? fyi<br />i? fyi<br />iÞ<br />j¼1expðfj<br />iÞ<br />¼<br />1<br />j6¼yi expð?gyi;jÞ:<br />ð44Þ<br />Using the vector g to denote:<br />g ¼½gy1;1<br />gy2;1<br />1<br />;...;gy1;y1?1<br />;...;gy2;y2?1<br />;...;gyn;yn?1<br />1<br />;gy1;y1þ1<br />1<br />;gy2;y2þ1<br />2<br />;...;gy1;J<br />;...;gy2;J<br />;gyn;ynþ1<br />n<br />1<br />;<br />222<br />;<br />...;gyn;1<br />nn<br />;...;gyn;J<br />n<br />?T;<br />ð45Þ<br />we get a whole formulation:<br />&quot;<br />pðgjDÞ /<br />Y<br />i<br />pðyijgiÞ<br />#<br />pðgjX;?Þ<br />¼<br />Y<br />i<br />1<br />1 þP<br />j6¼yiexpð?gyi;jÞ<br />&quot;#<br />pðgjX;?Þ:<br />ð46Þ<br />This formulation does not decrease the expressive power of<br />the model. Actually, (39) in Section 4.1 has a troubling<br />redundancy [5]. Neal [5] suggested that the redundancy<br />could be removed and an arbitrary asymmetry into the<br />prior could be produced by forcing one of latent functions<br />to always be zero. If one thinks of the case of J ¼ 2, it is<br />clear that the GPC formulation with (39) has an extra<br />redundant latent function comparing to the binary GPC<br />formulation. The formulation in this section does not have<br />this redundancy and becomes equivalent to the binary-class<br />GPC formulation with a sigmoid likelihood function, which<br />can be easily seen when we look at (46).<br />Similarlytothebinaryclassificationcase,wecandefinethe<br />likelihood function as pðyijgiÞ ¼ ð1 ? 2?ÞQ<br />resultant posterior of g is:<br />&quot;<br />j6¼yiHðgyi;j<br />i Þ þ ?. If<br />weput pðyijgiÞ ¼Q<br />j6¼yiHðgyi;j<br />i Þwithout?(orwhen? ¼ 0),the<br />pðgjDÞ /<br />Y<br />i<br />Y<br />j6¼yi<br />Hðgyi;j<br />i Þ<br />#<br />pðgjX;?Þ:<br />ð47Þ<br />1956IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 28,NO. 12, DECEMBER 2006<br />Fig. 3. Graphical model for the traditional multiclass GPC formulation<br />with n training data points and one test data point. xi and yi are<br />observed, ~ x is given, ~ y is what should be predicted, The function values<br />ffj<br />each j, hence we have the undirected edges, and are conditionally<br />independent over different j, hence we have no edge over different j.<br />iji ¼ 1;2;...;ng and~fjin the plate are latent and jointly Gaussian for</p>  <p>Page 10</p> <p>For EP, we can label the prior and likelihood terms in (47):<br />t0ðgÞ ¼ pðgjX;?Þ;<br />ð48Þ<br />tði;jÞðgÞ ¼ Hðgyi;j<br />ðfor j ¼ 1;...;yi? 1;yiþ 1;...;J;i ¼ 1;...;nÞ:<br />Then, by considering tði;jÞðgÞ as a likelihood term in binary<br />classification, we can apply EP to this multiclass GPC in the<br />same way we applied it to the binary GPC. Likewise, the<br />EM-EP algorithm can be straightforwardly applied to the<br />multiclass GPC. The multiclass versions of the EP and the<br />EM-EP algorithm work well in practice.<br />For prediction, we need:<br />Z<br />¼<br />l6¼~ y<br />where ~ g~ y¼ ½~ g~ y;1;...; ~ g~ y;~ y?1; ~ g~ y;~ yþ1;...; ~ g~ y;J?, and<br />Z<br />¼<br />i Þ<br />ð49Þ<br />pð~ yj~ x;D;?Þ ¼<br />pð~ yj~ g~ y;?Þpð~ g~ yjD; ~ x;?Þ d~ g~ y<br />ZY<br />Hð~ g~ y<br />lÞpð~ g~ yjD; ~ x;?Þ d~ g~ y;<br />ð50Þ<br />pð~ g~ yjD; ~ x;?Þ ¼<br />pðg; ~ g~ yjD; ~ x;?Þ dg<br />Z<br />pð~ g~ yj~ x;g;?ÞpðgjD;?Þdg:<br />ð51Þ<br />Using the Gaussian EP approximation to pðgjD;?Þ; the<br />terms in (51) are all Gaussian and, so, pð~ g~ yjD; ~ x;?Þ is a ðJ ?<br />1Þ-dimensional Gaussian. Then, the value of (50) is a volume<br />where all variables are positive in the ðJ ? 1Þ-dimensional<br />Gaussian. This can be calculated numerically [29] or<br />approximated by EP.<br />We can consider an approximation scheme to prediction<br />which produces only strict class labels rather than class<br />probabilities. We compute ~ gm;lfor every possible pair of<br />classes m ¼ 1;2;...;J; and l 6¼ m:<br />~ gm;l¼<br />i<br />j6¼yi<br />X<br />X<br />?j<br />i?ðyi;mÞ ? ?ðyi;lÞ ? ?ðm;jÞ þ ?ðj;lÞðÞcðxi; ~ xÞ;<br />ð52Þ<br />where ?j<br />Then, we choose one which most nearly satisfies ~ gm;l&gt; 0<br />for all l 6¼ m as follows:<br />~ y ¼ argmax<br />iis ?iin (20)corresponding to gyi;j<br />i<br />obtained from EP.<br />m<br />~ gm;<br />ð53Þ<br />where ~ gm¼P<br />Also, there can be ties where more than one ~ gmis<br />maximum.<br />We also propose a simpler approximation scheme that<br />uses the property that latent function values are<br />gj;l<br />j?. Let us consider the case that we only use the latent<br />function values ~ g1;lfor class 1 ðj?¼ 1Þ. If all ~ g1;lfor l 6¼ 1 are<br />positive, we assign the test data point ~ x to class 1.<br />Otherwise, we assign it to the class whose correponding<br />latent function value is minimum. The classification scheme<br />can be written as follows:<br />?<br />Both of those two approximation schemes work well in<br />practice and are simpler than the numerical integral (51),<br />but lose the ability to obtain probabilities for the labels.<br />l6¼mHð~ gm;lÞ. In case none of ~ gmis J ? 1, we<br />cannot be sure which one is the most probable prediction.<br />i¼ fj<br />i? fl<br />i. We use only latent functions gj?;l<br />i<br />for a fixed<br />~ y ¼<br />1<br />argminl6¼1~ g1;l<br />if ~ g1;l&gt; 0; for all l 6¼ 1;<br />otherwise:<br />ð54Þ<br />4.3<br />We applied the multiclass EM-EP GPC to three real-world<br />data sets. The detailed information for the data sets is in<br />Table 6. New Thyroid, Auto-Mpg, and Boston Housing data<br />sets were obtained from the UCI Machine Learning<br />Repository. In the Auto-Mpg data set, the eighth attribute,<br />origin, was used as the class attribute (three classes), and in<br />Boston Housing data set class 1, 2, 3 includes data points<br />where M ? 15:4, 15:4 &lt; M ? 23:7, and M &gt; 23:7, respec-<br />tively (M is the 14th attribute, median value of owner-<br />occupied homes in $1,000s).18Table 7 shows the classifica-<br />tion error rates of various methods. The experiment<br />protocol including the initial value setting and 10-fold<br />averaging procedure were the same as with the case of<br />binary-class classification (Section 3.5.2). For prediction in<br />GPC, we used the approximation scheme (54) which<br />produces a strict class label. Results for the Laplace method<br />or variational method are not given since no public code for<br />multiclass versions of these methods was found. We did not<br />show the experiment results for the hard decision boundary<br />case, because it is clear from the binary-class experiments<br />Experimental Results<br />KIM AND GHAHRAMANI: BAYESIAN GAUSSIAN PROCESS CLASSIFICATION WITH THE EM-EP ALGORITHM1957<br />18. This discretization actually creates a three-class ordinal variable, so<br />ordinal regression methods may be more appropriate [30].<br />TABLE 6<br />Detailed Information on the Real-World Data Sets<br />(The labels “dis.” and “con.” mean the number of discrete and<br />continuous variables in each data set, respectively.)<br />TABLE 7<br />Classification Error Rates of Various Methods<br />for Real-World Data Sets</p>  <p>Page 11</p> <p>that a soft decision boundary almost always outperforms<br />the hard decision boundary case. For the SVM experiments,<br />we used the same software as in the binay-class experi-<br />ments. The software uses the DAGSVM method [31] for<br />multiclass classification. For LDA, we used J ? 1 discrimi-<br />nant features when we have J classes.<br />Table 7 shows means of the classification error rates and<br />standard errors on the means. In all three cases, GPC-<br />EP(s,soft) is better than or as good as SVM-CV(s,soft). In the<br />New Thyroid and Auto-Mpg data sets, GPC-EP(m,soft) is<br />better than GPC-EP(s,soft) and in the New Thyroid data set,<br />the classification error of GPC-EP(m,soft) is less than half<br />the nearest competitor.<br />More information about the experiment results of the<br />multiclassGPCscanbefoundontheComputerSocietyDigital<br />Library at http://computer.org/tpami/archives.htm.<br />5CONCLUSION<br />Based on the work of [11], [12], we presented the EM-EP<br />algorithm for hyperparameter learning in Gaussian process<br />classifiers. Experiments on synthetic and real-world data<br />sets showed the usefulness of hyperparameters related to<br />lengthscales and latent noise. GPC with EM-EP showed<br />better performance than SVM with cross-validation on all<br />the data sets used in the experiments. We derived a new<br />EP method and an EM-EP algorithm for multiclass GPCs<br />and showed that multiclass GPCs had lower test error rate<br />than SVMs with cross-validation on the problems we tested.<br />Apart from competitive performance, Gaussian Process<br />Classifiers also have some other advantages over nonprob-<br />abilistic kernel methods because they are fully statistical<br />models. We can use the evidence for model selection and<br />kernel hyperparameter optimization. Also, given new data,<br />we can get a class probability rather than a hard decision.<br />Eventhoughwedidnotuseitinthispaper,priorinformation<br />can be used to inform learning of the hyperparameters (for<br />example, if some input features are thought to be more<br />relevant or the noise is thought to be high).<br />The main problem with GPCs, including the EM-EP<br />algorithm presented in the paper, is that it requires<br />Oðn3Þ computation of matrix inversions during learning.<br />However, sparse approaches for GPC with the EP algorithm<br />have recently been developed [18], [20], [28], [32] that could<br />be applied here. If our multiclass extension is combined<br />with sparse versions of EM-EP and parameterized kernels,<br />it could provide a powerful general classification system.<br />Bayesian versions of SVMs which have sparse solutions<br />have also been proposed in [33], [34], [35], [36], [37].<br />Although we did not address the issue of reducing<br />computational complexity in this paper, this is clearly an<br />important topic which has received a lot of attention.<br />To summarize, the main contributions of this paper are<br />the following:<br />.<br />We have provided a detailed derivation of the EM-<br />EP algorithm for GPCs based on the work in [11],<br />[12], [17] and our own previous work [16], [25]<br />(Appendix).<br />We have provided a theorem on the property of the<br />EM-EP algorithm (Section 3.4).<br />.<br />.<br />Wehave carried out extensive empirical comparisons<br />of EM-EP to other classification methods (NN,LDA),<br />SVM classifiers, and other GPC algorithms (Table 4<br />andTable5).EP-basedlearningofthekernelseemsto<br />perform very well compared to other methods.<br />We have derived a novel formulation for multiclass<br />classification suitable for EM-EP and tested it<br />empirically (Section 4).<br />We hope that these contributions will encourage others<br />.<br />to explore and further develop the highly flexible Gaussian<br />process models for learning and pattern recognition.<br />APPENDIX<br />M-STEP IN THE EM-EP ALGORITHM<br />We take a lower bound for the log evidence by Jensen’s<br />inequality as follows:<br />logpðyjX;?Þ ¼log<br />Z<br />qðfÞlogpðyjf;?ÞpðfjX;?covÞ<br />pðyjf;?ÞpðfjX;?covÞ df<br />?<br />Z<br />qðfÞ<br />df ¼ F:<br />ð55Þ<br />The lower bound F can be written as<br />Z<br />?<br />F ¼<br />qðfÞlogpðyjf;?Þ df þ<br />Z<br />Z<br />qðfÞlogpðfjX;?covÞ df<br />qðfÞlogqðfÞ df:<br />We use F?, F?cov, and HðqÞ, respectively, to denote the three<br />integrals that make up F in (56). Since HðqÞ is independent<br />of the hyperparamenter set ?, and ? is independent of ?cov,<br />we optimize F for ?, by optimizing F?covand F?for ?covand<br />?, respectively.<br />By expanding F?cov, we get<br />F?cov¼ Eq½logpðfjX;?covÞ? ¼ Eq ?1<br />¼ ?1<br />¼ ?1<br />2logj2?Cj ?1<br />2f&gt;C?1f<br />??<br />2logj2?Cj ?1<br />2logj2?Cj ?1<br />2Eq½f&gt;C?1f?<br />2Eq½f?&gt;C?1Eq½f? ?1<br />2trðC?1Cov½f?Þ:<br />ð56Þ<br />Differentiating F?covfor ?, using the E-step result (i.e., (27)),<br />we obtain<br />?<br />þ1<br />¼ ?1<br />@?<br />þ1<br />@F?cov<br />@?<br />¼ ?1<br />2tr C?1@C<br />2tr C?1@C<br />2tr C?1@C<br />2tr C?1@C<br />@?<br />?<br />þ1<br />2Eq½f?&gt;C?1@C<br />?<br />þ1<br />?<br />@?C?1Eq½f?<br />@?C?1Cov½f?<br />?<br />@?C?1A<br />?<br />?<br />?<br />2h&gt;C?1@C<br />@?C?1h<br />:<br />ð57Þ<br />Using h ¼ C? ?, we get (31).<br />1958IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 28,NO. 12,DECEMBER 2006</p>  <p>Page 12</p> <p>REFERENCES<br />[1]<br />[2]<br />V. Vapnik, The Nature of Statistical Learning Theory. Springer, 1995.<br />R. Herbrich, T. Graepel, and C. Campbell, “Bayes Point<br />Machines,” J. Machine Learning Research, vol. 1, pp. 245-279, 2001.<br />C.K.I. Williams and D. Barber, “Bayesian Classification with<br />Gaussian Processes,” IEEE Trans. Pattern Anlysis and Machine<br />Intelligence, vol. 20, no. 12, pp. 1342-1351, Dec. 1998.<br />M. Gibbs and D.J.C. MacKay, “Variational Gaussian Process<br />Classifiers,” IEEE Trans. Neural Networks, vol. 11, no. 6, p. 1458,<br />Nov. 2000.<br />R. Neal, “Regression and Classification Using Gaussian Process<br />Priors,” Bayesian Statistics 6, pp. 475-501, 1997.<br />A. O’Hagan, “On Curve Fitting and Optimal Design for<br />Regression,” J. Royal Statistical Soc., vol. 40, pp. 1-42, 1978.<br />C.K.I. Williams and C.E. Rasmussen, “Gaussian Processes for<br />Regression,” Proc. Neural Information Processing Systems Conf.<br />(NIPS-8), 1995.<br />M. Gibbs and D.J. C. MacKay, “Efficient Implementation of<br />Gaussian Processes,” draft manuscript (http://citeseer.nj.nec.<br />com/6489.html) 1997.<br />R. Neal, “Bayesian Learning for Neural Networks,” Lecture Notes<br />in Statistics, no. 118, 1996.<br />[10] C.E. Rasmussen, “Evaluation of Gaussian Processes and other<br />Methods for Non-Linear Regression,” PhD Thesis, Univ. of<br />Toronto, 1996.<br />[11] M. Opper and O. Winther, “Gaussian Processes for Classification:<br />Mean Field Algorithms,” Neural Computation, vol. 12, no. 11,<br />pp. 2655-2684, Nov 2000.<br />[12] T. Minka, “A Family of Algorithms for Approximate Bayesian<br />Inference,” PhD thesis, MIT, Jan. 2001, http://research.microsoft.<br />com/~minka/papers/ep/.<br />[13] M. Kuss and C. Rasmussen, “Assessing Approximate Inference<br />for Binary Gaussian Process Classification,” J. Machine Learning<br />Research, vol. 6, pp. 1679-1704, 2005.<br />[14] L. Csato, E. Fokoue, M. Opper, B. Schottky, and O. Winther,<br />“Efficient Approaches to Gaussian Process Classification,” Proc.<br />Neural Information Processing Systems Conf. (NIPS), vol. 13, 2000.<br />[15] B. Krishnapuram, A. Hartemink, L. Carin, and M. Figueiredo, “A<br />Bayesian Approach to Joint Feature Selection and Classifier<br />Design,” IEEE Trans. Pattern Analysis and Machine Intelligence,<br />vol. 26, no. 9, pp. 1105-1111, Sept. 2004.<br />[16] H.-C. Kim and Z. Ghahramani, “The EM-EP Algorithm for<br />Gaussian Process Classification,” Proc. Workshop Probabilistic<br />Graphical Models for Classification (ECML), 2003.<br />[17] M. Seeger, “Notes on Minka’s Expectation Propagation for<br />Gaussian Process Classification,” technical report, 2002.<br />[18] L. Csato and M. Opper, “Sparse Representation for Gaussian<br />Process Models,” Proc. Neural Information Processing Systems Conf.<br />(NIPS), vol. 13, 2000.<br />[19] L. Csato, M. Opper, and O. Winther, “TAP Gibbs Free Energy,<br />Belief Propagation and Sparsity,” Proc. Neural Information Proces-<br />sing Systems Conf. (NIPS), vol. 14, 2001.<br />[20] M. Seeger, N. Lawrence, and R. Herbrich, “Sparse Representation<br />for Gaussian Process Models,” Proc. Neural Information Processing<br />Systems Conf. (NIPS), vol. 15, 2002.<br />[21] T. Minka and J. Lafferty, “Expectation-Propagation for the<br />Generative Aspect Model,” Proc. 18th Conf. Uncertainty in Artificial<br />Intelligence (UAI), pp. 352-359, 2002.<br />[22] Y. Qi and T. Minka, “Expectation Propagation for Signal Detection<br />in Flat-Fading Channels,” technical report, MIT, 2003.<br />[23] T. Minka and Y. Qi, “Tree-Structured Approximations by<br />Expectation Propagation,” Proc. Neural Information Processing<br />Systems Conf. (NIPS), vol. 16, 2003.<br />[24] T. Heskes and O. Zoeter, “Expectation Propagation for Approx-<br />imate Inference in Dynamic Bayesian Networks,” Proc. 16th Conf.<br />Uncertainty in Artificial Intelligence (UAI), pp. 216-223, 2002.<br />[25] H.-C. Kim, “ Bayesian and Ensemble Kernel Classifiers,”<br />thesis, POSTECH, Jan. 2005, http://home.postech.ac.kr/~grass/<br />publication/.<br />[26] G.H. Golub and C.F.V. Loan, Matrix Computation. Johns Hopkins<br />Press, 1996.<br />[27] M.N. Gibbs, “Bayesian Gaussian Processes for Regression and<br />Classification,” PhD thesis, Univ. of Cambridge, 1997.<br />[28] M. Seeger and M.I. Jordan, “Sparse Gaussian Process Classifica-<br />tion with Multiple Classes,” Technical Report TR 661, Dept. of<br />Statistics, Univ. of California at Berkeley, 2004.<br />[3]<br />[4]<br />[5]<br />[6]<br />[7]<br />[8]<br />[9]<br />PhD<br />[29] A. Genz, “Numerical Computation of Multivariate Normal<br />Probabilities,” J. Computer Graph Statistics, vol. 1, pp. 141-149, 1992.<br />[30] W. Chu and Z. Ghahramani, “Gaussian Processes for Ordinal<br />Regression,” J. Machine Learning Research, vol. 6, pp. 1019-1041,<br />2005.<br />[31] J. Platt, N. Cristianini, and J. Shawe-Taylor, “Large Margin DAGs<br />for Multiclass Classification,” Proc. Neural Information Processing<br />Systems Conf. (NIPS), pp. 547-553, vol. 12, 2000.<br />[32] E. Snelson and Z. Ghahramani, “Sparse Parametric Gaussian<br />Processes,” Proc. Neural Information Processing Systems Conf.<br />(NIPS), vol. 18, 2005.<br />[33] M. Seeger, “Bayesian Model Selection for Support Vector<br />Machines, Gaussian Processes and Other Kernel Classifiers,” Proc.<br />Neural Information Processing Systems Conf. (NIPS), vol. 12, pp. 603-<br />609, 2000.<br />[34] J. Kwok, “Moderating the Outputs of Support Vector Machine<br />Classifiers,” IEEE Trans. Neural Networks, vol. 10, no. 5, pp. 1018-<br />1031, 1999.<br />[35] J. Kwok, “The Evidence Framework Applied to Support Vector<br />Machines,” IEEE Trans. Neural Networks, vol. 11, no. 5, pp. 1162-<br />1173, 2000.<br />[36] P. Sollich, “Bayesian Methods for Support Vector Machines:<br />Evidence and Predictive Class Probabilities,” Machine Learning,<br />vol. 46, pp. 21-52, 2002.<br />[37] W. Chu, “Bayesian Approach to Support Vector Machines,” PhD<br />thesis, Nat’l Univ. of Singapore, Jan. 2003.<br />Hyun-Chul Kim received the BS and BEng<br />degress in 1999, the MEng degree in 2001, and<br />the PhD degree in 2005 from from the POST-<br />ECH (Pohang University of Science and Tech-<br />nology). In 2002, he was a visting research<br />student in the Gatsby Computational Neu-<br />roscience Unit, University College London. He<br />is now a researcher at the POSTECH Informa-<br />tion Technology Laboratories. His current re-<br />search interests include machine learning,<br />Bayesian statistics, and pattern recognition. He has recently worked<br />on Gaussian processes, graphical models, and financial engineering.<br />Zoubin Ghahramani received the BA and<br />BSEng degrees from the University of Pennsyl-<br />vania in 1990, and the PhD degree in 1995 from<br />MIT, advised by Michael I Jordan. He was a<br />postdoctoral fellow in the Artificial Intelligence<br />Lab at the University of Toronto, working with<br />Geoffrey Hinton from 1995-1998. From 1998 to<br />2005, he was a faculty member at the Gatsby<br />Computational Neuroscience Unit, University<br />College London. He is now a professor of<br />information engineering at the University of Cambridge. He also has<br />an appointment as an associate research professor in the Machine<br />Learning Department at Carnegie Mellon University and is adjunct<br />faculty in the Gatsby Unit, University College London. His current<br />research interests include Bayesian approaches to machine learning,<br />artificial intelligence, statistics, information retrieval, bioinformatics, and<br />computational motor control. He has recently worked on Gaussian<br />processes, nonparametric Bayesian methods, clustering, approximate<br />inference algorithms, graphical models, Monte Carlo methods, and<br />semisupervised learning. He serves on the editorial boards of the IEEE<br />Transactions on Pattern Analysis and Machine Intelligence, Machine<br />Learning, the Journal on Machine Learning Research, the Journal on<br />Artificial Intelligence Research, and Bayesian Analysis. He is a member<br />of the IEEE and the IEEE Computer Society.<br />. For more information on this or any other computing topic,<br />please visit our Digital Library at www.computer.org/publications/dlib.<br />KIM AND GHAHRAMANI: BAYESIAN GAUSSIAN PROCESS CLASSIFICATION WITH THE EM-EP ALGORITHM1959</p>   </div> <div id="rgw20_56ab1dc5c8ffa" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw21_56ab1dc5c8ffa">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw22_56ab1dc5c8ffa"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.66.7491&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Bayesian Gaussian Process Classification with the EM-EP Algorithm">Bayesian Gaussian Process Classification with the ...</a> </div>  <div class="details">   Available from <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.66.7491&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow">psu.edu</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw29_56ab1dc5c8ffa" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (60) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw30_56ab1dc5c8ffa" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw31_56ab1dc5c8ffa" >  <div class="indent-left">  <div id="rgw32_56ab1dc5c8ffa" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/261756504_Individualized_Gaussian_process-based_prediction_and_detection_of_local_and_global_gray_matter_abnormalities_in_elderly_subjects">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Gerard_Ridgway" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Gerard R Ridgway </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw33_56ab1dc5c8ffa">  <li class="citation-context-item"> "global volume differences (Peelle et al., 2012). Gaussian process (GP) models have emerged as a flexible and elegant approach for prediction of continuous, i.e. y ∈ ℝ, or binary, i.e. y ∈ [0] [1] variables (Kim and Ghahramani, 2006; Rasmussen, 1996; Rasmussen and Williams, 2006). Recently,GPs were successfully introduced to the neuroimaging community. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/261756504_Individualized_Gaussian_process-based_prediction_and_detection_of_local_and_global_gray_matter_abnormalities_in_elderly_subjects"> <span class="publication-title js-publication-title">Individualized Gaussian process-based prediction and detection of local and global gray matter abnormalities in elderly subjects</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/16334843_G_Ziegler" class="authors js-author-name ga-publications-authors">G Ziegler</a> &middot;     <a href="researcher/39341622_GR_Ridgway" class="authors js-author-name ga-publications-authors">G.R. Ridgway</a> &middot;     <a href="researcher/78034_R_Dahnke" class="authors js-author-name ga-publications-authors">R Dahnke</a> &middot;     <a href="researcher/43392018_C_Gaser" class="authors js-author-name ga-publications-authors">C Gaser</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Structural imaging based on MRI is an integral component of the clinical assessment of patients with potential dementia. We here propose an individualized Gaussian process-based inference scheme for clinical decision support in healthy and pathological aging elderly subjects using MRI. The approach aims at quantitative and transparent support for clinicians who aim to detect structural abnormalities in patients at risk of Alzheimer&#39;s disease or other types of dementia. Firstly, we introduce a generative model incorporating our knowledge about normative decline of local and global grey matter volume across the brain in elderly. By supposing smooth structural trajectories the models account for the general course of age-related structural decline as well as late-life accelerated loss. Considering healthy subjects&#39; demography and global brain parameters as informative about normal brain aging variability affords individualized predictions in single cases. Using Gaussian process models as a normative reference, we predict new subjects&#39; brain scans and quantify the local grey matter abnormalities in terms of Normative Probability Maps (NPM) and global z-scores. By integrating the observed expectation error and the predictive uncertainty, the local maps and global scores exploit the advantages of Bayesian inference for clinical decisions and provide a valuable extension of diagnostic information about pathological aging. We validate the approach in simulated data and real MRI data. We train the GP framework using 1238 healthy subjects with ages 18-94 years, and predict in 415 independent test subjects diagnosed as healthy controls, Mild Cognitive Impairment and Alzheimer&#39;s disease. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Apr 2014  &middot; NeuroImage  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Gerard_Ridgway/publication/261756504_Individualized_Gaussian_process-based_prediction_and_detection_of_local_and_global_gray_matter_abnormalities_in_elderly_subjects/links/00b7d53a3f476c93b0000000.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw34_56ab1dc5c8ffa" >  <div class="indent-left">  <div id="rgw35_56ab1dc5c8ffa" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/235738119_A_New_Monte_Carlo_Based_Algorithm_for_the_Gaussian_Process_Classification_Problem">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Amir_Atiya" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Amir Atiya </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw36_56ab1dc5c8ffa">  <li class="citation-context-item"> "Among the wellknown proposed methods from the first category are the Laplace&#39;s approximation (Williams and Barber [44]) and the expectation propagation (Minka [30]). Also, some other efficient approximation-based methods include the work of Csató et al [6], Opper and Winther [35]), Gibbs and MacKay [15], Rifkin and Klautau [39], Jaakkola and Haussler [22], and Kim, and Ghahramani [26]. Work on the second category has been more scarce. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/235738119_A_New_Monte_Carlo_Based_Algorithm_for_the_Gaussian_Process_Classification_Problem"> <span class="publication-title js-publication-title">A New Monte Carlo Based Algorithm for the Gaussian Process Classification Problem</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/5797648_Amir_F_Atiya" class="authors js-author-name ga-publications-authors">Amir F. Atiya</a> &middot;     <a href="researcher/29941442_Hatem_A_Fayed" class="authors js-author-name ga-publications-authors">Hatem A. Fayed</a> &middot;     <a href="researcher/28281611_Ahmed_H_Abdel-Gawad" class="authors js-author-name ga-publications-authors">Ahmed H. Abdel-Gawad</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Gaussian process is a very promising novel technology that has been applied
to both the regression problem and the classification problem. While for the
regression problem it yields simple exact solutions, this is not the case for
the classification problem, because we encounter intractable integrals. In this
paper we develop a new derivation that transforms the problem into that of
evaluating the ratio of multivariate Gaussian orthant integrals. Moreover, we
develop a new Monte Carlo procedure that evaluates these integrals. It is based
on some aspects of bootstrap sampling and acceptancerejection. The proposed
approach has beneficial properties compared to the existing Markov Chain Monte
Carlo approach, such as simplicity, reliability, and speed. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Feb 2013  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Amir_Atiya/publication/235738119_A_New_Monte_Carlo_Based_Algorithm_for_the_Gaussian_Process_Classification_Problem/links/00b4951d65681e072c000000.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw37_56ab1dc5c8ffa" >  <div class="indent-left">  <div id="rgw38_56ab1dc5c8ffa" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/262286080_Nested_expectation_propagation_for_Gaussian_process_classification">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Aki_Vehtari" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Aki Vehtari </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw39_56ab1dc5c8ffa">  <li class="citation-context-item"> "This property can be used to transform the inference onto an equivalent non-redundant model which includes n(c − 1) unknown latent values with a Gaussian prior and a likelihood consisting of n(c − 1) factorizing terms. It follows that standard EP methodology for binary GP classification (Rasmussen and Williams, 2006) can be applied for posterior inference but a straightforward implementation results in a posterior representation scaling as O((c − 1) 3 n 3 ) and means to improve the scaling are not discussed by Kim and Ghahramani (2006). Contrary to the usual EP approach of maximizing the marginal likelihood approximation , Kim and Ghahramani (2006) determined the hyperparameters by maximizing a lower bound on the log marginal likelihood in a similar way as is done in the expectation maximization (EM) algorithm. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/262286080_Nested_expectation_propagation_for_Gaussian_process_classification"> <span class="publication-title js-publication-title">Nested expectation propagation for Gaussian process classification</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/80672769_Jaakko_Riihimaeki" class="authors js-author-name ga-publications-authors">Jaakko Riihimäki</a> &middot;     <a href="researcher/84345684_Pasi_Jylaenki" class="authors js-author-name ga-publications-authors">Pasi Jylänki</a> &middot;     <a href="researcher/296975_Aki_Vehtari" class="authors js-author-name ga-publications-authors">Aki Vehtari</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> This paper considers probabilistic multinomial probit classification using Gaussian process (GP) priors. Challenges with multiclass GP classification are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classification rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all between-class posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classification accuracy the differences between all the methods were small from a practical point of view. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Jan 2013  &middot; Journal of Machine Learning Research  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Aki_Vehtari/publication/262286080_Nested_expectation_propagation_for_Gaussian_process_classification/links/5507fe3c0cf27e990e08b19e.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw24_56ab1dc5c8ffa" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw25_56ab1dc5c8ffa">  </ul> </div> </div>   <div id="rgw16_56ab1dc5c8ffa" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw17_56ab1dc5c8ffa"> <div> <h5> <a href="publication/289122212_Early_warning_in_egg_production_curves_from_commercial_hens_A_SVM_approach" class="color-inherit ga-similar-publication-title"><span class="publication-title">Early warning in egg production curves from commercial hens: A SVM approach</span></a>  </h5>  <div class="authors"> <a href="researcher/2092024463_Ivan_Ramirez_Morales" class="authors ga-similar-publication-author">Iván Ramírez Morales</a>, <a href="researcher/2091984080_Daniel_Rivero_Cebrian" class="authors ga-similar-publication-author">Daniel Rivero Cebrián</a>, <a href="researcher/2078074129_Enrique_Fernandez_Blanco" class="authors ga-similar-publication-author">Enrique Fernández Blanco</a>, <a href="researcher/2095412982_Alejandro_Pazos_Sierra" class="authors ga-similar-publication-author">Alejandro Pazos Sierra</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56ab1dc5c8ffa"> <div> <h5> <a href="publication/291420195_Computer-Aided_Diagnosis_for_Breast_Ultrasound_Using_Computerized_BI-RADS_Features_and_Machine_Learning_Methods" class="color-inherit ga-similar-publication-title"><span class="publication-title">Computer-Aided Diagnosis for Breast Ultrasound Using Computerized BI-RADS Features and Machine Learning Methods</span></a>  </h5>  <div class="authors"> <a href="researcher/2095312725_Juan_Shan" class="authors ga-similar-publication-author">Juan Shan</a>, <a href="researcher/35357446_S_Kaisar_Alam" class="authors ga-similar-publication-author">S. Kaisar Alam</a>, <a href="researcher/2028535726_Brian_Garra" class="authors ga-similar-publication-author">Brian Garra</a>, <a href="researcher/2095361048_Yingtao_Zhang" class="authors ga-similar-publication-author">Yingtao Zhang</a>, <a href="researcher/2095294315_Tahira_Ahmed" class="authors ga-similar-publication-author">Tahira Ahmed</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56ab1dc5c8ffa"> <div> <h5> <a href="publication/288858015_Classification_of_geographic_origin_of_rice_by_data_mining_and_inductively_coupled_plasma_mass_spectrometry" class="color-inherit ga-similar-publication-title"><span class="publication-title">Classification of geographic origin of rice by data mining and inductively coupled plasma mass spectrometry</span></a>  </h5>  <div class="authors"> <a href="researcher/2089988328_Camila_Maione" class="authors ga-similar-publication-author">Camila Maione</a>, <a href="researcher/38413188_Bruno_Lemos_Batista" class="authors ga-similar-publication-author">Bruno Lemos Batista</a>, <a href="researcher/2091756426_Andres_Dobal_Campiglia" class="authors ga-similar-publication-author">Andres Dobal Campiglia</a>, <a href="researcher/39126115_Fernando_Barbosa" class="authors ga-similar-publication-author">Fernando Barbosa</a>, <a href="researcher/70363505_Rommel_Melgaco_Barbosa" class="authors ga-similar-publication-author">Rommel Melgaço Barbosa</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw41_56ab1dc5c8ffa" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw42_56ab1dc5c8ffa">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw43_56ab1dc5c8ffa" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=BFX8w1uhYuIl1dp5NJOLYk-qwsQrIlZDRW9oEQZLi6JFab4NxkOLPBhjj2_gkWaB" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="FeiseMWHVpVqUENLkJ1pP/lbYXd7jAiroUbs0H2htZz7HwtbC3hPQ3zqKDdLrX8oHhMkMp0rl0FFs3y4F8lR0X+XfOLyfSYuqnn8VsVasu96ohg66x/5SDIV3Uf10I8KnfHrSQqLNv5hL5h+cjmYLS7zXneqb+F+3gIizvsKmh3rI6vdaceWxoPHprk2BleU8oAmVTuFigi6iMH8OSTuiCXB2xz7MOcAA2+wIMQyYiZpQJ0zF1KE8Wxy+i1GibzW41E6sh3h5OwdHsmf51iiVVbQWojbHHKyGVYmMrmiR6k="/> <input type="hidden" name="urlAfterLogin" value="publication/6690142_Bayesian_Gaussian_Process_Classification_with_the_EM-EP_Algorithm"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vNjY5MDE0Ml9CYXllc2lhbl9HYXVzc2lhbl9Qcm9jZXNzX0NsYXNzaWZpY2F0aW9uX3dpdGhfdGhlX0VNLUVQX0FsZ29yaXRobQ%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vNjY5MDE0Ml9CYXllc2lhbl9HYXVzc2lhbl9Qcm9jZXNzX0NsYXNzaWZpY2F0aW9uX3dpdGhfdGhlX0VNLUVQX0FsZ29yaXRobQ%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vNjY5MDE0Ml9CYXllc2lhbl9HYXVzc2lhbl9Qcm9jZXNzX0NsYXNzaWZpY2F0aW9uX3dpdGhfdGhlX0VNLUVQX0FsZ29yaXRobQ%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw44_56ab1dc5c8ffa"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 1208;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"profileUrl":"researcher\/8942347_Hyun-Chul_Kim","fullname":"Hyun-Chul Kim","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2549355721578\/images\/template\/default\/profile\/profile_default_m.png","profileStats":[{"data":{"impactPoints":"26.06","widgetId":"rgw5_56ab1dc5c8ffa"},"id":"rgw5_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorImpactPoints.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorImpactPoints.html?authorUid=8942347","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationCount":19,"widgetId":"rgw6_56ab1dc5c8ffa"},"id":"rgw6_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorPublicationCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorPublicationCount.html?authorUid=8942347","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},null],"widgetId":"rgw4_56ab1dc5c8ffa"},"id":"rgw4_56ab1dc5c8ffa","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorBadge.html?authorUid=8942347","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw3_56ab1dc5c8ffa"},"id":"rgw3_56ab1dc5c8ffa","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=6690142","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":6690142,"title":"Bayesian Gaussian Process Classification with the EM-EP Algorithm","journalTitle":"IEEE Transactions on Pattern Analysis and Machine Intelligence","journalDetailsTooltip":{"data":{"journalTitle":"IEEE Transactions on Pattern Analysis and Machine Intelligence","journalAbbrev":"IEEE T PATTERN ANAL","publisher":"IEEE Computer Society; Institute of Electrical and Electronics Engineers, Institute of Electrical and Electronics Engineers","issn":"0162-8828","impactFactor":"5.78","fiveYearImpactFactor":"7.76","citedHalfLife":">10.0","immediacyIndex":"0.71","eigenFactor":"0.05","articleInfluence":"3.31","widgetId":"rgw8_56ab1dc5c8ffa"},"id":"rgw8_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=0162-8828","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":"Department of Industrial and Management Engineering, Pohang University of Science and Technology, Nam-gu, Pohang, Republic of China. ","type":"Article","details":{"doi":"10.1109\/TPAMI.2006.238","journalInfos":{"journal":"","publicationDate":"01\/2007;","publicationDateRobot":"2007-01","article":"28(12):1948-59.","journalTitle":"IEEE Transactions on Pattern Analysis and Machine Intelligence","journalUrl":"journal\/0162-8828_IEEE_Transactions_on_Pattern_Analysis_and_Machine_Intelligence","impactFactor":5.78}},"source":{"sourceUrl":"http:\/\/www.ncbi.nlm.nih.gov\/pubmed\/17108369","sourceName":"PubMed"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1109\/TPAMI.2006.238"},{"key":"rft.atitle","value":"Bayesian Gaussian Process Classification with the EM-EP Algorithm"},{"key":"rft.title","value":"IEEE transactions on pattern analysis and machine intelligence"},{"key":"rft.jtitle","value":"IEEE transactions on pattern analysis and machine intelligence"},{"key":"rft.volume","value":"28"},{"key":"rft.issue","value":"12"},{"key":"rft.date","value":"2007"},{"key":"rft.pages","value":"1948-59"},{"key":"rft.issn","value":"0162-8828"},{"key":"rft.au","value":"Hyun-Chul Kim,Zoubin Ghahramani"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw9_56ab1dc5c8ffa"},"id":"rgw9_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=6690142","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":6690142,"peopleItems":[{"data":{"authorUrl":"researcher\/8942347_Hyun-Chul_Kim","authorNameOnPublication":"Hyun-Chul Kim","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Hyun-Chul Kim","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/8942347_Hyun-Chul_Kim","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56ab1dc5c8ffa"},"id":"rgw12_56ab1dc5c8ffa","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=8942347&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56ab1dc5c8ffa"},"id":"rgw11_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=8942347&authorNameOnPublication=Hyun-Chul%20Kim","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/8159937_Zoubin_Ghahramani","authorNameOnPublication":"Zoubin Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Zoubin Ghahramani","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/8159937_Zoubin_Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw14_56ab1dc5c8ffa"},"id":"rgw14_56ab1dc5c8ffa","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=8159937&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw13_56ab1dc5c8ffa"},"id":"rgw13_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=8159937&authorNameOnPublication=Zoubin%20Ghahramani","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw10_56ab1dc5c8ffa"},"id":"rgw10_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=6690142&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":6690142,"abstract":"<noscript><\/noscript><div>Gaussian process classifiers (GPCs) are Bayesian probabilistic kernel classifiers. In GPCs, the probability of belonging to a certain class at an input location is monotonically related to the value of some latent function at that location. Starting from a Gaussian process prior over this latent function, data are used to infer both the posterior over the latent function and the values of hyperparameters to determine various aspects of the function. Recently, the expectation propagation (EP) approach has been proposed to infer the posterior over the latent function. Based on this work, we present an approximate EM algorithm, the EM-EP algorithm, to learn both the latent function and the hyperparameters. This algorithm is found to converge in practice and provides an efficient Bayesian framework for learning hyperparameters of the kernel. A multiclass extension of the EM-EP algorithm for GPCs is also derived. In the experimental results, the EM-EP algorithms are as good or better than other methods for GPCs or Support Vector Machines (SVMs) with cross-validation.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw15_56ab1dc5c8ffa"},"id":"rgw15_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=6690142","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/6690142_Bayesian_Gaussian_Process_Classification_with_the_EM-EP_Algorithm\/links\/0e5fb5a5f0c41c4932e99405\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw7_56ab1dc5c8ffa"},"id":"rgw7_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=6690142&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2092024463,"url":"researcher\/2092024463_Ivan_Ramirez_Morales","fullname":"Iv\u00e1n Ram\u00edrez Morales","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2091984080,"url":"researcher\/2091984080_Daniel_Rivero_Cebrian","fullname":"Daniel Rivero Cebri\u00e1n","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2078074129,"url":"researcher\/2078074129_Enrique_Fernandez_Blanco","fullname":"Enrique Fern\u00e1ndez Blanco","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095412982,"url":"researcher\/2095412982_Alejandro_Pazos_Sierra","fullname":"Alejandro Pazos Sierra","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2016","journal":"Computers and Electronics in Agriculture","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/289122212_Early_warning_in_egg_production_curves_from_commercial_hens_A_SVM_approach","usePlainButton":true,"publicationUid":289122212,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.76","url":"publication\/289122212_Early_warning_in_egg_production_curves_from_commercial_hens_A_SVM_approach","title":"Early warning in egg production curves from commercial hens: A SVM approach","displayTitleAsLink":true,"authors":[{"id":2092024463,"url":"researcher\/2092024463_Ivan_Ramirez_Morales","fullname":"Iv\u00e1n Ram\u00edrez Morales","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2091984080,"url":"researcher\/2091984080_Daniel_Rivero_Cebrian","fullname":"Daniel Rivero Cebri\u00e1n","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2078074129,"url":"researcher\/2078074129_Enrique_Fernandez_Blanco","fullname":"Enrique Fern\u00e1ndez Blanco","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095412982,"url":"researcher\/2095412982_Alejandro_Pazos_Sierra","fullname":"Alejandro Pazos Sierra","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Computers and Electronics in Agriculture 02\/2016; 121:169-179. DOI:10.1016\/j.compag.2015.12.009"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/289122212_Early_warning_in_egg_production_curves_from_commercial_hens_A_SVM_approach","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/289122212_Early_warning_in_egg_production_curves_from_commercial_hens_A_SVM_approach\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab1dc5c8ffa"},"id":"rgw17_56ab1dc5c8ffa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=289122212","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2095312725,"url":"researcher\/2095312725_Juan_Shan","fullname":"Juan Shan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":35357446,"url":"researcher\/35357446_S_Kaisar_Alam","fullname":"S. Kaisar Alam","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2028535726,"url":"researcher\/2028535726_Brian_Garra","fullname":"Brian Garra","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2095361048,"url":"researcher\/2095361048_Yingtao_Zhang","fullname":"Yingtao Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Ultrasound in medicine & biology","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291420195_Computer-Aided_Diagnosis_for_Breast_Ultrasound_Using_Computerized_BI-RADS_Features_and_Machine_Learning_Methods","usePlainButton":true,"publicationUid":291420195,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.21","url":"publication\/291420195_Computer-Aided_Diagnosis_for_Breast_Ultrasound_Using_Computerized_BI-RADS_Features_and_Machine_Learning_Methods","title":"Computer-Aided Diagnosis for Breast Ultrasound Using Computerized BI-RADS Features and Machine Learning Methods","displayTitleAsLink":true,"authors":[{"id":2095312725,"url":"researcher\/2095312725_Juan_Shan","fullname":"Juan Shan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":35357446,"url":"researcher\/35357446_S_Kaisar_Alam","fullname":"S. Kaisar Alam","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2028535726,"url":"researcher\/2028535726_Brian_Garra","fullname":"Brian Garra","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095361048,"url":"researcher\/2095361048_Yingtao_Zhang","fullname":"Yingtao Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095294315,"url":"researcher\/2095294315_Tahira_Ahmed","fullname":"Tahira Ahmed","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Ultrasound in medicine & biology 01\/2016;  DOI:10.1016\/j.ultrasmedbio.2015.11.016"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291420195_Computer-Aided_Diagnosis_for_Breast_Ultrasound_Using_Computerized_BI-RADS_Features_and_Machine_Learning_Methods","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291420195_Computer-Aided_Diagnosis_for_Breast_Ultrasound_Using_Computerized_BI-RADS_Features_and_Machine_Learning_Methods\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab1dc5c8ffa"},"id":"rgw18_56ab1dc5c8ffa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291420195","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2089988328,"url":"researcher\/2089988328_Camila_Maione","fullname":"Camila Maione","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38413188,"url":"researcher\/38413188_Bruno_Lemos_Batista","fullname":"Bruno Lemos Batista","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2091756426,"url":"researcher\/2091756426_Andres_Dobal_Campiglia","fullname":"Andres Dobal Campiglia","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":39126115,"url":"researcher\/39126115_Fernando_Barbosa","fullname":"Fernando Barbosa","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2016","journal":"Computers and Electronics in Agriculture","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/288858015_Classification_of_geographic_origin_of_rice_by_data_mining_and_inductively_coupled_plasma_mass_spectrometry","usePlainButton":true,"publicationUid":288858015,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.76","url":"publication\/288858015_Classification_of_geographic_origin_of_rice_by_data_mining_and_inductively_coupled_plasma_mass_spectrometry","title":"Classification of geographic origin of rice by data mining and inductively coupled plasma mass spectrometry","displayTitleAsLink":true,"authors":[{"id":2089988328,"url":"researcher\/2089988328_Camila_Maione","fullname":"Camila Maione","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38413188,"url":"researcher\/38413188_Bruno_Lemos_Batista","fullname":"Bruno Lemos Batista","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2091756426,"url":"researcher\/2091756426_Andres_Dobal_Campiglia","fullname":"Andres Dobal Campiglia","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39126115,"url":"researcher\/39126115_Fernando_Barbosa","fullname":"Fernando Barbosa","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70363505,"url":"researcher\/70363505_Rommel_Melgaco_Barbosa","fullname":"Rommel Melga\u00e7o Barbosa","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Computers and Electronics in Agriculture 02\/2016; 121:101-107. DOI:10.1016\/j.compag.2015.11.009"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/288858015_Classification_of_geographic_origin_of_rice_by_data_mining_and_inductively_coupled_plasma_mass_spectrometry","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/288858015_Classification_of_geographic_origin_of_rice_by_data_mining_and_inductively_coupled_plasma_mass_spectrometry\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab1dc5c8ffa"},"id":"rgw19_56ab1dc5c8ffa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=288858015","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw16_56ab1dc5c8ffa"},"id":"rgw16_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=6690142&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":6690142,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":6690142,"publicationType":"article","linkId":"0e5fb5a5f0c41c4932e99405","fileName":"Bayesian Gaussian Process Classification with the EM-EP Algorithm","fileUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.66.7491&amp;rep=rep1&amp;type=pdf","name":"psu.edu","nameUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.66.7491&amp;rep=rep1&amp;type=pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw22_56ab1dc5c8ffa"},"id":"rgw22_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=6690142&linkId=0e5fb5a5f0c41c4932e99405&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw21_56ab1dc5c8ffa"},"id":"rgw21_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=6690142&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":5,"valueFormatted":"5","widgetId":"rgw23_56ab1dc5c8ffa"},"id":"rgw23_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=6690142","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw20_56ab1dc5c8ffa"},"id":"rgw20_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=6690142&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":6690142,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw25_56ab1dc5c8ffa"},"id":"rgw25_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=6690142&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":5,"valueFormatted":"5","widgetId":"rgw26_56ab1dc5c8ffa"},"id":"rgw26_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=6690142","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw24_56ab1dc5c8ffa"},"id":"rgw24_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=6690142&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Bayesian Gaussian Process Classification\nwith the EM-EP Algorithm\nHyun-Chul Kim and Zoubin Ghahramani, Member, IEEE\nAbstract\u2014Gaussian process classifiers (GPCs) are Bayesian probabilistic kernel classifiers. In GPCs, the probability of belonging to a\ncertain class at an input location is monotonically related to the value of some latent function at that location. Starting from a Gaussian\nprocess prior over this latent function, data are used to infer both the posterior over the latent function and the values of\nhyperparameters to determine various aspects of the function. Recently, the expectation propagation (EP) approach has been\nproposed to infer the posterior over the latent function. Based on this work, we present an approximate EM algorithm, the EM-EP\nalgorithm, to learn both the latent function and the hyperparameters. This algorithm is found to converge in practice and provides an\nefficient Bayesian framework for learning hyperparameters of the kernel. A multiclass extension of the EM-EP algorithm for GPCs is\nalso derived. In the experimental results, the EM-EP algorithms are as good or better than other methods for GPCs or Support Vector\nMachines (SVMs) with cross-validation.\nIndex Terms\u2014Gaussian process classification, Bayesian methods, kernel methods, expectation propagation, EM-EP algorithm.\n\u00c7\n1\nK\nkernel classifiers are the support vector machine (SVM),\nBayes point machine (BPM), and Gaussian process classifier\n(GPC). The SVM was proposed as a classifier maximizing\nthe margin, which is the smallest distance between data\npoints and the class boundary [1]. SVMs have been a\npopular tool and have resulted in many successful applica-\ntions. The BPM is a kernel classifier whose goal is to\napproximate Bayes-optimal classification by finding the\ncenter of the mass of version space, which is the set of\nhyperplanes in feature space that separate the data [2]. It\nwas also shown that SVMs can be viewed as a form of Bayes\npoint machine which tries to find the center of the largest\nball to fit in version space. In contrast with the above two\nclassifiers, GPCs are Bayesian kernel classifiers derived\nfrom Gaussian process priors over functions which were\ndeveloped originally for regression [3], [4], [5].\nGaussian processes for regression [5], [6], [7], [8] assume\nthat the target function has a Gaussian process prior. This\nmeans that the density of any collection of target function\nvalues is modeled as a multivariate Gaussian density.\nUsually, the mean of this Gaussian is assumed to be zero\nand the covariance between the targets at two different\npoints is a decreasing function of their distance in input\nspace. This decreasing function is controlled by a small set\nINTRODUCTION\nERNEL classifiers have recently received much attention\nfrom the machine learning community. Some popular\nof hyperparameters that capture interpretable properties of\nthe function, such as the length scale of autocorrelation, the\noverall scale of the function, and the amount of noise. The\nposterior distributions of these hyperparameters given the\ndata can be inferred in a Bayesian way via Markov Chain\nMonte Carlo (MCMC) methods [5], [7] or they can be\nselected by maximizing the marginal likelihood (also\nknown as the evidence) [8]. A Bayesian treatment of\nmultilayer perceptrons for regression has been shown to\nconverge to a Gaussian process as the number of hidden\nnodes approaches to infinity, if the prior on input-to-hidden\nweights and hidden unit biases are independent and\nidentically distributed [9]. Empirically, Gaussian processes\nhave been shown to be an excellent method for nonlinear\nregression [10].\nIn GPCs, the target values are discrete class labels while\nthe target values in GP regression are continuous real\nvalues. It is not appropriate to assume that the target\nfunction with discrete outputs has a Gaussian process prior.\nWe assume that there is some latent function whose value at\na certain input location is monotonically related to the\nprobability of belonging to a certain class at that location\nand that the latent function rather than the target function\nhas a Gaussian process prior. We can use a Gaussian\nprocess as a prior of the latent function, and for multiclass\nclassification, one can use multiple GPs or a multivariate\nGP. Since only the class labels are observed in GPCs, we\nneed to integrate not only over hyperparameters but also\nover latent values of these functions at the data points.\nWilliams and Barber [3] used a Laplace approximation to\nintegrate over the latent values and Hybrid Monte Carlo\n(HMC) to integrate over the hyperparameters. Neal [5] used\nGibbs sampling to integrate over latent values and used\nHMC to integrate over hyperparameters. Gibbs and Mackay\n[4] used a variational approximation method to integrate\nover latent values and determined hyperparameters by\nmaximizing the marginal likelihood. Opper and Winther\n1948 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,VOL. 28, NO. 12,DECEMBER 2006\n. H.-C. Kim is with the Department of Industrial and Management\nEngineering, Pohang University of Science and Technology, San 31\nHyoja-Dong, Nam-gu, Pohang, 790-784, Republic of Korea.\nE-mail: grass@postech.ac.kr.\n. Z. Ghahramani is with the Department of Engineering, University of\nCambridge, Trumpington Street, Cambridge CB2 1PZ, UK.\nE-mail: zoubin@eng.cam.ac.uk.\nManuscript received 2 Nov. 2005; revised 3 Apr. 2006; accepted 12 Apr. 2006;\npublished online 12 Oct. 2006.\nRecommended for acceptance by M. Figueiredo.\nFor information on obtaining reprints of this article, please send e-mail to:\ntpami@computer.org, and reference IEEECS Log Number TPAMI-0595-1105.\n0162-8828\/06\/$20.00 ? 2006 IEEE Published by the IEEE Computer Society"},{"page":2,"text":"[11] used the TAP approach originally proposed in\nstatistical physics of disordered systems to integrate over\nlatent values.\nIt turns out that the TAP approach for GPCs is equivalent\nto the Expectation Propagation (EP) algorithm for approx-\nimate inference in Bayes point kernel machines [12]. EP has\nbeen shown to give results which are superior to Laplace\u2019s\nmethod and are very similar to MCMC methods both in\nterms of predictive distributions and marginal likelihood\nestimates [13]. In these previous papers, the focus has been\non approximate inference rather than determining hyper-\nparameters. Two potential methods for determining the\nhyperparameters have been proposed in [14]. The first\nmethod, which they called mean field method I, is to\nmaximize the variational lower bound of the evidence\nunder the assumption that the densities of latent values are\nindependent and Gaussian. The second method, which they\ncalled mean field method II, is to maximize the evidence\napproximated by Fourier transformation of the likelihood\nand a saddle point approximation. An EM algorithm for\nlearning the kernel length scales using an L1 prior has also\nbeen proposed [15].\nIn this paper, we propose and investigate a conceptually\nsimple EM-like algorithm to learn the hyperparameters\nwhich we call EM-EP.1In the E-step, we use EP to estimate\nthe joint density of latent values under the assumption that\nthe joint density is multivariate Gaussian. This multivariate\napproximation is better than factorized approximations\nsuch as the mean field method I. In the M-step, we\nmaximize with respect to the hyperparameters the varia-\ntional lower bound on the marginal likelihood given by\nusing the density of latent values obtained from the E-step.\nThese two steps are repeated until convergence. The idea of\nusing the variational lower bound for model selection in\nGPC was suggested in [17]. Here, we use a slightly different\nformulation for GPC and provide experimental results.\nAnother emphasis of this paper is examining the role of the\ndifferent hyperparameters and comparing these algorithms\nwith several variants of SVMs. We also propose an\nextension of the EM-EP algorithm for multiclass classifica-\ntion. Finally, although improving computational complexity\nof GPC learning through sparsification methods is an\nimportant research topic ([18], [19], [20]), we will not\naddress this problem in this paper.\nThe paper is organized as follows: Section 2 introduces\nGaussian process classification. In Section 3, we introduce\nthe EP method for GPCs, derive the EM-EP algorithm,\nand show the experimental results. In Section 4, we\nderive the EP method and the EM-EP algorithm for\nmulticlass GPCs and show experimental results. In\nSection 5, we discuss our approach and related work.\nSoftware implementing the EM-EP algorithm is available\nat http:\/\/home.postech.ac.kr\/~grass\/software\/.\n2GAUSSIAN PROCESS CLASSIFICATION\nLet us assume that we have a data set D of data points xi\nwith binary class labels yi2 f?1;\u00fe1g:\nD \u00bc f\u00f0xi;yi\u00deji \u00bc 1;2;...;ng;\nX \u00bc fxiji \u00bc 1;2;...;ng;\ny \u00bc fyiji \u00bc 1;2;...;ng:\nGiven this data set, the classification problem is to output\nthe correct class label for a new data point. To represent our\nuncertainty over class labels, one may want a method that\noutputs probabilities over the different labels for each new\ndata point.\nWe assume that the probability over class labels as a\nfunctionofxdependsonthevalueofsomelatentreal-valued\nfunction f\u00f0x\u00de. That is, for binary classification, given the\nvalue of f\u00f0x\u00de the probability of class label is independent of\nall other quantities: p\u00f0y \u00bc \u00fe1jx;f\u00f0x\u00de;D\u00de \u00bc p\u00f0y \u00bc \u00fe1jf\u00f0x\u00de\u00de.\nThe probability of observing y \u00bc \u00fe1 is assumed to be a\nmonotonically increasing function of f\u00f0x\u00de. This can take\nseveral forms, for example for fi\u00bc f\u00f0xi\u00de:\n1\n1\u00feexp\u00f0?yifi\u00de\n?\u00f0yifi\u00de\nH\u00f0yifi\u00de\n? \u00fe \u00f01 ? 2?\u00deH\u00f0yifi\u00de\nwhere H\u00f0z\u00de \u00bc 1 iff z > 0, ?\u00f0z\u00de is the cumulative normal\nfunction ?\u00f0z\u00de \u00bcRz\np\u00f0yijfi\u00de \u00bc\nsigmoid\ncumulative normal\nthreshold\nnoisy threshold;\n8\n>\n>\n>\n>\n:\n<\n\u00f01\u00de\n?1\n1ffiffiffiffi\n2?\np\nexp\u00f0?x2\n2\u00dedx, and 0 ? ? < 0:5.\nWe put a Gaussian process (see Fig. 1) prior on this\nfunction, meaning that any number of points evaluated\nfrom the function have a multivariate Gaussian density (see\n[7] for a review of GPs). Assume that this GP prior is\nparameterized by ? which we will call the hyperpara-\nmeters. We can write the probability of interest given ? as:\nZ\nThe second part of (2) is obtained by further integration\nover f \u00bc \u00bdf1;f2???fn?, the values of the latent function at the\ndata points.\np\u00f0~ yj~ x;D;?\u00de \u00bc\np\u00f0~ yj~f;?\u00dep\u00f0~fjD; ~ x;?\u00de d~f:\n\u00f02\u00de\nKIM AND GHAHRAMANI: BAYESIAN GAUSSIAN PROCESS CLASSIFICATION WITH THE EM-EP ALGORITHM1949\n1. An earlier version of this paper focusing on the EM-EP algorithm for\nbinary classification was presented at a workshop [16].\nFig. 1. In Gaussian processes for classification the class label is related\nto a latent function. (a) Examples of one-dimensional and (b) two-\ndimensional data showing a latent function sampled from a Gaussian\nprocess prior and the corresponding class label under the threshold\nmodel."},{"page":3,"text":"p\u00f0~fjD; ~ x;?\u00de \u00bc\nZ\nZ\np\u00f0f;~fjD; ~ x;?\u00de df\n\u00bc\np\u00f0~fj~ x;f;X;?\u00dep\u00f0fjD;?\u00dedf;\n\u00f03\u00de\nwhere\np\u00f0fjD;?\u00de \/ p\u00f0yjf;X;?\u00dep\u00f0fjX;?\u00de \u00bc\nY\nn\ni\u00bc1\np\u00f0yijfi;?\u00de\n !\np\u00f0fjX;?\u00de:\nThe first term is the probability of each observed class label\ngiven the latent function value, which can be of one of the\nforms in (1), while the second term is the GP prior over\nfunctions evaluated at the data. Writing the dependence of f\non x implicitly, the GP prior over functions can be written\n\u00f04\u00de\np\u00f0fjX;?\u00de \u00bc\n1\n\u00f02?\u00deN=2jC?j1=2exp ?1\n2\u00f0f ? ? ?\u00de>C?1\n?\u00f0f ? ? ?\u00de\n??\n;\n\u00f05\u00de\nwhere the mean ? ? is usually assumed to be zero ? ? \u00bc~0 and\neach term of a covariance matrix Cijis a function of xiand\nxj, i.e., c\u00f0xi;xj\u00de. This covariance function is the kernel which\ndefines how data points generalize to nearby data points.\nThe covariance function is parameterized by the hyper-\nparameters ?, which we can learn from the data. We will\ndescribe the particular covariance functions used in this\npaper later on. The Gaussian process classifier can be\nrepresented using the graphical model shown in Fig. 2.\nIn general, the class probability at a test point would be\nobtained by integrating over the hyperparameters weighted\nby their posterior probability\nZ\nThis integral is costly and there are usually many fewer\nhyperparameters than data points. Therefore, in this paper,\nrather than integrating over the hyperparameters, we fit\nthem by maximizing the marginal likelihood as ^? \u00bc\nargmax?p\u00f0Dj?\u00de and predict using these best fit hyperpara-\nmeters: p\u00f0~ yj~ x;D;^?\u00de. The marginal likelihood and p\u00f0fjD;?\u00de\nin (4) are both intractable due to the nonlinearities in (1). We\nuse the Expectation-Propagation (EP) algorithm to approx-\nimate both.\np\u00f0~ yj~ x;D\u00de \u00bc\np\u00f0~ yj~ x;D;?\u00dep\u00f0?jD\u00de d?:\n\u00f06\u00de\n3\n3.1\nThe Expectation Propagation (EP) algorithm is an approx-\nimate Bayesian inference method [12]. We briefly review EP\nin its general form before describing its application to GPCs.\nConsider a Bayesian inference problem where the\nposterior over some latent value (or parameter) f is\nproportional to the prior times likelihood terms for an\ni.i.d. data set\nTHE EM-EP ALGORITHM\nExpectation Propagation\np\u00f0fjy1;...;yn\u00de \/ p\u00f0f\u00de\nY\nn\ni\u00bc1\np\u00f0yijf\u00de \u00bc\nY\nn\ni\u00bc0\nti\u00f0f\u00de;\n\u00f07\u00de\nwhere t0\u00f0f\u00de \u00bc p\u00f0f\u00de and ti\u00f0f\u00de \u00bc p\u00f0yijf\u00de for i \u00bc 1;...;n. Notice\nthat, dropping some variables being conditioned on, (4) is of\nthis form. We approximate this distribution with a product\nof simple terms\nq\u00f0f\u00de \/~t0\u00f0f\u00de\nY\nn\ni\u00bc1\n~ti\u00f0f\u00de;\n\u00f08\u00de\nwhere each term (and therefore q) is assumed to be in the\nexponential family. EP iterates the following procedure\nover i until convergence:\nRemove the ith term from q\u00f0f\u00de: qni\u00f0f\u00de \u00bcQn\n3.\nFind a new~ti\u00f0f\u00de \u00bc t\u00f0f\u00de such that it minimizes the\nKullback-Leibler divergence2from qni\u00f0f\u00deti\u00f0f\u00de to\nqni\u00f0f\u00det\u00f0f\u00de:\n?\n\u00bc argmin\n1.\n2.\nj6\u00bci~tj\u00f0f\u00de.\nMultiply by the true ith factor: qni\u00f0f\u00deti\u00f0f\u00de \u00bc q\u00f0f\u00deti\u00f0f\u00de\n~ti\u00f0f\u00de.\n~tnew\ni\n\u00f0f\u00de \u00bc argmin\nt\u00f0f\u00deKL qni\u00f0f\u00deti\u00f0f\u00de\n?\n????\n????qni\u00f0f\u00det\u00f0f\u00de\n?\nt\u00f0f\u00deKL\nq\u00f0f\u00de\n~toldi\u00f0f\u00dep\u00f0yijf\u00de\n????\n????\nq\u00f0f\u00de\n~toldi\u00f0f\u00det\u00f0f\u00de\n?\n:\n\u00f09\u00de\nSince q is in the exponential family, this minimiza-\ntion is solved by matching moments of the approxi-\nmated distribution.\nThe algorithm is not guaranteed to converge although it\ndid in practice in all our examples. Assumed Density\nFiltering (ADF)3is a special online form of EP where only\none pass through the data is performed (i \u00bc 1;...n). EP can\nbe seen as an extension of ADF to batch situations.\nEP has been applied to several Bayesian learning\nproblems and its excellent performance has been demon-\nstrated on other problems. Minka showed that EP is better\nthan Laplace\u2019s method and the variational Bayes method in\nterms of accuracy and computational cost for simple\nBayesian learning problems such as the clutter problem\nand mixture weights learning problem [12]. It has been\nshown that EP provides better accuracy than variational\nmethods at a comparable cost for the generative aspect\nmodel [21]. EP was also applied to the signal detection\nproblem in flat-fading channels which can be formulated as\n1950 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,VOL. 28,NO. 12,DECEMBER 2006\n2. The Kullback-Leibler divergence between two distributions p\u00f0f\u00de and\nq\u00f0f\u00de is defined as: KL\u00f0p\u00f0f\u00dejjq\u00f0f\u00de\u00de \u00bcRp\u00f0f\u00delogp\u00f0f\u00de\nintelligence as different names such as online Bayesian learning, moment\nmatching, and weak marginalization.\nq\u00f0f\u00dedf.\n3. ADF appears in several fields such as control, statistics, and artificial\nFig. 2. Graphical model for GPCs with n training data points and one test\ndata point. xi and yi are observed, ~ x is given, ~ y is what should be\npredicted, fi and~f are latent and jointly Gaussian, hence have the\nundirected edges."},{"page":4,"text":"an estimation problem in a hybrid dynamic system with\nboth continuous and discrete variables [22]. In this problem,\nEP provides a much lower computational cost than Monte\nCarlo filter and smoothers. Tree-structured EP showed\nbetter accuracy and convergence than normal belief\npropagation, and a lower cost than variational trees or\ndouble-loop algorithms [23]. A Bayes point machine with\nEP which allows only hard decision boundaries showed\nbetter performance than a hard-margin support vector\nmachine in most cases [12]. EP is not guaranteed to\nconverge but in practice it converges in many cases. Its\ngeneralized version which is convergent but slower has\nbeen proposed [24].\n3.2\nWe describe EP for GPC referring to [11], [12], [20]. The\nform of the likelihood we use in the GPC is\nEP for Gaussian Process Classification\np\u00f0yijfi\u00de \u00bc ? \u00fe \u00f01 ? 2?\u00deH\u00f0yifi\u00de;\n\u00f010\u00de\nwhere H\u00f0x\u00de \u00bc 1 if x > 0, and otherwise 0. The hyperpara-\nmeter, ? in (10) models labeling error outliers. The\nEP algorithm approximates the posterior\np\u00f0fjD;?\u00de \u00bcp\u00f0fjX;?\u00dep\u00f0yjf;?\u00de\np\u00f0yjX;?\u00de\n\u00f011\u00de\nas a Gaussian having the form\nq\u00f0f\u00de \u00bc N\u00f0h;A\u00de;\n\u00f012\u00de\nwhere the GP prior p\u00f0fjX;?\u00de ? N\u00f00;C\u00de has the covariance\nmatrix C with elements Cij defined by the covariance\nfunction\n(\n\u00fe ?1\u00fe ?2?\u00f0i;j\u00de;\nwhere xm\niis the mth element of xi,\n?\nand?\u00f0xm\nmore reasonable distance measure than \u00f0xm\ndiscrete data.\nThe hyperparameter ?0specifies the overall vertical scale\nof variation of the latent values, ?1 the overall bias of the\nlatent values from zero mean, ?2the latent noise variance,\nand lm the (inverse) lengthscale for feature dimension m.\nThe cumulative normal density likelihood term in (1) is\nequivalent to using the threshold function in (10) with ? \u00bc 0\nand nonzero latent noise ?2.\nEP tries to approximate the posterior (11) which can be\nwritten as:\np\u00f0fjD;?\u00de \u00bcp\u00f0fjX;?\u00deQn\np\u00f0yijf\u00de \u00bc ti\u00f0f\u00de in (10) is approximated by\n~ti\u00f0f\u00de \u00bc siexp ?1\nCij\u00bc c\u00f0xi;xj\u00de \u00bc ?0exp ?1\n2\nX\nd\nm\u00bc1\nlmdm\u00f0xm\ni;xm\nj\u00de\n)\n\u00f013\u00de\ndm\u00f0xm\ni;xm\nj\u00de \u00bc\n\u00f0xm\n1 ? ?\u00f0xm\ni? xm\nj\u00de2\nif xmis continous;\nif xmis discrete;\ni;xm\nj\u00de\n\u00f014\u00de\ni;xm\nj\u00deisaKroneckerdeltafunction.1 ? ?\u00f0xm\ni;xm\nj\u00de2for\nj\u00deisa\ni? xm\ni\u00bc1p\u00f0yijf\u00de\np\u00f0yjX;?\u00de\n:\n\u00f015\u00de\n2vi\u00f0fi? mi\u00de2\n??\n:\n\u00f016\u00de\nFrom this initial setting, we can derive EP for GPC by\napplying the general idea described above. The details of\nthe derivation are in [25]. The resulting EP procedure is\nvirtually identical to the one derived for BPMs in [12]. We\ndefine the following notation:4\n? ? \u00bc diag\u00f0v1;...;vn\u00de;hi\u00bc E\u00bdfi?;hni\n?i\u00bc V ar\u00bdfi?;?ni\nwhere hni\ni\nand fni\ni\nare values obtained from a whole set\nexcept for xi. The EP algorithm is as follows: After the\ninitialization\ni\u00bc E\u00bdfni\ni?;\ni\u00bc Var\u00bdfni\ni?;\n\u00f017\u00de\nvi\u00bc 1;mi\u00bc 0;si\u00bc 1;hi\u00bc 0;?ni\nthe following process is performed until all \u00f0mi;vi;si\u00de\nconverge: Loop i \u00bc 1;2;...;n:\n1.\nRemove the approximate density ~ti (for ith data\npoint)fromtheposteriorq\u00f0f\u00detogetan\u201cold\u201dposterior\nqni\u00f0f\u00de, and get a marginal qni\u00f0fi\u00de \u00bc N\u00f0hni\nhni\ni\u00bc Cii;\n\u00f018\u00de\ni;?ni\ni\u00de:\ni\u00bc hi\u00fe ?ni\niv?1\ni\u00f0hi? mi\u00de;?ni\ni\u00bc \u00f01=Aii? 1=vi\u00de?1:\n\u00f019\u00de\n2.\nFind qnew\u00f0fi\u00de ? N\u00f0hi;?i\u00de which minimizes KL diver-\ngence from qni\u00f0fi\u00deti\u00f0fi\u00de to qnew\u00f0fi\u00de:\nz \u00bcyihni\n?ni\ni\nyiffiffiffiffiffiffi\nwhere ?\u00f0z\u00de is a cumulative normal density function\ndefined in (1).\nGet a new~ti\u00f0fi\u00de \u00bc siexp\u00f0?1\n1\n?ihi? 1\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\niffiffiffiffiffiffi\nq\nq\n;Zi\u00bc ? \u00fe \u00f01 ? 2?\u00de?\u00f0z\u00de;\n?i\u00bc\n?ni\ni\n\u00f01 ? 2?\u00deN\u00f0z;0;1\u00de\nZi\n;hi\u00bc hni\ni\u00fe ?ni\ni?i;\n\u00f020\u00de\n3.\n2vi\u00f0fi? mi\u00de2\u00de:\n?\n?ni\nvi\u00bc ?ni\ni\n?\nq\n;mi\u00bc hi\u00fe vi?i;\n \nsi\u00bc Zi\n1 \u00fe v?1\ni?ni\ni\nexp\ni?i\n2hi\n!\n:\n\u00f021\u00de\n4.\nObtain a new q\u00f0f\u00de ? N\u00f0h;A\u00de using a new~ti\u00f0fi\u00de:\nA \u00bc \u00f0C?1\u00fe ? ??1\u00de?1;h \u00bc A? ??1m:\n\u00f022\u00de\nThe approximate evidence Z\u00f0?\u00de\u00f0? p\u00f0yjX;?\u00de\u00de is as\nfollows:\nZ\u00f0?\u00de \u00bct0\u00f0f\u00deQn\nwhere r \u00bcP\ndone in O\u00f0n2\u00de by the Woodbury formula also known as the\nmatrix inversion lemma [26]. Our approximated posterior\ni\u00bc1~ti\u00f0fi\u00de\nq\u00f0f\u00de\nm2\ni\n\u00bc\nj? ?j1=2\njC \u00fe ? ?j1=2exp\u00f0?r=2\u00de\nmimj\nvivj. One iteration of the above\nY\nn\ni\u00bc1\nsi; \u00f023\u00de\ni\nvi?P\nijAij\nEP algorithm can be executed in O\u00f0n3\u00de because (22) can be\nKIM AND GHAHRAMANI: BAYESIAN GAUSSIAN PROCESS CLASSIFICATION WITH THE EM-EP ALGORITHM1951\n4. diag\u00f0v1;...;vn\u00de means a diagonal matrix whose diagonal elements are\nv1;...;vn. Similarly for diag\u00f0v\u00de."},{"page":5,"text":"over latent values is q\u00f0f\u00de \u00bc N\u00f0h;A\u00de. According to [11], it\ncan also be written as q\u00f0f\u00de \u00bc N\u00f0C? ?;A\u00de. The approximate\nevidence in (23) can be used to measure the quality of fit of\nkernels or their hyperparameters to the data for model\nselection. However, it is difficult to obtain an updating rule\nfrom (23). In the following section, we derive the algorithm\nto find the hyperparameters automatically, based not on\n(23) but a variational lower bound of the evidence.\nWe will demonstrate how to find the hyperparameters\nsoon, but for the moment, we\u2019d like to concentrate on how\nwe predict the class probabilities at a new point, ~ x. To begin\nwith, we obtain the density for a latent value~f correspond-\ning to ~ x from (3). We obtain, using the approximation in (12),\nZ\n? N\u00f0kT\u00f0C\u00de?1h;? ? kT\u00f0? ? \u00fe C\u00de?1k\u00de;\nwhere k \u00bc \u00bdc\u00f0~ x;x1\u00de;c\u00f0~ x;x2\u00de;...;c\u00f0~ x;xn\u00de? and ? \u00bc c\u00f0~ x; ~ x\u00de.\nProbability of ~ x being class ~ y is obtained from (2) as follows:\nZ\n0\n@\nif we assume the test data point does not have labeling\nerrors and\n0\n@\nif we assume the test data point might also have labeling\nerrors.\nStrict classification of a new data point ~ x can be done\naccording to\np\u00f0~fjD; ~ x;?\u00de \u00bc\np\u00f0~fj~ x;f;?\u00dep\u00f0fjD;?\u00de df\n\u00f024\u00de\np\u00f0~ yj~ x;D;?\u00de \u00bc\np\u00f0~ yj~f;?\u00dep\u00f0~fjD; ~ x;?\u00de d~f\n\u00bc ?\n~ ykTC?1h\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\n? ? kT\u00f0? ? \u00fe C\u00de?1k\u00de\nq\nB\n1\nA;\nC\n\u00f025\u00de\np\u00f0~ yj~ x;D;?\u00de \u00bc ? \u00fe \u00f01 ? 2?\u00de?\n~ ykTC?1h\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\n? ? kT\u00f0? ? \u00fe C\u00de?1k\u00de\nq\nB\n1\nA; \u00f026\u00de\nC\nargmax\n~ y\np\u00f0~ yj~ x;D;?\u00de \u00bc sgn\u00f0E\u00bd~f?\u00de \u00bc sgn\u00f0kTC?1h\u00de:\nEquivalently, since h \u00bc C? ?, classification can be conducted\naccording to sgn\u00f0Pn\nMore information about EP for GPC can be found on the\nComputer Society Digital Library at http:\/\/computer.org\/\ntpami\/archives.htm.\ni\u00bc1?ic\u00f0xi; ~ x\u00de\u00de which is the expression\nfound in [11].\n3.3\nIn the last section, we have presented the EP algorithm for\nGaussian process classification. It supplies the posterior\nover latent functions, the predictive class probability for\nnew data points, and the approximate evidence. One\nimportant component missing from the paper so far is an\nalgorithm to estimate the hyperparameters of the covar-\niance function.\nWe address the problem of estimating hyperparameters\nof the covariance function in the framework of Gaussian\nprocess regression with incomplete target values. This idea\nmakes it possible to apply an EM-like algorithm. In the\nE-step, we infer the approximate (Gaussian) density for\nlatent function values q\u00f0f\u00de using EP. In the M-step, using\nThe EM-EP Algorithm\nq\u00f0f\u00de obtained in the E-step, we maximize a lower bound on\np\u00f0yjX;?\u00de as a function of ?. The E-step and M-step are\nalternated until convergence.\n.\nE-step. EP iterations are performed given the\nhyperparameters. p\u00f0fjD\u00de is approximated as a\nGaussian density q\u00f0f\u00de:\nq\u00f0f\u00de \u00bc N\u00f0h;A\u00de \u00bc N\u00f0C? ?;A\u00de:\n\u00f027\u00de\n.\nM-step. Given q\u00f0f\u00de obtained from the E-step, find the\ncovariance function hyperparameters and the label-\ning error hyperparameter which maximize a lower\nbound of the log evidence logp\u00f0yjX;?\u00de. We define\ny \u00bc \u00bdy1;y2;...;yn?>, let ? represent all of the hyper-\nparameters of the model: ?;?0;?1;?2;l1;l2;...;lp, and\nlet ?covrepresent all those in ? except for ?. Then, we\nobtain the evidence\nZ\nSince the above integral is intractable, we use an\napproximation technique. We take a lower bound F\nfor the log evidence by Jensen\u2019s inequality, as\nfollows:\nZ\nZ\np\u00f0yjX;?\u00de \u00bc\np\u00f0yjf;?\u00dep\u00f0fjX;?cov\u00de df:\n\u00f028\u00de\nlogp\u00f0yjX;?\u00de \u00bc logp\u00f0yjf;?\u00dep\u00f0fjX;?cov\u00dedf\n\u00f029\u00de\n?\nq\u00f0f\u00delogp\u00f0yjf;?\u00dep\u00f0fjX;?cov\u00de\nq\u00f0f\u00de\ndf \u00bc F:\n\u00f030\u00de\nUsing the E-step result (27) and the fact that\np\u00f0fjX;?cov\u00de \u00bc N\u00f00;C?\u00de and~C \u00bc Cdiag\u00f0y\u00de, we ob-\ntain the following gradient update rule with respect\nto a covariance hyperparameter ?\u00f02 ?cov\u00de:5\n@F\n@?\u00bc1\n\u00fe1\n2? ?>@C\n2tr C?1@C\n@?? ? ?1\n?\n2tr C?1@C\n@?\n??\n@?C?1A\n?\n:\n\u00f031\u00de\nThe detailed derivation is in Appendix A.\n3.4A Property of the EM-EP Algorithm\nIt turns out that the gradient of the lower bound of the\nevidence p\u00f0yjX;?\u00de in the M-step of the EM-EP algorithm is\nin the same direction as the gradient of the approximate\nevidence obtained by EP when we deal with only the\nhyperparameters ? ?covin the prior density. The proof is as\nfollows:\nTheorem 1. In the M-step of the EM-EP algorithm, the gradient\nof the lower bound F of p\u00f0yjX;?\u00de under q\u00f0f\u00de with respect to\nthe hyperparameters ? ?cov6of the covariance function is in the\nsame direction as the gradient of approximate evidence Z\u00f0? ?cov\u00de\n(? p\u00f0yjX;? ?cov\u00de) in (23).\n1952 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE,VOL. 28,NO. 12,DECEMBER 2006\n5.@C\n6. ? ?cov\u00bc \u00bd?0;?1;?2;l1;l2;...;ld?.\n@?is an elementwise differentiation of C."},{"page":6,"text":"Proof. The gradient of F with respect to ? ?covis expressed:\nZ\ndf \u00bc r? ?cov\nr? ?covF \u00bc r? ?cov\nq\u00f0f\u00delogp\u00f0yjf;?\u00dep\u00f0fjX;? ?cov\u00de\nq\u00f0f\u00de\nZ\nq\u00f0f\u00delogp\u00f0fjX;? ?cov\u00de df:\n\u00f032\u00de\nThe gradient of Z\u00f0? ?cov\u00de with respect to ? ?covis expressed\nusing (23):\nZ\ndf \u00bc r? ?cov\nr? ?covlogZ\u00f0? ?cov\u00de \u00bc r? ?cov\nq\u00f0f\u00delogZ\u00f0? ?cov\u00de\nZ\nq\u00f0f\u00delog\nQn\ni\u00bc1~ti\u00f0f\u00det0\u00f0fj? ?cov\u00de\nq\u00f0f\u00de\ndf\n\u00f033\u00de\n\u00bc r? ?cov\nZ\nq\u00f0f\u00delogt0\u00f0fj? ?cov\u00de df:\n\u00f034\u00de\nFrom (32),(34)andthe factthat t0\u00f0fj? ?cov\u00de \u00bc p\u00f0fjX;? ?cov\u00de,\nwe obtain the following equation:\nr? ?covF \u00bc r? ?covlogZ\u00f0? ?cov\u00de \u00bc\n1\nZ\u00f0? ?cov\u00der? ?covZ\u00f0? ?cov\u00de:\nu t\nAccording to Theorem 1, when we use the EM-EP\nalgorithm with only covariance hyperparmeters, the M-step\nuses the same direction as the gradient of the approximate\nevidence Z\u00f0? ?cov\u00de. On the other hand, when we use the EM-\nEP algorithm with some hyperparameters related to the\nlikelihood, the M-step does not use the same direction as\nthe gradient of the approximate evidence Z\u00f0? ?cov\u00de.\nEven though the theoretical justification for the EM-EP\nalgorithm is harder, in practice generally better inference\n(E-step) should lead to better (hyperparameter) learning.\nSome examples have shown that the approximate evidence\nfrom EP agrees very well with the one from an MCMC\nmethod [13]. The EM-EP algorithm is more likely to learn\nthe hyperparameter which is a maximum of the approx-\nimate evidence by MCMC method, when its M-step uses\nthe gradient of the approximate evidence (Theorem 1).\n3.5Experimental Results\nTo demonstrate the EM-EP procedure, we start with\nhyperparameter learning in synthetic data sets. We then\nuse binary-class real world data sets to compare the\nproposed algorithm with SVMs and other classification\nmethods. In the M-step, we used the conjugate gradient\nmethod with line searches.7\nhyperparameters are optimized in log transformed spaces\nso as to avoid constrained optimization.\nAll covariance function\n3.5.1 Synthetic Data Sets\nFirst, we show with a simple intuitive example that the EM-\nEP algorithm learns the hyperparameters better than\nLaplace\u2019s method and the variational method. We have\nsampled a latent function in a two-dimensional input space\nfrom a Gaussian process prior with inverse lengthscales 0.5\nand 2.0 in the two dimensions. We then sampled 200 data\npoints randomly from a uniform(-10,10) distribution and\nusedthesignofthelatentfunctiontodefinetheclasslabelsof\nthe points. Using this data, we learned the hyperparameters\nof a GPC with Laplace\u2019s method [3], the variational method\n[4],andtheEM-EPalgorithm.Weperformedthisexperiment\n10 times for different latent functions. Table 1 shows the\nmeans and standard deviation of the lengthscale hyperpara-\nmeters learned by the three methods. All methods seem to\nunderestimate the lengthscale parameters in (13), which\ncorresponds to assuming functions with longer lengthscales\n(i.e., more slowly varying). This may indicate underfitting\ndue to limited data. The EM-EP algorithm shows the best\nresults, which are fairly close to the true value.\nTo show the usefulness of lengthscale hyperparameters,\nwe generated a simple data set with six features distributed\nas follows: x1;x2;x3? N\u00f0y;1\u00de and x4;x5;x6? N\u00f00;1\u00de,\nwhere y 2 f?1g is the class label. That is, x1;x2;x3 are\nrelevant features while x4;x5;x6 are irrelevant to the\nclassification problem. We generated 300 data samples for\na training set and 10,000 data samples for a test set. We tried\nthe EM-EP algorithm with a single lengthscale hyperpara-\nmeter for all dimensions, or with multiple lengthscale\nhyperparameters. As would be hoped, we saw that the\nlengthscale hyperparameters for the irrelevant features\n(x4;x5;x6) decreased to near zero. The approximate log\nevidence and classification error are shown in Table 2. The\nresult show that GPC with multiple lengthscale hyperpara-\nmeters was significantly better than one with a single\nlengthscale, as measured both by classification error rates as\nwell as approximate log evidence logZ\u00f0? ?cov\u00de.\n3.5.2 Real-World Data Sets\nWe applied the proposed algorithm to several real-world\ndata sets. The detailed information for the real-world data\nsets we used is in Table 3. Thyroid, Heart disease, and\nIonosphere data sets were obtained from the UCI Machine\nKIM AND GHAHRAMANI: BAYESIAN GAUSSIAN PROCESS CLASSIFICATION WITH THE EM-EP ALGORITHM1953\nTABLE 1\nComparison of GPCs with Laplace\u2019s Method, the Variational Method, and the EM-EP Algorithm\n7. The optimization procedure is described in Appendix B in [10] and the\ncode is available from http:\/\/www.kyb.tuebingen.mpg.de\/bs\/people\/\ncarl\/code\/minimize\/."},{"page":7,"text":"Learning Repository,8Crabs, and Pima data sets were\nobtained from the PRNN site,9and Boston Housing data set\nwere obtained from the R software site.10The Thyroid data\nset originally had three classes: \u201cnormal,\u201d \u201chyper,\u201d and\n\u201chypo,\u201d but we created a binary classification problem by\ngrouping hyper and hypo into \u201cnot normal.\u201d The Pima data\nset has a training set of 200 and two kinds of test sets, but\nwe used only the training set as a whole set for experiments.\nThe Boston Housing data set has 506 data points and\n20 variables. It has a pair of duplicated variables, one of\nwhich is wrong and the other is a corrected one for one\nattribute, and has another pair of duplicated variables\nwhich are town name and town number. We made a\nbinary-class data set by assigning a class label according to\nwhether housing price is greater than USD25000 or not. So,\nwe actually have 17 variables for our classification problem.\nTable 4 and Table 5 show the classification error rates of\nvarious methods. Each data set was divided into 10 folds.\nEach fold was subsequently used as a test set, while the\nother nine folds were used as a training set. The numbers in\nTable 4 and Table 5 are the means of those 10 error rates and\nstandard errors on the means. We tried three versions of the\nEM-EP Gaussian Process Classifier: GPC-EP(s,soft) used a\nsingle lengthscale hyperparameter for all feature dimen-\nsions, while GPC-EP(m,soft) used a different lengthscale\nhyperparameter for each feature dimension.11Finally, GPC-\nEP(s,hard) was a GPC with a single lengthscale hyperpara-\nmeter where the decision boundary was \u201chard\u201d in the sense\nthat the latent function noise parameter ?2 was fixed to\n1954IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 28,NO. 12, DECEMBER 2006\nTABLE 3\nDetailed Information on the Real-World Data Sets\n(The items \u201cdis.,\u201d \u201ccon.,\u201d \u201cclasses,\u201d and \u201cdata points\u201d mean the number\nof discrete variables, continuous variables, classes, and data points in\neach data set.)\nTABLE 4\nClassification Error Rates of Various Methods\nfor Real-World Data Sets (I)\nTABLE 5\nClassification Error Rates of Various Methods\nfor Real-World Data Sets (II)\nTABLE 2\nComparison of GPCs with a Single Lengthscale\nHyperparameter and with Multiple Ones\n8. Available from http:\/\/www.ics.uci.edu\/~mlearn\/MPRepository.\nhtml.\n9. Available from http:\/\/www.stats.ox.ac.uk\/pub\/PRNN.\n10. Available from http:\/\/www.maths.lth.se\/help\/R\/.R\/library\/\nspdep\/html\/boston.html.\n11. The initial values of hyperparameters for GPC-EP(s,soft) for the\nfirst fold were as follows: ?0\nand those for subsequent folds are the results for the former fold. For\nGPC-EP(m,soft), the initial values of the hyperparameters for every\nfold were the results learned for the same fold in GPC-EP(s,soft).\n0\u00bc 1, ?0\n1\u00bc 0:0001, ?0\n2\u00bc 0:001, l0\nm\u00bc 0:05;8m,"},{"page":8,"text":"zero or a very small number. In all GPC models, ? (cf (1))\nwas set to zero.12\nWe also tried other methods for GPC: GPC-VL(m,soft)\nused a variational lower bound, and GPC-VU(m,soft) used\na variational upper bound to infer latent values [4]. GPC-\nL(m,soft) used Laplace\u2019s method to infer latent values [3].\nAll use an optimization scheme for hyperparameters. All\nuse a multiple lengthscale hyperparameter, but, in the cases\nof GPC-VL(m,soft) and GPC-VU(m,soft), they used a\nlengthscale hyperparameter of type\nEven though they do not have latent value noise hyper-\nparameter ?2, all have a soft decision boundary, because\nthey use a sigmoid function as a likelihood and they have a\nsignal variance hyperparameter ?0.\nWe compared our results to several variants of SVMs.13\nWe wanted to distinguish the effect of the kernel choice\nfrom the effect of the different loss functions and noise\nmodel in SVMs vs GPCs. Thus, in SVM-EP(s,soft), the\nkernel, (i.e., covariance function) was set to be the same,\nwith the same hyperparameters as the corresponding GPC-\nEP(s,soft) trained using EM-EP except for the latent noise\nvariance ?2. Instead, the penalty parameter C allowing\ntraining errors (i.e., penalizing the SVM slack variables) was\nselected by five-fold cross-validation.14\nWe also applied both hard and soft-margin SVMs with a\nGaussian kernel with a single lengthscale hyperparameter\n(without ?0, ?1, and ?2) selected by 5-fold cross-validation.15\nFor hard-margin SVM, SVM-CV(hard), we only needed to\nperform a two-level grid search for l. For soft-margin SVMs,\nSVM-CV(soft), we also had to determine the penalty\nparameter C, so we performed a two-level grid search over\na two-dimensional parameter space \u00f0C;l\u00de.16Finally, for\ncomparison to baseline methods, we also examined the\nperformance of One Nearest Neighbor (1-NN), k Nearest\nNeighbor (k-NN)17and linear discriminant analysis (LDA).\nThe experimental results (in Table 4 and Table 5) for the\nthree versions of EM-EP applied to GPC models provide\ninteresting insights. GPC with latent function noise (GPC-\nEP(s,soft)), i.e., which explicitly allows soft boundaries, is\nbetter than or as good as the harder version (GPC-\nEP(s,hard)). This shows that allowing ambiguity at the\nboundary is important. For these size data sets, the model\nwith multiple lengthscale hyperparameters (GPC-EP(m,-\nsoft)) did not always outperform the single lengthscale\nmodel (GPC-EP(s,soft)). However, multiple lengthscales\ndid seem to be essential in learning the Crabs data set,\nwhere its error rate was less than half the nearest\ncompetitor, and the multiple lengthscale model usually\nperformed among the top methods. For higher-dimensional\ndata sets, fitting too many lengthscale hyperparameters can\n1\nminstead of type lm.\n?2\nclearly lead to the danger of overfitting, and it might be\nadvisable to do Bayesian averaging over these parameters.\nThe experimental results for the three variants of SVMs\nare also enlightening. The SVM with the same hyperpara-\nmeters as GPC trained by EM-EP (SVM-EP(s,soft)) is worse\nthan (Heart disease, Crabs, and Pima) or comparable to or\nslightly better than (Thyroid, Ionosphere, and Boston\nHousing) the corresponding GPC (GPC-EP(s,soft)). Hard-\nmargin SVM with cross-validation is worse than GPC with\na hard decision boundary on four out of six data sets and is\nslightly better than (or almost as good as) that on the other\ndata sets. In all data sets, GPC with a soft decision boundary\n(GPC-EP(m,soft) or GPC-EP(s,soft)) is better than or as good\nas soft-margin SVM (SVM-CV(soft)) with cross-validation.\nIn all cases, the EM-EP procedure seems to perform better than\ncross-validation, even when it comes to fitting the SVM kernel\nhyperparameters. Moreover, cross-validation would be com-\nputationally prohibitive for models with many hyperpara-\nmeters, such as the multiple lengthscale models.\nThe experiment results for GPCs with other approxima-\ntion methods than EP are interesting. GPC-EP (m,soft) is\nbetter than GPC-VL(m,soft), GPC-VU(m,soft), and GPC-\nL(m,soft) on four out of six data sets and is slightly worse\nthan (or almost as good as) the best of them on the other\ndata sets. EP seems to work better than the variational\napproximation method and Laplace approximation method\nin Gaussian process classification.\nGPC and SVM both have a time complexity of O\u00f0n3\u00de, but\nSVM is usually faster since it uses a sparse scheme.\nSVM-CV methods are very slow because of the need to\nsolve many quadratic programs during cross validation.\nMore information about the experiment results of the\nGPCs can be found on the Computer Society Digital Library\nat http:\/\/computer.org\/tpami\/archives.htm.\n4MULTICLASS CLASSIFICATION\nIn previous sections, we dealt with binary-class Gaussian\nprocess classification. Here, we consider a multiclass\nextension of Gaussian process classification.\n4.1\nIf the data has J classes, each data point has latent functions\nThe Traditional Multiclass GPC Formulation\nf1\u00f0?\u00de;f2\u00f0?\u00de;...;fJ\u00f0?\u00de:\nFor a data set D \u00bc f\u00f0xi;yi\u00deji \u00bc 1;2;...;ng, where\nyi2 f1;...;Jg;\nlatent function values are represented as\nf \u00bc \u00bdf1\nwhere J is the number of classes, n is the number of data\npoints, and fj\npoint related to class j.\nIn the literature, the GP prior for multiclass classification\nhas usually been chosen to have only intraclass correlations\n[3], [27]. The covariance matrix C for the prior of latent\nvalues is defined as\n1;f1\n2;...;f1\nn;f2\n1;f2\n2;...;f2\nn;...;fJ\n1;fJ\n2;...;fJ\nn?T;\n\u00f036\u00de\niis fj\u00f0xi\u00de, a latent function value of ith data\nKIM AND GHAHRAMANI: BAYESIAN GAUSSIAN PROCESS CLASSIFICATION WITH THE EM-EP ALGORITHM 1955\n12. An outlier robust classification algorithm with ? updated was\nproposed in [25].\n13. Using the MATLAB Support Vector Machine Toolbox available from\nhttp:\/\/theoval.sys.uea.ac.uk\/~gcc\/svm\/toolbox with modified kernel\nfunctions.\n14. First,we dida coarse\n0:5;1;1:5;2;2:5;3g to obtain C1. Then, we did a finer grid search over\nfCjlog10C \u00bc ?0:4 \u00fe logC1;?0:3 \u00fe logC1;...;0:4 \u00fe logC1g.\n15. Similarly to the selection of C, we did a two-level grid search over\nfljlog10l \u00bc ?3;?2:5;?2;?1:5;?1;?0:5;0g a n d\n?0:4 \u00fe log10l1; ?0:3 \u00fe logl1;...;0:4 \u00fe logl1g.\n16. The same grids as above for parameters C;l were used.\n17. k was selected by five-fold cross validation.\ngrid searchover\nfCjlog10C \u00bc 0,\nfljlog10l \u00bcfljlog10l \u00bc"},{"page":9,"text":"C \u00bc\nCf1\n0\n0\n0 ...\n...\n...\n0\n0\nCf2\n0CfJ\n2\n4\n3\n5;\n\u00f037\u00de\nwhere Cfj is a covariance matrix of latent values related to\nclass j. The covariance function for covariance matrix C will\nbe defined as\nCov\u00f0fj\ni;fl\nk\u00de \u00bc ?\u00f0j;l\u00dec\u00f0xi;xk\u00de:\n\u00f038\u00de\nSince we can assume that the mean is zero, the prior for\nthe latent function values f is p\u00f0f\u00de \u00bc N\u00f00;C\u00de.\nThe likelihood term p\u00f0yijfi\u00de, where fi\u00bc \u00bdf1\nyi2 f1;...;Jg; is defined by using a softmax function as\nfollows:\ni;f2\ni...fJ\ni?, and\np\u00f0yijfi\u00de \u00bc\nexp\u00f0fyi\nP\ni\u00de\njexp\u00f0fj\ni\u00de:\n\u00f039\u00de\nThe graphical model for this version of multiclass GPC is\nshown in Fig. 3.\nNow that we have the prior and likelihood for latent\nvalues f, we can get the posterior of f by Bayes\u2019 theorem:\np\u00f0fjD;?\u00de \u00bcp\u00f0yjf\u00dep\u00f0fjX;?\u00de\np\u00f0Dj?\u00de\n\/\nY\ni\nexp\u00f0fyi\nP\ni\u00de\njexp\u00f0fj\ni\u00dep\u00f0fjX;?\u00de:\n\u00f040\u00de\nThe class probability for the new data point p\u00f0~ yj~ x\u00de can be\nobtained in the same way as in the binary case.\nWe review the multiclass GPCs with the traditional GPC\nformulation. In [3], the latent function was inferred by\nLaplace\u2019s method. They used the Hybrid Monte Carlo\nmethod to integrate over the hyperparamters. In [27], the\nlatent function was inferred by variational methods. They\nmaximized the lower bound or the upper bound of the\nevidence to determine the hyperparameters. In [5], the\nMarkov Chain Monte Carlo method was used both to\nestimate the posterior of latent function values and to\nintegrate over the hyperparameters. Recently, a sparse\napproximation method for multiclass classification in the\ntraditional GPC formulation has been proposed in [28]. It\nuses EP with greedy active set selection of a training set\nbased on information-theoretic criteria.\n4.2A New Multiclass GPC Formulation\nWe now introduce a different representation for multi-\nclass classification using a new type of latent functions\ngyi;j\u00f0?\u00de, which are differences between fyi\u00f0?\u00de and fj\u00f0?\u00de.\nThis makes it possible to straightforwardly extend the\nEP algorithm for binary-class classification to the multi-\nclass case. Using the notation gyi;j\ngyi;j\ni\n\u00bc fyi\nCov\u00f0gyi;j\ni\n\u00bc gyi;j\u00f0xi\u00de, where\ni? fj\ni ;gyk;l\ni, we get, using (38):\nk\u00de \u00bc E\u00bdgyi;j\ni gyk;l\nk? \u00bc E\u00bd\u00f0fyi\ni? fj\ni\u00de\u00f0fyk\nk? fl\nk\u00de?\u00f041\u00de\n\u00bc E\u00bdfyi\nifyk\nk??E\u00bdfyi\nifl\nk? ? E\u00bdfj\nifyk\nk? \u00fe E\u00bdfj\nifl\nk? \u00f042\u00de\n\u00bc ?\u00f0yi;yk\u00de? ?\u00f0yi;l\u00de ? ?\u00f0yk;j\u00de \u00fe ?\u00f0j;l\u00de\u00f0\u00dec\u00f0xi;xk\u00de:\n\u00f043\u00de\nThis makes up the prior p\u00f0gjX\u00de.\nUsing gyi;j\ni\n\u00bc fyi\ni? fj\nexp\u00f0fyi\nPJ\n1 \u00feP\ni, we can rewrite:\np\u00f0yijfi\u00de \u00bc\ni\u00de\nj\u00bc1exp\u00f0fj\ni\u00de\u00bc\nexp\u00f0fyi\nPJ\ni? fyi\ni? fyi\ni\u00de\nj\u00bc1exp\u00f0fj\ni\u00de\n\u00bc\n1\nj6\u00bcyi exp\u00f0?gyi;j\u00de:\n\u00f044\u00de\nUsing the vector g to denote:\ng \u00bc\u00bdgy1;1\ngy2;1\n1\n;...;gy1;y1?1\n;...;gy2;y2?1\n;...;gyn;yn?1\n1\n;gy1;y1\u00fe1\n1\n;gy2;y2\u00fe1\n2\n;...;gy1;J\n;...;gy2;J\n;gyn;yn\u00fe1\nn\n1\n;\n222\n;\n...;gyn;1\nnn\n;...;gyn;J\nn\n?T;\n\u00f045\u00de\nwe get a whole formulation:\n\"\np\u00f0gjD\u00de \/\nY\ni\np\u00f0yijgi\u00de\n#\np\u00f0gjX;?\u00de\n\u00bc\nY\ni\n1\n1 \u00feP\nj6\u00bcyiexp\u00f0?gyi;j\u00de\n\"#\np\u00f0gjX;?\u00de:\n\u00f046\u00de\nThis formulation does not decrease the expressive power of\nthe model. Actually, (39) in Section 4.1 has a troubling\nredundancy [5]. Neal [5] suggested that the redundancy\ncould be removed and an arbitrary asymmetry into the\nprior could be produced by forcing one of latent functions\nto always be zero. If one thinks of the case of J \u00bc 2, it is\nclear that the GPC formulation with (39) has an extra\nredundant latent function comparing to the binary GPC\nformulation. The formulation in this section does not have\nthis redundancy and becomes equivalent to the binary-class\nGPC formulation with a sigmoid likelihood function, which\ncan be easily seen when we look at (46).\nSimilarlytothebinaryclassificationcase,wecandefinethe\nlikelihood function as p\u00f0yijgi\u00de \u00bc \u00f01 ? 2?\u00deQ\nresultant posterior of g is:\n\"\nj6\u00bcyiH\u00f0gyi;j\ni \u00de \u00fe ?. If\nweput p\u00f0yijgi\u00de \u00bcQ\nj6\u00bcyiH\u00f0gyi;j\ni \u00dewithout?(orwhen? \u00bc 0),the\np\u00f0gjD\u00de \/\nY\ni\nY\nj6\u00bcyi\nH\u00f0gyi;j\ni \u00de\n#\np\u00f0gjX;?\u00de:\n\u00f047\u00de\n1956IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 28,NO. 12, DECEMBER 2006\nFig. 3. Graphical model for the traditional multiclass GPC formulation\nwith n training data points and one test data point. xi and yi are\nobserved, ~ x is given, ~ y is what should be predicted, The function values\nffj\neach j, hence we have the undirected edges, and are conditionally\nindependent over different j, hence we have no edge over different j.\niji \u00bc 1;2;...;ng and~fjin the plate are latent and jointly Gaussian for"},{"page":10,"text":"For EP, we can label the prior and likelihood terms in (47):\nt0\u00f0g\u00de \u00bc p\u00f0gjX;?\u00de;\n\u00f048\u00de\nt\u00f0i;j\u00de\u00f0g\u00de \u00bc H\u00f0gyi;j\n\u00f0for j \u00bc 1;...;yi? 1;yi\u00fe 1;...;J;i \u00bc 1;...;n\u00de:\nThen, by considering t\u00f0i;j\u00de\u00f0g\u00de as a likelihood term in binary\nclassification, we can apply EP to this multiclass GPC in the\nsame way we applied it to the binary GPC. Likewise, the\nEM-EP algorithm can be straightforwardly applied to the\nmulticlass GPC. The multiclass versions of the EP and the\nEM-EP algorithm work well in practice.\nFor prediction, we need:\nZ\n\u00bc\nl6\u00bc~ y\nwhere ~ g~ y\u00bc \u00bd~ g~ y;1;...; ~ g~ y;~ y?1; ~ g~ y;~ y\u00fe1;...; ~ g~ y;J?, and\nZ\n\u00bc\ni \u00de\n\u00f049\u00de\np\u00f0~ yj~ x;D;?\u00de \u00bc\np\u00f0~ yj~ g~ y;?\u00dep\u00f0~ g~ yjD; ~ x;?\u00de d~ g~ y\nZY\nH\u00f0~ g~ y\nl\u00dep\u00f0~ g~ yjD; ~ x;?\u00de d~ g~ y;\n\u00f050\u00de\np\u00f0~ g~ yjD; ~ x;?\u00de \u00bc\np\u00f0g; ~ g~ yjD; ~ x;?\u00de dg\nZ\np\u00f0~ g~ yj~ x;g;?\u00dep\u00f0gjD;?\u00dedg:\n\u00f051\u00de\nUsing the Gaussian EP approximation to p\u00f0gjD;?\u00de; the\nterms in (51) are all Gaussian and, so, p\u00f0~ g~ yjD; ~ x;?\u00de is a \u00f0J ?\n1\u00de-dimensional Gaussian. Then, the value of (50) is a volume\nwhere all variables are positive in the \u00f0J ? 1\u00de-dimensional\nGaussian. This can be calculated numerically [29] or\napproximated by EP.\nWe can consider an approximation scheme to prediction\nwhich produces only strict class labels rather than class\nprobabilities. We compute ~ gm;lfor every possible pair of\nclasses m \u00bc 1;2;...;J; and l 6\u00bc m:\n~ gm;l\u00bc\ni\nj6\u00bcyi\nX\nX\n?j\ni?\u00f0yi;m\u00de ? ?\u00f0yi;l\u00de ? ?\u00f0m;j\u00de \u00fe ?\u00f0j;l\u00de\u00f0\u00dec\u00f0xi; ~ x\u00de;\n\u00f052\u00de\nwhere ?j\nThen, we choose one which most nearly satisfies ~ gm;l> 0\nfor all l 6\u00bc m as follows:\n~ y \u00bc argmax\niis ?iin (20)corresponding to gyi;j\ni\nobtained from EP.\nm\n~ gm;\n\u00f053\u00de\nwhere ~ gm\u00bcP\nAlso, there can be ties where more than one ~ gmis\nmaximum.\nWe also propose a simpler approximation scheme that\nuses the property that latent function values are\ngj;l\nj?. Let us consider the case that we only use the latent\nfunction values ~ g1;lfor class 1 \u00f0j?\u00bc 1\u00de. If all ~ g1;lfor l 6\u00bc 1 are\npositive, we assign the test data point ~ x to class 1.\nOtherwise, we assign it to the class whose correponding\nlatent function value is minimum. The classification scheme\ncan be written as follows:\n?\nBoth of those two approximation schemes work well in\npractice and are simpler than the numerical integral (51),\nbut lose the ability to obtain probabilities for the labels.\nl6\u00bcmH\u00f0~ gm;l\u00de. In case none of ~ gmis J ? 1, we\ncannot be sure which one is the most probable prediction.\ni\u00bc fj\ni? fl\ni. We use only latent functions gj?;l\ni\nfor a fixed\n~ y \u00bc\n1\nargminl6\u00bc1~ g1;l\nif ~ g1;l> 0; for all l 6\u00bc 1;\notherwise:\n\u00f054\u00de\n4.3\nWe applied the multiclass EM-EP GPC to three real-world\ndata sets. The detailed information for the data sets is in\nTable 6. New Thyroid, Auto-Mpg, and Boston Housing data\nsets were obtained from the UCI Machine Learning\nRepository. In the Auto-Mpg data set, the eighth attribute,\norigin, was used as the class attribute (three classes), and in\nBoston Housing data set class 1, 2, 3 includes data points\nwhere M ? 15:4, 15:4 < M ? 23:7, and M > 23:7, respec-\ntively (M is the 14th attribute, median value of owner-\noccupied homes in $1,000s).18Table 7 shows the classifica-\ntion error rates of various methods. The experiment\nprotocol including the initial value setting and 10-fold\naveraging procedure were the same as with the case of\nbinary-class classification (Section 3.5.2). For prediction in\nGPC, we used the approximation scheme (54) which\nproduces a strict class label. Results for the Laplace method\nor variational method are not given since no public code for\nmulticlass versions of these methods was found. We did not\nshow the experiment results for the hard decision boundary\ncase, because it is clear from the binary-class experiments\nExperimental Results\nKIM AND GHAHRAMANI: BAYESIAN GAUSSIAN PROCESS CLASSIFICATION WITH THE EM-EP ALGORITHM1957\n18. This discretization actually creates a three-class ordinal variable, so\nordinal regression methods may be more appropriate [30].\nTABLE 6\nDetailed Information on the Real-World Data Sets\n(The labels \u201cdis.\u201d and \u201ccon.\u201d mean the number of discrete and\ncontinuous variables in each data set, respectively.)\nTABLE 7\nClassification Error Rates of Various Methods\nfor Real-World Data Sets"},{"page":11,"text":"that a soft decision boundary almost always outperforms\nthe hard decision boundary case. For the SVM experiments,\nwe used the same software as in the binay-class experi-\nments. The software uses the DAGSVM method [31] for\nmulticlass classification. For LDA, we used J ? 1 discrimi-\nnant features when we have J classes.\nTable 7 shows means of the classification error rates and\nstandard errors on the means. In all three cases, GPC-\nEP(s,soft) is better than or as good as SVM-CV(s,soft). In the\nNew Thyroid and Auto-Mpg data sets, GPC-EP(m,soft) is\nbetter than GPC-EP(s,soft) and in the New Thyroid data set,\nthe classification error of GPC-EP(m,soft) is less than half\nthe nearest competitor.\nMore information about the experiment results of the\nmulticlassGPCscanbefoundontheComputerSocietyDigital\nLibrary at http:\/\/computer.org\/tpami\/archives.htm.\n5CONCLUSION\nBased on the work of [11], [12], we presented the EM-EP\nalgorithm for hyperparameter learning in Gaussian process\nclassifiers. Experiments on synthetic and real-world data\nsets showed the usefulness of hyperparameters related to\nlengthscales and latent noise. GPC with EM-EP showed\nbetter performance than SVM with cross-validation on all\nthe data sets used in the experiments. We derived a new\nEP method and an EM-EP algorithm for multiclass GPCs\nand showed that multiclass GPCs had lower test error rate\nthan SVMs with cross-validation on the problems we tested.\nApart from competitive performance, Gaussian Process\nClassifiers also have some other advantages over nonprob-\nabilistic kernel methods because they are fully statistical\nmodels. We can use the evidence for model selection and\nkernel hyperparameter optimization. Also, given new data,\nwe can get a class probability rather than a hard decision.\nEventhoughwedidnotuseitinthispaper,priorinformation\ncan be used to inform learning of the hyperparameters (for\nexample, if some input features are thought to be more\nrelevant or the noise is thought to be high).\nThe main problem with GPCs, including the EM-EP\nalgorithm presented in the paper, is that it requires\nO\u00f0n3\u00de computation of matrix inversions during learning.\nHowever, sparse approaches for GPC with the EP algorithm\nhave recently been developed [18], [20], [28], [32] that could\nbe applied here. If our multiclass extension is combined\nwith sparse versions of EM-EP and parameterized kernels,\nit could provide a powerful general classification system.\nBayesian versions of SVMs which have sparse solutions\nhave also been proposed in [33], [34], [35], [36], [37].\nAlthough we did not address the issue of reducing\ncomputational complexity in this paper, this is clearly an\nimportant topic which has received a lot of attention.\nTo summarize, the main contributions of this paper are\nthe following:\n.\nWe have provided a detailed derivation of the EM-\nEP algorithm for GPCs based on the work in [11],\n[12], [17] and our own previous work [16], [25]\n(Appendix).\nWe have provided a theorem on the property of the\nEM-EP algorithm (Section 3.4).\n.\n.\nWehave carried out extensive empirical comparisons\nof EM-EP to other classification methods (NN,LDA),\nSVM classifiers, and other GPC algorithms (Table 4\nandTable5).EP-basedlearningofthekernelseemsto\nperform very well compared to other methods.\nWe have derived a novel formulation for multiclass\nclassification suitable for EM-EP and tested it\nempirically (Section 4).\nWe hope that these contributions will encourage others\n.\nto explore and further develop the highly flexible Gaussian\nprocess models for learning and pattern recognition.\nAPPENDIX\nM-STEP IN THE EM-EP ALGORITHM\nWe take a lower bound for the log evidence by Jensen\u2019s\ninequality as follows:\nlogp\u00f0yjX;?\u00de \u00bclog\nZ\nq\u00f0f\u00delogp\u00f0yjf;?\u00dep\u00f0fjX;?cov\u00de\np\u00f0yjf;?\u00dep\u00f0fjX;?cov\u00de df\n?\nZ\nq\u00f0f\u00de\ndf \u00bc F:\n\u00f055\u00de\nThe lower bound F can be written as\nZ\n?\nF \u00bc\nq\u00f0f\u00delogp\u00f0yjf;?\u00de df \u00fe\nZ\nZ\nq\u00f0f\u00delogp\u00f0fjX;?cov\u00de df\nq\u00f0f\u00delogq\u00f0f\u00de df:\nWe use F?, F?cov, and H\u00f0q\u00de, respectively, to denote the three\nintegrals that make up F in (56). Since H\u00f0q\u00de is independent\nof the hyperparamenter set ?, and ? is independent of ?cov,\nwe optimize F for ?, by optimizing F?covand F?for ?covand\n?, respectively.\nBy expanding F?cov, we get\nF?cov\u00bc Eq\u00bdlogp\u00f0fjX;?cov\u00de? \u00bc Eq ?1\n\u00bc ?1\n\u00bc ?1\n2logj2?Cj ?1\n2f>C?1f\n??\n2logj2?Cj ?1\n2logj2?Cj ?1\n2Eq\u00bdf>C?1f?\n2Eq\u00bdf?>C?1Eq\u00bdf? ?1\n2tr\u00f0C?1Cov\u00bdf?\u00de:\n\u00f056\u00de\nDifferentiating F?covfor ?, using the E-step result (i.e., (27)),\nwe obtain\n?\n\u00fe1\n\u00bc ?1\n@?\n\u00fe1\n@F?cov\n@?\n\u00bc ?1\n2tr C?1@C\n2tr C?1@C\n2tr C?1@C\n2tr C?1@C\n@?\n?\n\u00fe1\n2Eq\u00bdf?>C?1@C\n?\n\u00fe1\n?\n@?C?1Eq\u00bdf?\n@?C?1Cov\u00bdf?\n?\n@?C?1A\n?\n?\n?\n2h>C?1@C\n@?C?1h\n:\n\u00f057\u00de\nUsing h \u00bc C? ?, we get (31).\n1958IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 28,NO. 12,DECEMBER 2006"},{"page":12,"text":"REFERENCES\n[1]\n[2]\nV. Vapnik, The Nature of Statistical Learning Theory. Springer, 1995.\nR. Herbrich, T. Graepel, and C. Campbell, \u201cBayes Point\nMachines,\u201d J. Machine Learning Research, vol. 1, pp. 245-279, 2001.\nC.K.I. Williams and D. Barber, \u201cBayesian Classification with\nGaussian Processes,\u201d IEEE Trans. Pattern Anlysis and Machine\nIntelligence, vol. 20, no. 12, pp. 1342-1351, Dec. 1998.\nM. Gibbs and D.J.C. MacKay, \u201cVariational Gaussian Process\nClassifiers,\u201d IEEE Trans. Neural Networks, vol. 11, no. 6, p. 1458,\nNov. 2000.\nR. Neal, \u201cRegression and Classification Using Gaussian Process\nPriors,\u201d Bayesian Statistics 6, pp. 475-501, 1997.\nA. O\u2019Hagan, \u201cOn Curve Fitting and Optimal Design for\nRegression,\u201d J. Royal Statistical Soc., vol. 40, pp. 1-42, 1978.\nC.K.I. Williams and C.E. Rasmussen, \u201cGaussian Processes for\nRegression,\u201d Proc. Neural Information Processing Systems Conf.\n(NIPS-8), 1995.\nM. Gibbs and D.J. C. MacKay, \u201cEfficient Implementation of\nGaussian Processes,\u201d draft manuscript (http:\/\/citeseer.nj.nec.\ncom\/6489.html) 1997.\nR. Neal, \u201cBayesian Learning for Neural Networks,\u201d Lecture Notes\nin Statistics, no. 118, 1996.\n[10] C.E. Rasmussen, \u201cEvaluation of Gaussian Processes and other\nMethods for Non-Linear Regression,\u201d PhD Thesis, Univ. of\nToronto, 1996.\n[11] M. Opper and O. Winther, \u201cGaussian Processes for Classification:\nMean Field Algorithms,\u201d Neural Computation, vol. 12, no. 11,\npp. 2655-2684, Nov 2000.\n[12] T. Minka, \u201cA Family of Algorithms for Approximate Bayesian\nInference,\u201d PhD thesis, MIT, Jan. 2001, http:\/\/research.microsoft.\ncom\/~minka\/papers\/ep\/.\n[13] M. Kuss and C. Rasmussen, \u201cAssessing Approximate Inference\nfor Binary Gaussian Process Classification,\u201d J. Machine Learning\nResearch, vol. 6, pp. 1679-1704, 2005.\n[14] L. Csato, E. Fokoue, M. Opper, B. Schottky, and O. Winther,\n\u201cEfficient Approaches to Gaussian Process Classification,\u201d Proc.\nNeural Information Processing Systems Conf. (NIPS), vol. 13, 2000.\n[15] B. Krishnapuram, A. Hartemink, L. Carin, and M. Figueiredo, \u201cA\nBayesian Approach to Joint Feature Selection and Classifier\nDesign,\u201d IEEE Trans. Pattern Analysis and Machine Intelligence,\nvol. 26, no. 9, pp. 1105-1111, Sept. 2004.\n[16] H.-C. Kim and Z. Ghahramani, \u201cThe EM-EP Algorithm for\nGaussian Process Classification,\u201d Proc. Workshop Probabilistic\nGraphical Models for Classification (ECML), 2003.\n[17] M. Seeger, \u201cNotes on Minka\u2019s Expectation Propagation for\nGaussian Process Classification,\u201d technical report, 2002.\n[18] L. Csato and M. Opper, \u201cSparse Representation for Gaussian\nProcess Models,\u201d Proc. Neural Information Processing Systems Conf.\n(NIPS), vol. 13, 2000.\n[19] L. Csato, M. Opper, and O. Winther, \u201cTAP Gibbs Free Energy,\nBelief Propagation and Sparsity,\u201d Proc. Neural Information Proces-\nsing Systems Conf. (NIPS), vol. 14, 2001.\n[20] M. Seeger, N. Lawrence, and R. Herbrich, \u201cSparse Representation\nfor Gaussian Process Models,\u201d Proc. Neural Information Processing\nSystems Conf. (NIPS), vol. 15, 2002.\n[21] T. Minka and J. Lafferty, \u201cExpectation-Propagation for the\nGenerative Aspect Model,\u201d Proc. 18th Conf. Uncertainty in Artificial\nIntelligence (UAI), pp. 352-359, 2002.\n[22] Y. Qi and T. Minka, \u201cExpectation Propagation for Signal Detection\nin Flat-Fading Channels,\u201d technical report, MIT, 2003.\n[23] T. Minka and Y. Qi, \u201cTree-Structured Approximations by\nExpectation Propagation,\u201d Proc. Neural Information Processing\nSystems Conf. (NIPS), vol. 16, 2003.\n[24] T. Heskes and O. Zoeter, \u201cExpectation Propagation for Approx-\nimate Inference in Dynamic Bayesian Networks,\u201d Proc. 16th Conf.\nUncertainty in Artificial Intelligence (UAI), pp. 216-223, 2002.\n[25] H.-C. Kim, \u201c Bayesian and Ensemble Kernel Classifiers,\u201d\nthesis, POSTECH, Jan. 2005, http:\/\/home.postech.ac.kr\/~grass\/\npublication\/.\n[26] G.H. Golub and C.F.V. Loan, Matrix Computation. Johns Hopkins\nPress, 1996.\n[27] M.N. Gibbs, \u201cBayesian Gaussian Processes for Regression and\nClassification,\u201d PhD thesis, Univ. of Cambridge, 1997.\n[28] M. Seeger and M.I. Jordan, \u201cSparse Gaussian Process Classifica-\ntion with Multiple Classes,\u201d Technical Report TR 661, Dept. of\nStatistics, Univ. of California at Berkeley, 2004.\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]\nPhD\n[29] A. Genz, \u201cNumerical Computation of Multivariate Normal\nProbabilities,\u201d J. Computer Graph Statistics, vol. 1, pp. 141-149, 1992.\n[30] W. Chu and Z. Ghahramani, \u201cGaussian Processes for Ordinal\nRegression,\u201d J. Machine Learning Research, vol. 6, pp. 1019-1041,\n2005.\n[31] J. Platt, N. Cristianini, and J. Shawe-Taylor, \u201cLarge Margin DAGs\nfor Multiclass Classification,\u201d Proc. Neural Information Processing\nSystems Conf. (NIPS), pp. 547-553, vol. 12, 2000.\n[32] E. Snelson and Z. Ghahramani, \u201cSparse Parametric Gaussian\nProcesses,\u201d Proc. Neural Information Processing Systems Conf.\n(NIPS), vol. 18, 2005.\n[33] M. Seeger, \u201cBayesian Model Selection for Support Vector\nMachines, Gaussian Processes and Other Kernel Classifiers,\u201d Proc.\nNeural Information Processing Systems Conf. (NIPS), vol. 12, pp. 603-\n609, 2000.\n[34] J. Kwok, \u201cModerating the Outputs of Support Vector Machine\nClassifiers,\u201d IEEE Trans. Neural Networks, vol. 10, no. 5, pp. 1018-\n1031, 1999.\n[35] J. Kwok, \u201cThe Evidence Framework Applied to Support Vector\nMachines,\u201d IEEE Trans. Neural Networks, vol. 11, no. 5, pp. 1162-\n1173, 2000.\n[36] P. Sollich, \u201cBayesian Methods for Support Vector Machines:\nEvidence and Predictive Class Probabilities,\u201d Machine Learning,\nvol. 46, pp. 21-52, 2002.\n[37] W. Chu, \u201cBayesian Approach to Support Vector Machines,\u201d PhD\nthesis, Nat\u2019l Univ. of Singapore, Jan. 2003.\nHyun-Chul Kim received the BS and BEng\ndegress in 1999, the MEng degree in 2001, and\nthe PhD degree in 2005 from from the POST-\nECH (Pohang University of Science and Tech-\nnology). In 2002, he was a visting research\nstudent in the Gatsby Computational Neu-\nroscience Unit, University College London. He\nis now a researcher at the POSTECH Informa-\ntion Technology Laboratories. His current re-\nsearch interests include machine learning,\nBayesian statistics, and pattern recognition. He has recently worked\non Gaussian processes, graphical models, and financial engineering.\nZoubin Ghahramani received the BA and\nBSEng degrees from the University of Pennsyl-\nvania in 1990, and the PhD degree in 1995 from\nMIT, advised by Michael I Jordan. He was a\npostdoctoral fellow in the Artificial Intelligence\nLab at the University of Toronto, working with\nGeoffrey Hinton from 1995-1998. From 1998 to\n2005, he was a faculty member at the Gatsby\nComputational Neuroscience Unit, University\nCollege London. He is now a professor of\ninformation engineering at the University of Cambridge. He also has\nan appointment as an associate research professor in the Machine\nLearning Department at Carnegie Mellon University and is adjunct\nfaculty in the Gatsby Unit, University College London. His current\nresearch interests include Bayesian approaches to machine learning,\nartificial intelligence, statistics, information retrieval, bioinformatics, and\ncomputational motor control. He has recently worked on Gaussian\nprocesses, nonparametric Bayesian methods, clustering, approximate\ninference algorithms, graphical models, Monte Carlo methods, and\nsemisupervised learning. He serves on the editorial boards of the IEEE\nTransactions on Pattern Analysis and Machine Intelligence, Machine\nLearning, the Journal on Machine Learning Research, the Journal on\nArtificial Intelligence Research, and Bayesian Analysis. He is a member\nof the IEEE and the IEEE Computer Society.\n. For more information on this or any other computing topic,\nplease visit our Digital Library at www.computer.org\/publications\/dlib.\nKIM AND GHAHRAMANI: BAYESIAN GAUSSIAN PROCESS CLASSIFICATION WITH THE EM-EP ALGORITHM1959"}],"widgetId":"rgw27_56ab1dc5c8ffa"},"id":"rgw27_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=6690142&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw28_56ab1dc5c8ffa"},"id":"rgw28_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=6690142&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":6690142,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":6690142,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":16334843,"url":"researcher\/16334843_G_Ziegler","fullname":"G Ziegler","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A274432958660608%401442441324113_m"},{"id":39341622,"url":"researcher\/39341622_GR_Ridgway","fullname":"G.R. Ridgway","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":78034,"url":"researcher\/78034_R_Dahnke","fullname":"R Dahnke","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":43392018,"url":"researcher\/43392018_C_Gaser","fullname":"C Gaser","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Apr 2014","journal":"NeuroImage","showEnrichedPublicationItem":false,"citationCount":7,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/261756504_Individualized_Gaussian_process-based_prediction_and_detection_of_local_and_global_gray_matter_abnormalities_in_elderly_subjects","usePlainButton":true,"publicationUid":261756504,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"6.36","url":"publication\/261756504_Individualized_Gaussian_process-based_prediction_and_detection_of_local_and_global_gray_matter_abnormalities_in_elderly_subjects","title":"Individualized Gaussian process-based prediction and detection of local and global gray matter abnormalities in elderly subjects","displayTitleAsLink":true,"authors":[{"id":16334843,"url":"researcher\/16334843_G_Ziegler","fullname":"G Ziegler","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A274432958660608%401442441324113_m"},{"id":39341622,"url":"researcher\/39341622_GR_Ridgway","fullname":"G.R. Ridgway","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":78034,"url":"researcher\/78034_R_Dahnke","fullname":"R Dahnke","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":43392018,"url":"researcher\/43392018_C_Gaser","fullname":"C Gaser","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["NeuroImage 04\/2014; 97(100). DOI:10.1016\/j.neuroimage.2014.04.018"],"abstract":"Structural imaging based on MRI is an integral component of the clinical assessment of patients with potential dementia. We here propose an individualized Gaussian process-based inference scheme for clinical decision support in healthy and pathological aging elderly subjects using MRI. The approach aims at quantitative and transparent support for clinicians who aim to detect structural abnormalities in patients at risk of Alzheimer's disease or other types of dementia. Firstly, we introduce a generative model incorporating our knowledge about normative decline of local and global grey matter volume across the brain in elderly. By supposing smooth structural trajectories the models account for the general course of age-related structural decline as well as late-life accelerated loss. Considering healthy subjects' demography and global brain parameters as informative about normal brain aging variability affords individualized predictions in single cases. Using Gaussian process models as a normative reference, we predict new subjects' brain scans and quantify the local grey matter abnormalities in terms of Normative Probability Maps (NPM) and global z-scores. By integrating the observed expectation error and the predictive uncertainty, the local maps and global scores exploit the advantages of Bayesian inference for clinical decisions and provide a valuable extension of diagnostic information about pathological aging. We validate the approach in simulated data and real MRI data. We train the GP framework using 1238 healthy subjects with ages 18-94 years, and predict in 415 independent test subjects diagnosed as healthy controls, Mild Cognitive Impairment and Alzheimer's disease.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/261756504_Individualized_Gaussian_process-based_prediction_and_detection_of_local_and_global_gray_matter_abnormalities_in_elderly_subjects","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Gerard_Ridgway\/publication\/261756504_Individualized_Gaussian_process-based_prediction_and_detection_of_local_and_global_gray_matter_abnormalities_in_elderly_subjects\/links\/00b7d53a3f476c93b0000000.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Gerard_Ridgway","sourceName":"Gerard R Ridgway","hasSourceUrl":true},"publicationUid":261756504,"publicationUrl":"publication\/261756504_Individualized_Gaussian_process-based_prediction_and_detection_of_local_and_global_gray_matter_abnormalities_in_elderly_subjects","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/261756504_Individualized_Gaussian_process-based_prediction_and_detection_of_local_and_global_gray_matter_abnormalities_in_elderly_subjects\/links\/00b7d53a3f476c93b0000000\/smallpreview.png","linkId":"00b7d53a3f476c93b0000000","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=261756504&reference=00b7d53a3f476c93b0000000&eventCode=&origin=publication_list","widgetId":"rgw32_56ab1dc5c8ffa"},"id":"rgw32_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=261756504&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"00b7d53a3f476c93b0000000","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":6690142,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/261756504_Individualized_Gaussian_process-based_prediction_and_detection_of_local_and_global_gray_matter_abnormalities_in_elderly_subjects\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["global volume differences (Peelle et al., 2012). Gaussian process (GP) models have emerged as a flexible and elegant approach for prediction of continuous, i.e. y \u2208 \u211d, or binary, i.e. y \u2208 [0] [1] variables (Kim and Ghahramani, 2006; Rasmussen, 1996; Rasmussen and Williams, 2006). Recently,GPs were successfully introduced to the neuroimaging community. "],"widgetId":"rgw33_56ab1dc5c8ffa"},"id":"rgw33_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw31_56ab1dc5c8ffa"},"id":"rgw31_56ab1dc5c8ffa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=261756504&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":5797648,"url":"researcher\/5797648_Amir_F_Atiya","fullname":"Amir F. Atiya","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272269612154898%401441925542199_m\/Amir_Atiya.png"},{"id":29941442,"url":"researcher\/29941442_Hatem_A_Fayed","fullname":"Hatem A. Fayed","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":28281611,"url":"researcher\/28281611_Ahmed_H_Abdel-Gawad","fullname":"Ahmed H. Abdel-Gawad","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Feb 2013","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/235738119_A_New_Monte_Carlo_Based_Algorithm_for_the_Gaussian_Process_Classification_Problem","usePlainButton":true,"publicationUid":235738119,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/235738119_A_New_Monte_Carlo_Based_Algorithm_for_the_Gaussian_Process_Classification_Problem","title":"A New Monte Carlo Based Algorithm for the Gaussian Process Classification Problem","displayTitleAsLink":true,"authors":[{"id":5797648,"url":"researcher\/5797648_Amir_F_Atiya","fullname":"Amir F. Atiya","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272269612154898%401441925542199_m\/Amir_Atiya.png"},{"id":29941442,"url":"researcher\/29941442_Hatem_A_Fayed","fullname":"Hatem A. Fayed","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":28281611,"url":"researcher\/28281611_Ahmed_H_Abdel-Gawad","fullname":"Ahmed H. Abdel-Gawad","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Gaussian process is a very promising novel technology that has been applied\nto both the regression problem and the classification problem. While for the\nregression problem it yields simple exact solutions, this is not the case for\nthe classification problem, because we encounter intractable integrals. In this\npaper we develop a new derivation that transforms the problem into that of\nevaluating the ratio of multivariate Gaussian orthant integrals. Moreover, we\ndevelop a new Monte Carlo procedure that evaluates these integrals. It is based\non some aspects of bootstrap sampling and acceptancerejection. The proposed\napproach has beneficial properties compared to the existing Markov Chain Monte\nCarlo approach, such as simplicity, reliability, and speed.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/235738119_A_New_Monte_Carlo_Based_Algorithm_for_the_Gaussian_Process_Classification_Problem","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Amir_Atiya\/publication\/235738119_A_New_Monte_Carlo_Based_Algorithm_for_the_Gaussian_Process_Classification_Problem\/links\/00b4951d65681e072c000000.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Amir_Atiya","sourceName":"Amir Atiya","hasSourceUrl":true},"publicationUid":235738119,"publicationUrl":"publication\/235738119_A_New_Monte_Carlo_Based_Algorithm_for_the_Gaussian_Process_Classification_Problem","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/235738119_A_New_Monte_Carlo_Based_Algorithm_for_the_Gaussian_Process_Classification_Problem\/links\/00b4951d65681e072c000000\/smallpreview.png","linkId":"00b4951d65681e072c000000","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=235738119&reference=00b4951d65681e072c000000&eventCode=&origin=publication_list","widgetId":"rgw35_56ab1dc5c8ffa"},"id":"rgw35_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=235738119&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"00b4951d65681e072c000000","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":6690142,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/235738119_A_New_Monte_Carlo_Based_Algorithm_for_the_Gaussian_Process_Classification_Problem\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Among the wellknown proposed methods from the first category are the Laplace's approximation (Williams and Barber [44]) and the expectation propagation (Minka [30]). Also, some other efficient approximation-based methods include the work of Csat\u00f3 et al [6], Opper and Winther [35]), Gibbs and MacKay [15], Rifkin and Klautau [39], Jaakkola and Haussler [22], and Kim, and Ghahramani [26]. Work on the second category has been more scarce. "],"widgetId":"rgw36_56ab1dc5c8ffa"},"id":"rgw36_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw34_56ab1dc5c8ffa"},"id":"rgw34_56ab1dc5c8ffa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=235738119&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":80672769,"url":"researcher\/80672769_Jaakko_Riihimaeki","fullname":"Jaakko Riihim\u00e4ki","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":84345684,"url":"researcher\/84345684_Pasi_Jylaenki","fullname":"Pasi Jyl\u00e4nki","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":296975,"url":"researcher\/296975_Aki_Vehtari","fullname":"Aki Vehtari","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Jan 2013","journal":"Journal of Machine Learning Research","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/262286080_Nested_expectation_propagation_for_Gaussian_process_classification","usePlainButton":true,"publicationUid":262286080,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.47","url":"publication\/262286080_Nested_expectation_propagation_for_Gaussian_process_classification","title":"Nested expectation propagation for Gaussian process classification","displayTitleAsLink":true,"authors":[{"id":80672769,"url":"researcher\/80672769_Jaakko_Riihimaeki","fullname":"Jaakko Riihim\u00e4ki","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":84345684,"url":"researcher\/84345684_Pasi_Jylaenki","fullname":"Pasi Jyl\u00e4nki","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":296975,"url":"researcher\/296975_Aki_Vehtari","fullname":"Aki Vehtari","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of Machine Learning Research 01\/2013; 14(1):75-109."],"abstract":"This paper considers probabilistic multinomial probit classification using Gaussian process (GP) priors. Challenges with multiclass GP classification are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classification rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all between-class posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classification accuracy the differences between all the methods were small from a practical point of view.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/262286080_Nested_expectation_propagation_for_Gaussian_process_classification","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Aki_Vehtari\/publication\/262286080_Nested_expectation_propagation_for_Gaussian_process_classification\/links\/5507fe3c0cf27e990e08b19e.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Aki_Vehtari","sourceName":"Aki Vehtari","hasSourceUrl":true},"publicationUid":262286080,"publicationUrl":"publication\/262286080_Nested_expectation_propagation_for_Gaussian_process_classification","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/262286080_Nested_expectation_propagation_for_Gaussian_process_classification\/links\/5507fe3c0cf27e990e08b19e\/smallpreview.png","linkId":"5507fe3c0cf27e990e08b19e","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=262286080&reference=5507fe3c0cf27e990e08b19e&eventCode=&origin=publication_list","widgetId":"rgw38_56ab1dc5c8ffa"},"id":"rgw38_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=262286080&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"5507fe3c0cf27e990e08b19e","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":6690142,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/262286080_Nested_expectation_propagation_for_Gaussian_process_classification\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["This property can be used to transform the inference onto an equivalent non-redundant model which includes n(c \u2212 1) unknown latent values with a Gaussian prior and a likelihood consisting of n(c \u2212 1) factorizing terms. It follows that standard EP methodology for binary GP classification (Rasmussen and Williams, 2006) can be applied for posterior inference but a straightforward implementation results in a posterior representation scaling as O((c \u2212 1) 3 n 3 ) and means to improve the scaling are not discussed by Kim and Ghahramani (2006). Contrary to the usual EP approach of maximizing the marginal likelihood approximation , Kim and Ghahramani (2006) determined the hyperparameters by maximizing a lower bound on the log marginal likelihood in a similar way as is done in the expectation maximization (EM) algorithm. "],"widgetId":"rgw39_56ab1dc5c8ffa"},"id":"rgw39_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw37_56ab1dc5c8ffa"},"id":"rgw37_56ab1dc5c8ffa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=262286080&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":6690142,"publicationLink":"publication\/6690142_Bayesian_Gaussian_Process_Classification_with_the_EM-EP_Algorithm","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw30_56ab1dc5c8ffa"},"id":"rgw30_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=6690142&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=60","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":60,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw29_56ab1dc5c8ffa"},"id":"rgw29_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=6690142&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/6690142_Bayesian_Gaussian_Process_Classification_with_the_EM-EP_Algorithm","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1dc5c8ffa"},"id":"rgw2_56ab1dc5c8ffa","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":6690142},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=6690142&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1dc5c8ffa"},"id":"rgw1_56ab1dc5c8ffa","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"JSEwHQZSHr6k4FCiDt2C39Nvz+rAvdDIShrgfo968QfgUU12NJuFa5VVEMmq8qz+hC1n0Q3HngoNaZhCDTIiItpB2\/upGm01TME1qmuF3eq\/febihN9gkeBFcqnlsxQ4lSluSmlA\/TrgCsppcdze0I9qHsssggIBXnJ7sCf+HTHzuM0H0fwXJQSP6DhgH4s5Utwez2OR65jaT0bk1+ZO+efO0xElPQKz4Ck6vxpRIU7s07wUG988zHxHhT23q8ImQP\/cfxZY62Ci3Zk8MVEjn\/DygwA844iNl6jb0haqajE=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/6690142_Bayesian_Gaussian_Process_Classification_with_the_EM-EP_Algorithm\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Bayesian Gaussian Process Classification with the EM-EP Algorithm\" \/>\n<meta property=\"og:description\" content=\"Gaussian process classifiers (GPCs) are Bayesian probabilistic kernel classifiers. In GPCs, the probability of belonging to a certain class at an input location is monotonically related to the...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/6690142_Bayesian_Gaussian_Process_Classification_with_the_EM-EP_Algorithm\/links\/0e5fb5a5f0c41c4932e99405\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/6690142_Bayesian_Gaussian_Process_Classification_with_the_EM-EP_Algorithm\" \/>\n<meta property=\"rg:id\" content=\"PB:6690142\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1109\/TPAMI.2006.238\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Bayesian Gaussian Process Classification with the EM-EP Algorithm\" \/>\n<meta name=\"citation_author\" content=\"Hyun-Chul Kim\" \/>\n<meta name=\"citation_author\" content=\"Zoubin Ghahramani\" \/>\n<meta name=\"citation_pmid\" content=\"17108369\" \/>\n<meta name=\"citation_publication_date\" content=\"2007\/01\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"IEEE Transactions on Pattern Analysis and Machine Intelligence\" \/>\n<meta name=\"citation_issn\" content=\"0162-8828\" \/>\n<meta name=\"citation_volume\" content=\"28\" \/>\n<meta name=\"citation_issue\" content=\"12\" \/>\n<meta name=\"citation_firstpage\" content=\"1948\" \/>\n<meta name=\"citation_lastpage\" content=\"59\" \/>\n<meta name=\"citation_doi\" content=\"10.1109\/TPAMI.2006.238\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/6690142_Bayesian_Gaussian_Process_Classification_with_the_EM-EP_Algorithm\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/6690142_Bayesian_Gaussian_Process_Classification_with_the_EM-EP_Algorithm\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-ac28c13a-a67f-4225-9464-51b21ced042d","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":1190,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw40_56ab1dc5c8ffa"},"id":"rgw40_56ab1dc5c8ffa","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-ac28c13a-a67f-4225-9464-51b21ced042d", "01eac8c69f48f6df381c59b73e3411b5fdb323f4");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-ac28c13a-a67f-4225-9464-51b21ced042d", "01eac8c69f48f6df381c59b73e3411b5fdb323f4");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw41_56ab1dc5c8ffa"},"id":"rgw41_56ab1dc5c8ffa","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/6690142_Bayesian_Gaussian_Process_Classification_with_the_EM-EP_Algorithm","requestToken":"FeiseMWHVpVqUENLkJ1pP\/lbYXd7jAiroUbs0H2htZz7HwtbC3hPQ3zqKDdLrX8oHhMkMp0rl0FFs3y4F8lR0X+XfOLyfSYuqnn8VsVasu96ohg66x\/5SDIV3Uf10I8KnfHrSQqLNv5hL5h+cjmYLS7zXneqb+F+3gIizvsKmh3rI6vdaceWxoPHprk2BleU8oAmVTuFigi6iMH8OSTuiCXB2xz7MOcAA2+wIMQyYiZpQJ0zF1KE8Wxy+i1GibzW41E6sh3h5OwdHsmf51iiVVbQWojbHHKyGVYmMrmiR6k=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=BFX8w1uhYuIl1dp5NJOLYk-qwsQrIlZDRW9oEQZLi6JFab4NxkOLPBhjj2_gkWaB","encodedUrlAfterLogin":"cHVibGljYXRpb24vNjY5MDE0Ml9CYXllc2lhbl9HYXVzc2lhbl9Qcm9jZXNzX0NsYXNzaWZpY2F0aW9uX3dpdGhfdGhlX0VNLUVQX0FsZ29yaXRobQ%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw43_56ab1dc5c8ffa"},"id":"rgw43_56ab1dc5c8ffa","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw42_56ab1dc5c8ffa"},"id":"rgw42_56ab1dc5c8ffa","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw44_56ab1dc5c8ffa"},"id":"rgw44_56ab1dc5c8ffa","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
