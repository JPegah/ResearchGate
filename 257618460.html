<!DOCTYPE html> <html lang="en" class="" id="rgw49_56ab1d7fd7391"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="eFn4oM48HkElv+fbtFPu29an7KTmnk3bfPhIiOWYT9cpUjCsehOtz8RoCVzLLC/uaMvMsp6vbxPkZlNUKLbwGGAmk6lRHQI4B+mcrY1scNKDWV7qn3IbdGQ/cXyx0AfNc6l4TBl7XGRjOIP6b9Fb7GAlVcW1qA7sntOktCx60Z1OggdeS9B4KMTbVPzFQVsGku/9VffHIbs48nrfF+ebZc3Msfc4YLGwMvYW7bdGOEkFoXrVmuJwLRuYrz/2La1KVWPCPrXndpjY7Kqtd4bs4jjje68wklFMJlgW5SqdL6A="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-80fde15b-4298-450a-b331-d6f7ba289b01",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="A comparative evaluation of stochastic-based inference methods for Gaussian process models" />
<meta property="og:description" content="Gaussian Process (GP) models are extensively used in data analysis given their flexible modeling capabilities and interpretability. The fully Bayesian treatment of GP models is analytically..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models/links/00463533941d28ab5b000000/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models" />
<meta property="rg:id" content="PB:257618460" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1007/s10994-013-5388-x" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="A comparative evaluation of stochastic-based inference methods for Gaussian process models" />
<meta name="citation_author" content="M. Filippone" />
<meta name="citation_author" content="M. Zhong" />
<meta name="citation_author" content="M. Girolami" />
<meta name="citation_publication_date" content="2013/10/01" />
<meta name="citation_journal_title" content="Machine Learning" />
<meta name="citation_issn" content="0885-6125" />
<meta name="citation_volume" content="93" />
<meta name="citation_issue" content="1" />
<meta name="citation_doi" content="10.1007/s10994-013-5388-x" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Maurizio_Filippone/publication/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models/links/00463533941d28ab5b000000.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>A comparative evaluation of stochastic-based inference methods for Gaussian process models (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: A comparative evaluation of stochastic-based inference methods for Gaussian process models on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1d7fd7391" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1d7fd7391" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab1d7fd7391">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1007%2Fs10994-013-5388-x&rft.atitle=A%20comparative%20evaluation%20of%20stochastic-based%20inference%20methods%20for%20Gaussian%20process%20models&rft.title=Machine%20Learning&rft.jtitle=Machine%20Learning&rft.volume=93&rft.issue=1&rft.date=2013&rft.issn=0885-6125&rft.au=M.%20Filippone%2CM.%20Zhong%2CM.%20Girolami&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">A comparative evaluation of stochastic-based inference methods for Gaussian process models</h1> <meta itemprop="headline" content="A comparative evaluation of stochastic-based inference methods for Gaussian process models">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models/links/00463533941d28ab5b000000/smallpreview.png">  <div id="rgw8_56ab1d7fd7391" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56ab1d7fd7391" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Maurizio_Filippone" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A273666076311560%401442258485613_m" title="Maurizio Filippone" alt="Maurizio Filippone" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Maurizio Filippone</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw10_56ab1d7fd7391" data-account-key="Maurizio_Filippone">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Maurizio_Filippone"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A273666076311560%401442258485613_l" title="Maurizio Filippone" alt="Maurizio Filippone" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Maurizio_Filippone" class="display-name">Maurizio Filippone</a>    </h5> <div class="truncate-single-line meta">   </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56ab1d7fd7391" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Mingjun_Zhong" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272395789402145%401441955626037_m" title="Mingjun Zhong" alt="Mingjun Zhong" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Mingjun Zhong</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw12_56ab1d7fd7391" data-account-key="Mingjun_Zhong">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Mingjun_Zhong"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272395789402145%401441955626037_l" title="Mingjun Zhong" alt="Mingjun Zhong" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Mingjun_Zhong" class="display-name">Mingjun Zhong</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Dalian_University_of_Technology" title="Dalian University of Technology">Dalian University of Technology</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw13_56ab1d7fd7391" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Mark_Girolami" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Mark Girolami" alt="Mark Girolami" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Mark Girolami</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw14_56ab1d7fd7391" data-account-key="Mark_Girolami">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Mark_Girolami"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Mark Girolami" alt="Mark Girolami" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Mark_Girolami" class="display-name">Mark Girolami</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_College_London" title="University College London">University College London</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/0885-6125_Machine_Learning"><span itemprop="name">Machine Learning</span></a> </span>    (Impact Factor: 1.89).     <meta itemprop="datePublished" content="2013-10">  10/2013;  93(1).    DOI:&nbsp;10.1007/s10994-013-5388-x           </div> <div id="rgw15_56ab1d7fd7391" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>Gaussian Process (GP) models are extensively used in data analysis given their flexible modeling capabilities and interpretability. The fully Bayesian treatment of GP models is analytically intractable, and therefore it is necessary to resort to either deterministic or stochastic approximations. This paper focuses on stochastic-based inference techniques. After discussing the challenges associated with the fully Bayesian treatment of GP models, a number of inference strategies based on Markov chain Monte Carlo methods are presented and rigorously assessed. In particular, strategies based on efficient parameterizations and efficient proposal mechanisms are extensively compared on simulated and real data on the basis of convergence speed, sampling efficiency, and computational cost.</div> </p>  </div>  </div>   </div>      <div class="action-container"> <div id="rgw16_56ab1d7fd7391" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw29_56ab1d7fd7391">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw41_56ab1d7fd7391">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Maurizio_Filippone/publication/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models/links/00463533941d28ab5b000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Maurizio_Filippone">Maurizio Filippone</a>, <span class="js-publication-date"> Mar 31, 2014 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw43_56ab1d7fd7391" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw44_56ab1d7fd7391" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw45_56ab1d7fd7391" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw46_56ab1d7fd7391" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw47_56ab1d7fd7391" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw48_56ab1d7fd7391" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw42_56ab1d7fd7391" src="https://www.researchgate.net/c/o1o9o3/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMaurizio_Filippone%2Fpublication%2F257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models%2Flinks%2F00463533941d28ab5b000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw28_56ab1d7fd7391"  itemprop="articleBody">  <p>Page 1</p> <p>Noname manuscript No.<br />(will be inserted by the editor)<br />A Comparative Evaluation of Stochastic-based<br />Inference Methods for Gaussian Process Models<br />M. Filippone · M. Zhong · M. Girolami<br />Received: date / Accepted: date<br />Abstract Gaussian Process models are extensively used in data analysis given<br />their flexible modeling capabilities and interpretability. The fully Bayesian treat-<br />ment of GP models is analytically intractable, and therefore it is necessary to<br />resort to either deterministic or stochastic approximations. This paper focuses on<br />stochastic-based inference techniques. First, challenges associated with the fully<br />Bayesian treatment of GP models are discussed, and then a number of inference<br />strategies based on Markov chain Monte Carlo methods are presented and rigor-<br />ously assessed. In particular, strategies based on efficient parameterizations and<br />efficient proposal mechanisms are extensively compared on simulated and real data<br />on the basis of speed of convergence, sampling efficiency, and computational cost.<br />Keywords Bayesian inference · Gaussian Processes · Markov chain Monte Carlo ·<br />hierarchical models · latent variable models<br />1 Introduction<br />Gaussian Process (GP) models represent a class of models that is fairly popular in<br />data analysis due to the associated flexibility and interpretability. Both those fea-<br />tures are a direct consequence of their rich parameterization. Flexibility is due to<br />the nonparametric prior over latent variables conditioning observations, whereas<br />interpretability is due to the parameterization of the structure associated with the<br />latent variables. Observations are conditionally independent given a set of jointly<br />Maurizio Filippone<br />School of Computing Science, University of Glasgow, United Kingdom.<br />E-mail: maurizio.filippone@glasgow.ac.uk<br />Mingjun Zhong<br />Department of Biomedical Engineering, Dalian University of Technology, P.R. China<br />E-mail: mingjun.zhong@gmail.com<br />Mark Girolami<br />Department of Statistical Science, University College London, United Kingdom.<br />E-mail: girolami@stats.ucl.ac.uk</p>  <p>Page 2</p> <p>2 M. Filippone et al.<br />Gaussian latent variables, and are assumed to be distributed according to the par-<br />ticular type of data being modeled. The covariance structure of the latent variables<br />is then parameterized by a set of (hyper)-parameters that characterizes the covari-<br />ance of the input vectors in terms of length-scales and intensity of interaction. GP<br />models comprise a large set of models, and this paper focuses in particular on<br />Logistic Regression with GP priors (LRG) (Rasmussen and Williams, 2006), Log-<br />Gaussian Cox model (LCX) (Møller et al., 1998), Stochastic Volatility model with<br />GP priors (VLT) (Wilson and Ghahramani, 2010), and Ordinal Regression with<br />GP priors (ORD) (Chu and Ghahramani, 2005).<br />Exact inference in GP models is analytically intractable. Most of the work to<br />tackle such an intractability focuses on deterministic approximations to integrate<br />out latent variables; those approaches include the Laplace Approximation (LA)<br />(Tierney and Kadane, 1986), Expectation Propagation (EP) (Minka, 2001), and<br />mean field approximations (Opper and Winther, 2000) (see, e.g., Rasmussen and<br />Williams (2006) for an extensive presentation of those approximations and Kuss<br />and Rasmussen (2005) for their assessment on LRG models). Those approximations<br />provide a computationally tractable way to integrate out latent variables, but it<br />is not possible to quantify the error that those approximations introduce in the<br />quantification of uncertainty in predictions (although EP for LRG is reported to<br />be very accurate in Kuss and Rasmussen (2005)); also, those methods target the<br />integration of latent variables only.<br />In the direction of providing a fully Bayesian treatment of GP models, it is<br />necessary to integrate out latent variables as well as hyper-parameters, and this is<br />usually done by quadrature methods (Cseke and Heskes, 2011; Rue et al., 2009),<br />thus limiting the number of hyper-parameters that can be employed in GP models.<br />Based on those considerations, this paper focuses on non-deterministic meth-<br />ods to carry out inference in GP models, and in particular on stochastic based<br />approximations based Markov Chain Monte Carlo (MCMC) methods. The use of<br />MCMC based inference methods is appealing as it provides asymptotic guarantees<br />of convergence to exact inference. In practice, this translates into the possibility<br />of achieving results with the desired level of accuracy (Flegal et al., 2007). Un-<br />fortunately, the use of MCMC methods for inference in GP models is extremely<br />difficult; the aim of this paper is to discuss the challenges associated with MCMC<br />based inference for GP models, and compare a number of strategies that have been<br />proposed in the literature to tackle them. A preliminary version of this work can<br />be found in Filippone et al. (2012)1.<br />To the best of our knowledge, this work (i) is the first attempt to extensively<br />assess the state-of-the-art in stochastic-based inference methods for GP models,<br />and (ii) sets the bar for new MCMC methods for inference in GP models. Along<br />with those contribution, this paper presents (iii) a variant of the Hybrid Monte<br />Carlo algorithm that outperforms state-of-the-art methods to sample from the<br />posterior distribution of the latent variables, and (iv) tests the combination of<br />parameterizations, as recently proposed in Yu and Meng (2011), in the case of GP<br />models.<br />1An implementation of the methods considered in this paper can be found at:<br />http://www.dcs.gla.ac.uk/~maurizio/pages/code.html</p>  <p>Page 3</p> <p>Title Suppressed Due to Excessive Length3<br />1.1 Gaussian Process Models<br />Let X = {x1,...,xn} be a set of n input vectors described by a set of d covariates<br />xi∈ Rd, associated with observed responses y = {y1,...,yn}. In GP models, the<br />generative process modeling the observed data y given X is as follows. Obser-<br />vations are assumed conditionally independent given a set of n latent variables<br />f = {f1,...,fn}, and distributed according to a certain distribution depending on<br />the particular type of data, e.g., Bernoulli for binary labels and Poisson for obser-<br />vations in the form of counts. This can be translated into a likelihood function of<br />the form p(y|f) =?n<br />In this work, latent variables are assumed to be drawn from a zero mean GP<br />prior with covariance function k. The GP prior is a prior over functions, and the<br />covariance structure given by k specifies the characteristics of such functions (i.e.,<br />degree of smoothness and marginal variance). Let k be parameterized by a vector<br />of (hyper)-parameters θ = (σ,ψτ1,...,ψτd), and assume:<br />i=1p(yi|fi), where for generality the distribution p(yi|fi) is<br />left unspecified.<br />k(xi,xj|θ) = σq(xi,xj|ψτ) = σ exp<br />?<br />−1<br />2<br />d<br />?<br />r=1<br />(xi− xj)2<br />exp(ψτr)2<br />(r)<br />?<br />(1)<br />with exp(ψτr) defining the length-scale of the interaction between the input vectors<br />for the rth covariate and σ giving the marginal variance for latent variables. This<br />type of covariance can be used for Automatic Relevance Determination (ARD)<br />(Mackay, 1994) of the covariates, as the values τi = exp(ψτi) can be interpreted<br />as length-scale parameters. This definition of covariance function is adopted in<br />many applications and is the one we will consider in the remainder of this paper.<br />Exponentiation of the hyper-parameters is convenient, so that standard MCMC<br />transition operators can be employed for ψτithus avoiding dealing with boundary<br />conditions or non-standard MCMC proposals (Robert and Casella, 2005). Let<br />Q be the matrix whose entries are qij = q(xi,xj|ψτ); the covariance matrix K<br />will then be K = σQ. The model is fully specified by choosing a prior p(θ) for<br />the hyper-parameters. The model structure is therefore hierarchical, with hyper-<br />parameters conditioning the latent variables that, in turn, condition observations,<br />so that p(y,f,θ) = p(y|f)p(f|θ)p(θ).<br />In a Bayesian setting, the predictive distribution for new input values x∗ can<br />be written in the following way (for the sake of clarity we drop the explicit condi-<br />tioning on X and x∗):<br />? ? ?<br />The left hand side of Eq. 2 is a full probability distribution characterizing the<br />uncertainty in predicting y∗ given the GP modeling assumption.<br />In this work we will focus on stochastic approximations for obtaining samples<br />from the posterior distribution of f and θ, so that we can obtain a Monte Carlo<br />estimate of the predictive distribution as follows:<br />p(y∗|y) =p(y∗|f∗)p(f∗|f,θ)p(f,θ|y)df∗dfdθ(2)<br />p(y∗|y) ?1<br />N<br />N<br />?<br />i=1<br />?<br />p(y∗|f∗)p(f∗|f(i),θ(i))df∗<br />(3)</p>  <p>Page 4</p> <p>4 M. Filippone et al.<br />where N denotes the number of samples used to compute the estimate. In Eq. 3<br />we denoted the ith samples from the posterior distribution of f and θ obtained<br />by means of MCMC methods by f(i)and θ(i). Note that the remaining integral is<br />univariate and it is generally easy to evaluate.<br />1.2 Challenges in MCMC based inference for GP models<br />Sampling from the posterior of latent variables and hyper-parameters by joint<br />proposals is not feasible; it is extremely unlikely to propose a set of latent variables<br />and hyper-parameters that are compatible with each other and observed data. This<br />forces one to consider Gibbs sampling types of schemes, where groups of variables<br />are updated one at time, leading to the following challenges:<br />(i) Due to the hierarchical structure of GP models, chains converge slowly and<br />mix poorly if the coupling effect between the groups of variables is not dealt with<br />properly. This requires some form of reparameterization or clever proposal mech-<br />anism that efficiently decouples the dependencies between the groups of variables.<br />This effect has drawn a lot of attention in the case of hierarchical models in gen-<br />eral (Yu and Meng, 2011), and recently in GP models Knorr-Held and Rue (2002);<br />Murray and Adams (2010). In Knorr-Held and Rue (2002) a joint update of latent<br />variables and hyper-parameters is proposed with the aim of avoiding proposals for<br />hyper-parameters to be conditioned on the values of latent variables. In Murray<br />and Adams (2010) a parameterization based on auxiliary data is proposed that<br />aims at reducing the coupling between the two groups of variables. Other ideas<br />involve the use of reparameterizations based on whitening the latent variables;<br />in the terminology of Yu and Meng (2011), this corresponds to employing the so<br />called Ancillary Augmentation (AA) parameterization. Recently, Yu and Meng<br />(2011) proposed to interweave parameterizations characterized by complementary<br />features in order to boost sampling efficiency. Parameterizations can be comple-<br />mentary in the sense that they offer better performances in either strong or weak<br />data limits; the idea of combining parameterizations is to achieve high sampling<br />efficiency in both strong and weak data scenarios. We are interested in comparing<br />the methods in Knorr-Held and Rue (2002); Murray and Adams (2010) and Yu<br />and Meng (2011) applied to GP models. Another possibility would be to approxi-<br />mately integrate out latent variables and obtain samples from the corresponding<br />approximate posterior of hyper-parameters. For GP classification this might be<br />a sensible thing to do, as the Expectation Propagation approximation has been<br />reported to be very accurate; however, this is peculiar to GP classification and for<br />general GP models it may not be the case.<br />(ii) Sampling hyper-parameters and latent variables cannot be done using ex-<br />act Gibbs steps, and it requires proposals that are accepted/rejected based on a<br />Hastings ratio, leading to a waste of expensive computations. Transition operators<br />characterized by acceptance mechanisms embedded in a Gibbs sampler, are usu-<br />ally referred to as Metropolis-within-Gibbs operators. Designing proposals that<br />guarantee high acceptance and independence between samples is extremely chal-<br />lenging, especially because latent variables can have dimensions in the order of<br />hundreds or thousands. We will compare several transition operators, for different<br />steps of the Gibbs sampler, with the aim of gaining insights about ways to strike a<br />good balance between efficiency and computational cost. We will consider transi-</p>  <p>Page 5</p> <p>Title Suppressed Due to Excessive Length5<br />tion operators characterized by proposal mechanisms with increasing complexity,<br />and in particular the Metropolis-Hastings (MH) operator which is based on ran-<br />dom walk types of proposals, Hybrid Monte Carlo (HMC) which uses the gradient<br />of the log-density of interest, and manifold methods (Girolami and Calderhead,<br />2011) which use curvature information (i.e., second derivatives of the log-density).<br />The paper is organized as follows: Sections 2 and 3 report the parameterization<br />strategies and the transition operators considered in this work. Sections 4 and 5<br />report an extensive comparison of those strategies and transition operators, on<br />simulated and real data, on the basis of efficiency, speed of convergence and com-<br />putational complexity; section 6 concludes the paper. For the sake of readability,<br />most of the technical derivations can be found in the appendices.<br />2 Dealing with the hierarchical structure of GP models<br />2.1 Sufficient and Ancillary Augmentation<br />From a generative perspective, the model structure is hierarchical with latent<br />variables representing a sufficient statistics for the hyper-parameters. This param-<br />eterization is referred to as Sufficient Augmentation (SA) in Yu and Meng (2011)<br />and allows one to express the joint density as<br />SAp(y,f,θ) = p(y|f)p(f|θ)p(θ)<br />It is also possible to introduce the decomposition of the matrix Q into the<br />product of two factors LLT, and view the generation of the latent variables as<br />f =√σLν with ν ∼ N(ν|0,I), which implies that f ∼ N(f|0,K) indeed. In the<br />remainder of this paper, we will consider L to be the lower triangular Cholesky<br />decomposition of K, but in principle any square root of K could be used. In this<br />way, ν is ancillary for θ and it is possible to express the joint density as<br />AAp(y,ν,θ) = p(y|ν,θ)p(ν)p(θ)<br />This parameterization is called Ancillary Augmentation (AA) in the terminology<br />of Yu and Meng (2011). In Murray and Adams (2010) SA and AA are referred to<br />unwhitened and whitened parameterizations respectively. Weak and strong data<br />limits can influence the efficiency in sampling using either parameterization. For<br />this reason, it is important to choose an efficient parameterizations for the particu-<br />lar problem under study and for the available amount of data, as both those aspects<br />can dramatically influence efficiency and speed of convergence of the chains.<br />2.2 Ancillarity-Sufficiency Interweaving Strategy - ASIS<br />In this section we briefly review the main results presented in Yu and Meng (2011)<br />on the combination of parameterizations to improve convergence and efficiency of<br />MCMC methods, and we will illustrate how these results can be applied to GP<br />models. Intuitively, combining parameterizations seems promising to take the best<br />from them in both weak and strong data limits, or at least, to avoid the possibility<br />that chains do not converge because of the wrong choice of parameterization.</p>  <p>Page 6</p> <p>6 M. Filippone et al.<br />Alternating the sampling in the SA and AA parameterizations is the most obvious<br />way of combining the two parameterizations, but as recently investigated in Yu<br />and Meng (2011), interweaving SA and AA is actually a more promising way<br />forward. From the theoretical perspective, the geometric rate of convergence r of<br />the scheme when the parameterizations are interweaved, is related to the rates of<br />the two schemes r1and r2by r ≤ R1,2√r1r2, where R1,2is the maximal correlation<br />between the latent variables for the two schemes. Given that the former expression<br />implies r ≤ max(r1,r2), combining two parameterizations leads to a scheme that<br />is better than the worst. This is already an advantage compared to using a single<br />scheme when one is in doubt on which scheme to use. However, the key result<br />is the fact that R1,2 can be very small depending on the two parameterizations,<br />so it is possible to make the combined scheme converge quickly even if neither<br />of the individual schemes do. In general, this result is quite remarkable, as once<br />different reparameterizations are available, combining them using the interweaving<br />strategy is simple to implement, and can dramatically boost sampling efficiency.<br />In GP models, the ASIS scheme amounts to interweaving SA and AA updates,<br />that following Yu and Meng (2011) yields:<br />f|y,θ<br />−→<br />θ|f<br />−→<br />ν = σ−1/2L−1f<br />−→<br />θ|y,ν<br />2.3 Knorr-Held and Rue (KHR)<br />The idea underpinning KHR, is to jointly sample parameters and latent variables<br />as follows. First, a set of hyper-parameters θ?|θ is proposed and then a set of<br />latent variables conditioned on the new set of hyper-parameters, namely f?|y,θ?,<br />is proposed. The proposal (θ?,f?) is then jointly accepted or rejected according to a<br />standard Hastings ratio. The key idea is to avoid making the proposal θ?accepted<br />on the basis of f to avoid the strong coupling effect due to the hierarchical nature<br />of the model. KHR was proposed in applications making use of Gaussian Markov<br />Random Fields, and we will discuss the application of this idea for GP models<br />in the section reporting the experiments. In order to avoid difficulties in devising<br />a proposal for sampling from f?|y,θ?), here we set the proposal as the Gaussian<br />obtained by constructing a Laplace approximation to p(f|y,θ?).<br />2.4 Surrogate Method (SURR)<br />In the SURR method (Murray and Adams, 2010), a set of auxiliary latent vari-<br />ables g is introduced as a noisy version of f; in particular, p(g|f,θ) = N(g|f,Sθ).<br />This construction yields a conditional distribution for f of the form p(f|g,θ) =<br />N(f|m,R), with R = Sθ−Sθ(Sθ+K)−1Sθand m = RS−1<br />R = DDT, the sampling of θ is then conditioned on the variables η defined as<br />f = Dη + m. The covariance Sθ is constructed to be diagonal with elements ob-<br />tained by matching the posterior for each latent variable individually or by Taylor<br />approximations (see Murray and Adams (2010) for details).<br />θg. After decomposing</p>  <p>Page 7</p> <p>Title Suppressed Due to Excessive Length7<br />3 MCMC transition operators considered in this work<br />This section presents the transition operators considered in this work. We are in-<br />terested in understanding whether and to what extent employing proposal mech-<br />anisms making use of gradient or curvature information of the target density im-<br />proves sampling efficiency and speed of convergence with respect to computational<br />complexity. We therefore consider transition operators with increasing complexity,<br />and in particular the Metropolis-Hastings (MH) operator which is based on ran-<br />dom walk types of proposals, the Hybrid Monte Carlo (HMC) operator which uses<br />gradient information, and the Simplified Manifold Metropolis Adjusted Langevin<br />Algorithm (SMMALA) operator which is one of the simplest manifold MCMC<br />methods proposed in Girolami and Calderhead (2011) using curvature informa-<br />tion.<br />For the sake of clarity, we will focus on the transitions operators for f, but the<br />same operators can be easily applied to θ. We will first present MH, HMC, and<br />SMMALA, and we will then discuss Elliptical Slice Sampling and a few variants<br />of MH and HMC that have been specifically proposed for sampling f, and do<br />not have counterparts for θ. In the case of latent variables, the operators aim<br />to leave the posterior p(f|y,θ) invariant; in the remainder of this work, W(f) is<br />defined as log[p(y|f)p(f|θ)], which equals the log of the desired target density up to<br />constants. In the case of hyper-parameters we can define the invariant distribution<br />according to the chosen parameterization and apply the operators presented here<br />for sampling θ rather than f.<br />3.1 Metropolis-Hastings - MH<br />The Metropolis-Hastings transition operator employs a proposal mechanism g(f?|f)<br />based on a random walk (Robert and Casella, 2005). A common choice is to use<br />a multivariate Gaussian proposal with covariance Σ centered at the former posi-<br />tion f, thus taking the form g(f?|f) = N(f?|f,Σ). For such a symmetric proposal<br />mechanism, f?is then accepted with probability min?1,exp(W(f?) − W(f))?.<br />3.2 Hybrid Monte Carlo - HMC<br />In Hybrid Monte Carlo (HMC) the proposals are based on the analogy with a<br />physical system, where a particle is simulated moving in a potential field (Neal,<br />1993). An auxiliary variable p, that plays the role of a momentum variable, is<br />drawn from N(p|0,M), where the covariance matrix M is the so called mass<br />matrix. The joint density of f and p factorizes as p(f,p) = exp(W(f))p(p), and<br />the negative log-joint density reads:<br />H(f,p) = −W(f) +1<br />2log(|M|) +1<br />2pTM−1p + const.<br />This is the Hamiltonian of the simulated particle, where the potential field is given<br />by −W(f) and the kinetic energy by the quadratic form in p. In order to draw</p>  <p>Page 8</p> <p>8M. Filippone et al.<br />proposals from p(f|y,θ), we can simulate the particle for a certain time interval,<br />introducing an analogous of time t and solving Hamilton’s equations<br />df<br />dt=∂H<br />∂p= M−1p<br />dp<br />dt= −∂H<br />∂f<br />= ∇fW<br />Given that there is no friction, the energy will be conserved during the mo-<br />tion of the particle. Solving directly Hamilton’s equations for general potential<br />fields, however, is analytically intractable, and therefore it is necessary to resort<br />to schemes where time is discretized. The leapfrog integrator discretizes the dy-<br />namics in λ steps, also known as leapfrog steps, and is volume preserving and<br />reversible (see Neal (1993) for details). The leapfrog integrator yields an update of<br />(f,p) into (f(λ),p(λ)). The discretization introduces an approximation such that<br />the total energy is not conserved, so a Metropolis accept/reject step of the form<br />min{1,exp(−H(f(λ),p(λ))+H(f,p))} is needed to ensure that HMC samples from<br />the correct invariant distribution. The HMC transition operator is reported in Al-<br />gorithm 1.<br />Algorithm 1 HMC transition operator when M = LMLT<br />M<br />1: f(0)= f; p(0)∼ N(p(0)|0,M)<br />2: λ = sample[1,...,λmax]<br />3: for (t = 0 to λ − 1) do<br />4:p(t+1/2)= p(t)+ε<br />5:f(t+1)= f(t)+ εM−1p(t+1/2)<br />6:p(t+1)= p(1/2)+ε<br />7: end for<br />8: r = min?0,H(f(0),p(0)) − H(f(λ),p(λ))?<br />9: u ∼ Exp(u|1)<br />10: if (r &gt; −u) then return f(λ)<br />11: else return f(0)<br />? z ∼ N(0,I); p(0)= LMz<br />2∇fW(f(t))<br />? M−1p = bcksub(LT<br />M,(fwdsub(LM,p)))<br />2∇fW(f(t+1))<br />? log|M| = 2?<br />ilog(LM)ii<br />? pTM−1p = ?fwdsub(LM,p)?2<br />3.3 Manifold MCMC - Simplified Manifold MALA - SMMALA<br />Manifold MCMC methods (Girolami and Calderhead, 2011) were proposed to<br />have an automatic mechanism to tune parameters in MALA and HMC, and are<br />based on the use of curvature through the Fisher Information (FI) matrix. The FI<br />matrix and the Christoffel symbols are the key quantities in information geometry<br />as they characterize the curvature and the connection on the statistical manifold<br />respectively. Consider a statistical model S = {p(y|ψ)|ψ ∈ Ψ} where y denotes<br />observed variables and ψ comprises all model parameters. Under conditions that<br />are generally satisfied for most commonly used models (Amari and Nagaoka, 2000),<br />S can be considered a C∞manifold, and is called statistical manifold. Let L =<br />log[p(y|ψ)]; the FI matrix G of S at ψ is defined as:<br />?<br />G(ψ) = Ep(y|ψ)<br />(∇ψL)(∇ψL)T?<br />= −Ep(y|ψ)[∇ψ∇ψL]</p>  <p>Page 9</p> <p>Title Suppressed Due to Excessive Length9<br />By definition, the FI matrix is positive semidefinite, and can be considered as the<br />natural metric on S.<br />In the case of GP models that are hierarchical we need to consider the statistical<br />manifolds associated with the two levels of the hierarchy separately. Let’s focus<br />on the statistical manifold associated with the model for y given f. The manifold<br />MALA (MMALA) algorithm (Girolami and Calderhead, 2011) defines a Langevin<br />diffusion with stationary distribution p(f|θ,y) on the Riemann manifold of density<br />functions. Denote its metric tensor by Gf,f. By employing a first order Euler<br />integrator to solve the diffusion, a proposal mechanism with density g(f?|f) =<br />N(f?|µ(f,?),?2G−1<br />which needs to be tuned, and the dth component of the mean function µ(f,?)dis<br />f,f) is obtained, where ? is the integration step size, a parameter<br />µ(f,?)d= fd+?2<br />2<br />?<br />G−1<br />f,f∇fW(f)<br />?<br />d− ?2<br />n<br />?<br />i=1<br />n<br />?<br />j=1<br />(G−1<br />f,f)i,jΓd<br />i,j<br />where Γd<br />and Nagaoka, 2000). Similarly to MALA (Roberts and Stramer, 2002), due to the<br />discretization error introduced by the first order approximation, convergence to the<br />stationary distribution is not guaranteed anymore and thus a standard Metropolis<br />accept/reject step is employed to correct this bias.<br />In the same spirit, it is possible to extend HMC to define Hamilton’s equations<br />on the statistical manifold. This was proposed and applied in Girolami and Calder-<br />head (2011) and called Riemann manifold Hamiltonian Monte Carlo (RM-HMC).<br />In this work, we will not consider RM-HMC or MMALA, as they both require<br />the derivatives of the FI matrix that would require several expensive operations.<br />Instead, we will consider a simplified version of MMALA (SMMALA), where we<br />assume a manifold with constant curvature, that effectively removes the term de-<br />pending on the Christoffel symbols, so that the mean of the proposal of SMMALA<br />becomes<br />µs(f,?) = f +?2<br />i,jare the Christoffel symbols of the metric in local coordinates (Amari<br />2G−1<br />f,f∇fW(f)<br />Furthermore, in the last subsection of this section we will present two variants of<br />HMC that bear some similarities with RM-HMC but are computationally cheaper.<br />The SMMALA transition operator is sketched in Algorithm 2.<br />Algorithm 2 SMMALA transition operator<br />1: µs(f,?) = f +?2<br />2G−1<br />f,f∇fW(f)? Gf,f= LGLT<br />G<br />? G−1<br />f,f∇fW(f) = bcksub(LT<br />? z ∼ N(0,I); f?= εbcksub(LT<br />G,(fwdsub(LG,∇fW(f))))<br />G,z) + µs(f,?)<br />? log|Gf,f| = 2?<br />2: f?∼ N(f?|µs(f,?),?2G−1<br />3: r = min{0,W(f?) − W(f) + log[g(f|f?)] − log[g(f?|f)]}<br />? (f?− µs(f,?))TG−1<br />4: u ∼ Exp(u|1)<br />5: if (r &gt; −u) then return f?<br />6: else return f<br />f,f)<br />ilog(LG)ii<br />f,f(f?− µs(f,?)) = ?fwdsub(LG,(f?− µs(f,?)))?2</p>  <p>Page 10</p> <p>10 M. Filippone et al.<br />3.4 Elliptical Slice sampling - ELL-SS<br />Elliptical Slice Sampling (ELL-SS) has been proposed in Murray et al. (2010) to<br />draw samples for f in GP models, and is based on slice sampling (Neal, 2003). Due<br />to the fact that latent variables are Gaussian, it is possible to derive this particular<br />version of slice sampling that is constrained on an ellipse. For completeness, we<br />report the transition operator in Algorithm 3 and we refer the reader to Murray<br />et al. (2010) for further details. Note that ELL-SS is quite appealing as it returns<br />Algorithm 3 ELL-SS transition operator<br />1: z ∼ N(0,K)<br />2: u ∼ Exp(u|1)<br />3: α ∼ U[0,2π]<br />4: f?= f cos(α) + zsin(α)<br />5: if (logp(y|f?) &gt; η) then return f?<br />6: else<br />7: if (α &lt; 0) then αmin= 0<br />8: else αmax= 0<br />9:α ∼ U[αmin,αmax]<br />10:Go to 4<br />η = logp(y|f) − u<br />[αmin,αmax] = [α − 2π,α]<br />? Set a threshold on the log-likelihood<br />? Define the bracket<br />? Shrink the bracket<br />a sample which does not need to be accepted or rejected (in fact, a rejection<br />mechanism is implicit within step 5), and the proposal mechanism does not have<br />any free parameters that need tuning.<br />3.5 Scaled versions of MH - MH v1 and MH v2<br />Due to the strong correlation of latent variables imposed by the GP prior, employ-<br />ing a MH operator with an isotropic covariance to sample latent variables leads to<br />extremely poor efficiency. In order to overcome this problem, Neal (1999) proposed<br />two versions of MH that we will denote by MH v1 and MH v2. In MH v1, a set of<br />latent variables z is drawn from the GP prior z ∼ N(z|0,K), and the proposal is<br />constructed as follows:<br />f?= f + αz<br />where the parameter α controls the degree of update. In MH v2, instead, the<br />proposal is as follows:<br />f?=1 − α2f + αz<br />In the latter case, given that the proposal satisfies detailed balance with respect<br />to the prior, the acceptance has to be based on the likelihood alone.<br />?<br />3.6 Scaled versions of HMC - HMC v1 and HMC v2<br />By a similar argument as in MH, it is possible to introduce scaled versions of HMC<br />that reduce the correlation between latent variables. This can be done by setting<br />the mass matrix of HMC according to the precision of the posterior distribution of</p>  <p>Page 11</p> <p>Title Suppressed Due to Excessive Length 11<br />latent variables. Similarly, from an information geometric perspective, it is sensible<br />to whiten latent variables according to the metric tensor of the statistical manifold.<br />We notice that the metric tensor associated to the model for y given f is K−1plus a<br />diagonal matrix which is a function of f (see appendix A for full details). Whitening<br />with respect to that metric tensor would be computationally very expensive for<br />GP models, as it would require the simulation of the Hamiltonian dynamics on a<br />manifold with a position-dependent curvature; this is implemented by RM-HMC<br />which requires the derivatives of the metric tensor as well as implicit leapfrog<br />iterations (Girolami and Calderhead, 2011). In order to reduce the computational<br />cost, we propose the following two options: (i) to approximate the diagonal term to<br />be independent of f so that M−1= (K−1+C)−1= C−1−C−1(K +C−1)−1C−1<br />with C diagonal and independent of f; we call this variant HMC v1. (ii) to ignore<br />the diagonal part of the metric tensor and set M−1= K; we call this variant<br />HMC v2. In HMC v1, one simple way to make C independent of f is to compute<br />it for the GP prior mean (which is zero), as proposed, e.g., in Christensen et al.<br />(2005); Vanhatalo and Vehtari (2007).<br />In both cases, it is possible to employ a standard HMC proposal that captures<br />part of the curvature of the statistical manifold. By introducing a variant of HMC<br />that, rather than using the Cholesky decomposition of the mass matrix requires<br />the decomposition of its inverse, it is possible to devise efficient implementations<br />of HMC v1 and HMC v2. We report this variant of the HMC transition operator<br />in Algorithm 4.<br />In HMC v1, employing this formulation of HMC is convenient as computing<br />the inverse of M is more stable than computing M = K−1+ C, that requires<br />a potentially unstable inversion of K. HMC v1 requires the computation of the<br />inverse of the mass matrix and its factorization each time a new value of θ is<br />proposed. In HMC v2, instead, no extra operations in O(n3) are required given<br />that K is already factorized, thus making it computationally very convenient.<br />Algorithm 4 HMC transition operator when M−1= LM−1LT<br />M−1<br />1: f(0)= f; p(0)∼ N(p(0)|0,M)<br />2: λ = sample[1,...,λmax]<br />3: for (t = 0 to λ − 1) do<br />4:p(t+1/2)= p(t)+ε<br />5:f(t+1)= f(t)+ εM−1p(t+1/2)<br />6:p(t+1)= p(1/2)+ε<br />7: end for<br />8: r = min?0,H(f(0),p(0)) − H(f(λ),p(λ))?<br />9: u ∼ Exp(u|1)<br />10: if (r &gt; −u) then return f(λ)<br />11: else return f(0)<br />? z ∼ N(0,I); p(0)= bcksub(LT<br />M−1,z)<br />2∇fW(f(t))<br />? M−1p = LM−1(LT<br />M−1p)<br />2∇fW(f(t+1))<br />? log|M| = −2?<br />ilog(LM−1)ii<br />M−1p?2<br />? pTM−1p = ?LT</p>  <p>Page 12</p> <p>12 M. Filippone et al.<br />4 Results on simulated data<br />In this section, we first report a study on the efficiency and speed of convergence of<br />different transition operators in sampling from posterior distribution of individual<br />groups of variables in the SA and AA parameterization. We then report the same<br />analysis to compare different parameterizations to obtain samples from the joint<br />posterior distribution of f and θ.<br />4.1 Experimental setup<br />We simulated data from the four GP models considered in this work, namely: LRG,<br />LCX, VLT, and ORD. We generated 10 data sets simulating from each of the four<br />models for all combinations of n = 100,400, and d = 2,10, for a total of 160 distinct<br />data sets. In order to isolate the effect of different likelihood functions in the<br />results, we seeded the generation of the input data matrix X, hyper-parameters,<br />and latent variables so that is was the same across different models. Covariates were<br />generated uniformly in the unit hyper-cube, and the parameters used to generate<br />latent variables were σ = exp(2), ψτi∼ U[−3,−1]. We imposed Gamma priors on<br />the length-scale parameters with shape a and rate b, p(τi) = Gam(τi|a = 1,b = 1).<br />We imposed an inverse Gamma prior p(σ) = invGam(σ|a = 1,b = 1), where a and<br />b are shape and scale parameters respectively on σ to exploit conjugacy in the SA<br />parameterization.<br />In all the experiments we collected 20000 samples after a burn-in phase of<br />5000 iterations; during the burn-in we also had an adaptive phase to allow the<br />samplers reach recommended acceptance rates (for example around 25% for MH).<br />The transition operators for f had the following tuning parameters: α for MH v1<br />and MH v2, and ε for SMMALA and the variants of HMC which used a maximum<br />of 10 leapfrog steps. The transition operators for θ employed the following pro-<br />posals: MH used a covariance Σ = αI, HMC used a mass matrix M = αI and 10<br />maximum leapfrog steps, and SMMALA used a step-size ε. Convergence analysis<br />was performed using theˆR potential scale reduction factor (Gelman and Rubin,<br />1992), which is a classic score used to assess convergence of MCMC algorithms.<br />The computation of theˆR value is based on the within and between chains vari-<br />ances; a value close to one indicates that convergence is reached. TheˆR value was<br />computed based on 10 chains initialized from the prior to study what efficiency<br />can be achieved without running preliminary simulations; this is different from<br />the initialization procedure suggested in Gelman and Rubin (1992) that requires<br />locating the modes of the target density. Due to the fairly diffuse priors on the<br />length-scale parameters, we noticed difficulty in achieving convergence in some<br />cases; we therefore initialized ψτirandomly in the interval [−3,−1]. The value of<br />ˆR was checked at 1000, 2000, 5000, 10000, 20000 iterations. We use the following<br />procedure to compactly visualize speed of convergence; we threshold the median<br />value ofˆR across 10 data sets at each checkpoint and use the following visual<br />coding to report speed of convergence: &lt; 1.1 &lt; &lt; 1.3 &lt; &lt; 2 &lt; , so that indi-<br />cates thatˆR &lt; 1.1,indicates that 1.1 &lt;ˆR &lt; 1.3, and so on. We then stack the<br />rectangles associated to each checkpoint where we computed the value ofˆR, thus<br />producing a sort of histogram of the median ofˆR over the iterations. Efficiency<br />of MCMC methods is compared based on the minimum of the Effective Sample</p>  <p>Page 13</p> <p>Title Suppressed Due to Excessive Length 13<br />Size (ESS) (Robert and Casella, 2005) computed across all the sampled variables.<br />We then report its mean and standard deviation across the 10 chains and the 10<br />different data sets for each combination of size of the data set, dimensionality, and<br />type of likelihood.<br />We are also interested in statistically assessing which methods achieve faster<br />convergence. In order to do so, we perform pairwise Mann-Whitney tests with<br />significance level of 0.05 comparing the value ofˆR at the last checkpoint for all<br />the chains across 10 data sets. This allows us to obtain an ordering of methods<br />in terms of speed of convergence. In each table we include a row at the bottom<br />reporting the result of such a test. We denote by 1|2 situations where the method<br />in row 1 of the corresponding table converges significantly faster than the method<br />in row 2. Instead, the notation 1,2 is used when the method in row 1 does not<br />converge significantly faster than the method in row 2.<br />As a measure of complexity, we counted the number of operations with com-<br />plexity in O(n3), namely number of Cholesky factorizations of n × n matrices<br />(#C), number of inversions of n × n matrices (#I)2, and number of multiplica-<br />tions of n × n matrices (#M). We believe that this is a more reliable measure<br />of complexity with respect to running time, as running time can be affected by<br />several implementation details and other factors that are not directly related to<br />the actual complexity of the algorithms.<br />4.2 Assessing the efficiency of samplers for individual groups of variables<br />In this section, we present an assessment of the efficiency of different transition<br />operators for each group of variables using both SA and AA parameterizations.<br />Computational complexity for all the operators considered in the next sections is<br />summarized in Tab. 1, where T represent the number of iterations, d the number of<br />covariates and¯λ the average number of leapfrog steps in HMC transition operators.<br />In the next sub-sections we first present results about the sampling of the latent<br />variables, and then we move onto presenting results on the sampling of the hyper-<br />parameters.<br />4.2.1 Sampling f|y,θ<br />In this section we focus on the sampling from the posterior distribution of the<br />latent variables f. The results can be found in Tab. 2, and they were obtained<br />by fixing θ to the values used to generate the data. First, we notice that different<br />likelihood functions heavily affect efficiency and speed of convergence; in the exam-<br />ples considered here, the results show that in LRG it is possible to achieve efficiency<br />one order of magnitude higher than in other models. The scaled versions of MH<br />work well in the case of LRG (MH v1 is slightly better than MH v2), but do not<br />offer guarantees of convergence on other models. ELL-SS achieves better efficiency<br />and convergence than the scaled versions of MH. SMMALA, which uses gradient<br />and curvature information, achieves good efficiency and faster convergence than<br />MH v1, MH v2, and ELL-SS, but at the cost of one operation in O(n3) at each<br />2This is a shorthand notation to denote a back and forward substitution of the identity<br />matrix using Cholesky factors.</p>  <p>Page 14</p> <p>14 M. Filippone et al.<br />Table 1 Breakdown of the number of operations in O(n3) required to apply the transition<br />operators considered in this work. #M, #I and #C represent number of multiplication of n×n<br />matrices, inversions of n × n matrices, and number of Cholesky decompositions respectively.<br />Counts are reported as functions of the number of iterations T and number of covariates d. In<br />HMC,¯λ denotes the average number of leapfrog steps in one iteration.<br />f|θ,y<br />#I<br />0<br />0<br />1<br />0<br />0<br />0<br />1<br />0<br />θ|f<br />#I<br />0<br />T¯λ<br />T<br />−<br />−<br />−<br />−<br />−<br />θ|ν,y<br />#I<br />0<br />0<br />0<br />−<br />−<br />−<br />−<br />−<br />#M<br />0<br />0<br />0<br />0<br />0<br />0<br />0<br />0<br />#C<br />1<br />1<br />T<br />1<br />1<br />1<br />2<br />1<br />#M<br />0<br />0<br />Td<br />−<br />−<br />−<br />−<br />−<br />#C<br />T<br />T<br />T<br />−<br />−<br />−<br />−<br />−<br />#M<br />0<br />0<br />0<br />−<br />−<br />−<br />−<br />−<br />#C<br />T MH<br />HMC<br />SMMALA<br />ELL-SS<br />MH v1<br />MH v2<br />HMC v1<br />HMC v2<br />T + Td¯λ<br />T + Td<br />−<br />−<br />−<br />−<br />−<br />iteration, as the metric tensor is a function of f and needs to be factorized at each<br />iteration. Overall, the results suggest that the scaled versions of HMC are the<br />best sampling methods for f|θ,y. HMC v1 is slightly better than HMC v2, but it<br />requires one extra inversion and one extra Cholesky decomposition compared to<br />HMC v2 that does not require any operations in O(n3) once the covariance matrix<br />of the GP is factorized.<br />4.2.2 SA parameterization - Sampling θ|f<br />In this section we present results about the sampling of hyper-parameters from the<br />posterior distribution θ|f,y which, given the hierarchical structure of the model,<br />is simply θ|f independently from the data model. As reported in Tab. 1, the<br />complexity of applying SMMALA and HMC is quite high compared to MH. MH<br />requires one Cholesky factorization of Q at each iteration. In HMC, at each leapfrog<br />step, the gradients of Q with respect to θ are needed and the cheapest way to do<br />this is by inverting Q first and noticing that all the remaining operations are in<br />O(n2); this is done¯λ times on average at every iteration of HMC. Similarly, in<br />SMMALA the gradient can be computed by inverting Q first; by doing so, the<br />metric tensor can then be computed by d multiplications with the derivatives of<br />Q and no other O(n3) operations.<br />The results are reported in Tab. 3, and were obtained by fixing f to the value<br />used to generate the data and sampling only the length-scale parameters, as σ can<br />be efficiently sampled using exact Gibbs steps. HMC improves quite substantially<br />on efficiency, but not on speed of convergence; it may be worth employing some<br />rescaling of the hyper-parameters to improve on this as suggested by Neal (1996).<br />The performance of SMMALA is highly variable in efficiency and it converges<br />more slowly than MH and HMC. This might be due to the skewness of the target<br />distribution, that is known to affect the efficiency of SMMALA (Stathopoulos and<br />Filippone, 2011). The results indicate that MH strikes a good balance between<br />efficiency and computational cost.</p>  <p>Page 15</p> <p>Title Suppressed Due to Excessive Length 15<br />Table 2 Comparison of transition operators to sample f|y,θ for data generated from models<br />with four different likelihoods. Minimum ESS is averaged over 10 chains for 10 different data<br />sets for each value of n and d. The last row in each sub-table reports the result of the statistical<br />test to assess which operators achieve significantly faster convergence.<br />LRG<br />n = 100n = 400<br />d = 2d = 10d = 2d = 10<br />ESS<br />67 (15)<br />204 (35)<br />756 (284)<br />321 (61)<br />3395 (400)<br />4004 (577)<br />6|5|3|4|2|1<br />ˆRESS<br />47 (3)<br />151 (7)<br />262 (30)<br />241 (11)<br />5163 (268)<br />5225 (224)<br />6|5|3,4|2|1<br />ˆRESS<br />22 (7)<br />67 (17)<br />457 (212)<br />104 (25)<br />1352 (380)<br />1566 (342)<br />6|5|3|4|2|1<br />ˆRESS<br />8 (1)<br />30 (2)<br />48 (5)<br />50 (2)<br />2962 (155)<br />2995 (129)<br />6|5|4|3|2|1<br />ˆR<br />MH v1<br />MH v2<br />SMMALA<br />ELL-SS<br />HMC v1<br />HMC v2<br />LCX<br />n = 100n = 400<br />d = 2d = 10d = 2d = 10<br />ESS<br />18 (16)<br />23 (24)<br />217 (155)<br />39 (42)<br />372 (277)<br />254 (197)<br />6|5|3|4|2|1<br />ˆRESS<br />6 (2)<br />6 (2)<br />39 (4)<br />11 (4)<br />188 (123)<br />188 (125)<br />6|5|3|4|1,2<br />ˆRESS<br />6 (5)<br />8 (7)<br />258 (177)<br />11 (11)<br />199 (200)<br />64 (37)<br />5|3,6|4|2|1<br />ˆRESS<br />1 (0)<br />1 (0)<br />7 (1)<br />2 (0)<br />81 (30)<br />80 (30)<br />6|5|3|4|2|1<br />ˆR<br />MH v1<br />MH v2<br />SMMALA<br />ELL-SS<br />HMC v1<br />HMC v2<br />VLT<br />n = 100n = 400<br />d = 2 d = 10d = 2d = 10<br />ESS<br />28 (13)<br />31 (16)<br />424 (216)<br />46 (20)<br />1494 (667)<br />418 (68)<br />4|3|2|1,5,6<br />ˆRESS<br />10 (2)<br />12 (2)<br />117 (13)<br />18 (4)<br />449 (42)<br />443 (39)<br />3|4|2|1,5,6<br />ˆRESS<br />15 (8)<br />16 (8)<br />418 (127)<br />22 (10)<br />1384 (392)<br />183 (31)<br />3,4|2|1,5,6<br />ˆR ESS<br />2 (0)<br />2 (0)<br />61 (7)<br />4 (1)<br />249 (25)<br />245 (25)<br />3|6|4,5|2|1<br />ˆR<br />MH v1<br />MH v2<br />SMMALA<br />ELL-SS<br />HMC v1<br />HMC v2<br />ORD<br />n = 100n = 400<br />d = 2d = 10d = 2d = 10<br />ESS<br />14 (8)<br />14 (9)<br />48 (89)<br />21 (11)<br />539 (650)<br />175 (54)<br />6|5|3|4|1,2<br />ˆR ESS<br />6 (2)<br />7 (2)<br />2 (0)<br />10 (2)<br />472 (39)<br />483 (37)<br />6|5|4|2|1|3<br />ˆR ESS<br />7 (5)<br />7 (5)<br />107 (156)<br />9 (5)<br />176 (200)<br />61 (22)<br />6|5|3|4|1,2<br />ˆR ESS<br />1 (0)<br />2 (0)<br />1 (0)<br />2 (0)<br />257 (23)<br />255 (24)<br />6|5|2,4|1,3<br />ˆR<br />MH v1<br />MH v2<br />SMMALA<br />ELL-SS<br />HMC v1<br />HMC v2<br />4.2.3 AA parameterization - Sampling θ|y,ν<br />In this section we present the sampling of the hyper-parameters from the posterior<br />distribution θ|y,ν, where we fixed ν to the values used to generate the data. The</p>  <p>Page 16</p> <p>16M. Filippone et al.<br />Table 3 Comparison of transition operators to sample θ|f. Minimum ESS is averaged over<br />10 chains for 10 different data sets for each value of n and d. The last row reports the result<br />of the statistical test to assess which operators achieve significantly faster convergence.<br />n = 100n = 400<br />d = 2d = 10d = 2d = 10<br />ESS<br />2024 (144)<br />11325 (915)<br />9592 (2052)<br />ˆRESS<br />156 (37)<br />830 (269)<br />61 (23)<br />ˆRESS<br />2124 (125)<br />12556 (661)<br />10241 (2672)<br />ˆR ESS<br />77 (33)<br />293 (137)<br />47 (17)<br />ˆR<br />MH<br />HMC<br />SMMALA<br />1|2,32|1|31|2|32|1|3<br />analysis of complexity shows that MH requires one Cholesky factorization at each<br />iteration only. In HMC, each leapfrog requires computing L and the gradient of L<br />with respect to θ and no other operations in O(n3); this can be computed using<br />the differentiation of the Cholesky algorithm which requires d operations in O(n3)<br />(Smith, 1995). Likewise, for SMMALA L and the d derivatives of L with respect<br />to θ are the only operations in O(n3) needed.<br />The results can be found in Tab. 4 and are again variable across different<br />models. In general SMMALA and HMC do not seem to offer faster convergence<br />with respect to the MH transition operator which is therefore competitive in terms<br />of efficiency relative to computational cost.<br />4.3 Assessing the efficiency of different parameterizations<br />After analyzing the results in the previous section, we decided to combine the<br />transition operators which achieved a good sampling efficiency with relatively low<br />computational cost and ease of implementation. We decided that a good combina-<br />tion to be used in AA, SA, ASIS, and SURR could be as follows: sampling f using<br />HMC v2 and θ using MH; HMC v2 and MH where adapted during the burn-in<br />phase and in HMC v2 we set the maximum number of leapfrog steps to 10. For<br />the sake of brevity, we focus on the LRG model only; the results on efficiency and<br />speed of convergence in sampling hyper-parameters are reported in Tab. 5.<br />It is striking to see how challenging it is to efficiently sample from the pos-<br />terior distribution of latent variables and hyper-parameters. Sampling efficiency<br />is generally low; this is consistent with our experience in other applications in-<br />volving sampling in hierarchical models (Filippone et al., 2012). As expected, the<br />SA parameterization is the worst among the ones we tested. The AA parameter-<br />ization, ASIS, and SURR generally offer good guarantees of convergence within<br />a few thousand iterations. SURR seems to be superior in efficiency, consistently<br />with what reported in Murray and Adams (2010), but it requires more operations<br />in O(n3) compared to AA and ASIS. ASIS slightly improves efficiency and speed<br />of convergence with respect to the AA scheme but requires double the number of<br />operations in O(n3). KHR seems effective in breaking the correlation between the<br />two groups of variables, but it may require several iterations within the approxi-<br />mation used to sample f. In the experiments considered here ¯ κ is around 8, so the<br />best compromise between computations and efficiency seems to be given by the<br />AA and ASIS parameterizations.</p>  <p>Page 17</p> <p>Title Suppressed Due to Excessive Length 17<br />Table 4 Comparison of transition operators to sample θ|y,ν for data generated from models<br />with four different likelihoods. Minimum ESS is computed as the average over 10 chains for 10<br />different data sets for each value of n and d. The last row in each sub-table reports the result<br />of the statistical test to assess which operators achieve significantly faster convergence.<br />LRG<br />n = 100n = 400<br />d = 2d = 10d = 2d = 10<br />ESS<br />556 (201)<br />2572 (1382)<br />3833 (2032)<br />ˆRESS<br />131 (33)<br />859 (278)<br />65 (42)<br />ˆRESS<br />512 (177)<br />2666 (973)<br />6877 (1584)<br />ˆR ESS<br />56 (11)<br />223 (39)<br />47 (21)<br />ˆR<br />MH<br />HMC<br />SMMALA<br />1,3|22|1|31|3|21|2,3<br />LCX<br />n = 100n = 400<br />d = 2d = 10d = 2d = 10<br />ESS<br />818 (386)<br />5169 (3297)<br />6158 (2788)<br />ˆR ESS<br />6 (4)<br />11 (8)<br />9 (6)<br />ˆRESS<br />1030 (397)<br />7145 (3852)<br />8377 (1815)<br />ˆR ESS<br />3 (1)<br />4 (3)<br />6 (4)<br />ˆR<br />MH<br />HMC<br />SMMALA<br />3|1,21,2|31,2,31,2|3<br />VLT<br />n = 100n = 400<br />d = 2d = 10d = 2d = 10<br />ESS<br />859 (318)<br />5680 (2634)<br />6274 (1896)<br />ˆR ESS<br />22 (6)<br />48 (20)<br />14 (9)<br />ˆR ESS<br />795 (270)<br />5233 (2482)<br />6950 (2763)<br />ˆR ESS<br />8 (6)<br />11 (11)<br />11 (9)<br />ˆR<br />MH<br />HMC<br />SMMALA<br />1,2,31|2|31,2,31|2|3<br />ORD<br />n = 100n = 400<br />d = 2d = 10d = 2d = 10<br />ESS<br />689 (159)<br />155 (296)<br />3356 (1661)<br />ˆR ESS<br />14 (7)<br />14 (11)<br />11 (8)<br />ˆRESS<br />552 (168)<br />79 (115)<br />2328 (1423)<br />ˆRESS<br />9 (6)<br />4 (4)<br />19 (27)<br />ˆR<br />MH<br />HMC<br />SMMALA<br />1,3|21,3|21,3|21,3|2<br />5 Results on real data<br />We repeated the comparison of different parameterizations on four UCI data sets<br />(Asuncion and Newman, 2007), namely the Pima, Wisconsin, SPECT, and Iono-<br />sphere data sets, which we modeled using LRG models; the results are reported<br />in Tab. 6. We used the same priors and experimental setup as in the previous<br />sections, except that all features were transformed to have zero mean and unit<br />standard deviation, and latent variables were sampled iterating five updates of<br />HMC v2. Also, chains were initialized sampling from the prior. Again, the SA<br />parameterization is the poorest in efficiency and speed of convergence, and the<br />AA parameterization improves on that; combining the two using ASIS slightly<br />improves on the AA parameterization, although the improvement is not dramatic.<br />The SURR method improves on the AA parameterization, consistently with what<br />reported in Murray and Adams (2010). The results of KHR are highly variable</p>  <p>Page 18</p> <p>18M. Filippone et al.<br />Table 5 Comparison of different strategies to sample f,θ|y for data generated from a<br />LRG model. The rightmost column reports the complexity of the different methods with re-<br />spect to number of inversion and Cholesky decompositions. In KHR, ¯ κ represents the average<br />number of iterations to run the Laplace Approximation.<br />n = 100n = 400<br />d = 2d = 10d = 2d = 10<br />ESS<br />131(57)<br />138(63)<br />856(360)<br />8(6)<br />173(95)<br />3|5|1,2|4<br />ˆRESS<br />117(34)<br />168(49)<br />177(48)<br />59(18)<br />90(32)<br />1,2|3,4,5<br />ˆR ESS<br />94(38)<br />98(39)<br />481(219)<br />5(2)<br />157(51)<br />3|5|1,2|4<br />ˆR ESS<br />47(17)<br />60(25)<br />116(32)<br />14(6)<br />35(15)<br />2,3|1|4,5<br />ˆR #I<br />0<br />0<br />0<br />0<br />T<br />#C<br />T<br />2T<br />AA<br />ASIS<br />KHR<br />SA<br />SURR<br />¯ κT + 2T<br />T<br />2T<br />Table 6 Comparison of different strategies to sample f,θ|y in four UCI data sets modeled<br />using a LRG model.<br />Pima Wisconsin<br />n = 683,d = 9<br />ESS<br />42 (15)<br />47 (11)<br />20 (10)<br />7 (3)<br />25 (14)<br />SPECT<br />n = 80,d = 22<br />ESS<br />99 (18)<br />215 (23)<br />101 (16)<br />97 (12)<br />84 (14)<br />Ionosphere<br />n = 351,d = 34<br />ESS<br />12 (5)<br />24 (8)<br />2 (2)<br />11 (7)<br />9 (4)<br />n = 768,d = 8<br />ESS<br />34 (4)<br />35 (8)<br />153 (14)<br />5 (2)<br />76 (10)<br />ˆR<br />ˆR<br />ˆR<br />ˆR<br />AA<br />ASIS<br />KHR<br />SA<br />SURR<br />across data sets; in cases where the approximation to sample latent variables is<br />accurate, the chains mix well. In some cases, however, the approximation is not<br />accurate enough to guarantee a good acceptance rate, and the chains can spend a<br />long time in the same position before accepting the joint proposal.<br />6 Conclusions<br />In this paper we studied and compared a number of state-of-the-art strategies to<br />carry out the fully Bayesian treatment of GP models. We focused on four GP mod-<br />els and performed an extensive evaluation of efficiency, speed of convergence, and<br />computational complexity of several transition operators and sampling strategies.<br />The results in this paper show that latent variables can be sampled quite<br />efficiently with little computational effort once the GP covariance matrix is fac-<br />torized. This can be achieved by a simple variant of HMC that we introduced in<br />this paper. About sampling hyper-parameters in different parameterizations, the<br />results presented here indicate that the gain in sampling efficiency given by the<br />use of complicated proposal mechanisms does not scale as much as their computa-<br />tional cost. It would be interesting to investigate some recently proposed variants<br />to slice sampling (Thompson and Neal, 2010) and Hybrid Monte Carlo (Hoffman<br />and Gelman, 2012) on the sampling of hyper-parameters.<br />The analysis of the results obtained by different parameterization suggest that<br />AA is a sensible and computationally cheap parameterization with good conver-<br />gence properties. AA performs similarly to ASIS at half the computational cost. It</p>  <p>Page 19</p> <p>Title Suppressed Due to Excessive Length 19<br />makes sense, however, to employ ASIS when in doubt about the best parameteri-<br />zation to use, although GP models with full covariance matrices will generally fall<br />into the weak data limit as the O(n2) space and O(n3) time complexities constrain<br />the number of data that can be processed.<br />In general, the results show how challenging it is to efficiently sample from<br />the posterior distribution of latent variables and hyper-parameters in GP models<br />and motivates further research into methods to do this efficiently. Some sampling<br />strategies, such as the one based on the AA parameterization, are capable of<br />achieving convergence within a reasonable number of iterations, and this makes it<br />possible to carry out the fully Bayesian treatment of GP models dealing with a<br />small to moderate number of samples. We have recently demonstrated that this is<br />indeed the case (Filippone et al., 2012), but more needs to be done in the direction<br />of developing robust stochastic based inference methods for GP models.<br />It would be interesting to investigate how performance are affected by the<br />choice of the design, which in the simulated data presented here was assumed uni-<br />form. Also, we studied in particular GP models with the squared exponential ARD<br />covariance function. It would be interesting to compare the method considered here<br />in models characterized by other covariance functions, such as the Mat´ ern one, or<br />sparse inverse covariance functions as in Rue et al. (2009); the latter, would make<br />it possible to test the strong data limit case. Finally, in this study we have not<br />included a mean function for the GP prior or extra parameters for the likelihood<br />function. This would require including the sampling of other quantities that may<br />further impact on efficiency and speed of convergence.<br />References<br />1. Amari, S. and H. Nagaoka (2000). Methods of Information Geometry, Volume 191<br />of Translations of Mathematical monographs. Oxford University Press.<br />2. Asuncion, A. and D. J. Newman (2007). UCI machine learning repository.<br />3. Christensen, O. F., G. O. Roberts, and J. S. Rosenthal (2005). Scaling limits for<br />the transient phase of local MetropolisHastings algorithms. Journal of the Royal<br />Statistical Society: Series B (Statistical Methodology) 67(2), 253–268.<br />4. Chu, W. and Z. Ghahramani (2005). Gaussian Processes for Ordinal Regression.<br />Journal of Machine Learning Research 6, 1019–1041.<br />5. Cseke, B. and T. Heskes (2011). Approximate Marginals in Latent Gaussian Mod-<br />els. Journal of Machine Learning Research 12, 417–454.<br />6. Filippone, M., A. F. Marquand, C. R. V. Blain, S. C. R. Williams, J. Mour˜ ao-<br />Miranda, and M. Girolami (2012). Probabilistic Prediction of Neurological Dis-<br />orders with a Statistical Assessment of Neuroimaging Data Modalities. Annals<br />of Applied Statistics 6(4), 1883–1905.<br />7. Filippone, M., M. Zhong, and M. Girolami (2012). On the fully Bayesian treatment<br />of latent Gaussian models using stochastic simulations. Technical Report TR-<br />2012-329, School of Computing Science, University of Glasgow.<br />8. Flegal, J. M., M. Haran, and G. L. Jones (2007). Markov Chain Monte Carlo: Can<br />We Trust the Third Significant Figure? Statistical Science 23(2), 250–260.<br />9. Gelman, A. and D. B. Rubin (1992). Inference from iterative simulation using<br />multiple sequences. Statistical Science 7(4), 457–472.</p>  <p>Page 20</p> <p>20M. Filippone et al.<br />10. Girolami, M. and B. Calderhead (2011). Riemann manifold Langevin and Hamil-<br />tonian Monte Carlo methods. Journal of the Royal Statistical Society: Series B<br />(Statistical Methodology) 73(2), 123–214.<br />11. Hoffman, M. D. and A. Gelman (2012). The No-U-Turn Sampler: Adaptively<br />Setting Path Lengths in Hamiltonian Monte Carlo. Journal of Machine Learning<br />Research to appear.<br />12. Knorr-Held, L. and H. Rue (2002). On Block Updating in Markov Random Field<br />Models for Disease Mapping. Scandinavian Journal of Statistics 29(4), 597–614.<br />13. Kuss, M. and C. E. Rasmussen (2005). Assessing Approximate Inference for Bi-<br />nary Gaussian Process Classification. Journal of Machine Learning Research 6,<br />1679–1704.<br />14. Mackay, D. J. C. (1994). Bayesian methods for backpropagation networks. In<br />E. Domany, J. L. van Hemmen, and K. Schulten (Eds.), Models of Neural Net-<br />works III, Chapter 6, pp. 211–254. Springer.<br />15. Minka, T. P. (2001). Expectation Propagation for approximate Bayesian inference.<br />In Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence,<br />UAI ’01, San Francisco, CA, USA, pp. 362–369. Morgan Kaufmann Publishers<br />Inc.<br />16. Møller, J., A. R. Syversveen, and R. P. Waagepetersen (1998). Log Gaussian Cox<br />Processes. Scandinavian Journal of Statistics 25(3), 451–482.<br />17. Murray, I. and R. P. Adams (2010). Slice sampling covariance hyperparameters<br />of latent Gaussian models. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor,<br />R. S. Zemel, and A. Culotta (Eds.), NIPS, pp. 1732–1740. Curran Associates,<br />Inc.<br />18. Murray, I., R. P. Adams, and D. J. C. MacKay (2010). Elliptical slice sampling.<br />Journal of Machine Learning Research - Proceedings Track 9, 541–548.<br />19. Neal, R. (2003). Slice Sampling. Annals of Statistics 31, 705–767.<br />20. Neal, R. M. (1993). Probabilistic inference using Markov chain Monte Carlo meth-<br />ods. Technical Report CRG-TR-93-1, Dept. of Computer Science, University of<br />Toronto.<br />21. Neal, R. M. (1996). Bayesian Learning for Neural Networks (Lecture Notes in<br />Statistics) (1 ed.). Springer.<br />22. Neal, R. M. (1999). Regression and classification using Gaussian process priors<br />(with discussion). Bayesian Statistics 6, 475–501.<br />23. Opper, M. and O. Winther (2000). Gaussian processes for classification: Mean-<br />field algorithms. Neural Computation 12(11), 2655–2684.<br />24. Rasmussen, C. E. and C. Williams (2006). Gaussian Processes for Machine Learn-<br />ing. MIT Press.<br />25. Robert, C. P. and G. Casella (2005). Monte Carlo Statistical Methods (Springer<br />Texts in Statistics). Secaucus, NJ, USA: Springer-Verlag New York, Inc.<br />26. Roberts, G. O. and O. Stramer (2002).<br />Hastings Algorithms. Methodology and Computing in Applied Probability 4(4),<br />337–357.<br />27. Rue, H., S. Martino, and N. Chopin (2009). Approximate Bayesian inference<br />for latent Gaussian models by using integrated nested Laplace approximations.<br />Journal of the Royal Statistical Society: Series B (Statistical Methodology) 71(2),<br />319–392.<br />28. Smith, S. P. (1995). Differentiation of the Cholesky Algorithm. Journal of Com-<br />putational and Graphical Statistics 4(2), 134–147.<br />Langevin Diffusions and Metropolis-</p>  <p>Page 21</p> <p>Title Suppressed Due to Excessive Length 21<br />29. Stathopoulos, V. and M. Filippone (2011). Discussion of the paper ”Riemann<br />manifold Langevin and Hamiltonian Monte Carlo methods” by Mark Girolami<br />and Ben Calderhead. Journal of the Royal Statistical Society, Series B (Statis-<br />tical Methodology) 73(2), 123–214.<br />30. Thompson, M. and R. M. Neal (2010).<br />Technical Report 1002, Department of Statistics, University of Toronto.<br />31. Tierney, L. and J. B. Kadane (1986). Accurate Approximations for Posterior<br />Moments and Marginal Densities. Journal of the American Statistical Associa-<br />tion 81(393), 82–86.<br />32. Vanhatalo, J. and A. Vehtari (2007). Sparse Log Gaussian Processes via MCMC<br />for Spatial Epidemiology. Journal of Machine Learning Research - Proceedings<br />Track 1, 73–89.<br />33. Wilson, A. G. and Z. Ghahramani (2010). Copula Processes. In J. D. Lafferty,<br />C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta (Eds.), NIPS,<br />pp. 2460–2468. Curran Associates, Inc.<br />34. Yu, Y. and X.-L. Meng (2011). To Center or Not to Center: That Is Not the<br />Question–An Ancillarity-Sufficiency Interweaving Strategy (ASIS) for Boosting<br />MCMC Efficiency. Journal of Computational and Graphical Statistics 20(3),<br />531–570.<br />Covariance-Adaptive Slice Sampling.<br />A SA and AA parameterizations<br />A.1 Sufficient Augmentation (SA)<br />We derive here the quantities needed to apply the transition operators considered in this work<br />in the SA parameterization. Let L = log[p(y|f)]. The log-joint density is:<br />log[p(y,f,θ)] = L −1<br />2log(|Q|) −n<br />2log(σ) −<br />1<br />2σfTQ−1f + log[p(θ)] + const.<br />Note that σ could be marginalized out, but it would not be possible to get manageable ex-<br />pressions for the metric tensor with respect to τ; for f, instead, this would be possible. We do<br />not pursue this here, and we leave it for future investigation.<br />By inspecting the log-joint density, we see that we can obtain the conditional density for<br />σ in the following form<br />log[p(σ|y,f,τ)] = −n<br />2log(σ) −<br />1<br />2σfTQ−1f + const.<br />which we recognize as an inverse Gamma. By placing an inverse Gamma prior on σ in the<br />form invGa(σ|a,b) with shape a and scale b, we can sample directly:<br />?<br />The gradients of the log-joint density needed to apply gradient based operators are:<br />σ ∼ invGa<br />σ<br />????a +n<br />2,b +1<br />2fTQ−1f<br />?<br />∇flog[p(y,f,θ)] = ∇fL −1<br />?<br />σQ−1f<br />∂ log[p(y,f,θ)]<br />∂ψτi<br />= −1<br />2Tr<br />Q−1∂Q<br />∂ψτi<br />?<br />+<br />1<br />2σfTQ−1∂Q<br />∂ψτi<br />Q−1f +∂ log[p(ψτ)]<br />∂ψτi<br />The FI for latent variables and parameters are:<br />R = FIf,f= Ey<br />?<br />(∇fL)(∇fL)T?<br />= −Ey[∇f∇fL]</p>  <p>Page 22</p> <p>22M. Filippone et al.<br />FIψτ,ψτ= Ef<br />?<br />(∇ψτlog[p(f|ψτ)])(∇ψτlog[p(f|ψτ)])T?<br />Given that the likelihood factorizes with respect to the observations, the Hessian of L with<br />respect to f is diagonal, so R = FIf,fis diagonal as well. The metric tensors are the FI matrices<br />plus the negative Hessian of the priors:<br />Gf,f= R +1<br />σQ−1<br />Gψτi,ψτj= +1<br />2Tr<br />?<br />Q−1∂Q<br />∂ψτj<br />Q−1∂Q<br />∂ψτi<br />?<br />−∂2log[p(ψτ)]<br />∂ψτi∂ψτj<br />A.2 Ancillary Augmentation (AA)<br />We derive here the quantities needed to apply the transition operators considered in this work<br />in the AA parameterization. The expression of the log-joint density is the same as in the SA<br />case, bearing in mind the transformation f =√σLν; this yields:<br />log[p(y,ν,θ)] = L(y|ν,θ) −1<br />2νTν + log[p(θ)] + const.<br />The gradient with respect to the hyper-parameters can be computed by using the chain<br />rule of derivation and standard properties of derivatives of vector valued functions:<br />∂ log[p(y,ν,θ)]<br />∂ψτi<br />=√σ(∇fL(y|f))T∂L<br />∂θi<br />ν +∂ log[p(ψτ)]<br />∂ψτi<br />The FI matrix is readily obtained as:<br />FIθi,θj= σνT∂LT<br />∂θi<br />R∂L<br />∂θj<br />ν<br />With the contribution (negative Hessian) of the prior, the metric tensor used in the manifold<br />methods results in:<br />Gθi,θj= σνT∂LT<br />∂θi<br />R∂L<br />∂θj<br />ν −∂2log[p(θ)]<br />∂θi∂θj<br />B GP models considered in this paper<br />B.1 Logistic regression with GP priors (LRG)<br />Let:<br />l+(f) = logistic(f) =<br />1<br />1 + exp(−f)<br />l−(f) = 1 − l+(f) = logistic(−f)<br />In logistic regression, observations follow a Bernoulli distribution with success probability given<br />by a sigmoid transformation of the associated latent variables:<br />p(y|f) =<br />n<br />?<br />i=1<br />p(yi|fi) =<br />n<br />?<br />i=1<br />Bern(yi|l+(fi)) =<br />n<br />?<br />i=1<br />l+(fi)yil−(fi)(1−yi)<br />The gradient with respect to f results in:<br />(∇fL)j= yj− l+(fj)<br />The computation of diagonal elements of the FI matrix for f requires the expectations of y2<br />which are the same as the expectations of yi, that are l+<br />i<br />i; this leads to Rii= l+(fi)l−(fi).</p>  <p>Page 23</p> <p>Title Suppressed Due to Excessive Length 23<br />B.2 Log-Gaussian Cox model (LCX)<br />In this model, observations follow a Poisson distribution with mean computed as an exponen-<br />tially transformed version of the corresponding latent variables:<br />p(y|f) =<br />n<br />?<br />i=1<br />p(yi|fi) =<br />n<br />?<br />i=1<br />Poisson(yi|exp(fi))<br />The gradient with respect to f and the diagonal elements of R result in:<br />(∇fL)j= yj− exp(fj)Rii= exp(fi)<br />B.3 Stochastic Volatility model with GP priors (VLT)<br />In this model, observations follow a zero mean Gaussian distribution with standard deviation<br />computed as an exponentially transformed version of the corresponding latent variable:<br />p(y|f) =<br />n<br />?<br />i=1<br />p(yi|fi) =<br />n<br />?<br />i=1<br />N(yi|0,exp(fi)2)<br />The gradient with respect to f and the diagonal elements of R result in:<br />(∇fL)j= exp(fi)−2y2<br />j− 1Rii= 2<br />B.4 Ordinal Regression with GP priors (ORD)<br />In this model, latent variables are thresholded at r points that will be denoted by b0,...,br,<br />with b0 = −∞ and br = +∞. Then, y is the index of the interval where the corresponding<br />latent variable f falls. The likelihood of an observed label yiassociated to the ith latent variable<br />fiis then:<br />¯ p(yi|fi) = 1<br />and zero otherwise. This model is usually modified to allow for a noise term δ (distributed as<br />N(δ|0,σ2<br />?<br />where:<br />z(s)<br />i<br />if byi−1&lt; f ≤ byi<br />δ)) in the latent variables so that:<br />p(yi|fi) =¯ p(yi|fi+ δ)N(δ|0,σ2<br />δ)dδ = Φ(z(yi)<br />i<br />) − Φ(z(yi−1)<br />i<br />)<br />=bs− fi<br />σδ<br />In particular:<br />L =<br />n<br />?<br />i=1<br />log<br />?<br />Φ(z(yi)<br />i<br />) − Φ(z(yi−1)<br />i<br />)<br />?<br />(∇fL)i=<br />1<br />σδ<br />N(z(yi−1)<br />Φ(z(yi)<br />i<br />|0,1) − N(z(yi)<br />) − Φ(z(yi−1)<br />i<br />|0,1)<br />)<br />i<br />i<br />By writing the diagonal elements of Hessian of the log-likelihood computed for yi= s<br />(∇f∇fL)(s)<br />ii<br />=<br />1<br />σ2<br />δ<br />z(s)<br />i<br />N(z(s)<br />i<br />|0,1) − z(s−1)<br />Φ(z(s)<br />i<br />i<br />N(z(s−1)<br />)<br />i<br />|0,1)<br />) − Φ(z(s−1)<br />i<br />−1<br />σ2<br />δ<br />?<br />N(z(s−1)<br />Φ(z(s)<br />i<br />|0,1) − N(z(s)<br />) − Φ(z(s−1)<br />i<br />|0,1)<br />)<br />ii<br />?2<br />it is possible to compute the expectation of the negative Hessian as:<br />Rii= −<br />r<br />?<br />s=1<br />(∇f∇fL)(s)<br />iip(s|fi) =<br />r<br />?<br />s=1<br />(∇f∇fL)(s)<br />ii<br />?<br />Φ(z(s−1)<br />i<br />) − Φ(z(s)<br />i<br />)<br />?<br />Note that the formulation in this paper is slightly different from the one in Chu and Ghahra-<br />mani (2005), where σ is dropped and thresholds are inferred instead.</p>  <a href="https://www.researchgate.net/profile/Maurizio_Filippone/publication/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models/links/00463533941d28ab5b000000.pdf">Download full-text</a> </div> <div id="rgw21_56ab1d7fd7391" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw22_56ab1d7fd7391">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw23_56ab1d7fd7391"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Maurizio_Filippone/publication/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models/links/00463533941d28ab5b000000.pdf" class="publication-viewer" title="ml13.pdf">ml13.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Maurizio_Filippone">Maurizio Filippone</a> &middot; May 23, 2014 </span>   </div>   <div class="details"> Available from <a href="http://dx.doi.org/10.1007/s10994-013-5388-x" target="_blank" rel="nofollow">10.1007/s10994-013-5388-x</a> </div>   </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw30_56ab1d7fd7391" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (9) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw31_56ab1d7fd7391" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw32_56ab1d7fd7391" >  <div class="indent-left">  <div id="rgw33_56ab1d7fd7391" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Vasek_Smidl" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Vasek Smidl </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw34_56ab1d7fd7391">  <li class="citation-context-item"> "This paper focuses on the problem of inferring GP covariance parameters when the marginal likelihood is computable but expensive, namely when GPs are used for regression and the likelihood is Gaussian. To date, most of the literature proposes the use of Markov chain Monte Carlo (MCMC) methods to characterize the posterior distribution over covariance parameters [36] [34] [14] [29]. Generally, MCMC methods are based on the iteration of the following two operations (i) proposal and (ii) an accept/reject step. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes"> <span class="publication-title js-publication-title">Adaptive Multiple Importance Sampling for Gaussian Processes</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2079162962_Xiaoyu_Xiong" class="authors js-author-name ga-publications-authors">Xiaoyu Xiong</a> &middot;     <a href="researcher/82161005_Vaclav_Smidl" class="authors js-author-name ga-publications-authors">Václav Šmídl</a> &middot;     <a href="researcher/70871340_Maurizio_Filippone" class="authors js-author-name ga-publications-authors">Maurizio Filippone</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> In applications of Gaussian processes where quantification of uncertainty is
a strict requirement, it is necessary to accurately characterize the posterior
distribution over Gaussian process covariance parameters. Normally, this is
done by means of Markov chain Monte Carlo (MCMC) algorithms. Focusing on
Gaussian process regression where the marginal likelihood is computable but
expensive to evaluate, this paper studies algorithms based on importance
sampling to carry out expectations under the posterior distribution over
covariance parameters. The results indicate that expectations computed using
Adaptive Multiple Importance Sampling converge faster per unit of computation
than those computed with MCMC algorithms for models with few covariance
parameters, and converge as fast as MCMC for models with up to around twenty
covariance parameters. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Aug 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Vasek_Smidl/publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes/links/55e94dff08ae65b6389aee89.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw35_56ab1d7fd7391" >  <div class="indent-left">  <div id="rgw36_56ab1d7fd7391" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/262954130_Pseudo-Marginal_Bayesian_Inference_for_Gaussian_Processes">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Maurizio_Filippone" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Maurizio Filippone </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw37_56ab1d7fd7391">  <li class="citation-context-item"> "Recently, there have been a few attempts to carry out inference using stochastic approximations based on Markov chain Monte Carlo (MCMC) methods [21], [22], [23], the idea being to leverage asymptotic guarantees of convergence of Monte Carlo estimates to the true values. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/262954130_Pseudo-Marginal_Bayesian_Inference_for_Gaussian_Processes"> <span class="publication-title js-publication-title">Pseudo-Marginal Bayesian Inference for Gaussian Processes</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/70871340_Maurizio_Filippone" class="authors js-author-name ga-publications-authors">Maurizio Filippone</a> &middot;     <a href="researcher/19524381_Mark_Girolami" class="authors js-author-name ga-publications-authors">Mark Girolami</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> The main challenges that arise when adopting Gaussian process priors in probabilistic modeling are how to carry out exact Bayesian inference and how to account for uncertainty on model parameters when making model-based predictions on out-of-sample data. Using probit regression as an illustrative working example, this paper presents a general and effective methodology based on the pseudo-marginal approach to Markov chain Monte Carlo that efficiently addresses both of these issues. The results presented in this paper show improvements over existing sampling methods to simulate from the posterior distribution over the parameters defining the covariance function of the Gaussian Process prior. This is particularly important as it offers a powerful tool to carry out full Bayesian inference of Gaussian Process based hierarchic statistical models in general. The results also demonstrate that Monte Carlo based integration of all model parameters is actually feasible in this class of models providing a superior quantification of uncertainty in predictions. Extensive comparisons with respect to state-of-the-art probabilistic classifiers confirm this assertion. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Nov 2014  &middot; IEEE Transactions on Pattern Analysis and Machine Intelligence  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Maurizio_Filippone/publication/262954130_Pseudo-Marginal_Bayesian_Inference_for_Gaussian_Processes/links/02e7e539706dd57b1b000000.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw38_56ab1d7fd7391" >  <div class="indent-left">  <div id="rgw39_56ab1d7fd7391" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/259151452_Scalable_iterative_methods_for_sampling_from_massive_Gaussian_random_vectors">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Anthony_Pettitt" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Anthony N. Pettitt </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw40_56ab1d7fd7391">  <li class="citation-context-item"> "The preconditioners considered in this paper are closely linked to the concept of &#39;centred&#39; and &#39;noncentred&#39; parameteristations of statistical models (Papaspiliopoulos et al., 2007; Strickland et al., 2008; Yu and Meng, 2011; Filippone et al., 2013). The idea can be illustrated simply for parameterdependent latent Gaussian models " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/259151452_Scalable_iterative_methods_for_sampling_from_massive_Gaussian_random_vectors"> <span class="publication-title js-publication-title">Scalable iterative methods for sampling from massive Gaussian random vectors</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/59251560_Daniel_P_Simpson" class="authors js-author-name ga-publications-authors">Daniel P. Simpson</a> &middot;     <a href="researcher/21587887_Ian_W_Turner" class="authors js-author-name ga-publications-authors">Ian W. Turner</a> &middot;     <a href="researcher/18683691_Christopher_M_Strickland" class="authors js-author-name ga-publications-authors">Christopher M. Strickland</a> &middot;     <a href="researcher/10428941_Anthony_N_Pettitt" class="authors js-author-name ga-publications-authors">Anthony N. Pettitt</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Sampling from Gaussian Markov random fields (GMRFs), that is multivariate
Gaussian ran- dom vectors that are parameterised by the inverse of their
covariance matrix, is a fundamental problem in computational statistics. In
this paper, we show how we can exploit arbitrarily accu- rate approximations to
a GMRF to speed up Krylov subspace sampling methods. We also show that these
methods can be used when computing the normalising constant of a large
multivariate Gaussian distribution, which is needed for both any
likelihood-based inference method. The method we derive is also applicable to
other structured Gaussian random vectors and, in particu- lar, we show that
when the precision matrix is a perturbation of a (block) circulant matrix, it
is still possible to derive O(n log n) sampling schemes. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Dec 2013  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Anthony_Pettitt/publication/259151452_Scalable_iterative_methods_for_sampling_from_massive_Gaussian_random_vectors/links/543faabf0cf2fd72f99c877f.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw25_56ab1d7fd7391" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw26_56ab1d7fd7391">  </ul> </div> </div>   <div id="rgw17_56ab1d7fd7391" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw18_56ab1d7fd7391"> <div> <h5> <a href="publication/291952666_Estimation_and_uncertainty_analysis_of_dose_response_in_an_inter-laboratory_experiment" class="color-inherit ga-similar-publication-title"><span class="publication-title">Estimation and uncertainty analysis of dose response in an inter-laboratory experiment</span></a>  </h5>  <div class="authors"> <a href="researcher/2095780451_Blaza_Toman" class="authors ga-similar-publication-author">Blaza Toman</a>, <a href="researcher/78359712_Matthias_Roesslein" class="authors ga-similar-publication-author">Matthias Rösslein</a>, <a href="researcher/39153865_John_T_Elliott" class="authors ga-similar-publication-author">John T Elliott</a>, <a href="researcher/38988484_Elijah_J_Petersen" class="authors ga-similar-publication-author">Elijah J Petersen</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56ab1d7fd7391"> <div> <h5> <a href="publication/283763151_Incorporation_of_bomb-produced_14C_into_fish_otoliths_A_novel_approach_for_evaluating_age_validation_and_bias_with_an_application_to_yellowfin_sole_and_northern_rockfish" class="color-inherit ga-similar-publication-title"><span class="publication-title">Incorporation of bomb-produced 14C into fish otoliths: A novel approach for evaluating age validation and bias with an application to yellowfin sole and northern rockfish</span></a>  </h5>  <div class="authors"> <a href="researcher/72268011_Craig_R_Kastelle" class="authors ga-similar-publication-author">Craig R. Kastelle</a>, <a href="researcher/74199780_Thomas_E_Helser" class="authors ga-similar-publication-author">Thomas E. Helser</a>, <a href="researcher/2053721311_Stephen_G_Wischniowski" class="authors ga-similar-publication-author">Stephen G. Wischniowski</a>, <a href="researcher/2009959056_Timothy_Loher" class="authors ga-similar-publication-author">Timothy Loher</a>, <a href="researcher/2084446468_Betty_J_Goetz" class="authors ga-similar-publication-author">Betty J. Goetz</a>, <a href="researcher/2084443070_Lisa_A_Kautzi" class="authors ga-similar-publication-author">Lisa A. Kautzi</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw20_56ab1d7fd7391"> <div> <h5> <a href="publication/290527223_Optimizing_spinning_time-domain_gravitational_waveforms_for_Advanced_LIGO_data_analysis" class="color-inherit ga-similar-publication-title"><span class="publication-title">Optimizing spinning time-domain gravitational waveforms for Advanced LIGO data analysis</span></a>  </h5>  <div class="authors"> <a href="researcher/2094140965_Caleb_Devine" class="authors ga-similar-publication-author">Caleb Devine</a>, <a href="researcher/2094050681_Zachariah_B_Etienne" class="authors ga-similar-publication-author">Zachariah B. Etienne</a>, <a href="researcher/2094090056_Sean_T_McWilliams" class="authors ga-similar-publication-author">Sean T. McWilliams</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw50_56ab1d7fd7391" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw51_56ab1d7fd7391">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw52_56ab1d7fd7391" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=qEm6MrICDwwmww44HvboxUdu3KGk-b7DgJzTxMM_iWGD4e0zyBxFIq9Z0yjVmKGb" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="uxdmyc3scNRMoJgtXnJtdZW3IqbbS/3tpRdPWDrRXvFSiTp9/Jzck3hoTCUUPo7VQMiFXe2IMM7SxMr+Sk+xWsrjsliVwsj7iuJVhwV5FOHeWOd/M/zEfj6s/ENZ8TnbNttKmjLTQarcpJ61uR8N1LuOHjJh5ohxE0BR0OrpjHeyk02RIXygu3Wfb8/xS4skAW6PE7PXE+uMellQJp8/FR+U4xPFt8Mga3iKuj6d1z0YRIQCAQlSEc6BpofsWz+ll5BSEGx0ccx7+v0LywazuxKUpE9Tor1efB++zxf6hiM="/> <input type="hidden" name="urlAfterLogin" value="publication/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjU3NjE4NDYwX0FfY29tcGFyYXRpdmVfZXZhbHVhdGlvbl9vZl9zdG9jaGFzdGljLWJhc2VkX2luZmVyZW5jZV9tZXRob2RzX2Zvcl9HYXVzc2lhbl9wcm9jZXNzX21vZGVscw%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjU3NjE4NDYwX0FfY29tcGFyYXRpdmVfZXZhbHVhdGlvbl9vZl9zdG9jaGFzdGljLWJhc2VkX2luZmVyZW5jZV9tZXRob2RzX2Zvcl9HYXVzc2lhbl9wcm9jZXNzX21vZGVscw%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjU3NjE4NDYwX0FfY29tcGFyYXRpdmVfZXZhbHVhdGlvbl9vZl9zdG9jaGFzdGljLWJhc2VkX2luZmVyZW5jZV9tZXRob2RzX2Zvcl9HYXVzc2lhbl9wcm9jZXNzX21vZGVscw%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw53_56ab1d7fd7391"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 905;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Maurizio Filippone","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273666076311560%401442258485613_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Maurizio_Filippone","institution":null,"institutionUrl":false,"widgetId":"rgw4_56ab1d7fd7391"},"id":"rgw4_56ab1d7fd7391","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=4709876","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab1d7fd7391"},"id":"rgw3_56ab1d7fd7391","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=257618460","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":257618460,"title":"A comparative evaluation of stochastic-based inference methods for Gaussian process models","journalTitle":"Machine Learning","journalDetailsTooltip":{"data":{"journalTitle":"Machine Learning","journalAbbrev":"MACH LEARN","publisher":"Springer Verlag","issn":"0885-6125","impactFactor":"1.89","fiveYearImpactFactor":"2.64","citedHalfLife":">10.0","immediacyIndex":"0.21","eigenFactor":"0.01","articleInfluence":"1.71","widgetId":"rgw6_56ab1d7fd7391"},"id":"rgw6_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=0885-6125","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Article","details":{"doi":"10.1007\/s10994-013-5388-x","journalInfos":{"journal":"","publicationDate":"10\/2013;","publicationDateRobot":"2013-10","article":"93(1).","journalTitle":"Machine Learning","journalUrl":"journal\/0885-6125_Machine_Learning","impactFactor":1.89}},"source":false,"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1007\/s10994-013-5388-x"},{"key":"rft.atitle","value":"A comparative evaluation of stochastic-based inference methods for Gaussian process models"},{"key":"rft.title","value":"Machine Learning"},{"key":"rft.jtitle","value":"Machine Learning"},{"key":"rft.volume","value":"93"},{"key":"rft.issue","value":"1"},{"key":"rft.date","value":"2013"},{"key":"rft.issn","value":"0885-6125"},{"key":"rft.au","value":"M. Filippone,M. Zhong,M. Girolami"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56ab1d7fd7391"},"id":"rgw7_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=257618460","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":257618460,"peopleItems":[{"data":{"authorNameOnPublication":"Maurizio Filippone","accountUrl":"profile\/Maurizio_Filippone","accountKey":"Maurizio_Filippone","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273666076311560%401442258485613_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Maurizio Filippone","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":false}},"professionalInstitutionName":false,"professionalInstitutionUrl":false,"url":"profile\/Maurizio_Filippone","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273666076311560%401442258485613_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Maurizio_Filippone","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw10_56ab1d7fd7391"},"id":"rgw10_56ab1d7fd7391","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=4709876&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":false,"score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":3,"publicationUid":257618460,"widgetId":"rgw9_56ab1d7fd7391"},"id":"rgw9_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=4709876&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=3&publicationUid=257618460","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Mingjun Zhong","accountUrl":"profile\/Mingjun_Zhong","accountKey":"Mingjun_Zhong","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272395789402145%401441955626037_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Mingjun Zhong","profile":{"professionalInstitution":{"professionalInstitutionName":"Dalian University of Technology","professionalInstitutionUrl":"institution\/Dalian_University_of_Technology"}},"professionalInstitutionName":"Dalian University of Technology","professionalInstitutionUrl":"institution\/Dalian_University_of_Technology","url":"profile\/Mingjun_Zhong","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272395789402145%401441955626037_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Mingjun_Zhong","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw12_56ab1d7fd7391"},"id":"rgw12_56ab1d7fd7391","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1862268&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Dalian University of Technology","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":3,"publicationUid":257618460,"widgetId":"rgw11_56ab1d7fd7391"},"id":"rgw11_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1862268&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=3&publicationUid=257618460","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Mark Girolami","accountUrl":"profile\/Mark_Girolami","accountKey":"Mark_Girolami","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Mark Girolami","profile":{"professionalInstitution":{"professionalInstitutionName":"University College London","professionalInstitutionUrl":"institution\/University_College_London"}},"professionalInstitutionName":"University College London","professionalInstitutionUrl":"institution\/University_College_London","url":"profile\/Mark_Girolami","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Mark_Girolami","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw14_56ab1d7fd7391"},"id":"rgw14_56ab1d7fd7391","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1816679&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University College London","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":3,"publicationUid":257618460,"widgetId":"rgw13_56ab1d7fd7391"},"id":"rgw13_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1816679&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=3&publicationUid=257618460","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab1d7fd7391"},"id":"rgw8_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=257618460&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":257618460,"abstract":"<noscript><\/noscript><div>Gaussian Process (GP) models are extensively used in data analysis given their flexible modeling capabilities and interpretability. The fully Bayesian treatment of GP models is analytically intractable, and therefore it is necessary to resort to either deterministic or stochastic approximations. This paper focuses on stochastic-based inference techniques. After discussing the challenges associated with the fully Bayesian treatment of GP models, a number of inference strategies based on Markov chain Monte Carlo methods are presented and rigorously assessed. In particular, strategies based on efficient parameterizations and efficient proposal mechanisms are extensively compared on simulated and real data on the basis of convergence speed, sampling efficiency, and computational cost.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw15_56ab1d7fd7391"},"id":"rgw15_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=257618460","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models\/links\/00463533941d28ab5b000000\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw16_56ab1d7fd7391"},"id":"rgw16_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab1d7fd7391"},"id":"rgw5_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=257618460&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2095780451,"url":"researcher\/2095780451_Blaza_Toman","fullname":"Blaza Toman","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":78359712,"url":"researcher\/78359712_Matthias_Roesslein","fullname":"Matthias R\u00f6sslein","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39153865,"url":"researcher\/39153865_John_T_Elliott","fullname":"John T Elliott","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38988484,"url":"researcher\/38988484_Elijah_J_Petersen","fullname":"Elijah J Petersen","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2016","journal":"Metrologia","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291952666_Estimation_and_uncertainty_analysis_of_dose_response_in_an_inter-laboratory_experiment","usePlainButton":true,"publicationUid":291952666,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.04","url":"publication\/291952666_Estimation_and_uncertainty_analysis_of_dose_response_in_an_inter-laboratory_experiment","title":"Estimation and uncertainty analysis of dose response in an inter-laboratory experiment","displayTitleAsLink":true,"authors":[{"id":2095780451,"url":"researcher\/2095780451_Blaza_Toman","fullname":"Blaza Toman","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":78359712,"url":"researcher\/78359712_Matthias_Roesslein","fullname":"Matthias R\u00f6sslein","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39153865,"url":"researcher\/39153865_John_T_Elliott","fullname":"John T Elliott","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38988484,"url":"researcher\/38988484_Elijah_J_Petersen","fullname":"Elijah J Petersen","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Metrologia 02\/2016; 53(1):S40-S45. DOI:10.1088\/0026-1394\/53\/1\/S40"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291952666_Estimation_and_uncertainty_analysis_of_dose_response_in_an_inter-laboratory_experiment","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291952666_Estimation_and_uncertainty_analysis_of_dose_response_in_an_inter-laboratory_experiment\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab1d7fd7391"},"id":"rgw18_56ab1d7fd7391","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291952666","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":72268011,"url":"researcher\/72268011_Craig_R_Kastelle","fullname":"Craig R. Kastelle","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":74199780,"url":"researcher\/74199780_Thomas_E_Helser","fullname":"Thomas E. Helser","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2053721311,"url":"researcher\/2053721311_Stephen_G_Wischniowski","fullname":"Stephen G. Wischniowski","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2009959056,"url":"researcher\/2009959056_Timothy_Loher","fullname":"Timothy Loher","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":2,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Ecological Modelling","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283763151_Incorporation_of_bomb-produced_14C_into_fish_otoliths_A_novel_approach_for_evaluating_age_validation_and_bias_with_an_application_to_yellowfin_sole_and_northern_rockfish","usePlainButton":true,"publicationUid":283763151,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.32","url":"publication\/283763151_Incorporation_of_bomb-produced_14C_into_fish_otoliths_A_novel_approach_for_evaluating_age_validation_and_bias_with_an_application_to_yellowfin_sole_and_northern_rockfish","title":"Incorporation of bomb-produced 14C into fish otoliths: A novel approach for evaluating age validation and bias with an application to yellowfin sole and northern rockfish","displayTitleAsLink":true,"authors":[{"id":72268011,"url":"researcher\/72268011_Craig_R_Kastelle","fullname":"Craig R. Kastelle","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":74199780,"url":"researcher\/74199780_Thomas_E_Helser","fullname":"Thomas E. Helser","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2053721311,"url":"researcher\/2053721311_Stephen_G_Wischniowski","fullname":"Stephen G. Wischniowski","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2009959056,"url":"researcher\/2009959056_Timothy_Loher","fullname":"Timothy Loher","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084446468,"url":"researcher\/2084446468_Betty_J_Goetz","fullname":"Betty J. Goetz","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084443070,"url":"researcher\/2084443070_Lisa_A_Kautzi","fullname":"Lisa A. Kautzi","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Ecological Modelling 01\/2016; 320:79-91. DOI:10.1016\/j.ecolmodel.2015.09.013"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283763151_Incorporation_of_bomb-produced_14C_into_fish_otoliths_A_novel_approach_for_evaluating_age_validation_and_bias_with_an_application_to_yellowfin_sole_and_northern_rockfish","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283763151_Incorporation_of_bomb-produced_14C_into_fish_otoliths_A_novel_approach_for_evaluating_age_validation_and_bias_with_an_application_to_yellowfin_sole_and_northern_rockfish\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab1d7fd7391"},"id":"rgw19_56ab1d7fd7391","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=283763151","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2094140965,"url":"researcher\/2094140965_Caleb_Devine","fullname":"Caleb Devine","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2094050681,"url":"researcher\/2094050681_Zachariah_B_Etienne","fullname":"Zachariah B. Etienne","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2094090056,"url":"researcher\/2094090056_Sean_T_McWilliams","fullname":"Sean T. McWilliams","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/290527223_Optimizing_spinning_time-domain_gravitational_waveforms_for_Advanced_LIGO_data_analysis","usePlainButton":true,"publicationUid":290527223,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/290527223_Optimizing_spinning_time-domain_gravitational_waveforms_for_Advanced_LIGO_data_analysis","title":"Optimizing spinning time-domain gravitational waveforms for Advanced LIGO data analysis","displayTitleAsLink":true,"authors":[{"id":2094140965,"url":"researcher\/2094140965_Caleb_Devine","fullname":"Caleb Devine","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2094050681,"url":"researcher\/2094050681_Zachariah_B_Etienne","fullname":"Zachariah B. Etienne","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2094090056,"url":"researcher\/2094090056_Sean_T_McWilliams","fullname":"Sean T. McWilliams","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/290527223_Optimizing_spinning_time-domain_gravitational_waveforms_for_Advanced_LIGO_data_analysis","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/290527223_Optimizing_spinning_time-domain_gravitational_waveforms_for_Advanced_LIGO_data_analysis\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56ab1d7fd7391"},"id":"rgw20_56ab1d7fd7391","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=290527223","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw17_56ab1d7fd7391"},"id":"rgw17_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=257618460&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":257618460,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":257618460,"publicationType":"article","linkId":"00463533941d28ab5b000000","fileName":"ml13.pdf","fileUrl":"profile\/Maurizio_Filippone\/publication\/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models\/links\/00463533941d28ab5b000000.pdf","name":"Maurizio Filippone","nameUrl":"profile\/Maurizio_Filippone","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":"10.1007\/s10994-013-5388-x","hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"publisherLink":"http:\/\/dx.doi.org\/10.1007\/s10994-013-5388-x","publisherLinkDisplay":"10.1007\/s10994-013-5388-x","isUserLink":true,"uploadDate":"May 23, 2014","fileSize":"376.45 KB","widgetId":"rgw23_56ab1d7fd7391"},"id":"rgw23_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=257618460&linkId=00463533941d28ab5b000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw22_56ab1d7fd7391"},"id":"rgw22_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=257618460&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":52,"valueFormatted":"52","widgetId":"rgw24_56ab1d7fd7391"},"id":"rgw24_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=257618460","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw21_56ab1d7fd7391"},"id":"rgw21_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=257618460&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":257618460,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw26_56ab1d7fd7391"},"id":"rgw26_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=257618460&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":52,"valueFormatted":"52","widgetId":"rgw27_56ab1d7fd7391"},"id":"rgw27_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=257618460","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw25_56ab1d7fd7391"},"id":"rgw25_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=257618460&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Noname manuscript No.\n(will be inserted by the editor)\nA Comparative Evaluation of Stochastic-based\nInference Methods for Gaussian Process Models\nM. Filippone \u00b7 M. Zhong \u00b7 M. Girolami\nReceived: date \/ Accepted: date\nAbstract Gaussian Process models are extensively used in data analysis given\ntheir flexible modeling capabilities and interpretability. The fully Bayesian treat-\nment of GP models is analytically intractable, and therefore it is necessary to\nresort to either deterministic or stochastic approximations. This paper focuses on\nstochastic-based inference techniques. First, challenges associated with the fully\nBayesian treatment of GP models are discussed, and then a number of inference\nstrategies based on Markov chain Monte Carlo methods are presented and rigor-\nously assessed. In particular, strategies based on efficient parameterizations and\nefficient proposal mechanisms are extensively compared on simulated and real data\non the basis of speed of convergence, sampling efficiency, and computational cost.\nKeywords Bayesian inference \u00b7 Gaussian Processes \u00b7 Markov chain Monte Carlo \u00b7\nhierarchical models \u00b7 latent variable models\n1 Introduction\nGaussian Process (GP) models represent a class of models that is fairly popular in\ndata analysis due to the associated flexibility and interpretability. Both those fea-\ntures are a direct consequence of their rich parameterization. Flexibility is due to\nthe nonparametric prior over latent variables conditioning observations, whereas\ninterpretability is due to the parameterization of the structure associated with the\nlatent variables. Observations are conditionally independent given a set of jointly\nMaurizio Filippone\nSchool of Computing Science, University of Glasgow, United Kingdom.\nE-mail: maurizio.filippone@glasgow.ac.uk\nMingjun Zhong\nDepartment of Biomedical Engineering, Dalian University of Technology, P.R. China\nE-mail: mingjun.zhong@gmail.com\nMark Girolami\nDepartment of Statistical Science, University College London, United Kingdom.\nE-mail: girolami@stats.ucl.ac.uk"},{"page":2,"text":"2 M. Filippone et al.\nGaussian latent variables, and are assumed to be distributed according to the par-\nticular type of data being modeled. The covariance structure of the latent variables\nis then parameterized by a set of (hyper)-parameters that characterizes the covari-\nance of the input vectors in terms of length-scales and intensity of interaction. GP\nmodels comprise a large set of models, and this paper focuses in particular on\nLogistic Regression with GP priors (LRG) (Rasmussen and Williams, 2006), Log-\nGaussian Cox model (LCX) (M\u00f8ller et al., 1998), Stochastic Volatility model with\nGP priors (VLT) (Wilson and Ghahramani, 2010), and Ordinal Regression with\nGP priors (ORD) (Chu and Ghahramani, 2005).\nExact inference in GP models is analytically intractable. Most of the work to\ntackle such an intractability focuses on deterministic approximations to integrate\nout latent variables; those approaches include the Laplace Approximation (LA)\n(Tierney and Kadane, 1986), Expectation Propagation (EP) (Minka, 2001), and\nmean field approximations (Opper and Winther, 2000) (see, e.g., Rasmussen and\nWilliams (2006) for an extensive presentation of those approximations and Kuss\nand Rasmussen (2005) for their assessment on LRG models). Those approximations\nprovide a computationally tractable way to integrate out latent variables, but it\nis not possible to quantify the error that those approximations introduce in the\nquantification of uncertainty in predictions (although EP for LRG is reported to\nbe very accurate in Kuss and Rasmussen (2005)); also, those methods target the\nintegration of latent variables only.\nIn the direction of providing a fully Bayesian treatment of GP models, it is\nnecessary to integrate out latent variables as well as hyper-parameters, and this is\nusually done by quadrature methods (Cseke and Heskes, 2011; Rue et al., 2009),\nthus limiting the number of hyper-parameters that can be employed in GP models.\nBased on those considerations, this paper focuses on non-deterministic meth-\nods to carry out inference in GP models, and in particular on stochastic based\napproximations based Markov Chain Monte Carlo (MCMC) methods. The use of\nMCMC based inference methods is appealing as it provides asymptotic guarantees\nof convergence to exact inference. In practice, this translates into the possibility\nof achieving results with the desired level of accuracy (Flegal et al., 2007). Un-\nfortunately, the use of MCMC methods for inference in GP models is extremely\ndifficult; the aim of this paper is to discuss the challenges associated with MCMC\nbased inference for GP models, and compare a number of strategies that have been\nproposed in the literature to tackle them. A preliminary version of this work can\nbe found in Filippone et al. (2012)1.\nTo the best of our knowledge, this work (i) is the first attempt to extensively\nassess the state-of-the-art in stochastic-based inference methods for GP models,\nand (ii) sets the bar for new MCMC methods for inference in GP models. Along\nwith those contribution, this paper presents (iii) a variant of the Hybrid Monte\nCarlo algorithm that outperforms state-of-the-art methods to sample from the\nposterior distribution of the latent variables, and (iv) tests the combination of\nparameterizations, as recently proposed in Yu and Meng (2011), in the case of GP\nmodels.\n1An implementation of the methods considered in this paper can be found at:\nhttp:\/\/www.dcs.gla.ac.uk\/~maurizio\/pages\/code.html"},{"page":3,"text":"Title Suppressed Due to Excessive Length3\n1.1 Gaussian Process Models\nLet X = {x1,...,xn} be a set of n input vectors described by a set of d covariates\nxi\u2208 Rd, associated with observed responses y = {y1,...,yn}. In GP models, the\ngenerative process modeling the observed data y given X is as follows. Obser-\nvations are assumed conditionally independent given a set of n latent variables\nf = {f1,...,fn}, and distributed according to a certain distribution depending on\nthe particular type of data, e.g., Bernoulli for binary labels and Poisson for obser-\nvations in the form of counts. This can be translated into a likelihood function of\nthe form p(y|f) =?n\nIn this work, latent variables are assumed to be drawn from a zero mean GP\nprior with covariance function k. The GP prior is a prior over functions, and the\ncovariance structure given by k specifies the characteristics of such functions (i.e.,\ndegree of smoothness and marginal variance). Let k be parameterized by a vector\nof (hyper)-parameters \u03b8 = (\u03c3,\u03c8\u03c41,...,\u03c8\u03c4d), and assume:\ni=1p(yi|fi), where for generality the distribution p(yi|fi) is\nleft unspecified.\nk(xi,xj|\u03b8) = \u03c3q(xi,xj|\u03c8\u03c4) = \u03c3 exp\n?\n\u22121\n2\nd\n?\nr=1\n(xi\u2212 xj)2\nexp(\u03c8\u03c4r)2\n(r)\n?\n(1)\nwith exp(\u03c8\u03c4r) defining the length-scale of the interaction between the input vectors\nfor the rth covariate and \u03c3 giving the marginal variance for latent variables. This\ntype of covariance can be used for Automatic Relevance Determination (ARD)\n(Mackay, 1994) of the covariates, as the values \u03c4i = exp(\u03c8\u03c4i) can be interpreted\nas length-scale parameters. This definition of covariance function is adopted in\nmany applications and is the one we will consider in the remainder of this paper.\nExponentiation of the hyper-parameters is convenient, so that standard MCMC\ntransition operators can be employed for \u03c8\u03c4ithus avoiding dealing with boundary\nconditions or non-standard MCMC proposals (Robert and Casella, 2005). Let\nQ be the matrix whose entries are qij = q(xi,xj|\u03c8\u03c4); the covariance matrix K\nwill then be K = \u03c3Q. The model is fully specified by choosing a prior p(\u03b8) for\nthe hyper-parameters. The model structure is therefore hierarchical, with hyper-\nparameters conditioning the latent variables that, in turn, condition observations,\nso that p(y,f,\u03b8) = p(y|f)p(f|\u03b8)p(\u03b8).\nIn a Bayesian setting, the predictive distribution for new input values x\u2217 can\nbe written in the following way (for the sake of clarity we drop the explicit condi-\ntioning on X and x\u2217):\n? ? ?\nThe left hand side of Eq. 2 is a full probability distribution characterizing the\nuncertainty in predicting y\u2217 given the GP modeling assumption.\nIn this work we will focus on stochastic approximations for obtaining samples\nfrom the posterior distribution of f and \u03b8, so that we can obtain a Monte Carlo\nestimate of the predictive distribution as follows:\np(y\u2217|y) =p(y\u2217|f\u2217)p(f\u2217|f,\u03b8)p(f,\u03b8|y)df\u2217dfd\u03b8(2)\np(y\u2217|y) ?1\nN\nN\n?\ni=1\n?\np(y\u2217|f\u2217)p(f\u2217|f(i),\u03b8(i))df\u2217\n(3)"},{"page":4,"text":"4 M. Filippone et al.\nwhere N denotes the number of samples used to compute the estimate. In Eq. 3\nwe denoted the ith samples from the posterior distribution of f and \u03b8 obtained\nby means of MCMC methods by f(i)and \u03b8(i). Note that the remaining integral is\nunivariate and it is generally easy to evaluate.\n1.2 Challenges in MCMC based inference for GP models\nSampling from the posterior of latent variables and hyper-parameters by joint\nproposals is not feasible; it is extremely unlikely to propose a set of latent variables\nand hyper-parameters that are compatible with each other and observed data. This\nforces one to consider Gibbs sampling types of schemes, where groups of variables\nare updated one at time, leading to the following challenges:\n(i) Due to the hierarchical structure of GP models, chains converge slowly and\nmix poorly if the coupling effect between the groups of variables is not dealt with\nproperly. This requires some form of reparameterization or clever proposal mech-\nanism that efficiently decouples the dependencies between the groups of variables.\nThis effect has drawn a lot of attention in the case of hierarchical models in gen-\neral (Yu and Meng, 2011), and recently in GP models Knorr-Held and Rue (2002);\nMurray and Adams (2010). In Knorr-Held and Rue (2002) a joint update of latent\nvariables and hyper-parameters is proposed with the aim of avoiding proposals for\nhyper-parameters to be conditioned on the values of latent variables. In Murray\nand Adams (2010) a parameterization based on auxiliary data is proposed that\naims at reducing the coupling between the two groups of variables. Other ideas\ninvolve the use of reparameterizations based on whitening the latent variables;\nin the terminology of Yu and Meng (2011), this corresponds to employing the so\ncalled Ancillary Augmentation (AA) parameterization. Recently, Yu and Meng\n(2011) proposed to interweave parameterizations characterized by complementary\nfeatures in order to boost sampling efficiency. Parameterizations can be comple-\nmentary in the sense that they offer better performances in either strong or weak\ndata limits; the idea of combining parameterizations is to achieve high sampling\nefficiency in both strong and weak data scenarios. We are interested in comparing\nthe methods in Knorr-Held and Rue (2002); Murray and Adams (2010) and Yu\nand Meng (2011) applied to GP models. Another possibility would be to approxi-\nmately integrate out latent variables and obtain samples from the corresponding\napproximate posterior of hyper-parameters. For GP classification this might be\na sensible thing to do, as the Expectation Propagation approximation has been\nreported to be very accurate; however, this is peculiar to GP classification and for\ngeneral GP models it may not be the case.\n(ii) Sampling hyper-parameters and latent variables cannot be done using ex-\nact Gibbs steps, and it requires proposals that are accepted\/rejected based on a\nHastings ratio, leading to a waste of expensive computations. Transition operators\ncharacterized by acceptance mechanisms embedded in a Gibbs sampler, are usu-\nally referred to as Metropolis-within-Gibbs operators. Designing proposals that\nguarantee high acceptance and independence between samples is extremely chal-\nlenging, especially because latent variables can have dimensions in the order of\nhundreds or thousands. We will compare several transition operators, for different\nsteps of the Gibbs sampler, with the aim of gaining insights about ways to strike a\ngood balance between efficiency and computational cost. We will consider transi-"},{"page":5,"text":"Title Suppressed Due to Excessive Length5\ntion operators characterized by proposal mechanisms with increasing complexity,\nand in particular the Metropolis-Hastings (MH) operator which is based on ran-\ndom walk types of proposals, Hybrid Monte Carlo (HMC) which uses the gradient\nof the log-density of interest, and manifold methods (Girolami and Calderhead,\n2011) which use curvature information (i.e., second derivatives of the log-density).\nThe paper is organized as follows: Sections 2 and 3 report the parameterization\nstrategies and the transition operators considered in this work. Sections 4 and 5\nreport an extensive comparison of those strategies and transition operators, on\nsimulated and real data, on the basis of efficiency, speed of convergence and com-\nputational complexity; section 6 concludes the paper. For the sake of readability,\nmost of the technical derivations can be found in the appendices.\n2 Dealing with the hierarchical structure of GP models\n2.1 Sufficient and Ancillary Augmentation\nFrom a generative perspective, the model structure is hierarchical with latent\nvariables representing a sufficient statistics for the hyper-parameters. This param-\neterization is referred to as Sufficient Augmentation (SA) in Yu and Meng (2011)\nand allows one to express the joint density as\nSAp(y,f,\u03b8) = p(y|f)p(f|\u03b8)p(\u03b8)\nIt is also possible to introduce the decomposition of the matrix Q into the\nproduct of two factors LLT, and view the generation of the latent variables as\nf =\u221a\u03c3L\u03bd with \u03bd \u223c N(\u03bd|0,I), which implies that f \u223c N(f|0,K) indeed. In the\nremainder of this paper, we will consider L to be the lower triangular Cholesky\ndecomposition of K, but in principle any square root of K could be used. In this\nway, \u03bd is ancillary for \u03b8 and it is possible to express the joint density as\nAAp(y,\u03bd,\u03b8) = p(y|\u03bd,\u03b8)p(\u03bd)p(\u03b8)\nThis parameterization is called Ancillary Augmentation (AA) in the terminology\nof Yu and Meng (2011). In Murray and Adams (2010) SA and AA are referred to\nunwhitened and whitened parameterizations respectively. Weak and strong data\nlimits can influence the efficiency in sampling using either parameterization. For\nthis reason, it is important to choose an efficient parameterizations for the particu-\nlar problem under study and for the available amount of data, as both those aspects\ncan dramatically influence efficiency and speed of convergence of the chains.\n2.2 Ancillarity-Sufficiency Interweaving Strategy - ASIS\nIn this section we briefly review the main results presented in Yu and Meng (2011)\non the combination of parameterizations to improve convergence and efficiency of\nMCMC methods, and we will illustrate how these results can be applied to GP\nmodels. Intuitively, combining parameterizations seems promising to take the best\nfrom them in both weak and strong data limits, or at least, to avoid the possibility\nthat chains do not converge because of the wrong choice of parameterization."},{"page":6,"text":"6 M. Filippone et al.\nAlternating the sampling in the SA and AA parameterizations is the most obvious\nway of combining the two parameterizations, but as recently investigated in Yu\nand Meng (2011), interweaving SA and AA is actually a more promising way\nforward. From the theoretical perspective, the geometric rate of convergence r of\nthe scheme when the parameterizations are interweaved, is related to the rates of\nthe two schemes r1and r2by r \u2264 R1,2\u221ar1r2, where R1,2is the maximal correlation\nbetween the latent variables for the two schemes. Given that the former expression\nimplies r \u2264 max(r1,r2), combining two parameterizations leads to a scheme that\nis better than the worst. This is already an advantage compared to using a single\nscheme when one is in doubt on which scheme to use. However, the key result\nis the fact that R1,2 can be very small depending on the two parameterizations,\nso it is possible to make the combined scheme converge quickly even if neither\nof the individual schemes do. In general, this result is quite remarkable, as once\ndifferent reparameterizations are available, combining them using the interweaving\nstrategy is simple to implement, and can dramatically boost sampling efficiency.\nIn GP models, the ASIS scheme amounts to interweaving SA and AA updates,\nthat following Yu and Meng (2011) yields:\nf|y,\u03b8\n\u2212\u2192\n\u03b8|f\n\u2212\u2192\n\u03bd = \u03c3\u22121\/2L\u22121f\n\u2212\u2192\n\u03b8|y,\u03bd\n2.3 Knorr-Held and Rue (KHR)\nThe idea underpinning KHR, is to jointly sample parameters and latent variables\nas follows. First, a set of hyper-parameters \u03b8?|\u03b8 is proposed and then a set of\nlatent variables conditioned on the new set of hyper-parameters, namely f?|y,\u03b8?,\nis proposed. The proposal (\u03b8?,f?) is then jointly accepted or rejected according to a\nstandard Hastings ratio. The key idea is to avoid making the proposal \u03b8?accepted\non the basis of f to avoid the strong coupling effect due to the hierarchical nature\nof the model. KHR was proposed in applications making use of Gaussian Markov\nRandom Fields, and we will discuss the application of this idea for GP models\nin the section reporting the experiments. In order to avoid difficulties in devising\na proposal for sampling from f?|y,\u03b8?), here we set the proposal as the Gaussian\nobtained by constructing a Laplace approximation to p(f|y,\u03b8?).\n2.4 Surrogate Method (SURR)\nIn the SURR method (Murray and Adams, 2010), a set of auxiliary latent vari-\nables g is introduced as a noisy version of f; in particular, p(g|f,\u03b8) = N(g|f,S\u03b8).\nThis construction yields a conditional distribution for f of the form p(f|g,\u03b8) =\nN(f|m,R), with R = S\u03b8\u2212S\u03b8(S\u03b8+K)\u22121S\u03b8and m = RS\u22121\nR = DDT, the sampling of \u03b8 is then conditioned on the variables \u03b7 defined as\nf = D\u03b7 + m. The covariance S\u03b8 is constructed to be diagonal with elements ob-\ntained by matching the posterior for each latent variable individually or by Taylor\napproximations (see Murray and Adams (2010) for details).\n\u03b8g. After decomposing"},{"page":7,"text":"Title Suppressed Due to Excessive Length7\n3 MCMC transition operators considered in this work\nThis section presents the transition operators considered in this work. We are in-\nterested in understanding whether and to what extent employing proposal mech-\nanisms making use of gradient or curvature information of the target density im-\nproves sampling efficiency and speed of convergence with respect to computational\ncomplexity. We therefore consider transition operators with increasing complexity,\nand in particular the Metropolis-Hastings (MH) operator which is based on ran-\ndom walk types of proposals, the Hybrid Monte Carlo (HMC) operator which uses\ngradient information, and the Simplified Manifold Metropolis Adjusted Langevin\nAlgorithm (SMMALA) operator which is one of the simplest manifold MCMC\nmethods proposed in Girolami and Calderhead (2011) using curvature informa-\ntion.\nFor the sake of clarity, we will focus on the transitions operators for f, but the\nsame operators can be easily applied to \u03b8. We will first present MH, HMC, and\nSMMALA, and we will then discuss Elliptical Slice Sampling and a few variants\nof MH and HMC that have been specifically proposed for sampling f, and do\nnot have counterparts for \u03b8. In the case of latent variables, the operators aim\nto leave the posterior p(f|y,\u03b8) invariant; in the remainder of this work, W(f) is\ndefined as log[p(y|f)p(f|\u03b8)], which equals the log of the desired target density up to\nconstants. In the case of hyper-parameters we can define the invariant distribution\naccording to the chosen parameterization and apply the operators presented here\nfor sampling \u03b8 rather than f.\n3.1 Metropolis-Hastings - MH\nThe Metropolis-Hastings transition operator employs a proposal mechanism g(f?|f)\nbased on a random walk (Robert and Casella, 2005). A common choice is to use\na multivariate Gaussian proposal with covariance \u03a3 centered at the former posi-\ntion f, thus taking the form g(f?|f) = N(f?|f,\u03a3). For such a symmetric proposal\nmechanism, f?is then accepted with probability min?1,exp(W(f?) \u2212 W(f))?.\n3.2 Hybrid Monte Carlo - HMC\nIn Hybrid Monte Carlo (HMC) the proposals are based on the analogy with a\nphysical system, where a particle is simulated moving in a potential field (Neal,\n1993). An auxiliary variable p, that plays the role of a momentum variable, is\ndrawn from N(p|0,M), where the covariance matrix M is the so called mass\nmatrix. The joint density of f and p factorizes as p(f,p) = exp(W(f))p(p), and\nthe negative log-joint density reads:\nH(f,p) = \u2212W(f) +1\n2log(|M|) +1\n2pTM\u22121p + const.\nThis is the Hamiltonian of the simulated particle, where the potential field is given\nby \u2212W(f) and the kinetic energy by the quadratic form in p. In order to draw"},{"page":8,"text":"8M. Filippone et al.\nproposals from p(f|y,\u03b8), we can simulate the particle for a certain time interval,\nintroducing an analogous of time t and solving Hamilton\u2019s equations\ndf\ndt=\u2202H\n\u2202p= M\u22121p\ndp\ndt= \u2212\u2202H\n\u2202f\n= \u2207fW\nGiven that there is no friction, the energy will be conserved during the mo-\ntion of the particle. Solving directly Hamilton\u2019s equations for general potential\nfields, however, is analytically intractable, and therefore it is necessary to resort\nto schemes where time is discretized. The leapfrog integrator discretizes the dy-\nnamics in \u03bb steps, also known as leapfrog steps, and is volume preserving and\nreversible (see Neal (1993) for details). The leapfrog integrator yields an update of\n(f,p) into (f(\u03bb),p(\u03bb)). The discretization introduces an approximation such that\nthe total energy is not conserved, so a Metropolis accept\/reject step of the form\nmin{1,exp(\u2212H(f(\u03bb),p(\u03bb))+H(f,p))} is needed to ensure that HMC samples from\nthe correct invariant distribution. The HMC transition operator is reported in Al-\ngorithm 1.\nAlgorithm 1 HMC transition operator when M = LMLT\nM\n1: f(0)= f; p(0)\u223c N(p(0)|0,M)\n2: \u03bb = sample[1,...,\u03bbmax]\n3: for (t = 0 to \u03bb \u2212 1) do\n4:p(t+1\/2)= p(t)+\u03b5\n5:f(t+1)= f(t)+ \u03b5M\u22121p(t+1\/2)\n6:p(t+1)= p(1\/2)+\u03b5\n7: end for\n8: r = min?0,H(f(0),p(0)) \u2212 H(f(\u03bb),p(\u03bb))?\n9: u \u223c Exp(u|1)\n10: if (r > \u2212u) then return f(\u03bb)\n11: else return f(0)\n? z \u223c N(0,I); p(0)= LMz\n2\u2207fW(f(t))\n? M\u22121p = bcksub(LT\nM,(fwdsub(LM,p)))\n2\u2207fW(f(t+1))\n? log|M| = 2?\nilog(LM)ii\n? pTM\u22121p = ?fwdsub(LM,p)?2\n3.3 Manifold MCMC - Simplified Manifold MALA - SMMALA\nManifold MCMC methods (Girolami and Calderhead, 2011) were proposed to\nhave an automatic mechanism to tune parameters in MALA and HMC, and are\nbased on the use of curvature through the Fisher Information (FI) matrix. The FI\nmatrix and the Christoffel symbols are the key quantities in information geometry\nas they characterize the curvature and the connection on the statistical manifold\nrespectively. Consider a statistical model S = {p(y|\u03c8)|\u03c8 \u2208 \u03a8} where y denotes\nobserved variables and \u03c8 comprises all model parameters. Under conditions that\nare generally satisfied for most commonly used models (Amari and Nagaoka, 2000),\nS can be considered a C\u221emanifold, and is called statistical manifold. Let L =\nlog[p(y|\u03c8)]; the FI matrix G of S at \u03c8 is defined as:\n?\nG(\u03c8) = Ep(y|\u03c8)\n(\u2207\u03c8L)(\u2207\u03c8L)T?\n= \u2212Ep(y|\u03c8)[\u2207\u03c8\u2207\u03c8L]"},{"page":9,"text":"Title Suppressed Due to Excessive Length9\nBy definition, the FI matrix is positive semidefinite, and can be considered as the\nnatural metric on S.\nIn the case of GP models that are hierarchical we need to consider the statistical\nmanifolds associated with the two levels of the hierarchy separately. Let\u2019s focus\non the statistical manifold associated with the model for y given f. The manifold\nMALA (MMALA) algorithm (Girolami and Calderhead, 2011) defines a Langevin\ndiffusion with stationary distribution p(f|\u03b8,y) on the Riemann manifold of density\nfunctions. Denote its metric tensor by Gf,f. By employing a first order Euler\nintegrator to solve the diffusion, a proposal mechanism with density g(f?|f) =\nN(f?|\u00b5(f,?),?2G\u22121\nwhich needs to be tuned, and the dth component of the mean function \u00b5(f,?)dis\nf,f) is obtained, where ? is the integration step size, a parameter\n\u00b5(f,?)d= fd+?2\n2\n?\nG\u22121\nf,f\u2207fW(f)\n?\nd\u2212 ?2\nn\n?\ni=1\nn\n?\nj=1\n(G\u22121\nf,f)i,j\u0393d\ni,j\nwhere \u0393d\nand Nagaoka, 2000). Similarly to MALA (Roberts and Stramer, 2002), due to the\ndiscretization error introduced by the first order approximation, convergence to the\nstationary distribution is not guaranteed anymore and thus a standard Metropolis\naccept\/reject step is employed to correct this bias.\nIn the same spirit, it is possible to extend HMC to define Hamilton\u2019s equations\non the statistical manifold. This was proposed and applied in Girolami and Calder-\nhead (2011) and called Riemann manifold Hamiltonian Monte Carlo (RM-HMC).\nIn this work, we will not consider RM-HMC or MMALA, as they both require\nthe derivatives of the FI matrix that would require several expensive operations.\nInstead, we will consider a simplified version of MMALA (SMMALA), where we\nassume a manifold with constant curvature, that effectively removes the term de-\npending on the Christoffel symbols, so that the mean of the proposal of SMMALA\nbecomes\n\u00b5s(f,?) = f +?2\ni,jare the Christoffel symbols of the metric in local coordinates (Amari\n2G\u22121\nf,f\u2207fW(f)\nFurthermore, in the last subsection of this section we will present two variants of\nHMC that bear some similarities with RM-HMC but are computationally cheaper.\nThe SMMALA transition operator is sketched in Algorithm 2.\nAlgorithm 2 SMMALA transition operator\n1: \u00b5s(f,?) = f +?2\n2G\u22121\nf,f\u2207fW(f)? Gf,f= LGLT\nG\n? G\u22121\nf,f\u2207fW(f) = bcksub(LT\n? z \u223c N(0,I); f?= \u03b5bcksub(LT\nG,(fwdsub(LG,\u2207fW(f))))\nG,z) + \u00b5s(f,?)\n? log|Gf,f| = 2?\n2: f?\u223c N(f?|\u00b5s(f,?),?2G\u22121\n3: r = min{0,W(f?) \u2212 W(f) + log[g(f|f?)] \u2212 log[g(f?|f)]}\n? (f?\u2212 \u00b5s(f,?))TG\u22121\n4: u \u223c Exp(u|1)\n5: if (r > \u2212u) then return f?\n6: else return f\nf,f)\nilog(LG)ii\nf,f(f?\u2212 \u00b5s(f,?)) = ?fwdsub(LG,(f?\u2212 \u00b5s(f,?)))?2"},{"page":10,"text":"10 M. Filippone et al.\n3.4 Elliptical Slice sampling - ELL-SS\nElliptical Slice Sampling (ELL-SS) has been proposed in Murray et al. (2010) to\ndraw samples for f in GP models, and is based on slice sampling (Neal, 2003). Due\nto the fact that latent variables are Gaussian, it is possible to derive this particular\nversion of slice sampling that is constrained on an ellipse. For completeness, we\nreport the transition operator in Algorithm 3 and we refer the reader to Murray\net al. (2010) for further details. Note that ELL-SS is quite appealing as it returns\nAlgorithm 3 ELL-SS transition operator\n1: z \u223c N(0,K)\n2: u \u223c Exp(u|1)\n3: \u03b1 \u223c U[0,2\u03c0]\n4: f?= f cos(\u03b1) + zsin(\u03b1)\n5: if (logp(y|f?) > \u03b7) then return f?\n6: else\n7: if (\u03b1 < 0) then \u03b1min= 0\n8: else \u03b1max= 0\n9:\u03b1 \u223c U[\u03b1min,\u03b1max]\n10:Go to 4\n\u03b7 = logp(y|f) \u2212 u\n[\u03b1min,\u03b1max] = [\u03b1 \u2212 2\u03c0,\u03b1]\n? Set a threshold on the log-likelihood\n? Define the bracket\n? Shrink the bracket\na sample which does not need to be accepted or rejected (in fact, a rejection\nmechanism is implicit within step 5), and the proposal mechanism does not have\nany free parameters that need tuning.\n3.5 Scaled versions of MH - MH v1 and MH v2\nDue to the strong correlation of latent variables imposed by the GP prior, employ-\ning a MH operator with an isotropic covariance to sample latent variables leads to\nextremely poor efficiency. In order to overcome this problem, Neal (1999) proposed\ntwo versions of MH that we will denote by MH v1 and MH v2. In MH v1, a set of\nlatent variables z is drawn from the GP prior z \u223c N(z|0,K), and the proposal is\nconstructed as follows:\nf?= f + \u03b1z\nwhere the parameter \u03b1 controls the degree of update. In MH v2, instead, the\nproposal is as follows:\nf?=1 \u2212 \u03b12f + \u03b1z\nIn the latter case, given that the proposal satisfies detailed balance with respect\nto the prior, the acceptance has to be based on the likelihood alone.\n?\n3.6 Scaled versions of HMC - HMC v1 and HMC v2\nBy a similar argument as in MH, it is possible to introduce scaled versions of HMC\nthat reduce the correlation between latent variables. This can be done by setting\nthe mass matrix of HMC according to the precision of the posterior distribution of"},{"page":11,"text":"Title Suppressed Due to Excessive Length 11\nlatent variables. Similarly, from an information geometric perspective, it is sensible\nto whiten latent variables according to the metric tensor of the statistical manifold.\nWe notice that the metric tensor associated to the model for y given f is K\u22121plus a\ndiagonal matrix which is a function of f (see appendix A for full details). Whitening\nwith respect to that metric tensor would be computationally very expensive for\nGP models, as it would require the simulation of the Hamiltonian dynamics on a\nmanifold with a position-dependent curvature; this is implemented by RM-HMC\nwhich requires the derivatives of the metric tensor as well as implicit leapfrog\niterations (Girolami and Calderhead, 2011). In order to reduce the computational\ncost, we propose the following two options: (i) to approximate the diagonal term to\nbe independent of f so that M\u22121= (K\u22121+C)\u22121= C\u22121\u2212C\u22121(K +C\u22121)\u22121C\u22121\nwith C diagonal and independent of f; we call this variant HMC v1. (ii) to ignore\nthe diagonal part of the metric tensor and set M\u22121= K; we call this variant\nHMC v2. In HMC v1, one simple way to make C independent of f is to compute\nit for the GP prior mean (which is zero), as proposed, e.g., in Christensen et al.\n(2005); Vanhatalo and Vehtari (2007).\nIn both cases, it is possible to employ a standard HMC proposal that captures\npart of the curvature of the statistical manifold. By introducing a variant of HMC\nthat, rather than using the Cholesky decomposition of the mass matrix requires\nthe decomposition of its inverse, it is possible to devise efficient implementations\nof HMC v1 and HMC v2. We report this variant of the HMC transition operator\nin Algorithm 4.\nIn HMC v1, employing this formulation of HMC is convenient as computing\nthe inverse of M is more stable than computing M = K\u22121+ C, that requires\na potentially unstable inversion of K. HMC v1 requires the computation of the\ninverse of the mass matrix and its factorization each time a new value of \u03b8 is\nproposed. In HMC v2, instead, no extra operations in O(n3) are required given\nthat K is already factorized, thus making it computationally very convenient.\nAlgorithm 4 HMC transition operator when M\u22121= LM\u22121LT\nM\u22121\n1: f(0)= f; p(0)\u223c N(p(0)|0,M)\n2: \u03bb = sample[1,...,\u03bbmax]\n3: for (t = 0 to \u03bb \u2212 1) do\n4:p(t+1\/2)= p(t)+\u03b5\n5:f(t+1)= f(t)+ \u03b5M\u22121p(t+1\/2)\n6:p(t+1)= p(1\/2)+\u03b5\n7: end for\n8: r = min?0,H(f(0),p(0)) \u2212 H(f(\u03bb),p(\u03bb))?\n9: u \u223c Exp(u|1)\n10: if (r > \u2212u) then return f(\u03bb)\n11: else return f(0)\n? z \u223c N(0,I); p(0)= bcksub(LT\nM\u22121,z)\n2\u2207fW(f(t))\n? M\u22121p = LM\u22121(LT\nM\u22121p)\n2\u2207fW(f(t+1))\n? log|M| = \u22122?\nilog(LM\u22121)ii\nM\u22121p?2\n? pTM\u22121p = ?LT"},{"page":12,"text":"12 M. Filippone et al.\n4 Results on simulated data\nIn this section, we first report a study on the efficiency and speed of convergence of\ndifferent transition operators in sampling from posterior distribution of individual\ngroups of variables in the SA and AA parameterization. We then report the same\nanalysis to compare different parameterizations to obtain samples from the joint\nposterior distribution of f and \u03b8.\n4.1 Experimental setup\nWe simulated data from the four GP models considered in this work, namely: LRG,\nLCX, VLT, and ORD. We generated 10 data sets simulating from each of the four\nmodels for all combinations of n = 100,400, and d = 2,10, for a total of 160 distinct\ndata sets. In order to isolate the effect of different likelihood functions in the\nresults, we seeded the generation of the input data matrix X, hyper-parameters,\nand latent variables so that is was the same across different models. Covariates were\ngenerated uniformly in the unit hyper-cube, and the parameters used to generate\nlatent variables were \u03c3 = exp(2), \u03c8\u03c4i\u223c U[\u22123,\u22121]. We imposed Gamma priors on\nthe length-scale parameters with shape a and rate b, p(\u03c4i) = Gam(\u03c4i|a = 1,b = 1).\nWe imposed an inverse Gamma prior p(\u03c3) = invGam(\u03c3|a = 1,b = 1), where a and\nb are shape and scale parameters respectively on \u03c3 to exploit conjugacy in the SA\nparameterization.\nIn all the experiments we collected 20000 samples after a burn-in phase of\n5000 iterations; during the burn-in we also had an adaptive phase to allow the\nsamplers reach recommended acceptance rates (for example around 25% for MH).\nThe transition operators for f had the following tuning parameters: \u03b1 for MH v1\nand MH v2, and \u03b5 for SMMALA and the variants of HMC which used a maximum\nof 10 leapfrog steps. The transition operators for \u03b8 employed the following pro-\nposals: MH used a covariance \u03a3 = \u03b1I, HMC used a mass matrix M = \u03b1I and 10\nmaximum leapfrog steps, and SMMALA used a step-size \u03b5. Convergence analysis\nwas performed using the\u02c6R potential scale reduction factor (Gelman and Rubin,\n1992), which is a classic score used to assess convergence of MCMC algorithms.\nThe computation of the\u02c6R value is based on the within and between chains vari-\nances; a value close to one indicates that convergence is reached. The\u02c6R value was\ncomputed based on 10 chains initialized from the prior to study what efficiency\ncan be achieved without running preliminary simulations; this is different from\nthe initialization procedure suggested in Gelman and Rubin (1992) that requires\nlocating the modes of the target density. Due to the fairly diffuse priors on the\nlength-scale parameters, we noticed difficulty in achieving convergence in some\ncases; we therefore initialized \u03c8\u03c4irandomly in the interval [\u22123,\u22121]. The value of\n\u02c6R was checked at 1000, 2000, 5000, 10000, 20000 iterations. We use the following\nprocedure to compactly visualize speed of convergence; we threshold the median\nvalue of\u02c6R across 10 data sets at each checkpoint and use the following visual\ncoding to report speed of convergence: < 1.1 < < 1.3 < < 2 < , so that indi-\ncates that\u02c6R < 1.1,indicates that 1.1 <\u02c6R < 1.3, and so on. We then stack the\nrectangles associated to each checkpoint where we computed the value of\u02c6R, thus\nproducing a sort of histogram of the median of\u02c6R over the iterations. Efficiency\nof MCMC methods is compared based on the minimum of the Effective Sample"},{"page":13,"text":"Title Suppressed Due to Excessive Length 13\nSize (ESS) (Robert and Casella, 2005) computed across all the sampled variables.\nWe then report its mean and standard deviation across the 10 chains and the 10\ndifferent data sets for each combination of size of the data set, dimensionality, and\ntype of likelihood.\nWe are also interested in statistically assessing which methods achieve faster\nconvergence. In order to do so, we perform pairwise Mann-Whitney tests with\nsignificance level of 0.05 comparing the value of\u02c6R at the last checkpoint for all\nthe chains across 10 data sets. This allows us to obtain an ordering of methods\nin terms of speed of convergence. In each table we include a row at the bottom\nreporting the result of such a test. We denote by 1|2 situations where the method\nin row 1 of the corresponding table converges significantly faster than the method\nin row 2. Instead, the notation 1,2 is used when the method in row 1 does not\nconverge significantly faster than the method in row 2.\nAs a measure of complexity, we counted the number of operations with com-\nplexity in O(n3), namely number of Cholesky factorizations of n \u00d7 n matrices\n(#C), number of inversions of n \u00d7 n matrices (#I)2, and number of multiplica-\ntions of n \u00d7 n matrices (#M). We believe that this is a more reliable measure\nof complexity with respect to running time, as running time can be affected by\nseveral implementation details and other factors that are not directly related to\nthe actual complexity of the algorithms.\n4.2 Assessing the efficiency of samplers for individual groups of variables\nIn this section, we present an assessment of the efficiency of different transition\noperators for each group of variables using both SA and AA parameterizations.\nComputational complexity for all the operators considered in the next sections is\nsummarized in Tab. 1, where T represent the number of iterations, d the number of\ncovariates and\u00af\u03bb the average number of leapfrog steps in HMC transition operators.\nIn the next sub-sections we first present results about the sampling of the latent\nvariables, and then we move onto presenting results on the sampling of the hyper-\nparameters.\n4.2.1 Sampling f|y,\u03b8\nIn this section we focus on the sampling from the posterior distribution of the\nlatent variables f. The results can be found in Tab. 2, and they were obtained\nby fixing \u03b8 to the values used to generate the data. First, we notice that different\nlikelihood functions heavily affect efficiency and speed of convergence; in the exam-\nples considered here, the results show that in LRG it is possible to achieve efficiency\none order of magnitude higher than in other models. The scaled versions of MH\nwork well in the case of LRG (MH v1 is slightly better than MH v2), but do not\noffer guarantees of convergence on other models. ELL-SS achieves better efficiency\nand convergence than the scaled versions of MH. SMMALA, which uses gradient\nand curvature information, achieves good efficiency and faster convergence than\nMH v1, MH v2, and ELL-SS, but at the cost of one operation in O(n3) at each\n2This is a shorthand notation to denote a back and forward substitution of the identity\nmatrix using Cholesky factors."},{"page":14,"text":"14 M. Filippone et al.\nTable 1 Breakdown of the number of operations in O(n3) required to apply the transition\noperators considered in this work. #M, #I and #C represent number of multiplication of n\u00d7n\nmatrices, inversions of n \u00d7 n matrices, and number of Cholesky decompositions respectively.\nCounts are reported as functions of the number of iterations T and number of covariates d. In\nHMC,\u00af\u03bb denotes the average number of leapfrog steps in one iteration.\nf|\u03b8,y\n#I\n0\n0\n1\n0\n0\n0\n1\n0\n\u03b8|f\n#I\n0\nT\u00af\u03bb\nT\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u03b8|\u03bd,y\n#I\n0\n0\n0\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n#M\n0\n0\n0\n0\n0\n0\n0\n0\n#C\n1\n1\nT\n1\n1\n1\n2\n1\n#M\n0\n0\nTd\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n#C\nT\nT\nT\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n#M\n0\n0\n0\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n#C\nT MH\nHMC\nSMMALA\nELL-SS\nMH v1\nMH v2\nHMC v1\nHMC v2\nT + Td\u00af\u03bb\nT + Td\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\niteration, as the metric tensor is a function of f and needs to be factorized at each\niteration. Overall, the results suggest that the scaled versions of HMC are the\nbest sampling methods for f|\u03b8,y. HMC v1 is slightly better than HMC v2, but it\nrequires one extra inversion and one extra Cholesky decomposition compared to\nHMC v2 that does not require any operations in O(n3) once the covariance matrix\nof the GP is factorized.\n4.2.2 SA parameterization - Sampling \u03b8|f\nIn this section we present results about the sampling of hyper-parameters from the\nposterior distribution \u03b8|f,y which, given the hierarchical structure of the model,\nis simply \u03b8|f independently from the data model. As reported in Tab. 1, the\ncomplexity of applying SMMALA and HMC is quite high compared to MH. MH\nrequires one Cholesky factorization of Q at each iteration. In HMC, at each leapfrog\nstep, the gradients of Q with respect to \u03b8 are needed and the cheapest way to do\nthis is by inverting Q first and noticing that all the remaining operations are in\nO(n2); this is done\u00af\u03bb times on average at every iteration of HMC. Similarly, in\nSMMALA the gradient can be computed by inverting Q first; by doing so, the\nmetric tensor can then be computed by d multiplications with the derivatives of\nQ and no other O(n3) operations.\nThe results are reported in Tab. 3, and were obtained by fixing f to the value\nused to generate the data and sampling only the length-scale parameters, as \u03c3 can\nbe efficiently sampled using exact Gibbs steps. HMC improves quite substantially\non efficiency, but not on speed of convergence; it may be worth employing some\nrescaling of the hyper-parameters to improve on this as suggested by Neal (1996).\nThe performance of SMMALA is highly variable in efficiency and it converges\nmore slowly than MH and HMC. This might be due to the skewness of the target\ndistribution, that is known to affect the efficiency of SMMALA (Stathopoulos and\nFilippone, 2011). The results indicate that MH strikes a good balance between\nefficiency and computational cost."},{"page":15,"text":"Title Suppressed Due to Excessive Length 15\nTable 2 Comparison of transition operators to sample f|y,\u03b8 for data generated from models\nwith four different likelihoods. Minimum ESS is averaged over 10 chains for 10 different data\nsets for each value of n and d. The last row in each sub-table reports the result of the statistical\ntest to assess which operators achieve significantly faster convergence.\nLRG\nn = 100n = 400\nd = 2d = 10d = 2d = 10\nESS\n67 (15)\n204 (35)\n756 (284)\n321 (61)\n3395 (400)\n4004 (577)\n6|5|3|4|2|1\n\u02c6RESS\n47 (3)\n151 (7)\n262 (30)\n241 (11)\n5163 (268)\n5225 (224)\n6|5|3,4|2|1\n\u02c6RESS\n22 (7)\n67 (17)\n457 (212)\n104 (25)\n1352 (380)\n1566 (342)\n6|5|3|4|2|1\n\u02c6RESS\n8 (1)\n30 (2)\n48 (5)\n50 (2)\n2962 (155)\n2995 (129)\n6|5|4|3|2|1\n\u02c6R\nMH v1\nMH v2\nSMMALA\nELL-SS\nHMC v1\nHMC v2\nLCX\nn = 100n = 400\nd = 2d = 10d = 2d = 10\nESS\n18 (16)\n23 (24)\n217 (155)\n39 (42)\n372 (277)\n254 (197)\n6|5|3|4|2|1\n\u02c6RESS\n6 (2)\n6 (2)\n39 (4)\n11 (4)\n188 (123)\n188 (125)\n6|5|3|4|1,2\n\u02c6RESS\n6 (5)\n8 (7)\n258 (177)\n11 (11)\n199 (200)\n64 (37)\n5|3,6|4|2|1\n\u02c6RESS\n1 (0)\n1 (0)\n7 (1)\n2 (0)\n81 (30)\n80 (30)\n6|5|3|4|2|1\n\u02c6R\nMH v1\nMH v2\nSMMALA\nELL-SS\nHMC v1\nHMC v2\nVLT\nn = 100n = 400\nd = 2 d = 10d = 2d = 10\nESS\n28 (13)\n31 (16)\n424 (216)\n46 (20)\n1494 (667)\n418 (68)\n4|3|2|1,5,6\n\u02c6RESS\n10 (2)\n12 (2)\n117 (13)\n18 (4)\n449 (42)\n443 (39)\n3|4|2|1,5,6\n\u02c6RESS\n15 (8)\n16 (8)\n418 (127)\n22 (10)\n1384 (392)\n183 (31)\n3,4|2|1,5,6\n\u02c6R ESS\n2 (0)\n2 (0)\n61 (7)\n4 (1)\n249 (25)\n245 (25)\n3|6|4,5|2|1\n\u02c6R\nMH v1\nMH v2\nSMMALA\nELL-SS\nHMC v1\nHMC v2\nORD\nn = 100n = 400\nd = 2d = 10d = 2d = 10\nESS\n14 (8)\n14 (9)\n48 (89)\n21 (11)\n539 (650)\n175 (54)\n6|5|3|4|1,2\n\u02c6R ESS\n6 (2)\n7 (2)\n2 (0)\n10 (2)\n472 (39)\n483 (37)\n6|5|4|2|1|3\n\u02c6R ESS\n7 (5)\n7 (5)\n107 (156)\n9 (5)\n176 (200)\n61 (22)\n6|5|3|4|1,2\n\u02c6R ESS\n1 (0)\n2 (0)\n1 (0)\n2 (0)\n257 (23)\n255 (24)\n6|5|2,4|1,3\n\u02c6R\nMH v1\nMH v2\nSMMALA\nELL-SS\nHMC v1\nHMC v2\n4.2.3 AA parameterization - Sampling \u03b8|y,\u03bd\nIn this section we present the sampling of the hyper-parameters from the posterior\ndistribution \u03b8|y,\u03bd, where we fixed \u03bd to the values used to generate the data. The"},{"page":16,"text":"16M. Filippone et al.\nTable 3 Comparison of transition operators to sample \u03b8|f. Minimum ESS is averaged over\n10 chains for 10 different data sets for each value of n and d. The last row reports the result\nof the statistical test to assess which operators achieve significantly faster convergence.\nn = 100n = 400\nd = 2d = 10d = 2d = 10\nESS\n2024 (144)\n11325 (915)\n9592 (2052)\n\u02c6RESS\n156 (37)\n830 (269)\n61 (23)\n\u02c6RESS\n2124 (125)\n12556 (661)\n10241 (2672)\n\u02c6R ESS\n77 (33)\n293 (137)\n47 (17)\n\u02c6R\nMH\nHMC\nSMMALA\n1|2,32|1|31|2|32|1|3\nanalysis of complexity shows that MH requires one Cholesky factorization at each\niteration only. In HMC, each leapfrog requires computing L and the gradient of L\nwith respect to \u03b8 and no other operations in O(n3); this can be computed using\nthe differentiation of the Cholesky algorithm which requires d operations in O(n3)\n(Smith, 1995). Likewise, for SMMALA L and the d derivatives of L with respect\nto \u03b8 are the only operations in O(n3) needed.\nThe results can be found in Tab. 4 and are again variable across different\nmodels. In general SMMALA and HMC do not seem to offer faster convergence\nwith respect to the MH transition operator which is therefore competitive in terms\nof efficiency relative to computational cost.\n4.3 Assessing the efficiency of different parameterizations\nAfter analyzing the results in the previous section, we decided to combine the\ntransition operators which achieved a good sampling efficiency with relatively low\ncomputational cost and ease of implementation. We decided that a good combina-\ntion to be used in AA, SA, ASIS, and SURR could be as follows: sampling f using\nHMC v2 and \u03b8 using MH; HMC v2 and MH where adapted during the burn-in\nphase and in HMC v2 we set the maximum number of leapfrog steps to 10. For\nthe sake of brevity, we focus on the LRG model only; the results on efficiency and\nspeed of convergence in sampling hyper-parameters are reported in Tab. 5.\nIt is striking to see how challenging it is to efficiently sample from the pos-\nterior distribution of latent variables and hyper-parameters. Sampling efficiency\nis generally low; this is consistent with our experience in other applications in-\nvolving sampling in hierarchical models (Filippone et al., 2012). As expected, the\nSA parameterization is the worst among the ones we tested. The AA parameter-\nization, ASIS, and SURR generally offer good guarantees of convergence within\na few thousand iterations. SURR seems to be superior in efficiency, consistently\nwith what reported in Murray and Adams (2010), but it requires more operations\nin O(n3) compared to AA and ASIS. ASIS slightly improves efficiency and speed\nof convergence with respect to the AA scheme but requires double the number of\noperations in O(n3). KHR seems effective in breaking the correlation between the\ntwo groups of variables, but it may require several iterations within the approxi-\nmation used to sample f. In the experiments considered here \u00af \u03ba is around 8, so the\nbest compromise between computations and efficiency seems to be given by the\nAA and ASIS parameterizations."},{"page":17,"text":"Title Suppressed Due to Excessive Length 17\nTable 4 Comparison of transition operators to sample \u03b8|y,\u03bd for data generated from models\nwith four different likelihoods. Minimum ESS is computed as the average over 10 chains for 10\ndifferent data sets for each value of n and d. The last row in each sub-table reports the result\nof the statistical test to assess which operators achieve significantly faster convergence.\nLRG\nn = 100n = 400\nd = 2d = 10d = 2d = 10\nESS\n556 (201)\n2572 (1382)\n3833 (2032)\n\u02c6RESS\n131 (33)\n859 (278)\n65 (42)\n\u02c6RESS\n512 (177)\n2666 (973)\n6877 (1584)\n\u02c6R ESS\n56 (11)\n223 (39)\n47 (21)\n\u02c6R\nMH\nHMC\nSMMALA\n1,3|22|1|31|3|21|2,3\nLCX\nn = 100n = 400\nd = 2d = 10d = 2d = 10\nESS\n818 (386)\n5169 (3297)\n6158 (2788)\n\u02c6R ESS\n6 (4)\n11 (8)\n9 (6)\n\u02c6RESS\n1030 (397)\n7145 (3852)\n8377 (1815)\n\u02c6R ESS\n3 (1)\n4 (3)\n6 (4)\n\u02c6R\nMH\nHMC\nSMMALA\n3|1,21,2|31,2,31,2|3\nVLT\nn = 100n = 400\nd = 2d = 10d = 2d = 10\nESS\n859 (318)\n5680 (2634)\n6274 (1896)\n\u02c6R ESS\n22 (6)\n48 (20)\n14 (9)\n\u02c6R ESS\n795 (270)\n5233 (2482)\n6950 (2763)\n\u02c6R ESS\n8 (6)\n11 (11)\n11 (9)\n\u02c6R\nMH\nHMC\nSMMALA\n1,2,31|2|31,2,31|2|3\nORD\nn = 100n = 400\nd = 2d = 10d = 2d = 10\nESS\n689 (159)\n155 (296)\n3356 (1661)\n\u02c6R ESS\n14 (7)\n14 (11)\n11 (8)\n\u02c6RESS\n552 (168)\n79 (115)\n2328 (1423)\n\u02c6RESS\n9 (6)\n4 (4)\n19 (27)\n\u02c6R\nMH\nHMC\nSMMALA\n1,3|21,3|21,3|21,3|2\n5 Results on real data\nWe repeated the comparison of different parameterizations on four UCI data sets\n(Asuncion and Newman, 2007), namely the Pima, Wisconsin, SPECT, and Iono-\nsphere data sets, which we modeled using LRG models; the results are reported\nin Tab. 6. We used the same priors and experimental setup as in the previous\nsections, except that all features were transformed to have zero mean and unit\nstandard deviation, and latent variables were sampled iterating five updates of\nHMC v2. Also, chains were initialized sampling from the prior. Again, the SA\nparameterization is the poorest in efficiency and speed of convergence, and the\nAA parameterization improves on that; combining the two using ASIS slightly\nimproves on the AA parameterization, although the improvement is not dramatic.\nThe SURR method improves on the AA parameterization, consistently with what\nreported in Murray and Adams (2010). The results of KHR are highly variable"},{"page":18,"text":"18M. Filippone et al.\nTable 5 Comparison of different strategies to sample f,\u03b8|y for data generated from a\nLRG model. The rightmost column reports the complexity of the different methods with re-\nspect to number of inversion and Cholesky decompositions. In KHR, \u00af \u03ba represents the average\nnumber of iterations to run the Laplace Approximation.\nn = 100n = 400\nd = 2d = 10d = 2d = 10\nESS\n131(57)\n138(63)\n856(360)\n8(6)\n173(95)\n3|5|1,2|4\n\u02c6RESS\n117(34)\n168(49)\n177(48)\n59(18)\n90(32)\n1,2|3,4,5\n\u02c6R ESS\n94(38)\n98(39)\n481(219)\n5(2)\n157(51)\n3|5|1,2|4\n\u02c6R ESS\n47(17)\n60(25)\n116(32)\n14(6)\n35(15)\n2,3|1|4,5\n\u02c6R #I\n0\n0\n0\n0\nT\n#C\nT\n2T\nAA\nASIS\nKHR\nSA\nSURR\n\u00af \u03baT + 2T\nT\n2T\nTable 6 Comparison of different strategies to sample f,\u03b8|y in four UCI data sets modeled\nusing a LRG model.\nPima Wisconsin\nn = 683,d = 9\nESS\n42 (15)\n47 (11)\n20 (10)\n7 (3)\n25 (14)\nSPECT\nn = 80,d = 22\nESS\n99 (18)\n215 (23)\n101 (16)\n97 (12)\n84 (14)\nIonosphere\nn = 351,d = 34\nESS\n12 (5)\n24 (8)\n2 (2)\n11 (7)\n9 (4)\nn = 768,d = 8\nESS\n34 (4)\n35 (8)\n153 (14)\n5 (2)\n76 (10)\n\u02c6R\n\u02c6R\n\u02c6R\n\u02c6R\nAA\nASIS\nKHR\nSA\nSURR\nacross data sets; in cases where the approximation to sample latent variables is\naccurate, the chains mix well. In some cases, however, the approximation is not\naccurate enough to guarantee a good acceptance rate, and the chains can spend a\nlong time in the same position before accepting the joint proposal.\n6 Conclusions\nIn this paper we studied and compared a number of state-of-the-art strategies to\ncarry out the fully Bayesian treatment of GP models. We focused on four GP mod-\nels and performed an extensive evaluation of efficiency, speed of convergence, and\ncomputational complexity of several transition operators and sampling strategies.\nThe results in this paper show that latent variables can be sampled quite\nefficiently with little computational effort once the GP covariance matrix is fac-\ntorized. This can be achieved by a simple variant of HMC that we introduced in\nthis paper. About sampling hyper-parameters in different parameterizations, the\nresults presented here indicate that the gain in sampling efficiency given by the\nuse of complicated proposal mechanisms does not scale as much as their computa-\ntional cost. It would be interesting to investigate some recently proposed variants\nto slice sampling (Thompson and Neal, 2010) and Hybrid Monte Carlo (Hoffman\nand Gelman, 2012) on the sampling of hyper-parameters.\nThe analysis of the results obtained by different parameterization suggest that\nAA is a sensible and computationally cheap parameterization with good conver-\ngence properties. AA performs similarly to ASIS at half the computational cost. It"},{"page":19,"text":"Title Suppressed Due to Excessive Length 19\nmakes sense, however, to employ ASIS when in doubt about the best parameteri-\nzation to use, although GP models with full covariance matrices will generally fall\ninto the weak data limit as the O(n2) space and O(n3) time complexities constrain\nthe number of data that can be processed.\nIn general, the results show how challenging it is to efficiently sample from\nthe posterior distribution of latent variables and hyper-parameters in GP models\nand motivates further research into methods to do this efficiently. Some sampling\nstrategies, such as the one based on the AA parameterization, are capable of\nachieving convergence within a reasonable number of iterations, and this makes it\npossible to carry out the fully Bayesian treatment of GP models dealing with a\nsmall to moderate number of samples. We have recently demonstrated that this is\nindeed the case (Filippone et al., 2012), but more needs to be done in the direction\nof developing robust stochastic based inference methods for GP models.\nIt would be interesting to investigate how performance are affected by the\nchoice of the design, which in the simulated data presented here was assumed uni-\nform. Also, we studied in particular GP models with the squared exponential ARD\ncovariance function. It would be interesting to compare the method considered here\nin models characterized by other covariance functions, such as the Mat\u00b4 ern one, or\nsparse inverse covariance functions as in Rue et al. (2009); the latter, would make\nit possible to test the strong data limit case. Finally, in this study we have not\nincluded a mean function for the GP prior or extra parameters for the likelihood\nfunction. This would require including the sampling of other quantities that may\nfurther impact on efficiency and speed of convergence.\nReferences\n1. Amari, S. and H. Nagaoka (2000). Methods of Information Geometry, Volume 191\nof Translations of Mathematical monographs. Oxford University Press.\n2. Asuncion, A. and D. J. Newman (2007). UCI machine learning repository.\n3. Christensen, O. F., G. O. Roberts, and J. S. Rosenthal (2005). Scaling limits for\nthe transient phase of local MetropolisHastings algorithms. Journal of the Royal\nStatistical Society: Series B (Statistical Methodology) 67(2), 253\u2013268.\n4. Chu, W. and Z. Ghahramani (2005). Gaussian Processes for Ordinal Regression.\nJournal of Machine Learning Research 6, 1019\u20131041.\n5. Cseke, B. and T. Heskes (2011). Approximate Marginals in Latent Gaussian Mod-\nels. Journal of Machine Learning Research 12, 417\u2013454.\n6. Filippone, M., A. F. Marquand, C. R. V. Blain, S. C. R. Williams, J. Mour\u02dc ao-\nMiranda, and M. Girolami (2012). Probabilistic Prediction of Neurological Dis-\norders with a Statistical Assessment of Neuroimaging Data Modalities. Annals\nof Applied Statistics 6(4), 1883\u20131905.\n7. Filippone, M., M. Zhong, and M. Girolami (2012). On the fully Bayesian treatment\nof latent Gaussian models using stochastic simulations. Technical Report TR-\n2012-329, School of Computing Science, University of Glasgow.\n8. Flegal, J. M., M. Haran, and G. L. Jones (2007). Markov Chain Monte Carlo: Can\nWe Trust the Third Significant Figure? Statistical Science 23(2), 250\u2013260.\n9. Gelman, A. and D. B. Rubin (1992). Inference from iterative simulation using\nmultiple sequences. Statistical Science 7(4), 457\u2013472."},{"page":20,"text":"20M. Filippone et al.\n10. Girolami, M. and B. Calderhead (2011). Riemann manifold Langevin and Hamil-\ntonian Monte Carlo methods. Journal of the Royal Statistical Society: Series B\n(Statistical Methodology) 73(2), 123\u2013214.\n11. Hoffman, M. D. and A. Gelman (2012). The No-U-Turn Sampler: Adaptively\nSetting Path Lengths in Hamiltonian Monte Carlo. Journal of Machine Learning\nResearch to appear.\n12. Knorr-Held, L. and H. Rue (2002). On Block Updating in Markov Random Field\nModels for Disease Mapping. Scandinavian Journal of Statistics 29(4), 597\u2013614.\n13. Kuss, M. and C. E. Rasmussen (2005). Assessing Approximate Inference for Bi-\nnary Gaussian Process Classification. Journal of Machine Learning Research 6,\n1679\u20131704.\n14. Mackay, D. J. C. (1994). Bayesian methods for backpropagation networks. In\nE. Domany, J. L. van Hemmen, and K. Schulten (Eds.), Models of Neural Net-\nworks III, Chapter 6, pp. 211\u2013254. Springer.\n15. Minka, T. P. (2001). Expectation Propagation for approximate Bayesian inference.\nIn Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence,\nUAI \u201901, San Francisco, CA, USA, pp. 362\u2013369. Morgan Kaufmann Publishers\nInc.\n16. M\u00f8ller, J., A. R. Syversveen, and R. P. Waagepetersen (1998). Log Gaussian Cox\nProcesses. Scandinavian Journal of Statistics 25(3), 451\u2013482.\n17. Murray, I. and R. P. Adams (2010). Slice sampling covariance hyperparameters\nof latent Gaussian models. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor,\nR. S. Zemel, and A. Culotta (Eds.), NIPS, pp. 1732\u20131740. Curran Associates,\nInc.\n18. Murray, I., R. P. Adams, and D. J. C. MacKay (2010). Elliptical slice sampling.\nJournal of Machine Learning Research - Proceedings Track 9, 541\u2013548.\n19. Neal, R. (2003). Slice Sampling. Annals of Statistics 31, 705\u2013767.\n20. Neal, R. M. (1993). Probabilistic inference using Markov chain Monte Carlo meth-\nods. Technical Report CRG-TR-93-1, Dept. of Computer Science, University of\nToronto.\n21. Neal, R. M. (1996). Bayesian Learning for Neural Networks (Lecture Notes in\nStatistics) (1 ed.). Springer.\n22. Neal, R. M. (1999). Regression and classification using Gaussian process priors\n(with discussion). Bayesian Statistics 6, 475\u2013501.\n23. Opper, M. and O. Winther (2000). Gaussian processes for classification: Mean-\nfield algorithms. Neural Computation 12(11), 2655\u20132684.\n24. Rasmussen, C. E. and C. Williams (2006). Gaussian Processes for Machine Learn-\ning. MIT Press.\n25. Robert, C. P. and G. Casella (2005). Monte Carlo Statistical Methods (Springer\nTexts in Statistics). Secaucus, NJ, USA: Springer-Verlag New York, Inc.\n26. Roberts, G. O. and O. Stramer (2002).\nHastings Algorithms. Methodology and Computing in Applied Probability 4(4),\n337\u2013357.\n27. Rue, H., S. Martino, and N. Chopin (2009). Approximate Bayesian inference\nfor latent Gaussian models by using integrated nested Laplace approximations.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology) 71(2),\n319\u2013392.\n28. Smith, S. P. (1995). Differentiation of the Cholesky Algorithm. Journal of Com-\nputational and Graphical Statistics 4(2), 134\u2013147.\nLangevin Diffusions and Metropolis-"},{"page":21,"text":"Title Suppressed Due to Excessive Length 21\n29. Stathopoulos, V. and M. Filippone (2011). Discussion of the paper \u201dRiemann\nmanifold Langevin and Hamiltonian Monte Carlo methods\u201d by Mark Girolami\nand Ben Calderhead. Journal of the Royal Statistical Society, Series B (Statis-\ntical Methodology) 73(2), 123\u2013214.\n30. Thompson, M. and R. M. Neal (2010).\nTechnical Report 1002, Department of Statistics, University of Toronto.\n31. Tierney, L. and J. B. Kadane (1986). Accurate Approximations for Posterior\nMoments and Marginal Densities. Journal of the American Statistical Associa-\ntion 81(393), 82\u201386.\n32. Vanhatalo, J. and A. Vehtari (2007). Sparse Log Gaussian Processes via MCMC\nfor Spatial Epidemiology. Journal of Machine Learning Research - Proceedings\nTrack 1, 73\u201389.\n33. Wilson, A. G. and Z. Ghahramani (2010). Copula Processes. In J. D. Lafferty,\nC. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta (Eds.), NIPS,\npp. 2460\u20132468. Curran Associates, Inc.\n34. Yu, Y. and X.-L. Meng (2011). To Center or Not to Center: That Is Not the\nQuestion\u2013An Ancillarity-Sufficiency Interweaving Strategy (ASIS) for Boosting\nMCMC Efficiency. Journal of Computational and Graphical Statistics 20(3),\n531\u2013570.\nCovariance-Adaptive Slice Sampling.\nA SA and AA parameterizations\nA.1 Sufficient Augmentation (SA)\nWe derive here the quantities needed to apply the transition operators considered in this work\nin the SA parameterization. Let L = log[p(y|f)]. The log-joint density is:\nlog[p(y,f,\u03b8)] = L \u22121\n2log(|Q|) \u2212n\n2log(\u03c3) \u2212\n1\n2\u03c3fTQ\u22121f + log[p(\u03b8)] + const.\nNote that \u03c3 could be marginalized out, but it would not be possible to get manageable ex-\npressions for the metric tensor with respect to \u03c4; for f, instead, this would be possible. We do\nnot pursue this here, and we leave it for future investigation.\nBy inspecting the log-joint density, we see that we can obtain the conditional density for\n\u03c3 in the following form\nlog[p(\u03c3|y,f,\u03c4)] = \u2212n\n2log(\u03c3) \u2212\n1\n2\u03c3fTQ\u22121f + const.\nwhich we recognize as an inverse Gamma. By placing an inverse Gamma prior on \u03c3 in the\nform invGa(\u03c3|a,b) with shape a and scale b, we can sample directly:\n?\nThe gradients of the log-joint density needed to apply gradient based operators are:\n\u03c3 \u223c invGa\n\u03c3\n????a +n\n2,b +1\n2fTQ\u22121f\n?\n\u2207flog[p(y,f,\u03b8)] = \u2207fL \u22121\n?\n\u03c3Q\u22121f\n\u2202 log[p(y,f,\u03b8)]\n\u2202\u03c8\u03c4i\n= \u22121\n2Tr\nQ\u22121\u2202Q\n\u2202\u03c8\u03c4i\n?\n+\n1\n2\u03c3fTQ\u22121\u2202Q\n\u2202\u03c8\u03c4i\nQ\u22121f +\u2202 log[p(\u03c8\u03c4)]\n\u2202\u03c8\u03c4i\nThe FI for latent variables and parameters are:\nR = FIf,f= Ey\n?\n(\u2207fL)(\u2207fL)T?\n= \u2212Ey[\u2207f\u2207fL]"},{"page":22,"text":"22M. Filippone et al.\nFI\u03c8\u03c4,\u03c8\u03c4= Ef\n?\n(\u2207\u03c8\u03c4log[p(f|\u03c8\u03c4)])(\u2207\u03c8\u03c4log[p(f|\u03c8\u03c4)])T?\nGiven that the likelihood factorizes with respect to the observations, the Hessian of L with\nrespect to f is diagonal, so R = FIf,fis diagonal as well. The metric tensors are the FI matrices\nplus the negative Hessian of the priors:\nGf,f= R +1\n\u03c3Q\u22121\nG\u03c8\u03c4i,\u03c8\u03c4j= +1\n2Tr\n?\nQ\u22121\u2202Q\n\u2202\u03c8\u03c4j\nQ\u22121\u2202Q\n\u2202\u03c8\u03c4i\n?\n\u2212\u22022log[p(\u03c8\u03c4)]\n\u2202\u03c8\u03c4i\u2202\u03c8\u03c4j\nA.2 Ancillary Augmentation (AA)\nWe derive here the quantities needed to apply the transition operators considered in this work\nin the AA parameterization. The expression of the log-joint density is the same as in the SA\ncase, bearing in mind the transformation f =\u221a\u03c3L\u03bd; this yields:\nlog[p(y,\u03bd,\u03b8)] = L(y|\u03bd,\u03b8) \u22121\n2\u03bdT\u03bd + log[p(\u03b8)] + const.\nThe gradient with respect to the hyper-parameters can be computed by using the chain\nrule of derivation and standard properties of derivatives of vector valued functions:\n\u2202 log[p(y,\u03bd,\u03b8)]\n\u2202\u03c8\u03c4i\n=\u221a\u03c3(\u2207fL(y|f))T\u2202L\n\u2202\u03b8i\n\u03bd +\u2202 log[p(\u03c8\u03c4)]\n\u2202\u03c8\u03c4i\nThe FI matrix is readily obtained as:\nFI\u03b8i,\u03b8j= \u03c3\u03bdT\u2202LT\n\u2202\u03b8i\nR\u2202L\n\u2202\u03b8j\n\u03bd\nWith the contribution (negative Hessian) of the prior, the metric tensor used in the manifold\nmethods results in:\nG\u03b8i,\u03b8j= \u03c3\u03bdT\u2202LT\n\u2202\u03b8i\nR\u2202L\n\u2202\u03b8j\n\u03bd \u2212\u22022log[p(\u03b8)]\n\u2202\u03b8i\u2202\u03b8j\nB GP models considered in this paper\nB.1 Logistic regression with GP priors (LRG)\nLet:\nl+(f) = logistic(f) =\n1\n1 + exp(\u2212f)\nl\u2212(f) = 1 \u2212 l+(f) = logistic(\u2212f)\nIn logistic regression, observations follow a Bernoulli distribution with success probability given\nby a sigmoid transformation of the associated latent variables:\np(y|f) =\nn\n?\ni=1\np(yi|fi) =\nn\n?\ni=1\nBern(yi|l+(fi)) =\nn\n?\ni=1\nl+(fi)yil\u2212(fi)(1\u2212yi)\nThe gradient with respect to f results in:\n(\u2207fL)j= yj\u2212 l+(fj)\nThe computation of diagonal elements of the FI matrix for f requires the expectations of y2\nwhich are the same as the expectations of yi, that are l+\ni\ni; this leads to Rii= l+(fi)l\u2212(fi)."},{"page":23,"text":"Title Suppressed Due to Excessive Length 23\nB.2 Log-Gaussian Cox model (LCX)\nIn this model, observations follow a Poisson distribution with mean computed as an exponen-\ntially transformed version of the corresponding latent variables:\np(y|f) =\nn\n?\ni=1\np(yi|fi) =\nn\n?\ni=1\nPoisson(yi|exp(fi))\nThe gradient with respect to f and the diagonal elements of R result in:\n(\u2207fL)j= yj\u2212 exp(fj)Rii= exp(fi)\nB.3 Stochastic Volatility model with GP priors (VLT)\nIn this model, observations follow a zero mean Gaussian distribution with standard deviation\ncomputed as an exponentially transformed version of the corresponding latent variable:\np(y|f) =\nn\n?\ni=1\np(yi|fi) =\nn\n?\ni=1\nN(yi|0,exp(fi)2)\nThe gradient with respect to f and the diagonal elements of R result in:\n(\u2207fL)j= exp(fi)\u22122y2\nj\u2212 1Rii= 2\nB.4 Ordinal Regression with GP priors (ORD)\nIn this model, latent variables are thresholded at r points that will be denoted by b0,...,br,\nwith b0 = \u2212\u221e and br = +\u221e. Then, y is the index of the interval where the corresponding\nlatent variable f falls. The likelihood of an observed label yiassociated to the ith latent variable\nfiis then:\n\u00af p(yi|fi) = 1\nand zero otherwise. This model is usually modified to allow for a noise term \u03b4 (distributed as\nN(\u03b4|0,\u03c32\n?\nwhere:\nz(s)\ni\nif byi\u22121< f \u2264 byi\n\u03b4)) in the latent variables so that:\np(yi|fi) =\u00af p(yi|fi+ \u03b4)N(\u03b4|0,\u03c32\n\u03b4)d\u03b4 = \u03a6(z(yi)\ni\n) \u2212 \u03a6(z(yi\u22121)\ni\n)\n=bs\u2212 fi\n\u03c3\u03b4\nIn particular:\nL =\nn\n?\ni=1\nlog\n?\n\u03a6(z(yi)\ni\n) \u2212 \u03a6(z(yi\u22121)\ni\n)\n?\n(\u2207fL)i=\n1\n\u03c3\u03b4\nN(z(yi\u22121)\n\u03a6(z(yi)\ni\n|0,1) \u2212 N(z(yi)\n) \u2212 \u03a6(z(yi\u22121)\ni\n|0,1)\n)\ni\ni\nBy writing the diagonal elements of Hessian of the log-likelihood computed for yi= s\n(\u2207f\u2207fL)(s)\nii\n=\n1\n\u03c32\n\u03b4\nz(s)\ni\nN(z(s)\ni\n|0,1) \u2212 z(s\u22121)\n\u03a6(z(s)\ni\ni\nN(z(s\u22121)\n)\ni\n|0,1)\n) \u2212 \u03a6(z(s\u22121)\ni\n\u22121\n\u03c32\n\u03b4\n?\nN(z(s\u22121)\n\u03a6(z(s)\ni\n|0,1) \u2212 N(z(s)\n) \u2212 \u03a6(z(s\u22121)\ni\n|0,1)\n)\nii\n?2\nit is possible to compute the expectation of the negative Hessian as:\nRii= \u2212\nr\n?\ns=1\n(\u2207f\u2207fL)(s)\niip(s|fi) =\nr\n?\ns=1\n(\u2207f\u2207fL)(s)\nii\n?\n\u03a6(z(s\u22121)\ni\n) \u2212 \u03a6(z(s)\ni\n)\n?\nNote that the formulation in this paper is slightly different from the one in Chu and Ghahra-\nmani (2005), where \u03c3 is dropped and thresholds are inferred instead."}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Maurizio_Filippone\/publication\/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models\/links\/00463533941d28ab5b000000.pdf","widgetId":"rgw28_56ab1d7fd7391"},"id":"rgw28_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=257618460&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw29_56ab1d7fd7391"},"id":"rgw29_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=257618460&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":257618460,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":257618460,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2079162962,"url":"researcher\/2079162962_Xiaoyu_Xiong","fullname":"Xiaoyu Xiong","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":82161005,"url":"researcher\/82161005_Vaclav_Smidl","fullname":"V\u00e1clav \u0160m\u00eddl","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70871340,"url":"researcher\/70871340_Maurizio_Filippone","fullname":"Maurizio Filippone","last":true,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273666076311560%401442258485613_m"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Aug 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes","usePlainButton":true,"publicationUid":280773011,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes","title":"Adaptive Multiple Importance Sampling for Gaussian Processes","displayTitleAsLink":true,"authors":[{"id":2079162962,"url":"researcher\/2079162962_Xiaoyu_Xiong","fullname":"Xiaoyu Xiong","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":82161005,"url":"researcher\/82161005_Vaclav_Smidl","fullname":"V\u00e1clav \u0160m\u00eddl","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70871340,"url":"researcher\/70871340_Maurizio_Filippone","fullname":"Maurizio Filippone","last":true,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273666076311560%401442258485613_m"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"In applications of Gaussian processes where quantification of uncertainty is\na strict requirement, it is necessary to accurately characterize the posterior\ndistribution over Gaussian process covariance parameters. Normally, this is\ndone by means of Markov chain Monte Carlo (MCMC) algorithms. Focusing on\nGaussian process regression where the marginal likelihood is computable but\nexpensive to evaluate, this paper studies algorithms based on importance\nsampling to carry out expectations under the posterior distribution over\ncovariance parameters. The results indicate that expectations computed using\nAdaptive Multiple Importance Sampling converge faster per unit of computation\nthan those computed with MCMC algorithms for models with few covariance\nparameters, and converge as fast as MCMC for models with up to around twenty\ncovariance parameters.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Vasek_Smidl\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/links\/55e94dff08ae65b6389aee89.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Vasek_Smidl","sourceName":"Vasek Smidl","hasSourceUrl":true},"publicationUid":280773011,"publicationUrl":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/links\/55e94dff08ae65b6389aee89\/smallpreview.png","linkId":"55e94dff08ae65b6389aee89","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=280773011&reference=55e94dff08ae65b6389aee89&eventCode=&origin=publication_list","widgetId":"rgw33_56ab1d7fd7391"},"id":"rgw33_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=280773011&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"55e94dff08ae65b6389aee89","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":257618460,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["This paper focuses on the problem of inferring GP covariance parameters when the marginal likelihood is computable but expensive, namely when GPs are used for regression and the likelihood is Gaussian. To date, most of the literature proposes the use of Markov chain Monte Carlo (MCMC) methods to characterize the posterior distribution over covariance parameters [36] [34] [14] [29]. Generally, MCMC methods are based on the iteration of the following two operations (i) proposal and (ii) an accept\/reject step. "],"widgetId":"rgw34_56ab1d7fd7391"},"id":"rgw34_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw32_56ab1d7fd7391"},"id":"rgw32_56ab1d7fd7391","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=280773011&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":70871340,"url":"researcher\/70871340_Maurizio_Filippone","fullname":"Maurizio Filippone","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273666076311560%401442258485613_m"},{"id":19524381,"url":"researcher\/19524381_Mark_Girolami","fullname":"Mark Girolami","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Nov 2014","journal":"IEEE Transactions on Pattern Analysis and Machine Intelligence","showEnrichedPublicationItem":false,"citationCount":11,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/262954130_Pseudo-Marginal_Bayesian_Inference_for_Gaussian_Processes","usePlainButton":true,"publicationUid":262954130,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"5.78","url":"publication\/262954130_Pseudo-Marginal_Bayesian_Inference_for_Gaussian_Processes","title":"Pseudo-Marginal Bayesian Inference for Gaussian Processes","displayTitleAsLink":true,"authors":[{"id":70871340,"url":"researcher\/70871340_Maurizio_Filippone","fullname":"Maurizio Filippone","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273666076311560%401442258485613_m"},{"id":19524381,"url":"researcher\/19524381_Mark_Girolami","fullname":"Mark Girolami","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["IEEE Transactions on Pattern Analysis and Machine Intelligence 11\/2014;  DOI:10.1109\/TPAMI.2014.2316530"],"abstract":"The main challenges that arise when adopting Gaussian process priors in probabilistic modeling are how to carry out exact Bayesian inference and how to account for uncertainty on model parameters when making model-based predictions on out-of-sample data. Using probit regression as an illustrative working example, this paper presents a general and effective methodology based on the pseudo-marginal approach to Markov chain Monte Carlo that efficiently addresses both of these issues. The results presented in this paper show improvements over existing sampling methods to simulate from the posterior distribution over the parameters defining the covariance function of the Gaussian Process prior. This is particularly important as it offers a powerful tool to carry out full Bayesian inference of Gaussian Process based hierarchic statistical models in general. The results also demonstrate that Monte Carlo based integration of all model parameters is actually feasible in this class of models providing a superior quantification of uncertainty in predictions. Extensive comparisons with respect to state-of-the-art probabilistic classifiers confirm this assertion.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/262954130_Pseudo-Marginal_Bayesian_Inference_for_Gaussian_Processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Maurizio_Filippone\/publication\/262954130_Pseudo-Marginal_Bayesian_Inference_for_Gaussian_Processes\/links\/02e7e539706dd57b1b000000.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Maurizio_Filippone","sourceName":"Maurizio Filippone","hasSourceUrl":true},"publicationUid":262954130,"publicationUrl":"publication\/262954130_Pseudo-Marginal_Bayesian_Inference_for_Gaussian_Processes","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/262954130_Pseudo-Marginal_Bayesian_Inference_for_Gaussian_Processes\/links\/02e7e539706dd57b1b000000\/smallpreview.png","linkId":"02e7e539706dd57b1b000000","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=262954130&reference=02e7e539706dd57b1b000000&eventCode=&origin=publication_list","widgetId":"rgw36_56ab1d7fd7391"},"id":"rgw36_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=262954130&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"02e7e539706dd57b1b000000","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":257618460,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/262954130_Pseudo-Marginal_Bayesian_Inference_for_Gaussian_Processes\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Recently, there have been a few attempts to carry out inference using stochastic approximations based on Markov chain Monte Carlo (MCMC) methods [21], [22], [23], the idea being to leverage asymptotic guarantees of convergence of Monte Carlo estimates to the true values. "],"widgetId":"rgw37_56ab1d7fd7391"},"id":"rgw37_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw35_56ab1d7fd7391"},"id":"rgw35_56ab1d7fd7391","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=262954130&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":59251560,"url":"researcher\/59251560_Daniel_P_Simpson","fullname":"Daniel P. Simpson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":21587887,"url":"researcher\/21587887_Ian_W_Turner","fullname":"Ian W. Turner","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277184523587603%401443097348465_m\/Ian_Turner5.png"},{"id":18683691,"url":"researcher\/18683691_Christopher_M_Strickland","fullname":"Christopher M. Strickland","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10428941,"url":"researcher\/10428941_Anthony_N_Pettitt","fullname":"Anthony N. Pettitt","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Dec 2013","journal":null,"showEnrichedPublicationItem":false,"citationCount":1,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/259151452_Scalable_iterative_methods_for_sampling_from_massive_Gaussian_random_vectors","usePlainButton":true,"publicationUid":259151452,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/259151452_Scalable_iterative_methods_for_sampling_from_massive_Gaussian_random_vectors","title":"Scalable iterative methods for sampling from massive Gaussian random vectors","displayTitleAsLink":true,"authors":[{"id":59251560,"url":"researcher\/59251560_Daniel_P_Simpson","fullname":"Daniel P. Simpson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":21587887,"url":"researcher\/21587887_Ian_W_Turner","fullname":"Ian W. Turner","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277184523587603%401443097348465_m\/Ian_Turner5.png"},{"id":18683691,"url":"researcher\/18683691_Christopher_M_Strickland","fullname":"Christopher M. Strickland","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10428941,"url":"researcher\/10428941_Anthony_N_Pettitt","fullname":"Anthony N. Pettitt","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Sampling from Gaussian Markov random fields (GMRFs), that is multivariate\nGaussian ran- dom vectors that are parameterised by the inverse of their\ncovariance matrix, is a fundamental problem in computational statistics. In\nthis paper, we show how we can exploit arbitrarily accu- rate approximations to\na GMRF to speed up Krylov subspace sampling methods. We also show that these\nmethods can be used when computing the normalising constant of a large\nmultivariate Gaussian distribution, which is needed for both any\nlikelihood-based inference method. The method we derive is also applicable to\nother structured Gaussian random vectors and, in particu- lar, we show that\nwhen the precision matrix is a perturbation of a (block) circulant matrix, it\nis still possible to derive O(n log n) sampling schemes.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/259151452_Scalable_iterative_methods_for_sampling_from_massive_Gaussian_random_vectors","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Anthony_Pettitt\/publication\/259151452_Scalable_iterative_methods_for_sampling_from_massive_Gaussian_random_vectors\/links\/543faabf0cf2fd72f99c877f.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Anthony_Pettitt","sourceName":"Anthony N. Pettitt","hasSourceUrl":true},"publicationUid":259151452,"publicationUrl":"publication\/259151452_Scalable_iterative_methods_for_sampling_from_massive_Gaussian_random_vectors","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/259151452_Scalable_iterative_methods_for_sampling_from_massive_Gaussian_random_vectors\/links\/543faabf0cf2fd72f99c877f\/smallpreview.png","linkId":"543faabf0cf2fd72f99c877f","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=259151452&reference=543faabf0cf2fd72f99c877f&eventCode=&origin=publication_list","widgetId":"rgw39_56ab1d7fd7391"},"id":"rgw39_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=259151452&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"543faabf0cf2fd72f99c877f","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":257618460,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/259151452_Scalable_iterative_methods_for_sampling_from_massive_Gaussian_random_vectors\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["The preconditioners considered in this paper are closely linked to the concept of 'centred' and 'noncentred' parameteristations of statistical models (Papaspiliopoulos et al., 2007; Strickland et al., 2008; Yu and Meng, 2011; Filippone et al., 2013). The idea can be illustrated simply for parameterdependent latent Gaussian models "],"widgetId":"rgw40_56ab1d7fd7391"},"id":"rgw40_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw38_56ab1d7fd7391"},"id":"rgw38_56ab1d7fd7391","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=259151452&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":257618460,"publicationLink":"publication\/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw31_56ab1d7fd7391"},"id":"rgw31_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=257618460&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=9","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":9,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw30_56ab1d7fd7391"},"id":"rgw30_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=257618460&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"00463533941d28ab5b000000","name":"Maurizio Filippone","date":"Mar 31, 2014 ","nameLink":"profile\/Maurizio_Filippone","filename":"ml13.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Maurizio_Filippone\/publication\/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models\/links\/00463533941d28ab5b000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Maurizio_Filippone\/publication\/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models\/links\/00463533941d28ab5b000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"8286475a8f8702796d39eb9a3bf42d8f","showFileSizeNote":false,"fileSize":"376.45 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"00463533941d28ab5b000000","name":"Maurizio Filippone","date":"Mar 31, 2014 ","nameLink":"profile\/Maurizio_Filippone","filename":"ml13.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Maurizio_Filippone\/publication\/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models\/links\/00463533941d28ab5b000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Maurizio_Filippone\/publication\/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models\/links\/00463533941d28ab5b000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"8286475a8f8702796d39eb9a3bf42d8f","showFileSizeNote":false,"fileSize":"376.45 KB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=wZJyO0MUTY24hUoTX1dqsmzgENqK5gubbV2lUpqBrPsLbLqMbznyDGBGcDvhqA8B03lVLJHlsXMv5ekd1OW-_w.0G9uHqq4Ss4eSG_OctU-_gotT0SCyCI6wY2sxKn1ph0TRUMDbzoO2Pdug3uhshmztuFQE81ufaFlmblJOCFiNw","clickOnPill":"publication.PublicationFigures.html?_sg=T17ttVrGQqUm8aiskrPd6Omtd88405PcYMmS0lSJnSYeN8Ct4KpcNJCBUQ_jboFc0cBy3YwGBSvGyKOwB6JE1A.xLXT25kBX07UmJBzFKqMhmoHsKCgnhJU2GT_znYJ_M35_KyzzXZjM7sg93_TOdwiVIQmlpPdkLEYV5VcNlL9Kw"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1o9o3\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMaurizio_Filippone%2Fpublication%2F257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models%2Flinks%2F00463533941d28ab5b000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1o9o3\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=LAN3LUDjuQWfpbcS7DHH2BCyWMxOsh9DKq3_m14POKowB_B438mECAPeI9K1eTkurXeDUrmGm7YyeA4wLamZZQ","urlHash":"4a660333b3aa269aafef57ae70d0bd88","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=pSQ0y1LDXTMYpOhJhZ_ZtVLApvcXGP0YgYRP7CfZjQl0f2wNSffxGmvAgDOOS-ircX5o_LcHzjrUoA4Xh8pQF3qcHIHPI_NL50CeoUS3BnE.wE5XCyFUkvMTrjkND5JV-wyfQVPPq5MOLkq21x3WmI7UAzTA9P0h0H3z_gsDKn-sPgeaAl40P09ei1ZF7SZ6Jg.ibuBjRZdWEkLlOvXe0fuMY_vRZzJiH-78YrhXcFf_DvAMNJPjD3J_-aa_p2VzHQ1KDNvob02kmY9AIPoQC1SeQ","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"00463533941d28ab5b000000","trackedDownloads":{"00463533941d28ab5b000000":{"v":false,"d":false}},"assetId":"AS:100023157133325@1400858788145","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":257618460,"commentCursorPromo":null,"widgetId":"rgw42_56ab1d7fd7391"},"id":"rgw42_56ab1d7fd7391","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMaurizio_Filippone%2Fpublication%2F257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models%2Flinks%2F00463533941d28ab5b000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A100023157133325%401400858788145&publicationUid=257618460&linkId=00463533941d28ab5b000000&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"A comparative evaluation of stochastic-based inference methods for Gaussian process models","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=pbWLykHYefWJuPTBKnEmTr74nHVveMBZj_2n3JrkeGECJamKCa0Er3hkeBwN4Oj3x-UB1csRBd0o7OP-DLb8v_WR7K7qFI9_JFUg75MC34Q.AyZytKBP_Hhf886olwts0AP0Wu1q3ByRU2jLT1bxeg0QsdTuhCtplPHei0wd0gETliB-O7YOb9pOBWmN4k0mVg.G9F8SizFvWGD5OaNWy2XmPfu1Bhq7v74wZvb2YAjhv0hsi60x8K15XXWZKkaX2JMe8FAiWqMv6m3mss2_qP30w","publicationUid":257618460,"trackedDownloads":{"00463533941d28ab5b000000":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw44_56ab1d7fd7391"},"id":"rgw44_56ab1d7fd7391","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw45_56ab1d7fd7391"},"id":"rgw45_56ab1d7fd7391","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw46_56ab1d7fd7391"},"id":"rgw46_56ab1d7fd7391","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw47_56ab1d7fd7391"},"id":"rgw47_56ab1d7fd7391","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw48_56ab1d7fd7391"},"id":"rgw48_56ab1d7fd7391","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw43_56ab1d7fd7391"},"id":"rgw43_56ab1d7fd7391","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw41_56ab1d7fd7391"},"id":"rgw41_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1d7fd7391"},"id":"rgw2_56ab1d7fd7391","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":257618460},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=257618460&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1d7fd7391"},"id":"rgw1_56ab1d7fd7391","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"eFn4oM48HkElv+fbtFPu29an7KTmnk3bfPhIiOWYT9cpUjCsehOtz8RoCVzLLC\/uaMvMsp6vbxPkZlNUKLbwGGAmk6lRHQI4B+mcrY1scNKDWV7qn3IbdGQ\/cXyx0AfNc6l4TBl7XGRjOIP6b9Fb7GAlVcW1qA7sntOktCx60Z1OggdeS9B4KMTbVPzFQVsGku\/9VffHIbs48nrfF+ebZc3Msfc4YLGwMvYW7bdGOEkFoXrVmuJwLRuYrz\/2La1KVWPCPrXndpjY7Kqtd4bs4jjje68wklFMJlgW5SqdL6A=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"A comparative evaluation of stochastic-based inference methods for Gaussian process models\" \/>\n<meta property=\"og:description\" content=\"Gaussian Process (GP) models are extensively used in data analysis given their flexible modeling capabilities and interpretability. The fully Bayesian treatment of GP models is analytically...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models\/links\/00463533941d28ab5b000000\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models\" \/>\n<meta property=\"rg:id\" content=\"PB:257618460\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1007\/s10994-013-5388-x\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"A comparative evaluation of stochastic-based inference methods for Gaussian process models\" \/>\n<meta name=\"citation_author\" content=\"M. Filippone\" \/>\n<meta name=\"citation_author\" content=\"M. Zhong\" \/>\n<meta name=\"citation_author\" content=\"M. Girolami\" \/>\n<meta name=\"citation_publication_date\" content=\"2013\/10\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"Machine Learning\" \/>\n<meta name=\"citation_issn\" content=\"0885-6125\" \/>\n<meta name=\"citation_volume\" content=\"93\" \/>\n<meta name=\"citation_issue\" content=\"1\" \/>\n<meta name=\"citation_doi\" content=\"10.1007\/s10994-013-5388-x\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Maurizio_Filippone\/publication\/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models\/links\/00463533941d28ab5b000000.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-80fde15b-4298-450a-b331-d6f7ba289b01","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":880,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw49_56ab1d7fd7391"},"id":"rgw49_56ab1d7fd7391","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-80fde15b-4298-450a-b331-d6f7ba289b01", "57e6c87ff0621752f49014b7d5e5f78b90e9a52b");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-80fde15b-4298-450a-b331-d6f7ba289b01", "57e6c87ff0621752f49014b7d5e5f78b90e9a52b");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw50_56ab1d7fd7391"},"id":"rgw50_56ab1d7fd7391","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/257618460_A_comparative_evaluation_of_stochastic-based_inference_methods_for_Gaussian_process_models","requestToken":"uxdmyc3scNRMoJgtXnJtdZW3IqbbS\/3tpRdPWDrRXvFSiTp9\/Jzck3hoTCUUPo7VQMiFXe2IMM7SxMr+Sk+xWsrjsliVwsj7iuJVhwV5FOHeWOd\/M\/zEfj6s\/ENZ8TnbNttKmjLTQarcpJ61uR8N1LuOHjJh5ohxE0BR0OrpjHeyk02RIXygu3Wfb8\/xS4skAW6PE7PXE+uMellQJp8\/FR+U4xPFt8Mga3iKuj6d1z0YRIQCAQlSEc6BpofsWz+ll5BSEGx0ccx7+v0LywazuxKUpE9Tor1efB++zxf6hiM=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=qEm6MrICDwwmww44HvboxUdu3KGk-b7DgJzTxMM_iWGD4e0zyBxFIq9Z0yjVmKGb","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjU3NjE4NDYwX0FfY29tcGFyYXRpdmVfZXZhbHVhdGlvbl9vZl9zdG9jaGFzdGljLWJhc2VkX2luZmVyZW5jZV9tZXRob2RzX2Zvcl9HYXVzc2lhbl9wcm9jZXNzX21vZGVscw%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw52_56ab1d7fd7391"},"id":"rgw52_56ab1d7fd7391","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw51_56ab1d7fd7391"},"id":"rgw51_56ab1d7fd7391","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw53_56ab1d7fd7391"},"id":"rgw53_56ab1d7fd7391","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
