<!DOCTYPE html> <html lang="en" class="" id="rgw24_56aba07e5d27a"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="EbQaOb9XpnwA9oHq1Idg6CiDE95ixmCTDZ3TmuIMotdErDva6RjAOJFrxoz6gm+3+K3kHXU7bLY76J2VGwjewYaEm1aeLF5TgT+G1Vs84s9uSF0AeKWQYDqkZiuq2NVpHpY2IGoqqrJW4jgF+s6AMM5LSVADHCo96xaLfQ2gLDJOhkMoJLHeZTXmethR5V5rkAL2OoxC80yxlKnVXuETE3P8kxQYhyYcT1sGYe/GvM7vT1Qb20Kq6MFi8BiqQKpLYi2UhST6az4srjhkHpjHSZ/jtelKpJncvfLXYwUAVDc="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-336539a7-9ccf-48e2-8059-e6854f6ab797",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/220320048_Variational_Learning_of_Inducing_Variables_in_Sparse_Gaussian_Processes" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Variational Learning of Inducing Variables in Sparse Gaussian Processes" />
<meta property="og:description" content="Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/220320048_Variational_Learning_of_Inducing_Variables_in_Sparse_Gaussian_Processes/links/0ffa78e00cf25dfdcf53dd6e/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/220320048_Variational_Learning_of_Inducing_Variables_in_Sparse_Gaussian_Processes" />
<meta property="rg:id" content="PB:220320048" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Variational Learning of Inducing Variables in Sparse Gaussian Processes" />
<meta name="citation_author" content="Michalis K. Titsias" />
<meta name="citation_publication_date" content="2009/04/01" />
<meta name="citation_volume" content="5" />
<meta name="citation_firstpage" content="567" />
<meta name="citation_lastpage" content="574" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/220320048_Variational_Learning_of_Inducing_Variables_in_Sparse_Gaussian_Processes" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/220320048_Variational_Learning_of_Inducing_Variables_in_Sparse_Gaussian_Processes" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Variational Learning of Inducing Variables in Sparse Gaussian Processes</title>
<meta name="description" content="Variational Learning of Inducing Variables in Sparse Gaussian Processes on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56aba07e5d27a" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56aba07e5d27a" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56aba07e5d27a">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Variational%20Learning%20of%20Inducing%20Variables%20in%20Sparse%20Gaussian%20Processes&rft.title=Journal%20of%20Machine%20Learning%20Research%20-%20Proceedings%20Track&rft.jtitle=Journal%20of%20Machine%20Learning%20Research%20-%20Proceedings%20Track&rft.volume=5&rft.date=2009&rft.pages=567-574&rft.au=Michalis%20K.%20Titsias&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Variational Learning of Inducing Variables in Sparse Gaussian Processes</h1> <meta itemprop="headline" content="Variational Learning of Inducing Variables in Sparse Gaussian Processes">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/220320048_Variational_Learning_of_Inducing_Variables_in_Sparse_Gaussian_Processes/links/0ffa78e00cf25dfdcf53dd6e/smallpreview.png">  <div id="rgw7_56aba07e5d27a" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56aba07e5d27a" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Michalis_Titsias2" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A279280278884356%401443597015332_m" title="Michalis Titsias" alt="Michalis Titsias" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Michalis Titsias</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw9_56aba07e5d27a" data-account-key="Michalis_Titsias2">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Michalis_Titsias2"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A279280278884356%401443597015332_l" title="Michalis Titsias" alt="Michalis Titsias" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Michalis_Titsias2" class="display-name">Michalis Titsias</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Athens_University_of_Economics_and_Business" title="Athens University of Economics and Business">Athens University of Economics and Business</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">     Journal of Machine Learning Research - Proceedings Track   <meta itemprop="datePublished" content="2009-04">  04/2009;  5:567-574.             <div class="pub-source"> Source: <a href="http://dblp.uni-trier.de/db/journals/jmlr/jmlrp5.html#Titsias09" rel="nofollow">DBLP</a> </div>  </div> <div id="rgw10_56aba07e5d27a" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are defined to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.</div> </p>  </div>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw23_56aba07e5d27a">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw22_56aba07e5d27a"  itemprop="articleBody">  <p>Page 1</p> <p>Variational Learning of Inducing Variables in Sparse Gaussian<br />Processes<br />Michalis K. Titsias<br />School of Computer Science,<br />University of Manchester, UK<br />mtitsias@cs.man.ac.uk<br />Abstract<br />Sparse Gaussian process methods that use in-<br />ducing variables require the selection of the<br />inducing inputs and the kernel hyperparam-<br />eters. We introduce a variational formula-<br />tion for sparse approximations that jointly<br />infers the inducing inputs and the kernel hy-<br />perparameters by maximizing a lower bound<br />of the true log marginal likelihood. The key<br />property of this formulation is that the in-<br />ducing inputs are defined to be variational<br />parameters which are selected by minimizing<br />the Kullback-Leibler divergence between the<br />variational distribution and the exact poste-<br />rior distribution over the latent function val-<br />ues. We apply this technique to regression<br />and we compare it with other approaches in<br />the literature.<br />1INTRODUCTION<br />The application of Gaussian process (GP) models is in-<br />tractable for large datasets because the time complex-<br />ity scales as O(n3) and the storage as O(n2) where<br />n is the number of training examples. To overcome<br />this limitation, many approximate or sparse meth-<br />ods have been proposed in the literature (Williams<br />and Seeger, 2001; Smola and Bartlett, 2001; Csato<br />and Opper, 2002; Lawrence et al., 2002; Seeger<br />et al., 2003; Schwaighofer and Tresp, 2003; Snelson<br />and Ghahramani, 2006; Qui˜ nonero-Candela and Ras-<br />mussen, 2005). These methods construct an approxi-<br />mation based on a small set of m support or inducing<br />variables that allow the reduction of the time com-<br />Appearing in Proceedings of the 12thInternational Confe-<br />rence on Artificial Intelligence and Statistics (AISTATS)<br />2009, Clearwater Beach, Florida, USA. Volume 5 of JMLR:<br />W&amp;CP 5. Copyright 2009 by the authors.<br />plexity from O(n3) to O(nm2). They mainly differ<br />in the strategies they use to select the inducing in-<br />puts which are typically selected from the training or<br />test examples. Snelson and Ghahramani (2006) allow<br />the inducing variables to be considered as auxiliary<br />pseudo-inputs that are inferred along with kernel hy-<br />perparameters using continuous optimization.<br />Approximate marginal likelihoods are appropriate ob-<br />jective functions for model selection in sparse GP mod-<br />els.Existing state-of-the-art methods (Snelson and<br />Ghahramani, 2006; Seeger et al., 2003) derive such ap-<br />proximations by modifying the GP prior (Qui˜ nonero-<br />Candela and Rasmussen, 2005) and then computing<br />the marginal likelihood of the modified model. This<br />approach turns the inducing inputs into additional ker-<br />nel hyperparameters. While this can increase flexibil-<br />ity when we fit the data, it can also lead to overfitting<br />when we optimize with respect to all unknown hyper-<br />parameters. Furthermore, fitting a modified model is<br />not so rigorous approximation procedure since there is<br />no distance between the exact and the modified model<br />that is minimized.<br />In this paper we introduce a variational method that<br />jointly selects the inducing inputs and the hyperpa-<br />rameters by maximizing a lower bound to the exact<br />marginal likelihood. The important difference between<br />this formulation and previous methods is that here the<br />inducing inputs are defined to be variational param-<br />eters which are selected by minimizing the Kullback-<br />Leibler (KL) divergence between a variational GP and<br />the true posterior GP. This allows i) to avoid overfit-<br />ting and ii) to rigorously approximate the exact GP<br />model by minimizing a distance between the sparse<br />model and the exact one. The selection of the inducing<br />inputs and hyperparameters is achieved either by ap-<br />plying continuous optimization over all unknown quan-<br />tities or by using a variational EM algorithm where at<br />the E step we greedily select the inducing inputs from<br />the training data and at the M step we update the<br />hyperparameters. In contrast to previous greedy ap-</p>  <p>Page 2</p> <p>Variational Learning of Inducing Variables in Sparse Gaussian Processes<br />proaches, e.g. (Seeger et al., 2003), our scheme mono-<br />tonically increases the optimized objective function.<br />We apply the variational method to regression with<br />additive Gaussian noise and we compare its perfor-<br />mance to training schemes based on the projected pro-<br />cess marginal likelihood (Seeger et al., 2003; Csato and<br />Opper, 2002) and the sparse pseudo-inputs marginal<br />likelihood (Snelson and Ghahramani, 2006).<br />Our method is most closely related to the variational<br />sparse GP method described in (Csato and Opper,<br />2002; Seeger, 2003) that is applied to GP classifica-<br />tion (Seeger, 2003). The main difference between our<br />formulation and these techniques is that we maximize<br />a variational lower bound in order to select the induc-<br />ing inputs, while these methods use variational bounds<br />for estimating only the kernel hyperparameters.<br />2SPARSE GP REGRESSION<br />A GP is a set of random variables {f(x)|x ∈ X} for<br />which any finite subset follows a Gaussian distribution.<br />To describe a GP, we only need to specify the mean<br />function m(x) and a covariance function k(x,x′). The<br />covariance function typically depends on a set of hy-<br />perparameters θ. A GP can be used as a prior over a<br />real-valued function f(x). This prior can be combined<br />with data to give a posterior over the function.<br />Suppose we have a training dataset {(xi,yi)}n<br />noisy realizations of some unobserved or latent func-<br />tion so that each scalar yiis obtained by adding Gaus-<br />sian noise to f(x) at input xi, i.e. yi= fi+ ǫi, where<br />ǫi∼ N(0,σ2) and fi= f(xi). Let X denote all train-<br />ing inputs, y all outputs and f the corresponding train-<br />ing latent function values. The joint probability model<br />is p(y,f) = p(y|f)p(f) where p(y|f) is the likelihood<br />and p(f) the GP prior. The data induce a posterior<br />GP which is specified by a posterior mean function and<br />a posterior covariance function:<br />i=1of n<br />my(x) = Kxn(σ2I + Knn)−1y,<br />(1)<br />ky(x,x′) = k(x,x′) − Kxn(σ2I + Knn)−1Knx′.<br />Here, Knnis the n×n covariance matrix on the train-<br />ing inputs, Kxn is n-dimensional row vector of ker-<br />nel function values between x and the training in-<br />puts and Knx = KT<br />xn.<br />posterior GP can be answered by the above mean<br />and covariance functions.<br />sian posterior distribution p(f|y) on the training la-<br />tent variables f is computed by evaluating eq. (1) at<br />the inputs X. Similarly the prediction of the output<br />y∗= f∗+ ǫ∗at some unseen input x∗is described by<br />p(y∗|y) = N(y∗|my(x∗),ky(x∗,x∗) + σ2). The poste-<br />rior GP depends on the values of the hyperparameters<br />Any query related to the<br />For instance, the Gaus-<br />(θ,σ2) which can be estimated by maximizing the log<br />marginal likelihood given by<br />logp(y) = log[N(y|0,σ2I + Knn)].<br />(2)<br />Although the above GP approach is elegant, it is in-<br />tractable for large datasets since the computations re-<br />quire the inversion of a matrix of size n × n which<br />scales as O(n3). Thus, we need to consider approx-<br />imate or sparse methods in order to deal with large<br />datasets. Advanced sparse methods use a small set<br />of m function points as support or inducing variables.<br />This yields a time complexity that scales as O(nm2).<br />Important issues in these methods involve the selec-<br />tion of the inducing variables and the hyperparame-<br />ters. For reviews of current approaches see chapter 8<br />in (Rasmussen and Williams, 2006) and (Qui˜ nonero-<br />Candela and Rasmussen, 2005).<br />Suppose we wish to use m inducing variables to con-<br />struct our sparse GP method. The inducing variables<br />are latent function values evaluated at some inputs<br />Xm. Xmcan be a subset of the training inputs or aux-<br />iliary pseudo-points (Snelson and Ghahramani, 2006).<br />Learning Xmand the hyperparameters (θ,σ2) is the<br />crucial problem we need to solve in order to obtain<br />a sparse GP method. An approximation to the true<br />log marginal likelihood in eq. (2) can allow us to infer<br />these quantities. The current state-of-the-art approxi-<br />mate marginal likelihood is given in the sparse pseudo-<br />inputs GP method (SPGP) proposed in (Snelson and<br />Ghahramani, 2006). A related objective function used<br />in (Seeger et al., 2003) corresponds to the projected<br />process approximation (PP). These approximate log<br />marginal likelihoods have the form<br />F = log[N(y|0,σ2I + Qnn)],<br />(3)<br />where Qnnis an approximation to the true covariance<br />Knn. In PP, Qnn= KnmK−1<br />variance is replaced by the Nystr¨ om approximation.<br />Here, Kmmis the m×m covariance matrix on the in-<br />ducing inputs, Knmis the n×m cross-covariance ma-<br />trix between training and inducing points and Kmn=<br />KT<br />KnmK−1<br />mmKmn, i.e. the Nystr¨ om approximation is cor-<br />rected to be exact in the diagonal. By contrasting eq.<br />(2) with (3), it is clear that F is obtained by mod-<br />ifying the GP prior. This implies that the inducing<br />inputs Xmplay the role of extra kernel hyperparam-<br />eters (similar to θ) that parametrize the covariance<br />matrix Qnn. However because the prior has changed,<br />continuous optimization of F with respect to Xmdoes<br />not reliably approximate the exact GP model. Fur-<br />ther, since F is heavily parametrized with the extra<br />hyperparameters Xm, overfitting can occur especially<br />when we jointly optimize over (Xm,θ,σ2).<br />mmKmn, i.e. the exact co-<br />nm. In SPGP, Qnn= diag[Knn− KnmK−1<br />mmKmn] +</p>  <p>Page 3</p> <p>Titsias<br />In the next section, we propose a formulation for<br />sparse GP regression that follows a different philos-<br />ophy. Rather than modifying the exact GP model, we<br />minimize a distance between the exact posterior GP<br />and a variational approximation. The inducing inputs<br />Xmbecome now variational parameters which are rig-<br />orously selected so as the distance is minimized.<br />3VARIATIONAL LEARNING<br />We wish to define a sparse method that directly ap-<br />proximates the posterior GP mean and covariance<br />functions in eq. (1).This posterior GP can be<br />also described by the predictive Gaussian p(z|y) =<br />?p(z|f)p(f|y)df, where p(z|f) denotes the conditional<br />prior over any finite set of function points z. Suppose<br />that we wish to approximate the above Bayesian inte-<br />gral by using a small set of m auxiliary inducing vari-<br />ables fmevaluated at the pseudo-inputs Xm, which are<br />independent from the training inputs. fmare just func-<br />tion points drawn from the same GP prior as the train-<br />ing function values f. By using the augmented joint<br />model p(y|f)p(z,fm,f), we equivalently write p(z|y) as<br />?<br />p(z|y) =<br />p(z|fm,f)p(f|fm,y)p(fm|y)dfdfm.<br />(4)<br />Suppose now that fm is a sufficient statistic for the<br />parameter f in the sense that z and f are independent<br />given fm, i.e. it holds p(z|fm,f) = p(z|fm). The above<br />can be written as<br />?<br />?<br />q(z) =<br />p(z|fm)p(f|fm)φ(fm)dfdfm<br />=<br />p(z|fm)φ(fm)dfm=<br />?<br />q(z,fm)dfm,<br />(5)<br />where q(z) = p(z|y) and φ(fm) = p(fm|y).<br />p(f|fm) = p(f|fm,y) is true since y is a noisy ver-<br />sion of f and because of the assumption we made that<br />any z is conditionally independent from f given fm1.<br />In practise it is difficult to find inducing variables fm<br />that are sufficient statistics. Thus, we expect q(z) to<br />be only an approximation to p(z|y). In such case, we<br />can choose φ(fm) to be a “free” variational Gaussian<br />distribution, where in general φ(fm) ?= p(fm|y), that<br />depends on a mean vector µ and a covariance matrix<br />A. By using eq. (5), we can write down the approxi-<br />mate posterior GP mean and covariance functions as<br />follows<br />mq<br />Here,<br />y(x) = KxmK−1<br />mmµ,<br />(6)<br />kq<br />y(x,x′) = k(x,x′) − KxmK−1<br />mmKmx′ + KxmBKmx′,<br />1From p(z|fm,y) =<br />fact p(z|fm,f) = p(z|fm), the result follows.<br />R<br />p(y|f)p(z,fm,f)df<br />p(y|f)p(z,fm,f)dfdzand by using the<br />R<br />where B = K−1<br />eral form of the sparse posterior GP which is computed<br />in O(nm2). The question that now arises is how do we<br />select the φ distribution, i.e. (µ, A), and the inducing<br />inputs Xm. Next we describe a variational method<br />that allows to jointly specify these quantities and treat<br />Xmas a variational parameter which is rigorously se-<br />lected by minimizing the KL divergence.<br />mmAK−1<br />mm. The above defines the gen-<br />A principled procedure to specify φ and the inducing<br />inputs Xmis to form the variational distribution q(f)<br />and the exact posterior p(f|y) on the training function<br />values f, and then minimize a distance between these<br />two distributions. Equivalently, we can minimize a dis-<br />tance between the augmented true posterior p(f,fm|y)<br />and the augmented variational posterior q(f,fm) where<br />clearly from eq. (5) q(f,fm) = p(f|fm)φ(fm).<br />augmented true posterior is associated with the aug-<br />mented joint model<br />The<br />p(y,f,fm) = p(y|f)p(f|fm)p(fm),<br />(7)<br />which is equivalent to the initial model p(y,f) =<br />p(f|y)p(f), since by marginalizing out fmfrom the for-<br />mer we always recover the latter. In particular, notice<br />that the conditional prior p(f|fm) and the marginal<br />prior p(fm) depend on the specific values of the in-<br />ducing inputs Xm. However, this dependence never<br />affects the posterior p(f|y) or the marginal likelihood<br />p(y). Hence, the augmented representation has a set<br />of “free” parameters Xmwhich can be treated as varia-<br />tional parameters as opposed to the model parameters.<br />To determine the variational quantities (Xm,φ), we<br />minimize the KL divergence KL(q(f,fm)||p(f,fm|y)).<br />This minimization is equivalently expressed as the<br />maximization of the following variational lower bound<br />of the true log marginal likelihood:<br />?<br />FV(Xm,φ) =<br />p(f|fm)φ(fm)logp(y|f)p(fm)<br />φ(fm)<br />dfdfm,<br />(8)<br />where the term p(f|fm) inside the log cancels out. We<br />can firstly maximize the bound by analytically solving<br />for the optimal choice of the variational distribution<br />φ. The bound after this maximization is<br />FV(Xm) = log?N(y|0,σ2I + Qnn)?−<br />1<br />2σ2Tr(? K),<br />(9)<br />where Qnn = KnmK−1<br />Knn−KnmK−1<br />bound are given in a technical report (Titsias, 2009).<br />The novelty of the above objective function is that<br />it contains a regularization trace term: −<br />This clearly differentiates FV from all marginal likeli-<br />hoods, described by eq. (3), that were previously ap-<br />plied to sparse GP regression. We will analyze the<br />trace term shortly.<br />mmKmn and? K = Cov(f|fm) =<br />mmKmn. Details of the derivation of this<br />1<br />2σ2Tr(? K).</p>  <p>Page 4</p> <p>Variational Learning of Inducing Variables in Sparse Gaussian Processes<br />The quantity in eq. (9) is computed in O(nm2) time<br />and is a lower bound of the true log marginal likeli-<br />hood for any value of the inducing inputs Xm. Fur-<br />ther maximization of the bound can be achieved by<br />optimizing over Xm and optionally over the number<br />of these variables. Note that the inducing inputs de-<br />termine the flexibility of the variational distribution<br />q(f,fm) = p(f|fm)φ(fm) since by tuning Xmwe adapt<br />both p(f|fm) and the underlying optimal distribution<br />φ∗. To compute this optimal φ∗, we differentiate eq.<br />(8) with respect to φ(fm) without imposing any con-<br />straints. This gives:<br />φ∗(fm) = N(fm|µ,A),<br />(10)<br />where µ = σ−2KmmΣKmny, A = KmmΣKmm and<br />Σ = (Kmm+ σ−2KmnKnm)−1. This now fully speci-<br />fies our variational GP and we can use eq. (6) to make<br />predictions in unseen input points. Clearly, the pre-<br />dictive distribution is exactly the one used by the pro-<br />jected process (PP) that has been previously proposed<br />in (Csato and Opper, 2002; Seeger et al., 2003). Thus,<br />as far as the predictive distribution is concerned the<br />above method is equivalent to PP.<br />However, the variational method is very different to<br />PP and SPGP as far as the selection of the inducing<br />inputs and the kernel hyperparameters is concerned.<br />This is because of the extra regularization term that<br />appears in the bound in eq. (9) and does not appear in<br />the approximate log marginal likelihoods used in PP<br />(Seeger et al., 2003) and SPGP (Snelson and Ghahra-<br />mani, 2006). As discussed in section 2, for the latter<br />objective functions, the role of Xm is to form a set<br />of extra kernel hyperparameters. In contrast, for the<br />lower bound, the inputs Xm become variational pa-<br />rameters due to the KL divergence that is minimized.<br />To look into the functional form of the bound, note<br />that FV is the sum of the PP log likelihood and the<br />regularization trace term −1<br />tempts to maximize the PP log likelihood and simul-<br />taneously minimize the trace Tr(? K). Tr(? K) repre-<br />which also corresponds to the squared error of predict-<br />ing the training latent values f from the inducing vari-<br />ables fm:<br />Tr(? K) = 0, the Nystr¨ om approximation is exact, i.e.<br />ing variables become sufficient statistics and we can<br />reproduce exactly the full GP prediction. Note that<br />the trace Tr(? K) itself has been used as a criterion for<br />in (Smola and Sch¨ olkopf, 2000) and is similar to the<br />criterion used in (Lawrence et al., 2002).<br />2σ−2Tr(? K). Thus, FV at-<br />sents the total variance of the conditional prior p(f|fm)<br />?p(f,fm)||KnmK−1<br />mmfm− f||2dfdfm. When<br />Knn= KnmK−1<br />mmKmn, which means that the induc-<br />selecting the inducing points from the training data<br />When we maximize the variational lower bound, the<br />hyperparameters (σ2,θ) are regularized. It is easy to<br />see how this is achieved for the noise variance σ2. At<br />a local maxima, σ2satisfies:<br />?<br />fm<br />σ2=1<br />n<br />φ∗(fm)||y − α||2dfm+1<br />nTr(? K),<br />(11)<br />where ||z|| denotes the Euclidean norm and α =<br />mmfm. This decomposition reveals<br />that the obtained σ2will be equal to the estimated<br />“actual” noise plus a “correction” term that is the av-<br />erage squared error of predicting the training latent<br />values from the inducing variables.<br />?[f|fm] = KnmK−1<br />So far we assumed that the inducing inputs are se-<br />lected by applying gradient-based optimization. How-<br />ever, this can be difficult in high dimensional input<br />spaces as the number of variables becomes very large.<br />Further, the kernel function might not be differentiable<br />with respect to the inputs. In such cases we can still<br />apply the variational method by selecting the induc-<br />ing inputs from the training inputs.<br />property of this discrete optimization scheme is that<br />FV monotonically increases when we greedily select in-<br />ducing inputs and adapt the hyperparameters. Next<br />we discuss this greedy selection method.<br />An important<br />3.1GREEDY SELECTION<br />Let m ⊂ {1,...,n} be the indices of a subset of data<br />that are used as the inducing variables. The training<br />points that are not part of the inducing set are indexed<br />by n−m and are called the remaining points, e.g. fn−m<br />denotes the remaining latent function values.<br />variational method is applied similarly to the pseudo-<br />inputs case.Assuming the variational distribution<br />q(f) = p(fn−m|fm)φ(fm), we can express a variational<br />bound that has the same form as the bound in eq. (9)<br />with the only difference that? K = Cov(fn−m|fm).<br />The selection of inducing variables among the training<br />data requires a prohibitive combinatorial search. A<br />suboptimal solution can be based on a greedy selection<br />scheme where we start with an empty inducing set<br />m = ∅ and a remaining set n − m = {1,...,n}. At<br />each iteration, we add a training point j ∈ J ⊂ n−m,<br />where J is a randomly chosen working set, into the<br />inducing set that maximizes the selection criterion ∆j.<br />The<br />It is important to interleave the greedy selection pro-<br />cess with the adaption of the hyperparameters (σ2,θ).<br />This can be viewed as an EM-like algorithm; at the<br />E step we add one point into the inducing set and at<br />the M step we update the hyperparameters. To ob-<br />tain a reliable convergence, the approximate marginal<br />likelihood must monotonically increase at each E or M<br />step. The PP and SPGP log likelihoods do not sat-<br />isfy such a requirement because they can also decrease<br />as we add points into the inducing set. In contrast,</p>  <p>Page 5</p> <p>Titsias<br />the bound FV is guaranteed to monotonically increase<br />since now the EM-like algorithm is a variational EM.<br />To clarify this, we state the following proposition.<br />Proposition 1. Let (Xm,fm) be the current set of<br />inducing points and m the corresponding set of indices.<br />Any point i ∈ n − m added into the inducing set can<br />never decrease the lower bound.<br />Proof:<br />the variational distribution is p(fn−m|fm)φ∗(fm) =<br />p(fn−(m∪i)|fi,fm)p(fi|fm)φ∗(fm).<br />new point, the term p(fi|fm)φ∗(fm) is replaced by the<br />optimal φ∗(fi,fm) distribution.<br />crease the lower bound or leave it invariant. A more<br />detailed proof is given in (Titsias, 2009).<br />Before the new point (fi,xi) is added,<br />When we add the<br />This can either in-<br />A consequence of the above proposition is that the<br />greedy selection process monotonically increases the<br />lower bound and this holds for any possible crite-<br />rion ∆. An obvious choice is to use FV as the cri-<br />terion, which can be evaluated in O(nm) time for any<br />candidate point in the working set J. Such a selec-<br />tion process maximizes the decrease in the divergence<br />KL(q(f)||p(f|y)).<br />4 COMPARISON<br />In this section we compare the lower bound FV, the<br />PP and the SPGP log likelihood in some toy prob-<br />lems. All these functions are continuous with respect<br />to (Xm,σ2,θ) and can be maximized using gradient-<br />based optimization.<br />Our working example will be the one-dimensional<br />dataset2considered in Snelson and Ghahramani (2006)<br />that consists of 200 training points; see Figure 1. We<br />train a sparse GP model using the squared exponential<br />kernel defined by σ2<br />dataset is small and the full GP model is tractable,<br />we compare the sparse approximations with the ex-<br />act GP prediction. The plots in the first row of Fig-<br />ure 1 show the predictive distributions for the three<br />methods assuming 15 inducing inputs. The left plot<br />displays the mean prediction with two-standard error<br />bars (shown as blue solid lines) obtained by the max-<br />imization of FV. The prediction of the full GP model<br />is displayed using dashed red lines. The middle plot<br />shows the corresponding solution found by PP and the<br />right plot the solution found by SPGP. The prediction<br />obtained by the variational method almost exactly re-<br />produces the full GP prediction. The final value of<br />the variational lower bound was −55.5708, while the<br />value of the maximized true log marginal likelihood<br />was −55.5647. Further, the estimated hyperparame-<br />ters found by FV match the hyperparameters found<br />fexp(−1<br />2ℓ2||xi− xj||2). Since the<br />2obtained from www.gatsby.ucl.ac.uk/∼snelson/.<br />by maximizing the true log marginal likelihood. In<br />contrast, training the sparse model using the PP log<br />likelihood gives a poor approximation.<br />method gave a much more satisfactory answer than<br />PP although not as good as the variational method.<br />The SPGP<br />To consider a more challenging problem, we decrease<br />the number of the original 200 training examples by<br />maintaining only 20 of them3. We repeat the experi-<br />ment above using exactly the same setup. The second<br />row of Figure 1, displays the predictive distributions<br />of the three methods.The prediction of the varia-<br />tional method is identical to the full GP prediction<br />and the hyperparameters match those obtained by full<br />GP training. On the other hand, the PP log likeli-<br />hood leads to a significant overfitting of the training<br />data since the mean curve interpolates the training<br />points and the error bars are very noisy. SPGP pro-<br />vides a solution that significantly disagrees with the<br />full GP prediction both in terms of the mean predic-<br />tion and the errors bars.<br />the error bars found by SPGP varies a lot in differ-<br />ent input regions.This nonstationarity is achieved<br />by setting σ2very close to zero and modelling the<br />actual noise by the heteroscedastic diagonal matrix<br />diag[Knn− KnmK−1<br />mmKmn]. The fact that this diago-<br />nal matrix (the sum of its elements is the trace Tr(? K))<br />proximated.<br />Notice that the width of<br />is large indicates that the full GP model is not well ap-<br />The reason PP and SPGP do not recover the full GP<br />model when we optimize over (Xm,σ2,θ) is not the<br />local maxima. To clarify this point, we repeated the<br />experiments by initializing the PP and SPGP log like-<br />lihoods to optimal inducing inputs and hyperparam-<br />eters values where the later are obtained by full GP<br />training. The predictions found are similar to those<br />shown in Figure 1. A way to ensure that the full GP<br />model will be recovered as we increase the number<br />of inducing inputs is to select them from the training<br />inputs. This, however, turns the continuous optimiza-<br />tion problem into a discrete one and moreover PP and<br />SPGP face the non-smooth convergence problem.<br />Regarding FV, it is clear from section 3 that by maxi-<br />mizing over Xmwe approach the full GP model in the<br />sense of KL(q(f,fm)|p(f,fm|y)). Something less clear<br />is that FV efficiently regularizes the hyperparameters<br />(σ2,θ) so as overfitting is avoided. This is achieved by<br />the regularization trace term: −1<br />Tr(? K) is large because there are not sufficiently many<br />ters that give a smoother function. Also, when Tr(? K)<br />2σ−2Tr(? K). When<br />inducing variables, this term favours kernel parame-<br />is large the decomposition in eq. (11) implies that σ2<br />3The points were chosen from the original set according<br />to the MATLAB command: X = X(1:10:end).</p>  <p>Page 6</p> <p>Variational Learning of Inducing Variables in Sparse Gaussian Processes<br />Figure 1: The first row corresponds to 200 training points and the second row to 20 training points. The first column<br />shows the prediction (blue solid lines) obtained by maximizing FV over the 15 pseudo-inputs and the hyperparameters.<br />The full GP prediction is shown with red dashed lines. Initial locations of the pseudo-inputs are shown on the top as<br />crosses, while final positions are given on the bottom as crosses. The second column shows the predictive distributions<br />found by PP and similarly the third column for SPGP.<br />must increase as well. These properties are useful for<br />avoiding overfitting and also imply that the prediction<br />obtained by FV will tend to be smoother than the pre-<br />diction of the full GP model. In contrast, the PP and<br />SPGP log likelihoods can find more flexible solutions<br />than the full GP prediction which indicates that they<br />are prone to overfitting.<br />5EXPERIMENTS<br />In this section we compare the variational lower bound<br />(VAR), the projected process approximate log likeli-<br />hood (PP) and the sparse pseudo-inputs GP (SPGP)<br />log likelihood in four real datasets. As a baseline tech-<br />nique, we use the subset of data (SD) method. For<br />all sparse GP methods we jointly maximize the al-<br />ternative objective functions w.r.t. hyperparameters<br />(θ,σ2) and the inducing inputs Xmusing the conju-<br />gate gradients algorithm. Xmis initialized to a ran-<br />domly chosen subset of training inputs. In each run<br />all methods are initialized to the same inducing in-<br />puts and hyperparameters.<br />ria we use are the standardized mean squared error<br />(SMSE), given by<br />T<br />negative log probability density (SNLP) as defined in<br />(Rasmussen and Williams, 2006). Smaller values for<br />both error measures imply better performance. In all<br />the experiments we use the squared-exponential kernel<br />with varied length-scale.<br />The performance crite-<br />1<br />||y∗−f∗||2<br />var(y∗), and the standardized<br />Firstly, we consider the Boston-housing dataset, which<br />consists of 455 training examples and 51 test examples.<br />Since the dataset is small, full GP training is tractable.<br />In the first experiment, we fix the parameters (θ,σ2)<br />to values obtained by training the full GP model. Thus<br />we can investigate the difference of the methods solely<br />on how the inducing inputs are selected. We rigorously<br />compare the methods by calculating the moments-<br />matching divergence KL(p(f∗|y)||q(f∗)) between the<br />true test posterior p(f∗|y) and each of the approxi-<br />mate test posteriors. For the SPGP method the ap-<br />proximate test posterior distribution is computed by<br />using the exact test conditional p(f∗|fm). Figure 2(a)<br />show the KL divergence as the number of inducing<br />points increases. Means and one-standard error bars<br />were obtained by repeating the experiment 10 times.<br />Note that only the VAR method is able to match the<br />full GP model; for around 200 points we closely match<br />the full GP prediction. Interestingly, when the induc-<br />ing inputs are initialized to all training inputs, i.e.<br />Xm = X, PP and SPGP still give a different solu-<br />tion from the full GP model despite the fact that the<br />hyperparameters are kept fixed to the values of the<br />full GP model. The reason this is happening is that<br />they are not lower bounds to the true log marginal<br />likelihood and as shown in Figure 2(c) they become<br />upper bounds. To show that the effective selection<br />of the inducing inputs achieved by VAR is not a co-<br />incidence, we compare it with the case where the in-<br />puts are kept fixed to their initial randomly selected<br />training inputs. Figure 2(b) displays the evolution of<br />the KL divergence for the VAR, the random selection<br />plus PP (RSPP) and the SD method. Note that the<br />only difference between VAR and RSPP is that VAR</p>  <p>Page 7</p> <p>Titsias<br />100200 300400<br />0<br />10<br />20<br />30<br />40<br />50<br />60<br />70<br />Number of inducing variables<br />KL(p||q)<br />VAR<br />PP<br />SPGP<br />100200300 400<br />0<br />10<br />20<br />30<br />40<br />50<br />60<br />70<br />Number of inducing variables<br />KL(p||q)<br />VAR<br />RSPP<br />SD<br />100<br />Number of inducing variables<br />200300400<br />−1500<br />−1000<br />−500<br />0<br />Log marginal likelihood<br />FullGP<br />VAR<br />PP<br />SPGP<br />(a)(b)(c)<br />100200300400<br />0.05<br />0.1<br />0.15<br />0.2<br />Number of inducing variables<br />SMSE<br />FullGP<br />VAR<br />PP<br />SPGP<br />100200 300400<br />−1.5<br />−1<br />−0.5<br />0<br />0.5<br />1<br />1.5<br />2<br />Number of inducing variables<br />SNLP<br />FullGP<br />VAR<br />PP<br />SPGP<br />100200300400<br />−600<br />−400<br />−200<br />0<br />200<br />400<br />Number of inducing variables<br />Log marginal likelihood<br />FullGP<br />VAR<br />PP<br />SPGP<br />(d)(e)(f)<br />Figure 2: (a) show the KL divergence as the number of inducing variables increases for the VAR the PP and SPGP<br />methods. Similarly (b) show the divergence for the VAR, RSPP and SD methods. (c) displays the approximate log<br />marginal likelihoods; the true log marginal likelihood value is displayed by using the dotted horizontal line. (d) and (e)<br />show the SMSE and SNLP errors (obtained by joint learning hyperparameters and inducing-inputs) against the number<br />of inducing variables. (f) shows the corresponding log marginal likelihoods.<br />optimizes the lower bound over the initial values of<br />the inducing inputs, while RSPP just keep them fixed.<br />Clearly RSPP significantly improves over the SD pre-<br />diction, and VAR significantly improves over RSPP.<br />In a second experiment, we jointly learn inducing vari-<br />ables and hyperparameters and compare the meth-<br />ods in terms of the SMSE and SNLP errors.<br />results are displayed in the second row of Figure 2.<br />Note that the PP and SPGP methods achieve a much<br />higher log likelihood value (Figure 2(f)) than the true<br />log marginal likelihood. However, the error measures<br />clearly indicate that the PP log likelihood significantly<br />overfits the data. SPGP gives better SMSE error than<br />the full GP model but it overfits w.r.t. the SNLP error.<br />The variational method matches the full GP model.<br />The<br />We now consider three large datasets: the kin40k<br />dataset, the sarcos and the abalone datasets4that<br />have been widely used before. Note that the abalone<br />dataset is small enough so as we will be able to train<br />the full GP model. The inputs were normalized to<br />have zero mean and unit variance on the training set<br />4kin40k:<br />ida.first.fraunhofer.de/ anton/data.html.<br />sarcos: 44,484 training, 4,449 test, 21 attributes,<br />www.gaussianprocess.org/gpml/data/.<br />abalone:3,133 training,<br />www.liaad.up.pt/ ltorgo/Regression/DataSets.html.<br />10000 training, 30000 test, 8 attributes,<br />1,044 test, 8 attributes,<br />and the outputs were centered so as to have zero mean<br />on the training set. For the kin40k and the sarcos<br />datasets, the SD method was obtained in a subset of<br />2000 training points. We vary the size of the inducing<br />variables in powers of two from 16 to 1024. For the<br />sarcos dataset, the experiment for 1024 was not per-<br />formed since is was unrealistically expensive. All the<br />objective functions were jointly maximized over induc-<br />ing inputs and hyperparameters. The experiment was<br />repeated 5 times. Figure 3 shows the results.<br />From the plots in Figure 3, we can conclude the fol-<br />lowing. The PP log likelihood is significantly prone to<br />overfitting as the SNLP errors clearly indicate. How-<br />ever, note that in the kin40k and sarcos datasets,<br />PP gave the best performance w.r.t. to SMSE error.<br />This is probably because of the ability of PP to in-<br />terpolate the training examples that can lead to good<br />SMSE error when the actual observation noise is low.<br />SPGP often has the worst performance in terms of the<br />SMSE error and almost always the best performance<br />in terms of the SNLP error. In the abalone dataset,<br />SPGP had significantly better SNLP error than the<br />full GP model. Since the SNLP error depends on the<br />predictive variances, we believe that the good perfor-<br />mance of SPGP is due to its heteroscedastic ability.<br />For example, in the kin40k dataset, SPGP makes<br />σ2almost zero and thus the actual noise in the like-<br />lihood is modelled by the heteroscedastic covariance</p>  <p>Page 8</p> <p>Variational Learning of Inducing Variables in Sparse Gaussian Processes<br />02004006008001000<br />0<br />0.05<br />0.1<br />Number of inducing variables<br />SMSE<br />SD<br />VAR<br />PP<br />SPGP<br />0100200300400 500<br />0.01<br />0.015<br />0.02<br />0.025<br />0.03<br />0.035<br />0.04<br />0.045<br />0.05<br />Number of inducing variables<br />SMSE<br />SD<br />VAR<br />PP<br />SPGP<br />0200400 6008001000<br />0.4<br />0.42<br />0.44<br />0.46<br />0.48<br />0.5<br />Number of inducing variables<br />SMSE<br />FullGP<br />VAR<br />PP<br />SPGP<br />02004006008001000<br />−2.5<br />−2<br />−1.5<br />−1<br />−0.5<br />0<br />0.5<br />1<br />Number of inducing variables<br />SNLP<br />SD<br />VAR<br />PP<br />SPGP<br />0 100200300 400500<br />−2.5<br />−2<br />−1.5<br />−1<br />−0.5<br />0<br />Number of inducing variables<br />SNLP<br />SD<br />VAR<br />PP<br />SPGP<br />0 200 4006008001000<br />−0.5<br />−0.4<br />−0.3<br />−0.2<br />−0.1<br />0<br />0.1<br />Number of inducing variables<br />SNLP<br />FullGP<br />VAR<br />PP<br />SPGP<br />Figure 3: The first column displays the SMSE (top) and SNLP (bottom) errors for the kin40k dataset with respect to<br />the number of inducing points. The second column shows the corresponding plots for the sarcos dataset and similarly<br />the third column shows the results for the abalone dataset.<br />diag[Knn− KnmK−1<br />term is large may indicate that the full GP model is<br />not well approximated. Finally the variational method<br />has good performance. VAR never had the worst per-<br />formance and it didn’t exhibit overfitting. The exam-<br />ples in section 4, the Boston-housing and the abalone<br />dataset indicate that the VAR method remains much<br />closer to the full GP model than the other methods.<br />mmKmn]. The fact that the latter<br />6CONCLUSION<br />We proposed a variational framework for sparse GP<br />regression that can reliably learn inducing inputs and<br />hyperparameters by minimizing the KL divergence be-<br />tween the true posterior GP and an approximate one.<br />This method can be more generally applicable. Cur-<br />rently we apply this technique to classification. An<br />interesting topic for the future is to apply this method<br />to GP models that assume multiple latent functions.<br />Acknowledgments<br />I am grateful to Neil Lawrence for his help. This work<br />is funded by EPSRC Grant No EP/F005687/1 ”Gaus-<br />sian Processes for Systems Identification with Appli-<br />cations in Systems Biology”.<br />References<br />Csato, L. and Opper, M. (2002). Sparse online Gaussian<br />processes. Neural Computation, 14:641–668.<br />Lawrence, N. D., Seeger, M., and Herbrich, R. (2002). Fast<br />sparse Gaussian process methods: the informative vector<br />machine. In Neural Information Processing Systems, 13.<br />MIT Press.<br />Qui˜ nonero-Candela, J. and Rasmussen, C. E. (2005). A<br />unifying view of sparse approximate Gaussian process re-<br />gression. Journal of Machine Learning Research, 6:1939–<br />1959.<br />Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian<br />Processes for Machine Learning. MIT Press.<br />Schwaighofer, A. and Tresp, V. (2003). Transductive and<br />inductive methods for approximate Gaussian process re-<br />gression. In Neural Information Processing Systems 15.<br />MIT Press.<br />Seeger, M. (2003).<br />Bayesian Gaussian Process Models:<br />PAC-Bayesian Generalisation Error Bounds and Sparse<br />Approximations. PhD thesis, University of Edinburgh.<br />Seeger, M., Williams, C. K. I., and Lawrence, N. D. (2003).<br />Fast forward selection to speed up sparse Gaussian pro-<br />cess regression. In Ninth International Workshop on Ar-<br />tificial Intelligence. MIT Press.<br />Smola, A. J. and Bartlett, P. (2001). Sparse greedy Gaus-<br />sian process regression. In Neural Information Process-<br />ing Systems, 13. MIT Press.<br />Smola, A. J. and Sch¨ olkopf, B. (2000). Sparse greedy ma-<br />trix approximations for machine learning. In Interna-<br />tional Conference on Machine Learning.<br />Snelson, E. and Ghahramani, Z. (2006). Sparse Gaussian<br />process using pseudo-inputs. In Neural Information Pro-<br />cessing Systems, 13. MIT Press.<br />Titsias, M. K. (2009).Variational Model Selection for<br />Sparse Gaussian Process Regression. Technical report,<br />School of Computer Science, University of Manchester.<br />Williams, C. K. I. and Seeger, M. (2001).<br />Nystr¨ om method to speed up kernel machines. In Neural<br />Information Processing Systems 13. MIT Press.<br />Using the</p>   </div> <div id="rgw15_56aba07e5d27a" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw16_56aba07e5d27a">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw17_56aba07e5d27a"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://eprints.pascal-network.org/archive/00006353/01/aistats_varGP.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Variational Learning of Inducing Variables in Sparse Gaussian Processes">Variational Learning of Inducing Variables in Spar...</a> </div>  <div class="details">   Available from <a href="http://eprints.pascal-network.org/archive/00006353/01/aistats_varGP.pdf" target="_blank" rel="nofollow">pascal-network.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw19_56aba07e5d27a" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw20_56aba07e5d27a">  </ul> </div> </div>   <div id="rgw11_56aba07e5d27a" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw12_56aba07e5d27a"> <div> <h5> <a href="publication/221363047_Fast_Sparse_Gaussian_Processes_Learning_for_Man-Made_Structure_Classification" class="color-inherit ga-similar-publication-title"><span class="publication-title">Fast Sparse Gaussian Processes Learning for Man-Made Structure Classification</span></a>  </h5>  <div class="authors"> <a href="researcher/14731597_Hang_Zhou" class="authors ga-similar-publication-author">Hang Zhou</a>, <a href="researcher/54254084_David_Suter" class="authors ga-similar-publication-author">David Suter</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw13_56aba07e5d27a"> <div> <h5> <a href="publication/228092034_Variable_noise_and_dimensionality_reduction_for_sparse_Gaussianprocesses" class="color-inherit ga-similar-publication-title"><span class="publication-title">Variable noise and dimensionality reduction for sparse Gaussian
processes</span></a>  </h5>  <div class="authors"> <a href="researcher/70012264_Edward_Snelson" class="authors ga-similar-publication-author">Edward Snelson</a>, <a href="researcher/8159937_Zoubin_Ghahramani" class="authors ga-similar-publication-author">Zoubin Ghahramani</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw14_56aba07e5d27a"> <div> <h5> <a href="publication/221613306_A_sparse_gaussian_processes_classification_framework_for_fast_tag_suggestions" class="color-inherit ga-similar-publication-title"><span class="publication-title">A sparse gaussian processes classification framework for fast tag suggestions</span></a>  </h5>  <div class="authors"> <a href="researcher/71036854_Yang_Song" class="authors ga-similar-publication-author">Yang Song</a>, <a href="researcher/2029079421_Lu_Zhang" class="authors ga-similar-publication-author">Lu Zhang</a>, <a href="researcher/13884415_C_Lee_Giles" class="authors ga-similar-publication-author">C. Lee Giles</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw25_56aba07e5d27a" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw26_56aba07e5d27a">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw27_56aba07e5d27a" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=s4nnMRi8NgZ7ZHhNcE7m3t7fXu8dYPJo_2ZDBQO7VMYSqqaZ5rno-sRZoX_55ryX" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="uI3p9s/XQbWegF33BFwAgnEq+RFB0Ywt8ipmkZVY88AYVtzzrowF1Ecn/AIxds6AvVohD4QWiZ+UyfMcNmDVwMG2qcK85t827S/Fruh+lNwUTos77/nMziTovnUGtTv1ZVFm/hPyBwPHgd+2Atd/Bim81sKJp4aeOxDaxwPSNWxiMuVXRkydKKdlqVLlUve9xJtq0SMU6FFR/yjo2Z6DVk/LMQYi/o6Tl47tm5Si4CLj4inwplbWPgTVnAp0qeCY+/dEUH6p7aWYWWjqMMHoHIGfakv3mte2FbHfeWOT7L4="/> <input type="hidden" name="urlAfterLogin" value="publication/220320048_Variational_Learning_of_Inducing_Variables_in_Sparse_Gaussian_Processes"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMzIwMDQ4X1ZhcmlhdGlvbmFsX0xlYXJuaW5nX29mX0luZHVjaW5nX1ZhcmlhYmxlc19pbl9TcGFyc2VfR2F1c3NpYW5fUHJvY2Vzc2Vz"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMzIwMDQ4X1ZhcmlhdGlvbmFsX0xlYXJuaW5nX29mX0luZHVjaW5nX1ZhcmlhYmxlc19pbl9TcGFyc2VfR2F1c3NpYW5fUHJvY2Vzc2Vz"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMzIwMDQ4X1ZhcmlhdGlvbmFsX0xlYXJuaW5nX29mX0luZHVjaW5nX1ZhcmlhYmxlc19pbl9TcGFyc2VfR2F1c3NpYW5fUHJvY2Vzc2Vz"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw28_56aba07e5d27a"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 469;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Michalis Titsias","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A279280278884356%401443597015332_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Michalis_Titsias2","institution":"Athens University of Economics and Business","institutionUrl":false,"widgetId":"rgw4_56aba07e5d27a"},"id":"rgw4_56aba07e5d27a","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=6761812","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56aba07e5d27a"},"id":"rgw3_56aba07e5d27a","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=220320048","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":220320048,"title":"Variational Learning of Inducing Variables in Sparse Gaussian Processes","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"Journal of Machine Learning Research - Proceedings Track","publicationDate":"04\/2009;","publicationDateRobot":"2009-04","article":"5:567-574."}},"source":{"sourceUrl":"http:\/\/dblp.uni-trier.de\/db\/journals\/jmlr\/jmlrp5.html#Titsias09","sourceName":"DBLP"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Variational Learning of Inducing Variables in Sparse Gaussian Processes"},{"key":"rft.title","value":"Journal of Machine Learning Research - Proceedings Track"},{"key":"rft.jtitle","value":"Journal of Machine Learning Research - Proceedings Track"},{"key":"rft.volume","value":"5"},{"key":"rft.date","value":"2009"},{"key":"rft.pages","value":"567-574"},{"key":"rft.au","value":"Michalis K. Titsias"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56aba07e5d27a"},"id":"rgw6_56aba07e5d27a","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=220320048","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":220320048,"peopleItems":[{"data":{"authorNameOnPublication":"Michalis Titsias","accountUrl":"profile\/Michalis_Titsias2","accountKey":"Michalis_Titsias2","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A279280278884356%401443597015332_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Michalis Titsias","profile":{"professionalInstitution":{"professionalInstitutionName":"Athens University of Economics and Business","professionalInstitutionUrl":"institution\/Athens_University_of_Economics_and_Business"}},"professionalInstitutionName":"Athens University of Economics and Business","professionalInstitutionUrl":"institution\/Athens_University_of_Economics_and_Business","url":"profile\/Michalis_Titsias2","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A279280278884356%401443597015332_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Michalis_Titsias2","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw9_56aba07e5d27a"},"id":"rgw9_56aba07e5d27a","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=6761812&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Athens University of Economics and Business","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":1,"accountCount":1,"publicationUid":220320048,"widgetId":"rgw8_56aba07e5d27a"},"id":"rgw8_56aba07e5d27a","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=6761812&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=1&accountCount=1&publicationUid=220320048","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56aba07e5d27a"},"id":"rgw7_56aba07e5d27a","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=220320048&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":220320048,"abstract":"<noscript><\/noscript><div>Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are defined to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw10_56aba07e5d27a"},"id":"rgw10_56aba07e5d27a","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=220320048","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/220320048_Variational_Learning_of_Inducing_Variables_in_Sparse_Gaussian_Processes\/links\/0ffa78e00cf25dfdcf53dd6e\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw5_56aba07e5d27a"},"id":"rgw5_56aba07e5d27a","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=220320048&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":14731597,"url":"researcher\/14731597_Hang_Zhou","fullname":"Hang Zhou","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":54254084,"url":"researcher\/54254084_David_Suter","fullname":"David Suter","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Conference Paper","publicationDate":"Jun 2007","journal":"Proceedings \/ CVPR, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE Computer Society Conference on Computer Vision and Pattern Recognition","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/221363047_Fast_Sparse_Gaussian_Processes_Learning_for_Man-Made_Structure_Classification","usePlainButton":true,"publicationUid":221363047,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/221363047_Fast_Sparse_Gaussian_Processes_Learning_for_Man-Made_Structure_Classification","title":"Fast Sparse Gaussian Processes Learning for Man-Made Structure Classification","displayTitleAsLink":true,"authors":[{"id":14731597,"url":"researcher\/14731597_Hang_Zhou","fullname":"Hang Zhou","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":54254084,"url":"researcher\/54254084_David_Suter","fullname":"David Suter","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["2007 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2007), 18-23 June 2007, Minneapolis, Minnesota, USA; 06\/2007"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/221363047_Fast_Sparse_Gaussian_Processes_Learning_for_Man-Made_Structure_Classification","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/221363047_Fast_Sparse_Gaussian_Processes_Learning_for_Man-Made_Structure_Classification\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw12_56aba07e5d27a"},"id":"rgw12_56aba07e5d27a","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=221363047","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":70012264,"url":"researcher\/70012264_Edward_Snelson","fullname":"Edward Snelson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jun 2012","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/228092034_Variable_noise_and_dimensionality_reduction_for_sparse_Gaussianprocesses","usePlainButton":true,"publicationUid":228092034,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/228092034_Variable_noise_and_dimensionality_reduction_for_sparse_Gaussianprocesses","title":"Variable noise and dimensionality reduction for sparse Gaussian\nprocesses","displayTitleAsLink":true,"authors":[{"id":70012264,"url":"researcher\/70012264_Edward_Snelson","fullname":"Edward Snelson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/228092034_Variable_noise_and_dimensionality_reduction_for_sparse_Gaussianprocesses","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/228092034_Variable_noise_and_dimensionality_reduction_for_sparse_Gaussianprocesses\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw13_56aba07e5d27a"},"id":"rgw13_56aba07e5d27a","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=228092034","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":71036854,"url":"researcher\/71036854_Yang_Song","fullname":"Yang Song","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2029079421,"url":"researcher\/2029079421_Lu_Zhang","fullname":"Lu Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":13884415,"url":"researcher\/13884415_C_Lee_Giles","fullname":"C. Lee Giles","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Conference Paper","publicationDate":"Oct 2008","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/221613306_A_sparse_gaussian_processes_classification_framework_for_fast_tag_suggestions","usePlainButton":true,"publicationUid":221613306,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/221613306_A_sparse_gaussian_processes_classification_framework_for_fast_tag_suggestions","title":"A sparse gaussian processes classification framework for fast tag suggestions","displayTitleAsLink":true,"authors":[{"id":71036854,"url":"researcher\/71036854_Yang_Song","fullname":"Yang Song","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2029079421,"url":"researcher\/2029079421_Lu_Zhang","fullname":"Lu Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":13884415,"url":"researcher\/13884415_C_Lee_Giles","fullname":"C. Lee Giles","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Proceedings of the 17th ACM Conference on Information and Knowledge Management, CIKM 2008, Napa Valley, California, USA, October 26-30, 2008; 10\/2008"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/221613306_A_sparse_gaussian_processes_classification_framework_for_fast_tag_suggestions","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/221613306_A_sparse_gaussian_processes_classification_framework_for_fast_tag_suggestions\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw14_56aba07e5d27a"},"id":"rgw14_56aba07e5d27a","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=221613306","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw11_56aba07e5d27a"},"id":"rgw11_56aba07e5d27a","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=220320048&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":220320048,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":220320048,"publicationType":"article","linkId":"0ffa78e00cf25dfdcf53dd6e","fileName":"Variational Learning of Inducing Variables in Sparse Gaussian Processes","fileUrl":"http:\/\/eprints.pascal-network.org\/archive\/00006353\/01\/aistats_varGP.pdf","name":"pascal-network.org","nameUrl":"http:\/\/eprints.pascal-network.org\/archive\/00006353\/01\/aistats_varGP.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw17_56aba07e5d27a"},"id":"rgw17_56aba07e5d27a","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=220320048&linkId=0ffa78e00cf25dfdcf53dd6e&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw16_56aba07e5d27a"},"id":"rgw16_56aba07e5d27a","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=220320048&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":4,"valueFormatted":"4","widgetId":"rgw18_56aba07e5d27a"},"id":"rgw18_56aba07e5d27a","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=220320048","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw15_56aba07e5d27a"},"id":"rgw15_56aba07e5d27a","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=220320048&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":220320048,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw20_56aba07e5d27a"},"id":"rgw20_56aba07e5d27a","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=220320048&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":4,"valueFormatted":"4","widgetId":"rgw21_56aba07e5d27a"},"id":"rgw21_56aba07e5d27a","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=220320048","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw19_56aba07e5d27a"},"id":"rgw19_56aba07e5d27a","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=220320048&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Variational Learning of Inducing Variables in Sparse Gaussian\nProcesses\nMichalis K. Titsias\nSchool of Computer Science,\nUniversity of Manchester, UK\nmtitsias@cs.man.ac.uk\nAbstract\nSparse Gaussian process methods that use in-\nducing variables require the selection of the\ninducing inputs and the kernel hyperparam-\neters. We introduce a variational formula-\ntion for sparse approximations that jointly\ninfers the inducing inputs and the kernel hy-\nperparameters by maximizing a lower bound\nof the true log marginal likelihood. The key\nproperty of this formulation is that the in-\nducing inputs are defined to be variational\nparameters which are selected by minimizing\nthe Kullback-Leibler divergence between the\nvariational distribution and the exact poste-\nrior distribution over the latent function val-\nues. We apply this technique to regression\nand we compare it with other approaches in\nthe literature.\n1INTRODUCTION\nThe application of Gaussian process (GP) models is in-\ntractable for large datasets because the time complex-\nity scales as O(n3) and the storage as O(n2) where\nn is the number of training examples. To overcome\nthis limitation, many approximate or sparse meth-\nods have been proposed in the literature (Williams\nand Seeger, 2001; Smola and Bartlett, 2001; Csato\nand Opper, 2002; Lawrence et al., 2002; Seeger\net al., 2003; Schwaighofer and Tresp, 2003; Snelson\nand Ghahramani, 2006; Qui\u02dc nonero-Candela and Ras-\nmussen, 2005). These methods construct an approxi-\nmation based on a small set of m support or inducing\nvariables that allow the reduction of the time com-\nAppearing in Proceedings of the 12thInternational Confe-\nrence on Artificial Intelligence and Statistics (AISTATS)\n2009, Clearwater Beach, Florida, USA. Volume 5 of JMLR:\nW&CP 5. Copyright 2009 by the authors.\nplexity from O(n3) to O(nm2). They mainly differ\nin the strategies they use to select the inducing in-\nputs which are typically selected from the training or\ntest examples. Snelson and Ghahramani (2006) allow\nthe inducing variables to be considered as auxiliary\npseudo-inputs that are inferred along with kernel hy-\nperparameters using continuous optimization.\nApproximate marginal likelihoods are appropriate ob-\njective functions for model selection in sparse GP mod-\nels.Existing state-of-the-art methods (Snelson and\nGhahramani, 2006; Seeger et al., 2003) derive such ap-\nproximations by modifying the GP prior (Qui\u02dc nonero-\nCandela and Rasmussen, 2005) and then computing\nthe marginal likelihood of the modified model. This\napproach turns the inducing inputs into additional ker-\nnel hyperparameters. While this can increase flexibil-\nity when we fit the data, it can also lead to overfitting\nwhen we optimize with respect to all unknown hyper-\nparameters. Furthermore, fitting a modified model is\nnot so rigorous approximation procedure since there is\nno distance between the exact and the modified model\nthat is minimized.\nIn this paper we introduce a variational method that\njointly selects the inducing inputs and the hyperpa-\nrameters by maximizing a lower bound to the exact\nmarginal likelihood. The important difference between\nthis formulation and previous methods is that here the\ninducing inputs are defined to be variational param-\neters which are selected by minimizing the Kullback-\nLeibler (KL) divergence between a variational GP and\nthe true posterior GP. This allows i) to avoid overfit-\nting and ii) to rigorously approximate the exact GP\nmodel by minimizing a distance between the sparse\nmodel and the exact one. The selection of the inducing\ninputs and hyperparameters is achieved either by ap-\nplying continuous optimization over all unknown quan-\ntities or by using a variational EM algorithm where at\nthe E step we greedily select the inducing inputs from\nthe training data and at the M step we update the\nhyperparameters. In contrast to previous greedy ap-"},{"page":2,"text":"Variational Learning of Inducing Variables in Sparse Gaussian Processes\nproaches, e.g. (Seeger et al., 2003), our scheme mono-\ntonically increases the optimized objective function.\nWe apply the variational method to regression with\nadditive Gaussian noise and we compare its perfor-\nmance to training schemes based on the projected pro-\ncess marginal likelihood (Seeger et al., 2003; Csato and\nOpper, 2002) and the sparse pseudo-inputs marginal\nlikelihood (Snelson and Ghahramani, 2006).\nOur method is most closely related to the variational\nsparse GP method described in (Csato and Opper,\n2002; Seeger, 2003) that is applied to GP classifica-\ntion (Seeger, 2003). The main difference between our\nformulation and these techniques is that we maximize\na variational lower bound in order to select the induc-\ning inputs, while these methods use variational bounds\nfor estimating only the kernel hyperparameters.\n2SPARSE GP REGRESSION\nA GP is a set of random variables {f(x)|x \u2208 X} for\nwhich any finite subset follows a Gaussian distribution.\nTo describe a GP, we only need to specify the mean\nfunction m(x) and a covariance function k(x,x\u2032). The\ncovariance function typically depends on a set of hy-\nperparameters \u03b8. A GP can be used as a prior over a\nreal-valued function f(x). This prior can be combined\nwith data to give a posterior over the function.\nSuppose we have a training dataset {(xi,yi)}n\nnoisy realizations of some unobserved or latent func-\ntion so that each scalar yiis obtained by adding Gaus-\nsian noise to f(x) at input xi, i.e. yi= fi+ \u01ebi, where\n\u01ebi\u223c N(0,\u03c32) and fi= f(xi). Let X denote all train-\ning inputs, y all outputs and f the corresponding train-\ning latent function values. The joint probability model\nis p(y,f) = p(y|f)p(f) where p(y|f) is the likelihood\nand p(f) the GP prior. The data induce a posterior\nGP which is specified by a posterior mean function and\na posterior covariance function:\ni=1of n\nmy(x) = Kxn(\u03c32I + Knn)\u22121y,\n(1)\nky(x,x\u2032) = k(x,x\u2032) \u2212 Kxn(\u03c32I + Knn)\u22121Knx\u2032.\nHere, Knnis the n\u00d7n covariance matrix on the train-\ning inputs, Kxn is n-dimensional row vector of ker-\nnel function values between x and the training in-\nputs and Knx = KT\nxn.\nposterior GP can be answered by the above mean\nand covariance functions.\nsian posterior distribution p(f|y) on the training la-\ntent variables f is computed by evaluating eq. (1) at\nthe inputs X. Similarly the prediction of the output\ny\u2217= f\u2217+ \u01eb\u2217at some unseen input x\u2217is described by\np(y\u2217|y) = N(y\u2217|my(x\u2217),ky(x\u2217,x\u2217) + \u03c32). The poste-\nrior GP depends on the values of the hyperparameters\nAny query related to the\nFor instance, the Gaus-\n(\u03b8,\u03c32) which can be estimated by maximizing the log\nmarginal likelihood given by\nlogp(y) = log[N(y|0,\u03c32I + Knn)].\n(2)\nAlthough the above GP approach is elegant, it is in-\ntractable for large datasets since the computations re-\nquire the inversion of a matrix of size n \u00d7 n which\nscales as O(n3). Thus, we need to consider approx-\nimate or sparse methods in order to deal with large\ndatasets. Advanced sparse methods use a small set\nof m function points as support or inducing variables.\nThis yields a time complexity that scales as O(nm2).\nImportant issues in these methods involve the selec-\ntion of the inducing variables and the hyperparame-\nters. For reviews of current approaches see chapter 8\nin (Rasmussen and Williams, 2006) and (Qui\u02dc nonero-\nCandela and Rasmussen, 2005).\nSuppose we wish to use m inducing variables to con-\nstruct our sparse GP method. The inducing variables\nare latent function values evaluated at some inputs\nXm. Xmcan be a subset of the training inputs or aux-\niliary pseudo-points (Snelson and Ghahramani, 2006).\nLearning Xmand the hyperparameters (\u03b8,\u03c32) is the\ncrucial problem we need to solve in order to obtain\na sparse GP method. An approximation to the true\nlog marginal likelihood in eq. (2) can allow us to infer\nthese quantities. The current state-of-the-art approxi-\nmate marginal likelihood is given in the sparse pseudo-\ninputs GP method (SPGP) proposed in (Snelson and\nGhahramani, 2006). A related objective function used\nin (Seeger et al., 2003) corresponds to the projected\nprocess approximation (PP). These approximate log\nmarginal likelihoods have the form\nF = log[N(y|0,\u03c32I + Qnn)],\n(3)\nwhere Qnnis an approximation to the true covariance\nKnn. In PP, Qnn= KnmK\u22121\nvariance is replaced by the Nystr\u00a8 om approximation.\nHere, Kmmis the m\u00d7m covariance matrix on the in-\nducing inputs, Knmis the n\u00d7m cross-covariance ma-\ntrix between training and inducing points and Kmn=\nKT\nKnmK\u22121\nmmKmn, i.e. the Nystr\u00a8 om approximation is cor-\nrected to be exact in the diagonal. By contrasting eq.\n(2) with (3), it is clear that F is obtained by mod-\nifying the GP prior. This implies that the inducing\ninputs Xmplay the role of extra kernel hyperparam-\neters (similar to \u03b8) that parametrize the covariance\nmatrix Qnn. However because the prior has changed,\ncontinuous optimization of F with respect to Xmdoes\nnot reliably approximate the exact GP model. Fur-\nther, since F is heavily parametrized with the extra\nhyperparameters Xm, overfitting can occur especially\nwhen we jointly optimize over (Xm,\u03b8,\u03c32).\nmmKmn, i.e. the exact co-\nnm. In SPGP, Qnn= diag[Knn\u2212 KnmK\u22121\nmmKmn] +"},{"page":3,"text":"Titsias\nIn the next section, we propose a formulation for\nsparse GP regression that follows a different philos-\nophy. Rather than modifying the exact GP model, we\nminimize a distance between the exact posterior GP\nand a variational approximation. The inducing inputs\nXmbecome now variational parameters which are rig-\norously selected so as the distance is minimized.\n3VARIATIONAL LEARNING\nWe wish to define a sparse method that directly ap-\nproximates the posterior GP mean and covariance\nfunctions in eq. (1).This posterior GP can be\nalso described by the predictive Gaussian p(z|y) =\n?p(z|f)p(f|y)df, where p(z|f) denotes the conditional\nprior over any finite set of function points z. Suppose\nthat we wish to approximate the above Bayesian inte-\ngral by using a small set of m auxiliary inducing vari-\nables fmevaluated at the pseudo-inputs Xm, which are\nindependent from the training inputs. fmare just func-\ntion points drawn from the same GP prior as the train-\ning function values f. By using the augmented joint\nmodel p(y|f)p(z,fm,f), we equivalently write p(z|y) as\n?\np(z|y) =\np(z|fm,f)p(f|fm,y)p(fm|y)dfdfm.\n(4)\nSuppose now that fm is a sufficient statistic for the\nparameter f in the sense that z and f are independent\ngiven fm, i.e. it holds p(z|fm,f) = p(z|fm). The above\ncan be written as\n?\n?\nq(z) =\np(z|fm)p(f|fm)\u03c6(fm)dfdfm\n=\np(z|fm)\u03c6(fm)dfm=\n?\nq(z,fm)dfm,\n(5)\nwhere q(z) = p(z|y) and \u03c6(fm) = p(fm|y).\np(f|fm) = p(f|fm,y) is true since y is a noisy ver-\nsion of f and because of the assumption we made that\nany z is conditionally independent from f given fm1.\nIn practise it is difficult to find inducing variables fm\nthat are sufficient statistics. Thus, we expect q(z) to\nbe only an approximation to p(z|y). In such case, we\ncan choose \u03c6(fm) to be a \u201cfree\u201d variational Gaussian\ndistribution, where in general \u03c6(fm) ?= p(fm|y), that\ndepends on a mean vector \u00b5 and a covariance matrix\nA. By using eq. (5), we can write down the approxi-\nmate posterior GP mean and covariance functions as\nfollows\nmq\nHere,\ny(x) = KxmK\u22121\nmm\u00b5,\n(6)\nkq\ny(x,x\u2032) = k(x,x\u2032) \u2212 KxmK\u22121\nmmKmx\u2032 + KxmBKmx\u2032,\n1From p(z|fm,y) =\nfact p(z|fm,f) = p(z|fm), the result follows.\nR\np(y|f)p(z,fm,f)df\np(y|f)p(z,fm,f)dfdzand by using the\nR\nwhere B = K\u22121\neral form of the sparse posterior GP which is computed\nin O(nm2). The question that now arises is how do we\nselect the \u03c6 distribution, i.e. (\u00b5, A), and the inducing\ninputs Xm. Next we describe a variational method\nthat allows to jointly specify these quantities and treat\nXmas a variational parameter which is rigorously se-\nlected by minimizing the KL divergence.\nmmAK\u22121\nmm. The above defines the gen-\nA principled procedure to specify \u03c6 and the inducing\ninputs Xmis to form the variational distribution q(f)\nand the exact posterior p(f|y) on the training function\nvalues f, and then minimize a distance between these\ntwo distributions. Equivalently, we can minimize a dis-\ntance between the augmented true posterior p(f,fm|y)\nand the augmented variational posterior q(f,fm) where\nclearly from eq. (5) q(f,fm) = p(f|fm)\u03c6(fm).\naugmented true posterior is associated with the aug-\nmented joint model\nThe\np(y,f,fm) = p(y|f)p(f|fm)p(fm),\n(7)\nwhich is equivalent to the initial model p(y,f) =\np(f|y)p(f), since by marginalizing out fmfrom the for-\nmer we always recover the latter. In particular, notice\nthat the conditional prior p(f|fm) and the marginal\nprior p(fm) depend on the specific values of the in-\nducing inputs Xm. However, this dependence never\naffects the posterior p(f|y) or the marginal likelihood\np(y). Hence, the augmented representation has a set\nof \u201cfree\u201d parameters Xmwhich can be treated as varia-\ntional parameters as opposed to the model parameters.\nTo determine the variational quantities (Xm,\u03c6), we\nminimize the KL divergence KL(q(f,fm)||p(f,fm|y)).\nThis minimization is equivalently expressed as the\nmaximization of the following variational lower bound\nof the true log marginal likelihood:\n?\nFV(Xm,\u03c6) =\np(f|fm)\u03c6(fm)logp(y|f)p(fm)\n\u03c6(fm)\ndfdfm,\n(8)\nwhere the term p(f|fm) inside the log cancels out. We\ncan firstly maximize the bound by analytically solving\nfor the optimal choice of the variational distribution\n\u03c6. The bound after this maximization is\nFV(Xm) = log?N(y|0,\u03c32I + Qnn)?\u2212\n1\n2\u03c32Tr(? K),\n(9)\nwhere Qnn = KnmK\u22121\nKnn\u2212KnmK\u22121\nbound are given in a technical report (Titsias, 2009).\nThe novelty of the above objective function is that\nit contains a regularization trace term: \u2212\nThis clearly differentiates FV from all marginal likeli-\nhoods, described by eq. (3), that were previously ap-\nplied to sparse GP regression. We will analyze the\ntrace term shortly.\nmmKmn and? K = Cov(f|fm) =\nmmKmn. Details of the derivation of this\n1\n2\u03c32Tr(? K)."},{"page":4,"text":"Variational Learning of Inducing Variables in Sparse Gaussian Processes\nThe quantity in eq. (9) is computed in O(nm2) time\nand is a lower bound of the true log marginal likeli-\nhood for any value of the inducing inputs Xm. Fur-\nther maximization of the bound can be achieved by\noptimizing over Xm and optionally over the number\nof these variables. Note that the inducing inputs de-\ntermine the flexibility of the variational distribution\nq(f,fm) = p(f|fm)\u03c6(fm) since by tuning Xmwe adapt\nboth p(f|fm) and the underlying optimal distribution\n\u03c6\u2217. To compute this optimal \u03c6\u2217, we differentiate eq.\n(8) with respect to \u03c6(fm) without imposing any con-\nstraints. This gives:\n\u03c6\u2217(fm) = N(fm|\u00b5,A),\n(10)\nwhere \u00b5 = \u03c3\u22122Kmm\u03a3Kmny, A = Kmm\u03a3Kmm and\n\u03a3 = (Kmm+ \u03c3\u22122KmnKnm)\u22121. This now fully speci-\nfies our variational GP and we can use eq. (6) to make\npredictions in unseen input points. Clearly, the pre-\ndictive distribution is exactly the one used by the pro-\njected process (PP) that has been previously proposed\nin (Csato and Opper, 2002; Seeger et al., 2003). Thus,\nas far as the predictive distribution is concerned the\nabove method is equivalent to PP.\nHowever, the variational method is very different to\nPP and SPGP as far as the selection of the inducing\ninputs and the kernel hyperparameters is concerned.\nThis is because of the extra regularization term that\nappears in the bound in eq. (9) and does not appear in\nthe approximate log marginal likelihoods used in PP\n(Seeger et al., 2003) and SPGP (Snelson and Ghahra-\nmani, 2006). As discussed in section 2, for the latter\nobjective functions, the role of Xm is to form a set\nof extra kernel hyperparameters. In contrast, for the\nlower bound, the inputs Xm become variational pa-\nrameters due to the KL divergence that is minimized.\nTo look into the functional form of the bound, note\nthat FV is the sum of the PP log likelihood and the\nregularization trace term \u22121\ntempts to maximize the PP log likelihood and simul-\ntaneously minimize the trace Tr(? K). Tr(? K) repre-\nwhich also corresponds to the squared error of predict-\ning the training latent values f from the inducing vari-\nables fm:\nTr(? K) = 0, the Nystr\u00a8 om approximation is exact, i.e.\ning variables become sufficient statistics and we can\nreproduce exactly the full GP prediction. Note that\nthe trace Tr(? K) itself has been used as a criterion for\nin (Smola and Sch\u00a8 olkopf, 2000) and is similar to the\ncriterion used in (Lawrence et al., 2002).\n2\u03c3\u22122Tr(? K). Thus, FV at-\nsents the total variance of the conditional prior p(f|fm)\n?p(f,fm)||KnmK\u22121\nmmfm\u2212 f||2dfdfm. When\nKnn= KnmK\u22121\nmmKmn, which means that the induc-\nselecting the inducing points from the training data\nWhen we maximize the variational lower bound, the\nhyperparameters (\u03c32,\u03b8) are regularized. It is easy to\nsee how this is achieved for the noise variance \u03c32. At\na local maxima, \u03c32satisfies:\n?\nfm\n\u03c32=1\nn\n\u03c6\u2217(fm)||y \u2212 \u03b1||2dfm+1\nnTr(? K),\n(11)\nwhere ||z|| denotes the Euclidean norm and \u03b1 =\nmmfm. This decomposition reveals\nthat the obtained \u03c32will be equal to the estimated\n\u201cactual\u201d noise plus a \u201ccorrection\u201d term that is the av-\nerage squared error of predicting the training latent\nvalues from the inducing variables.\n?[f|fm] = KnmK\u22121\nSo far we assumed that the inducing inputs are se-\nlected by applying gradient-based optimization. How-\never, this can be difficult in high dimensional input\nspaces as the number of variables becomes very large.\nFurther, the kernel function might not be differentiable\nwith respect to the inputs. In such cases we can still\napply the variational method by selecting the induc-\ning inputs from the training inputs.\nproperty of this discrete optimization scheme is that\nFV monotonically increases when we greedily select in-\nducing inputs and adapt the hyperparameters. Next\nwe discuss this greedy selection method.\nAn important\n3.1GREEDY SELECTION\nLet m \u2282 {1,...,n} be the indices of a subset of data\nthat are used as the inducing variables. The training\npoints that are not part of the inducing set are indexed\nby n\u2212m and are called the remaining points, e.g. fn\u2212m\ndenotes the remaining latent function values.\nvariational method is applied similarly to the pseudo-\ninputs case.Assuming the variational distribution\nq(f) = p(fn\u2212m|fm)\u03c6(fm), we can express a variational\nbound that has the same form as the bound in eq. (9)\nwith the only difference that? K = Cov(fn\u2212m|fm).\nThe selection of inducing variables among the training\ndata requires a prohibitive combinatorial search. A\nsuboptimal solution can be based on a greedy selection\nscheme where we start with an empty inducing set\nm = \u2205 and a remaining set n \u2212 m = {1,...,n}. At\neach iteration, we add a training point j \u2208 J \u2282 n\u2212m,\nwhere J is a randomly chosen working set, into the\ninducing set that maximizes the selection criterion \u2206j.\nThe\nIt is important to interleave the greedy selection pro-\ncess with the adaption of the hyperparameters (\u03c32,\u03b8).\nThis can be viewed as an EM-like algorithm; at the\nE step we add one point into the inducing set and at\nthe M step we update the hyperparameters. To ob-\ntain a reliable convergence, the approximate marginal\nlikelihood must monotonically increase at each E or M\nstep. The PP and SPGP log likelihoods do not sat-\nisfy such a requirement because they can also decrease\nas we add points into the inducing set. In contrast,"},{"page":5,"text":"Titsias\nthe bound FV is guaranteed to monotonically increase\nsince now the EM-like algorithm is a variational EM.\nTo clarify this, we state the following proposition.\nProposition 1. Let (Xm,fm) be the current set of\ninducing points and m the corresponding set of indices.\nAny point i \u2208 n \u2212 m added into the inducing set can\nnever decrease the lower bound.\nProof:\nthe variational distribution is p(fn\u2212m|fm)\u03c6\u2217(fm) =\np(fn\u2212(m\u222ai)|fi,fm)p(fi|fm)\u03c6\u2217(fm).\nnew point, the term p(fi|fm)\u03c6\u2217(fm) is replaced by the\noptimal \u03c6\u2217(fi,fm) distribution.\ncrease the lower bound or leave it invariant. A more\ndetailed proof is given in (Titsias, 2009).\nBefore the new point (fi,xi) is added,\nWhen we add the\nThis can either in-\nA consequence of the above proposition is that the\ngreedy selection process monotonically increases the\nlower bound and this holds for any possible crite-\nrion \u2206. An obvious choice is to use FV as the cri-\nterion, which can be evaluated in O(nm) time for any\ncandidate point in the working set J. Such a selec-\ntion process maximizes the decrease in the divergence\nKL(q(f)||p(f|y)).\n4 COMPARISON\nIn this section we compare the lower bound FV, the\nPP and the SPGP log likelihood in some toy prob-\nlems. All these functions are continuous with respect\nto (Xm,\u03c32,\u03b8) and can be maximized using gradient-\nbased optimization.\nOur working example will be the one-dimensional\ndataset2considered in Snelson and Ghahramani (2006)\nthat consists of 200 training points; see Figure 1. We\ntrain a sparse GP model using the squared exponential\nkernel defined by \u03c32\ndataset is small and the full GP model is tractable,\nwe compare the sparse approximations with the ex-\nact GP prediction. The plots in the first row of Fig-\nure 1 show the predictive distributions for the three\nmethods assuming 15 inducing inputs. The left plot\ndisplays the mean prediction with two-standard error\nbars (shown as blue solid lines) obtained by the max-\nimization of FV. The prediction of the full GP model\nis displayed using dashed red lines. The middle plot\nshows the corresponding solution found by PP and the\nright plot the solution found by SPGP. The prediction\nobtained by the variational method almost exactly re-\nproduces the full GP prediction. The final value of\nthe variational lower bound was \u221255.5708, while the\nvalue of the maximized true log marginal likelihood\nwas \u221255.5647. Further, the estimated hyperparame-\nters found by FV match the hyperparameters found\nfexp(\u22121\n2\u21132||xi\u2212 xj||2). Since the\n2obtained from www.gatsby.ucl.ac.uk\/\u223csnelson\/.\nby maximizing the true log marginal likelihood. In\ncontrast, training the sparse model using the PP log\nlikelihood gives a poor approximation.\nmethod gave a much more satisfactory answer than\nPP although not as good as the variational method.\nThe SPGP\nTo consider a more challenging problem, we decrease\nthe number of the original 200 training examples by\nmaintaining only 20 of them3. We repeat the experi-\nment above using exactly the same setup. The second\nrow of Figure 1, displays the predictive distributions\nof the three methods.The prediction of the varia-\ntional method is identical to the full GP prediction\nand the hyperparameters match those obtained by full\nGP training. On the other hand, the PP log likeli-\nhood leads to a significant overfitting of the training\ndata since the mean curve interpolates the training\npoints and the error bars are very noisy. SPGP pro-\nvides a solution that significantly disagrees with the\nfull GP prediction both in terms of the mean predic-\ntion and the errors bars.\nthe error bars found by SPGP varies a lot in differ-\nent input regions.This nonstationarity is achieved\nby setting \u03c32very close to zero and modelling the\nactual noise by the heteroscedastic diagonal matrix\ndiag[Knn\u2212 KnmK\u22121\nmmKmn]. The fact that this diago-\nnal matrix (the sum of its elements is the trace Tr(? K))\nproximated.\nNotice that the width of\nis large indicates that the full GP model is not well ap-\nThe reason PP and SPGP do not recover the full GP\nmodel when we optimize over (Xm,\u03c32,\u03b8) is not the\nlocal maxima. To clarify this point, we repeated the\nexperiments by initializing the PP and SPGP log like-\nlihoods to optimal inducing inputs and hyperparam-\neters values where the later are obtained by full GP\ntraining. The predictions found are similar to those\nshown in Figure 1. A way to ensure that the full GP\nmodel will be recovered as we increase the number\nof inducing inputs is to select them from the training\ninputs. This, however, turns the continuous optimiza-\ntion problem into a discrete one and moreover PP and\nSPGP face the non-smooth convergence problem.\nRegarding FV, it is clear from section 3 that by maxi-\nmizing over Xmwe approach the full GP model in the\nsense of KL(q(f,fm)|p(f,fm|y)). Something less clear\nis that FV efficiently regularizes the hyperparameters\n(\u03c32,\u03b8) so as overfitting is avoided. This is achieved by\nthe regularization trace term: \u22121\nTr(? K) is large because there are not sufficiently many\nters that give a smoother function. Also, when Tr(? K)\n2\u03c3\u22122Tr(? K). When\ninducing variables, this term favours kernel parame-\nis large the decomposition in eq. (11) implies that \u03c32\n3The points were chosen from the original set according\nto the MATLAB command: X = X(1:10:end)."},{"page":6,"text":"Variational Learning of Inducing Variables in Sparse Gaussian Processes\nFigure 1: The first row corresponds to 200 training points and the second row to 20 training points. The first column\nshows the prediction (blue solid lines) obtained by maximizing FV over the 15 pseudo-inputs and the hyperparameters.\nThe full GP prediction is shown with red dashed lines. Initial locations of the pseudo-inputs are shown on the top as\ncrosses, while final positions are given on the bottom as crosses. The second column shows the predictive distributions\nfound by PP and similarly the third column for SPGP.\nmust increase as well. These properties are useful for\navoiding overfitting and also imply that the prediction\nobtained by FV will tend to be smoother than the pre-\ndiction of the full GP model. In contrast, the PP and\nSPGP log likelihoods can find more flexible solutions\nthan the full GP prediction which indicates that they\nare prone to overfitting.\n5EXPERIMENTS\nIn this section we compare the variational lower bound\n(VAR), the projected process approximate log likeli-\nhood (PP) and the sparse pseudo-inputs GP (SPGP)\nlog likelihood in four real datasets. As a baseline tech-\nnique, we use the subset of data (SD) method. For\nall sparse GP methods we jointly maximize the al-\nternative objective functions w.r.t. hyperparameters\n(\u03b8,\u03c32) and the inducing inputs Xmusing the conju-\ngate gradients algorithm. Xmis initialized to a ran-\ndomly chosen subset of training inputs. In each run\nall methods are initialized to the same inducing in-\nputs and hyperparameters.\nria we use are the standardized mean squared error\n(SMSE), given by\nT\nnegative log probability density (SNLP) as defined in\n(Rasmussen and Williams, 2006). Smaller values for\nboth error measures imply better performance. In all\nthe experiments we use the squared-exponential kernel\nwith varied length-scale.\nThe performance crite-\n1\n||y\u2217\u2212f\u2217||2\nvar(y\u2217), and the standardized\nFirstly, we consider the Boston-housing dataset, which\nconsists of 455 training examples and 51 test examples.\nSince the dataset is small, full GP training is tractable.\nIn the first experiment, we fix the parameters (\u03b8,\u03c32)\nto values obtained by training the full GP model. Thus\nwe can investigate the difference of the methods solely\non how the inducing inputs are selected. We rigorously\ncompare the methods by calculating the moments-\nmatching divergence KL(p(f\u2217|y)||q(f\u2217)) between the\ntrue test posterior p(f\u2217|y) and each of the approxi-\nmate test posteriors. For the SPGP method the ap-\nproximate test posterior distribution is computed by\nusing the exact test conditional p(f\u2217|fm). Figure 2(a)\nshow the KL divergence as the number of inducing\npoints increases. Means and one-standard error bars\nwere obtained by repeating the experiment 10 times.\nNote that only the VAR method is able to match the\nfull GP model; for around 200 points we closely match\nthe full GP prediction. Interestingly, when the induc-\ning inputs are initialized to all training inputs, i.e.\nXm = X, PP and SPGP still give a different solu-\ntion from the full GP model despite the fact that the\nhyperparameters are kept fixed to the values of the\nfull GP model. The reason this is happening is that\nthey are not lower bounds to the true log marginal\nlikelihood and as shown in Figure 2(c) they become\nupper bounds. To show that the effective selection\nof the inducing inputs achieved by VAR is not a co-\nincidence, we compare it with the case where the in-\nputs are kept fixed to their initial randomly selected\ntraining inputs. Figure 2(b) displays the evolution of\nthe KL divergence for the VAR, the random selection\nplus PP (RSPP) and the SD method. Note that the\nonly difference between VAR and RSPP is that VAR"},{"page":7,"text":"Titsias\n100200 300400\n0\n10\n20\n30\n40\n50\n60\n70\nNumber of inducing variables\nKL(p||q)\nVAR\nPP\nSPGP\n100200300 400\n0\n10\n20\n30\n40\n50\n60\n70\nNumber of inducing variables\nKL(p||q)\nVAR\nRSPP\nSD\n100\nNumber of inducing variables\n200300400\n\u22121500\n\u22121000\n\u2212500\n0\nLog marginal likelihood\nFullGP\nVAR\nPP\nSPGP\n(a)(b)(c)\n100200300400\n0.05\n0.1\n0.15\n0.2\nNumber of inducing variables\nSMSE\nFullGP\nVAR\nPP\nSPGP\n100200 300400\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n2\nNumber of inducing variables\nSNLP\nFullGP\nVAR\nPP\nSPGP\n100200300400\n\u2212600\n\u2212400\n\u2212200\n0\n200\n400\nNumber of inducing variables\nLog marginal likelihood\nFullGP\nVAR\nPP\nSPGP\n(d)(e)(f)\nFigure 2: (a) show the KL divergence as the number of inducing variables increases for the VAR the PP and SPGP\nmethods. Similarly (b) show the divergence for the VAR, RSPP and SD methods. (c) displays the approximate log\nmarginal likelihoods; the true log marginal likelihood value is displayed by using the dotted horizontal line. (d) and (e)\nshow the SMSE and SNLP errors (obtained by joint learning hyperparameters and inducing-inputs) against the number\nof inducing variables. (f) shows the corresponding log marginal likelihoods.\noptimizes the lower bound over the initial values of\nthe inducing inputs, while RSPP just keep them fixed.\nClearly RSPP significantly improves over the SD pre-\ndiction, and VAR significantly improves over RSPP.\nIn a second experiment, we jointly learn inducing vari-\nables and hyperparameters and compare the meth-\nods in terms of the SMSE and SNLP errors.\nresults are displayed in the second row of Figure 2.\nNote that the PP and SPGP methods achieve a much\nhigher log likelihood value (Figure 2(f)) than the true\nlog marginal likelihood. However, the error measures\nclearly indicate that the PP log likelihood significantly\noverfits the data. SPGP gives better SMSE error than\nthe full GP model but it overfits w.r.t. the SNLP error.\nThe variational method matches the full GP model.\nThe\nWe now consider three large datasets: the kin40k\ndataset, the sarcos and the abalone datasets4that\nhave been widely used before. Note that the abalone\ndataset is small enough so as we will be able to train\nthe full GP model. The inputs were normalized to\nhave zero mean and unit variance on the training set\n4kin40k:\nida.first.fraunhofer.de\/ anton\/data.html.\nsarcos: 44,484 training, 4,449 test, 21 attributes,\nwww.gaussianprocess.org\/gpml\/data\/.\nabalone:3,133 training,\nwww.liaad.up.pt\/ ltorgo\/Regression\/DataSets.html.\n10000 training, 30000 test, 8 attributes,\n1,044 test, 8 attributes,\nand the outputs were centered so as to have zero mean\non the training set. For the kin40k and the sarcos\ndatasets, the SD method was obtained in a subset of\n2000 training points. We vary the size of the inducing\nvariables in powers of two from 16 to 1024. For the\nsarcos dataset, the experiment for 1024 was not per-\nformed since is was unrealistically expensive. All the\nobjective functions were jointly maximized over induc-\ning inputs and hyperparameters. The experiment was\nrepeated 5 times. Figure 3 shows the results.\nFrom the plots in Figure 3, we can conclude the fol-\nlowing. The PP log likelihood is significantly prone to\noverfitting as the SNLP errors clearly indicate. How-\never, note that in the kin40k and sarcos datasets,\nPP gave the best performance w.r.t. to SMSE error.\nThis is probably because of the ability of PP to in-\nterpolate the training examples that can lead to good\nSMSE error when the actual observation noise is low.\nSPGP often has the worst performance in terms of the\nSMSE error and almost always the best performance\nin terms of the SNLP error. In the abalone dataset,\nSPGP had significantly better SNLP error than the\nfull GP model. Since the SNLP error depends on the\npredictive variances, we believe that the good perfor-\nmance of SPGP is due to its heteroscedastic ability.\nFor example, in the kin40k dataset, SPGP makes\n\u03c32almost zero and thus the actual noise in the like-\nlihood is modelled by the heteroscedastic covariance"},{"page":8,"text":"Variational Learning of Inducing Variables in Sparse Gaussian Processes\n02004006008001000\n0\n0.05\n0.1\nNumber of inducing variables\nSMSE\nSD\nVAR\nPP\nSPGP\n0100200300400 500\n0.01\n0.015\n0.02\n0.025\n0.03\n0.035\n0.04\n0.045\n0.05\nNumber of inducing variables\nSMSE\nSD\nVAR\nPP\nSPGP\n0200400 6008001000\n0.4\n0.42\n0.44\n0.46\n0.48\n0.5\nNumber of inducing variables\nSMSE\nFullGP\nVAR\nPP\nSPGP\n02004006008001000\n\u22122.5\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\nNumber of inducing variables\nSNLP\nSD\nVAR\nPP\nSPGP\n0 100200300 400500\n\u22122.5\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\nNumber of inducing variables\nSNLP\nSD\nVAR\nPP\nSPGP\n0 200 4006008001000\n\u22120.5\n\u22120.4\n\u22120.3\n\u22120.2\n\u22120.1\n0\n0.1\nNumber of inducing variables\nSNLP\nFullGP\nVAR\nPP\nSPGP\nFigure 3: The first column displays the SMSE (top) and SNLP (bottom) errors for the kin40k dataset with respect to\nthe number of inducing points. The second column shows the corresponding plots for the sarcos dataset and similarly\nthe third column shows the results for the abalone dataset.\ndiag[Knn\u2212 KnmK\u22121\nterm is large may indicate that the full GP model is\nnot well approximated. Finally the variational method\nhas good performance. VAR never had the worst per-\nformance and it didn\u2019t exhibit overfitting. The exam-\nples in section 4, the Boston-housing and the abalone\ndataset indicate that the VAR method remains much\ncloser to the full GP model than the other methods.\nmmKmn]. The fact that the latter\n6CONCLUSION\nWe proposed a variational framework for sparse GP\nregression that can reliably learn inducing inputs and\nhyperparameters by minimizing the KL divergence be-\ntween the true posterior GP and an approximate one.\nThis method can be more generally applicable. Cur-\nrently we apply this technique to classification. An\ninteresting topic for the future is to apply this method\nto GP models that assume multiple latent functions.\nAcknowledgments\nI am grateful to Neil Lawrence for his help. This work\nis funded by EPSRC Grant No EP\/F005687\/1 \u201dGaus-\nsian Processes for Systems Identification with Appli-\ncations in Systems Biology\u201d.\nReferences\nCsato, L. and Opper, M. (2002). Sparse online Gaussian\nprocesses. Neural Computation, 14:641\u2013668.\nLawrence, N. D., Seeger, M., and Herbrich, R. (2002). Fast\nsparse Gaussian process methods: the informative vector\nmachine. In Neural Information Processing Systems, 13.\nMIT Press.\nQui\u02dc nonero-Candela, J. and Rasmussen, C. E. (2005). A\nunifying view of sparse approximate Gaussian process re-\ngression. Journal of Machine Learning Research, 6:1939\u2013\n1959.\nRasmussen, C. E. and Williams, C. K. I. (2006). Gaussian\nProcesses for Machine Learning. MIT Press.\nSchwaighofer, A. and Tresp, V. (2003). Transductive and\ninductive methods for approximate Gaussian process re-\ngression. In Neural Information Processing Systems 15.\nMIT Press.\nSeeger, M. (2003).\nBayesian Gaussian Process Models:\nPAC-Bayesian Generalisation Error Bounds and Sparse\nApproximations. PhD thesis, University of Edinburgh.\nSeeger, M., Williams, C. K. I., and Lawrence, N. D. (2003).\nFast forward selection to speed up sparse Gaussian pro-\ncess regression. In Ninth International Workshop on Ar-\ntificial Intelligence. MIT Press.\nSmola, A. J. and Bartlett, P. (2001). Sparse greedy Gaus-\nsian process regression. In Neural Information Process-\ning Systems, 13. MIT Press.\nSmola, A. J. and Sch\u00a8 olkopf, B. (2000). Sparse greedy ma-\ntrix approximations for machine learning. In Interna-\ntional Conference on Machine Learning.\nSnelson, E. and Ghahramani, Z. (2006). Sparse Gaussian\nprocess using pseudo-inputs. In Neural Information Pro-\ncessing Systems, 13. MIT Press.\nTitsias, M. K. (2009).Variational Model Selection for\nSparse Gaussian Process Regression. Technical report,\nSchool of Computer Science, University of Manchester.\nWilliams, C. K. I. and Seeger, M. (2001).\nNystr\u00a8 om method to speed up kernel machines. In Neural\nInformation Processing Systems 13. MIT Press.\nUsing the"}],"widgetId":"rgw22_56aba07e5d27a"},"id":"rgw22_56aba07e5d27a","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=220320048&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw23_56aba07e5d27a"},"id":"rgw23_56aba07e5d27a","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=220320048&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":220320048,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/220320048_Variational_Learning_of_Inducing_Variables_in_Sparse_Gaussian_Processes","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56aba07e5d27a"},"id":"rgw2_56aba07e5d27a","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":220320048},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=220320048&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56aba07e5d27a"},"id":"rgw1_56aba07e5d27a","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"EbQaOb9XpnwA9oHq1Idg6CiDE95ixmCTDZ3TmuIMotdErDva6RjAOJFrxoz6gm+3+K3kHXU7bLY76J2VGwjewYaEm1aeLF5TgT+G1Vs84s9uSF0AeKWQYDqkZiuq2NVpHpY2IGoqqrJW4jgF+s6AMM5LSVADHCo96xaLfQ2gLDJOhkMoJLHeZTXmethR5V5rkAL2OoxC80yxlKnVXuETE3P8kxQYhyYcT1sGYe\/GvM7vT1Qb20Kq6MFi8BiqQKpLYi2UhST6az4srjhkHpjHSZ\/jtelKpJncvfLXYwUAVDc=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/220320048_Variational_Learning_of_Inducing_Variables_in_Sparse_Gaussian_Processes\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Variational Learning of Inducing Variables in Sparse Gaussian Processes\" \/>\n<meta property=\"og:description\" content=\"Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/220320048_Variational_Learning_of_Inducing_Variables_in_Sparse_Gaussian_Processes\/links\/0ffa78e00cf25dfdcf53dd6e\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/220320048_Variational_Learning_of_Inducing_Variables_in_Sparse_Gaussian_Processes\" \/>\n<meta property=\"rg:id\" content=\"PB:220320048\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Variational Learning of Inducing Variables in Sparse Gaussian Processes\" \/>\n<meta name=\"citation_author\" content=\"Michalis K. Titsias\" \/>\n<meta name=\"citation_publication_date\" content=\"2009\/04\/01\" \/>\n<meta name=\"citation_volume\" content=\"5\" \/>\n<meta name=\"citation_firstpage\" content=\"567\" \/>\n<meta name=\"citation_lastpage\" content=\"574\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/220320048_Variational_Learning_of_Inducing_Variables_in_Sparse_Gaussian_Processes\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/220320048_Variational_Learning_of_Inducing_Variables_in_Sparse_Gaussian_Processes\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-336539a7-9ccf-48e2-8059-e6854f6ab797","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":451,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw24_56aba07e5d27a"},"id":"rgw24_56aba07e5d27a","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-336539a7-9ccf-48e2-8059-e6854f6ab797", "993f6d0f87c9d7a4fb696f9384f47aa585e3bf02");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-336539a7-9ccf-48e2-8059-e6854f6ab797", "993f6d0f87c9d7a4fb696f9384f47aa585e3bf02");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw25_56aba07e5d27a"},"id":"rgw25_56aba07e5d27a","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/220320048_Variational_Learning_of_Inducing_Variables_in_Sparse_Gaussian_Processes","requestToken":"uI3p9s\/XQbWegF33BFwAgnEq+RFB0Ywt8ipmkZVY88AYVtzzrowF1Ecn\/AIxds6AvVohD4QWiZ+UyfMcNmDVwMG2qcK85t827S\/Fruh+lNwUTos77\/nMziTovnUGtTv1ZVFm\/hPyBwPHgd+2Atd\/Bim81sKJp4aeOxDaxwPSNWxiMuVXRkydKKdlqVLlUve9xJtq0SMU6FFR\/yjo2Z6DVk\/LMQYi\/o6Tl47tm5Si4CLj4inwplbWPgTVnAp0qeCY+\/dEUH6p7aWYWWjqMMHoHIGfakv3mte2FbHfeWOT7L4=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=s4nnMRi8NgZ7ZHhNcE7m3t7fXu8dYPJo_2ZDBQO7VMYSqqaZ5rno-sRZoX_55ryX","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjIwMzIwMDQ4X1ZhcmlhdGlvbmFsX0xlYXJuaW5nX29mX0luZHVjaW5nX1ZhcmlhYmxlc19pbl9TcGFyc2VfR2F1c3NpYW5fUHJvY2Vzc2Vz","signupCallToAction":"Join for free","widgetId":"rgw27_56aba07e5d27a"},"id":"rgw27_56aba07e5d27a","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw26_56aba07e5d27a"},"id":"rgw26_56aba07e5d27a","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw28_56aba07e5d27a"},"id":"rgw28_56aba07e5d27a","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
