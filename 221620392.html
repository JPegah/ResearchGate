<!DOCTYPE html> <html lang="en" class="" id="rgw40_56ab9f64d00da"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="IfP9Fyve8xIepSBlc4YXRYvgSrcvmvtXvhN5kZoItAl1X6flUa0NqSX2AWaJpbqGkDHEQcMsI40gf6v82fCykqX18oTCwueEsv44bDrljXwUpp8ha2/etPyoeo1sX5I9YXwDlyx4boQNefTkSrCjvFFj9R5n0+hEJUSZShIFQSfqhTv7N3ifGSgldd4fGj+HGMtYEx/9SArqnlmtf/Nr6dKiIZNx/34cNDFHQWRlGU3evBp1wqiMOl2bdnCbKwKredBkRX337ckDHF7nzN2HfbHz7AKPyY6bzXIASTKhGBE="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-bda1041a-584d-416f-8c71-b93131a0a98e",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process" />
<meta property="og:description" content="Nonparametric Bayesian models provide a framework for flexi ble probabilistic modelling of complex datasets. Unfortunately, the high-dimensional averages re- quired for Bayesian methods can be..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process/links/0a85e5388a8f2cb1b4000000/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process" />
<meta property="rg:id" content="PB:221620392" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process" />
<meta name="citation_author" content="Finale Doshi-Velez" />
<meta name="citation_author" content="David Knowles" />
<meta name="citation_author" content="Shakir Mohamed" />
<meta name="citation_author" content="Zoubin Ghahramani" />
<meta name="citation_conference_title" content="Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009. Proceedings of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada." />
<meta name="citation_publication_date" content="2009/12/01" />
<meta name="citation_firstpage" content="1294" />
<meta name="citation_lastpage" content="1302" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Finale_Doshi_Velez/publication/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process/links/0a85e5388a8f2cb1b4000000.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Conference Paper');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab9f64d00da" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab9f64d00da" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab9f64d00da">  <div class="type-label"> Conference Paper   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Large%20Scale%20Nonparametric%20Bayesian%20Inference%3A%20Data%20Parallelisation%20in%20the%20Indian%20Buffet%20Process&rft.title=Advances%20in%20Neural%20Information%20Processing%20Systems%2022%20-%20Proceedings%20of%20the%202009%20Conference&rft.jtitle=Advances%20in%20Neural%20Information%20Processing%20Systems%2022%20-%20Proceedings%20of%20the%202009%20Conference&rft.date=2009&rft.pages=1294-1302&rft.au=Finale%20Doshi-Velez%2CDavid%20Knowles%2CShakir%20Mohamed%2CZoubin%20Ghahramani&rft.genre=inProceedings"></span> <h1 class="pub-title" itemprop="name">Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process</h1> <meta itemprop="headline" content="Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process/links/0a85e5388a8f2cb1b4000000/smallpreview.png">  <div id="rgw7_56ab9f64d00da" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab9f64d00da" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Finale_Doshi_Velez" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Finale Doshi velez" alt="Finale Doshi velez" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Finale Doshi velez</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw9_56ab9f64d00da" data-account-key="Finale_Doshi_Velez">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Finale_Doshi_Velez"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Finale Doshi velez" alt="Finale Doshi velez" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Finale_Doshi_Velez" class="display-name">Finale Doshi velez</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Harvard_University" title="Harvard University">Harvard University</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56ab9f64d00da" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/David_Knowles2" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="David A. Knowles" alt="David A. Knowles" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">David A. Knowles</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw11_56ab9f64d00da" data-account-key="David_Knowles2">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/David_Knowles2"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="David A. Knowles" alt="David A. Knowles" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/David_Knowles2" class="display-name">David A. Knowles</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Stanford_University" title="Stanford University">Stanford University</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56ab9f64d00da"> <a href="researcher/59315559_Shakir_Mohamed" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Shakir Mohamed" alt="Shakir Mohamed" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Shakir Mohamed</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw13_56ab9f64d00da">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/59315559_Shakir_Mohamed"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Shakir Mohamed" alt="Shakir Mohamed" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/59315559_Shakir_Mohamed" class="display-name">Shakir Mohamed</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw14_56ab9f64d00da"> <a href="researcher/8159937_Zoubin_Ghahramani" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Zoubin Ghahramani</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw15_56ab9f64d00da">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/8159937_Zoubin_Ghahramani"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/8159937_Zoubin_Ghahramani" class="display-name">Zoubin Ghahramani</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">         Conference: Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009. Proceedings of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada.      <div class="pub-source"> Source: <a href="http://dblp.uni-trier.de/db/conf/nips/nips2009.html#Doshi-VelezKMG09" rel="nofollow">DBLP</a> </div>  </div> <div id="rgw16_56ab9f64d00da" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>Nonparametric Bayesian models provide a framework for flexi ble probabilistic modelling of complex datasets. Unfortunately, the high-dimensional averages re- quired for Bayesian methods can be slow, especially with the unbounded repre- sentations used by nonparametric models. We address the challenge of scaling Bayesian inference to the increasingly large datasets found in real-world appli- cations. We focus on parallelisation of inference in the Ind ian Buffet Process (IBP), which allows data points to have an unbounded number of sparse latent features. Our novel MCMC sampler divides a large data set between multiple processors and uses message passing to compute the global likelihoods and pos- teriors. This algorithm, the first parallel inference schem e for IBP-based models, scales to datasets orders of magnitude larger than have previously been possible.</div> </p>  </div>  </div>   </div>      <div class="action-container"> <div id="rgw17_56ab9f64d00da" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw31_56ab9f64d00da">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw32_56ab9f64d00da">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Finale_Doshi_Velez/publication/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process/links/0a85e5388a8f2cb1b4000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Finale_Doshi_Velez">Finale Doshi velez</a>, <span class="js-publication-date"> May 30, 2014 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw34_56ab9f64d00da" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw35_56ab9f64d00da" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw36_56ab9f64d00da" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw37_56ab9f64d00da" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw38_56ab9f64d00da" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw39_56ab9f64d00da" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw33_56ab9f64d00da" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FFinale_Doshi_Velez%2Fpublication%2F221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process%2Flinks%2F0a85e5388a8f2cb1b4000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw30_56ab9f64d00da"  itemprop="articleBody">  <p>Page 1</p> <p>Large Scale Nonparametric Bayesian Inference:<br />Data Parallelisation in the Indian Buffet Process<br />Finale Doshi-Velez∗<br />University of Cambridge<br />Cambridge, CB21PZ, UK<br />finale@alum.mit.edu<br />David Knowles∗<br />University of Cambridge<br />Cambridge, CB21PZ, UK<br />dak33@cam.ac.uk<br />Shakir Mohamed∗<br />University of Cambridge<br />Cambridge, CB21PZ, UK<br />sm694@cam.ac.uk<br />Zoubin Ghahramani<br />University of Cambridge<br />Cambridge, CB21PZ, UK<br />zoubin@eng.cam.ac.uk<br />Abstract<br />Nonparametric Bayesian models provide a framework for flexible probabilistic<br />modelling of complex datasets. Unfortunately, the high-dimensional averages re-<br />quired for Bayesian methods can be slow, especially with the unbounded repre-<br />sentations used by nonparametric models. We address the challenge of scaling<br />Bayesian inference to the increasingly large datasets found in real-world appli-<br />cations. We focus on parallelisation of inference in the Indian Buffet Process<br />(IBP), which allows data points to have an unbounded number of sparse latent<br />features. Our novel MCMC sampler divides a large data set between multiple<br />processors and uses message passing to compute the global likelihoods and pos-<br />teriors. This algorithm, the first parallel inference scheme for IBP-based models,<br />scales to datasets orders of magnitude larger than have previously been possible.<br />1Introduction<br />From information retrieval to recommender systems, from bioinformatics to financial market anal-<br />ysis, the amount of data available to researchers has exploded in recent years. While large, these<br />datasets are often still sparse: For example, a biologist may have expression levels from thousands<br />of genes from only a few people. A ratings database may contain millions of users and thousands<br />of movies, but each user may have only rated a few movies. In such settings, Bayesian methods<br />provide a robust approach to drawing inferences and making predictions from sparse information.<br />At the heart of Bayesian methods is the idea that all unknown quantities should be averaged over<br />when making predictions. Computing these high-dimensional average is thus a key challenge in<br />scaling Bayesian inference to large datasets, especially for nonparametric models.<br />Advances in multicore and distributed computing provide one answer to this challenge: if each pro-<br />cessor can consider only a small part of the data, then inference in these large datasets might become<br />more tractable. However, such data parallelisation of inference is nontrivial—while simple models<br />might only require pooling a small number of sufficient statistics [1], inference in more complex<br />models might require the frequent communication of complex, high-dimensional probability distri-<br />butions between processors. Building on work on approximate asynchronous multicore inference<br />for topic models [2], we develop a message passing framework for data-parallel Bayesian inference<br />applicable to a variety of models, including matrix factorization and the Indian Buffet Process (IBP).<br />∗Authors contributed equally.<br />1</p>  <p>Page 2</p> <p>Nonparametric models are attractive for large datasets because they automatically adapt to the com-<br />plexity of the data, relieving the researcher from the need to specify aspects of the model such as the<br />number of latent factors. Much recent work in nonparametric Bayesian modelling has focused on<br />the Chinese restaurant process (CRP), which is a discrete distribution that can be used to assign data<br />points to an unbounded number of clusters. However, many real-world datasets have observations<br />that may belong to multiple clusters—for example, a gene may have multiple functions; an image<br />may contain multiple objects. The IBP [3] is a distribution over infinite sparse binary matrices that<br />allows data points to be represented by an unbounded number of sparse latent features or factors.<br />While the parallelisation method we present in this paper is applicable to a broad set of models, we<br />focus on inference for the IBP because of its unique challenges and potential.<br />Many serial procedures have been developed for inference in the IBP, including variants of Gibbs<br />sampling [3, 4], which may be augmented with Metropolis split-merge proposals [5], slice sam-<br />pling [6], particle filtering [7], and variational inference [8]. With the exception of the accelerated<br />Gibbs sampler of [4], these methods have been applied to datasets with less than 1,000 observations.<br />To achieve efficient paralellisation, we exploit an idea recently introduced in [4], which maintains<br />a distribution over parameters while sampling. Coupled with a message passing scheme over pro-<br />cessors, this idea enables computations for inference to be distributed over many processors with<br />few losses in accuracy. We demonstrate our approach on a problem with 100,000 observations. The<br />largest application of IBP inference to date, our work opens the use of the IBP and similar models<br />to a variety of data-intensive applications.<br />2Latent Feature Model<br />The IBP can be used to define models in which each observation is associated with a set of latent<br />factors or features. A binary feature-assignment matrix Z represents which observations possess<br />which hidden features, where Znk = 1 if observation n has feature k and Znk = 0 otherwise.<br />For example, the observations might be images and the hidden features could be possible objects in<br />those images. Importantly, the IBP allows the set of such possible hidden features to be unbounded.<br />To generate a sample from the IBP, we first imagine that the rows of Z (the observations) are cus-<br />tomers and the columns of Z (the features) are dishes in an infinite buffet. The first customer takes<br />the first Poisson(α) dishes. The following customers try previously sampled dishes with probability<br />mk/n, where mkis the number of people who tried dish k before customer n. Each customer also<br />takes Poisson(α/n) new dishes. The value Znkrecords if customer n tried dish k. This generative<br />process allows an unbounded set of features but guarantees that a finite dataset will contain a finite<br />number of features with probability one. The process is also exchangeable in that the order in which<br />customers visit the buffet has no impact on the distribution of Z. Finally, if the effect of possessing<br />a feature is independent of the feature index, the model is also exchangeable in the columns of Z.<br />We associate with the feature assignment matrix Z, a feature matrix A with rows that parameterise<br />the effect that possessing each feature has on the data. Given these matrices, we write the probability<br />of the data as P(X|Z,A). Our work requires that P(A|X,Z) can be computed or approximated<br />efficiently by an exponential family distribution. Specifically, we apply our techniques to both a<br />fully-conjugate linear-Gaussian model and non-conjugate Bernoulli model.<br />Linear Gaussian Model.<br />We model an N×D real-valued data matrix X as a product:<br />X = ZA + ǫ,<br />(1)<br />where Z is the binary feature-assignment matrix and A is a K by D real-valued matrix with an<br />independent Gaussian prior N(0,σ2<br />of the N by D noise matrix ǫ is independent with a N(0,σ2<br />posterior on the features A is Gaussian, given by mean and covariance<br />?−1<br />a) on each element (see cartoon in Figure 1(a)). Each element<br />n) distribution. Given Z and X, the<br />µA=<br />?<br />ZTZ +σ2<br />x<br />σ2<br />a<br />I<br />ZTXΣA= σ2<br />x<br />?<br />ZTZ +σ2<br />x<br />σ2<br />a<br />I<br />?−1<br />(2)<br />Bernoulli Model.<br />We use a leaky, noisy-or likelihood for each element of an N×D matrix X:<br />?<br />P(Xnd= 1|Z,A) = 1 − ǫ λ<br />kZnkAkd.<br />(3)<br />2</p>  <p>Page 3</p> <p>XZA<br />...<br />*<br />DD<br />~<br />NN<br />...<br />+ ε<br />K<br />K<br />(a) Representation of the linear-Gaussian model.<br />The data X is generated from the product of the<br />feature assignment matrix Z and feature matrix A.<br />In the Bernoulli model, the product ZA adjusts the<br />probability of X = 1<br />prior<br />posterior<br />statistics<br />statistics<br />posterior<br />posterior<br />statistics<br />posterior<br />statistics<br />P3<br />P4<br />P2<br />P1<br />Root<br />(b) Message passing process. Pro-<br />cessors send sufficient statistics of<br />the likelihood up to the root, which<br />calculates and sends the (exact) pos-<br />terior back to the processors.<br />Figure 1: Diagrammatic representation of the model structure and the message passing process.<br />Each element of the A matrix is binary with independent Bernoulli(pA) priors. The parameters ǫ<br />and λ determine how “leaky” and how “noisy” the or-function is, respectively. Typical hyperpa-<br />rameter values are ǫ = 0.95 and λ = 0.2. The posterior P(A|X,Z) cannot be computed in closed<br />form; however, a mean-field variational posterior in which we approximate P(A|X,Z) as product<br />of independent Bernoulli variables?K,D<br />3Parallel Inference<br />k,dqkd(akd) can be readily derived.<br />We describe both synchronous and asynchronous procedures for approximate, parallel inference in<br />the IBP that combines MCMC with message passing. We first partition the data among the proces-<br />sors, using Xpto denote the subset of observations X assigned to processor p. We use Zpto denote<br />the latent features associated with the data on processor p. In [4], the distribution P(A|X−n,Z−n)<br />was used to derive an accelerated sampler for sampling Zn, where n indexes the nthobservation and<br />−n is the set of all observations except n. In our parallel inference approach, each processor p main-<br />tains a distribution Pp(A|X−n,Z−n), a local approximation to P(A|X−n,Z−n). The distributions<br />Ppare updated via message passing between the processors.<br />The inference alternates between three steps:<br />• Message passing: processors communicate to compute the exact P(A|X,Z).<br />• Gibbs sampling: processors sample a new set of Zp’s in parallel.<br />• Hyperparameter sampling: a root processor resamples global hyperparameters<br />The sampler is approximate because during Gibbs sampling, all processors resample elements of Z<br />at the same time; their posteriors Pp(A|X,Z) are no longer the true P(A|X,Z).<br />Message Passing<br />We use Bayes rule to factorise the posterior over features P(A|Z,X):<br />P(A|Z,X) ∝ P(A)<br />?<br />p<br />P(Xp|Zp,A)<br />(4)<br />If the prior P(A) and the likelihoods P(Xp|Zp,A) are conjugate exponential family models, then<br />the sufficient statistics of P(A|Z,X) are the sum of the sufficient statistics of each term on the right<br />side of equation (4). For example, the sufficient statistics in the linear-Gaussian model are means<br />and covariances; in the Bernoulli model, they are counts of how often each element Akdequals one.<br />The linear-Gaussian messages have size O(K2+KD), and the Bernoulli messages O(KD), where<br />K is the number of features. For nonparametric models such as the IBP, the number of features K<br />grows as O(logN). This slow growth means that messages remain small, even for large datasets.<br />The most straightforward way to compute the full posterior is to arrange processors in a tree archi-<br />tecture, as belief propagation is then exact. The message s from processor p to processor q is:<br />sp→q= lp+<br />?<br />r∈N(p)\q<br />sr→p<br />3</p>  <p>Page 4</p> <p>where N(p)\q are the processors attached to p besides q and lpare the sufficient statistics from<br />processor p. A dummy neighbour containing the statistics of the prior is connected to (an arbitrarily<br />designated) root processor. Also passed are the feature counts mp<br />feature k within processor p. (See figure 1(b) for a cartoon.)<br />k=?<br />n∈XpZp<br />nk, the popularity of<br />Gibbs Sampling<br />In general, Znkcan be Gibbs-sampled using Bayes rule<br />P(Znk|Z−nk,X) ∝ P(Znk|Z−nk)P(X|Z).<br />The probability P(Znk|Z−nk) depends on the size of the dataset N and the number of observations<br />mkusing feature k. At the beginning of the Gibbs sampling stage, each processor has the correct<br />values of mk. We compute m−p<br />k<br />= mk−mp<br />updated, approximate mk≈ m−p<br />k<br />+ mp<br />current stage (good for popular features).<br />k, and, as the processor’s internal feature counts mp<br />k. This approximation assumes m−p<br />kare<br />k<br />stays fixed during the<br />The collapsed likelihood P(X|Z) integrating out the feature values A is given by:<br />P(X|Z) ∝<br />?<br />A<br />P(Xn|Zn,A)P(A|Z−n,X−n)dA,<br />where the partial posterior P(A|Z−n,X−n) ∝<br />can be efficiently computed by subtracting observation n’s contribution to the sufficient statistics.1<br />For non-conjugate models, we can use an exponential family distribution Q(A) to approximate<br />P(A|X,Z) during message passing. A draw A ∼ Q−p(A) is then used to initialise an uncollapsed<br />Gibbssampler. TheoutputtedsamplesofAareusedtocomputesufficientstatisticsforthelikelihood<br />P(X|Z). In both cases, new features are added as described in [3].<br />P(A|Z,X)<br />P(Xn|Zn,A). In conjugate models, P(A|Z−n,X−n)<br />Hyperparameter Resampling<br />likelihood can also be sampled during inference. Resampling α depends only on the total number of<br />active features; thus it can easily be resampled at the root and propagated to the other processors. In<br />the linear-Gaussian model, the posteriors on the noise and feature variances (starting from gamma<br />priors) depend on various squared-errors, which can also be computed in a distributed fashion.<br />The IBP concentration parameter α and hyperparameters of the<br />For more general, non-conjugate models, resampling the hyperparameters requires two steps. In<br />the first step, a hyperparameter value is proposed by the root and propagated to the processors.<br />The processors each compute the likelihood of the current and proposed hyperparameter values and<br />propagate this value back to root. The root evaluates a Metropolis step for the hyperparameters<br />and propagates the decision back to the leaves. The two-step approach introduces a latency in the<br />resampling but does not require any additional message passing rounds.<br />Asynchronous Operation<br />perparameter resampling as if they occur in separate phases. In practice, these phases may occur<br />asynchronously: between its Gibbs sweeps, each processor updates its feature posterior based on<br />the most current messages it has received and sends likelihood messages to its parent. Likewise,<br />the root continuously resamples hyperparameters and propagates the values down through the tree.<br />While another layer of approximation, this asynchronous form of message passing allows faster pro-<br />cessors to share information and perform more inference on their data instead of waiting for slower<br />processors.<br />So far we have discussed message passing, Gibbs sampling, and hy-<br />Implementation Note<br />considered with care. Other parallel inference for nonparametric models, such as the HDP [2],<br />simply matched features by their index, that is, assumed that the ithfeature on processor p was also<br />the ithfeature on processor q. In the IBP, we find that this indiscriminate feature merging is often<br />disastrous when adding or deleting features: if none of the observations in a particular processor are<br />using a feature, we cannot simply delete that column of Z and shift the other features over—doing<br />so destroys the alignment of features across processors.<br />When performing parallel inference in the IBP, a few factors need to be<br />1In the IBP, only the linear-Gaussian model exhibits this conjugate structure. However, many other matrix<br />factorization models (such as PCA) often have this conjugate form.<br />4</p>  <p>Page 5</p> <p>4 Comparison to Exact Metropolis<br />Because all Zp’s are sampled at once, the posteriors Pp(A|X,Z) used by each processor in section 3<br />are no longer exact. Below we show how Metropolis–Hastings (MH) steps can make the parallel<br />sampler exact, but introduce significant computational overheads both in computing the transition<br />probabilities and in the message passing. We argue that trying to do exact inference is a poor<br />use of computational resources (especially as any finite chain will not be exact); empirically, the<br />approximate sampler behaves similarly to the MH sampler while finding higher likelihood regions<br />in the data.<br />Exact Parallel Metropolis Sampler.<br />each stage of the approximate inference to make the sampler exact. Unfortunately, the approximate<br />sampler makes several non-independent random choices in each stage of the inference, making the<br />reverse proposal inconvenient to compute. We circumvent this issue by fixing the random seed, mak-<br />ing the initial stage of the approximate sampler a deterministic function, and then add independent<br />random noise to create a proposal distribution. This approach makes both the forward and reverse<br />transition probabilities simple to compute.<br />Ideally, we would simply add an MH accept/reject step after<br />Formally, letˆ Zpbe the matrix output after a set of Gibbs sweeps on Zp. We use all theˆ Zp’s to<br />propose a new Z′matrix. The acceptance probability of the proposal is<br />min(1,P(X|Z′)P(Z′)Q(Z′→ Z)<br />P(X|Z)P(Z)Q(Z → Z′)),<br />(5)<br />where the likelihood terms P(X|Z) and P(Z) are readily computed in a distributed fashion. For<br />the transition distribution Q, we note that if we set the random seed r, then the matrixˆ Zpfrom the<br />Gibbs sweeps in the processor is some deterministic function of the input matrix Zp. The proposal<br />Zp′is a (stochastic) noisy representation ofˆ Zpin which for example<br />P(Zp′<br />nk= 1) = .99 if<br />ˆZp<br />nk= 1,P(Zp′<br />nk= 1) = .01 if<br />ˆZp<br />nk= 0<br />(6)<br />where K should be at least the number of features inˆ Zp. We set Zp′<br />in figure 2.)<br />nk= 0 for k &gt; K. (See cartoon<br />To compute the backward probability, we take Zp′and apply the same number of Gibbs sampling<br />sweeps with the same random seed r. The resultingˆZp′is a deterministic function of Zp′. The<br />backward probability Q(Zp′→ Zp) which is the probability of going from Zp′to Zpusing 6.<br />While the transition probabilities can be computed in a distributed, asynchronous fashion, all of the<br />processors must synchronise when deciding whether to accept the proposal.<br />Experimental Comparison<br />niques, we ran each inference type on 1000 block images of [3] on 5 simulated processors. Each<br />test was repeated 25 times. For each of the 25 tests, we create a held out dataset by setting elements<br />of the last 100 images as missing values. For the first 50 test images, we set all even numbered<br />dimensions as the missing elements, and every odd numbered dimension as the missing values for<br />the last 50 images. Each sampler was run for 10,000 iterations with 5 Gibbs sweeps per iteration;<br />statistics were collected from the second half of the chain. To keep the probability of an acceptance<br />reasonable, we allowed each processor to change only small parts of its Zp: the feature assignments<br />Znfor 1, 5, or 10 data points each during each sweep.<br />In table 1, we see that the approximate sampler runs about five times faster than the exact samplers<br />while achieving comparable (or better) predictive likelihoods and reconstruction errors on held-<br />out data. Both the acceptance rates and the predictive likelihoods fall as the exact sampler tries<br />to take larger steps, suggesting that the difference between the approximate and exact sampler’s<br />performance on predictive likelihood is due to poor mixing by the exact sampler. Figure 4 shows<br />empirical CDFs for the number of features k , IBP concentration parameter α, the noise variance σ2<br />and the feature variance σ2<br />exact Metropolis samplers (gray) for the variances; the concentration parameter is smaller, but the<br />feature counts are similar to the single-processor case.<br />To compare the exact Metropolis and approximate inference tech-<br />n,<br />a. The approximate sampler (black) produces similar CDFs to the various<br />5</p>  <p>Page 6</p> <p>Zp Zp<br />Gibbs with<br />fixed seed<br />Zp’<br />Random<br />noise<br />Figure 2: Cartoon of MH<br />proposal<br />MethodTime (s)Test L2<br />Error<br />0.0468<br />0.0488<br />0.0555<br />0.0487<br />Test<br />Likelihood<br />0.1098<br />0.0893<br />0.0196<br />0.1292<br />LogMH Accept<br />Proportion<br />0.1106<br />0.0121<br />0.0062<br />-<br />MH, n = 1<br />MH, n = 5<br />MH, n = 10<br />Approximate<br />717<br />1075<br />1486<br />179<br />Table 1: Evaluation of exact and approximate methods.<br />56789<br />Feature Count<br />101112131415<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />0.8<br />0.9<br />1<br />Empirical CDF for IBP Concentration<br />Cumilative Probability<br /> <br /> <br />Single Processor<br />Approximate Sampling<br />Exact Sampling, various windows<br />(a) Active feature count k<br />0.511.522.53 3.5<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />0.8<br />0.9<br />1<br />Empirical CDF for IBP Concentration<br />IBP Concentration Parameter<br />Cumilative Probability<br /> <br /> <br />Single Processor<br />Approximate Sampling<br />Exact Sampling, various windows<br />(b) IBP Concentration α<br />0.10.150.20.250.3<br />Noise Variance<br />0.350.40.450.50.550.6<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />0.8<br />0.9<br />1<br />Empirical CDF for Noise Variance<br />Cumilative Probability<br /> <br /> <br />Single Processor<br />Approximate Sampling<br />Exact Sampling, various windows<br />(c) Noise variance σ2<br />x<br />0.20.250.30.350.40.450.5<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />0.8<br />0.9<br />1<br />Empirical CDF for Feature Variance<br />Feature Variance<br />Cumilative Probability<br /> <br /> <br />Single Processor<br />Approximate Sampling<br />Exact Sampling, various windows<br />(d) Feature variance σ2<br />a<br />Figure 3: Empirical CDFs: The solid black line is the approximate sampler; the three solid gray lines<br />are the MH samplers with n equal to 1, 5, and 10 (lighter shades indicate larger n. The approximate<br />sampler and the MH samplers for smaller n have similar CDFs; the n = 10 MH sampler’s differing<br />CDF indicates it did not mix in 7500 iterations (reasonable since its acceptance rate was 0.0062).<br />5Analysis of Mixing Properties<br />We ran a series of experiments on 10,000 36-dimensional block images of [3] to study the effects<br />of various sampler configurations on running time, performance, and mixing time properties of<br />the sampler. 5000 elements of the data matrix were held-out as test data. Figure 4 shows test<br />log-likelihoods using 1, 7, 31 and 127 parallel processors simulated in software, using 1000 outer<br />iterations with 5 Gibbs inner iterations each. The parallel samplers have similar test likelihoods as<br />the serial algorithm with significant savings in running time. The characteristic shape of the test<br />likelihood, similar across all testing regimes, indicates how the features are learned. Initially, a large<br />number of features are added, which provides improvements in the test likelihood. A refinement<br />phase, in which excess features are pruned, provides further improvements.<br />Figure 4 shows hairiness-index plots for each of the test cases after thinning and burn-in. The hairi-<br />ness index, based on the method of CUSUM for monitoring MCMC convergence [9, 10], monitors<br />how often the derivatives of sampler statistics—in our case, the number of features, the test likeli-<br />hood, and α—change in sign; infrequent changes in sign indicate that the sampler may not be mixed.<br />The outer bounds on the plots are the 95% confidence bounds. The index stays within the bounds<br />suggesting that the chains are mixing.<br />Finally, we considered the trade-off between mixing and running time as the number of outer it-<br />erations and inner Gibbs iterations are varied. Each combination of inner and outer iterations was<br />set so that the total number of Gibbs sweeps through the data was 5000. Mixing efficiency was<br />−1012345<br />−2<br />−1.5<br />−1<br />−0.5<br />0<br />0.5<br />Test loglikelihood<br />Time (s)<br />Test Loglikelihood for inner = 5 and outer = 1000 iterations<br /> <br /> <br />20 40 60 80100<br />Processors = 31<br />0<br />0.5<br />1<br />Processors = 1<br />Hairiness Index<br />20 40 60 80 100<br />Processors = 127<br />0<br />0.5<br />1<br />Processors = 7<br />Hairiness Index<br />20 40 60 80100<br />0<br />0.5<br />1<br />Hairiness Index<br />20 40 60 80100<br />0<br />0.5<br />1<br />Hairiness Index<br />Proc = 1<br />Proc = 7<br />Proc = 31<br />Proc = 127<br />Figure 4: Change in likelihood for various numbers of processors over the simulation time. The<br />corresponding hairiness index plots are shown on the left.<br />6</p>  <p>Page 7</p> <p>01020<br />#Inner Iterations<br />304050<br />0.4<br />0.5<br />0.6<br />0.7<br />0.8<br />0.9<br />1<br /># Effective Samples per Outer Iter.<br /> <br /> <br />Proc = 1<br />Proc = 7<br />Proc = 31<br />Proc = 127<br />1731127<br />10<br />1<br />10<br />2<br />10<br />3<br />10<br />4<br />10<br />5<br /># Processors<br />Total Time (s)<br /> <br /> <br />i = 50, o = 100<br />i = 20, o = 250<br />i = 10, o = 500<br />i = 5, o = 1000<br />i = 1, o = 5000<br />Figure 5: Effects of changing the number of inner iterations on: (a) The effective sample size (b)<br />Total running time (Gibbs and Message passing).<br />Table 2: Test log-likelihoods on real-world datasets for the serial, synchronous and asynchronous<br />inference types.<br />DatasetND Description Serial<br />p = 1<br />-4.74<br />Synch<br />p = 16<br />-4.77<br />Async<br />p = 16<br />-4.84AR Faces [11]26001598faces with lighting, acces-<br />sories (real-valued)<br />STDFT of a piano recording<br />(real-valued)<br />indicatorsof<br />(binary-valued)<br />Piano [12]57931161-1.435-1.182-1.228<br />Flickr [13]1000001000imagetags—-0.0584<br />measured via the effective number of samples per sample [10], which evaluates what fraction of<br />the samples are independent (ideally, we would want all samples to be independent, but MCMC<br />produces dependent chains). Running time for Gibbs sampling was taken to be the time required by<br />the slowest processor (since all processors must synchronize before message passing); the total time<br />reflected the Gibbs time and the message-passing time. As seen in figure 5, completing fewer inner<br />Gibbs iterations per outer iteration results in faster mixing, which is sensible as the processors are<br />communicating about their data more often. However, having fewer inner iterations requires more<br />frequent message passing; as the number of processors becomes large, the cost of message passing<br />becomes a limiting factor.2<br />6Real-world Experiments<br />We tested our parallel scheme on three real world datasets on a 16 node cluster using the Matlab<br />Distributed Computing Engine, using 3 inner Gibbs iterations per outer iteration. The first dataset<br />was a set of 2,600 frontal face images with 1,598 dimensions [11]. While not extremely large,<br />the high-dimensionality of the dataset makes it challenging for other inference approaches. The<br />piano dataset [12] consisted of 57,931 samples from a 161-dimensional short-time discrete Fourier<br />transform of a piano piece. Finally, the binary-valued Flickr dataset [13] indicated whether each<br />of 1000 popular keywords occurred in the tags of 100,000 images from Flickr. Performance was<br />measured using test likelihoods and running time. Test likelihoods look only at held-out data and<br />thus they allow us to ‘honestly’ evaluate the model’s fit. Table 2 summarises the data and shows that<br />all approaches had similar test-likelihood performance.<br />In the faces and music datasets, the Gibbs time per iteration improved almost linearly as the number<br />of processors increased (figure 6). For example, we observed a 14x-speedup for p = 16 in the music<br />dataset. Meanwhile, the message passing time remained small even with 16 processors—7% of the<br />Gibbs time for the faces data and 0.1% of the Gibbs time for the music data. However, waiting for<br />synchronisation became a significant factor in the synchronous sampler. Figure 6(c) compares the<br />times for running inference serially, synchronously and asynchronously with 16 processors. The<br />2Webelievepartofthetimingresultsmaybeanartifact, asthesimulationoverestimatesthemessagepassing<br />time. In the actual parallel system (section 6), the cost of message passing was negligible.<br />7</p>  <p>Page 8</p> <p>1248 16<br />0<br />20<br />40<br />60<br />80<br />100<br />120<br />number of processors<br />mean time per outer iteration/s<br /> <br /> <br />sampling<br />waiting<br />(a) Timing analysis for faces<br />dataset<br />124816<br />0<br />200<br />400<br />600<br />800<br />1000<br />1200<br />number of processors<br />mean time per iteration/s<br /> <br /> <br />sampling<br />waiting<br />(b) Timing analysis for music<br />dataset<br />10<br />−2<br />10<br />0<br />10<br />2<br />10<br />4<br />−2.8<br />−2.6<br />−2.4<br />−2.2<br />−2<br />−1.8x 10<br />7<br />time/s<br />log joint<br /> <br /> <br />serial P=1<br />synchronous P=16<br />asynchronous P=16<br />(c) Timingcomparisonfordifferent<br />approaches<br />Figure 6: Bar charts comparing sampling time and waiting times for synchronous parallel inference.<br />asynchronous inference is 1.64 times faster than the synchronous case, reducing the computational<br />time from 11.8s per iteration to 7.2s.<br />7 Discussion and Conclusion<br />As datasets grow, parallelisation is an increasingly attractive and important feature for doing infer-<br />ence. Not only does it allow multiple processors/multicore technologies to be leveraged for large-<br />scale analyses, but it also reduces the amount of data and associated structures that each processor<br />needs to keep in memory. Existing work has focused both on general techniques to efficiently split<br />variables across processors in undirected graphical models [14] and factor graphs [15] and specific<br />models such as LDA [16, 17]. Our work falls in between: we leverage properties of a specific kind<br />of parallelisation—data parallelisation—for a fairly broad class of models.<br />Specifically, we describe a parallel inference procedure that allows nonparametric Bayesian models<br />based on the Indian Buffet Process to be applied to large datasets. The IBP poses specific challenges<br />to data parallelisation in that the dimensionality of the representation changes during inference and<br />may be unbounded. Our contribution is an algorithm for data-parallelisation that leverages a com-<br />pact representation of the feature posterior that approximately decorrelates the data stored on each<br />processor, thus limiting the communication bandwidth between processors. While we focused on<br />the IBP, the ideas presented here are applicable to a more general problems in unsupervised learning<br />including bilinear models such as PCA, NMF, and ICA.<br />Our sampler is approximate, and we show that in conjugate models, it behaves similarly to an ex-<br />act sampler—but with much less computational overhead. However, as seen in the Bernoulli case,<br />variational message passing for non-conjugate data doesn’t always produce good results if the ap-<br />proximating distribution is a poor match for the true feature posterior. Determining when variational<br />message passing is successful is an interesting question for future work. Other interesting directions<br />include approaches for dynamically optimising the network topology (for example, slower proces-<br />sors could be moved lower in the tree). Finally, we note that a middle ground between synchronous<br />and asynchronous operations as we presented them might be a system that gives each processor a<br />certain amount of time, instead of a certain number of iterations, to do Gibbs sweeps. Further study<br />along these avenues should lead to even more efficient data-parallel Bayesian inference techniques.<br />8</p>  <p>Page 9</p> <p>References<br />[1] C. Chu, S. Kim, Y. Lin, Y. Yu, G. Bradski, A. Ng, and K. Olukotun, “Map-reduce for machine<br />learning on multicore,” in Advances in Neural Information Processing Systems, p. 281, MIT<br />Press, 2007.<br />[2] A. Asuncion, P. Smyth, and M. Welling, “Asynchronous distributed learning of topic models,”<br />in Advances in Neural Information Processing Systems 21, 2008.<br />[3] T. Griffiths and Z. Ghahramani, “Infinite latent feature models and the Indian buffet process,”<br />in Advances in Neural Information Processing Systems, vol. 16, NIPS, 2006.<br />[4] F. Doshi-Velez and Z. Ghahramani, “Accelerated inference for the Indian buffet process,” in<br />International Conference on Machine Learning, 2009.<br />[5] E. Meeds, Z. Ghahramani, R. Neal, and S. Roweis, “Modeling dyadic data with binary latent<br />factors,” in Advances in Neural Information Processing Systems, vol. 19, pp. 977–984, 2007.<br />[6] Y. W. Teh, D. G¨ or¨ ur, and Z. Ghahramani, “Stick-breaking construction for the Indian buffet<br />process,” in Proceedings of the Intl. Conf. on Artificial Intelligence and Statistics, vol. 11,<br />pp. 556–563, 2007.<br />[7] F. Wood and T. L. Griffiths, “Particle filtering for nonparametric Bayesian matrix factoriza-<br />tion,” in Advances in Neural Information Processing Systems, vol. 19, pp. 1513–1520, 2007.<br />[8] F. Doshi-Velez, K. T. Miller, J. Van Gael, and Y. W. Teh, “Variational inference for the In-<br />dian buffet process,” in Proceedings of the Intl. Conf. on Artificial Intelligence and Statistics,<br />vol. 12, pp. 137–144, 2009.<br />[9] S. P. Brooks and G. O. Roberts, “Convergence assessment techniques for Markov Chain Monte<br />Carlo,” Statistics and Computing, vol. 8, pp. 319–335, 1998.<br />[10] C. R. Robert and G. Casella, Monte Carlo Statistical Methods. Springer, second ed., 2004.<br />[11] A. M. Mart’inez and A. C. Kak, “PCA versus LDA,” IEEE Trans. Pattern Anal. Mach. Intelli-<br />gence, vol. 23, pp. 228–233, 2001.<br />[12] G. E. Poliner and D. P. W. Ellis, “A discriminative model for polyphonic piano transcription,”<br />EURASIP J. Appl. Signal Process., vol. 2007, no. 1, pp. 154–154, 2007.<br />[13] T. Kollar and N. Roy, “Utilizing object-object and object-scene context when planning to find<br />things.,” in International Conference on Robotics and Automation, 2009.<br />[14] C. G. Joseph Gonzalez, Yucheng Low, “Residual splash for optimally parallelizing belief prop-<br />agation,” in Proceedings of the Twelfth International Conference on Artificial Intelligence and<br />Statistics (D. van Dyk and M. Welling, eds.), vol. 5, pp. 177–184, JMLR, 2009.<br />[15] D. Stern, R. Herbrich, and T. Graepel, “Matchbox: Large scale online Bayesian recommenda-<br />tions,” in 18th International World Wide Web Conference (WWW2009), April 2009.<br />[16] R. Nallapati, W. Cohen, and J. Lafferty, “Parallelized variational EM for Latent Dirichlet Al-<br />location: An experimental evaluation of speed and scalability,” in ICDMW ’07: Proceedings<br />of the Seventh IEEE International Conference on Data Mining Workshops, (Washington, DC,<br />USA), pp. 349–354, IEEE Computer Society, 2007.<br />[17] D. Newman, A. Asuncion, P. Smyth, and M. Welling, “Distributed inference for Latent Dirich-<br />let Allocation,” in Advances in Neural Information Processing Systems 20 (J. Platt, D. Koller,<br />Y. Singer, and S. Roweis, eds.), pp. 1081–1088, Cambridge, MA: MIT Press, 2008.<br />9</p>  <a href="https://www.researchgate.net/profile/Finale_Doshi_Velez/publication/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process/links/0a85e5388a8f2cb1b4000000.pdf">Download full-text</a> </div> <div id="rgw22_56ab9f64d00da" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw23_56ab9f64d00da">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw24_56ab9f64d00da"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Finale_Doshi_Velez/publication/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process/links/0a85e5388a8f2cb1b4000000.pdf" class="publication-viewer" title="0a85e5388a8f2cb1b4000000.pdf">0a85e5388a8f2cb1b4000000.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Finale_Doshi_Velez">Finale Doshi velez</a> &middot; Jun 11, 2014 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw25_56ab9f64d00da"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://mlg.eng.cam.ac.uk/shakir/papers/NIPS_pIBP.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process">Large Scale Nonparametric Bayesian Inference: Data...</a> </div>  <div class="details">   Available from <a href="http://mlg.eng.cam.ac.uk/shakir/papers/NIPS_pIBP.pdf" target="_blank" rel="nofollow">cam.ac.uk</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw27_56ab9f64d00da" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw28_56ab9f64d00da">  </ul> </div> </div>   <div id="rgw18_56ab9f64d00da" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw19_56ab9f64d00da"> <div> <h5> <a href="publication/291437088_Bayesian_inference_of_natural_selection_from_allele_frequency_time_series" class="color-inherit ga-similar-publication-title"><span class="publication-title">Bayesian inference of natural selection from allele frequency time series</span></a>  </h5>  <div class="authors"> <a href="researcher/59302372_Joshua_G_Schraiber" class="authors ga-similar-publication-author">Joshua G. Schraiber</a>, <a href="researcher/9066602_Steven_N_Evans" class="authors ga-similar-publication-author">Steven N. Evans</a>, <a href="researcher/38766258_Montgomery_Slatkin" class="authors ga-similar-publication-author">Montgomery Slatkin</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw20_56ab9f64d00da"> <div> <h5> <a href="publication/291391742_Bayesian_Inference_for_Assessing_Effects_of_Email_Marketing_Campaigns" class="color-inherit ga-similar-publication-title"><span class="publication-title">Bayesian Inference for Assessing Effects of Email Marketing Campaigns</span></a>  </h5>  <div class="authors"> <a href="researcher/2095208736_Jiexing_Wu" class="authors ga-similar-publication-author">Jiexing Wu</a>, <a href="researcher/2055045507_Kate_J_Li" class="authors ga-similar-publication-author">Kate J. Li</a>, <a href="researcher/2095197229_Jun_S_Liu" class="authors ga-similar-publication-author">Jun S. Liu</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw21_56ab9f64d00da"> <div> <h5> <a href="publication/288713796_Probabilistic_Model-Based_Approach_for_Heart_Beat_Detection" class="color-inherit ga-similar-publication-title"><span class="publication-title">Probabilistic Model-Based Approach for Heart Beat Detection</span></a>  </h5>  <div class="authors"> <a href="researcher/2091574500_Hugh_Chen" class="authors ga-similar-publication-author">Hugh Chen</a>, <a href="researcher/82585176_Yusuf_Erol" class="authors ga-similar-publication-author">Yusuf Erol</a>, <a href="researcher/2091364196_Eric_Shen" class="authors ga-similar-publication-author">Eric Shen</a>, <a href="researcher/2091338617_Stuart_Russell" class="authors ga-similar-publication-author">Stuart Russell</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw41_56ab9f64d00da" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw42_56ab9f64d00da">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw43_56ab9f64d00da" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=AAARaAngVyn10itWBlxSNsW_6QoRekonxbR9BSOPDOff2c8tt6Zs1ab-66s2cISf" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="5Qx46VhX9Vo/yXpKt1yjl/YwEN7phP2ey80Ez3ZFvPvtJvG3PAmo2DFtwl1xripeJDEVyni5ZygzW6vD32Pv8LGaZIYceE/Lq+mUa83x+wwMwKj7d4Ca2b6+BJww7rygL+Wd2ba5190Y2E8NHtbyEJtA4z9flRhNwRjbZXGWC36k87k795q+xaMQwEqnT8zAjgleCV2eyWGK6r4/W226xjVHOXn2mNerJGgSangjV1+C2pNmbWujj3brDup23vv3T0dr2c46MqkQRYY7KAK1J5wAkGnxV6svoO6PF0pFs48="/> <input type="hidden" name="urlAfterLogin" value="publication/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjIxNjIwMzkyX0xhcmdlX1NjYWxlX05vbnBhcmFtZXRyaWNfQmF5ZXNpYW5fSW5mZXJlbmNlX0RhdGFfUGFyYWxsZWxpc2F0aW9uX2luX3RoZV9JbmRpYW5fQnVmZmV0X1Byb2Nlc3M%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjIxNjIwMzkyX0xhcmdlX1NjYWxlX05vbnBhcmFtZXRyaWNfQmF5ZXNpYW5fSW5mZXJlbmNlX0RhdGFfUGFyYWxsZWxpc2F0aW9uX2luX3RoZV9JbmRpYW5fQnVmZmV0X1Byb2Nlc3M%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjIxNjIwMzkyX0xhcmdlX1NjYWxlX05vbnBhcmFtZXRyaWNfQmF5ZXNpYW5fSW5mZXJlbmNlX0RhdGFfUGFyYWxsZWxpc2F0aW9uX2luX3RoZV9JbmRpYW5fQnVmZmV0X1Byb2Nlc3M%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw44_56ab9f64d00da"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 335;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Finale Doshi velez","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Finale_Doshi_Velez","institution":"Harvard University","institutionUrl":false,"widgetId":"rgw4_56ab9f64d00da"},"id":"rgw4_56ab9f64d00da","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=5125534","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab9f64d00da"},"id":"rgw3_56ab9f64d00da","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=221620392","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":221620392,"title":"Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Conference Paper","details":{"conferenceInfos":"Conference: Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009. Proceedings of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada."},"source":{"sourceUrl":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2009.html#Doshi-VelezKMG09","sourceName":"DBLP"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process"},{"key":"rft.title","value":"Advances in Neural Information Processing Systems 22 - Proceedings of the 2009 Conference"},{"key":"rft.jtitle","value":"Advances in Neural Information Processing Systems 22 - Proceedings of the 2009 Conference"},{"key":"rft.date","value":"2009"},{"key":"rft.pages","value":"1294-1302"},{"key":"rft.au","value":"Finale Doshi-Velez,David Knowles,Shakir Mohamed,Zoubin Ghahramani"},{"key":"rft.genre","value":"inProceedings"}],"widgetId":"rgw6_56ab9f64d00da"},"id":"rgw6_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=221620392","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":221620392,"peopleItems":[{"data":{"authorNameOnPublication":"Finale Doshi velez","accountUrl":"profile\/Finale_Doshi_Velez","accountKey":"Finale_Doshi_Velez","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Finale Doshi velez","profile":{"professionalInstitution":{"professionalInstitutionName":"Harvard University","professionalInstitutionUrl":"institution\/Harvard_University"}},"professionalInstitutionName":"Harvard University","professionalInstitutionUrl":"institution\/Harvard_University","url":"profile\/Finale_Doshi_Velez","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Finale_Doshi_Velez","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw9_56ab9f64d00da"},"id":"rgw9_56ab9f64d00da","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=5125534&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Harvard University","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":4,"accountCount":2,"publicationUid":221620392,"widgetId":"rgw8_56ab9f64d00da"},"id":"rgw8_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=5125534&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=4&accountCount=2&publicationUid=221620392","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"David A. Knowles","accountUrl":"profile\/David_Knowles2","accountKey":"David_Knowles2","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"David A. Knowles","profile":{"professionalInstitution":{"professionalInstitutionName":"Stanford University","professionalInstitutionUrl":"institution\/Stanford_University"}},"professionalInstitutionName":"Stanford University","professionalInstitutionUrl":"institution\/Stanford_University","url":"profile\/David_Knowles2","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"David_Knowles2","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw11_56ab9f64d00da"},"id":"rgw11_56ab9f64d00da","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=7693199&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Stanford University","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":4,"accountCount":2,"publicationUid":221620392,"widgetId":"rgw10_56ab9f64d00da"},"id":"rgw10_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=7693199&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=4&accountCount=2&publicationUid=221620392","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/59315559_Shakir_Mohamed","authorNameOnPublication":"Shakir Mohamed","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Shakir Mohamed","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/59315559_Shakir_Mohamed","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw13_56ab9f64d00da"},"id":"rgw13_56ab9f64d00da","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=59315559&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw12_56ab9f64d00da"},"id":"rgw12_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=59315559&authorNameOnPublication=Shakir%20Mohamed","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/8159937_Zoubin_Ghahramani","authorNameOnPublication":"Zoubin Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Zoubin Ghahramani","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/8159937_Zoubin_Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw15_56ab9f64d00da"},"id":"rgw15_56ab9f64d00da","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=8159937&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw14_56ab9f64d00da"},"id":"rgw14_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=8159937&authorNameOnPublication=Zoubin%20Ghahramani","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab9f64d00da"},"id":"rgw7_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=221620392&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":221620392,"abstract":"<noscript><\/noscript><div>Nonparametric Bayesian models provide a framework for flexi ble probabilistic modelling of complex datasets. Unfortunately, the high-dimensional averages re- quired for Bayesian methods can be slow, especially with the unbounded repre- sentations used by nonparametric models. We address the challenge of scaling Bayesian inference to the increasingly large datasets found in real-world appli- cations. We focus on parallelisation of inference in the Ind ian Buffet Process (IBP), which allows data points to have an unbounded number of sparse latent features. Our novel MCMC sampler divides a large data set between multiple processors and uses message passing to compute the global likelihoods and pos- teriors. This algorithm, the first parallel inference schem e for IBP-based models, scales to datasets orders of magnitude larger than have previously been possible.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw16_56ab9f64d00da"},"id":"rgw16_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=221620392","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process\/links\/0a85e5388a8f2cb1b4000000\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw17_56ab9f64d00da"},"id":"rgw17_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab9f64d00da"},"id":"rgw5_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=221620392&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":59302372,"url":"researcher\/59302372_Joshua_G_Schraiber","fullname":"Joshua G. Schraiber","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9066602,"url":"researcher\/9066602_Steven_N_Evans","fullname":"Steven N. Evans","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38766258,"url":"researcher\/38766258_Montgomery_Slatkin","fullname":"Montgomery Slatkin","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291437088_Bayesian_inference_of_natural_selection_from_allele_frequency_time_series","usePlainButton":true,"publicationUid":291437088,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/291437088_Bayesian_inference_of_natural_selection_from_allele_frequency_time_series","title":"Bayesian inference of natural selection from allele frequency time series","displayTitleAsLink":true,"authors":[{"id":59302372,"url":"researcher\/59302372_Joshua_G_Schraiber","fullname":"Joshua G. Schraiber","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9066602,"url":"researcher\/9066602_Steven_N_Evans","fullname":"Steven N. Evans","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38766258,"url":"researcher\/38766258_Montgomery_Slatkin","fullname":"Montgomery Slatkin","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291437088_Bayesian_inference_of_natural_selection_from_allele_frequency_time_series","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291437088_Bayesian_inference_of_natural_selection_from_allele_frequency_time_series\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab9f64d00da"},"id":"rgw19_56ab9f64d00da","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291437088","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2095208736,"url":"researcher\/2095208736_Jiexing_Wu","fullname":"Jiexing Wu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2055045507,"url":"researcher\/2055045507_Kate_J_Li","fullname":"Kate J. Li","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095197229,"url":"researcher\/2095197229_Jun_S_Liu","fullname":"Jun S. Liu","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Journal of Business and Economic Statistics","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291391742_Bayesian_Inference_for_Assessing_Effects_of_Email_Marketing_Campaigns","usePlainButton":true,"publicationUid":291391742,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.24","url":"publication\/291391742_Bayesian_Inference_for_Assessing_Effects_of_Email_Marketing_Campaigns","title":"Bayesian Inference for Assessing Effects of Email Marketing Campaigns","displayTitleAsLink":true,"authors":[{"id":2095208736,"url":"researcher\/2095208736_Jiexing_Wu","fullname":"Jiexing Wu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2055045507,"url":"researcher\/2055045507_Kate_J_Li","fullname":"Kate J. Li","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095197229,"url":"researcher\/2095197229_Jun_S_Liu","fullname":"Jun S. Liu","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of Business and Economic Statistics 01\/2016;  DOI:10.1080\/07350015.2016.1141096"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291391742_Bayesian_Inference_for_Assessing_Effects_of_Email_Marketing_Campaigns","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291391742_Bayesian_Inference_for_Assessing_Effects_of_Email_Marketing_Campaigns\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56ab9f64d00da"},"id":"rgw20_56ab9f64d00da","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291391742","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2091574500,"url":"researcher\/2091574500_Hugh_Chen","fullname":"Hugh Chen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":82585176,"url":"researcher\/82585176_Yusuf_Erol","fullname":"Yusuf Erol","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2091364196,"url":"researcher\/2091364196_Eric_Shen","fullname":"Eric Shen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2091338617,"url":"researcher\/2091338617_Stuart_Russell","fullname":"Stuart Russell","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Dec 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/288713796_Probabilistic_Model-Based_Approach_for_Heart_Beat_Detection","usePlainButton":true,"publicationUid":288713796,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/288713796_Probabilistic_Model-Based_Approach_for_Heart_Beat_Detection","title":"Probabilistic Model-Based Approach for Heart Beat Detection","displayTitleAsLink":true,"authors":[{"id":2091574500,"url":"researcher\/2091574500_Hugh_Chen","fullname":"Hugh Chen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":82585176,"url":"researcher\/82585176_Yusuf_Erol","fullname":"Yusuf Erol","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2091364196,"url":"researcher\/2091364196_Eric_Shen","fullname":"Eric Shen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2091338617,"url":"researcher\/2091338617_Stuart_Russell","fullname":"Stuart Russell","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/288713796_Probabilistic_Model-Based_Approach_for_Heart_Beat_Detection","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/288713796_Probabilistic_Model-Based_Approach_for_Heart_Beat_Detection\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw21_56ab9f64d00da"},"id":"rgw21_56ab9f64d00da","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=288713796","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw18_56ab9f64d00da"},"id":"rgw18_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=221620392&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":221620392,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":221620392,"publicationType":"inProceedings","linkId":"0a85e5388a8f2cb1b4000000","fileName":"0a85e5388a8f2cb1b4000000.pdf","fileUrl":"profile\/Finale_Doshi_Velez\/publication\/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process\/links\/0a85e5388a8f2cb1b4000000.pdf","name":"Finale Doshi velez","nameUrl":"profile\/Finale_Doshi_Velez","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Jun 11, 2014","fileSize":"309.63 KB","widgetId":"rgw24_56ab9f64d00da"},"id":"rgw24_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=221620392&linkId=0a85e5388a8f2cb1b4000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":221620392,"publicationType":"inProceedings","linkId":"0ffbf5500cf22ec95c079817","fileName":"Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process","fileUrl":"http:\/\/mlg.eng.cam.ac.uk\/shakir\/papers\/NIPS_pIBP.pdf","name":"cam.ac.uk","nameUrl":"http:\/\/mlg.eng.cam.ac.uk\/shakir\/papers\/NIPS_pIBP.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw25_56ab9f64d00da"},"id":"rgw25_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=221620392&linkId=0ffbf5500cf22ec95c079817&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw23_56ab9f64d00da"},"id":"rgw23_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=221620392&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":23,"valueFormatted":"23","widgetId":"rgw26_56ab9f64d00da"},"id":"rgw26_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=221620392","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw22_56ab9f64d00da"},"id":"rgw22_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=221620392&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":221620392,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw28_56ab9f64d00da"},"id":"rgw28_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=221620392&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":23,"valueFormatted":"23","widgetId":"rgw29_56ab9f64d00da"},"id":"rgw29_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=221620392","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw27_56ab9f64d00da"},"id":"rgw27_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=221620392&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Large Scale Nonparametric Bayesian Inference:\nData Parallelisation in the Indian Buffet Process\nFinale Doshi-Velez\u2217\nUniversity of Cambridge\nCambridge, CB21PZ, UK\nfinale@alum.mit.edu\nDavid Knowles\u2217\nUniversity of Cambridge\nCambridge, CB21PZ, UK\ndak33@cam.ac.uk\nShakir Mohamed\u2217\nUniversity of Cambridge\nCambridge, CB21PZ, UK\nsm694@cam.ac.uk\nZoubin Ghahramani\nUniversity of Cambridge\nCambridge, CB21PZ, UK\nzoubin@eng.cam.ac.uk\nAbstract\nNonparametric Bayesian models provide a framework for flexible probabilistic\nmodelling of complex datasets. Unfortunately, the high-dimensional averages re-\nquired for Bayesian methods can be slow, especially with the unbounded repre-\nsentations used by nonparametric models. We address the challenge of scaling\nBayesian inference to the increasingly large datasets found in real-world appli-\ncations. We focus on parallelisation of inference in the Indian Buffet Process\n(IBP), which allows data points to have an unbounded number of sparse latent\nfeatures. Our novel MCMC sampler divides a large data set between multiple\nprocessors and uses message passing to compute the global likelihoods and pos-\nteriors. This algorithm, the first parallel inference scheme for IBP-based models,\nscales to datasets orders of magnitude larger than have previously been possible.\n1Introduction\nFrom information retrieval to recommender systems, from bioinformatics to financial market anal-\nysis, the amount of data available to researchers has exploded in recent years. While large, these\ndatasets are often still sparse: For example, a biologist may have expression levels from thousands\nof genes from only a few people. A ratings database may contain millions of users and thousands\nof movies, but each user may have only rated a few movies. In such settings, Bayesian methods\nprovide a robust approach to drawing inferences and making predictions from sparse information.\nAt the heart of Bayesian methods is the idea that all unknown quantities should be averaged over\nwhen making predictions. Computing these high-dimensional average is thus a key challenge in\nscaling Bayesian inference to large datasets, especially for nonparametric models.\nAdvances in multicore and distributed computing provide one answer to this challenge: if each pro-\ncessor can consider only a small part of the data, then inference in these large datasets might become\nmore tractable. However, such data parallelisation of inference is nontrivial\u2014while simple models\nmight only require pooling a small number of sufficient statistics [1], inference in more complex\nmodels might require the frequent communication of complex, high-dimensional probability distri-\nbutions between processors. Building on work on approximate asynchronous multicore inference\nfor topic models [2], we develop a message passing framework for data-parallel Bayesian inference\napplicable to a variety of models, including matrix factorization and the Indian Buffet Process (IBP).\n\u2217Authors contributed equally.\n1"},{"page":2,"text":"Nonparametric models are attractive for large datasets because they automatically adapt to the com-\nplexity of the data, relieving the researcher from the need to specify aspects of the model such as the\nnumber of latent factors. Much recent work in nonparametric Bayesian modelling has focused on\nthe Chinese restaurant process (CRP), which is a discrete distribution that can be used to assign data\npoints to an unbounded number of clusters. However, many real-world datasets have observations\nthat may belong to multiple clusters\u2014for example, a gene may have multiple functions; an image\nmay contain multiple objects. The IBP [3] is a distribution over infinite sparse binary matrices that\nallows data points to be represented by an unbounded number of sparse latent features or factors.\nWhile the parallelisation method we present in this paper is applicable to a broad set of models, we\nfocus on inference for the IBP because of its unique challenges and potential.\nMany serial procedures have been developed for inference in the IBP, including variants of Gibbs\nsampling [3, 4], which may be augmented with Metropolis split-merge proposals [5], slice sam-\npling [6], particle filtering [7], and variational inference [8]. With the exception of the accelerated\nGibbs sampler of [4], these methods have been applied to datasets with less than 1,000 observations.\nTo achieve efficient paralellisation, we exploit an idea recently introduced in [4], which maintains\na distribution over parameters while sampling. Coupled with a message passing scheme over pro-\ncessors, this idea enables computations for inference to be distributed over many processors with\nfew losses in accuracy. We demonstrate our approach on a problem with 100,000 observations. The\nlargest application of IBP inference to date, our work opens the use of the IBP and similar models\nto a variety of data-intensive applications.\n2Latent Feature Model\nThe IBP can be used to define models in which each observation is associated with a set of latent\nfactors or features. A binary feature-assignment matrix Z represents which observations possess\nwhich hidden features, where Znk = 1 if observation n has feature k and Znk = 0 otherwise.\nFor example, the observations might be images and the hidden features could be possible objects in\nthose images. Importantly, the IBP allows the set of such possible hidden features to be unbounded.\nTo generate a sample from the IBP, we first imagine that the rows of Z (the observations) are cus-\ntomers and the columns of Z (the features) are dishes in an infinite buffet. The first customer takes\nthe first Poisson(\u03b1) dishes. The following customers try previously sampled dishes with probability\nmk\/n, where mkis the number of people who tried dish k before customer n. Each customer also\ntakes Poisson(\u03b1\/n) new dishes. The value Znkrecords if customer n tried dish k. This generative\nprocess allows an unbounded set of features but guarantees that a finite dataset will contain a finite\nnumber of features with probability one. The process is also exchangeable in that the order in which\ncustomers visit the buffet has no impact on the distribution of Z. Finally, if the effect of possessing\na feature is independent of the feature index, the model is also exchangeable in the columns of Z.\nWe associate with the feature assignment matrix Z, a feature matrix A with rows that parameterise\nthe effect that possessing each feature has on the data. Given these matrices, we write the probability\nof the data as P(X|Z,A). Our work requires that P(A|X,Z) can be computed or approximated\nefficiently by an exponential family distribution. Specifically, we apply our techniques to both a\nfully-conjugate linear-Gaussian model and non-conjugate Bernoulli model.\nLinear Gaussian Model.\nWe model an N\u00d7D real-valued data matrix X as a product:\nX = ZA + \u01eb,\n(1)\nwhere Z is the binary feature-assignment matrix and A is a K by D real-valued matrix with an\nindependent Gaussian prior N(0,\u03c32\nof the N by D noise matrix \u01eb is independent with a N(0,\u03c32\nposterior on the features A is Gaussian, given by mean and covariance\n?\u22121\na) on each element (see cartoon in Figure 1(a)). Each element\nn) distribution. Given Z and X, the\n\u00b5A=\n?\nZTZ +\u03c32\nx\n\u03c32\na\nI\nZTX\u03a3A= \u03c32\nx\n?\nZTZ +\u03c32\nx\n\u03c32\na\nI\n?\u22121\n(2)\nBernoulli Model.\nWe use a leaky, noisy-or likelihood for each element of an N\u00d7D matrix X:\n?\nP(Xnd= 1|Z,A) = 1 \u2212 \u01eb \u03bb\nkZnkAkd.\n(3)\n2"},{"page":3,"text":"XZA\n...\n*\nDD\n~\nNN\n...\n+ \u03b5\nK\nK\n(a) Representation of the linear-Gaussian model.\nThe data X is generated from the product of the\nfeature assignment matrix Z and feature matrix A.\nIn the Bernoulli model, the product ZA adjusts the\nprobability of X = 1\nprior\nposterior\nstatistics\nstatistics\nposterior\nposterior\nstatistics\nposterior\nstatistics\nP3\nP4\nP2\nP1\nRoot\n(b) Message passing process. Pro-\ncessors send sufficient statistics of\nthe likelihood up to the root, which\ncalculates and sends the (exact) pos-\nterior back to the processors.\nFigure 1: Diagrammatic representation of the model structure and the message passing process.\nEach element of the A matrix is binary with independent Bernoulli(pA) priors. The parameters \u01eb\nand \u03bb determine how \u201cleaky\u201d and how \u201cnoisy\u201d the or-function is, respectively. Typical hyperpa-\nrameter values are \u01eb = 0.95 and \u03bb = 0.2. The posterior P(A|X,Z) cannot be computed in closed\nform; however, a mean-field variational posterior in which we approximate P(A|X,Z) as product\nof independent Bernoulli variables?K,D\n3Parallel Inference\nk,dqkd(akd) can be readily derived.\nWe describe both synchronous and asynchronous procedures for approximate, parallel inference in\nthe IBP that combines MCMC with message passing. We first partition the data among the proces-\nsors, using Xpto denote the subset of observations X assigned to processor p. We use Zpto denote\nthe latent features associated with the data on processor p. In [4], the distribution P(A|X\u2212n,Z\u2212n)\nwas used to derive an accelerated sampler for sampling Zn, where n indexes the nthobservation and\n\u2212n is the set of all observations except n. In our parallel inference approach, each processor p main-\ntains a distribution Pp(A|X\u2212n,Z\u2212n), a local approximation to P(A|X\u2212n,Z\u2212n). The distributions\nPpare updated via message passing between the processors.\nThe inference alternates between three steps:\n\u2022 Message passing: processors communicate to compute the exact P(A|X,Z).\n\u2022 Gibbs sampling: processors sample a new set of Zp\u2019s in parallel.\n\u2022 Hyperparameter sampling: a root processor resamples global hyperparameters\nThe sampler is approximate because during Gibbs sampling, all processors resample elements of Z\nat the same time; their posteriors Pp(A|X,Z) are no longer the true P(A|X,Z).\nMessage Passing\nWe use Bayes rule to factorise the posterior over features P(A|Z,X):\nP(A|Z,X) \u221d P(A)\n?\np\nP(Xp|Zp,A)\n(4)\nIf the prior P(A) and the likelihoods P(Xp|Zp,A) are conjugate exponential family models, then\nthe sufficient statistics of P(A|Z,X) are the sum of the sufficient statistics of each term on the right\nside of equation (4). For example, the sufficient statistics in the linear-Gaussian model are means\nand covariances; in the Bernoulli model, they are counts of how often each element Akdequals one.\nThe linear-Gaussian messages have size O(K2+KD), and the Bernoulli messages O(KD), where\nK is the number of features. For nonparametric models such as the IBP, the number of features K\ngrows as O(logN). This slow growth means that messages remain small, even for large datasets.\nThe most straightforward way to compute the full posterior is to arrange processors in a tree archi-\ntecture, as belief propagation is then exact. The message s from processor p to processor q is:\nsp\u2192q= lp+\n?\nr\u2208N(p)\\q\nsr\u2192p\n3"},{"page":4,"text":"where N(p)\\q are the processors attached to p besides q and lpare the sufficient statistics from\nprocessor p. A dummy neighbour containing the statistics of the prior is connected to (an arbitrarily\ndesignated) root processor. Also passed are the feature counts mp\nfeature k within processor p. (See figure 1(b) for a cartoon.)\nk=?\nn\u2208XpZp\nnk, the popularity of\nGibbs Sampling\nIn general, Znkcan be Gibbs-sampled using Bayes rule\nP(Znk|Z\u2212nk,X) \u221d P(Znk|Z\u2212nk)P(X|Z).\nThe probability P(Znk|Z\u2212nk) depends on the size of the dataset N and the number of observations\nmkusing feature k. At the beginning of the Gibbs sampling stage, each processor has the correct\nvalues of mk. We compute m\u2212p\nk\n= mk\u2212mp\nupdated, approximate mk\u2248 m\u2212p\nk\n+ mp\ncurrent stage (good for popular features).\nk, and, as the processor\u2019s internal feature counts mp\nk. This approximation assumes m\u2212p\nkare\nk\nstays fixed during the\nThe collapsed likelihood P(X|Z) integrating out the feature values A is given by:\nP(X|Z) \u221d\n?\nA\nP(Xn|Zn,A)P(A|Z\u2212n,X\u2212n)dA,\nwhere the partial posterior P(A|Z\u2212n,X\u2212n) \u221d\ncan be efficiently computed by subtracting observation n\u2019s contribution to the sufficient statistics.1\nFor non-conjugate models, we can use an exponential family distribution Q(A) to approximate\nP(A|X,Z) during message passing. A draw A \u223c Q\u2212p(A) is then used to initialise an uncollapsed\nGibbssampler. TheoutputtedsamplesofAareusedtocomputesufficientstatisticsforthelikelihood\nP(X|Z). In both cases, new features are added as described in [3].\nP(A|Z,X)\nP(Xn|Zn,A). In conjugate models, P(A|Z\u2212n,X\u2212n)\nHyperparameter Resampling\nlikelihood can also be sampled during inference. Resampling \u03b1 depends only on the total number of\nactive features; thus it can easily be resampled at the root and propagated to the other processors. In\nthe linear-Gaussian model, the posteriors on the noise and feature variances (starting from gamma\npriors) depend on various squared-errors, which can also be computed in a distributed fashion.\nThe IBP concentration parameter \u03b1 and hyperparameters of the\nFor more general, non-conjugate models, resampling the hyperparameters requires two steps. In\nthe first step, a hyperparameter value is proposed by the root and propagated to the processors.\nThe processors each compute the likelihood of the current and proposed hyperparameter values and\npropagate this value back to root. The root evaluates a Metropolis step for the hyperparameters\nand propagates the decision back to the leaves. The two-step approach introduces a latency in the\nresampling but does not require any additional message passing rounds.\nAsynchronous Operation\nperparameter resampling as if they occur in separate phases. In practice, these phases may occur\nasynchronously: between its Gibbs sweeps, each processor updates its feature posterior based on\nthe most current messages it has received and sends likelihood messages to its parent. Likewise,\nthe root continuously resamples hyperparameters and propagates the values down through the tree.\nWhile another layer of approximation, this asynchronous form of message passing allows faster pro-\ncessors to share information and perform more inference on their data instead of waiting for slower\nprocessors.\nSo far we have discussed message passing, Gibbs sampling, and hy-\nImplementation Note\nconsidered with care. Other parallel inference for nonparametric models, such as the HDP [2],\nsimply matched features by their index, that is, assumed that the ithfeature on processor p was also\nthe ithfeature on processor q. In the IBP, we find that this indiscriminate feature merging is often\ndisastrous when adding or deleting features: if none of the observations in a particular processor are\nusing a feature, we cannot simply delete that column of Z and shift the other features over\u2014doing\nso destroys the alignment of features across processors.\nWhen performing parallel inference in the IBP, a few factors need to be\n1In the IBP, only the linear-Gaussian model exhibits this conjugate structure. However, many other matrix\nfactorization models (such as PCA) often have this conjugate form.\n4"},{"page":5,"text":"4 Comparison to Exact Metropolis\nBecause all Zp\u2019s are sampled at once, the posteriors Pp(A|X,Z) used by each processor in section 3\nare no longer exact. Below we show how Metropolis\u2013Hastings (MH) steps can make the parallel\nsampler exact, but introduce significant computational overheads both in computing the transition\nprobabilities and in the message passing. We argue that trying to do exact inference is a poor\nuse of computational resources (especially as any finite chain will not be exact); empirically, the\napproximate sampler behaves similarly to the MH sampler while finding higher likelihood regions\nin the data.\nExact Parallel Metropolis Sampler.\neach stage of the approximate inference to make the sampler exact. Unfortunately, the approximate\nsampler makes several non-independent random choices in each stage of the inference, making the\nreverse proposal inconvenient to compute. We circumvent this issue by fixing the random seed, mak-\ning the initial stage of the approximate sampler a deterministic function, and then add independent\nrandom noise to create a proposal distribution. This approach makes both the forward and reverse\ntransition probabilities simple to compute.\nIdeally, we would simply add an MH accept\/reject step after\nFormally, let\u02c6 Zpbe the matrix output after a set of Gibbs sweeps on Zp. We use all the\u02c6 Zp\u2019s to\npropose a new Z\u2032matrix. The acceptance probability of the proposal is\nmin(1,P(X|Z\u2032)P(Z\u2032)Q(Z\u2032\u2192 Z)\nP(X|Z)P(Z)Q(Z \u2192 Z\u2032)),\n(5)\nwhere the likelihood terms P(X|Z) and P(Z) are readily computed in a distributed fashion. For\nthe transition distribution Q, we note that if we set the random seed r, then the matrix\u02c6 Zpfrom the\nGibbs sweeps in the processor is some deterministic function of the input matrix Zp. The proposal\nZp\u2032is a (stochastic) noisy representation of\u02c6 Zpin which for example\nP(Zp\u2032\nnk= 1) = .99 if\n\u02c6Zp\nnk= 1,P(Zp\u2032\nnk= 1) = .01 if\n\u02c6Zp\nnk= 0\n(6)\nwhere K should be at least the number of features in\u02c6 Zp. We set Zp\u2032\nin figure 2.)\nnk= 0 for k > K. (See cartoon\nTo compute the backward probability, we take Zp\u2032and apply the same number of Gibbs sampling\nsweeps with the same random seed r. The resulting\u02c6Zp\u2032is a deterministic function of Zp\u2032. The\nbackward probability Q(Zp\u2032\u2192 Zp) which is the probability of going from Zp\u2032to Zpusing 6.\nWhile the transition probabilities can be computed in a distributed, asynchronous fashion, all of the\nprocessors must synchronise when deciding whether to accept the proposal.\nExperimental Comparison\nniques, we ran each inference type on 1000 block images of [3] on 5 simulated processors. Each\ntest was repeated 25 times. For each of the 25 tests, we create a held out dataset by setting elements\nof the last 100 images as missing values. For the first 50 test images, we set all even numbered\ndimensions as the missing elements, and every odd numbered dimension as the missing values for\nthe last 50 images. Each sampler was run for 10,000 iterations with 5 Gibbs sweeps per iteration;\nstatistics were collected from the second half of the chain. To keep the probability of an acceptance\nreasonable, we allowed each processor to change only small parts of its Zp: the feature assignments\nZnfor 1, 5, or 10 data points each during each sweep.\nIn table 1, we see that the approximate sampler runs about five times faster than the exact samplers\nwhile achieving comparable (or better) predictive likelihoods and reconstruction errors on held-\nout data. Both the acceptance rates and the predictive likelihoods fall as the exact sampler tries\nto take larger steps, suggesting that the difference between the approximate and exact sampler\u2019s\nperformance on predictive likelihood is due to poor mixing by the exact sampler. Figure 4 shows\nempirical CDFs for the number of features k , IBP concentration parameter \u03b1, the noise variance \u03c32\nand the feature variance \u03c32\nexact Metropolis samplers (gray) for the variances; the concentration parameter is smaller, but the\nfeature counts are similar to the single-processor case.\nTo compare the exact Metropolis and approximate inference tech-\nn,\na. The approximate sampler (black) produces similar CDFs to the various\n5"},{"page":6,"text":"Zp Zp\nGibbs with\nfixed seed\nZp\u2019\nRandom\nnoise\nFigure 2: Cartoon of MH\nproposal\nMethodTime (s)Test L2\nError\n0.0468\n0.0488\n0.0555\n0.0487\nTest\nLikelihood\n0.1098\n0.0893\n0.0196\n0.1292\nLogMH Accept\nProportion\n0.1106\n0.0121\n0.0062\n-\nMH, n = 1\nMH, n = 5\nMH, n = 10\nApproximate\n717\n1075\n1486\n179\nTable 1: Evaluation of exact and approximate methods.\n56789\nFeature Count\n101112131415\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nEmpirical CDF for IBP Concentration\nCumilative Probability\n \n \nSingle Processor\nApproximate Sampling\nExact Sampling, various windows\n(a) Active feature count k\n0.511.522.53 3.5\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nEmpirical CDF for IBP Concentration\nIBP Concentration Parameter\nCumilative Probability\n \n \nSingle Processor\nApproximate Sampling\nExact Sampling, various windows\n(b) IBP Concentration \u03b1\n0.10.150.20.250.3\nNoise Variance\n0.350.40.450.50.550.6\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nEmpirical CDF for Noise Variance\nCumilative Probability\n \n \nSingle Processor\nApproximate Sampling\nExact Sampling, various windows\n(c) Noise variance \u03c32\nx\n0.20.250.30.350.40.450.5\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nEmpirical CDF for Feature Variance\nFeature Variance\nCumilative Probability\n \n \nSingle Processor\nApproximate Sampling\nExact Sampling, various windows\n(d) Feature variance \u03c32\na\nFigure 3: Empirical CDFs: The solid black line is the approximate sampler; the three solid gray lines\nare the MH samplers with n equal to 1, 5, and 10 (lighter shades indicate larger n. The approximate\nsampler and the MH samplers for smaller n have similar CDFs; the n = 10 MH sampler\u2019s differing\nCDF indicates it did not mix in 7500 iterations (reasonable since its acceptance rate was 0.0062).\n5Analysis of Mixing Properties\nWe ran a series of experiments on 10,000 36-dimensional block images of [3] to study the effects\nof various sampler configurations on running time, performance, and mixing time properties of\nthe sampler. 5000 elements of the data matrix were held-out as test data. Figure 4 shows test\nlog-likelihoods using 1, 7, 31 and 127 parallel processors simulated in software, using 1000 outer\niterations with 5 Gibbs inner iterations each. The parallel samplers have similar test likelihoods as\nthe serial algorithm with significant savings in running time. The characteristic shape of the test\nlikelihood, similar across all testing regimes, indicates how the features are learned. Initially, a large\nnumber of features are added, which provides improvements in the test likelihood. A refinement\nphase, in which excess features are pruned, provides further improvements.\nFigure 4 shows hairiness-index plots for each of the test cases after thinning and burn-in. The hairi-\nness index, based on the method of CUSUM for monitoring MCMC convergence [9, 10], monitors\nhow often the derivatives of sampler statistics\u2014in our case, the number of features, the test likeli-\nhood, and \u03b1\u2014change in sign; infrequent changes in sign indicate that the sampler may not be mixed.\nThe outer bounds on the plots are the 95% confidence bounds. The index stays within the bounds\nsuggesting that the chains are mixing.\nFinally, we considered the trade-off between mixing and running time as the number of outer it-\nerations and inner Gibbs iterations are varied. Each combination of inner and outer iterations was\nset so that the total number of Gibbs sweeps through the data was 5000. Mixing efficiency was\n\u22121012345\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\nTest loglikelihood\nTime (s)\nTest Loglikelihood for inner = 5 and outer = 1000 iterations\n \n \n20 40 60 80100\nProcessors = 31\n0\n0.5\n1\nProcessors = 1\nHairiness Index\n20 40 60 80 100\nProcessors = 127\n0\n0.5\n1\nProcessors = 7\nHairiness Index\n20 40 60 80100\n0\n0.5\n1\nHairiness Index\n20 40 60 80100\n0\n0.5\n1\nHairiness Index\nProc = 1\nProc = 7\nProc = 31\nProc = 127\nFigure 4: Change in likelihood for various numbers of processors over the simulation time. The\ncorresponding hairiness index plots are shown on the left.\n6"},{"page":7,"text":"01020\n#Inner Iterations\n304050\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n# Effective Samples per Outer Iter.\n \n \nProc = 1\nProc = 7\nProc = 31\nProc = 127\n1731127\n10\n1\n10\n2\n10\n3\n10\n4\n10\n5\n# Processors\nTotal Time (s)\n \n \ni = 50, o = 100\ni = 20, o = 250\ni = 10, o = 500\ni = 5, o = 1000\ni = 1, o = 5000\nFigure 5: Effects of changing the number of inner iterations on: (a) The effective sample size (b)\nTotal running time (Gibbs and Message passing).\nTable 2: Test log-likelihoods on real-world datasets for the serial, synchronous and asynchronous\ninference types.\nDatasetND Description Serial\np = 1\n-4.74\nSynch\np = 16\n-4.77\nAsync\np = 16\n-4.84AR Faces [11]26001598faces with lighting, acces-\nsories (real-valued)\nSTDFT of a piano recording\n(real-valued)\nindicatorsof\n(binary-valued)\nPiano [12]57931161-1.435-1.182-1.228\nFlickr [13]1000001000imagetags\u2014-0.0584\nmeasured via the effective number of samples per sample [10], which evaluates what fraction of\nthe samples are independent (ideally, we would want all samples to be independent, but MCMC\nproduces dependent chains). Running time for Gibbs sampling was taken to be the time required by\nthe slowest processor (since all processors must synchronize before message passing); the total time\nreflected the Gibbs time and the message-passing time. As seen in figure 5, completing fewer inner\nGibbs iterations per outer iteration results in faster mixing, which is sensible as the processors are\ncommunicating about their data more often. However, having fewer inner iterations requires more\nfrequent message passing; as the number of processors becomes large, the cost of message passing\nbecomes a limiting factor.2\n6Real-world Experiments\nWe tested our parallel scheme on three real world datasets on a 16 node cluster using the Matlab\nDistributed Computing Engine, using 3 inner Gibbs iterations per outer iteration. The first dataset\nwas a set of 2,600 frontal face images with 1,598 dimensions [11]. While not extremely large,\nthe high-dimensionality of the dataset makes it challenging for other inference approaches. The\npiano dataset [12] consisted of 57,931 samples from a 161-dimensional short-time discrete Fourier\ntransform of a piano piece. Finally, the binary-valued Flickr dataset [13] indicated whether each\nof 1000 popular keywords occurred in the tags of 100,000 images from Flickr. Performance was\nmeasured using test likelihoods and running time. Test likelihoods look only at held-out data and\nthus they allow us to \u2018honestly\u2019 evaluate the model\u2019s fit. Table 2 summarises the data and shows that\nall approaches had similar test-likelihood performance.\nIn the faces and music datasets, the Gibbs time per iteration improved almost linearly as the number\nof processors increased (figure 6). For example, we observed a 14x-speedup for p = 16 in the music\ndataset. Meanwhile, the message passing time remained small even with 16 processors\u20147% of the\nGibbs time for the faces data and 0.1% of the Gibbs time for the music data. However, waiting for\nsynchronisation became a significant factor in the synchronous sampler. Figure 6(c) compares the\ntimes for running inference serially, synchronously and asynchronously with 16 processors. The\n2Webelievepartofthetimingresultsmaybeanartifact, asthesimulationoverestimatesthemessagepassing\ntime. In the actual parallel system (section 6), the cost of message passing was negligible.\n7"},{"page":8,"text":"1248 16\n0\n20\n40\n60\n80\n100\n120\nnumber of processors\nmean time per outer iteration\/s\n \n \nsampling\nwaiting\n(a) Timing analysis for faces\ndataset\n124816\n0\n200\n400\n600\n800\n1000\n1200\nnumber of processors\nmean time per iteration\/s\n \n \nsampling\nwaiting\n(b) Timing analysis for music\ndataset\n10\n\u22122\n10\n0\n10\n2\n10\n4\n\u22122.8\n\u22122.6\n\u22122.4\n\u22122.2\n\u22122\n\u22121.8x 10\n7\ntime\/s\nlog joint\n \n \nserial P=1\nsynchronous P=16\nasynchronous P=16\n(c) Timingcomparisonfordifferent\napproaches\nFigure 6: Bar charts comparing sampling time and waiting times for synchronous parallel inference.\nasynchronous inference is 1.64 times faster than the synchronous case, reducing the computational\ntime from 11.8s per iteration to 7.2s.\n7 Discussion and Conclusion\nAs datasets grow, parallelisation is an increasingly attractive and important feature for doing infer-\nence. Not only does it allow multiple processors\/multicore technologies to be leveraged for large-\nscale analyses, but it also reduces the amount of data and associated structures that each processor\nneeds to keep in memory. Existing work has focused both on general techniques to efficiently split\nvariables across processors in undirected graphical models [14] and factor graphs [15] and specific\nmodels such as LDA [16, 17]. Our work falls in between: we leverage properties of a specific kind\nof parallelisation\u2014data parallelisation\u2014for a fairly broad class of models.\nSpecifically, we describe a parallel inference procedure that allows nonparametric Bayesian models\nbased on the Indian Buffet Process to be applied to large datasets. The IBP poses specific challenges\nto data parallelisation in that the dimensionality of the representation changes during inference and\nmay be unbounded. Our contribution is an algorithm for data-parallelisation that leverages a com-\npact representation of the feature posterior that approximately decorrelates the data stored on each\nprocessor, thus limiting the communication bandwidth between processors. While we focused on\nthe IBP, the ideas presented here are applicable to a more general problems in unsupervised learning\nincluding bilinear models such as PCA, NMF, and ICA.\nOur sampler is approximate, and we show that in conjugate models, it behaves similarly to an ex-\nact sampler\u2014but with much less computational overhead. However, as seen in the Bernoulli case,\nvariational message passing for non-conjugate data doesn\u2019t always produce good results if the ap-\nproximating distribution is a poor match for the true feature posterior. Determining when variational\nmessage passing is successful is an interesting question for future work. Other interesting directions\ninclude approaches for dynamically optimising the network topology (for example, slower proces-\nsors could be moved lower in the tree). Finally, we note that a middle ground between synchronous\nand asynchronous operations as we presented them might be a system that gives each processor a\ncertain amount of time, instead of a certain number of iterations, to do Gibbs sweeps. Further study\nalong these avenues should lead to even more efficient data-parallel Bayesian inference techniques.\n8"},{"page":9,"text":"References\n[1] C. Chu, S. Kim, Y. Lin, Y. Yu, G. Bradski, A. Ng, and K. Olukotun, \u201cMap-reduce for machine\nlearning on multicore,\u201d in Advances in Neural Information Processing Systems, p. 281, MIT\nPress, 2007.\n[2] A. Asuncion, P. Smyth, and M. Welling, \u201cAsynchronous distributed learning of topic models,\u201d\nin Advances in Neural Information Processing Systems 21, 2008.\n[3] T. Griffiths and Z. Ghahramani, \u201cInfinite latent feature models and the Indian buffet process,\u201d\nin Advances in Neural Information Processing Systems, vol. 16, NIPS, 2006.\n[4] F. Doshi-Velez and Z. Ghahramani, \u201cAccelerated inference for the Indian buffet process,\u201d in\nInternational Conference on Machine Learning, 2009.\n[5] E. Meeds, Z. Ghahramani, R. Neal, and S. Roweis, \u201cModeling dyadic data with binary latent\nfactors,\u201d in Advances in Neural Information Processing Systems, vol. 19, pp. 977\u2013984, 2007.\n[6] Y. W. Teh, D. G\u00a8 or\u00a8 ur, and Z. Ghahramani, \u201cStick-breaking construction for the Indian buffet\nprocess,\u201d in Proceedings of the Intl. Conf. on Artificial Intelligence and Statistics, vol. 11,\npp. 556\u2013563, 2007.\n[7] F. Wood and T. L. Griffiths, \u201cParticle filtering for nonparametric Bayesian matrix factoriza-\ntion,\u201d in Advances in Neural Information Processing Systems, vol. 19, pp. 1513\u20131520, 2007.\n[8] F. Doshi-Velez, K. T. Miller, J. Van Gael, and Y. W. Teh, \u201cVariational inference for the In-\ndian buffet process,\u201d in Proceedings of the Intl. Conf. on Artificial Intelligence and Statistics,\nvol. 12, pp. 137\u2013144, 2009.\n[9] S. P. Brooks and G. O. Roberts, \u201cConvergence assessment techniques for Markov Chain Monte\nCarlo,\u201d Statistics and Computing, vol. 8, pp. 319\u2013335, 1998.\n[10] C. R. Robert and G. Casella, Monte Carlo Statistical Methods. Springer, second ed., 2004.\n[11] A. M. Mart\u2019inez and A. C. Kak, \u201cPCA versus LDA,\u201d IEEE Trans. Pattern Anal. Mach. Intelli-\ngence, vol. 23, pp. 228\u2013233, 2001.\n[12] G. E. Poliner and D. P. W. Ellis, \u201cA discriminative model for polyphonic piano transcription,\u201d\nEURASIP J. Appl. Signal Process., vol. 2007, no. 1, pp. 154\u2013154, 2007.\n[13] T. Kollar and N. Roy, \u201cUtilizing object-object and object-scene context when planning to find\nthings.,\u201d in International Conference on Robotics and Automation, 2009.\n[14] C. G. Joseph Gonzalez, Yucheng Low, \u201cResidual splash for optimally parallelizing belief prop-\nagation,\u201d in Proceedings of the Twelfth International Conference on Artificial Intelligence and\nStatistics (D. van Dyk and M. Welling, eds.), vol. 5, pp. 177\u2013184, JMLR, 2009.\n[15] D. Stern, R. Herbrich, and T. Graepel, \u201cMatchbox: Large scale online Bayesian recommenda-\ntions,\u201d in 18th International World Wide Web Conference (WWW2009), April 2009.\n[16] R. Nallapati, W. Cohen, and J. Lafferty, \u201cParallelized variational EM for Latent Dirichlet Al-\nlocation: An experimental evaluation of speed and scalability,\u201d in ICDMW \u201907: Proceedings\nof the Seventh IEEE International Conference on Data Mining Workshops, (Washington, DC,\nUSA), pp. 349\u2013354, IEEE Computer Society, 2007.\n[17] D. Newman, A. Asuncion, P. Smyth, and M. Welling, \u201cDistributed inference for Latent Dirich-\nlet Allocation,\u201d in Advances in Neural Information Processing Systems 20 (J. Platt, D. Koller,\nY. Singer, and S. Roweis, eds.), pp. 1081\u20131088, Cambridge, MA: MIT Press, 2008.\n9"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Finale_Doshi_Velez\/publication\/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process\/links\/0a85e5388a8f2cb1b4000000.pdf","widgetId":"rgw30_56ab9f64d00da"},"id":"rgw30_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=221620392&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw31_56ab9f64d00da"},"id":"rgw31_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=221620392&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":221620392,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"0a85e5388a8f2cb1b4000000","name":"Finale Doshi velez","date":"May 30, 2014 ","nameLink":"profile\/Finale_Doshi_Velez","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Finale_Doshi_Velez\/publication\/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process\/links\/0a85e5388a8f2cb1b4000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Finale_Doshi_Velez\/publication\/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process\/links\/0a85e5388a8f2cb1b4000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"fa4422396e4064f7e6cf284d7ca404ce","showFileSizeNote":false,"fileSize":"309.63 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"0a85e5388a8f2cb1b4000000","name":"Finale Doshi velez","date":"May 30, 2014 ","nameLink":"profile\/Finale_Doshi_Velez","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Finale_Doshi_Velez\/publication\/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process\/links\/0a85e5388a8f2cb1b4000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Finale_Doshi_Velez\/publication\/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process\/links\/0a85e5388a8f2cb1b4000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"fa4422396e4064f7e6cf284d7ca404ce","showFileSizeNote":false,"fileSize":"309.63 KB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Conference Paper","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=YY1dijgJO6ANlwvjWrW1nhJJY9uN6LIAtmygKzEjTqWFZjCYFg_BTmsLYUoXfd3_VOt61t8bhDv2ys3OkrkdbQ.t8-dGLlS0XaT5g6odkXgJj0i8xl6jyoEWJWhzaLNWRb64xFRM9vuUR4dOKUyJMAeh-t5O6Hzr8z3Peu1JaYN_Q","clickOnPill":"publication.PublicationFigures.html?_sg=VVRPl_qilSk9VC66n2lY0m_jJq7fMowIZGXqQMWYHYPthCPoXekubsUKTGKbs0IoUZf5ZwNj4kS33R4KG5TaBA.FUtGQr3pZ-lwyMdP41LH1dGPTcW_knZFNTSUJR-czMO3s2beOc9EphLr3qhUD4ohNJ8FcdOCRPy0RJlbxdKY9w"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FFinale_Doshi_Velez%2Fpublication%2F221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process%2Flinks%2F0a85e5388a8f2cb1b4000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=09H4rXBqT-UyRqqQOBKFVF93i2uEniIDV8DuNoOcqHbAsx7uiaNOtk_3kgPnECPNBGfjrkQwpngp_sBw-a7iAQ","urlHash":"cc431bb624293f6fe4889555ce7f53ad","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=XNMnCBxXagSdGDIng0sY2SIzpxAaUihOtWgoXUgv3ix-U4lWLUq53cDVjD-MdpoqkerO5RHjwDt5WOMjfmf82TBl9IKTi8sZDJp9t9KKuLg.sa-61tro3Cx3cdyRP5XlrbOaPuw0ZsRp1cFEIgY3ZV6gz4UBqDxiL_JzDEYLBQyzKZPKbhhJbcBli7Lh4Kh8vg.xx4iGcBO8LcxmE0t-jX60036sofF4NnwwvBZeaA4BFe7sLGzuGP7oi_YA31SvtrO5ZPbQhtIyCX9ydf4SwlRIw","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"0a85e5388a8f2cb1b4000000","trackedDownloads":{"0a85e5388a8f2cb1b4000000":{"v":false,"d":false}},"assetId":"AS:106662723588101@1402441784229","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":221620392,"commentCursorPromo":null,"widgetId":"rgw33_56ab9f64d00da"},"id":"rgw33_56ab9f64d00da","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FFinale_Doshi_Velez%2Fpublication%2F221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process%2Flinks%2F0a85e5388a8f2cb1b4000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A106662723588101%401402441784229&publicationUid=221620392&linkId=0a85e5388a8f2cb1b4000000&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process","publicationType":"Conference Paper","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=4DuvWbmtl9Z3FIU_GNq7h8HNpM70y4r13se85YzEKzMBTRXn0mla9o81EcyfVnZlj1Mvc33EiKHxhGTVqH57AxJfR_Z5-AZyFICpmbsmPjU.WasB0WRB-YKzw2HM2_z3uZUx-cNQebs7Y5P00ilt752Wb6VZMCBRuIgY87ZXztMn8d158SaHIkvtRK2d7r8Skw.jVvZ0nmeIiDkdIcN_HKGvf4jAJxuOQI4e6xFHryG_7il4wp_Q0KQM0Y7bd_4XSJlnFCRDdwepGW0v2Nfm5taow","publicationUid":221620392,"trackedDownloads":{"0a85e5388a8f2cb1b4000000":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw35_56ab9f64d00da"},"id":"rgw35_56ab9f64d00da","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw36_56ab9f64d00da"},"id":"rgw36_56ab9f64d00da","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw37_56ab9f64d00da"},"id":"rgw37_56ab9f64d00da","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw38_56ab9f64d00da"},"id":"rgw38_56ab9f64d00da","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw39_56ab9f64d00da"},"id":"rgw39_56ab9f64d00da","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw34_56ab9f64d00da"},"id":"rgw34_56ab9f64d00da","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw32_56ab9f64d00da"},"id":"rgw32_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab9f64d00da"},"id":"rgw2_56ab9f64d00da","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":221620392},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=221620392&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab9f64d00da"},"id":"rgw1_56ab9f64d00da","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"IfP9Fyve8xIepSBlc4YXRYvgSrcvmvtXvhN5kZoItAl1X6flUa0NqSX2AWaJpbqGkDHEQcMsI40gf6v82fCykqX18oTCwueEsv44bDrljXwUpp8ha2\/etPyoeo1sX5I9YXwDlyx4boQNefTkSrCjvFFj9R5n0+hEJUSZShIFQSfqhTv7N3ifGSgldd4fGj+HGMtYEx\/9SArqnlmtf\/Nr6dKiIZNx\/34cNDFHQWRlGU3evBp1wqiMOl2bdnCbKwKredBkRX337ckDHF7nzN2HfbHz7AKPyY6bzXIASTKhGBE=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process\" \/>\n<meta property=\"og:description\" content=\"Nonparametric Bayesian models provide a framework for flexi ble probabilistic modelling of complex datasets. Unfortunately, the high-dimensional averages re- quired for Bayesian methods can be...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process\/links\/0a85e5388a8f2cb1b4000000\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process\" \/>\n<meta property=\"rg:id\" content=\"PB:221620392\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process\" \/>\n<meta name=\"citation_author\" content=\"Finale Doshi-Velez\" \/>\n<meta name=\"citation_author\" content=\"David Knowles\" \/>\n<meta name=\"citation_author\" content=\"Shakir Mohamed\" \/>\n<meta name=\"citation_author\" content=\"Zoubin Ghahramani\" \/>\n<meta name=\"citation_conference_title\" content=\"Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009. Proceedings of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada.\" \/>\n<meta name=\"citation_publication_date\" content=\"2009\/12\/01\" \/>\n<meta name=\"citation_firstpage\" content=\"1294\" \/>\n<meta name=\"citation_lastpage\" content=\"1302\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Finale_Doshi_Velez\/publication\/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process\/links\/0a85e5388a8f2cb1b4000000.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Conference Paper');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-bda1041a-584d-416f-8c71-b93131a0a98e","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":317,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw40_56ab9f64d00da"},"id":"rgw40_56ab9f64d00da","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-bda1041a-584d-416f-8c71-b93131a0a98e", "d71aab3f63d146461c551aff0e1df1b9e11aefd1");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-bda1041a-584d-416f-8c71-b93131a0a98e", "d71aab3f63d146461c551aff0e1df1b9e11aefd1");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw41_56ab9f64d00da"},"id":"rgw41_56ab9f64d00da","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process","requestToken":"5Qx46VhX9Vo\/yXpKt1yjl\/YwEN7phP2ey80Ez3ZFvPvtJvG3PAmo2DFtwl1xripeJDEVyni5ZygzW6vD32Pv8LGaZIYceE\/Lq+mUa83x+wwMwKj7d4Ca2b6+BJww7rygL+Wd2ba5190Y2E8NHtbyEJtA4z9flRhNwRjbZXGWC36k87k795q+xaMQwEqnT8zAjgleCV2eyWGK6r4\/W226xjVHOXn2mNerJGgSangjV1+C2pNmbWujj3brDup23vv3T0dr2c46MqkQRYY7KAK1J5wAkGnxV6svoO6PF0pFs48=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=AAARaAngVyn10itWBlxSNsW_6QoRekonxbR9BSOPDOff2c8tt6Zs1ab-66s2cISf","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjIxNjIwMzkyX0xhcmdlX1NjYWxlX05vbnBhcmFtZXRyaWNfQmF5ZXNpYW5fSW5mZXJlbmNlX0RhdGFfUGFyYWxsZWxpc2F0aW9uX2luX3RoZV9JbmRpYW5fQnVmZmV0X1Byb2Nlc3M%3D","signupCallToAction":"Join for free","widgetId":"rgw43_56ab9f64d00da"},"id":"rgw43_56ab9f64d00da","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw42_56ab9f64d00da"},"id":"rgw42_56ab9f64d00da","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw44_56ab9f64d00da"},"id":"rgw44_56ab9f64d00da","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Conference Paper","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
