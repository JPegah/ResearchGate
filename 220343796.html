<!DOCTYPE html> <html lang="en" class="" id="rgw41_56ab9ebb431f0"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="wtIQruKs3DMqzg7QxRa7w69jyE6VHjdLncojctwdtBOMa73uAs063GcAK9h3ssHew/5UBaUEqcLBfECWSGHw5aNyvR8itWlDKUUHiv8lKVfGbBM5QLlJArAulj6/2uBcWcO3ylufWtVbuz9tflRfhhbIf/0fyFgxpoN8edI/8F7oJqEjvSg8zw0E1PW+q60dajzm0lpIuYeqJs8i7seeoSnRD3zGqbhWLG/e/+2FQFS6vJF60njLcf0zINKkK4sHMIRJ7zsQtAEYTso1Pyl/LR7l+ycBwCe4jGe7X7G7o58="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-6584da8a-e5fd-4511-b1d1-f10127da45bb",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Finite-time Analysis of the Multiarmed Bandit Problem" />
<meta property="og:description" content="Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem/links/09e4150ec228dace78000000/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem" />
<meta property="rg:id" content="PB:220343796" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1023/A:1013689704352" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Finite-time Analysis of the Multiarmed Bandit Problem" />
<meta name="citation_author" content="Peter Auer" />
<meta name="citation_author" content="Nicolò Cesa-Bianchi" />
<meta name="citation_author" content="Paul Fischer" />
<meta name="citation_publication_date" content="2002/05/01" />
<meta name="citation_journal_title" content="Machine Learning" />
<meta name="citation_issn" content="0885-6125" />
<meta name="citation_volume" content="47" />
<meta name="citation_issue" content="2-3" />
<meta name="citation_firstpage" content="235" />
<meta name="citation_lastpage" content="256" />
<meta name="citation_doi" content="10.1023/A:1013689704352" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Nicolo_Cesa-Bianchi/publication/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem/links/09e4150ec228dace78000000.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/215868066921738/styles/pow/publicliterature/FigureList.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Finite-time Analysis of the Multiarmed Bandit Problem (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Finite-time Analysis of the Multiarmed Bandit Problem on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab9ebb431f0" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab9ebb431f0" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab9ebb431f0">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1023%2FA%3A1013689704352&rft.atitle=Finite-time%20Analysis%20of%20the%20Multiarmed%20Bandit%20Problem&rft.title=Machine%20Learning&rft.jtitle=Machine%20Learning&rft.volume=47&rft.issue=2-3&rft.date=2002&rft.pages=235-256&rft.issn=0885-6125&rft.au=Peter%20Auer%2CNicol%C3%B2%20Cesa-Bianchi%2CPaul%20Fischer&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Finite-time Analysis of the Multiarmed Bandit Problem</h1> <meta itemprop="headline" content="Finite-time Analysis of the Multiarmed Bandit Problem">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem/links/09e4150ec228dace78000000/smallpreview.png">  <div id="rgw8_56ab9ebb431f0" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56ab9ebb431f0" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Peter_Auer" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Peter Auer" alt="Peter Auer" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Peter Auer</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw10_56ab9ebb431f0" data-account-key="Peter_Auer">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Peter_Auer"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Peter Auer" alt="Peter Auer" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Peter_Auer" class="display-name">Peter Auer</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Montanuniversitaet_Leoben" title="Montanuniversität Leoben">Montanuniversität Leoben</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56ab9ebb431f0" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Nicolo_Cesa-Bianchi" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A282101564887045%401444269662768_m" title="Nicolò Cesa-Bianchi" alt="Nicolò Cesa-Bianchi" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Nicolò Cesa-Bianchi</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw12_56ab9ebb431f0" data-account-key="Nicolo_Cesa-Bianchi">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Nicolo_Cesa-Bianchi"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A282101564887045%401444269662768_l" title="Nicolò Cesa-Bianchi" alt="Nicolò Cesa-Bianchi" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Nicolo_Cesa-Bianchi" class="display-name">Nicolò Cesa-Bianchi</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_of_Milan" title="University of Milan">University of Milan</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw13_56ab9ebb431f0"> <a href="researcher/6525015_Paul_Fischer" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Paul Fischer" alt="Paul Fischer" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Paul Fischer</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw14_56ab9ebb431f0">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/6525015_Paul_Fischer"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Paul Fischer" alt="Paul Fischer" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/6525015_Paul_Fischer" class="display-name">Paul Fischer</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">  <div> DTI, University of Milan, I-26013, Crema, Italy </div>      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/0885-6125_Machine_Learning"><span itemprop="name">Machine Learning</span></a> </span>    (Impact Factor: 1.89).     <meta itemprop="datePublished" content="2002-05">  05/2002;  47(2-3):235-256.    DOI:&nbsp;10.1023/A:1013689704352           <div class="pub-source"> Source: <a href="http://dblp.uni-trier.de/db/journals/ml/ml47.html#AuerCF02" rel="nofollow">DBLP</a> </div>  </div> <div id="rgw15_56ab9ebb431f0" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.</div> </p>  </div>  </div>   </div>     <div id="rgw16_56ab9ebb431f0" class="figure-carousel"> <div class="carousel-hd"> Figures in this publication </div> <div class="carousel-bd"> <ul class="clearfix">  <li> <a href="/figure/220343796_fig1_Figure-1" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 1 . " data-key="220343796_fig1_Figure-1"> <img class="fig" src="https://www.researchgate.net/profile/Nicolo_Cesa-Bianchi/publication/220343796/figure/fig1/Figure-1_small.png" alt="Figure 1 . " title="Figure 1 . "/> </a> </li>  <li> <a href="/figure/220343796_fig2_Figure-2" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 2 . " data-key="220343796_fig2_Figure-2"> <img class="fig" src="https://www.researchgate.net/profile/Nicolo_Cesa-Bianchi/publication/220343796/figure/fig2/Figure-2_small.png" alt="Figure 2 . " title="Figure 2 . "/> </a> </li>  <li> <a href="/figure/220343796_fig3_Figure-3" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 3 . " data-key="220343796_fig3_Figure-3"> <img class="fig" src="https://www.researchgate.net/profile/Nicolo_Cesa-Bianchi/publication/220343796/figure/fig3/Figure-3_small.png" alt="Figure 3 . " title="Figure 3 . "/> </a> </li>  <li> <a href="/figure/220343796_fig4_Figure-4" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 4 . " data-key="220343796_fig4_Figure-4"> <img class="fig" src="https://www.researchgate.net/profile/Nicolo_Cesa-Bianchi/publication/220343796/figure/fig4/Figure-4_small.png" alt="Figure 4 . " title="Figure 4 . "/> </a> </li>  <li> <a href="/figure/220343796_fig5_Figure-5" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 5 . " data-key="220343796_fig5_Figure-5"> <img class="fig" src="https://www.researchgate.net/profile/Nicolo_Cesa-Bianchi/publication/220343796/figure/fig5/Figure-5_small.png" alt="Figure 5 . " title="Figure 5 . "/> </a> </li>  <li> <a href="/figure/220343796_fig6_Figure-6" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 6 . " data-key="220343796_fig6_Figure-6"> <img class="fig" src="https://www.researchgate.net/profile/Nicolo_Cesa-Bianchi/publication/220343796/figure/fig6/Figure-6_small.png" alt="Figure 6 . " title="Figure 6 . "/> </a> </li>  <li> <a href="/figure/220343796_fig7_Figure-7" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 7 . " data-key="220343796_fig7_Figure-7"> <img class="fig" src="https://www.researchgate.net/profile/Nicolo_Cesa-Bianchi/publication/220343796/figure/fig7/Figure-7_small.png" alt="Figure 7 . " title="Figure 7 . "/> </a> </li>  <li> <a href="/figure/220343796_fig8_Figure-8" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 8 . " data-key="220343796_fig8_Figure-8"> <img class="fig" src="https://www.researchgate.net/profile/Nicolo_Cesa-Bianchi/publication/220343796/figure/fig8/Figure-8_small.png" alt="Figure 8 . " title="Figure 8 . "/> </a> </li>  <li> <a href="/figure/220343796_fig9_Figure-9" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 9 . " data-key="220343796_fig9_Figure-9"> <img class="fig" src="https://www.researchgate.net/profile/Nicolo_Cesa-Bianchi/publication/220343796/figure/fig9/Figure-9_small.png" alt="Figure 9 . " title="Figure 9 . "/> </a> </li>  <li> <a href="/figure/220343796_fig10_Figure-10" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 10 . " data-key="220343796_fig10_Figure-10"> <img class="fig" src="https://www.researchgate.net/profile/Nicolo_Cesa-Bianchi/publication/220343796/figure/fig10/Figure-10_small.png" alt="Figure 10 . " title="Figure 10 . "/> </a> </li>  <li> <a href="/figure/220343796_fig11_Figure-11" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 11 . " data-key="220343796_fig11_Figure-11"> <img class="fig" src="https://www.researchgate.net/profile/Nicolo_Cesa-Bianchi/publication/220343796/figure/fig11/Figure-11_small.png" alt="Figure 11 . " title="Figure 11 . "/> </a> </li>  <li> <a href="/figure/220343796_fig12_Figure-12" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 12 . " data-key="220343796_fig12_Figure-12"> <img class="fig" src="https://www.researchgate.net/profile/Nicolo_Cesa-Bianchi/publication/220343796/figure/fig12/Figure-12_small.png" alt="Figure 12 . " title="Figure 12 . "/> </a> </li>  </ul> </div> </div> <div class="action-container"> <div id="rgw17_56ab9ebb431f0" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw32_56ab9ebb431f0">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw33_56ab9ebb431f0">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Nicolo_Cesa-Bianchi/publication/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem/links/09e4150ec228dace78000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Nicolo_Cesa-Bianchi">Nicolò Cesa-Bianchi</a>   </span>  </div>  <div class="social-share-container"><div id="rgw35_56ab9ebb431f0" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw36_56ab9ebb431f0" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw37_56ab9ebb431f0" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw38_56ab9ebb431f0" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw39_56ab9ebb431f0" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw40_56ab9ebb431f0" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw34_56ab9ebb431f0" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FNicolo_Cesa-Bianchi%2Fpublication%2F220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem%2Flinks%2F09e4150ec228dace78000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw31_56ab9ebb431f0"  itemprop="articleBody">  <p>Page 1</p> <p>Machine Learning, 47, 235–256, 2002<br />c ? 2002 Kluwer Academic Publishers. Manufactured in The Netherlands.<br />Finite-time Analysis of the Multiarmed Bandit<br />Problem*<br />PETER AUER<br />University of Technology Graz, A-8010 Graz, Austria<br />pauer@igi.tu-graz.ac.at<br />NICOL`O CESA-BIANCHI<br />DTI, University of Milan, via Bramante 65, I-26013 Crema, Italy<br />cesa-bianchi@dti.unimi.it<br />PAUL FISCHER<br />Lehrstuhl Informatik II, Universit¨ at Dortmund, D-44221 Dortmund, Germany<br />fischer@ls2.informatik.uni-dortmund.de<br />Editor: Jyrki Kivinen<br />Abstract.<br />a balance between exploring the environment to find profitable actions while taking the empirically best action as<br />often as possible. A popular measure of a policy’s success in addressing this dilemma is the regret, that is the loss<br />due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the<br />exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show<br />that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies<br />which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we<br />show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies,<br />and for all reward distributions with bounded support.<br />Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for<br />Keywords:<br />bandit problems, adaptive allocation rules, finite horizon regret<br />1.Introduction<br />The exploration versus exploitation dilemma can be described as the search for a balance<br />between exploring the environment to find profitable actions while taking the empirically<br />best action as often as possible. The simplest instance of this dilemma is perhaps the<br />multi-armedbandit,aproblemextensivelystudiedinstatistics(Berry&amp;Fristedt,1985)that<br />has also turned out to be fundamental in different areas of artificial intelligence, such as<br />reinforcement learning (Sutton &amp; Barto, 1998) and evolutionary programming (Holland,<br />1992).<br />In its most basic formulation, a K-armed bandit problem is defined by random variables<br />Xi,nfor 1≤i ≤ K and n ≥1, where each i is the index of a gambling machine (i.e., the<br />“arm” of a bandit). Successive plays of machine i yield rewards Xi,1, Xi,2,... which are<br />∗ApreliminaryversionappearedinProc.of15thInternationalConferenceonMachineLearning,pages100–108.<br />Morgan Kaufmann, 1998</p>  <p>Page 2</p> <p>236<br />P. AUER, N. CESA-BIANCHI AND P. FISCHER<br />independent and identically distributed according to an unknown law with unknown ex-<br />pectation µi. Independence also holds for rewards across machines; i.e., Xi,sand Xj,tare<br />independent (and usually not identically distributed) for each 1 ≤ i &lt; j ≤ K and each<br />s,t ≥ 1.<br />A policy, or allocation strategy, A is an algorithm that chooses the next machine to play<br />based on the sequence of past plays and obtained rewards. Let Ti(n) be the number of times<br />machine i has been played by A during the first n plays. Then the regret of A after n plays<br />is defined by<br />µ∗n − µj<br />K<br />?<br />j=1<br />IE[Tj(n)]where µ∗def<br />= max<br />1≤i≤Kµi<br />and IE[·] denotes expectation. Thus the regret is the expected loss due to the fact that the<br />policy does not always play the best machine.<br />In their classical paper, Lai and Robbins (1985) found, for specific families of reward<br />distributions (indexed by a single real parameter), policies satisfying<br />IE[Tj(n)] ≤<br />?<br />1<br />D(pj?p∗)+ o(1)<br />?<br />lnn<br />(1)<br />where o(1) → 0 as n → ∞ and<br />D(pj?p∗)<br />def<br />=<br />?<br />pjlnpj<br />p∗<br />is the Kullback-Leibler divergence between the reward density pjof any suboptimal ma-<br />chine j and the reward density p∗of the machine with highest reward expectation µ∗.<br />Hence, under these policies the optimal machine is played exponentially more often than<br />any other machine, at least asymptotically. Lai and Robbins also proved that this regret is<br />the best possible. Namely, for any allocation strategy and for any suboptimal machine j,<br />IE[Tj(n)]≥(lnn)/D(pj?p∗) asymptotically, provided that the reward distributions satisfy<br />some mild assumptions.<br />These policies work by associating a quantity called upper confidence index to each ma-<br />chine.Thecomputationofthisindexisgenerallyhard.Infact,itreliesontheentiresequence<br />of rewards obtained so far from a given machine. Once the index for each machine is com-<br />puted, the policy uses it as an estimate for the corresponding reward expectation, picking<br />forthenextplaythemachinewiththecurrenthighestindex.Morerecently,Agrawal(1995)<br />introduced a family of policies where the index can be expressed as simple function of<br />the total reward obtained so far from the machine. These policies are thus much easier to<br />compute than Lai and Robbins’, yet their regret retains the optimal logarithmic behavior<br />(though with a larger leading constant in some cases).1<br />In this paper we strengthen previous results by showing policies that achieve logarithmic<br />regret uniformly over time, rather than only asymptotically. Our policies are also simple to<br />implement and computationally efficient. In Theorem 1 we show that a simple variant of<br />Agrawal’s index-based policy has finite-time regret logarithmically bounded for arbitrary<br />sets of reward distributions with bounded support (a regret with better constants is proven</p>  <p>Page 3</p> <p>FINITE-TIME ANALYSIS<br />237<br />in Theorem 2 for a more complicated version of this policy). A similar result is shown<br />in Theorem 3 for a variant of the well-known randomized ε-greedy heuristic. Finally, in<br />Theorem 4 we show another index-based policy with logarithmically bounded finite-time<br />regret for the natural case when the reward distributions are normally distributed with<br />unknown means and variances.<br />Throughout the paper, and whenever the distributions of rewards for each machine are<br />understood from the context, we define<br />?i<br />def<br />= µ∗− µi<br />where, we recall, µiis the reward expectation for machinei and µ∗is any maximal element<br />in the set {µ1,...,µK}.<br />2.Main results<br />Our first result shows that there exists an allocation strategy, UCB1, achieving logarithmic<br />regret uniformly over n and without any preliminary knowledge about the reward distri-<br />butions (apart from the fact that their support is in [0,1]). The policy UCB1 (sketched in<br />figure 1) is derived from the index-based policy of Agrawal (1995). The index of this policy<br />isthesumoftwoterms.Thefirsttermissimplythecurrentaveragereward.Thesecondterm<br />is related to the size (according to Chernoff-Hoeffding bounds, see Fact 1) of the one-sided<br />confidence interval for the average reward within which the true expected reward falls with<br />overwhelming probability.<br />Theorem 1.<br />distributions P1,..., PKwith support in [0,1], then its expected regret after any number<br />n of plays is at most<br />For all K &gt; 1, if policy UCB1 is run on K machines having arbitrary reward<br />?<br />8<br />?<br />i:µi&lt;µ∗<br />?lnn<br />?i<br />??<br />+<br />?<br />1 +π2<br />3<br />??<br />K<br />?<br />j=1<br />?j<br />?<br />where µ1,...,µKare the expected values of P1,..., PK.<br />Figure 1.Sketch of the deterministic policy UCB1 (see Theorem 1).</p>  <p>Page 4</p> <p>238<br />P. AUER, N. CESA-BIANCHI AND P. FISCHER<br />Figure 2. Sketch of the deterministic policy UCB2 (see Theorem 2).<br />To prove Theorem 1 we show that, for any suboptimal machine j,<br />IE[Tj(n)] ≤<br />8<br />?2<br />j<br />lnn<br />(2)<br />plus a small constant. The leading constant 8/?2<br />1/D(pj? p∗) in Lai and Robbins’ result (1). In fact, one can show that D(pj? p∗) ≥ 2?2<br />where the constant 2 is the best possible.<br />Usingaslightlymorecomplicatedpolicy,whichwecall UCB2(seefigure2),wecanbring<br />the main constant of (2) arbitrarily close to 1/(2?2<br />The plays are divided in epochs. In each new epoch a machine i is picked and then<br />played τ(ri+ 1) − τ(ri) times, where τ is an exponential function and riis the number of<br />epochs played by that machine so far. The machine picked in each new epoch is the one<br />maximizing ¯ xi+ an,ri, where n is the current number of plays, ¯ xiis the current average<br />reward for machine i, and<br />iis worse than the corresponding constant<br />j<br />j). The policy UCB2 works as follows.<br />an,r=<br />?<br />(1 + α)ln(en/τ(r))<br />2τ(r)<br />(3)<br />where<br />τ(r) = ?(1 + α)r?.<br />In the next result we state a bound on the regret of UCB2. The constant cα, here left unspec-<br />ified, is defined in (18) in the appendix, where the theorem is also proven.<br />Theorem 2.<br />having arbitrary reward distributions P1,..., PKwith support in [0,1], then its expected<br />regret after any number<br />For all K &gt; 1, if policy UCB2 is run with input 0 &lt; α &lt; 1 on K machines<br />n ≥ max<br />i:µi&lt;µ∗<br />1<br />2?2<br />i</p>  <p>Page 5</p> <p>FINITE-TIME ANALYSIS<br />239<br />of plays is at most<br />?<br />i :µi&lt;µ∗<br />?(1 + α)(1 + 4α)ln?2e?2<br />in?<br />2?i<br />+cα<br />?i<br />?<br />(4)<br />where µ1,...,µKare the expected values of P1,..., PK.<br />Remark.<br />trarily close to 1/(2?2<br />traded-off by letting α = αnbe slowly decreasing with the number n of plays.<br />By choosing α small, the constant of the leading term in the sum (4) gets arbi-<br />i); however, cα→ ∞ as α → 0. The two terms in the sum can be<br />A simple and well-known policy for the bandit problem is the so-called ε-greedy rule<br />(seeSutton,&amp;Barto,1998).Thispolicyprescribestoplaywithprobability1−εthemachine<br />withthehighestaveragereward,andwithprobabilityεarandomlychosenmachine.Clearly,<br />the constant exploration probability ε causes a linear (rather than logarithmic) growth in<br />the regret. The obvious fix is to let ε go to zero with a certain rate, so that the exploration<br />probability decreases as our estimates for the reward expectations become more accurate.<br />It turns out that a rate of 1/n, where n is, as usual, the index of the current play, allows<br />to prove a logarithmic bound on the regret. The resulting policy, εn-GREEDY, is shown in<br />figure 3.<br />Theorem 3.<br />[0,1], if policy εn-GREEDY is run with input parameter<br />For all K &gt;1 and for all reward distributions P1,..., PKwith support in<br />0 &lt; d ≤ min<br />i:µi&lt;µ∗?i,<br />Figure 3.Sketch of the randomized policy εn-GREEDY (see Theorem 3).</p>  <p>Page 6</p> <p>240<br />P. AUER, N. CESA-BIANCHI AND P. FISCHER<br />then the probability that after any number n ≥cK/d of plays εn-GREEDY chooses a subop-<br />timal machine j is at most<br />c<br />d2n+ 2<br />?c<br />?<br />d2ln(n − 1)d2e1/2<br />cK<br />(n − 1)d2e1/2<br />Forc largeenough(e.g.c &gt; 5)theaboveboundisoforderc/(d2n)+o(1/n)for<br />n → ∞, as the second and third terms in the bound are O(1/n1+ε) for some ε &gt; 0 (recall<br />that 0 &lt; d &lt; 1). Note also that this is a result stronger than those of Theorems 1 and 2, as<br />it establishes a bound on the instantaneous regret. However, unlike Theorems 1 and 2, here<br />we need to know a lower bound d on the difference between the reward expectations of the<br />best and the second best machine.<br />cK<br />??<br />cK<br />(n − 1)d2e1/2<br />?c/(5d2)<br />+4e<br />d2<br />?c/2<br />.<br />Remark.<br />Our last result concerns a special case, i.e. the bandit problem with normally distributed<br />rewards. Surprisingly, we could not find in the literature regret bounds (not even asymp-<br />totical) for the case when both the mean and the variance of the reward distributions are<br />unknown. Here, we show that an index-based policy called UCB1-NORMAL, see figure 4,<br />achieves logarithmic regret uniformly over n without knowing means and variances of the<br />reward distributions. However, our proof is based on certain bounds on the tails of the χ2<br />and the Student distribution that we could only verify numerically. These bounds are stated<br />as Conjecture 1 and Conjecture 2 in the Appendix.<br />The choice of the index in UCB1-NORMAL is based, as for UCB1, on the size of the one-<br />sidedconfidenceintervalfortheaveragerewardwithinwhichthetrueexpectedrewardfalls<br />with overwhelming probability. In the case of UCB1, the reward distribution was unknown,<br />and we used Chernoff-Hoeffding bounds to compute the index. In this case we know that<br />Figure 4. Sketch of the deterministic policy UCB1-NORMAL (see Theorem 4).</p>  <p>Page 7</p> <p>FINITE-TIME ANALYSIS<br />241<br />the distribution is normal, and for computing the index we use the sample variance as an<br />estimate of the unknown variance.<br />Theorem 4.<br />reward distributions P1,..., PK, then its expected regret after any number n of plays is at<br />most<br />For all K &gt; 1, if policy UCB1-NORMAL is run on K machines having normal<br />256(logn)<br />?<br />?<br />i:µi&lt;µ∗<br />σ2<br />i<br />?i<br />?<br />+<br />?<br />1 +π2<br />2<br />+ 8 logn<br />??<br />K<br />?<br />j=1<br />?j<br />?<br />where µ1,...,µK and σ2<br />P1,..., PK.<br />1,...,σ2<br />Kare the means and variances of the distributions<br />As a final remark for this section, note that Theorems 1–3 also hold for rewards that are not<br />independent across machines, i.e. Xi,sand Xj,tmight be dependent for any s, t, and i ?= j.<br />Furthermore, we also do not need that the rewards of a single arm are i.i.d., but only the<br />weaker assumption that IE[Xi,t| Xi,1,..., Xi,t−1] = µifor all 1 ≤ t ≤ n.<br />3. Proofs<br />Recall that, for each 1 ≤ i ≤ K, IE[Xi,n] = µifor all n ≥ 1 and µ∗= max1≤i≤Kµi. Also,<br />for any fixed policy A, Ti(n) is the number of times machine i has been played by A in the<br />firstn plays.Ofcourse,wealwayshave?K<br />For each 1 ≤ i ≤ K and n ≥ 1 define<br />¯Xi,n=1<br />n<br />t=1<br />Given µ1,...,µK, we call optimal the machine with the least index i such that µi= µ∗.<br />In what follows, we will always put a superscript “∗” to any quantity which refers to the<br />optimal machine. For example we write T∗(n) and¯X∗<br />the index of the optimal machine.<br />Some further notation: For any predicate ? we define {?(x)} to be the indicator fuction<br />of the event ?(x); i.e., {?(x)} = 1 if ?(x) is true and {?(x)} = 0 otherwise. Finally,<br />Var[X] denotes the variance of the random variable X.<br />Note that the regret after n plays can be written as<br />i=1Ti(n) = n.Wealsodefinether.v.’s I1, I2,...,<br />where Itdenotes the machine played at time t.<br />n ?<br />Xi,t.<br />ninstead of Ti(n) and¯Xi,n, where i is<br />?<br />j:µj&lt;µ∗<br />?jIE[Tj(n)] (5)<br />So we can bound the regret by simply bounding each IE[Tj(n)].<br />We will make use of the following standard exponential inequalities for bounded random<br />variables (see, e.g., the appendix of Pollard, 1984).</p>  <p>Page 8</p> <p>242<br />P. AUER, N. CESA-BIANCHI AND P. FISCHER<br />Fact 1 (Chernoff-Hoeffding bound). Let X1,..., Xnbe random variables with common<br />range [0,1] and such that IE[Xt|X1,..., Xt−1] = µ. Let Sn= X1+ ··· + Xn. Then for<br />all a ≥ 0<br />IP{Sn≥ nµ + a} ≤ e−2a2/n<br />Fact 2 (Bernstein inequality). Let X1,..., Xnbe random variables with range [0,1] and<br />and<br />IP{Sn≤ nµ − a} ≤ e−2a2/n<br />n ?<br />t=1<br />Var[Xt| Xt−1,..., X1] = σ2.<br />Let Sn= X1+ ··· + Xn. Then for all a ≥ 0<br />IP{Sn≥ IE[Sn] + a} ≤ exp<br />?<br />−<br />a2/2<br />σ2+ a/2<br />?<br />.<br />Proof of Theorem 1:<br />on any sequence of plays. More precisely, for each t ≥ 1 we bound the indicator function<br />of It= i as follows. Let ? be an arbitrary positive integer.<br />n ?<br />≤ ? +<br />t=K+1<br />n ?<br />+ ct−1,Ti(t−1), Ti(t − 1) ≥ ??<br />≤ ? +<br />t=K+1<br />∞<br />?<br />Now observe that¯X∗<br />hold<br />Let ct,s=√(2lnt)/s. For any machine i, we upper bound Ti(n)<br />Ti(n) = 1 +<br />t=K+1<br />n ?<br />{It= i}<br />{It= i, Ti(t − 1) ≥ ?}<br />≤ ? +<br />t=K+1<br />?¯X∗<br />T∗(t−1)+ ct−1,T∗(t−1)≤¯Xi,Ti(t−1)<br />n ?<br />?<br />min<br />0&lt;s&lt;t<br />¯X∗<br />s+ ct−1,s≤ max<br />?≤si&lt;t<br />¯Xi,si+ ct−1,si<br />?<br />≤ ? +<br />t=1<br />t−1<br />?<br />s=1<br />t−1<br />?<br />si=?<br />?¯X∗<br />s+ ct,s≤¯Xi,si+ ct,si<br />?.<br />(6)<br />s+ ct,s≤¯Xi,si+ ct,siimplies that at least one of the following must<br />¯X∗<br />s≤ µ∗− ct,s<br />¯Xi,si≥ µi+ ct,si<br />µ∗&lt; µi+ 2ct,si.<br />We bound the probability of events (7) and (8) using Fact 1 (Chernoff-Hoeffding bound)<br />(7)<br />(8)<br />(9)<br />IP{¯X∗<br />s≤ µ∗− ct,s} ≤ e−4lnt= t−4</p>  <p>Page 9</p> <p>FINITE-TIME ANALYSIS<br />243<br />IP?¯Xi,si≥ µi+ ct,si<br />For ? = ?(8lnn)/?2<br />µ∗− µi− 2ct,si= µ∗− µi− 2<br />for si≥ (8lnn)/?2<br />?≤ e−4lnt= t−4.<br />i?, (9) is false. In fact<br />?<br />2(lnt)/si≥ µ∗− µi− ?i= 0<br />i. So we get<br />IE[Ti(n)] ≤<br />?8lnn<br />×?IP{¯X∗<br />?8lnn<br />≤8lnn<br />?2<br />i<br />?2<br />i<br />?<br />+<br />∞<br />?<br />t=1<br />t−1<br />?<br />s=1<br />t−1<br />?<br />si=?(8lnn)/?2<br />i?<br />s≤ µ∗− ct,s} + IP?¯Xi,si≥ µi+ ct,si<br />?<br />+ 1 +π2<br />3<br />??<br />≤<br />?2<br />i<br />+<br />∞<br />?<br />t=1<br />t ?<br />s=1<br />t ?<br />si=1<br />2t−4<br />which concludes the proof.<br />✷<br />Proof of Theorem 3:<br />Recall that, for n ≥ cK/d2, εn= cK/(d2n). Let<br />x0=<br />1<br />2K<br />n ?<br />t=1<br />εt.<br />The probability that machine j is chosen at time n is<br />IP{In= j} ≤εn<br />K+<br />?<br />1 −εn<br />K<br />?<br />IP?¯Xj,Tj(n−1)≥¯X∗<br />T∗(n−1)<br />?<br />and<br />IP?¯Xj,Tj(n)≥¯X∗<br />≤ IP<br />T∗(n)<br />?<br />?<br />¯Xj,Tj(n)≥ µj+?j<br />2<br />?<br />+ IP<br />?<br />¯X∗<br />T∗(n)≤ µ∗−?j<br />2<br />?<br />.<br />(10)<br />Nowtheanalysisforbothtermsontheright-handsideisthesame.Let TR<br />of plays in which machine j was chosen at random in the first n plays. Then we have<br />j(n)bethenumber<br />IP<br />?<br />n ?<br />¯Xj,Tj(n)≥ µj+?j<br />2<br />?<br />=<br />t=1<br />IP<br />?<br />Tj(n) = t ∧¯Xj,t≥ µj+?j<br />2<br />?<br />(11)</p>  <p>Page 10</p> <p>244<br />P. AUER, N. CESA-BIANCHI AND P. FISCHER<br />=<br />n ?<br />n ?<br />by Fact 1 (Chernoff-Hoeffding bound)<br />t=1<br />IP<br />?<br />?<br />Tj(n) = t |¯Xj,t≥ µj+?j<br />2<br />?<br />?<br />· IP<br />?<br />¯Xj,t≥ µj+?j<br />2<br />?<br />≤<br />t=1<br />IPTj(n) = t |¯Xj,t≥ µj+?j<br />2<br />· e−?2<br />jt/2<br />≤<br />?x0?<br />?<br />since?∞<br />?<br />≤ x0· IP?TR<br />t=1<br />IP<br />?<br />Tj(n) = t |¯Xj,t≥ µj+?j<br />2<br />?<br />+<br />2<br />?2<br />j<br />e−?2<br />j?x0?/2<br />t=x+1e−κt≤1<br />j(n) ≤ t |¯Xj,t≥ µj+?j<br />κe−κx<br />≤<br />?x0?<br />t=1<br />IP<br />?<br />TR<br />2<br />?<br />+<br />2<br />?2<br />j<br />e−?2<br />j?x0?/2<br />j(n) ≤ x0<br />?+<br />2<br />?2<br />j<br />e−?2<br />j?x0?/2<br />(12)<br />whereinthelastlinewedroppedtheconditioningbecauseeachmachineisplayedatrandom<br />independently of the previous choices of the policy. Since<br />IE?TR<br />j(n)?=<br />1<br />K<br />n ?<br />t=1<br />εt<br />and<br />Var?TR<br />j(n)?=<br />n ?<br />t=1<br />εt<br />K<br />?<br />1 −εt<br />K<br />?<br />≤<br />1<br />K<br />n ?<br />t=1<br />εt,<br />by Bernstein’s inequality (2) we get<br />IP?TR<br />j(n) ≤ x0<br />?≤ e−x0/5.<br />(13)<br />Finally it remains to lower bound x0. For n ≥ n?= cK/d2, εn= cK/(d2n) and we have<br />x0=<br />1<br />2K<br />n ?<br />n?<br />?<br />t=1<br />εt<br />=<br />1<br />2K<br />t=1<br />εt+<br />1<br />2K<br />n ?<br />t=n?+1<br />εt<br />≥<br />n?<br />2K+<br />c<br />d2lnnd2e1/2<br />c<br />d2lnn<br />n?<br />≥<br />cK<br />.</p>  <p>Page 11</p> <p>FINITE-TIME ANALYSIS<br />245<br />Thus, using (10)–(13) and the above lower bound on x0we obtain<br />IP{In= j} ≤εn<br />K+ 2x0e−x0/5+<br />4<br />?2<br />j<br />e−?2<br />j?x0?/2<br />≤<br />c<br />d2n+ 2<br />+4e<br />d2<br />?c<br />d2ln(n − 1)d2e1/2<br />cK<br />(n − 1)d2e1/2<br />cK<br />?c/2<br />??<br />cK<br />(n − 1)d2e1/2<br />?c/(5d2)<br />?<br />.<br />This concludes the proof.<br />✷<br />4.Experiments<br />For practical purposes, the bound of Theorem 1 can be tuned more finely. We use<br />Vj(s)<br />def<br />=<br />?<br />1<br />s<br />s ?<br />τ=1<br />X2<br />j,τ<br />?<br />−¯X2<br />j,s+<br />?2lnt<br />s<br />as un upper confidence bound for the variance of machine j. As before, this means that<br />machine j, which has been played s times during the first t plays, has a variance that is<br />at most the sample variance plus√(2lnt)/s. We then replace the upper confidence bound<br />?2ln(n)/njof policy UCB1 with<br />?<br />nj<br />lnn<br />min{1/4,Vj(nj)}<br />(the factor 1/4 is an upper bound on the variance of a Bernoulli random variable). This<br />variant, which we call UCB1-TUNED, performs substantially better than UCB1 in essentially<br />all of our experiments. However, we are not able to prove a regret bound.<br />We compared the empirical behaviour policies UCB1-TUNED, UCB2, and εn-GREEDY on<br />Bernoulli reward distributions with different parameters shown in the table below.<br />123456789 10<br />1 0.90.6<br />2 0.90.8<br />30.55 0.45<br />11 0.90.60.6 0.60.60.6 0.6 0.6 0.60.6<br />120.9 0.80.8 0.80.7 0.7 0.70.6 0.60.6<br />13 0.9 0.80.8 0.8 0.80.8 0.8 0.80.80.8<br />14 0.550.450.450.450.450.45 0.450.450.45 0.45</p>  <p>Page 12</p> <p>246<br />P. AUER, N. CESA-BIANCHI AND P. FISCHER<br />Rows 1–3 define reward distributions for a 2-armed bandit problem, whereas rows 11–<br />14 define reward distributions for a 10-armed bandit problem. The entries in each row<br />denote the reward expectations (i.e. the probabilities of getting a reward 1, as we work with<br />Bernoulli distributions) for the machines indexed by the columns. Note that distributions 1<br />and 11 are “easy” (the reward of the optimal machine has low variance and the differences<br />µ∗−µiare all large), whereas distributions 3 and 14 are “hard” (the reward of the optimal<br />machine has high variance and some of the differences µ∗− µiare small).<br />We made experiments to test the different policies (or the same policy with different<br />input parameters) on the seven distributions listed above. In each experiment we tracked<br />two performance measures: (1) the percentage of plays of the optimal machine; (2) the<br />actual regret, that is the difference between the reward of the optimal machine and the<br />reward of the machine played. The plot for each experiment shows, on a semi-logarithmic<br />scale, the behaviour of these quantities during 100,000 plays averaged over 100 different<br />runs. We ran a first round of experiments on distribution 2 to find out good values for the<br />parameters of the policies. If a parameter is chosen too small, then the regret grows linearly<br />(exponentially in the semi-logarithmic plot); if a parameter is chosen too large then the<br />regret grows logarithmically, but with a large leading constant (corresponding to a steep<br />line in the semi-logarithmic plot).<br />Policy UCB2 is relatively insensitive to the choice of its parameter α, as long as it is<br />kept relatively small (see figure 5). A fixed value 0.001 has been used for all the remaining<br />experiments. On other hand, the choice of c in policy εn-GREEDY is difficult as there is no<br />value that works reasonably well for all the distributions that we considered. Therefore, we<br />have roughly searched for the best value for each distribution. In the plots, we will also<br />showtheperformanceofεn-GREEDYforvaluesofc aroundthisempiricallybestvalue.This<br />shows that the performance degrades rapidly if this parameter is not appropriately tuned.<br />Finally, in each experiment the parameter d of εn-GREEDY was set to<br />? = µ∗− max<br />i:µi&lt;µ∗µi.<br />Figure 5.Search for the best value of parameter α of policy UCB2.</p>  <p>Page 13</p> <p>FINITE-TIME ANALYSIS<br />247<br />4.1. Comparison between policies<br />We can summarize the comparison of all the policies on the seven distributions as follows<br />(see Figs. 6–12).<br />– An optimally tuned εn-GREEDY performs almost always best. Significant exceptions are<br />distributions12and14:thisisbecauseεn-GREEDYexploresuniformlyoverallmachines,<br />thus the policy is hurt if there are several nonoptimal machines, especially when their<br />reward expectations differ a lot. Furthermore, if εn-GREEDY is not well tuned its perfor-<br />mance degrades rapidly (except for distribution 13, on which εn-GREEDY performs well<br />a wide range of values of its parameter).<br />– In most cases, UCB1-TUNED performs comparably to a well-tuned εn-GREEDY. Further-<br />more, UCB1-TUNED is not very sensitive to the variance of the machines, that is why it<br />performs similarly on distributions 2 and 3, and on distributions 13 and 14.<br />– Policy UCB2 performs similarly to UCB1-TUNED, but always slightly worse.<br />Figure 6. Comparison on distribution 1 (2 machines with parameters 0.9,0.6).<br />Figure 7. Comparison on distribution 2 (2 machines with parameters 0.9,0.8).</p>  <p>Page 14</p> <p>248<br />P. AUER, N. CESA-BIANCHI AND P. FISCHER<br />Figure 8.Comparison on distribution 3 (2 machines with parameters 0.55,0.45).<br />Figure 9.Comparison on distribution 11 (10 machines with parameters 0.9,0.6,...,0.6).<br />Figure 10.<br />0.6,0.6).<br />Comparison on distribution 12 (10 machines with parameters 0.9,0.8,0.8,0.8,0.7,0.7,0.7,0.6,</p>  <p>Page 15</p> <p>FINITE-TIME ANALYSIS<br />249<br />Figure 11.Comparison on distribution 13 (10 machines with parameters 0.9,0.8,...,0.8).<br />Figure 12. Comparison on distribution 14 (10 machines with parameters 0.55,0.45,...,0.45).<br />5. Conclusions<br />Wehaveshownsimpleandefficientpoliciesforthebanditproblemthat,onanysetofreward<br />distributions with known bounded support, exhibit uniform logarithmic regret. Our policies<br />are deterministic and based on upper confidence bounds, with the exception of εn-GREEDY,<br />a randomized allocation rule that is a dynamic variant of the ε-greedy heuristic. Moreover,<br />our policies are robust with respect to the introduction of moderate dependencies in the<br />reward processes.<br />This work can be extended in many ways. A more general version of the bandit problem<br />is obtained by removing the stationarity assumption on reward expectations (see Berry &amp;<br />Fristedt, 1985; Gittins, 1989 for extensions of the basic bandit problem). For example,<br />suppose that a stochastic reward process {Xi,s:s = 1,2,...} is associated to each machine<br />i = 1,..., K. Here, playing machine i at time t yields a reward Xi,sand causes the current</p>  <p>Page 16</p> <p>250<br />P. AUER, N. CESA-BIANCHI AND P. FISCHER<br />state s of i to change to s +1, whereas the states of other machines remain frozen. A well-<br />studied problem in this setup is the maximization of the total expected reward in a sequence<br />of n plays. There are methods, like the Gittins allocation indices, that allow to find the<br />optimal machine to play at each time n by considering each reward process independently<br />from the others (even though the globally optimal solution depends on all the processes).<br />However,computationoftheGittinsindicesfortheaverage(undiscounted)rewardcriterion<br />used here requires preliminary knowledge about the reward processes (see, e.g., Ishikida &amp;<br />Varaiya,1994).Toovercomethisrequirement,onecanlearntheGittinsindices,asproposed<br />inDuff(1995)forthecaseoffinite-stateMarkovianrewardprocesses.However,thereareno<br />finite-time regret bounds shown for this solution. At the moment, we do not know whether<br />our techniques could be extended to these more general bandit problems.<br />Appendix A: Proof of Theorem 2<br />Note that<br />τ(r) ≤ (1 + α)r+ 1 ≤ τ(r − 1)(1 + α) + 1<br />for r ≥ 1. Assume that n ≥ 1/(2?2<br />(1 + 4α)ln?2en?2<br />(14)<br />j) for all j and let ˜ rjbe the largest integer such that<br />?<br />τ(˜ rj− 1) ≤<br />j<br />2?2<br />j<br />.<br />Note that ˜ rj≥ 1. We have<br />Tj(n) ≤ 1 +<br />?<br />r≥1<br />(τ(r) − τ(r − 1)) {machine j finishes its r-th epoch}<br />≤ τ(˜ rj) +<br />?<br />r&gt;˜ rj<br />(τ(r) − τ(r − 1)) {machine j finishes its r-th epoch}<br />Now consider the following chain of implications<br />machine j finishes its r-th epoch<br />⇒ ∃i ≥ 0, ∃t ≥ τ(r − 1) + τ(i) such that<br />?¯Xj,τ(r−1)+ at,r−1<br />⇒ ∃t ≥ τ(r − 1) such that?¯Xj,τ(r−1)+ at,r−1<br />?¯X∗<br />⇒¯Xj,τ(r−1)+ an,r−1≥ µ∗− α?j/2<br />or<br />∃i ≥ 0 such that¯X∗<br />where the last implication hold because at,ris increasing in t. Hence<br />?≥?¯X∗<br />τ(i)+ at,i<br />?<br />?≥ µ∗− α?j/2<br />or<br />∃i ≥ 0, ∃t?≥ τ(r − 1) + τ(i) such that<br />τ(i)+ at?,i<br />?≤ µ∗− α?j/2<br />τ(i)+ aτ(r−1)+τ(i),i≤ µ∗− α?j/2<br />IE[Tj(n)] ≤ τ(˜ rj) +<br />?<br />r&gt;˜ rj<br />(τ(r) − τ(r − 1))IP?¯Xj,τ(r−1)+ an,r−1≥ µ∗− α?j/2?</p>  <p>Page 17</p> <p>FINITE-TIME ANALYSIS<br />251<br />+<br />?<br />r&gt;˜ rj<br />?<br />τ(i)+ aτ(r−1)+τ(i),i≤ µ∗− α?j/2?.<br />j) implies ln(2en?2<br />i≥0<br />(τ(r) − τ(r − 1))<br />· IP?¯X∗<br />(15)<br />The assumption n ≥ 1/(2?2<br />j) ≥ 1. Therefore, for r &gt; ˜ rj, we have<br />τ(r − 1) &gt;<br />(1 + 4α) ln?2en?2<br />j<br />?<br />2?2<br />j<br />(16)<br />and<br />an,r−1=<br />?<br />(1 + α)ln(en/τ(r − 1))<br />2τ(r − 1)<br />?<br />?<br />?<br />using τ(r − 1) &gt; 1/2?2<br />?<br />≤ ?j<br />(1 + α)ln(en/τ(r − 1))<br />?1 + 4α)ln(2en?2<br />?<br />j<br />?<br />using (16) above<br />≤ ?j<br />?<br />?1 + α)ln(2en?2<br />j<br />?<br />j<br />?1 + 4α)ln(2en?2<br />?<br />jderived from (16)<br />≤ ?j<br />1 + α<br />1 + 4α.<br />(17)<br />We start by bounding the first sum in (15). Using (17) and Fact 1 (Chernoff-Hoeffding<br />bound) we get<br />IP?¯Xj,τ(r−1)+ an,r−1≥ µ∗− α?j/2?<br />≤ exp?− 2τ(r − 1)?2<br />≤ exp?−2τ(r − 1)?2<br />= exp?−τ(r − 1)?2<br />for α &lt; 1/10. Now let g(x) = (x − 1)/(1 + α). By (14) we get g(x) ≤ τ(r − 1) for<br />τ(r − 1) ≤ x ≤ τ(r) and r ≥ 1. Hence<br />?<br />≤<br />r&gt;˜ rj<br />?∞<br />= IP{¯Xj,τ(r−1)+ an,r−1≥ µj+ ?j− α?j/2}<br />j<br />?1 − α/2 −<br />jα2?2?<br />?<br />(1 + α)/(1 + 4α)?2?<br />j(1 − α/2 − (1 − α))2?<br />r&gt;˜ rj<br />(τ(r) − τ(r − 1))IP?¯Xj,τ(r−1)+ an,r−1≥ µ∗− α?j/2?<br />?<br />e−cg(x)dx<br />(τ(r) − τ(r − 1)) exp?− τ(r − 1)?2<br />jα2?<br />≤<br />0</p>  <p>Page 18</p> <p>252<br />P. AUER, N. CESA-BIANCHI AND P. FISCHER<br />where c = (?jα)2&lt; 1. Further manipulation yields<br />?∞<br />0<br />exp<br />?<br />−<br />c<br />1 + α(x − 1)<br />?<br />dx = ec/(1+α)1 + α<br />c<br />≤(1 + α)e<br />(?jα)2.<br />We continue by bounding the second sum in (15). Using once more Fact 1, we get<br />?<br />≤<br />r&gt;˜ rj<br />?<br />?<br />· exp<br />?<br />??<br />?<br />??<br />i≥0<br />(τ(r) − τ(r − 1))IP?¯X∗<br />?<br />?<br />exp{−τ(i)(α?j)2/2}<br />τ(i)+ aτ(r−1)+τ(i),i≤ µ∗− α?j/2?<br />i≥0<br />r&gt;˜ rj<br />(τ(r) − τ(r − 1))<br />− τ(i)(α?j)2<br />2<br />− (1 + α) ln<br />?<br />eτ(r − 1) + τ(i)<br />τ(i)<br />??<br />≤<br />i≥0<br />·<br />r&gt;˜ rj<br />exp{−τ(i)(α?j)2/2}<br />(τ(r) − τ(r − 1)) exp<br />?<br />− (1 + α)ln<br />?<br />1 +τ(r − 1)<br />τ(i)<br />???<br />=<br />i≥0<br />·<br />r&gt;˜ rj<br />(τ(r) − τ(r − 1))<br />?<br />??∞<br />1 +τ(r − 1)<br />τ(i)<br />?−(1+α)?<br />x − 1<br />(1 + α)τ(i)<br />?<br />1 + α<br />α<br />1 + α<br />≤<br />?<br />?<br />?<br />?1 + α<br />i≥0<br />exp{−τ(i)(α?j)2/2}<br />0<br />?<br />1 + α<br />α<br />1 +<br />?−(1+α)<br />1<br />dx<br />?<br />=<br />i≥0<br />τ(i)exp{−τ(i)(α?j)2/2}<br />?<br />?<br />1 −<br />(1 + α)τ(i)<br />α<br />?−α?<br />?−α?<br />≤<br />i≥0<br />τ(i)exp{−τ(i)(α?j)2/2}<br />?<br />as τ(i) ≥ 1<br />=<br />α<br />?1+α?<br />i≥0<br />τ(i)exp{−τ(i)(α?j)2/2}.<br />Now, as (1 + α)x−1≤ τ(i) ≤ (1 + α)x+ 1 for i ≤ x ≤ i + 1, we can bound the series in<br />the last formula above with an integral<br />?<br />≤ 1 +<br />i≥0<br />τ(i)exp{−τ(i)(α?j)2/2}<br />?∞<br />1<br />((1 + α)x+ 1)exp{−(1 + α)x−1(α?j)2/2}dx</p>  <p>Page 19</p> <p>FINITE-TIME ANALYSIS<br />253<br />≤ 1 +<br />?∞<br />by change of variable z = (1 + α)x<br />= 1 +<br />ln(1 + α)<br />1<br />z + 1<br />z ln(1 + α)exp<br />?<br />−z(α?j)2<br />2(1 + α)<br />?<br />dz<br />1<br />?e−λ<br />λ<br />+<br />?∞<br />λ<br />e−x<br />x<br />dx<br />?<br />where we set<br />λ =<br />(α?j)2<br />2(1 + α).<br />As 0 &lt; α,?j&lt; 1, we have 0 &lt; λ &lt; 1/4. To upper bound the bracketed formula above,<br />consider the function<br />F(λ) = e−λ+ λ<br />?∞<br />λ<br />e−x<br />x<br />dx<br />with derivatives<br />F?(λ) =<br />?∞<br />λ<br />e−x<br />x<br />dx − 2e−λ<br />F??(λ) = 2λe−λ−<br />?∞<br />λ<br />e−x<br />x<br />dx.<br />In the interval (0,1/4), F?is seen to have a zero at λ = 0.0108.... As F??(λ) &lt; 0 in the<br />same interval, this is the unique maximum of F, and we find F(0.0108...) &lt; 11/10. So<br />we have<br />e−λ<br />λ<br />+<br />?∞<br />λ<br />e−x<br />x<br />dx &lt;<br />11<br />10λ=11(1 + α)<br />5(α?j)2<br />Piecing everything together, and using (14) to upper bound τ(˜ rj), we find that<br />IE[Tj(n)] ≤ τ(˜ rj) +(1 + α)e<br />(1 + α)(1 + 4α)ln?2en?2<br />(?jα)2+<br />?1 + α<br />α<br />?1+α?<br />+cα<br />1 +<br />11(1 + α)<br />5(α?j)2ln(1 + α)<br />?<br />≤<br />j<br />?<br />2?2<br />j<br />?2<br />j<br />where<br />cα= 1 +(1 + α)e<br />α2<br />+<br />?1 + α<br />α<br />?1+α?<br />1 +<br />11(1 + α)<br />5α2ln(1 + α)<br />?<br />.<br />(18)<br />This concludes the proof.<br />✷</p>  <p>Page 20</p> <p>254<br />P. AUER, N. CESA-BIANCHI AND P. FISCHER<br />Appendix B:Proof of Theorem 4<br />The proof goes very much along the same lines as the proof of Theorem 1. It is based on<br />the two following conjectures which we only verified numerically.<br />Conjecture 1.<br />all 0≤a ≤√2(s + 1),<br />IP{X ≥ a} ≤ e−a2/4.<br />Let X be a Student random variable with s degrees of freedom. Then, for<br />Conjecture 2.<br />Let X be a χ2random variable with s degrees of freedom. Then<br />IP{X ≥ 4s} ≤ e−(s+1)/2.<br />We now proceed with the proof of Theorem 4. Let<br />Qi,n=<br />n ?<br />t=1<br />X2<br />i,t.<br />Fix a machine i and, for any s and t, set<br />ct,s=<br />?<br />16 ·Qi,s− s¯X2<br />i,s<br />s − 1<br />·lnt<br />s<br />Let c∗<br />proceed exactly as in the first part of the proof of Theorem 1 obtaining, for any positive<br />integer ?,<br />t,sbe the corresponding quantity for the optimal machine. To upper bound Ti(n), we<br />Ti(n) ≤ ? +<br />∞<br />?<br />t=1<br />t−1<br />?<br />s=1<br />t−1<br />?<br />si=?<br />?{¯X∗<br />s≤ µ∗− c∗<br />t,s} +?¯Xi,si≥ µi+ ct,si<br />?<br />+?µ∗&lt; µi+ 2ct,si<br />??.<br />?<br />The random variable (¯Xi,si−µi)/<br />with si− 1 degrees of freedom (see, e.g., Wilks, 1962, 8.4.3 page 211). Therefore, using<br />Conjecture 1 with s = si− 1 and a = 4√lnt, we get<br />(Qi,si− si¯X2<br />i,si)/(si(si− 1)) has a Student distribution<br />IP?¯Xi,si≥ µi+ ct,si<br />?= IP<br /><br /><br /><br />¯Xi,si− µi<br />??Qi,si− si¯X2<br />s≤ µ∗−c∗<br />i,si<br />?/(si(si− 1))<br />t,sis bounded analogously. Finally, since<br />≥ 4√lnt<br /><br /><br />≤ t−4<br />for all si≥ 8lnt. The probability of¯X∗<br />(Qi,si−si¯X2<br />i,si)/σ2<br />iis χ2-distributed with si−1 degrees of freedom (see, e.g., Wilks, 1962,</p>  <p>Page 21</p> <p>FINITE-TIME ANALYSIS<br />255<br />8.4.1 page 208). Therefore, using Conjecture 2 with s = si− 1 and a = 4s, we get<br />??Qi,si− si¯X2<br />≤ IP??Qi,si− si¯X2<br />IP?µ∗&lt; µi+ 2ct,si<br />?= IP<br />i,si<br />??σ2<br />??σ2<br />i&gt; (si− 1)?2<br />i<br />σ2<br />i<br />si<br />64lnt<br />?<br />i,si<br />i&gt; 4(si− 1)?<br />≤ e−si/2≤ t−4<br />for<br />si≥ max<br />?<br />256σ2<br />i<br />?2<br />i<br />,8<br />?<br />lnt.<br />Setting<br />? =<br />?<br />max<br />?<br />256σ2<br />i<br />?2<br />i<br />,8<br />?<br />lnt<br />?<br />completes the proof of the theorem.<br />✷<br />Acknowledgments<br />The support from ESPRIT Working Group EP 27150, Neural and Computational Learning<br />II (NeuroCOLT II), is gratefully acknowledged.<br />Note<br />1. Similar extensions of Lai and Robbins’ results were also obtained by Yakowitz and Lowe (1991), and by<br />Burnetas and Katehakis (1996).<br />References<br />Agrawal, R. (1995). Sample mean based index policies with O(logn) regret for the multi-armed bandit problem.<br />Advances in Applied Probability, 27, 1054–1078.<br />Berry, D., &amp; Fristedt, B. (1985). Bandit problems. London: Chapman and Hall.<br />Burnetas, A., &amp; Katehakis, M. (1996). Optimal adaptive policies for sequential allocation problems. Advances in<br />Applied Mathematics, 17:2, 122–142.<br />Duff,M.(1995).Q-learningforbanditproblems.InProceedingsofthe12thInternationalConferenceonMachine<br />Learning (pp. 209–217).<br />Gittins, J. (1989). Multi-armed bandit allocation indices, Wiley-Interscience series in Systems and Optimization.<br />New York: John Wiley and Sons.<br />Holland, J. (1992). Adaptation in natural and artificial systems. Cambridge: MIT Press/Bradford Books.<br />Ishikida, T., &amp; Varaiya, P. (1994). Multi-armed bandit problem revisited. Journal of Optimization Theory and<br />Applications, 83:1, 113–154.<br />Lai,T.,&amp;Robbins,H.(1985).Asymptoticallyefficientadaptiveallocationrules.AdvancesinAppliedMathematics,<br />6, 4–22.<br />Pollard, D. (1984). Convergence of stochastic processes. Berlin: Springer.</p>  <p>Page 22</p> <p>256<br />P. AUER, N. CESA-BIANCHI AND P. FISCHER<br />Sutton, R., &amp; Barto, A. (1998). Reinforcement learning, an introduction. Cambridge: MIT Press/Bradford<br />Books.<br />Wilks, S. (1962). Matematical statistics. New York: John Wiley and Sons.<br />Yakowitz, S., &amp; Lowe, W. (1991). Nonparametric bandit methods. Annals of Operations Research, 28, 297–<br />312.<br />Received September 29, 2000<br />Revised May 21, 2001<br />Accepted June 20, 2001<br />Final manuscript June 20, 2001</p>  <a href="https://www.researchgate.net/profile/Nicolo_Cesa-Bianchi/publication/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem/links/09e4150ec228dace78000000.pdf">Download full-text</a> </div> <div id="rgw22_56ab9ebb431f0" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw23_56ab9ebb431f0">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw24_56ab9ebb431f0"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.9211&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Finite-time Analysis of the Multiarmed Bandit Problem">Finite-time Analysis of the Multiarmed Bandit Prob...</a> </div>  <div class="details">   Available from <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.9211&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow">psu.edu</a>  </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw25_56ab9ebb431f0"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Nicolo_Cesa-Bianchi/publication/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem/links/09e4150ec228dace78000000.pdf" class="publication-viewer" title="09e4150ec228dace78000000.pdf">09e4150ec228dace78000000.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Nicolo_Cesa-Bianchi">Nicolò Cesa-Bianchi</a> &middot; Jan 20, 2016 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw26_56ab9ebb431f0"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.9211&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Finite-time Analysis of the Multiarmed Bandit Problem">Finite-time Analysis of the Multiarmed Bandit Prob...</a> </div>  <div class="details">   Available from <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.9211&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow">citeseerx.ist.psu.edu</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw28_56ab9ebb431f0" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw29_56ab9ebb431f0">  </ul> </div> </div>   <div id="rgw18_56ab9ebb431f0" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw19_56ab9ebb431f0"> <div> <h5> <a href="publication/224611951_A_Structured_Multiarmed_Bandit_Problem_and_the_Greedy_Policy" class="color-inherit ga-similar-publication-title"><span class="publication-title">A Structured Multiarmed Bandit Problem and the Greedy Policy</span></a>  </h5>  <div class="authors"> <a href="researcher/69953988_Adam_J_Mersereau" class="authors ga-similar-publication-author">Adam J. Mersereau</a>, <a href="researcher/8776177_Paat_Rusmevichientong" class="authors ga-similar-publication-author">Paat Rusmevichientong</a>, <a href="researcher/34424235_John_N_Tsitsiklis" class="authors ga-similar-publication-author">John N. Tsitsiklis</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw20_56ab9ebb431f0"> <div> <h5> <a href="publication/254005963_Mean_field_equilibria_of_multiarmed_bandit_games" class="color-inherit ga-similar-publication-title"><span class="publication-title">Mean field equilibria of multiarmed bandit games</span></a>  </h5>  <div class="authors"> <a href="researcher/11998071_Ramakrishna_Gummadi" class="authors ga-similar-publication-author">Ramakrishna Gummadi</a>, <a href="researcher/10959030_Ramesh_Johari" class="authors ga-similar-publication-author">Ramesh Johari</a>, <a href="researcher/2026783500_Jia_Yuan_Yu" class="authors ga-similar-publication-author">Jia Yuan Yu</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw21_56ab9ebb431f0"> <div> <h5> <a href="publication/35973452_Multiple_machine_maintenance_applying_a_separable_value_function_approximation_to_a_variation_of_the_multiarmed_bandit" class="color-inherit ga-similar-publication-title"><span class="publication-title">Multiple machine maintenance : applying a separable value function approximation to a variation of the multiarmed bandit /</span></a>  </h5>  <div class="authors"> <a href="researcher/47218312_Lin" class="authors ga-similar-publication-author">Lin</a>, <a href="researcher/2037536799_Haixia" class="authors ga-similar-publication-author">Haixia</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw42_56ab9ebb431f0" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw43_56ab9ebb431f0">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw44_56ab9ebb431f0" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=YXzbqWZYCA2ZFxekdLbNCYyH3loNN07HF6PChRL-oB5PHGZnbHLo2nKTz1mEopd-" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="opVpsc+l3b71+RIZxuZJtxWCoFm98PX0ol/VahQ4n/AMHQi3moEJVnH2guh23Z25PIZWBFnI6sKkE5G11Hv1hWwvGE0MQujOVasiv+2jOSACAHplZ3XYXfuV/GLYMUTskv62SaD2BVmJaVw23poNsxhXLhFSuL5gFzU4ODrryxqxdQY3AJMgvnULR7wVdju1Bwp64pl6VuvT5z67OOR+Aa7HXpEHld+EP42fEPb+uvYDvoEZ1e6Z/ZjejDzeh5ifF07JrfkaFs11kgvp2m9v8JN96fA7JJXbZ4Z8lTknlg0="/> <input type="hidden" name="urlAfterLogin" value="publication/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMzQzNzk2X0Zpbml0ZS10aW1lX0FuYWx5c2lzX29mX3RoZV9NdWx0aWFybWVkX0JhbmRpdF9Qcm9ibGVt"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMzQzNzk2X0Zpbml0ZS10aW1lX0FuYWx5c2lzX29mX3RoZV9NdWx0aWFybWVkX0JhbmRpdF9Qcm9ibGVt"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMzQzNzk2X0Zpbml0ZS10aW1lX0FuYWx5c2lzX29mX3RoZV9NdWx0aWFybWVkX0JhbmRpdF9Qcm9ibGVt"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw45_56ab9ebb431f0"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 611;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FigureList","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Peter Auer","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Peter_Auer","institution":"Montanuniversit\u00e4t Leoben","institutionUrl":false,"widgetId":"rgw4_56ab9ebb431f0"},"id":"rgw4_56ab9ebb431f0","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=1853311","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab9ebb431f0"},"id":"rgw3_56ab9ebb431f0","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=220343796","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":220343796,"title":"Finite-time Analysis of the Multiarmed Bandit Problem","journalTitle":"Machine Learning","journalDetailsTooltip":{"data":{"journalTitle":"Machine Learning","journalAbbrev":"MACH LEARN","publisher":"Springer Verlag","issn":"0885-6125","impactFactor":"1.89","fiveYearImpactFactor":"2.64","citedHalfLife":">10.0","immediacyIndex":"0.21","eigenFactor":"0.01","articleInfluence":"1.71","widgetId":"rgw6_56ab9ebb431f0"},"id":"rgw6_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=0885-6125","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":"DTI, University of Milan, I-26013, Crema, Italy","type":"Article","details":{"doi":"10.1023\/A:1013689704352","journalInfos":{"journal":"","publicationDate":"05\/2002;","publicationDateRobot":"2002-05","article":"47(2-3):235-256.","journalTitle":"Machine Learning","journalUrl":"journal\/0885-6125_Machine_Learning","impactFactor":1.89}},"source":{"sourceUrl":"http:\/\/dblp.uni-trier.de\/db\/journals\/ml\/ml47.html#AuerCF02","sourceName":"DBLP"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1023\/A:1013689704352"},{"key":"rft.atitle","value":"Finite-time Analysis of the Multiarmed Bandit Problem"},{"key":"rft.title","value":"Machine Learning"},{"key":"rft.jtitle","value":"Machine Learning"},{"key":"rft.volume","value":"47"},{"key":"rft.issue","value":"2-3"},{"key":"rft.date","value":"2002"},{"key":"rft.pages","value":"235-256"},{"key":"rft.issn","value":"0885-6125"},{"key":"rft.au","value":"Peter Auer,Nicol\u00f2 Cesa-Bianchi,Paul Fischer"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56ab9ebb431f0"},"id":"rgw7_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=220343796","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":220343796,"peopleItems":[{"data":{"authorNameOnPublication":"Peter Auer","accountUrl":"profile\/Peter_Auer","accountKey":"Peter_Auer","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Peter Auer","profile":{"professionalInstitution":{"professionalInstitutionName":"Montanuniversit\u00e4t Leoben","professionalInstitutionUrl":"institution\/Montanuniversitaet_Leoben"}},"professionalInstitutionName":"Montanuniversit\u00e4t Leoben","professionalInstitutionUrl":"institution\/Montanuniversitaet_Leoben","url":"profile\/Peter_Auer","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Peter_Auer","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw10_56ab9ebb431f0"},"id":"rgw10_56ab9ebb431f0","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1853311&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Montanuniversit\u00e4t Leoben","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":2,"publicationUid":220343796,"widgetId":"rgw9_56ab9ebb431f0"},"id":"rgw9_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1853311&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=2&publicationUid=220343796","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Nicol\u00f2 Cesa-Bianchi","accountUrl":"profile\/Nicolo_Cesa-Bianchi","accountKey":"Nicolo_Cesa-Bianchi","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A282101564887045%401444269662768_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Nicol\u00f2 Cesa-Bianchi","profile":{"professionalInstitution":{"professionalInstitutionName":"University of Milan","professionalInstitutionUrl":"institution\/University_of_Milan"}},"professionalInstitutionName":"University of Milan","professionalInstitutionUrl":"institution\/University_of_Milan","url":"profile\/Nicolo_Cesa-Bianchi","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A282101564887045%401444269662768_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Nicolo_Cesa-Bianchi","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw12_56ab9ebb431f0"},"id":"rgw12_56ab9ebb431f0","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1833357&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University of Milan","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":2,"publicationUid":220343796,"widgetId":"rgw11_56ab9ebb431f0"},"id":"rgw11_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1833357&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=2&publicationUid=220343796","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/6525015_Paul_Fischer","authorNameOnPublication":"Paul Fischer","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Paul Fischer","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/6525015_Paul_Fischer","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw14_56ab9ebb431f0"},"id":"rgw14_56ab9ebb431f0","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=6525015&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw13_56ab9ebb431f0"},"id":"rgw13_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=6525015&authorNameOnPublication=Paul%20Fischer","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab9ebb431f0"},"id":"rgw8_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=220343796&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":220343796,"abstract":"<noscript><\/noscript><div>Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration\/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw15_56ab9ebb431f0"},"id":"rgw15_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=220343796","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":{"data":{"figures":[{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig1\/Figure-1.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig1\/Figure-1_small.png","figureUrl":"\/figure\/220343796_fig1_Figure-1","selected":false,"title":"Figure 1 .\u00a0","key":"220343796_fig1_Figure-1"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig2\/Figure-2.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig2\/Figure-2_small.png","figureUrl":"\/figure\/220343796_fig2_Figure-2","selected":false,"title":"Figure 2 .\u00a0","key":"220343796_fig2_Figure-2"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig3\/Figure-3.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig3\/Figure-3_small.png","figureUrl":"\/figure\/220343796_fig3_Figure-3","selected":false,"title":"Figure 3 .\u00a0","key":"220343796_fig3_Figure-3"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig4\/Figure-4.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig4\/Figure-4_small.png","figureUrl":"\/figure\/220343796_fig4_Figure-4","selected":false,"title":"Figure 4 .\u00a0","key":"220343796_fig4_Figure-4"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig5\/Figure-5.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig5\/Figure-5_small.png","figureUrl":"\/figure\/220343796_fig5_Figure-5","selected":false,"title":"Figure 5 .\u00a0","key":"220343796_fig5_Figure-5"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig6\/Figure-6.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig6\/Figure-6_small.png","figureUrl":"\/figure\/220343796_fig6_Figure-6","selected":false,"title":"Figure 6 .\u00a0","key":"220343796_fig6_Figure-6"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig7\/Figure-7.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig7\/Figure-7_small.png","figureUrl":"\/figure\/220343796_fig7_Figure-7","selected":false,"title":"Figure 7 .\u00a0","key":"220343796_fig7_Figure-7"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig8\/Figure-8.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig8\/Figure-8_small.png","figureUrl":"\/figure\/220343796_fig8_Figure-8","selected":false,"title":"Figure 8 .\u00a0","key":"220343796_fig8_Figure-8"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig9\/Figure-9.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig9\/Figure-9_small.png","figureUrl":"\/figure\/220343796_fig9_Figure-9","selected":false,"title":"Figure 9 .\u00a0","key":"220343796_fig9_Figure-9"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig10\/Figure-10.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig10\/Figure-10_small.png","figureUrl":"\/figure\/220343796_fig10_Figure-10","selected":false,"title":"Figure 10 .\u00a0","key":"220343796_fig10_Figure-10"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig11\/Figure-11.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig11\/Figure-11_small.png","figureUrl":"\/figure\/220343796_fig11_Figure-11","selected":false,"title":"Figure 11 .\u00a0","key":"220343796_fig11_Figure-11"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig12\/Figure-12.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796\/figure\/fig12\/Figure-12_small.png","figureUrl":"\/figure\/220343796_fig12_Figure-12","selected":false,"title":"Figure 12 .\u00a0","key":"220343796_fig12_Figure-12"}],"readerDocId":"2811193","linkBehaviour":"dialog","isDialog":true,"headerText":"Figures in this publication","isNewPublicationDesign":false,"widgetId":"rgw16_56ab9ebb431f0"},"id":"rgw16_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/FigureList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FigureList.html?readerDocId=2811193&isDialog=1&linkBehaviour=dialog","viewClass":"views.publicliterature.FigureListView","yuiModules":["rg.views.publicliterature.FigureListView","css-pow-publicliterature-FigureList"],"stylesheets":["pow\/publicliterature\/FigureList.css"],"_isYUI":true},"previewImage":"https:\/\/i1.rgstatic.net\/publication\/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem\/links\/09e4150ec228dace78000000\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw17_56ab9ebb431f0"},"id":"rgw17_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab9ebb431f0"},"id":"rgw5_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=220343796&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":69953988,"url":"researcher\/69953988_Adam_J_Mersereau","fullname":"Adam J. Mersereau","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8776177,"url":"researcher\/8776177_Paat_Rusmevichientong","fullname":"Paat Rusmevichientong","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":34424235,"url":"researcher\/34424235_John_N_Tsitsiklis","fullname":"John N. Tsitsiklis","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2010","journal":"IEEE Transactions on Automatic Control","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/224611951_A_Structured_Multiarmed_Bandit_Problem_and_the_Greedy_Policy","usePlainButton":true,"publicationUid":224611951,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.78","url":"publication\/224611951_A_Structured_Multiarmed_Bandit_Problem_and_the_Greedy_Policy","title":"A Structured Multiarmed Bandit Problem and the Greedy Policy","displayTitleAsLink":true,"authors":[{"id":69953988,"url":"researcher\/69953988_Adam_J_Mersereau","fullname":"Adam J. Mersereau","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8776177,"url":"researcher\/8776177_Paat_Rusmevichientong","fullname":"Paat Rusmevichientong","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":34424235,"url":"researcher\/34424235_John_N_Tsitsiklis","fullname":"John N. Tsitsiklis","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["IEEE Transactions on Automatic Control 01\/2010; 54(12-54):2787 - 2802. DOI:10.1109\/TAC.2009.2031725"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/224611951_A_Structured_Multiarmed_Bandit_Problem_and_the_Greedy_Policy","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/224611951_A_Structured_Multiarmed_Bandit_Problem_and_the_Greedy_Policy\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab9ebb431f0"},"id":"rgw19_56ab9ebb431f0","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=224611951","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":11998071,"url":"researcher\/11998071_Ramakrishna_Gummadi","fullname":"Ramakrishna Gummadi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10959030,"url":"researcher\/10959030_Ramesh_Johari","fullname":"Ramesh Johari","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2026783500,"url":"researcher\/2026783500_Jia_Yuan_Yu","fullname":"Jia Yuan Yu","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Apr 2013","journal":"SSRN Electronic Journal","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/254005963_Mean_field_equilibria_of_multiarmed_bandit_games","usePlainButton":true,"publicationUid":254005963,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/254005963_Mean_field_equilibria_of_multiarmed_bandit_games","title":"Mean field equilibria of multiarmed bandit games","displayTitleAsLink":true,"authors":[{"id":11998071,"url":"researcher\/11998071_Ramakrishna_Gummadi","fullname":"Ramakrishna Gummadi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10959030,"url":"researcher\/10959030_Ramesh_Johari","fullname":"Ramesh Johari","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2026783500,"url":"researcher\/2026783500_Jia_Yuan_Yu","fullname":"Jia Yuan Yu","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["SSRN Electronic Journal 04\/2013;  DOI:10.1145\/2229012.2229060"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/254005963_Mean_field_equilibria_of_multiarmed_bandit_games","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/254005963_Mean_field_equilibria_of_multiarmed_bandit_games\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56ab9ebb431f0"},"id":"rgw20_56ab9ebb431f0","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=254005963","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":47218312,"url":"researcher\/47218312_Lin","fullname":"Lin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2037536799,"url":"researcher\/2037536799_Haixia","fullname":"Haixia","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/35973452_Multiple_machine_maintenance_applying_a_separable_value_function_approximation_to_a_variation_of_the_multiarmed_bandit","usePlainButton":true,"publicationUid":35973452,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/35973452_Multiple_machine_maintenance_applying_a_separable_value_function_approximation_to_a_variation_of_the_multiarmed_bandit","title":"Multiple machine maintenance : applying a separable value function approximation to a variation of the multiarmed bandit \/","displayTitleAsLink":true,"authors":[{"id":47218312,"url":"researcher\/47218312_Lin","fullname":"Lin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2037536799,"url":"researcher\/2037536799_Haixia","fullname":"Haixia","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/35973452_Multiple_machine_maintenance_applying_a_separable_value_function_approximation_to_a_variation_of_the_multiarmed_bandit","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/35973452_Multiple_machine_maintenance_applying_a_separable_value_function_approximation_to_a_variation_of_the_multiarmed_bandit\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw21_56ab9ebb431f0"},"id":"rgw21_56ab9ebb431f0","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=35973452","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw18_56ab9ebb431f0"},"id":"rgw18_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=220343796&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":220343796,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":220343796,"publicationType":"article","linkId":"00045c830cf2847a19eec044","fileName":"Finite-time Analysis of the Multiarmed Bandit Problem","fileUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.98.9211&amp;rep=rep1&amp;type=pdf","name":"psu.edu","nameUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.98.9211&amp;rep=rep1&amp;type=pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw24_56ab9ebb431f0"},"id":"rgw24_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=220343796&linkId=00045c830cf2847a19eec044&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":220343796,"publicationType":"article","linkId":"09e4150ec228dace78000000","fileName":"09e4150ec228dace78000000.pdf","fileUrl":"profile\/Nicolo_Cesa-Bianchi\/publication\/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem\/links\/09e4150ec228dace78000000.pdf","name":"Nicol\u00f2 Cesa-Bianchi","nameUrl":"profile\/Nicolo_Cesa-Bianchi","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Jan 20, 2016","fileSize":"202.98 KB","widgetId":"rgw25_56ab9ebb431f0"},"id":"rgw25_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=220343796&linkId=09e4150ec228dace78000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":220343796,"publicationType":"article","linkId":"0fe14b4a0cf2bf000eda2ff3","fileName":"Finite-time Analysis of the Multiarmed Bandit Problem","fileUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.98.9211&amp;rep=rep1&amp;type=pdf","name":"citeseerx.ist.psu.edu","nameUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.98.9211&amp;rep=rep1&amp;type=pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw26_56ab9ebb431f0"},"id":"rgw26_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=220343796&linkId=0fe14b4a0cf2bf000eda2ff3&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw23_56ab9ebb431f0"},"id":"rgw23_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=220343796&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":3,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":138,"valueFormatted":"138","widgetId":"rgw27_56ab9ebb431f0"},"id":"rgw27_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=220343796","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw22_56ab9ebb431f0"},"id":"rgw22_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=220343796&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":220343796,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw29_56ab9ebb431f0"},"id":"rgw29_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=220343796&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":138,"valueFormatted":"138","widgetId":"rgw30_56ab9ebb431f0"},"id":"rgw30_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=220343796","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw28_56ab9ebb431f0"},"id":"rgw28_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=220343796&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Machine Learning, 47, 235\u2013256, 2002\nc ? 2002 Kluwer Academic Publishers. Manufactured in The Netherlands.\nFinite-time Analysis of the Multiarmed Bandit\nProblem*\nPETER AUER\nUniversity of Technology Graz, A-8010 Graz, Austria\npauer@igi.tu-graz.ac.at\nNICOL`O CESA-BIANCHI\nDTI, University of Milan, via Bramante 65, I-26013 Crema, Italy\ncesa-bianchi@dti.unimi.it\nPAUL FISCHER\nLehrstuhl Informatik II, Universit\u00a8 at Dortmund, D-44221 Dortmund, Germany\nfischer@ls2.informatik.uni-dortmund.de\nEditor: Jyrki Kivinen\nAbstract.\na balance between exploring the environment to find profitable actions while taking the empirically best action as\noften as possible. A popular measure of a policy\u2019s success in addressing this dilemma is the regret, that is the loss\ndue to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the\nexploration\/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show\nthat the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies\nwhich asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we\nshow that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies,\nand for all reward distributions with bounded support.\nReinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for\nKeywords:\nbandit problems, adaptive allocation rules, finite horizon regret\n1.Introduction\nThe exploration versus exploitation dilemma can be described as the search for a balance\nbetween exploring the environment to find profitable actions while taking the empirically\nbest action as often as possible. The simplest instance of this dilemma is perhaps the\nmulti-armedbandit,aproblemextensivelystudiedinstatistics(Berry&Fristedt,1985)that\nhas also turned out to be fundamental in different areas of artificial intelligence, such as\nreinforcement learning (Sutton & Barto, 1998) and evolutionary programming (Holland,\n1992).\nIn its most basic formulation, a K-armed bandit problem is defined by random variables\nXi,nfor 1\u2264i \u2264 K and n \u22651, where each i is the index of a gambling machine (i.e., the\n\u201carm\u201d of a bandit). Successive plays of machine i yield rewards Xi,1, Xi,2,... which are\n\u2217ApreliminaryversionappearedinProc.of15thInternationalConferenceonMachineLearning,pages100\u2013108.\nMorgan Kaufmann, 1998"},{"page":2,"text":"236\nP. AUER, N. CESA-BIANCHI AND P. FISCHER\nindependent and identically distributed according to an unknown law with unknown ex-\npectation \u00b5i. Independence also holds for rewards across machines; i.e., Xi,sand Xj,tare\nindependent (and usually not identically distributed) for each 1 \u2264 i < j \u2264 K and each\ns,t \u2265 1.\nA policy, or allocation strategy, A is an algorithm that chooses the next machine to play\nbased on the sequence of past plays and obtained rewards. Let Ti(n) be the number of times\nmachine i has been played by A during the first n plays. Then the regret of A after n plays\nis defined by\n\u00b5\u2217n \u2212 \u00b5j\nK\n?\nj=1\nIE[Tj(n)]where \u00b5\u2217def\n= max\n1\u2264i\u2264K\u00b5i\nand IE[\u00b7] denotes expectation. Thus the regret is the expected loss due to the fact that the\npolicy does not always play the best machine.\nIn their classical paper, Lai and Robbins (1985) found, for specific families of reward\ndistributions (indexed by a single real parameter), policies satisfying\nIE[Tj(n)] \u2264\n?\n1\nD(pj?p\u2217)+ o(1)\n?\nlnn\n(1)\nwhere o(1) \u2192 0 as n \u2192 \u221e and\nD(pj?p\u2217)\ndef\n=\n?\npjlnpj\np\u2217\nis the Kullback-Leibler divergence between the reward density pjof any suboptimal ma-\nchine j and the reward density p\u2217of the machine with highest reward expectation \u00b5\u2217.\nHence, under these policies the optimal machine is played exponentially more often than\nany other machine, at least asymptotically. Lai and Robbins also proved that this regret is\nthe best possible. Namely, for any allocation strategy and for any suboptimal machine j,\nIE[Tj(n)]\u2265(lnn)\/D(pj?p\u2217) asymptotically, provided that the reward distributions satisfy\nsome mild assumptions.\nThese policies work by associating a quantity called upper confidence index to each ma-\nchine.Thecomputationofthisindexisgenerallyhard.Infact,itreliesontheentiresequence\nof rewards obtained so far from a given machine. Once the index for each machine is com-\nputed, the policy uses it as an estimate for the corresponding reward expectation, picking\nforthenextplaythemachinewiththecurrenthighestindex.Morerecently,Agrawal(1995)\nintroduced a family of policies where the index can be expressed as simple function of\nthe total reward obtained so far from the machine. These policies are thus much easier to\ncompute than Lai and Robbins\u2019, yet their regret retains the optimal logarithmic behavior\n(though with a larger leading constant in some cases).1\nIn this paper we strengthen previous results by showing policies that achieve logarithmic\nregret uniformly over time, rather than only asymptotically. Our policies are also simple to\nimplement and computationally efficient. In Theorem 1 we show that a simple variant of\nAgrawal\u2019s index-based policy has finite-time regret logarithmically bounded for arbitrary\nsets of reward distributions with bounded support (a regret with better constants is proven"},{"page":3,"text":"FINITE-TIME ANALYSIS\n237\nin Theorem 2 for a more complicated version of this policy). A similar result is shown\nin Theorem 3 for a variant of the well-known randomized \u03b5-greedy heuristic. Finally, in\nTheorem 4 we show another index-based policy with logarithmically bounded finite-time\nregret for the natural case when the reward distributions are normally distributed with\nunknown means and variances.\nThroughout the paper, and whenever the distributions of rewards for each machine are\nunderstood from the context, we define\n?i\ndef\n= \u00b5\u2217\u2212 \u00b5i\nwhere, we recall, \u00b5iis the reward expectation for machinei and \u00b5\u2217is any maximal element\nin the set {\u00b51,...,\u00b5K}.\n2.Main results\nOur first result shows that there exists an allocation strategy, UCB1, achieving logarithmic\nregret uniformly over n and without any preliminary knowledge about the reward distri-\nbutions (apart from the fact that their support is in [0,1]). The policy UCB1 (sketched in\nfigure 1) is derived from the index-based policy of Agrawal (1995). The index of this policy\nisthesumoftwoterms.Thefirsttermissimplythecurrentaveragereward.Thesecondterm\nis related to the size (according to Chernoff-Hoeffding bounds, see Fact 1) of the one-sided\nconfidence interval for the average reward within which the true expected reward falls with\noverwhelming probability.\nTheorem 1.\ndistributions P1,..., PKwith support in [0,1], then its expected regret after any number\nn of plays is at most\nFor all K > 1, if policy UCB1 is run on K machines having arbitrary reward\n?\n8\n?\ni:\u00b5i<\u00b5\u2217\n?lnn\n?i\n??\n+\n?\n1 +\u03c02\n3\n??\nK\n?\nj=1\n?j\n?\nwhere \u00b51,...,\u00b5Kare the expected values of P1,..., PK.\nFigure 1.Sketch of the deterministic policy UCB1 (see Theorem 1)."},{"page":4,"text":"238\nP. AUER, N. CESA-BIANCHI AND P. FISCHER\nFigure 2. Sketch of the deterministic policy UCB2 (see Theorem 2).\nTo prove Theorem 1 we show that, for any suboptimal machine j,\nIE[Tj(n)] \u2264\n8\n?2\nj\nlnn\n(2)\nplus a small constant. The leading constant 8\/?2\n1\/D(pj? p\u2217) in Lai and Robbins\u2019 result (1). In fact, one can show that D(pj? p\u2217) \u2265 2?2\nwhere the constant 2 is the best possible.\nUsingaslightlymorecomplicatedpolicy,whichwecall UCB2(seefigure2),wecanbring\nthe main constant of (2) arbitrarily close to 1\/(2?2\nThe plays are divided in epochs. In each new epoch a machine i is picked and then\nplayed \u03c4(ri+ 1) \u2212 \u03c4(ri) times, where \u03c4 is an exponential function and riis the number of\nepochs played by that machine so far. The machine picked in each new epoch is the one\nmaximizing \u00af xi+ an,ri, where n is the current number of plays, \u00af xiis the current average\nreward for machine i, and\niis worse than the corresponding constant\nj\nj). The policy UCB2 works as follows.\nan,r=\n?\n(1 + \u03b1)ln(en\/\u03c4(r))\n2\u03c4(r)\n(3)\nwhere\n\u03c4(r) = ?(1 + \u03b1)r?.\nIn the next result we state a bound on the regret of UCB2. The constant c\u03b1, here left unspec-\nified, is defined in (18) in the appendix, where the theorem is also proven.\nTheorem 2.\nhaving arbitrary reward distributions P1,..., PKwith support in [0,1], then its expected\nregret after any number\nFor all K > 1, if policy UCB2 is run with input 0 < \u03b1 < 1 on K machines\nn \u2265 max\ni:\u00b5i<\u00b5\u2217\n1\n2?2\ni"},{"page":5,"text":"FINITE-TIME ANALYSIS\n239\nof plays is at most\n?\ni :\u00b5i<\u00b5\u2217\n?(1 + \u03b1)(1 + 4\u03b1)ln?2e?2\nin?\n2?i\n+c\u03b1\n?i\n?\n(4)\nwhere \u00b51,...,\u00b5Kare the expected values of P1,..., PK.\nRemark.\ntrarily close to 1\/(2?2\ntraded-off by letting \u03b1 = \u03b1nbe slowly decreasing with the number n of plays.\nBy choosing \u03b1 small, the constant of the leading term in the sum (4) gets arbi-\ni); however, c\u03b1\u2192 \u221e as \u03b1 \u2192 0. The two terms in the sum can be\nA simple and well-known policy for the bandit problem is the so-called \u03b5-greedy rule\n(seeSutton,&Barto,1998).Thispolicyprescribestoplaywithprobability1\u2212\u03b5themachine\nwiththehighestaveragereward,andwithprobability\u03b5arandomlychosenmachine.Clearly,\nthe constant exploration probability \u03b5 causes a linear (rather than logarithmic) growth in\nthe regret. The obvious fix is to let \u03b5 go to zero with a certain rate, so that the exploration\nprobability decreases as our estimates for the reward expectations become more accurate.\nIt turns out that a rate of 1\/n, where n is, as usual, the index of the current play, allows\nto prove a logarithmic bound on the regret. The resulting policy, \u03b5n-GREEDY, is shown in\nfigure 3.\nTheorem 3.\n[0,1], if policy \u03b5n-GREEDY is run with input parameter\nFor all K >1 and for all reward distributions P1,..., PKwith support in\n0 < d \u2264 min\ni:\u00b5i<\u00b5\u2217?i,\nFigure 3.Sketch of the randomized policy \u03b5n-GREEDY (see Theorem 3)."},{"page":6,"text":"240\nP. AUER, N. CESA-BIANCHI AND P. FISCHER\nthen the probability that after any number n \u2265cK\/d of plays \u03b5n-GREEDY chooses a subop-\ntimal machine j is at most\nc\nd2n+ 2\n?c\n?\nd2ln(n \u2212 1)d2e1\/2\ncK\n(n \u2212 1)d2e1\/2\nForc largeenough(e.g.c > 5)theaboveboundisoforderc\/(d2n)+o(1\/n)for\nn \u2192 \u221e, as the second and third terms in the bound are O(1\/n1+\u03b5) for some \u03b5 > 0 (recall\nthat 0 < d < 1). Note also that this is a result stronger than those of Theorems 1 and 2, as\nit establishes a bound on the instantaneous regret. However, unlike Theorems 1 and 2, here\nwe need to know a lower bound d on the difference between the reward expectations of the\nbest and the second best machine.\ncK\n??\ncK\n(n \u2212 1)d2e1\/2\n?c\/(5d2)\n+4e\nd2\n?c\/2\n.\nRemark.\nOur last result concerns a special case, i.e. the bandit problem with normally distributed\nrewards. Surprisingly, we could not find in the literature regret bounds (not even asymp-\ntotical) for the case when both the mean and the variance of the reward distributions are\nunknown. Here, we show that an index-based policy called UCB1-NORMAL, see figure 4,\nachieves logarithmic regret uniformly over n without knowing means and variances of the\nreward distributions. However, our proof is based on certain bounds on the tails of the \u03c72\nand the Student distribution that we could only verify numerically. These bounds are stated\nas Conjecture 1 and Conjecture 2 in the Appendix.\nThe choice of the index in UCB1-NORMAL is based, as for UCB1, on the size of the one-\nsidedconfidenceintervalfortheaveragerewardwithinwhichthetrueexpectedrewardfalls\nwith overwhelming probability. In the case of UCB1, the reward distribution was unknown,\nand we used Chernoff-Hoeffding bounds to compute the index. In this case we know that\nFigure 4. Sketch of the deterministic policy UCB1-NORMAL (see Theorem 4)."},{"page":7,"text":"FINITE-TIME ANALYSIS\n241\nthe distribution is normal, and for computing the index we use the sample variance as an\nestimate of the unknown variance.\nTheorem 4.\nreward distributions P1,..., PK, then its expected regret after any number n of plays is at\nmost\nFor all K > 1, if policy UCB1-NORMAL is run on K machines having normal\n256(logn)\n?\n?\ni:\u00b5i<\u00b5\u2217\n\u03c32\ni\n?i\n?\n+\n?\n1 +\u03c02\n2\n+ 8 logn\n??\nK\n?\nj=1\n?j\n?\nwhere \u00b51,...,\u00b5K and \u03c32\nP1,..., PK.\n1,...,\u03c32\nKare the means and variances of the distributions\nAs a final remark for this section, note that Theorems 1\u20133 also hold for rewards that are not\nindependent across machines, i.e. Xi,sand Xj,tmight be dependent for any s, t, and i ?= j.\nFurthermore, we also do not need that the rewards of a single arm are i.i.d., but only the\nweaker assumption that IE[Xi,t| Xi,1,..., Xi,t\u22121] = \u00b5ifor all 1 \u2264 t \u2264 n.\n3. Proofs\nRecall that, for each 1 \u2264 i \u2264 K, IE[Xi,n] = \u00b5ifor all n \u2265 1 and \u00b5\u2217= max1\u2264i\u2264K\u00b5i. Also,\nfor any fixed policy A, Ti(n) is the number of times machine i has been played by A in the\nfirstn plays.Ofcourse,wealwayshave?K\nFor each 1 \u2264 i \u2264 K and n \u2265 1 define\n\u00afXi,n=1\nn\nt=1\nGiven \u00b51,...,\u00b5K, we call optimal the machine with the least index i such that \u00b5i= \u00b5\u2217.\nIn what follows, we will always put a superscript \u201c\u2217\u201d to any quantity which refers to the\noptimal machine. For example we write T\u2217(n) and\u00afX\u2217\nthe index of the optimal machine.\nSome further notation: For any predicate ? we define {?(x)} to be the indicator fuction\nof the event ?(x); i.e., {?(x)} = 1 if ?(x) is true and {?(x)} = 0 otherwise. Finally,\nVar[X] denotes the variance of the random variable X.\nNote that the regret after n plays can be written as\ni=1Ti(n) = n.Wealsodefinether.v.\u2019s I1, I2,...,\nwhere Itdenotes the machine played at time t.\nn ?\nXi,t.\nninstead of Ti(n) and\u00afXi,n, where i is\n?\nj:\u00b5j<\u00b5\u2217\n?jIE[Tj(n)] (5)\nSo we can bound the regret by simply bounding each IE[Tj(n)].\nWe will make use of the following standard exponential inequalities for bounded random\nvariables (see, e.g., the appendix of Pollard, 1984)."},{"page":8,"text":"242\nP. AUER, N. CESA-BIANCHI AND P. FISCHER\nFact 1 (Chernoff-Hoeffding bound). Let X1,..., Xnbe random variables with common\nrange [0,1] and such that IE[Xt|X1,..., Xt\u22121] = \u00b5. Let Sn= X1+ \u00b7\u00b7\u00b7 + Xn. Then for\nall a \u2265 0\nIP{Sn\u2265 n\u00b5 + a} \u2264 e\u22122a2\/n\nFact 2 (Bernstein inequality). Let X1,..., Xnbe random variables with range [0,1] and\nand\nIP{Sn\u2264 n\u00b5 \u2212 a} \u2264 e\u22122a2\/n\nn ?\nt=1\nVar[Xt| Xt\u22121,..., X1] = \u03c32.\nLet Sn= X1+ \u00b7\u00b7\u00b7 + Xn. Then for all a \u2265 0\nIP{Sn\u2265 IE[Sn] + a} \u2264 exp\n?\n\u2212\na2\/2\n\u03c32+ a\/2\n?\n.\nProof of Theorem 1:\non any sequence of plays. More precisely, for each t \u2265 1 we bound the indicator function\nof It= i as follows. Let ? be an arbitrary positive integer.\nn ?\n\u2264 ? +\nt=K+1\nn ?\n+ ct\u22121,Ti(t\u22121), Ti(t \u2212 1) \u2265 ??\n\u2264 ? +\nt=K+1\n\u221e\n?\nNow observe that\u00afX\u2217\nhold\nLet ct,s=\u221a(2lnt)\/s. For any machine i, we upper bound Ti(n)\nTi(n) = 1 +\nt=K+1\nn ?\n{It= i}\n{It= i, Ti(t \u2212 1) \u2265 ?}\n\u2264 ? +\nt=K+1\n?\u00afX\u2217\nT\u2217(t\u22121)+ ct\u22121,T\u2217(t\u22121)\u2264\u00afXi,Ti(t\u22121)\nn ?\n?\nmin\n0<s<t\n\u00afX\u2217\ns+ ct\u22121,s\u2264 max\n?\u2264si<t\n\u00afXi,si+ ct\u22121,si\n?\n\u2264 ? +\nt=1\nt\u22121\n?\ns=1\nt\u22121\n?\nsi=?\n?\u00afX\u2217\ns+ ct,s\u2264\u00afXi,si+ ct,si\n?.\n(6)\ns+ ct,s\u2264\u00afXi,si+ ct,siimplies that at least one of the following must\n\u00afX\u2217\ns\u2264 \u00b5\u2217\u2212 ct,s\n\u00afXi,si\u2265 \u00b5i+ ct,si\n\u00b5\u2217< \u00b5i+ 2ct,si.\nWe bound the probability of events (7) and (8) using Fact 1 (Chernoff-Hoeffding bound)\n(7)\n(8)\n(9)\nIP{\u00afX\u2217\ns\u2264 \u00b5\u2217\u2212 ct,s} \u2264 e\u22124lnt= t\u22124"},{"page":9,"text":"FINITE-TIME ANALYSIS\n243\nIP?\u00afXi,si\u2265 \u00b5i+ ct,si\nFor ? = ?(8lnn)\/?2\n\u00b5\u2217\u2212 \u00b5i\u2212 2ct,si= \u00b5\u2217\u2212 \u00b5i\u2212 2\nfor si\u2265 (8lnn)\/?2\n?\u2264 e\u22124lnt= t\u22124.\ni?, (9) is false. In fact\n?\n2(lnt)\/si\u2265 \u00b5\u2217\u2212 \u00b5i\u2212 ?i= 0\ni. So we get\nIE[Ti(n)] \u2264\n?8lnn\n\u00d7?IP{\u00afX\u2217\n?8lnn\n\u22648lnn\n?2\ni\n?2\ni\n?\n+\n\u221e\n?\nt=1\nt\u22121\n?\ns=1\nt\u22121\n?\nsi=?(8lnn)\/?2\ni?\ns\u2264 \u00b5\u2217\u2212 ct,s} + IP?\u00afXi,si\u2265 \u00b5i+ ct,si\n?\n+ 1 +\u03c02\n3\n??\n\u2264\n?2\ni\n+\n\u221e\n?\nt=1\nt ?\ns=1\nt ?\nsi=1\n2t\u22124\nwhich concludes the proof.\n\u2737\nProof of Theorem 3:\nRecall that, for n \u2265 cK\/d2, \u03b5n= cK\/(d2n). Let\nx0=\n1\n2K\nn ?\nt=1\n\u03b5t.\nThe probability that machine j is chosen at time n is\nIP{In= j} \u2264\u03b5n\nK+\n?\n1 \u2212\u03b5n\nK\n?\nIP?\u00afXj,Tj(n\u22121)\u2265\u00afX\u2217\nT\u2217(n\u22121)\n?\nand\nIP?\u00afXj,Tj(n)\u2265\u00afX\u2217\n\u2264 IP\nT\u2217(n)\n?\n?\n\u00afXj,Tj(n)\u2265 \u00b5j+?j\n2\n?\n+ IP\n?\n\u00afX\u2217\nT\u2217(n)\u2264 \u00b5\u2217\u2212?j\n2\n?\n.\n(10)\nNowtheanalysisforbothtermsontheright-handsideisthesame.Let TR\nof plays in which machine j was chosen at random in the first n plays. Then we have\nj(n)bethenumber\nIP\n?\nn ?\n\u00afXj,Tj(n)\u2265 \u00b5j+?j\n2\n?\n=\nt=1\nIP\n?\nTj(n) = t \u2227\u00afXj,t\u2265 \u00b5j+?j\n2\n?\n(11)"},{"page":10,"text":"244\nP. AUER, N. CESA-BIANCHI AND P. FISCHER\n=\nn ?\nn ?\nby Fact 1 (Chernoff-Hoeffding bound)\nt=1\nIP\n?\n?\nTj(n) = t |\u00afXj,t\u2265 \u00b5j+?j\n2\n?\n?\n\u00b7 IP\n?\n\u00afXj,t\u2265 \u00b5j+?j\n2\n?\n\u2264\nt=1\nIPTj(n) = t |\u00afXj,t\u2265 \u00b5j+?j\n2\n\u00b7 e\u2212?2\njt\/2\n\u2264\n?x0?\n?\nsince?\u221e\n?\n\u2264 x0\u00b7 IP?TR\nt=1\nIP\n?\nTj(n) = t |\u00afXj,t\u2265 \u00b5j+?j\n2\n?\n+\n2\n?2\nj\ne\u2212?2\nj?x0?\/2\nt=x+1e\u2212\u03bat\u22641\nj(n) \u2264 t |\u00afXj,t\u2265 \u00b5j+?j\n\u03bae\u2212\u03bax\n\u2264\n?x0?\nt=1\nIP\n?\nTR\n2\n?\n+\n2\n?2\nj\ne\u2212?2\nj?x0?\/2\nj(n) \u2264 x0\n?+\n2\n?2\nj\ne\u2212?2\nj?x0?\/2\n(12)\nwhereinthelastlinewedroppedtheconditioningbecauseeachmachineisplayedatrandom\nindependently of the previous choices of the policy. Since\nIE?TR\nj(n)?=\n1\nK\nn ?\nt=1\n\u03b5t\nand\nVar?TR\nj(n)?=\nn ?\nt=1\n\u03b5t\nK\n?\n1 \u2212\u03b5t\nK\n?\n\u2264\n1\nK\nn ?\nt=1\n\u03b5t,\nby Bernstein\u2019s inequality (2) we get\nIP?TR\nj(n) \u2264 x0\n?\u2264 e\u2212x0\/5.\n(13)\nFinally it remains to lower bound x0. For n \u2265 n?= cK\/d2, \u03b5n= cK\/(d2n) and we have\nx0=\n1\n2K\nn ?\nn?\n?\nt=1\n\u03b5t\n=\n1\n2K\nt=1\n\u03b5t+\n1\n2K\nn ?\nt=n?+1\n\u03b5t\n\u2265\nn?\n2K+\nc\nd2lnnd2e1\/2\nc\nd2lnn\nn?\n\u2265\ncK\n."},{"page":11,"text":"FINITE-TIME ANALYSIS\n245\nThus, using (10)\u2013(13) and the above lower bound on x0we obtain\nIP{In= j} \u2264\u03b5n\nK+ 2x0e\u2212x0\/5+\n4\n?2\nj\ne\u2212?2\nj?x0?\/2\n\u2264\nc\nd2n+ 2\n+4e\nd2\n?c\nd2ln(n \u2212 1)d2e1\/2\ncK\n(n \u2212 1)d2e1\/2\ncK\n?c\/2\n??\ncK\n(n \u2212 1)d2e1\/2\n?c\/(5d2)\n?\n.\nThis concludes the proof.\n\u2737\n4.Experiments\nFor practical purposes, the bound of Theorem 1 can be tuned more finely. We use\nVj(s)\ndef\n=\n?\n1\ns\ns ?\n\u03c4=1\nX2\nj,\u03c4\n?\n\u2212\u00afX2\nj,s+\n?2lnt\ns\nas un upper confidence bound for the variance of machine j. As before, this means that\nmachine j, which has been played s times during the first t plays, has a variance that is\nat most the sample variance plus\u221a(2lnt)\/s. We then replace the upper confidence bound\n?2ln(n)\/njof policy UCB1 with\n?\nnj\nlnn\nmin{1\/4,Vj(nj)}\n(the factor 1\/4 is an upper bound on the variance of a Bernoulli random variable). This\nvariant, which we call UCB1-TUNED, performs substantially better than UCB1 in essentially\nall of our experiments. However, we are not able to prove a regret bound.\nWe compared the empirical behaviour policies UCB1-TUNED, UCB2, and \u03b5n-GREEDY on\nBernoulli reward distributions with different parameters shown in the table below.\n123456789 10\n1 0.90.6\n2 0.90.8\n30.55 0.45\n11 0.90.60.6 0.60.60.6 0.6 0.6 0.60.6\n120.9 0.80.8 0.80.7 0.7 0.70.6 0.60.6\n13 0.9 0.80.8 0.8 0.80.8 0.8 0.80.80.8\n14 0.550.450.450.450.450.45 0.450.450.45 0.45"},{"page":12,"text":"246\nP. AUER, N. CESA-BIANCHI AND P. FISCHER\nRows 1\u20133 define reward distributions for a 2-armed bandit problem, whereas rows 11\u2013\n14 define reward distributions for a 10-armed bandit problem. The entries in each row\ndenote the reward expectations (i.e. the probabilities of getting a reward 1, as we work with\nBernoulli distributions) for the machines indexed by the columns. Note that distributions 1\nand 11 are \u201ceasy\u201d (the reward of the optimal machine has low variance and the differences\n\u00b5\u2217\u2212\u00b5iare all large), whereas distributions 3 and 14 are \u201chard\u201d (the reward of the optimal\nmachine has high variance and some of the differences \u00b5\u2217\u2212 \u00b5iare small).\nWe made experiments to test the different policies (or the same policy with different\ninput parameters) on the seven distributions listed above. In each experiment we tracked\ntwo performance measures: (1) the percentage of plays of the optimal machine; (2) the\nactual regret, that is the difference between the reward of the optimal machine and the\nreward of the machine played. The plot for each experiment shows, on a semi-logarithmic\nscale, the behaviour of these quantities during 100,000 plays averaged over 100 different\nruns. We ran a first round of experiments on distribution 2 to find out good values for the\nparameters of the policies. If a parameter is chosen too small, then the regret grows linearly\n(exponentially in the semi-logarithmic plot); if a parameter is chosen too large then the\nregret grows logarithmically, but with a large leading constant (corresponding to a steep\nline in the semi-logarithmic plot).\nPolicy UCB2 is relatively insensitive to the choice of its parameter \u03b1, as long as it is\nkept relatively small (see figure 5). A fixed value 0.001 has been used for all the remaining\nexperiments. On other hand, the choice of c in policy \u03b5n-GREEDY is difficult as there is no\nvalue that works reasonably well for all the distributions that we considered. Therefore, we\nhave roughly searched for the best value for each distribution. In the plots, we will also\nshowtheperformanceof\u03b5n-GREEDYforvaluesofc aroundthisempiricallybestvalue.This\nshows that the performance degrades rapidly if this parameter is not appropriately tuned.\nFinally, in each experiment the parameter d of \u03b5n-GREEDY was set to\n? = \u00b5\u2217\u2212 max\ni:\u00b5i<\u00b5\u2217\u00b5i.\nFigure 5.Search for the best value of parameter \u03b1 of policy UCB2."},{"page":13,"text":"FINITE-TIME ANALYSIS\n247\n4.1. Comparison between policies\nWe can summarize the comparison of all the policies on the seven distributions as follows\n(see Figs. 6\u201312).\n\u2013 An optimally tuned \u03b5n-GREEDY performs almost always best. Significant exceptions are\ndistributions12and14:thisisbecause\u03b5n-GREEDYexploresuniformlyoverallmachines,\nthus the policy is hurt if there are several nonoptimal machines, especially when their\nreward expectations differ a lot. Furthermore, if \u03b5n-GREEDY is not well tuned its perfor-\nmance degrades rapidly (except for distribution 13, on which \u03b5n-GREEDY performs well\na wide range of values of its parameter).\n\u2013 In most cases, UCB1-TUNED performs comparably to a well-tuned \u03b5n-GREEDY. Further-\nmore, UCB1-TUNED is not very sensitive to the variance of the machines, that is why it\nperforms similarly on distributions 2 and 3, and on distributions 13 and 14.\n\u2013 Policy UCB2 performs similarly to UCB1-TUNED, but always slightly worse.\nFigure 6. Comparison on distribution 1 (2 machines with parameters 0.9,0.6).\nFigure 7. Comparison on distribution 2 (2 machines with parameters 0.9,0.8)."},{"page":14,"text":"248\nP. AUER, N. CESA-BIANCHI AND P. FISCHER\nFigure 8.Comparison on distribution 3 (2 machines with parameters 0.55,0.45).\nFigure 9.Comparison on distribution 11 (10 machines with parameters 0.9,0.6,...,0.6).\nFigure 10.\n0.6,0.6).\nComparison on distribution 12 (10 machines with parameters 0.9,0.8,0.8,0.8,0.7,0.7,0.7,0.6,"},{"page":15,"text":"FINITE-TIME ANALYSIS\n249\nFigure 11.Comparison on distribution 13 (10 machines with parameters 0.9,0.8,...,0.8).\nFigure 12. Comparison on distribution 14 (10 machines with parameters 0.55,0.45,...,0.45).\n5. Conclusions\nWehaveshownsimpleandefficientpoliciesforthebanditproblemthat,onanysetofreward\ndistributions with known bounded support, exhibit uniform logarithmic regret. Our policies\nare deterministic and based on upper confidence bounds, with the exception of \u03b5n-GREEDY,\na randomized allocation rule that is a dynamic variant of the \u03b5-greedy heuristic. Moreover,\nour policies are robust with respect to the introduction of moderate dependencies in the\nreward processes.\nThis work can be extended in many ways. A more general version of the bandit problem\nis obtained by removing the stationarity assumption on reward expectations (see Berry &\nFristedt, 1985; Gittins, 1989 for extensions of the basic bandit problem). For example,\nsuppose that a stochastic reward process {Xi,s:s = 1,2,...} is associated to each machine\ni = 1,..., K. Here, playing machine i at time t yields a reward Xi,sand causes the current"},{"page":16,"text":"250\nP. AUER, N. CESA-BIANCHI AND P. FISCHER\nstate s of i to change to s +1, whereas the states of other machines remain frozen. A well-\nstudied problem in this setup is the maximization of the total expected reward in a sequence\nof n plays. There are methods, like the Gittins allocation indices, that allow to find the\noptimal machine to play at each time n by considering each reward process independently\nfrom the others (even though the globally optimal solution depends on all the processes).\nHowever,computationoftheGittinsindicesfortheaverage(undiscounted)rewardcriterion\nused here requires preliminary knowledge about the reward processes (see, e.g., Ishikida &\nVaraiya,1994).Toovercomethisrequirement,onecanlearntheGittinsindices,asproposed\ninDuff(1995)forthecaseoffinite-stateMarkovianrewardprocesses.However,thereareno\nfinite-time regret bounds shown for this solution. At the moment, we do not know whether\nour techniques could be extended to these more general bandit problems.\nAppendix A: Proof of Theorem 2\nNote that\n\u03c4(r) \u2264 (1 + \u03b1)r+ 1 \u2264 \u03c4(r \u2212 1)(1 + \u03b1) + 1\nfor r \u2265 1. Assume that n \u2265 1\/(2?2\n(1 + 4\u03b1)ln?2en?2\n(14)\nj) for all j and let \u02dc rjbe the largest integer such that\n?\n\u03c4(\u02dc rj\u2212 1) \u2264\nj\n2?2\nj\n.\nNote that \u02dc rj\u2265 1. We have\nTj(n) \u2264 1 +\n?\nr\u22651\n(\u03c4(r) \u2212 \u03c4(r \u2212 1)) {machine j finishes its r-th epoch}\n\u2264 \u03c4(\u02dc rj) +\n?\nr>\u02dc rj\n(\u03c4(r) \u2212 \u03c4(r \u2212 1)) {machine j finishes its r-th epoch}\nNow consider the following chain of implications\nmachine j finishes its r-th epoch\n\u21d2 \u2203i \u2265 0, \u2203t \u2265 \u03c4(r \u2212 1) + \u03c4(i) such that\n?\u00afXj,\u03c4(r\u22121)+ at,r\u22121\n\u21d2 \u2203t \u2265 \u03c4(r \u2212 1) such that?\u00afXj,\u03c4(r\u22121)+ at,r\u22121\n?\u00afX\u2217\n\u21d2\u00afXj,\u03c4(r\u22121)+ an,r\u22121\u2265 \u00b5\u2217\u2212 \u03b1?j\/2\nor\n\u2203i \u2265 0 such that\u00afX\u2217\nwhere the last implication hold because at,ris increasing in t. Hence\n?\u2265?\u00afX\u2217\n\u03c4(i)+ at,i\n?\n?\u2265 \u00b5\u2217\u2212 \u03b1?j\/2\nor\n\u2203i \u2265 0, \u2203t?\u2265 \u03c4(r \u2212 1) + \u03c4(i) such that\n\u03c4(i)+ at?,i\n?\u2264 \u00b5\u2217\u2212 \u03b1?j\/2\n\u03c4(i)+ a\u03c4(r\u22121)+\u03c4(i),i\u2264 \u00b5\u2217\u2212 \u03b1?j\/2\nIE[Tj(n)] \u2264 \u03c4(\u02dc rj) +\n?\nr>\u02dc rj\n(\u03c4(r) \u2212 \u03c4(r \u2212 1))IP?\u00afXj,\u03c4(r\u22121)+ an,r\u22121\u2265 \u00b5\u2217\u2212 \u03b1?j\/2?"},{"page":17,"text":"FINITE-TIME ANALYSIS\n251\n+\n?\nr>\u02dc rj\n?\n\u03c4(i)+ a\u03c4(r\u22121)+\u03c4(i),i\u2264 \u00b5\u2217\u2212 \u03b1?j\/2?.\nj) implies ln(2en?2\ni\u22650\n(\u03c4(r) \u2212 \u03c4(r \u2212 1))\n\u00b7 IP?\u00afX\u2217\n(15)\nThe assumption n \u2265 1\/(2?2\nj) \u2265 1. Therefore, for r > \u02dc rj, we have\n\u03c4(r \u2212 1) >\n(1 + 4\u03b1) ln?2en?2\nj\n?\n2?2\nj\n(16)\nand\nan,r\u22121=\n?\n(1 + \u03b1)ln(en\/\u03c4(r \u2212 1))\n2\u03c4(r \u2212 1)\n?\n?\n?\nusing \u03c4(r \u2212 1) > 1\/2?2\n?\n\u2264 ?j\n(1 + \u03b1)ln(en\/\u03c4(r \u2212 1))\n?1 + 4\u03b1)ln(2en?2\n?\nj\n?\nusing (16) above\n\u2264 ?j\n?\n?1 + \u03b1)ln(2en?2\nj\n?\nj\n?1 + 4\u03b1)ln(2en?2\n?\njderived from (16)\n\u2264 ?j\n1 + \u03b1\n1 + 4\u03b1.\n(17)\nWe start by bounding the first sum in (15). Using (17) and Fact 1 (Chernoff-Hoeffding\nbound) we get\nIP?\u00afXj,\u03c4(r\u22121)+ an,r\u22121\u2265 \u00b5\u2217\u2212 \u03b1?j\/2?\n\u2264 exp?\u2212 2\u03c4(r \u2212 1)?2\n\u2264 exp?\u22122\u03c4(r \u2212 1)?2\n= exp?\u2212\u03c4(r \u2212 1)?2\nfor \u03b1 < 1\/10. Now let g(x) = (x \u2212 1)\/(1 + \u03b1). By (14) we get g(x) \u2264 \u03c4(r \u2212 1) for\n\u03c4(r \u2212 1) \u2264 x \u2264 \u03c4(r) and r \u2265 1. Hence\n?\n\u2264\nr>\u02dc rj\n?\u221e\n= IP{\u00afXj,\u03c4(r\u22121)+ an,r\u22121\u2265 \u00b5j+ ?j\u2212 \u03b1?j\/2}\nj\n?1 \u2212 \u03b1\/2 \u2212\nj\u03b12?2?\n?\n(1 + \u03b1)\/(1 + 4\u03b1)?2?\nj(1 \u2212 \u03b1\/2 \u2212 (1 \u2212 \u03b1))2?\nr>\u02dc rj\n(\u03c4(r) \u2212 \u03c4(r \u2212 1))IP?\u00afXj,\u03c4(r\u22121)+ an,r\u22121\u2265 \u00b5\u2217\u2212 \u03b1?j\/2?\n?\ne\u2212cg(x)dx\n(\u03c4(r) \u2212 \u03c4(r \u2212 1)) exp?\u2212 \u03c4(r \u2212 1)?2\nj\u03b12?\n\u2264\n0"},{"page":18,"text":"252\nP. AUER, N. CESA-BIANCHI AND P. FISCHER\nwhere c = (?j\u03b1)2< 1. Further manipulation yields\n?\u221e\n0\nexp\n?\n\u2212\nc\n1 + \u03b1(x \u2212 1)\n?\ndx = ec\/(1+\u03b1)1 + \u03b1\nc\n\u2264(1 + \u03b1)e\n(?j\u03b1)2.\nWe continue by bounding the second sum in (15). Using once more Fact 1, we get\n?\n\u2264\nr>\u02dc rj\n?\n?\n\u00b7 exp\n?\n??\n?\n??\ni\u22650\n(\u03c4(r) \u2212 \u03c4(r \u2212 1))IP?\u00afX\u2217\n?\n?\nexp{\u2212\u03c4(i)(\u03b1?j)2\/2}\n\u03c4(i)+ a\u03c4(r\u22121)+\u03c4(i),i\u2264 \u00b5\u2217\u2212 \u03b1?j\/2?\ni\u22650\nr>\u02dc rj\n(\u03c4(r) \u2212 \u03c4(r \u2212 1))\n\u2212 \u03c4(i)(\u03b1?j)2\n2\n\u2212 (1 + \u03b1) ln\n?\ne\u03c4(r \u2212 1) + \u03c4(i)\n\u03c4(i)\n??\n\u2264\ni\u22650\n\u00b7\nr>\u02dc rj\nexp{\u2212\u03c4(i)(\u03b1?j)2\/2}\n(\u03c4(r) \u2212 \u03c4(r \u2212 1)) exp\n?\n\u2212 (1 + \u03b1)ln\n?\n1 +\u03c4(r \u2212 1)\n\u03c4(i)\n???\n=\ni\u22650\n\u00b7\nr>\u02dc rj\n(\u03c4(r) \u2212 \u03c4(r \u2212 1))\n?\n??\u221e\n1 +\u03c4(r \u2212 1)\n\u03c4(i)\n?\u2212(1+\u03b1)?\nx \u2212 1\n(1 + \u03b1)\u03c4(i)\n?\n1 + \u03b1\n\u03b1\n1 + \u03b1\n\u2264\n?\n?\n?\n?1 + \u03b1\ni\u22650\nexp{\u2212\u03c4(i)(\u03b1?j)2\/2}\n0\n?\n1 + \u03b1\n\u03b1\n1 +\n?\u2212(1+\u03b1)\n1\ndx\n?\n=\ni\u22650\n\u03c4(i)exp{\u2212\u03c4(i)(\u03b1?j)2\/2}\n?\n?\n1 \u2212\n(1 + \u03b1)\u03c4(i)\n\u03b1\n?\u2212\u03b1?\n?\u2212\u03b1?\n\u2264\ni\u22650\n\u03c4(i)exp{\u2212\u03c4(i)(\u03b1?j)2\/2}\n?\nas \u03c4(i) \u2265 1\n=\n\u03b1\n?1+\u03b1?\ni\u22650\n\u03c4(i)exp{\u2212\u03c4(i)(\u03b1?j)2\/2}.\nNow, as (1 + \u03b1)x\u22121\u2264 \u03c4(i) \u2264 (1 + \u03b1)x+ 1 for i \u2264 x \u2264 i + 1, we can bound the series in\nthe last formula above with an integral\n?\n\u2264 1 +\ni\u22650\n\u03c4(i)exp{\u2212\u03c4(i)(\u03b1?j)2\/2}\n?\u221e\n1\n((1 + \u03b1)x+ 1)exp{\u2212(1 + \u03b1)x\u22121(\u03b1?j)2\/2}dx"},{"page":19,"text":"FINITE-TIME ANALYSIS\n253\n\u2264 1 +\n?\u221e\nby change of variable z = (1 + \u03b1)x\n= 1 +\nln(1 + \u03b1)\n1\nz + 1\nz ln(1 + \u03b1)exp\n?\n\u2212z(\u03b1?j)2\n2(1 + \u03b1)\n?\ndz\n1\n?e\u2212\u03bb\n\u03bb\n+\n?\u221e\n\u03bb\ne\u2212x\nx\ndx\n?\nwhere we set\n\u03bb =\n(\u03b1?j)2\n2(1 + \u03b1).\nAs 0 < \u03b1,?j< 1, we have 0 < \u03bb < 1\/4. To upper bound the bracketed formula above,\nconsider the function\nF(\u03bb) = e\u2212\u03bb+ \u03bb\n?\u221e\n\u03bb\ne\u2212x\nx\ndx\nwith derivatives\nF?(\u03bb) =\n?\u221e\n\u03bb\ne\u2212x\nx\ndx \u2212 2e\u2212\u03bb\nF??(\u03bb) = 2\u03bbe\u2212\u03bb\u2212\n?\u221e\n\u03bb\ne\u2212x\nx\ndx.\nIn the interval (0,1\/4), F?is seen to have a zero at \u03bb = 0.0108.... As F??(\u03bb) < 0 in the\nsame interval, this is the unique maximum of F, and we find F(0.0108...) < 11\/10. So\nwe have\ne\u2212\u03bb\n\u03bb\n+\n?\u221e\n\u03bb\ne\u2212x\nx\ndx <\n11\n10\u03bb=11(1 + \u03b1)\n5(\u03b1?j)2\nPiecing everything together, and using (14) to upper bound \u03c4(\u02dc rj), we find that\nIE[Tj(n)] \u2264 \u03c4(\u02dc rj) +(1 + \u03b1)e\n(1 + \u03b1)(1 + 4\u03b1)ln?2en?2\n(?j\u03b1)2+\n?1 + \u03b1\n\u03b1\n?1+\u03b1?\n+c\u03b1\n1 +\n11(1 + \u03b1)\n5(\u03b1?j)2ln(1 + \u03b1)\n?\n\u2264\nj\n?\n2?2\nj\n?2\nj\nwhere\nc\u03b1= 1 +(1 + \u03b1)e\n\u03b12\n+\n?1 + \u03b1\n\u03b1\n?1+\u03b1?\n1 +\n11(1 + \u03b1)\n5\u03b12ln(1 + \u03b1)\n?\n.\n(18)\nThis concludes the proof.\n\u2737"},{"page":20,"text":"254\nP. AUER, N. CESA-BIANCHI AND P. FISCHER\nAppendix B:Proof of Theorem 4\nThe proof goes very much along the same lines as the proof of Theorem 1. It is based on\nthe two following conjectures which we only verified numerically.\nConjecture 1.\nall 0\u2264a \u2264\u221a2(s + 1),\nIP{X \u2265 a} \u2264 e\u2212a2\/4.\nLet X be a Student random variable with s degrees of freedom. Then, for\nConjecture 2.\nLet X be a \u03c72random variable with s degrees of freedom. Then\nIP{X \u2265 4s} \u2264 e\u2212(s+1)\/2.\nWe now proceed with the proof of Theorem 4. Let\nQi,n=\nn ?\nt=1\nX2\ni,t.\nFix a machine i and, for any s and t, set\nct,s=\n?\n16 \u00b7Qi,s\u2212 s\u00afX2\ni,s\ns \u2212 1\n\u00b7lnt\ns\nLet c\u2217\nproceed exactly as in the first part of the proof of Theorem 1 obtaining, for any positive\ninteger ?,\nt,sbe the corresponding quantity for the optimal machine. To upper bound Ti(n), we\nTi(n) \u2264 ? +\n\u221e\n?\nt=1\nt\u22121\n?\ns=1\nt\u22121\n?\nsi=?\n?{\u00afX\u2217\ns\u2264 \u00b5\u2217\u2212 c\u2217\nt,s} +?\u00afXi,si\u2265 \u00b5i+ ct,si\n?\n+?\u00b5\u2217< \u00b5i+ 2ct,si\n??.\n?\nThe random variable (\u00afXi,si\u2212\u00b5i)\/\nwith si\u2212 1 degrees of freedom (see, e.g., Wilks, 1962, 8.4.3 page 211). Therefore, using\nConjecture 1 with s = si\u2212 1 and a = 4\u221alnt, we get\n(Qi,si\u2212 si\u00afX2\ni,si)\/(si(si\u2212 1)) has a Student distribution\nIP?\u00afXi,si\u2265 \u00b5i+ ct,si\n?= IP\n\uf8f1\n\uf8f3\n\uf8f2\n\u00afXi,si\u2212 \u00b5i\n??Qi,si\u2212 si\u00afX2\ns\u2264 \u00b5\u2217\u2212c\u2217\ni,si\n?\/(si(si\u2212 1))\nt,sis bounded analogously. Finally, since\n\u2265 4\u221alnt\n\uf8fc\n\uf8fd\n\uf8fe\u2264 t\u22124\nfor all si\u2265 8lnt. The probability of\u00afX\u2217\n(Qi,si\u2212si\u00afX2\ni,si)\/\u03c32\niis \u03c72-distributed with si\u22121 degrees of freedom (see, e.g., Wilks, 1962,"},{"page":21,"text":"FINITE-TIME ANALYSIS\n255\n8.4.1 page 208). Therefore, using Conjecture 2 with s = si\u2212 1 and a = 4s, we get\n??Qi,si\u2212 si\u00afX2\n\u2264 IP??Qi,si\u2212 si\u00afX2\nIP?\u00b5\u2217< \u00b5i+ 2ct,si\n?= IP\ni,si\n??\u03c32\n??\u03c32\ni> (si\u2212 1)?2\ni\n\u03c32\ni\nsi\n64lnt\n?\ni,si\ni> 4(si\u2212 1)?\n\u2264 e\u2212si\/2\u2264 t\u22124\nfor\nsi\u2265 max\n?\n256\u03c32\ni\n?2\ni\n,8\n?\nlnt.\nSetting\n? =\n?\nmax\n?\n256\u03c32\ni\n?2\ni\n,8\n?\nlnt\n?\ncompletes the proof of the theorem.\n\u2737\nAcknowledgments\nThe support from ESPRIT Working Group EP 27150, Neural and Computational Learning\nII (NeuroCOLT II), is gratefully acknowledged.\nNote\n1. Similar extensions of Lai and Robbins\u2019 results were also obtained by Yakowitz and Lowe (1991), and by\nBurnetas and Katehakis (1996).\nReferences\nAgrawal, R. (1995). Sample mean based index policies with O(logn) regret for the multi-armed bandit problem.\nAdvances in Applied Probability, 27, 1054\u20131078.\nBerry, D., & Fristedt, B. (1985). Bandit problems. London: Chapman and Hall.\nBurnetas, A., & Katehakis, M. (1996). Optimal adaptive policies for sequential allocation problems. Advances in\nApplied Mathematics, 17:2, 122\u2013142.\nDuff,M.(1995).Q-learningforbanditproblems.InProceedingsofthe12thInternationalConferenceonMachine\nLearning (pp. 209\u2013217).\nGittins, J. (1989). Multi-armed bandit allocation indices, Wiley-Interscience series in Systems and Optimization.\nNew York: John Wiley and Sons.\nHolland, J. (1992). Adaptation in natural and artificial systems. Cambridge: MIT Press\/Bradford Books.\nIshikida, T., & Varaiya, P. (1994). Multi-armed bandit problem revisited. Journal of Optimization Theory and\nApplications, 83:1, 113\u2013154.\nLai,T.,&Robbins,H.(1985).Asymptoticallyefficientadaptiveallocationrules.AdvancesinAppliedMathematics,\n6, 4\u201322.\nPollard, D. (1984). Convergence of stochastic processes. Berlin: Springer."},{"page":22,"text":"256\nP. AUER, N. CESA-BIANCHI AND P. FISCHER\nSutton, R., & Barto, A. (1998). Reinforcement learning, an introduction. Cambridge: MIT Press\/Bradford\nBooks.\nWilks, S. (1962). Matematical statistics. New York: John Wiley and Sons.\nYakowitz, S., & Lowe, W. (1991). Nonparametric bandit methods. Annals of Operations Research, 28, 297\u2013\n312.\nReceived September 29, 2000\nRevised May 21, 2001\nAccepted June 20, 2001\nFinal manuscript June 20, 2001"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem\/links\/09e4150ec228dace78000000.pdf","widgetId":"rgw31_56ab9ebb431f0"},"id":"rgw31_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=220343796&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw32_56ab9ebb431f0"},"id":"rgw32_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=220343796&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":220343796,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"09e4150ec228dace78000000","name":"Nicol\u00f2 Cesa-Bianchi","date":null,"nameLink":"profile\/Nicolo_Cesa-Bianchi","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem\/links\/09e4150ec228dace78000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem\/links\/09e4150ec228dace78000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"77ff021da3d547cca7ef1ed8da2c07e7","showFileSizeNote":false,"fileSize":"202.98 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"09e4150ec228dace78000000","name":"Nicol\u00f2 Cesa-Bianchi","date":null,"nameLink":"profile\/Nicolo_Cesa-Bianchi","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem\/links\/09e4150ec228dace78000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem\/links\/09e4150ec228dace78000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"77ff021da3d547cca7ef1ed8da2c07e7","showFileSizeNote":false,"fileSize":"202.98 KB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[{"props":{"position":"float","orientation":"portrait","coords":"pag:3:rect:117.53,671.62,29.01,7.30","ordinal":"1"},"assetId":"AS:277365050626048@1443140389592"},{"props":{"position":"float","orientation":"portrait","coords":"pag:4:rect:117.53,289.27,29.01,7.30","ordinal":"2"},"assetId":"AS:277365050626049@1443140389662"},{"props":{"position":"float","orientation":"portrait","coords":"pag:5:rect:117.53,671.62,29.01,7.30","ordinal":"3"},"assetId":"AS:277365050626050@1443140389711"},{"props":{"position":"float","orientation":"portrait","coords":"pag:6:rect:117.53,673.12,29.01,7.30","ordinal":"4"},"assetId":"AS:277365050626051@1443140389731"},{"props":{"position":"float","orientation":"portrait","coords":"pag:12:rect:117.53,671.62,29.01,7.30","ordinal":"5"},"assetId":"AS:277365050626052@1443140389765"},{"props":{"position":"float","orientation":"portrait","coords":"pag:13:rect:117.53,496.02,29.01,7.30","ordinal":"6"},"assetId":"AS:277365050626053@1443140389791"},{"props":{"position":"float","orientation":"portrait","coords":"pag:13:rect:117.53,671.62,29.01,7.30","ordinal":"7"},"assetId":"AS:277365050626054@1443140389831"},{"props":{"position":"float","orientation":"portrait","coords":"pag:14:rect:117.53,306.52,29.01,7.30","ordinal":"8"},"assetId":"AS:277365050626055@1443140389863"},{"props":{"position":"float","orientation":"portrait","coords":"pag:14:rect:117.53,481.12,29.01,7.30","ordinal":"9"},"assetId":"AS:277365050626056@1443140389982"},{"props":{"position":"float","orientation":"portrait","coords":"pag:14:rect:117.53,656.72,33.96,7.30","ordinal":"0"},"assetId":"AS:277365054820352@1443140390009"},{"props":{"position":"float","orientation":"portrait","coords":"pag:15:rect:117.53,301.27,33.00,7.30","ordinal":"1"},"assetId":"AS:277365054820353@1443140390053"},{"props":{"position":"float","orientation":"portrait","coords":"pag:15:rect:117.53,490.02,33.00,7.30","ordinal":"2"},"assetId":"AS:277365054820354@1443140390083"}],"figureAssetIds":["AS:277365050626048@1443140389592","AS:277365050626049@1443140389662","AS:277365050626050@1443140389711","AS:277365050626051@1443140389731","AS:277365050626052@1443140389765","AS:277365050626053@1443140389791","AS:277365050626054@1443140389831","AS:277365050626055@1443140389863","AS:277365050626056@1443140389982","AS:277365054820352@1443140390009","AS:277365054820353@1443140390053","AS:277365054820354@1443140390083"],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=LADpWEv9nOb8Czq2vFXSwT4siFACYp2kwtu8SDlCSrOxTz8uaItfWYL1Jpbeu5JOaSVyjQeRG5VGomN1-WO-ig.vl7k_JqGAIsxtroIJJ9f1HY6yTJm15br4vvDnhPoMibJI_amXOdIvOvC8QSFVXvRyw5RfgwCVhqwqVJIgyWPCQ","clickOnPill":"publication.PublicationFigures.html?_sg=tOq7YwGQq0VN5OL6kXXDs1jZFLmEK6C0TLdtmS4oRekygmp6-rA_GXHy-dPbXCxG4w_21i4yr6AHyxl-Mzw2QA.wl38DlRI0CiSGqhPc17p0omWP1a95EzGuQMCbH0Xg7rqGIRQvImQzT20YLTlOMK-Mc_i62sVBRVOX4fP94KuDA"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FNicolo_Cesa-Bianchi%2Fpublication%2F220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem%2Flinks%2F09e4150ec228dace78000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=KqkiVX0iayTVbIfub3yztxnHXVEf4p0W8I4CWr9q8bVH5niL69jQ9OqCJI7QPLCUaa_XcBKB7TQLc3UQrjn5-Q","urlHash":"793d20d84ddfd142a05f2ae32a3ba266","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=4MOKKAQsQweQvAcFik9KgmLEKT5NHJfxWvb0PtiWHk9ax8_EY_N_OiogNIaUpvHtc_IgyyS5ezjQQYnRoRf_owJ7dGA0lFURWemnCsL47uw.zKoWSn9rJyrtBb3ILlFBPyzfmPhcgzXFeioEzG7cOoiVf07TWSqhF_iONomDISYBoQD1DaLLCU2okZP-530aWQ.7mPRr4ENJFgJ9xUBZlj5Yev7oLFbd8_x3cV8uV20hbSxbASWt0z7DAMHC-WqUiI1xwt6Yq_kgiaTR5gkBzKHdg","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"09e4150ec228dace78000000","trackedDownloads":{"09e4150ec228dace78000000":{"v":false,"d":false}},"assetId":"AS:102833424699401@1401528807959","readerDocId":"2811193","assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":220343796,"commentCursorPromo":null,"widgetId":"rgw34_56ab9ebb431f0"},"id":"rgw34_56ab9ebb431f0","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FNicolo_Cesa-Bianchi%2Fpublication%2F220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem%2Flinks%2F09e4150ec228dace78000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A102833424699401%401401528807959&publicationUid=220343796&linkId=09e4150ec228dace78000000&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Finite-time Analysis of the Multiarmed Bandit Problem","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=_Yarcg813AagZYhYFeuYLlEXfJ347SNWsdLp5YiDx5bXcxFwV-_gRwBL8IqEDTiM-U29vRSX4El9UZxdd4oqyuMQDTkrdZKab0AL6CxYHZ4.6Sq2ukPGYlJSbTbdbA923JPYx74_mmTFc8up6EUfW3_8J6qLUtACgPolGO76pAfe3liYh1InY77ziwN80L_h2A.CbQ9K7W1f0KmSmSfqf4WUGfaX3c2sc3LR8SINOMX11Zd1WxulpT0wyGXbLYp-EyKi7AM9B7g4Ha7RSqANwQXxQ","publicationUid":220343796,"trackedDownloads":{"09e4150ec228dace78000000":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw36_56ab9ebb431f0"},"id":"rgw36_56ab9ebb431f0","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw37_56ab9ebb431f0"},"id":"rgw37_56ab9ebb431f0","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw38_56ab9ebb431f0"},"id":"rgw38_56ab9ebb431f0","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw39_56ab9ebb431f0"},"id":"rgw39_56ab9ebb431f0","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw40_56ab9ebb431f0"},"id":"rgw40_56ab9ebb431f0","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw35_56ab9ebb431f0"},"id":"rgw35_56ab9ebb431f0","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw33_56ab9ebb431f0"},"id":"rgw33_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab9ebb431f0"},"id":"rgw2_56ab9ebb431f0","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":220343796},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=220343796&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab9ebb431f0"},"id":"rgw1_56ab9ebb431f0","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"wtIQruKs3DMqzg7QxRa7w69jyE6VHjdLncojctwdtBOMa73uAs063GcAK9h3ssHew\/5UBaUEqcLBfECWSGHw5aNyvR8itWlDKUUHiv8lKVfGbBM5QLlJArAulj6\/2uBcWcO3ylufWtVbuz9tflRfhhbIf\/0fyFgxpoN8edI\/8F7oJqEjvSg8zw0E1PW+q60dajzm0lpIuYeqJs8i7seeoSnRD3zGqbhWLG\/e\/+2FQFS6vJF60njLcf0zINKkK4sHMIRJ7zsQtAEYTso1Pyl\/LR7l+ycBwCe4jGe7X7G7o58=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Finite-time Analysis of the Multiarmed Bandit Problem\" \/>\n<meta property=\"og:description\" content=\"Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem\/links\/09e4150ec228dace78000000\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem\" \/>\n<meta property=\"rg:id\" content=\"PB:220343796\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1023\/A:1013689704352\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Finite-time Analysis of the Multiarmed Bandit Problem\" \/>\n<meta name=\"citation_author\" content=\"Peter Auer\" \/>\n<meta name=\"citation_author\" content=\"Nicol\u00f2 Cesa-Bianchi\" \/>\n<meta name=\"citation_author\" content=\"Paul Fischer\" \/>\n<meta name=\"citation_publication_date\" content=\"2002\/05\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"Machine Learning\" \/>\n<meta name=\"citation_issn\" content=\"0885-6125\" \/>\n<meta name=\"citation_volume\" content=\"47\" \/>\n<meta name=\"citation_issue\" content=\"2-3\" \/>\n<meta name=\"citation_firstpage\" content=\"235\" \/>\n<meta name=\"citation_lastpage\" content=\"256\" \/>\n<meta name=\"citation_doi\" content=\"10.1023\/A:1013689704352\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Nicolo_Cesa-Bianchi\/publication\/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem\/links\/09e4150ec228dace78000000.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/215868066921738\/styles\/pow\/publicliterature\/FigureList.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-6584da8a-e5fd-4511-b1d1-f10127da45bb","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":595,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw41_56ab9ebb431f0"},"id":"rgw41_56ab9ebb431f0","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-6584da8a-e5fd-4511-b1d1-f10127da45bb", "747ff00da197abed49f1e30d2763091c9cd3eb49");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-6584da8a-e5fd-4511-b1d1-f10127da45bb", "747ff00da197abed49f1e30d2763091c9cd3eb49");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw42_56ab9ebb431f0"},"id":"rgw42_56ab9ebb431f0","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/220343796_Finite-time_Analysis_of_the_Multiarmed_Bandit_Problem","requestToken":"opVpsc+l3b71+RIZxuZJtxWCoFm98PX0ol\/VahQ4n\/AMHQi3moEJVnH2guh23Z25PIZWBFnI6sKkE5G11Hv1hWwvGE0MQujOVasiv+2jOSACAHplZ3XYXfuV\/GLYMUTskv62SaD2BVmJaVw23poNsxhXLhFSuL5gFzU4ODrryxqxdQY3AJMgvnULR7wVdju1Bwp64pl6VuvT5z67OOR+Aa7HXpEHld+EP42fEPb+uvYDvoEZ1e6Z\/ZjejDzeh5ifF07JrfkaFs11kgvp2m9v8JN96fA7JJXbZ4Z8lTknlg0=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=YXzbqWZYCA2ZFxekdLbNCYyH3loNN07HF6PChRL-oB5PHGZnbHLo2nKTz1mEopd-","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjIwMzQzNzk2X0Zpbml0ZS10aW1lX0FuYWx5c2lzX29mX3RoZV9NdWx0aWFybWVkX0JhbmRpdF9Qcm9ibGVt","signupCallToAction":"Join for free","widgetId":"rgw44_56ab9ebb431f0"},"id":"rgw44_56ab9ebb431f0","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw43_56ab9ebb431f0"},"id":"rgw43_56ab9ebb431f0","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw45_56ab9ebb431f0"},"id":"rgw45_56ab9ebb431f0","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
