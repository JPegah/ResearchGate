<!DOCTYPE html> <html lang="en" class="" id="rgw39_56aba2472d207"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="kvF43ui9A0czC+6tEEy+hfkFcyfnRJLV3ueKFOp1GrJenSJZEYhCgTPqhXhV5q3bLZ2ihnRQaj8GBH6jrnPIF1oYz3IHn8CjqiozUWl5xvt8kp5S0c7TFzl41nG43LO6xajnlcWdvn3uwP2I/gfKtPx4m+LbguyQdPhB5AISetq+K6ZV725QtngdSOAHxO+5DaVrb5q173Xq9xEYzObR5xFBU8YpgAo4zhoULonnL+7Ebq187TiDLBOaO3QJImmhQbtuK4w3qgLQXDWB4iZDZTSXEs/OZ/0PJc7NstLWcAw="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-f12ec667-6176-443a-b952-faceb5b7e5dd",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Choosing the Sample Size of a Computer Experiment: A Practical Guide" />
<meta property="og:description" content="We produce reasons and evidence supporting the informal rule that the number of runs for an effective initial computer experiment should be about 10 times the input dimension. Our arguments..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide/links/0046352e6a4733d593000000/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide" />
<meta property="rg:id" content="PB:237548813" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1198/TECH.2009.08040" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Choosing the Sample Size of a Computer Experiment: A Practical Guide" />
<meta name="citation_author" content="Jason L. Loeppky" />
<meta name="citation_author" content="Jerome Sacks" />
<meta name="citation_author" content="William J. Welch" />
<meta name="citation_publication_date" content="2009/11/01" />
<meta name="citation_journal_title" content="Technometrics" />
<meta name="citation_issn" content="1537-2723" />
<meta name="citation_volume" content="51" />
<meta name="citation_issue" content="4" />
<meta name="citation_firstpage" content="366" />
<meta name="citation_lastpage" content="376" />
<meta name="citation_doi" content="10.1198/TECH.2009.08040" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/William_Welch/publication/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide/links/0046352e6a4733d593000000.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Choosing the Sample Size of a Computer Experiment: A Practical Guide (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Choosing the Sample Size of a Computer Experiment: A Practical Guide on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56aba2472d207" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56aba2472d207" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56aba2472d207">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1198%2FTECH.2009.08040&rft.atitle=Choosing%20the%20Sample%20Size%20of%20a%20Computer%20Experiment%3A%20A%20Practical%20Guide&rft.title=Technometrics&rft.jtitle=Technometrics&rft.volume=51&rft.issue=4&rft.date=2009&rft.pages=366-376&rft.issn=1537-2723&rft.au=Jason%20L.%20Loeppky%2CJerome%20Sacks%2CWilliam%20J.%20Welch&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Choosing the Sample Size of a Computer Experiment: A Practical Guide</h1> <meta itemprop="headline" content="Choosing the Sample Size of a Computer Experiment: A Practical Guide">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide/links/0046352e6a4733d593000000/smallpreview.png">  <div id="rgw8_56aba2472d207" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56aba2472d207" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Jason_Loeppky" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Jason L. Loeppky" alt="Jason L. Loeppky" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Jason L. Loeppky</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw10_56aba2472d207" data-account-key="Jason_Loeppky">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Jason_Loeppky"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Jason L. Loeppky" alt="Jason L. Loeppky" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Jason_Loeppky" class="display-name">Jason L. Loeppky</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_of_British_Columbia-Okanagan" title="University of British Columbia - Okanagan">University of British Columbia - Okanagan</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56aba2472d207"> <a href="researcher/8998449_Jerome_Sacks" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Jerome Sacks" alt="Jerome Sacks" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Jerome Sacks</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56aba2472d207">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/8998449_Jerome_Sacks"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Jerome Sacks" alt="Jerome Sacks" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/8998449_Jerome_Sacks" class="display-name">Jerome Sacks</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw13_56aba2472d207" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/William_Welch" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A277690474090520%401443217976996_m" title="William J. Welch" alt="William J. Welch" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">William J. Welch</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw14_56aba2472d207" data-account-key="William_Welch">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/William_Welch"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A277690474090520%401443217976996_l" title="William J. Welch" alt="William J. Welch" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/William_Welch" class="display-name">William J. Welch</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_of_British_Columbia-Vancouver" title="University of British Columbia - Vancouver">University of British Columbia - Vancouver</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/1537-2723_Technometrics"><span itemprop="name">Technometrics</span></a> </span>    (Impact Factor: 1.81).     <meta itemprop="datePublished" content="2009-11">  11/2009;  51(4):366-376.    DOI:&nbsp;10.1198/TECH.2009.08040           </div> <div id="rgw15_56aba2472d207" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>We produce reasons and evidence supporting the informal rule that the number of runs for an effective initial computer experiment should be about 10 times the input dimension. Our arguments quantify two key characteristics of computer codes that affect the sample size required for a desired level of accuracy when approximating the code via a Gaussian process (GP). The first characteristic is the total sensitivity of a code output variable to all input variables. The second corresponds to the way this total sensitivity is distributed across the input variables, specifically the possible presence of a few prominent input factors and many impotent ones (effect sparsity). Both measures relate directly to the correlation structure in the GP approximation of the code. In this way, the article moves towards a more formal treatment of sample size for a computer experiment. The evidence supporting these arguments stems primarily from a simulation study and via specific codes modeling climate and ligand activation of G-protein.</div> </p>  </div>   </div>      <div class="action-container"> <div id="rgw16_56aba2472d207" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw30_56aba2472d207">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw31_56aba2472d207">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/William_Welch/publication/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide/links/0046352e6a4733d593000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/William_Welch">William J. Welch</a>, <span class="js-publication-date"> Jan 27, 2014 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw33_56aba2472d207" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw34_56aba2472d207" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw35_56aba2472d207" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw36_56aba2472d207" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw37_56aba2472d207" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw38_56aba2472d207" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw32_56aba2472d207" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FWilliam_Welch%2Fpublication%2F237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide%2Flinks%2F0046352e6a4733d593000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw29_56aba2472d207"  itemprop="articleBody">  <p>Page 1</p> <p>THE UNIVERSITY OF BRITISH <br />COLUMBIA <br /> <br />DEPARTMENT OF STATISTICS <br /> <br /> <br />TECHNICAL REPORT #238 <br /> <br /> <br /> <br /> <br />Choosing the Sample Size of a Computer <br />Experiment: A Practical Guide <br /> <br /> <br /> <br /> <br />Jason Leoppky, Jerome Sacks, William J. Welch <br /> <br /> <br /> <br /> <br /> <br /> <br />February 2008</p>  <p>Page 2</p> <p>Choosing the Sample Size of a Computer Experiment: A<br />Practical Guide<br />Jason L. LoeppkyJerome Sacks<br />Mathematics, Statistics, and Physics<br />University of British Columbia, Okanagan<br />Kelowna, BC V1V 1V7, CANADA<br />(jason@stat.ubc.ca)<br />National Institute of Statistical Sciences<br />Research Triangle Park, NC, 27709<br />(sacks@niss.org)<br />William J. Welch<br />Department of Statistics<br />University of British Columbia<br />Vancouver, BC V6T 1Z2, CANADA<br />(will@stat.ubc.ca)<br />February 19, 2008<br />Abstract<br />We produce reasons and evidence supporting the informal rule that the number of<br />runs for an effective initial computer experiment should be about 10 times the input<br />dimension. Our arguments quantify two key characteristics of computer codes that<br />affect the sample size required for a desired level of accuracy when approximating<br />the code via a Gaussian process (GP). The first characteristic is the total sensitivity<br />of a code output variable to all input variables. The second corresponds to the<br />way this total sensitivity is distributed across the input variables, specifically the<br />possible presence of a few prominent input factors and many impotent ones (effect<br />sparsity). Both measures relate directly to the correlation structure in the GP<br />approximation of the code. In this way, the article moves towards a more formal<br />treatment of sample size for a computer experiment. The evidence supporting these<br />arguments stems primarily from a simulation study and via specific codes modeling<br />climate and ligand activation of G-protein.<br />KEYWORDS: Computer experiment, Gaussian process, Random function, Latin<br />hypercube design, Sample size.<br />1 Introduction<br />Choosing the sample size of any experiment is an important issue in the design of ex-<br />periments, yet there is a lack of formal guidance. The reasons range from inadequate<br />prior information about the process under study to inadequate results (and inability) for<br />making necessary calculations. In standard regression settings, finding a sample size to<br />1</p>  <p>Page 3</p> <p>produce satisfactory predictive accuracy depends on the design points of the data col-<br />lection and the error variance, but both the form of the regression model and the error<br />variance are typically unknown a priori. Bayesian strategies can be deployed with some<br />difficulty.<br />Deterministic computer experiments present a wholly different set of challenges, pri-<br />marily because concepts such as randomization and replication play no role and predictive<br />accuracy of the model is affected solely by bias. Because physical experimentation is ab-<br />sent, the constraints on experimental size are typically caused by the time it takes to<br />make runs of the code. Such constraints are often vague and flexible. Where budget<br />issues prevail (“you get this much computer time to make your runs”) the choice of sam-<br />ple size, n, is taken out of our hands. Nevertheless, it is useful to have some practical<br />guidance in choosing n and to know if the selected n is adequate to achieve stated goals.<br />In addition to guiding an experimenter in the choice of n for a specific experiment, we<br />will consider more general questions. In particular, what is the role of dimensionality of<br />the input space? If the curse of dimensionality applies, high-dimensional problems might<br />require huge, even intractable, sample sizes for good prediction accuracy. On the other<br />hand, if the total sensitivity of the function to all input variables is kept fixed, with this<br />sensitivity just spread over more input variables, dimensionality might conceivably have<br />a limited effect on accuracy, as in Monte Carlo integration. In this article, how total<br />sensitivity grows with dimension and how this sensitivity is spread across the dimensions<br />are key to understanding prediction accuracy, and hence sample size. Indeed, the article<br />is really about defining the properties of functions that arise in practice, from which<br />simple rules about sample size follow for that class of problems.<br />Little has been written on this topic. Among the few exceptions, Chapman et al.<br />(1994) and Jones et al. (1998) used the often quoted rule of selecting a sample size<br />that is 10 times the number of inputs. Although this rule has proved useful in practice<br />it lacks theoretical underpinning. One theoretical exploration by Chen (1996) showed<br />that, for a single varying input to the computer code whose output is under study, the<br />order of the prediction error is n−nfor very smooth output functions and for an equally<br />2</p>  <p>Page 4</p> <p>spaced design. In higher dimensions, Chen (1996) produced results on rates for product<br />designs. Though these rates are instructive, product designs are impractical and more<br />precise understanding of prediction error is needed for choosing a sample size in practical<br />settings. The key conclusion we arrive at is that the empirically based recommendation<br />of n = 10d is a good path to follow for a large class of problems.<br />An Example<br />Yi et al. (2005) studied a computer model of ligand activation of G-protein in yeast<br />where the computer code takes four inputs and solves a system of ordinary differential<br />equations (details are in Section 3). Following the path taken in the literature since<br />1989 (Sacks et al., 1989b; Currin et al., 1991), approximate the computer output using a<br />Gaussian Process (GP) constructed from a set of code runs. The question that concerns<br />us here is: How many runs are needed to obtain adequate prediction accuracy at untried<br />test points? In the G-protein example, the code is relatively quick to run and we are able<br />to investigate the effect of n on the prediction error by making runs for various values of<br />n. For each value of n the code was evaluated at inputs from an n-point maximin Latin<br />hypercube design (LHD) in 4 dimensions (McKay et al., 1979; Morris and Mitchell, 1995).<br />The plot in Figure 1 shows the square root of the integrated mean squared error (RIMSE)<br />for predictions using the GP model, for various choice of n. (RIMSE is computed for<br />both a set of 120 hold-out points and by leave-one-out cross-validation.) The minor<br />improvement in RIMSE for sample sizes greater than 40 = 10d is a feature of many<br />problems.<br />In general, characterization of the factors affecting approximation accuracy, and hence<br />sample size, requires precise formulation of the goals of the experiment. Such a formu-<br />lation is often elusive, however. We restrict attention to the experimental objective of<br />approximating the code on the basis of sample runs. Even here, the choice of measures<br />of accuracy is open to subjective judgment. Those we use are given in (5) and (6) be-<br />low. Issues such as optimization of a target criterion could bring other considerations,<br />especially that of fully sequential experimentation.<br />We have obscured the role of the design of the location of inputs in this process. Con-<br />3</p>  <p>Page 5</p> <p>3035 40 4550 55 606570 7580<br />0<br />0.01<br />0.02<br />0.03<br />0.04<br />0.05<br />0.06<br />0.07<br />0.08<br />0.09<br />0.1<br />sample size<br />RIMSE<br />Figure 1: Root integrated mean squared error (RIMSE) of prediction against n for the<br />G-protein example. The solid line shows RIMSE computed for a hold-out sample; the<br />dashed line shows RIMSE from leave one out cross-validation.<br />siderable experience built up over a number of applications leads us to restrict attention<br />to designs that are space-filling and, for the problems we address this is well managed<br />by maximin LHDs (which we used in the G-protein example), but simpler to construct<br />zero-correlation LHDs (Gough and Welch, 1994; Owen, 1994) could also be deployed.<br />While there are many issues that can be addressed in determining sample size we<br />focus on these:<br />• Is n = 10d a good rule? What are the limitations of such a rule?<br />• How does accuracy increase with n? When are feasible sample sizes available?<br />• What impact do criteria have on assessing accuracy?<br />• What should be done when a criterion for accuracy is not met?<br />We will partially answer these questions, enough to provide useful practical advice for the<br />choice of n. Our approach to this problem investigates properties of the GP and the effect<br />of n on prediction by first finding connections between the design and the complexity of<br />4</p>  <p>Page 6</p> <p>the problem and then conducting a simulation study. The simulations focus on deciding<br />if n = 10d is a reasonable rule and characterizing the complexity of problems that can<br />be dealt with using n = 10d.<br />The paper is organized as follows. Section 2 reviews the GP model and gives specific<br />formulations of the measures of accuracy we use. Section 3 explores the G-protein ex-<br />ample in more detail. Section 4 investigates the relationships among dimension, sample<br />size and complexity of the problem that guide the simulation study in Section 5 and 6.<br />Section 7 discusses strategies for a follow-up experiment to augment an initial design and<br />the implications for several examples. Finally, in Sections 8 and 9 we comment on open<br />and future issues and summarize our conclusions.<br />2 The Gaussian Process Model<br />A complex computer code mathematically describes the relationship between several<br />input variables and one or more (possibly functional) output variables. Usually, the<br />computer model of interest is computationally demanding, and scientific objectives like<br />optimization would require too many evaluations if the code is used directly. As a con-<br />sequence, strategies relying on computationally efficient statistical approximation (emu-<br />lation) of the code have been developed and have proved effective. Following the path<br />taken in the literature since 1989 (Sacks et al., 1989a,b; Currin et al., 1991; O’Hagan,<br />1992), we place a homogeneous Gaussian process prior on the possible output functions,<br />which leads to an approximator given by the posterior mean conditional on the data<br />from the computer experiment. Although the output from the computer model is often<br />multivariate, we will restrict our attention to scalar output. The results for scalar output<br />can be carried over by using principal component analysis or wavelet decompositions of<br />functional output as in Higdon et al. (2005) and Bayarri et al. (2007).<br />The computer code output is denoted by y(x), where the code’s vector-valued input,<br />x = (x1,...,xd), is assumed to be a point in a d-dimensional unit cube. As long as the<br />input space is rectangular, there is no loss of generality here because any rectangle can<br />5</p>  <p>Page 7</p> <p>be transformed simply to the unit cube with only trivial implications for the analysis<br />method to be described.<br />The GP model places a prior on the class of possible y(x) functions. Let Y (x) denote<br />the random function whose distribution is determined by the prior. Specifically, we take<br />Y (x) = µ + Z(x),<br />where µ is a mean parameter and Z(x) is a Gaussian stochastic process with mean zero<br />and constant variance σ2. In this model, the correlation structure is crucial to prediction.<br />At two input vectors, x and x′, we take the correlation between Y (x) and Y (x′) as<br />R(x,x′) = exp(−h(x,x′)), (1)<br />where<br />h(x,x′) =<br />d<br />?<br />j=1<br />θj|xj− x′<br />j|pj, (2)<br />is a measure of distance between x and x′with weights θj ≥ 0 and distance-metric<br />parameters 1 ≤ pj≤ 2.<br />Experience in a variety of circumstances (Higdon et al., 2004; Linkletter et al., 2006)<br />suggests that very smooth, even analytic, output is typical, especially in engineering<br />contexts. As such it is often the case that pj is fixed at 2 for all j, leading to the<br />Gaussian correlation function. We adopt this special case for most of the article, but<br />return to the issue of pj&lt; 2 in Sections 7 and 8. With pj= 2, it is easily shown that<br />E<br />????<br />∂Y (x)<br />∂xj<br />????<br />2<br />= 2σ2θj.<br />Hence, the weight θjmay be interpreted as a measure of the “sensitivity” of Y (x) to xj.<br />Characterizing the distribution of the distances in (2) across design points as a function<br />of the values of the sensitivity measures, θ1,...,θd, (Section 4) leads to an understanding<br />of the the factors affecting prediction accuracy and hence sample size.<br />Suppose we make n runs of the code at a design D of input vectors x(1),...,x(n)in<br />[0,1]d, leading to the data y = (y(x(1)),...,y(x(n)))T. The predictorˆY (x) of Y (x) is the<br />6</p>  <p>Page 8</p> <p>posterior mean of Y (x) given the data and θ = (θ1,...,θd):<br />ˆY (x) = E (Y (x)|y,θ) = ˆ µ + rT(x)R−1(y −ˆ µ1), (3)<br />where r(x) = (R(x,x(1)),...,R(x,x(n)))Tis an n × 1 vector, R is an n × n matrix with<br />element i,j given by R(x(i),x(j)), and ˆ µ is an estimate of µ, often from the method of<br />maximum likelihood. The mean squared error (MSE) ofˆY (x), taking account of the<br />uncertainty from estimating µ by maximum likelihood, is given by<br />MSE(ˆY (x)) = E<br />?ˆY (x) − Y (x)<br />?2<br />= σ2<br />?<br />1 − rT(x)R−1r(x) +(1 − 1TR−1r(x))2<br />1TR−11<br />?<br />,<br />(4)<br />where 1 is an n × 1 vector with all elements equal to 1. In practice, σ2and θ also have<br />to be estimated, again often by maximum likelihood (Welch et al., 1992).<br />MSE in (4) can be directly computed given an experimental design and θ, and is<br />used in Section 4 for theoretical arguments. However, for our empirical studies we take a<br />different path to define prediction accuracy by using leave-one-out cross-validation(CV)<br />(Currin et al., 1991; Chapman et al., 1994; Gough and Welch, 1994) as follows.<br />Given a design D with sample size n and code runs y, denote the cross-validated<br />prediction of y(x(i)) byˆY−i(x(i)), which is the predictor (3) from the n−1 runs excluding<br />run i. Then the cross-validated error of prediction isˆY−i(x(i)) − y(x(i)) for i = 1,...,n.<br />Average and maximum measures of error based on cross-validation are given by<br />?<br />n<br />i=1<br />?<br />?<br />?1<br />n<br />?<br />?ˆY−i(x(i)) − y(x(i))<br />?2<br />and<br />max<br />x(1),...,x(n)<br />???ˆY−i(x(i)) − y(x(i))<br />???.<br />We also normalize for the scale of the function by dividing by the range of the values of<br />y in the data, leading to the following inaccuracy summaries:<br />eavg=<br />?<br />1<br />n<br />?n<br />i=1<br />?ˆY−i(x(i)) − y(x(i))<br />range of y(x(1)),...,y(x(n))<br />?2<br />(5)<br />7</p>  <p>Page 9</p> <p>and<br />emax=<br />max<br />x(1),...,x(n)<br />range of y(x(1)),...,y(x(n)).<br />???ˆY−i(x(i)) − y(x(i))<br />???<br />(6)<br />The tolerable level of inaccuracy will be application-specific, but we will typically take<br />eavg&lt; 0.1 as the target for a “useful” approximation of the code.<br />For questions relating to the sample size of an initial design, we do not have data<br />available, at least not code data. But we can simulate data using the GP model with<br />given θ. We will distinguish e depending on whether the data are from code runs or from<br />simulations by eavg|codeand eavgrespectively.<br />Before code runs are made, we can perform replicate simulations of the random func-<br />tion and obtain a collection of eavg values, which will then provide an empirical dis-<br />tribution of (5). The average of the simulations will then be an estimate of expected<br />inaccuracy as formulated in (5). Similarly we can get an estimate of expected accuracy<br />as formulated in (6).<br />Why proceed with simulations rather than attempt direct computation of expected<br />values, for example? There are four reasons:<br />1. The ratios in (5) and (6) are appealing measures of accuracy. Producing expected<br />values or other quantities of these measures are hopeless without simulation; they<br />are readily estimated via simulation.<br />2. As described in Section 7, after the sample size is selected and the computer exper-<br />iment is run we can evaluate e·|codeand, with the information from the simulations,<br />especially their empirical distribution, we can gauge whether the GP model and<br />sample size are well matched to the actual code.<br />3. Even if we take expectation and remove all randomness in the data, there are<br />other sources of randomness in practice. Most experimental designs are isomorphic<br />with respect to various symmetries such as interchanging the columns. Different<br />versions of the design within the equivalence class would lead to different measures<br />of prediction error, even after taking expectation with respect to the data.<br />8</p>  <p>Page 10</p> <p>3G-protein Computer Code<br />The ligand activation of G-protein in yeast is described by Yi et al. (2005). The computer<br />code solves a system of ordinary differential equations (ODEs) with nine parameters that<br />can vary. The system dynamics, the differential equations, are given by:<br />˙ η1 = −u1η1x + u2η2− u3η1+ u5<br />˙ η2 = u1η1x − u2η2− u4η2<br />˙ η3 = −u6η2η3+ u8(Gtot− η3− η4)(Gtot− η3)<br />˙ η4 = u6η2η3− u7η4<br />y = (Gtot− η3)/Gtot,<br />where η1,...,η4are concentrations of four chemical species, ˙ ηi≡<br />tration of the ligand; u1,...,u8is a vector of 8 kinetic parameters; Gtotis the (fixed)<br />∂ηi<br />∂t; x is the concen-<br />total concentration of G-protein complex after 30 seconds; and y is the normalized con-<br />centration of a relevant part of the complex. In one study (Feeley et al., 2007), five of<br />these kinetic parameters are fixed (only allowing x, u1, u6, and u7to vary). The GP<br />model is used to construct an approximation as a function of the transformed variables<br />log(x),log(u1),log(u6),log(u7) each of which is further transformed to [0,1].<br />The ODE solver is quick to run and enables us to evaluate the affect of n on the<br />criterion in (5) using a real model. The design points at which the code is run are<br />selected by using maximin LHDs. These space-filling designs have proved to be highly<br />effective in the study and application of computer experiments.<br />The values of n we use are multiples (7,10,15,20) of the dimension, 4, and also<br />include 33, the number of runs made in the Feeley et al. (2007) study. For each choice of<br />n, we run the ODE solver to obtain data {y(x(i));x(i)∈ D}. The data are modeled as if<br />they were the realizations of a GP, and maximum likelihood estimates of ˆ µ, ˆ σ andˆθ are<br />obtained for the parameters of the GP (see Section 2) for each choice of n. For each value<br />of n, we use the code runs to calculate eavg|codefrom (5), except no normalization for the<br />range is made here. (In any case, the normalization factor is close to 1 at about 0.8 and<br />9</p>  <p>Page 11</p> <p>makes little difference.) We also compare this measure with the analog from a set of new<br />test points. We generate an additional independent 120-point maximin LHD, D0, and<br />evaluate the ODE solver to obtain data for the out-of-sample test points. The same 120<br />test runs are used for all evaluations. Using the test sample, the analogous version of (5)<br />is computed by replacing the average in the numerator by<br />1<br />120<br />?120<br />i=1<br />?ˆY (x) − y(x)<br />?2.<br />The plot in Figure 1 shows how the two unnormalized eavg|codemeasures behave as n<br />changes. A major point is that eavg|codechanges little as n increases past 40, nor is there<br />any substantial difference between using cross validation instead of a new test sample.<br />That cross validation leads to larger errors is not surprising, since leaving out one point<br />can produce a big gap making it hard to predict the omitted point. This is relevant<br />because the use of new test data is a luxury, only enjoyed if the code can be run quickly,<br />and so we rely on cross validation for measuring accuracy.<br />Judging the quality of prediction over a wide range of scenarios is simply not possible<br />through runs of the computer code unless the code is very quick to run. Therefore, we<br />will rely on simulated data generated by using a GP. Because a GP model often has<br />similar properties to those of the computer codes we expect to encounter in practice, we<br />are at least close to mimicking reality. Before simulating, however, we need to know the<br />important factors in function complexity, so that an efficient and insightful simulation<br />study may be conducted.<br />4 Effect of d, θ, and n on prediction accuracy<br />Intuitively, we know that when we predict Y (x) at some x, the design-point neighbors<br />of x will tend to be closer as n becomes larger, improving accuracy. If θ has many<br />large values, however, the correlation between Y (x) and Y for the neighbors will be<br />low, even for nearby points, leading to poorer prediction accuracy. Here, we develop this<br />intuition into some quantitative rules relating d, θ, and n to distances and the correlation<br />structure, shedding some light on how prediction accuracy depends on these quantities.<br />First, we consider how the theoretical mean squared error, MSE(ˆY (x)) in (4), depends<br />10</p>  <p>Page 12</p> <p>on d, θ, and n. Recall that the empirical definitions of prediction accuracy in (5) and (6)<br />are normalized for scale. Similarly, without loss of generality, we can ignore σ2in a<br />normalized version of mean squared prediction error:<br />MSEnorm(ˆY (x)) = 1 − rT(x)R−1r(x) +(1 − 1TR−1r(x))2<br />1TR−11<br />. (7)<br />We see that MSEnorm(ˆY (x)) is determined by R and r(x) only. Thus, it is a function<br />of n—as R and r(x) are an n × n matrix and an n × 1 vector, respectively—and the<br />correlations in R and r(x). Dimensionality, d, affects MSEnorm(ˆY (x)) only indirectly via<br />these correlations.<br />For simplicity, we will explore the factors affecting MSEnorm(ˆY (x)) for completely<br />random Latin hypercube designs (where the columns are permuted independently). We<br />consider the case where n is fixed and look at the effect of d and θ1,...,θd on the<br />correlation structure and on MSEnorm(ˆY (x)) averaged over x. Our argument establishes<br />two main results:<br />1. For moderately large d and a random LHD, the distribution of inter-point squared<br />distance (weighted by θ1,...,θd) in (2) can be approximated by a normal distribu-<br />tion, with mean and variance given by simple functions of θ1,...,θd. The approxi-<br />mate distribution of (off-diagonal) correlations in R follows from the transformation<br />in (1).<br />2. Under the same conditions, the distribution of correlations in r(x) for x drawn<br />randomly from [0,1]dis similar to the distribution of correlations in R.<br />We recognize that the matrix inverse in (7) makes MSEnormmuch more complicated<br />than can be explained by these distributions of correlations. Nonetheless, the simula-<br />tions in Section 5 and 6 show that the factors affecting the correlation distribution explain<br />much of the effect of d and θ1,...,θdon our empirical accuracy measures. Indeed, un-<br />derstanding these factors leads to simulation studies with straightforward interpretation.<br />Take two points, x and x′, at random from a random LHD. An LHD is defined here<br />11</p>  <p>Page 13</p> <p>to have fixed grid points {0,1/(n − 1),...,1} for each variable xj. Let<br />hj= |xj− x′<br />j|<br />be the unweighted distance in dimension j appearing in h in (2). The first two moments<br />of h2<br />jare given by Lemma 1.<br />Lemma 1: Let hj be the distance between two randomly chosen points xj and x′<br />jin<br />dimension j for a random LHD. Then<br />P(hj= i/(n − 1)) =n − i<br />?n<br />2<br />?<br />for i = 1,...,n − 1,<br />E(h2<br />j) ≡ m1(n) =1<br />6<br />n(n + 1)<br />(n − 1)2,<br />and<br />Var(h2<br />j) ≡ m2(n) =<br />1<br />180<br />n(n − 2)(n + 1)(7n + 9)<br />(n − 1)4<br />.<br />The proof of Lemma 1 can be found in Appendix A. Note that the two moments converge<br />to 1/6 and 7/180 as n → ∞.<br />If d = 1 than the probability distribution P(hj = i/(n − 1)) in Lemma 1 exactly<br />describes the distribution of all possible distances between distinct points x and x′. That<br />is, since the design points are on the grid, every possible distance i/(n − 1) occurs n − i<br />times.<br />If d &gt; 1, however, not all of the possible distances over all dimensions will be ob-<br />served in any one design, and we rely on the moments given in Lemma 1 to describe<br />behavior. Specifically, for two randomly chosen points, the squared distance in (2) across<br />all dimensions which arises if pj= 2 has expectation<br />E(h) = m1(n)<br />d<br />?<br />j=1<br />θj. (8)<br />For a completely random LHD, which has independently permuted columns,<br />Var(h) = m2(n)<br />d<br />?<br />j=1<br />θ2<br />j. (9)<br />12</p>  <p>Page 14</p> <p>Furthermore, as d increases, the central limit theorem applies unless there are a few<br />θj weights that dominate, so that h is approximately normal with mean and variance<br />given by (8) and (9). Hence, the correlation in (1) is approximately log-normal with<br />these moments (after a change of sign). Figure 2 compares the empirical distributions<br />for a single random LHD with the approximations, for d = 10, n = 100, and θ =<br />(2.71,2.17,1.69,1.27,0.91,0.61,0.37,0.19,0.07,0.01) the approximations are seen to be<br />good. The values chosen for θj comprise a canonical configuration, to be explained in<br />Section 5.<br />012345<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />0.35<br />0.4<br />0.45<br />0.5<br />h<br />density<br />0 0.2 0.4<br />correlation<br />0.60.81<br />0<br />0.5<br />1<br />1.5<br />2<br />2.5<br />3<br />3.5<br />4<br />density<br />Figure 2: Distribution of squared distance (left panel) and correlation (right panel) for<br />a randomly chosen pair of points from a random Latin hypercube design with d = 10,<br />n = 100, and θ = (2.71,2.17,1.69,1.27,0.91,0.61,0.37,0.19,0.07,0.01)<br />Similarly, Figure 3 shows the analogous distributions for the vector r(x). The em-<br />pirical distribution of the distance and the correlation are taken over a single random<br />LHD, D, and the distance between x and all n points in D is computed for 50 randomly<br />chosen test point x ∈ [0,1]d. The same normal or log-normal approximations established<br />above for inter-point distance or correlation are transferred to test-point to design-point<br />distance or correlation. It is seen that the correlations in r(x) behave like those in R.<br />Note that there is a negative impact on prediction accuracy when the mean distance<br />13</p>  <p>Page 15</p> <p>012345<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />0.35<br />0.4<br />0.45<br />0.5<br />h<br />density<br />0 0.2 0.4<br />correlation<br />0.6 0.81<br />0<br />0.5<br />1<br />1.5<br />2<br />2.5<br />3<br />3.5<br />4<br />density<br />Figure 3: Distribution of squared distance (left panel) and correlation (right panel) for a<br />randomly chosen test point and a random Latin hypercube design with d = 10, n = 100,<br />and θ = (2.71,2.17,1.69,1.27,0.91,0.61,0.37,0.19,0.07,0.01)<br />in (8) increases or when the variance in (9) decreases. When the variance decreases, small<br />squared distances (high correlations) are less likely, whereas high-correlation neighboring<br />points lead to good prediction accuracy. Indeed, if we want to predict Y (x), just one<br />close design neighbor, x(i), of x, close in the sense of correlation, may by itself give good<br />prediction accuracy. Let ρ = exp(−h(x,x(i))) be the correlation for these two points.<br />Predicting using only Y (x(i)) provides an upper bound on MSEnormobtained from (7).<br />As R is the scalar 1, and we have<br />MSEnorm(ˆY (x)) &lt; 1 − ρ2+ (1 − ρ)2= 2(1 − ρ).<br />If ρ is larger than about 0.95, or equivalently h is less than about 0.05, MSEnorm(ˆY (x)) &lt;<br />(10)<br />0.1, the target we have set.<br />The magnitudes of the correlations in R and r(x), which lead to MSEnorm in (7),<br />depend on τ =?d<br />practical consequences. First, these two quantities are used to plan the simulations in<br />j=1θj and ψ =?d<br />j=1θ2<br />jto a good approximation. There are two<br />Section 5 and 6. We find that the behavior of the empirical analog, eavg, of MSEnorm<br />is largely dependent on τ and ψ. Secondly, the distributions of the correlations in r(x)<br />14</p>  <p>Page 16</p> <p>(for random test points) and in R (between design points) are similar. The implication<br />is that accuracy estimates will be similar from cross validation based on leaving out one<br />design point at a time versus random test points.<br />There are many possible θ1,...,θdconfigurations, and we examine three special cases,<br />ordered from worst to best behavior in terms of the effect of dimensionality.<br />1. Suppose θ1= ··· = θd= θ, i.e., as dimensionality increases, further equally active<br />variables are added. Then, τ = dθ and ψ = dθ2. Thus, the mean of the distribution<br />of h increases linearly with d, the standard deviation of the distribution increases as<br />√d, and the h distribution becomes stochastically larger with d. For large enough<br />d, prediction accuracy will be poor, even if θ is small.<br />2. Suppose τ is kept constant, i.e., a fixed amount of total sensitivity is spread across<br />all dimensions. Clearly, ψ takes its minimum value of ψ = τ2/d when θ1= ··· =<br />θd= τ/d. Thus, equally active factors are worst for prediction accuracy. Moreover,<br />as ψ = τ2/d decreases with d, this effect becomes worse as d increases. For large<br />enough d, the h distribution will become concentrated at its mean, m1(n)τ, and<br />the limiting accuracy depends on τ. In this sense, if the total amount of sensitivity<br />is kept constant, the worst-case effect of dimensionality is small.<br />3. Alternatively, suppose we keep τ and ψ constant as d increases. Write<br />ψ =<br />d<br />?<br />j=1<br />θ2<br />j=<br />d<br />?<br />j=1<br />(θj−¯θ)2+1<br />dτ2. (11)<br />Because the second term on the right decreases with d,?d<br />to keep ψ constant. Another way of looking at the fact that θ1,...,θdmust become<br />j=1(θj−¯θ)2must increase<br />more variable with d to maintain prediction accuracy is that some dimensions are<br />more active than others, or there is effect sparsity.<br />The argument that accuracy decreases as τ =?d<br />decreases is borne out by the simulations in Section 5 and 6.<br />j=1θjincreases or as ψ =?d<br />j=1θ2<br />j<br />15</p>  <p>Page 17</p> <p>The quantitative effect of n on accuracy is less obvious, however. The mean and<br />variance of the squared distance distribution in (8) and (9) do not depend on n in<br />the limit. Thus, R and r(x) in (7) have elements that depend only weakly on n in this<br />statistical sense. Rather, MSEnormdepends on n because R and r(x) have more elements.<br />The closest neighbor in the bound (10) will tend to become closer with larger n, thus<br />driving the bound down. A full analysis of the impact of using all n design points is<br />complicated by the inverse of R in (7). All that we can say is that harder problems<br />(larger τ and smaller ψ) will require larger sample sizes, regardless of dimensionality<br />to a large extent. This insight greatly facilitates quantification of the impact of n by<br />simulation in Section 5 and 6.<br />If p1= ··· = pd, but the common value is less than 2, m1(n) and m2(n) in Lemma 1<br />will change. The mean and variance of h in (8) and in (9) will still depend on τ and ψ,<br />however.<br />5 Simulation Results for Average Error<br />The arguments in Section 4 suggest that the effect of the correlation parameters on eavg<br />is through τ and ψ, thereby diminishing the role of d. To investigate this further we will<br />engage a simulation study that changes dimension but keeps τ,ψ fixed.<br />But before doing so we must decide on the configurations of the θ vectors to be<br />explored. Past experience has indicated that for well-behaved outputs there may be<br />a few large components of θ, a few moderately sized, and the remainder small. For<br />example, for the G-protein model and the 33-run experiment,ˆθ = (1.71,0.29,0.27,0.25)<br />has one moderate value and the other three are small. From this point of view, we will<br />adopt a two-parameter class of canonical configurations of θ, defined by<br />??<br />Here θj decreases in j and?d<br />characteristics we expect, especially as d gets large. Examples of θ configurations for<br />θj= τ<br />1 −j − 1<br />d<br />?b<br />−<br />?<br />1 −j<br />d<br />?b?<br />for j = 1,...,d,and b ≥ 1,τ &gt; 0. (12)<br />j=1θj = τ. The generated θ vector tends to have the<br />16</p>  <p>Page 18</p> <p>d = 10 and τ = 1 are given in Table 1. When τ ?= 1, the value of θ is found by<br />multiplying each θjin the table by τ.<br />bψθ1<br />θ2<br />θ3<br />θ4<br />θ5<br />θ6<br />θ7<br />θ8<br />θ9<br />θ10<br />1 0.1 0.10.10.10.1 0.10.1 0.10.1 0.1 0.1<br />3 0.18 0.2710.217 0.1690.127 0.0910.0610.037 0.019 0.0070.001<br />9 0.45 0.6130.2530.094 0.0300.0080.0020000<br />Table 1: Configurations of θ for d = 10<br />Data for the simulation study are generated as follows. Given d and n, select a<br />maximin LHD D of n points in [0,1]d. Fix values of µ = 0,σ2= 1, p = 2 and select<br />a canonical θ (as specified above) for the parameters of the GP given in (2). Gener-<br />ate 50 independent realizations of the GP resulting in 50 different sets of observations<br />{y(x(i));x(i)∈ D}. Since, the measure of accuracy in (5) or (6) is standardized by the<br />range, the particular value of σ2= 1 is largely irrelevant.<br />For each data set form a predictor using (3) with the value of θ the same as that used<br />to generate the simulated data. Alternatively, for each data set, we could estimate θ<br />and construct a predictor withˆθ. We found that there is no essential difference between<br />predictors based on θ andˆθ in terms of our summary measures of prediction accuracy,<br />and using the fixed θ takes much less time in our extensive study. The predictor leads<br />to a value of eavgin (5) for each data set.<br />We start with d = 5 and b = 1 in (12), which results in θj = τ/5,j = 1,...,5. As<br />argued in Section 4, this choice of θ minimizes ψ for a fixed τ and represent a “worst<br />case” starting point. For a given τ value, ψ = τ2/5 when d = 5. If τ and ψ are kept<br />constant as d changes, then the canonical θ vector must satisfy?θ2<br />15, and 20, this means that b = 3.445, 5.507, and 7.55, respectively, in (12). Values of<br />j= τ2/5. For d = 10,<br />τ = 3, 10, 20, and 40 are chosen to cover problems from “easy” to “very hard”.<br />The arguments in Section 4 suggest that similar accuracy should be obtained in any<br />dimension for fixed values of n, τ and ψ, but intuitively we expect that n must increase<br />17</p>  <p>Page 19</p> <p>with d. Our results for the first two moments may not fully explain the behavior of<br />the tails of the distribution of h, and small distances in particular play a prominent role.<br />Thus, we allow n to increase modestly with d, specifically linearly. We also allow different<br />rates, i.e., n = kd, where k = 7, 10, 15, or 20.<br />The four panels in Figure 4 correspond to τ = 3, 10, 20, and 40, respectively. In each<br />panel, four curves are plotted, for d = 5, 10, 15, and 20, respectively. A curve shows<br />the mean of eavgcomputed from the 50 realizations of the GP, which we denote by ¯ eavg,<br />plotted against k (recall n = kd). Several features of these plots are worth singling out:<br />• All curves lie below 0.20 suggesting that even in very hard problems (τ = 40) the<br />average error does not get extremely large.<br />• The case of τ = 3 represents an “easy” problem owing to the small components of<br />θ.<br />• When ψ is fixed, the curves for d = 5, 10, 15, and 20 are all quite close.<br />• The choice of n = 10d leads to predictions that on average are accurate to within<br />10% of the range of the data providing that τ ≤ 10; reliable fits are barely obtainable<br />or not obtainable for τ ≥ 20.<br />• The improvement in fit for sample sizes greater than n = 10d is marginal.<br />Suppose ¯ eavgdecreases with n approximately at the convergence rate n−c. The rate<br />c can be estimated from the points shown in Figure 4 from the slope of the least squares<br />fit of log(¯ eavg) regressed on log(k). The estimated rates are in Table 2.<br />There are a few interesting things to notice in Table 2. For easy problems (τ = 3)<br />convergence rates close to 1 are achievable for dimensions as large as d = 20 so that<br />doubling sample size can reduce eavg by half. On the other hand, in hard problems<br />the rates of convergence can be very small. For example, when d = 15 and τ = 20,<br />it takes about 8 times as many runs to reduce eavg by half. When τ = 40 it appears<br />hopeless to reduce eavgsubstantially without enormous sample sizes. In such situations,<br />the computer experiment may have to be reformulated and restricted.<br />18</p>  <p>Page 20</p> <p>10 15 20<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />k<br />τ=3<br />e<br />1015 20<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />k<br />τ=10<br />e<br />101520<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />k<br />τ=20<br />e<br />101520<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />k<br />τ=40<br />e<br />Figure 4: The four panels correspond to four values of τ, and ψ = τ2/5. In each panel,<br />¯ eavgis plotted against k for d = 5 (solid line), d = 10 (dashed line), d = 15 (dotted line)<br />and d = 20 (dot-dashed line).<br />τ<br />d3 1020 40<br />5 1.340.63 0.430.08<br />100.97 0.570.31 0.14<br />15 0.96 0.530.360.13<br />20 0.870.51 0.28 0.19<br />Table 2: Estimated convergence rates for ¯ eavg<br />The arguments in Section 4 suggest that for fixed total sensitivity τ, dividing τ equally<br />across the d input variables is the worst case for prediction accuracy, i.e., ψ = τ2/d.<br />Figure 5 explores worst-case problems by plotting eavg against τ. There is a separate<br />19</p>  <p>Page 21</p> <p>plot for d = 5,10,15,20, and n = 10d throughout. Fifty simulated realizations are made<br />for each value of τ. The lines in Figure 5 drawn through the averages of eavg show<br />1020 3040<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />τ<br />e<br />d=5<br />10 2030 40<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />τ<br />e<br />d=10<br />10 2030 40<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />τ<br />e<br />d=15<br />10 20 3040<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />τ<br />e<br />d=20<br />Figure 5: The four panels correspond to d = 5, 10, 15, and 20, respectively. In each<br />panel, eavg(squares) from 50 realizations and ¯ eavg(solid line) are plotted against τ. The<br />horizontal line indicates accuracy to within 10% of the range of the data.<br />little difference as d increases, as predicted in Section 4 for the worst case studied here.<br />There is a small dimensionality effect (and n = 10d is increasing with d), but the total<br />sensitivity, τ, is the important factor. For τ ≥ 20, eavg is above the target of 0.1 for<br />all d studied, though it tends to remain below about 0.2 to 0.25. This latter fact is a<br />somewhat surprising feature and, in fact, holds for τ as large as 100 (not shown).<br />To investigate the more realistic situation where the problem has some degree of<br />sparsity we allow ψ to vary. We fix d = 10 and n = 100. As suggested by Figure 4, for<br />fixed values of τ and ψ, results for other values of d (with n = 10d) are similar. For each<br />fixed value of τ we increase the value of ψ so that the sparsity is increased, and the total<br />sensitivity of the function is shifted to fewer and fewer dimensions.<br />20</p>  <p>Page 22</p> <p>246<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />ψ<br />e<br />τ=3<br />2040 60<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />ψ<br />e<br />τ=10<br />50 100150200 250 300<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />ψ<br />e<br />τ=20<br />200400 600 800 10001200<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />ψ<br />e<br />τ=40<br />Figure 6: The four panels correspond to four values of τ. In each panel, eavg(squares)<br />from 50 realizations and ¯ eavg (solid line) are plotted against ψ. The horizontal line<br />indicates accuracy to within 10% of the range of the data.<br />Even a moderate degree of sparsity can result in drastic reduction of error. The<br />τ = 40 panel in Figure 6 is interesting, since even in such a complex problem reasonable<br />accuracy can be obtained when there is a degree of sparsity. In particular the last few<br />values of ψ represent situations where the 10-dimensional problem contains five or fewer<br />active dimensions.<br />6 Simulation Results for Maximum Error<br />In Section 5 results were presented corresponding to ¯ eavg; in this section we discuss the<br />similarities and differences that arise in using ¯ emax, i.e. the average of emaxin (6) across<br />simulations. The four panels in Figure 7 correspond to τ = 3, 10, 20, and 40, respectively.<br />In each panel, curves for ¯ emax(averaged across 50 simulations) are plotted for d = 5, 10,<br />21</p>  <p>Page 23</p> <p>15, and 20. Clearly, one would not expect to see the same level of accuracy as obtained<br />using ¯ eavgso we set a threshold of 0.2 as opposed to the threshold of 0.1 used for eavg.<br />Comparing this to the plots in Figure 4 there a few things to notice:<br />101520<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />k<br />τ=3<br />e<br />101520<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />k<br />τ=10<br />e<br />101520<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />k<br />τ=20<br />e<br />101520<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />k<br />τ=40<br />e<br />Figure 7: The four panels correspond to four values of τ and ψ = τ2/5. In each panel<br />¯ emaxis plotted against k for d = 5 (solid line), d = 10 (dashed line), d = 15 (dotted line)<br />and d = 20 (dot-dashed line).<br />• The case of τ = 3 again represents an “easy” problem owing to the small compo-<br />nents of θ.<br />• When τ is fixed, the curves for d = 5, 10, 15, and 20 are all quite close.<br />• The choice of n = 10d leads to predictions that on average are accurate to within<br />20% to 30% of the range of the data providing that τ ≤ 10; reliable fits are barely<br />obtainable or not obtainable for τ ≥ 20. When τ = 10 thresholds for emax are<br />somewhat harder to reach compared to those using eavg.<br />22</p>  <p>Page 24</p> <p>• The improvement in fit for sample sizes greater than n = 10d is marginal.<br />The analysis leading to Table 2 can be duplicated for ¯ emax; the result is Table 3. The<br />convergence rates, as expected, are lower than for ¯ eavgand are essentially 0 when τ = 40.<br />Table 3 can be used in the same way as Table 2 is used in Section 7 to derive sample<br />sizes needed to reduce ¯ emax.<br />τ<br />d3 1020 40<br />5 1.110.41 0.260.00<br />10 0.740.370.140.03<br />150.720.370.220.00<br />200.640.280.110.04<br />Table 3: Estimated convergence rates for ¯ emax<br />Figure 8 explores worst-case problems (equal θj) by plotting emaxagainst τ. There is<br />a separate plot for d = 5,10,15,20, and n = 10d throughout. Fifty simulated realizations<br />are made for each value of τ. The lines in Figure 8 drawn through the averages of emax<br />show little difference as d increases. Comparing this to the analogous plot in Figure 5<br />we see the same general trend using emax as opposed to eavg. There is again a small<br />dimensionality effect, but the total sensitivity, τ, is the important factor. For τ ≥ 20,<br />emaxis above the target of 0.2 for all d.<br />The Figure 9 shows the effect of sparsity when using emaxand corresponds to the plot<br />in Figure 6 for eavg. Even a moderate degree of sparsity can result in drastic reduction of<br />error. The τ = 40 panel in Figure 9 is interesting, since even in such a complex problem<br />reasonable accuracy can be obtained when there is a degree of sparsity. In particular the<br />last few values of ψ represent situations where the 10-dimensional problem contains five<br />or fewer active dimensions. This is similar to what was found in Figure 6.<br />23</p>  <p>Page 25</p> <p>1020 3040<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />τ<br />e<br />d=5<br />1020 3040<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />τ<br />e<br />d=10<br />1020 3040<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />τ<br />e<br />d=15<br />102030 40<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />τ<br />e<br />d=20<br />Figure 8: The four panels correspond to d = 5, 10, 15, and 20, respectively. In each<br />panel, emax(squares) from 50 realizations and ¯ emax(solid line) are plotted against τ. The<br />horizontal line indicates accuracy to within 20% of the range of the data.<br />7Follow-up Experiments<br />Suppose an initial experiment of given sample size has been conducted. We now have<br />real data from running the code to fit a GP model following the methodology described<br />in Section 2. Estimates of the correlation parameters, θ and p, are available, as well as<br />values of eavg|codeand emax|codecomputed from (5) and (6).<br />What should be done to augment the initial design, if anything? Set eAas a threshold<br />value for acceptable eavg (for example, eA= 0.1). If eavg|code &lt; eAthen nothing more<br />needs to be done to increase accuracy. If eavg|code&gt; eAthen we propose the following<br />follow-up strategy:<br />1. Do a simulation study using the estimated correlation parameters,ˆθ and ˆ p, and<br />the initial sample size. Compute eavg(and emax) for each realized data set.<br />24</p>  <p>Page 26</p> <p>246<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />ψ<br />e<br />τ=3<br />204060<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />ψ<br />e<br />τ=10<br />50 100150200250 300<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />ψ<br />e<br />τ=20<br />200 400600 800 10001200<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />ψ<br />e<br />τ=40<br />Figure 9: The four panels correspond to four values of τ. In each panel, emax(squares)<br />from 50 realizations and ¯ emax (solid line) are plotted against ψ. The horizontal line<br />indicates accuracy to within 20% of the range of the data.<br />2. If the distribution of the eavg values from the simulations suggests a non-trivial<br />probability of exceeding eA, it is plausible that the initial sample size is inadequate<br />and we could go to Step 4. Otherwise, continue with Step 3.<br />3. If the distribution of the eavg values from the simulations is inconsistent with<br />eavg|code, the appropriateness of the GP model is in question; see Section 8. Of<br />course, it would be foolish to worry about small inconsistencies, and some subjec-<br />tivity is inevitable in assessing what is substantial.<br />4. To decide on a follow-up sample size, explore plausible choices of n through a<br />simulation study using the estimated correlation parameters,ˆθ and ˆ p.<br />In order to illustrate the points above, we revisit the G-protein example in Section 3.<br />The initial sample size of n = 33 led to eavg|code = 0.0221 (see Figure 1). For most<br />25</p>  <p>Page 27</p> <p>examples this would be considered small, and there would be nothing further to do.<br />On the other hand, if we wanted to reduce the error rate by a half, a simulation could<br />be performed using the outline above. Since ˆ p = 2, we could also consult Table 2 to<br />calculate the desired sample size. From the analysis of the n = 33 runs, ˆ τ = 2.52 and<br />ˆψ = 3.15. For d = 5 and τ = 3, Table 2 gives a rate of convergence of 1.34, which is very<br />likely to be conservative. Thus, a sample size of at least n = 33(21/1.34) = 55 is required.<br />From Figure 1, eavg|codeat n = 60 is 0.0137, which is just slightly larger than half of the<br />observed value when n = 33.<br />Gough and Welch (1994) considered a model for ocean circulation with d = 7 inputs<br />and seven different outputs, y1,...,y7. From an initial experiment yielding 36 good runs,<br />a GP was fit separately for each output. In each case p = 2 for each output y1,...,y7.<br />Estimated values of θ are provided in Table 4. The ˆ τ andˆψ rows in Table 4 summarize<br />y1<br />y2<br />y3<br />y4<br />y5<br />y6<br />y7<br />ˆθ1<br />0.3190.5447.8990.3010.1001.249 2.512<br />ˆθ2<br />00.0260.0010.5520.8270.001 0.003<br />ˆθ3<br />0.6860.012 0.01200.115 0.066 0.037<br />ˆθ4<br />0.267 1.2020.02100.156 0.4780<br />ˆθ5<br />0.0290.197 0.0030.060 0.044 0.1191.463<br />ˆθ6<br />0 0.008 0.006 1.1360.6850.4330.665<br />ˆθ7<br />0.229 0.031 0.1290.069 0.0090.0821.001<br />ˆ τ 1.532.02 8.07 2.121.94 2.435.68<br />ˆψ 0.69 1.78 62.421.69 1.21 2.019.89<br />Table 4: Estimates of θ for the ocean-circulation model.<br />the GP fits from the 36 runs.<br />If eA= 0.1, the values for eavg|codein Table 5 show that a reasonable approximation<br />has been obtained. Moreover, for all output variables, eavg|codeis within ¯ eavg± 2?sd(eavg),<br />i.e., eavg|codelies within the support of the empirical distribution of simulated eavg. The<br />26</p>  <p>Page 28</p> <p>ny1<br />y2<br />y3<br />y4<br />y5<br />y6<br />y7<br />36eavg|code<br />0.0340.0410.0170.037 0.0340.039 0.078<br />¯ eavg<br />0.028 0.033 0.0220.0300.042 0.051 0.072<br />?sd(eavg)<br />?sd(eavg)<br />0.008 0.0110.0080.0090.0110.0260.021<br />70¯ eavg<br />0.0080.011 0.0070.0090.0180.021 0.032<br />0.0040.003 0.0030.0070.0060.0060.009<br />Table 5: Actual and simulated accuracy measures for the ocean-circulation model<br />accuracy for y7is lower than for the other output variables, however, and we consider<br />reducing eavg|codefrom 0.078 to, say, 0.05. In this case ˆ p = 2, and again we can use Table<br />2 for guidance in choosing an appropriate sample size.<br />Interpolating the convergence rates in the table suggest that ¯ eavgshould be decreasing<br />at rate roughly 1/n, and reducing the sample size to 0.05 would require a sample size<br />of approximately 56 runs. Alternatively, cutting ¯ eavgby half would require doubling the<br />run size. As no further code runs are available we investigate this strategy by simulating<br />what would have happened if n = 70 runs had been performed: for y7, ¯ eavgis reduced by<br />just over a factor of 2, as predicted.<br />Chapman et al. (1994) analyzed a computer code describing the seasonal growth and<br />decline of Arctic sea ice. The code had d = 13 input variables and four outputs, y1,...,y4.<br />From an initial design of n1= 69 runs, GPs were fit separately for each output. Every<br />fitted GP had at least one input variables with ˆ pj &lt; 2. Estimated values of θj and<br />αj= 2 − pjfor n1= 69 runs are provided in Table 6.<br />Estimated vales of θjand αj= 2 − pjfor n = 157 runs are provided in Table 7.<br />The values of eavg|codeare in Table 8; each is below 0.1, and if eA= 0.1 we would be<br />tempted to stop.<br />The eavg|codevalues for y3and y4are below 0.1 and within 2?sd(eavg) of ¯ eavgbut the<br />sistent with the observed emax|codevalues, as evidenced by ¯ emaxand?sd(emax) in Table 8,<br />27<br />emax|codevalues of about 0.5 are of concern. The simulated emaxdistributions are incon-</p>  <p>Page 29</p> <p>y1<br />y2<br />y3<br />y4<br />j<br />ˆθj<br />ˆ αj<br />ˆθj<br />ˆ αj<br />ˆθj<br />ˆ αj<br />ˆθj<br />ˆ αj<br />10.0100.0000.0230.0000.021 0.0000.045 0.000<br />20.069 0.0000.011 0.0000.0000.3440.000 1.000<br />30.4550.0000.000 0.0000.5330.000 0.4450.000<br />40.020 0.0000.0210.0000.2150.000 0.2660.000<br />50.0010.000 0.126 0.0000.0001.0000.386 0.000<br />60.024 0.000 0.1050.000 0.0000.0000.0530.038<br />7 0.168 0.0000.0300.000 1.8830.000 0.8720.000<br />80.3010.000 2.1710.0000.252 0.0940.1810.316<br />9 0.058 0.000 0.3590.0000.0010.000 0.0280.000<br />10 0.064 0.0000.0000.000 0.094 0.8640.5540.000<br />11 0.0151.000 0.3860.000 0.990 0.5131.182 0.188<br />120.000 0.907 0.0030.000 0.0000.344 0.0110.000<br />130.0001.000 0.4160.135 0.000 0.0000.000 0.000<br />Table 6: Estimates of θjand αj= 2 − pjfor the sea-ice code using n1= 69 runs.<br />although for y3the range of the simulated emaxvalues covers emax|code. The sea-ice code<br />failed to converge for 12 of 81 attempted runs (hence the 69 good runs), a suggestion of<br />erratic behavior of the code and a possible explanation of the difference between actual<br />and simulated error in some regions of the input space.<br />Faced by similar concerns about the approximation accuracy from the initial experi-<br />ment, Chapman et al. (1994) opted to make additional runs and ended up with a total<br />of 157 good code runs. As these are the only follow-up runs available, we restrict our<br />analysis to seeing whether we can predict by simulation the impact of such a follow-up<br />experiment.<br />The accuracy measures for the n = 157 runs conducted and from simulation are<br />28</p>  <p>Page 30</p> <p>y1<br />y2<br />y3<br />y4<br />j<br />ˆθj<br />ˆ αj<br />ˆθj<br />ˆ αj<br />ˆθj<br />ˆ αj<br />ˆθj<br />ˆ αj<br />10.0320.313 0.030 0.1320.048 0.5790.314 0.000<br />20.015 0.1150.0220.0130.0031.0000.0670.000<br />30.4210.0000.014 0.1580.3240.546 0.3160.503<br />40.007 0.0000.0270.3750.001 0.0000.0000.000<br />50.008 0.100 0.0330.5940.000 0.8200.2200.000<br />6 0.043 0.0000.182 0.0000.0170.3510.2890.000<br />70.216 0.0000.013 0.0002.272 0.0101.7110.015<br />8 0.5630.0001.273 0.1410.2140.322 0.4760.085<br />9 0.093 0.0000.3470.0000.000 0.0000.060 0.000<br />10 0.1820.3220.006 0.536 0.4030.6520.331 0.255<br />11 0.1840.0590.023 0.000 0.9080.3070.357 0.525<br />12 0.000 0.9070.000 0.9070.000 0.9070.000 0.344<br />130.003 1.0000.002 0.817 0.0000.9030.000 0.344<br />Table 7: Estimates of θjand αj= 2 − pjfor the sea-ice code using n = 157 runs.<br />compared in Table 8. Relative to n = 69, simulation suggests only modest reduction in<br />eavg. For y4, even this modest reduction is not realized by eavg|code. With n = 157 runs, the<br />simulated values of emaxare again inconsistent with emax|codefor the troublesome y3and<br />y4. Although the magnitude of the maximum error is underestimated, the simulations<br />correctly predict that there will be little impact on emax|codefrom the further runs. Thus,<br />the simulation study leads to the same conclusion that Chapman et al. (1994) reached<br />after the follow-up experiment: Taking more runs is not effective. Alternative ways of<br />proceeding are discussed in Section 8.<br />29</p>  <p>Page 31</p> <p>ny1<br />y2<br />y3<br />y4<br />Average error<br />69eavg|code<br />0.0430.044 0.0930.099<br />¯ eavg<br />0.048 0.0440.0790.089<br />?sd(eavg)<br />¯ eavg<br />0.011 0.0130.0190.018<br />157eavg|code<br />0.032 0.0310.0790.096<br />0.0290.0290.056 0.062<br />?sd(eavg)0.0080.009 0.0110.011<br />Maximum Error<br />69emax|code<br />0.2490.1240.4660.559<br />¯ emax<br />0.139 0.1280.2250.263<br />?sd(emax)<br />¯ emax<br />0.0390.0520.0710.079<br />157emax|code<br />0.189 0.1160.4460.494<br />0.103 0.0960.1820.203<br />?sd(emax) 0.0350.0330.045 0.055<br />Table 8: Actual and simulated accuracy measures for the sea-ice code<br />8 Comments and Open Issues<br />There are several open issues, concerned mainly with follow-up once an initial set of code<br />runs has been collected and analyzed.<br />Effective dimensionality<br />The ocean-circulation model (Gough and Welch, 1994) had an initial sample size of<br />n = 36, about half the recommended value of n = 10d. Even so, a good fit was obtained.<br />A closer look at this application shows thatˆθ has elements that are near zero for three<br />of the input variables. Thus, the input space is effectively reduced to d = 4 dimensions,<br />leading to a recommendation of n = 40. If there are good a priori reasons to expect that<br />30</p>  <p>Page 32</p> <p>the number of active dimensions, d0, is less than d then choosing n = 10d0could be a<br />useful complement to the recommended strategy, especially if there are serious budget<br />constraints.<br />The GP model is a poor fit<br />Good general strategies to cope with lack of fit of the GP model are not readily<br />available. There is interesting work by Gramacy and Lee (2007) which could be useful<br />when runs are plentiful. The approach used extensively by Aslett et al. (1998) and by<br />Gramacy and Lee (2007), of narrowing the space of inputs, better enables approximation<br />of code output by a homogeneous GP; the assumption of homogeneity is less sustainable<br />when the input space is too large. But how to do this in a measured way is not clear and<br />needs further research.<br />Canonical configurations of θ<br />For p = 2, we chose a simple two-parameter family in our analyses in Section 5 and 6.<br />Other sets of values for θ can be explored, but we find little incentive to do so for the<br />purpose of settling on initial sample size. We have found that even if θ is not a canonical<br />configuration there is little to no difference in distributions of eavgor emaxrelative to a<br />canonical θ provided τ and ψ are the same.<br />Treating a GP with p ?= 2 (as in the sea-ice example)<br />We have not discussed the relevance, nor the use, of τ and ψ when p ?= 2. The<br />interpretation of τ and ψ values need to be reexamined.<br />In the case of the exponential correlation function (all pj = 1), the implied prior<br />distribution is on a much larger class of functions and achieving good accuracy is more<br />difficult. It is easy to work out the mean and variance of h1<br />jas in Lemma 1, and again<br />we find that τ and ψ should be important. The exact values are given in Lemma 2.<br />Lemma 2: Let hjbe the distance between two randomly chosen points for variable xj<br />in a random LHD. Then<br />E(hj) =1<br />3<br />(n + 1)<br />(n − 1),<br />31</p>  <p>Page 33</p> <p>and<br />Var(hj) =<br />1<br />18<br />(n − 2)(n + 1)<br />(n − 1)2<br />.<br />The proof of Lemma 2 can be found in Appendix A. Note that the two moments converge<br />to 1/3 and 1/18 as n → ∞, i.e., they do not depend on n in the limit. The mean of<br />h1<br />j, is now approximately twice that for the case pj= 2, indicating that larger samples<br />could be needed to achieve desired accuracy. How this all plays out in analogues of the<br />analyses in Section 5 and 6, to enable follow-up recommendations has yet to be explored.<br />When 1 &lt; pj&lt; 2, exact calculations of the mean and variance of hpj<br />j are not avail-<br />able. Approximations are obtainable as follows, however. Assume that xj and x′<br />jare<br />approximately independent and uniform on [0,1], and again let hj= |xj− x′<br />that hpj<br />j|. We find<br />jhas<br />E(hpj<br />j) =<br />2<br />(pj+ 1)(pj+ 2)<br />and<br />E(h2pj<br />j ) =<br />1<br />(pj+ 1)(2pj+ 1).<br />For pj= 2 this produces E(h2<br />j) = 1/6 and Var(h2<br />j) = 7/180, which are the asymptotic<br />values found in Lemma 1.<br />For the general case, with pj varying with xj, assume the design is a completely<br />random LHD. Asymptotically, h(x,x′) =?d<br />on both θ and p:<br />d<br />?<br />Similarly, it has asymptotic variance<br />j=1θj|xj− x′<br />j|pjin (2) has mean depending<br />E(h) =<br />j=1<br />θj<br />2<br />(pj+ 1)(pj+ 2).<br />Var(h) =<br />d<br />?<br />j=1<br />θ2<br />j<br />?<br />1<br />(pj+ 1)(2pj+ 1)−<br />4<br />(pj+ 1)2(pj+ 2)2<br />?<br />.<br />Defining canonical sets of correlation parameters is now much more complicated. Some<br />preliminary calculations for the sea-ice application suggest that the convergence rates for<br />p ?= 2 are different from those obtained when p = 2 and thus one must examine rates<br />for various combinations of both θ and p. This too calls for additional examination.<br />32</p>  <p>Page 34</p> <p>9 Discussion<br />In the introduction we raised a set of issues that should be treated. In the subsequent<br />sections we have provided evidence that:<br />• “n = 10d” is a viable and valuable rule-of-thumb for choosing an initial sample size<br />for a computer experiment.<br />• Criteria can make a difference for post-experimental analysis but have less influence<br />on initial sample size. The sea-ice example shows that the conflict between the eavg<br />and emaxcriteria has implications, as spelled out in Section 7. However, as seen in<br />Section 6 both criteria support the “n = 10d” rule.<br />• When p = 2 there is good information about rates at which error decreases with<br />n and when feasible sample sizes are available. These depend on the parameters τ<br />and ψ, whose values are not known until the post-experimental stage and are then<br />useful for deciding how to follow-up. When p ?= 2 much remains to be done.<br />• In the case that accuracy goals are not met with an initial sample size, a follow-up<br />strategy is needed, but a full analysis is lacking and is a topic for further inquiry.<br />APPENDIX A: Proof of Lemma 1<br />Proof of Lemma 1:<br />Let D be an n × d random LHD, and let xj and x′<br />of the design in dimension j. The construction of the LHD ensures that xj?= x′<br />jbe any two randomly chosen runs<br />j, and<br />hence xj and x′<br />jare dependent random variables. There are a total of<br />?n<br />2<br />?<br />possible<br />pairs of points and each pair is equally likely. Clearly, P(xj = i/(n − 1)) = 1/n and<br />P(x′<br />j= k/(n − 1)|xj = i/(n − 1)) = 1/(n − 1). Consider any two points that are an<br />absolute distance of i/(n − 1) apart. By a simple counting argument, there are n − i<br />33</p>  <p>Page 35</p> <p>pairs giving rise to this distance. This establishes<br />P(hj= i/(n − 1)) =(n − i)<br />?n<br />2<br />?<br />=2(n − i)<br />n(n − 1),i = 1,...,n − 1.<br />The expected value of h2<br />jis<br />E(h2<br />j) = E<br />?<br />i2<br />(n − 1)2<br />2<br />n(n − 1)3<br />?<br />=<br />1<br />(n − 1)2<br />?<br />2<br />n(n − 1)<br />?<br />n−1<br />?<br />i=1<br />i2(n − i)<br />?<br />=<br />?<br />n<br />n−1<br />?<br />i=1<br />i2−<br />n−1<br />?<br />i=1<br />i3<br />=1<br />6<br />n(n + 1)<br />(n − 1)2.<br />Similarly,<br />Var(h2<br />j) = E(h4<br />j) − E(h2<br />j)2= E<br />?<br />n−1<br />?<br />i4<br />(n − 1)4<br />?<br />−<br />?<br />?1<br />?1<br />6<br />n(n + 1)<br />(n − 1)2<br />n(n + 1)<br />(n − 1)2<br />?2<br />=<br />1<br />(n − 1)4<br />1<br />180<br />?<br />2<br />n(n − 1)<br />i=1<br />i4(n − i)−<br />6<br />?2<br />=<br />n(n − 2)(n + 1)(7n + 9)<br />(n − 1)4<br />.<br />Algebra was carried out in Maple. 2<br />Proof of Lemma 2:<br />Following Lemma 1, the expected value is:<br />E(hj) = E<br />?<br />i<br />(n − 1)<br />2<br />?<br />=<br />1<br />(n − 1)<br />n−1<br />?<br />?<br />2<br />n(n − 1)<br />?<br />n−1<br />?<br />i=1<br />i(n − i)<br />?<br />=<br />n(n − 1)2<br />?<br />n<br />i=1<br />i −<br />n−1<br />?<br />i=1<br />i2<br />=1<br />3<br />(n + 1)<br />(n − 1)<br />Similarly, the variance is:<br />Var(h2<br />j) = E(h2<br />j) − E(hj)2=1<br />(n − 2)(n + 1)<br />(n − 1)2<br />6<br />n(n + 1)<br />(n − 1)2−<br />?1<br />3<br />(n + 1)<br />(n − 1)<br />?2<br />=<br />1<br />18<br />.<br />Algebra was carried out in Maple. 2<br />ACKNOWLEDGEMENTS<br />The research of Loeppky and Welch was supported by grants from the Natural Sci-<br />ences and Engineering Research Council of Canada.<br />34</p>  <p>Page 36</p> <p>References<br />Aslett, R., Buck, R. J., Duvall, S. G., Sacks, J., and Welch, W. J. (1998), “Circuit<br />Optimization via Sequential Computer Experiments: Design of an Ouput Buffer,”<br />Applied Statistics, 47, 31–48.<br />Bayarri, M. J., Berger, J. O., Garcia-Donato, G., Sacks, J., Walsh, D., Cafeo, J., and<br />Parthasarathy, R. (2007), “Computer Model Validation with Function Output,” An-<br />nals of Statistics, 35, 1874–1906.<br />Chapman, W. L., Welch, W. J., Bowman, K. P., Sacks, J., and Walsh, J. E. (1994),<br />“Arctic sea ice variability: Model sensitivities and a multidecadal simulation,” Journal<br />of Geophysical Research, 99, 919–936.<br />Chen, X. (1996), “Properties of Models for Computer Experiments,” Ph.D. thesis, Uni-<br />versity of Waterloo.<br />Currin, C., Mitchell, T., Morris, M., and Ylvisaker, D. (1991), “Bayesian Prediction of<br />Deterministic Functions, With Applications to the Design and Analysis of Computer<br />Experiments,” Journal of the American Statistical Association, 86, 953–963.<br />Feeley, R., Frenklach, M., Paulo, R., and Sacks, J. (2007), “A Study of the G-protein<br />Computer Model,” Tech. rep., Unpublished.<br />Gough, W. A. and Welch, W. J. (1994), “Parameter Space Exploration of an Ocean<br />General Circulation Model Using an Isopycnal Mixing Parameterization,” Journal of<br />Marine Research, 52, 773–796.<br />Gramacy, R. B. and Lee, H. K. H. (2007), “Bayesian Treed Gaussian Process Mod-<br />els with an Application to Computer Modeling,” Journal of the American Statistical<br />Association, to appear.<br />Higdon, D., Kennedy, M., Cavendish, J. C., Cafeo, J. A., and Ryne, R. D. (2004),<br />“Combining Field Data and Computer Simulation for Calibration and Prediction,”<br />SIAM Journal on Scientific Computing, 26, 448–466.<br />35</p>  <p>Page 37</p> <p>Higdon, D., Williams, R., Moore, L., McKay, M., and Keller-McNulty, S. (2005), “Uncer-<br />tainty Quantification for Combining Experimental Data and Computer Simulations,”<br />in Society for Modeling and Simulation International, eds. Pace, D. and Stevenson, S.<br />Jones, D. R., Schonlau, M., and Welch, W. J. (1998), “Efficient global optimization of<br />expensive black-box functions,” Journal of Global Optimization, 13, 455–492.<br />Linkletter, C., Bingham, D., Hengartner, N., Higdon, D., and Ye, K. Q. (2006), “Variable<br />Selection for Gaussian Process Models in Computer Experiments,” Technometrics, 48,<br />478–490.<br />McKay, M. D., Beckman, R. J., and Conover, W. J. (1979), “A Comparison of Three<br />Methods for Selecting Values of Input Variables in the Analysis of Ouput from a<br />Computer Code,” Technometrics, 21, 239–245.<br />Morris, M. D. and Mitchell, T. J. (1995), “Exploratory Designs for Computational Ex-<br />periments,” Journal of Statistical Planning and Inference, 43, 381–402.<br />O’Hagan, A. (1992), “Some Bayesian Numerical Analysis,” in Bayesian Statistics 4, eds.<br />Bernardo, J. M., Berger, J. O., Dawid, A. P., and Smith, A. F. M., Oxford University<br />Press, pp. 345–363.<br />Owen, A. B. (1994), “Controlling Correlations in Latin Hypercube Samples,” Journal of<br />the American Statistical Association, 89, 1517–1522.<br />Sacks, J., Schiller, S. B., and Welch, W. J. (1989a), “Designs for Computer Experiments,”<br />Technometrics, 31, 41–47.<br />Sacks, J., Welch, W. J., Mitchell, T. J., and Wynn, H. P. (1989b), “Designs and Analysis<br />of Computer Experiments (with Discussion),” Statistical Science, 4, 409–435.<br />Welch, W. J., Buck, R. J., Sacks, J., Wynn, H. P., Mitchell, T. J., and Morris, M. D.<br />(1992), “Screening, Predicting, and Computer Experiments,” Technometrics, 34, 15–<br />25.<br />36</p>  <p>Page 38</p> <p>Yi, T.-M., Fazel, M., Liu, X., Otitoju, T., Papachristodoulou, A., Prajna, S., and Doyle.,<br />J. (2005), “Application of Robust Model Validation Using SOSTOOLS to the Study of<br />G-Protein Signaling in Yeast,” in Proceedings of Foundations of Systems Biology and<br />Engineering.<br />37</p>  <a href="https://www.researchgate.net/profile/William_Welch/publication/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide/links/0046352e6a4733d593000000.pdf">Download full-text</a> </div> <div id="rgw21_56aba2472d207" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw22_56aba2472d207">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw23_56aba2472d207"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/William_Welch/publication/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide/links/0046352e6a4733d593000000.pdf" class="publication-viewer" title="0046352e6a4733d593000000.pdf">0046352e6a4733d593000000.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/William_Welch">William J. Welch</a> &middot; May 27, 2014 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw24_56aba2472d207"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://www.stat.ubc.ca/Research/TechReports/techreports/238.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Choosing the Sample Size of a Computer Experiment: A Practical Guide">Choosing the Sample Size of a Computer Experiment:...</a> </div>  <div class="details">   Available from <a href="http://www.stat.ubc.ca/Research/TechReports/techreports/238.pdf" target="_blank" rel="nofollow">stat.ubc.ca</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw26_56aba2472d207" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw27_56aba2472d207">  </ul> </div> </div>   <div id="rgw17_56aba2472d207" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw18_56aba2472d207"> <div> <h5> <a href="publication/291545776_Comments_on_Adaptive_increase_in_sample_size_when_interim_results_are_promising_A_practical_guide_with_examples%27_by_CR_Mehta_and_SJ_Pocock_reply" class="color-inherit ga-similar-publication-title"><span class="publication-title">Comments on Adaptive increase in sample size when interim results are promising: A practical guide with examples&#39; by C.R. Mehta and S.J. Pocock reply</span></a>  </h5>  <div class="authors"> <a href="researcher/2095328695_CR_Mehta" class="authors ga-similar-publication-author">CR Mehta</a>, <a href="researcher/39091317_SJ_Pocock" class="authors ga-similar-publication-author">SJ Pocock</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56aba2472d207"> <div> <h5> <a href="publication/227861250_Exact_conditional_and_unconditional_sample_size_for_pair-matched_studies_with_binary_outcome_A_practical_guide" class="color-inherit ga-similar-publication-title"><span class="publication-title">Exact conditional and unconditional sample size for pair‐matched studies with binary outcome: A practical guide</span></a>  </h5>  <div class="authors"> <a href="researcher/39465419_Patrick_Royston" class="authors ga-similar-publication-author">Patrick Royston</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw20_56aba2472d207"> <div> <h5> <a href="publication/14688765_Exact_conditional_and_unconditional_sample_size_for_pair-matched_studies_with_binary_outcome_A_practical_guide" class="color-inherit ga-similar-publication-title"><span class="publication-title">Exact conditional and unconditional sample size for pair-matched studies with binary outcome: A practical guide</span></a>  </h5>  <div class="authors"> <a href="researcher/39465419_P_Royston" class="authors ga-similar-publication-author">P Royston</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw40_56aba2472d207" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw41_56aba2472d207">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw42_56aba2472d207" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=3X8kdAldWFIdN6BRPdbJ1urbFsb568jXBDnKrAddEIDX8KYx5nWPEtaFXtRHi58d" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="4yFYe47FiRXmn1fiWCTWaHXFHLZAIbHL+MiGZfKnfWe/AOTOAda8FC1FqaPzGd9HDoEUKgHrMDV3VSEuh8q1WDjSLV4YUmXS3Aq4gzj2DYtToURP6wbtomyN5n+bJDJ2gVMz/C1qRS5aV9A4pWAElRzmcTXgYYwlQfrWxEhBVxAqR42O60l39jfRKbuAEnOOQqfAkV8LYvCWEPxIxq9D/ZaEradbFPZkEid9G+emEVbYid836Vb/oqcGR5mcJ29YHhtymqfSSxSirmf8z7Pe9P4vbBAz9a7x8OMpD83D65k="/> <input type="hidden" name="urlAfterLogin" value="publication/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjM3NTQ4ODEzX0Nob29zaW5nX3RoZV9TYW1wbGVfU2l6ZV9vZl9hX0NvbXB1dGVyX0V4cGVyaW1lbnRfQV9QcmFjdGljYWxfR3VpZGU%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjM3NTQ4ODEzX0Nob29zaW5nX3RoZV9TYW1wbGVfU2l6ZV9vZl9hX0NvbXB1dGVyX0V4cGVyaW1lbnRfQV9QcmFjdGljYWxfR3VpZGU%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjM3NTQ4ODEzX0Nob29zaW5nX3RoZV9TYW1wbGVfU2l6ZV9vZl9hX0NvbXB1dGVyX0V4cGVyaW1lbnRfQV9QcmFjdGljYWxfR3VpZGU%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw43_56aba2472d207"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 712;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2198378204065/javascript/min/lib/error_logging.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Jason L. Loeppky","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Jason_Loeppky","institution":"University of British Columbia - Okanagan","institutionUrl":false,"widgetId":"rgw4_56aba2472d207"},"id":"rgw4_56aba2472d207","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=3673360","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56aba2472d207"},"id":"rgw3_56aba2472d207","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=237548813","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":237548813,"title":"Choosing the Sample Size of a Computer Experiment: A Practical Guide","journalTitle":"Technometrics","journalDetailsTooltip":{"data":{"journalTitle":"Technometrics","journalAbbrev":"Technometrics","publisher":"American Society for Quality Control; American Statistical Association, Taylor & Francis","issn":"1537-2723","impactFactor":"1.81","fiveYearImpactFactor":"2.47","citedHalfLife":">10.0","immediacyIndex":"0.20","eigenFactor":"0.00","articleInfluence":"1.70","widgetId":"rgw6_56aba2472d207"},"id":"rgw6_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=1537-2723","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Article","details":{"doi":"10.1198\/TECH.2009.08040","journalInfos":{"journal":"","publicationDate":"11\/2009;","publicationDateRobot":"2009-11","article":"51(4):366-376.","journalTitle":"Technometrics","journalUrl":"journal\/1537-2723_Technometrics","impactFactor":1.81}},"source":false,"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1198\/TECH.2009.08040"},{"key":"rft.atitle","value":"Choosing the Sample Size of a Computer Experiment: A Practical Guide"},{"key":"rft.title","value":"Technometrics"},{"key":"rft.jtitle","value":"Technometrics"},{"key":"rft.volume","value":"51"},{"key":"rft.issue","value":"4"},{"key":"rft.date","value":"2009"},{"key":"rft.pages","value":"366-376"},{"key":"rft.issn","value":"1537-2723"},{"key":"rft.au","value":"Jason L. Loeppky,Jerome Sacks,William J. Welch"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56aba2472d207"},"id":"rgw7_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=237548813","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":237548813,"peopleItems":[{"data":{"authorNameOnPublication":"Jason L. Loeppky","accountUrl":"profile\/Jason_Loeppky","accountKey":"Jason_Loeppky","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Jason L. Loeppky","profile":{"professionalInstitution":{"professionalInstitutionName":"University of British Columbia - Okanagan","professionalInstitutionUrl":"institution\/University_of_British_Columbia-Okanagan"}},"professionalInstitutionName":"University of British Columbia - Okanagan","professionalInstitutionUrl":"institution\/University_of_British_Columbia-Okanagan","url":"profile\/Jason_Loeppky","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Jason_Loeppky","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw10_56aba2472d207"},"id":"rgw10_56aba2472d207","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=3673360&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University of British Columbia - Okanagan","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":2,"publicationUid":237548813,"widgetId":"rgw9_56aba2472d207"},"id":"rgw9_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=3673360&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=2&publicationUid=237548813","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/8998449_Jerome_Sacks","authorNameOnPublication":"Jerome Sacks","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Jerome Sacks","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/8998449_Jerome_Sacks","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56aba2472d207"},"id":"rgw12_56aba2472d207","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=8998449&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56aba2472d207"},"id":"rgw11_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=8998449&authorNameOnPublication=Jerome%20Sacks","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"William J. Welch","accountUrl":"profile\/William_Welch","accountKey":"William_Welch","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277690474090520%401443217976996_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"William J. Welch","profile":{"professionalInstitution":{"professionalInstitutionName":"University of British Columbia - Vancouver","professionalInstitutionUrl":"institution\/University_of_British_Columbia-Vancouver"}},"professionalInstitutionName":"University of British Columbia - Vancouver","professionalInstitutionUrl":"institution\/University_of_British_Columbia-Vancouver","url":"profile\/William_Welch","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277690474090520%401443217976996_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"William_Welch","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw14_56aba2472d207"},"id":"rgw14_56aba2472d207","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=4311938&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University of British Columbia - Vancouver","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":2,"publicationUid":237548813,"widgetId":"rgw13_56aba2472d207"},"id":"rgw13_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=4311938&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=2&publicationUid=237548813","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56aba2472d207"},"id":"rgw8_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=237548813&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":237548813,"abstract":"<noscript><\/noscript><div>We produce reasons and evidence supporting the informal rule that the number of runs for an effective initial computer experiment should be about 10 times the input dimension. Our arguments quantify two key characteristics of computer codes that affect the sample size required for a desired level of accuracy when approximating the code via a Gaussian process (GP). The first characteristic is the total sensitivity of a code output variable to all input variables. The second corresponds to the way this total sensitivity is distributed across the input variables, specifically the possible presence of a few prominent input factors and many impotent ones (effect sparsity). Both measures relate directly to the correlation structure in the GP approximation of the code. In this way, the article moves towards a more formal treatment of sample size for a computer experiment. The evidence supporting these arguments stems primarily from a simulation study and via specific codes modeling climate and ligand activation of G-protein.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw15_56aba2472d207"},"id":"rgw15_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=237548813","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide\/links\/0046352e6a4733d593000000\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw16_56aba2472d207"},"id":"rgw16_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56aba2472d207"},"id":"rgw5_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=237548813&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2095328695,"url":"researcher\/2095328695_CR_Mehta","fullname":"CR Mehta","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39091317,"url":"researcher\/39091317_SJ_Pocock","fullname":"SJ Pocock","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2012","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291545776_Comments_on_Adaptive_increase_in_sample_size_when_interim_results_are_promising_A_practical_guide_with_examples'_by_CR_Mehta_and_SJ_Pocock_reply","usePlainButton":true,"publicationUid":291545776,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/291545776_Comments_on_Adaptive_increase_in_sample_size_when_interim_results_are_promising_A_practical_guide_with_examples%27_by_CR_Mehta_and_SJ_Pocock_reply","title":"Comments on Adaptive increase in sample size when interim results are promising: A practical guide with examples' by C.R. Mehta and S.J. Pocock reply","displayTitleAsLink":true,"authors":[{"id":2095328695,"url":"researcher\/2095328695_CR_Mehta","fullname":"CR Mehta","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39091317,"url":"researcher\/39091317_SJ_Pocock","fullname":"SJ Pocock","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291545776_Comments_on_Adaptive_increase_in_sample_size_when_interim_results_are_promising_A_practical_guide_with_examples'_by_CR_Mehta_and_SJ_Pocock_reply","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291545776_Comments_on_Adaptive_increase_in_sample_size_when_interim_results_are_promising_A_practical_guide_with_examples'_by_CR_Mehta_and_SJ_Pocock_reply\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56aba2472d207"},"id":"rgw18_56aba2472d207","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291545776","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":39465419,"url":"researcher\/39465419_Patrick_Royston","fullname":"Patrick Royston","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Apr 1993","journal":"Statistics in Medicine","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/227861250_Exact_conditional_and_unconditional_sample_size_for_pair-matched_studies_with_binary_outcome_A_practical_guide","usePlainButton":true,"publicationUid":227861250,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.83","url":"publication\/227861250_Exact_conditional_and_unconditional_sample_size_for_pair-matched_studies_with_binary_outcome_A_practical_guide","title":"Exact conditional and unconditional sample size for pair\u2010matched studies with binary outcome: A practical guide","displayTitleAsLink":true,"authors":[{"id":39465419,"url":"researcher\/39465419_Patrick_Royston","fullname":"Patrick Royston","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Statistics in Medicine 04\/1993; 12(7):699 - 712. DOI:10.1002\/sim.4780120709"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/227861250_Exact_conditional_and_unconditional_sample_size_for_pair-matched_studies_with_binary_outcome_A_practical_guide","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/227861250_Exact_conditional_and_unconditional_sample_size_for_pair-matched_studies_with_binary_outcome_A_practical_guide\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56aba2472d207"},"id":"rgw19_56aba2472d207","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=227861250","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":39465419,"url":"researcher\/39465419_P_Royston","fullname":"P Royston","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"May 1993","journal":"Statistics in Medicine","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/14688765_Exact_conditional_and_unconditional_sample_size_for_pair-matched_studies_with_binary_outcome_A_practical_guide","usePlainButton":true,"publicationUid":14688765,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.83","url":"publication\/14688765_Exact_conditional_and_unconditional_sample_size_for_pair-matched_studies_with_binary_outcome_A_practical_guide","title":"Exact conditional and unconditional sample size for pair-matched studies with binary outcome: A practical guide","displayTitleAsLink":true,"authors":[{"id":39465419,"url":"researcher\/39465419_P_Royston","fullname":"P Royston","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Statistics in Medicine 05\/1993; 12(7):699-712."],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/14688765_Exact_conditional_and_unconditional_sample_size_for_pair-matched_studies_with_binary_outcome_A_practical_guide","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/14688765_Exact_conditional_and_unconditional_sample_size_for_pair-matched_studies_with_binary_outcome_A_practical_guide\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56aba2472d207"},"id":"rgw20_56aba2472d207","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=14688765","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw17_56aba2472d207"},"id":"rgw17_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=237548813&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":237548813,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":237548813,"publicationType":"article","linkId":"0046352e6a4733d593000000","fileName":"0046352e6a4733d593000000.pdf","fileUrl":"profile\/William_Welch\/publication\/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide\/links\/0046352e6a4733d593000000.pdf","name":"William J. Welch","nameUrl":"profile\/William_Welch","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"May 27, 2014","fileSize":"383.51 KB","widgetId":"rgw23_56aba2472d207"},"id":"rgw23_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=237548813&linkId=0046352e6a4733d593000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":237548813,"publicationType":"article","linkId":"025d576c0cf2c9435482658f","fileName":"Choosing the Sample Size of a Computer Experiment: A Practical Guide","fileUrl":"http:\/\/www.stat.ubc.ca\/Research\/TechReports\/techreports\/238.pdf","name":"stat.ubc.ca","nameUrl":"http:\/\/www.stat.ubc.ca\/Research\/TechReports\/techreports\/238.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw24_56aba2472d207"},"id":"rgw24_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=237548813&linkId=025d576c0cf2c9435482658f&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw22_56aba2472d207"},"id":"rgw22_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=237548813&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":41,"valueFormatted":"41","widgetId":"rgw25_56aba2472d207"},"id":"rgw25_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=237548813","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw21_56aba2472d207"},"id":"rgw21_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=237548813&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":237548813,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw27_56aba2472d207"},"id":"rgw27_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=237548813&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":41,"valueFormatted":"41","widgetId":"rgw28_56aba2472d207"},"id":"rgw28_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=237548813","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw26_56aba2472d207"},"id":"rgw26_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=237548813&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"THE UNIVERSITY OF BRITISH \nCOLUMBIA \n \nDEPARTMENT OF STATISTICS \n \n \nTECHNICAL REPORT #238 \n \n \n \n \nChoosing the Sample Size of a Computer \nExperiment: A Practical Guide \n \n \n \n \nJason Leoppky, Jerome Sacks, William J. Welch \n \n \n \n \n \n \nFebruary 2008"},{"page":2,"text":"Choosing the Sample Size of a Computer Experiment: A\nPractical Guide\nJason L. LoeppkyJerome Sacks\nMathematics, Statistics, and Physics\nUniversity of British Columbia, Okanagan\nKelowna, BC V1V 1V7, CANADA\n(jason@stat.ubc.ca)\nNational Institute of Statistical Sciences\nResearch Triangle Park, NC, 27709\n(sacks@niss.org)\nWilliam J. Welch\nDepartment of Statistics\nUniversity of British Columbia\nVancouver, BC V6T 1Z2, CANADA\n(will@stat.ubc.ca)\nFebruary 19, 2008\nAbstract\nWe produce reasons and evidence supporting the informal rule that the number of\nruns for an effective initial computer experiment should be about 10 times the input\ndimension. Our arguments quantify two key characteristics of computer codes that\naffect the sample size required for a desired level of accuracy when approximating\nthe code via a Gaussian process (GP). The first characteristic is the total sensitivity\nof a code output variable to all input variables. The second corresponds to the\nway this total sensitivity is distributed across the input variables, specifically the\npossible presence of a few prominent input factors and many impotent ones (effect\nsparsity). Both measures relate directly to the correlation structure in the GP\napproximation of the code. In this way, the article moves towards a more formal\ntreatment of sample size for a computer experiment. The evidence supporting these\narguments stems primarily from a simulation study and via specific codes modeling\nclimate and ligand activation of G-protein.\nKEYWORDS: Computer experiment, Gaussian process, Random function, Latin\nhypercube design, Sample size.\n1 Introduction\nChoosing the sample size of any experiment is an important issue in the design of ex-\nperiments, yet there is a lack of formal guidance. The reasons range from inadequate\nprior information about the process under study to inadequate results (and inability) for\nmaking necessary calculations. In standard regression settings, finding a sample size to\n1"},{"page":3,"text":"produce satisfactory predictive accuracy depends on the design points of the data col-\nlection and the error variance, but both the form of the regression model and the error\nvariance are typically unknown a priori. Bayesian strategies can be deployed with some\ndifficulty.\nDeterministic computer experiments present a wholly different set of challenges, pri-\nmarily because concepts such as randomization and replication play no role and predictive\naccuracy of the model is affected solely by bias. Because physical experimentation is ab-\nsent, the constraints on experimental size are typically caused by the time it takes to\nmake runs of the code. Such constraints are often vague and flexible. Where budget\nissues prevail (\u201cyou get this much computer time to make your runs\u201d) the choice of sam-\nple size, n, is taken out of our hands. Nevertheless, it is useful to have some practical\nguidance in choosing n and to know if the selected n is adequate to achieve stated goals.\nIn addition to guiding an experimenter in the choice of n for a specific experiment, we\nwill consider more general questions. In particular, what is the role of dimensionality of\nthe input space? If the curse of dimensionality applies, high-dimensional problems might\nrequire huge, even intractable, sample sizes for good prediction accuracy. On the other\nhand, if the total sensitivity of the function to all input variables is kept fixed, with this\nsensitivity just spread over more input variables, dimensionality might conceivably have\na limited effect on accuracy, as in Monte Carlo integration. In this article, how total\nsensitivity grows with dimension and how this sensitivity is spread across the dimensions\nare key to understanding prediction accuracy, and hence sample size. Indeed, the article\nis really about defining the properties of functions that arise in practice, from which\nsimple rules about sample size follow for that class of problems.\nLittle has been written on this topic. Among the few exceptions, Chapman et al.\n(1994) and Jones et al. (1998) used the often quoted rule of selecting a sample size\nthat is 10 times the number of inputs. Although this rule has proved useful in practice\nit lacks theoretical underpinning. One theoretical exploration by Chen (1996) showed\nthat, for a single varying input to the computer code whose output is under study, the\norder of the prediction error is n\u2212nfor very smooth output functions and for an equally\n2"},{"page":4,"text":"spaced design. In higher dimensions, Chen (1996) produced results on rates for product\ndesigns. Though these rates are instructive, product designs are impractical and more\nprecise understanding of prediction error is needed for choosing a sample size in practical\nsettings. The key conclusion we arrive at is that the empirically based recommendation\nof n = 10d is a good path to follow for a large class of problems.\nAn Example\nYi et al. (2005) studied a computer model of ligand activation of G-protein in yeast\nwhere the computer code takes four inputs and solves a system of ordinary differential\nequations (details are in Section 3). Following the path taken in the literature since\n1989 (Sacks et al., 1989b; Currin et al., 1991), approximate the computer output using a\nGaussian Process (GP) constructed from a set of code runs. The question that concerns\nus here is: How many runs are needed to obtain adequate prediction accuracy at untried\ntest points? In the G-protein example, the code is relatively quick to run and we are able\nto investigate the effect of n on the prediction error by making runs for various values of\nn. For each value of n the code was evaluated at inputs from an n-point maximin Latin\nhypercube design (LHD) in 4 dimensions (McKay et al., 1979; Morris and Mitchell, 1995).\nThe plot in Figure 1 shows the square root of the integrated mean squared error (RIMSE)\nfor predictions using the GP model, for various choice of n. (RIMSE is computed for\nboth a set of 120 hold-out points and by leave-one-out cross-validation.) The minor\nimprovement in RIMSE for sample sizes greater than 40 = 10d is a feature of many\nproblems.\nIn general, characterization of the factors affecting approximation accuracy, and hence\nsample size, requires precise formulation of the goals of the experiment. Such a formu-\nlation is often elusive, however. We restrict attention to the experimental objective of\napproximating the code on the basis of sample runs. Even here, the choice of measures\nof accuracy is open to subjective judgment. Those we use are given in (5) and (6) be-\nlow. Issues such as optimization of a target criterion could bring other considerations,\nespecially that of fully sequential experimentation.\nWe have obscured the role of the design of the location of inputs in this process. Con-\n3"},{"page":5,"text":"3035 40 4550 55 606570 7580\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.1\nsample size\nRIMSE\nFigure 1: Root integrated mean squared error (RIMSE) of prediction against n for the\nG-protein example. The solid line shows RIMSE computed for a hold-out sample; the\ndashed line shows RIMSE from leave one out cross-validation.\nsiderable experience built up over a number of applications leads us to restrict attention\nto designs that are space-filling and, for the problems we address this is well managed\nby maximin LHDs (which we used in the G-protein example), but simpler to construct\nzero-correlation LHDs (Gough and Welch, 1994; Owen, 1994) could also be deployed.\nWhile there are many issues that can be addressed in determining sample size we\nfocus on these:\n\u2022 Is n = 10d a good rule? What are the limitations of such a rule?\n\u2022 How does accuracy increase with n? When are feasible sample sizes available?\n\u2022 What impact do criteria have on assessing accuracy?\n\u2022 What should be done when a criterion for accuracy is not met?\nWe will partially answer these questions, enough to provide useful practical advice for the\nchoice of n. Our approach to this problem investigates properties of the GP and the effect\nof n on prediction by first finding connections between the design and the complexity of\n4"},{"page":6,"text":"the problem and then conducting a simulation study. The simulations focus on deciding\nif n = 10d is a reasonable rule and characterizing the complexity of problems that can\nbe dealt with using n = 10d.\nThe paper is organized as follows. Section 2 reviews the GP model and gives specific\nformulations of the measures of accuracy we use. Section 3 explores the G-protein ex-\nample in more detail. Section 4 investigates the relationships among dimension, sample\nsize and complexity of the problem that guide the simulation study in Section 5 and 6.\nSection 7 discusses strategies for a follow-up experiment to augment an initial design and\nthe implications for several examples. Finally, in Sections 8 and 9 we comment on open\nand future issues and summarize our conclusions.\n2 The Gaussian Process Model\nA complex computer code mathematically describes the relationship between several\ninput variables and one or more (possibly functional) output variables. Usually, the\ncomputer model of interest is computationally demanding, and scientific objectives like\noptimization would require too many evaluations if the code is used directly. As a con-\nsequence, strategies relying on computationally efficient statistical approximation (emu-\nlation) of the code have been developed and have proved effective. Following the path\ntaken in the literature since 1989 (Sacks et al., 1989a,b; Currin et al., 1991; O\u2019Hagan,\n1992), we place a homogeneous Gaussian process prior on the possible output functions,\nwhich leads to an approximator given by the posterior mean conditional on the data\nfrom the computer experiment. Although the output from the computer model is often\nmultivariate, we will restrict our attention to scalar output. The results for scalar output\ncan be carried over by using principal component analysis or wavelet decompositions of\nfunctional output as in Higdon et al. (2005) and Bayarri et al. (2007).\nThe computer code output is denoted by y(x), where the code\u2019s vector-valued input,\nx = (x1,...,xd), is assumed to be a point in a d-dimensional unit cube. As long as the\ninput space is rectangular, there is no loss of generality here because any rectangle can\n5"},{"page":7,"text":"be transformed simply to the unit cube with only trivial implications for the analysis\nmethod to be described.\nThe GP model places a prior on the class of possible y(x) functions. Let Y (x) denote\nthe random function whose distribution is determined by the prior. Specifically, we take\nY (x) = \u00b5 + Z(x),\nwhere \u00b5 is a mean parameter and Z(x) is a Gaussian stochastic process with mean zero\nand constant variance \u03c32. In this model, the correlation structure is crucial to prediction.\nAt two input vectors, x and x\u2032, we take the correlation between Y (x) and Y (x\u2032) as\nR(x,x\u2032) = exp(\u2212h(x,x\u2032)), (1)\nwhere\nh(x,x\u2032) =\nd\n?\nj=1\n\u03b8j|xj\u2212 x\u2032\nj|pj, (2)\nis a measure of distance between x and x\u2032with weights \u03b8j \u2265 0 and distance-metric\nparameters 1 \u2264 pj\u2264 2.\nExperience in a variety of circumstances (Higdon et al., 2004; Linkletter et al., 2006)\nsuggests that very smooth, even analytic, output is typical, especially in engineering\ncontexts. As such it is often the case that pj is fixed at 2 for all j, leading to the\nGaussian correlation function. We adopt this special case for most of the article, but\nreturn to the issue of pj< 2 in Sections 7 and 8. With pj= 2, it is easily shown that\nE\n????\n\u2202Y (x)\n\u2202xj\n????\n2\n= 2\u03c32\u03b8j.\nHence, the weight \u03b8jmay be interpreted as a measure of the \u201csensitivity\u201d of Y (x) to xj.\nCharacterizing the distribution of the distances in (2) across design points as a function\nof the values of the sensitivity measures, \u03b81,...,\u03b8d, (Section 4) leads to an understanding\nof the the factors affecting prediction accuracy and hence sample size.\nSuppose we make n runs of the code at a design D of input vectors x(1),...,x(n)in\n[0,1]d, leading to the data y = (y(x(1)),...,y(x(n)))T. The predictor\u02c6Y (x) of Y (x) is the\n6"},{"page":8,"text":"posterior mean of Y (x) given the data and \u03b8 = (\u03b81,...,\u03b8d):\n\u02c6Y (x) = E (Y (x)|y,\u03b8) = \u02c6 \u00b5 + rT(x)R\u22121(y \u2212\u02c6 \u00b51), (3)\nwhere r(x) = (R(x,x(1)),...,R(x,x(n)))Tis an n \u00d7 1 vector, R is an n \u00d7 n matrix with\nelement i,j given by R(x(i),x(j)), and \u02c6 \u00b5 is an estimate of \u00b5, often from the method of\nmaximum likelihood. The mean squared error (MSE) of\u02c6Y (x), taking account of the\nuncertainty from estimating \u00b5 by maximum likelihood, is given by\nMSE(\u02c6Y (x)) = E\n?\u02c6Y (x) \u2212 Y (x)\n?2\n= \u03c32\n?\n1 \u2212 rT(x)R\u22121r(x) +(1 \u2212 1TR\u22121r(x))2\n1TR\u221211\n?\n,\n(4)\nwhere 1 is an n \u00d7 1 vector with all elements equal to 1. In practice, \u03c32and \u03b8 also have\nto be estimated, again often by maximum likelihood (Welch et al., 1992).\nMSE in (4) can be directly computed given an experimental design and \u03b8, and is\nused in Section 4 for theoretical arguments. However, for our empirical studies we take a\ndifferent path to define prediction accuracy by using leave-one-out cross-validation(CV)\n(Currin et al., 1991; Chapman et al., 1994; Gough and Welch, 1994) as follows.\nGiven a design D with sample size n and code runs y, denote the cross-validated\nprediction of y(x(i)) by\u02c6Y\u2212i(x(i)), which is the predictor (3) from the n\u22121 runs excluding\nrun i. Then the cross-validated error of prediction is\u02c6Y\u2212i(x(i)) \u2212 y(x(i)) for i = 1,...,n.\nAverage and maximum measures of error based on cross-validation are given by\n?\nn\ni=1\n?\n?\n?1\nn\n?\n?\u02c6Y\u2212i(x(i)) \u2212 y(x(i))\n?2\nand\nmax\nx(1),...,x(n)\n???\u02c6Y\u2212i(x(i)) \u2212 y(x(i))\n???.\nWe also normalize for the scale of the function by dividing by the range of the values of\ny in the data, leading to the following inaccuracy summaries:\neavg=\n?\n1\nn\n?n\ni=1\n?\u02c6Y\u2212i(x(i)) \u2212 y(x(i))\nrange of y(x(1)),...,y(x(n))\n?2\n(5)\n7"},{"page":9,"text":"and\nemax=\nmax\nx(1),...,x(n)\nrange of y(x(1)),...,y(x(n)).\n???\u02c6Y\u2212i(x(i)) \u2212 y(x(i))\n???\n(6)\nThe tolerable level of inaccuracy will be application-specific, but we will typically take\neavg< 0.1 as the target for a \u201cuseful\u201d approximation of the code.\nFor questions relating to the sample size of an initial design, we do not have data\navailable, at least not code data. But we can simulate data using the GP model with\ngiven \u03b8. We will distinguish e depending on whether the data are from code runs or from\nsimulations by eavg|codeand eavgrespectively.\nBefore code runs are made, we can perform replicate simulations of the random func-\ntion and obtain a collection of eavg values, which will then provide an empirical dis-\ntribution of (5). The average of the simulations will then be an estimate of expected\ninaccuracy as formulated in (5). Similarly we can get an estimate of expected accuracy\nas formulated in (6).\nWhy proceed with simulations rather than attempt direct computation of expected\nvalues, for example? There are four reasons:\n1. The ratios in (5) and (6) are appealing measures of accuracy. Producing expected\nvalues or other quantities of these measures are hopeless without simulation; they\nare readily estimated via simulation.\n2. As described in Section 7, after the sample size is selected and the computer exper-\niment is run we can evaluate e\u00b7|codeand, with the information from the simulations,\nespecially their empirical distribution, we can gauge whether the GP model and\nsample size are well matched to the actual code.\n3. Even if we take expectation and remove all randomness in the data, there are\nother sources of randomness in practice. Most experimental designs are isomorphic\nwith respect to various symmetries such as interchanging the columns. Different\nversions of the design within the equivalence class would lead to different measures\nof prediction error, even after taking expectation with respect to the data.\n8"},{"page":10,"text":"3G-protein Computer Code\nThe ligand activation of G-protein in yeast is described by Yi et al. (2005). The computer\ncode solves a system of ordinary differential equations (ODEs) with nine parameters that\ncan vary. The system dynamics, the differential equations, are given by:\n\u02d9 \u03b71 = \u2212u1\u03b71x + u2\u03b72\u2212 u3\u03b71+ u5\n\u02d9 \u03b72 = u1\u03b71x \u2212 u2\u03b72\u2212 u4\u03b72\n\u02d9 \u03b73 = \u2212u6\u03b72\u03b73+ u8(Gtot\u2212 \u03b73\u2212 \u03b74)(Gtot\u2212 \u03b73)\n\u02d9 \u03b74 = u6\u03b72\u03b73\u2212 u7\u03b74\ny = (Gtot\u2212 \u03b73)\/Gtot,\nwhere \u03b71,...,\u03b74are concentrations of four chemical species, \u02d9 \u03b7i\u2261\ntration of the ligand; u1,...,u8is a vector of 8 kinetic parameters; Gtotis the (fixed)\n\u2202\u03b7i\n\u2202t; x is the concen-\ntotal concentration of G-protein complex after 30 seconds; and y is the normalized con-\ncentration of a relevant part of the complex. In one study (Feeley et al., 2007), five of\nthese kinetic parameters are fixed (only allowing x, u1, u6, and u7to vary). The GP\nmodel is used to construct an approximation as a function of the transformed variables\nlog(x),log(u1),log(u6),log(u7) each of which is further transformed to [0,1].\nThe ODE solver is quick to run and enables us to evaluate the affect of n on the\ncriterion in (5) using a real model. The design points at which the code is run are\nselected by using maximin LHDs. These space-filling designs have proved to be highly\neffective in the study and application of computer experiments.\nThe values of n we use are multiples (7,10,15,20) of the dimension, 4, and also\ninclude 33, the number of runs made in the Feeley et al. (2007) study. For each choice of\nn, we run the ODE solver to obtain data {y(x(i));x(i)\u2208 D}. The data are modeled as if\nthey were the realizations of a GP, and maximum likelihood estimates of \u02c6 \u00b5, \u02c6 \u03c3 and\u02c6\u03b8 are\nobtained for the parameters of the GP (see Section 2) for each choice of n. For each value\nof n, we use the code runs to calculate eavg|codefrom (5), except no normalization for the\nrange is made here. (In any case, the normalization factor is close to 1 at about 0.8 and\n9"},{"page":11,"text":"makes little difference.) We also compare this measure with the analog from a set of new\ntest points. We generate an additional independent 120-point maximin LHD, D0, and\nevaluate the ODE solver to obtain data for the out-of-sample test points. The same 120\ntest runs are used for all evaluations. Using the test sample, the analogous version of (5)\nis computed by replacing the average in the numerator by\n1\n120\n?120\ni=1\n?\u02c6Y (x) \u2212 y(x)\n?2.\nThe plot in Figure 1 shows how the two unnormalized eavg|codemeasures behave as n\nchanges. A major point is that eavg|codechanges little as n increases past 40, nor is there\nany substantial difference between using cross validation instead of a new test sample.\nThat cross validation leads to larger errors is not surprising, since leaving out one point\ncan produce a big gap making it hard to predict the omitted point. This is relevant\nbecause the use of new test data is a luxury, only enjoyed if the code can be run quickly,\nand so we rely on cross validation for measuring accuracy.\nJudging the quality of prediction over a wide range of scenarios is simply not possible\nthrough runs of the computer code unless the code is very quick to run. Therefore, we\nwill rely on simulated data generated by using a GP. Because a GP model often has\nsimilar properties to those of the computer codes we expect to encounter in practice, we\nare at least close to mimicking reality. Before simulating, however, we need to know the\nimportant factors in function complexity, so that an efficient and insightful simulation\nstudy may be conducted.\n4 Effect of d, \u03b8, and n on prediction accuracy\nIntuitively, we know that when we predict Y (x) at some x, the design-point neighbors\nof x will tend to be closer as n becomes larger, improving accuracy. If \u03b8 has many\nlarge values, however, the correlation between Y (x) and Y for the neighbors will be\nlow, even for nearby points, leading to poorer prediction accuracy. Here, we develop this\nintuition into some quantitative rules relating d, \u03b8, and n to distances and the correlation\nstructure, shedding some light on how prediction accuracy depends on these quantities.\nFirst, we consider how the theoretical mean squared error, MSE(\u02c6Y (x)) in (4), depends\n10"},{"page":12,"text":"on d, \u03b8, and n. Recall that the empirical definitions of prediction accuracy in (5) and (6)\nare normalized for scale. Similarly, without loss of generality, we can ignore \u03c32in a\nnormalized version of mean squared prediction error:\nMSEnorm(\u02c6Y (x)) = 1 \u2212 rT(x)R\u22121r(x) +(1 \u2212 1TR\u22121r(x))2\n1TR\u221211\n. (7)\nWe see that MSEnorm(\u02c6Y (x)) is determined by R and r(x) only. Thus, it is a function\nof n\u2014as R and r(x) are an n \u00d7 n matrix and an n \u00d7 1 vector, respectively\u2014and the\ncorrelations in R and r(x). Dimensionality, d, affects MSEnorm(\u02c6Y (x)) only indirectly via\nthese correlations.\nFor simplicity, we will explore the factors affecting MSEnorm(\u02c6Y (x)) for completely\nrandom Latin hypercube designs (where the columns are permuted independently). We\nconsider the case where n is fixed and look at the effect of d and \u03b81,...,\u03b8d on the\ncorrelation structure and on MSEnorm(\u02c6Y (x)) averaged over x. Our argument establishes\ntwo main results:\n1. For moderately large d and a random LHD, the distribution of inter-point squared\ndistance (weighted by \u03b81,...,\u03b8d) in (2) can be approximated by a normal distribu-\ntion, with mean and variance given by simple functions of \u03b81,...,\u03b8d. The approxi-\nmate distribution of (off-diagonal) correlations in R follows from the transformation\nin (1).\n2. Under the same conditions, the distribution of correlations in r(x) for x drawn\nrandomly from [0,1]dis similar to the distribution of correlations in R.\nWe recognize that the matrix inverse in (7) makes MSEnormmuch more complicated\nthan can be explained by these distributions of correlations. Nonetheless, the simula-\ntions in Section 5 and 6 show that the factors affecting the correlation distribution explain\nmuch of the effect of d and \u03b81,...,\u03b8don our empirical accuracy measures. Indeed, un-\nderstanding these factors leads to simulation studies with straightforward interpretation.\nTake two points, x and x\u2032, at random from a random LHD. An LHD is defined here\n11"},{"page":13,"text":"to have fixed grid points {0,1\/(n \u2212 1),...,1} for each variable xj. Let\nhj= |xj\u2212 x\u2032\nj|\nbe the unweighted distance in dimension j appearing in h in (2). The first two moments\nof h2\njare given by Lemma 1.\nLemma 1: Let hj be the distance between two randomly chosen points xj and x\u2032\njin\ndimension j for a random LHD. Then\nP(hj= i\/(n \u2212 1)) =n \u2212 i\n?n\n2\n?\nfor i = 1,...,n \u2212 1,\nE(h2\nj) \u2261 m1(n) =1\n6\nn(n + 1)\n(n \u2212 1)2,\nand\nVar(h2\nj) \u2261 m2(n) =\n1\n180\nn(n \u2212 2)(n + 1)(7n + 9)\n(n \u2212 1)4\n.\nThe proof of Lemma 1 can be found in Appendix A. Note that the two moments converge\nto 1\/6 and 7\/180 as n \u2192 \u221e.\nIf d = 1 than the probability distribution P(hj = i\/(n \u2212 1)) in Lemma 1 exactly\ndescribes the distribution of all possible distances between distinct points x and x\u2032. That\nis, since the design points are on the grid, every possible distance i\/(n \u2212 1) occurs n \u2212 i\ntimes.\nIf d > 1, however, not all of the possible distances over all dimensions will be ob-\nserved in any one design, and we rely on the moments given in Lemma 1 to describe\nbehavior. Specifically, for two randomly chosen points, the squared distance in (2) across\nall dimensions which arises if pj= 2 has expectation\nE(h) = m1(n)\nd\n?\nj=1\n\u03b8j. (8)\nFor a completely random LHD, which has independently permuted columns,\nVar(h) = m2(n)\nd\n?\nj=1\n\u03b82\nj. (9)\n12"},{"page":14,"text":"Furthermore, as d increases, the central limit theorem applies unless there are a few\n\u03b8j weights that dominate, so that h is approximately normal with mean and variance\ngiven by (8) and (9). Hence, the correlation in (1) is approximately log-normal with\nthese moments (after a change of sign). Figure 2 compares the empirical distributions\nfor a single random LHD with the approximations, for d = 10, n = 100, and \u03b8 =\n(2.71,2.17,1.69,1.27,0.91,0.61,0.37,0.19,0.07,0.01) the approximations are seen to be\ngood. The values chosen for \u03b8j comprise a canonical configuration, to be explained in\nSection 5.\n012345\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nh\ndensity\n0 0.2 0.4\ncorrelation\n0.60.81\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\ndensity\nFigure 2: Distribution of squared distance (left panel) and correlation (right panel) for\na randomly chosen pair of points from a random Latin hypercube design with d = 10,\nn = 100, and \u03b8 = (2.71,2.17,1.69,1.27,0.91,0.61,0.37,0.19,0.07,0.01)\nSimilarly, Figure 3 shows the analogous distributions for the vector r(x). The em-\npirical distribution of the distance and the correlation are taken over a single random\nLHD, D, and the distance between x and all n points in D is computed for 50 randomly\nchosen test point x \u2208 [0,1]d. The same normal or log-normal approximations established\nabove for inter-point distance or correlation are transferred to test-point to design-point\ndistance or correlation. It is seen that the correlations in r(x) behave like those in R.\nNote that there is a negative impact on prediction accuracy when the mean distance\n13"},{"page":15,"text":"012345\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nh\ndensity\n0 0.2 0.4\ncorrelation\n0.6 0.81\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\ndensity\nFigure 3: Distribution of squared distance (left panel) and correlation (right panel) for a\nrandomly chosen test point and a random Latin hypercube design with d = 10, n = 100,\nand \u03b8 = (2.71,2.17,1.69,1.27,0.91,0.61,0.37,0.19,0.07,0.01)\nin (8) increases or when the variance in (9) decreases. When the variance decreases, small\nsquared distances (high correlations) are less likely, whereas high-correlation neighboring\npoints lead to good prediction accuracy. Indeed, if we want to predict Y (x), just one\nclose design neighbor, x(i), of x, close in the sense of correlation, may by itself give good\nprediction accuracy. Let \u03c1 = exp(\u2212h(x,x(i))) be the correlation for these two points.\nPredicting using only Y (x(i)) provides an upper bound on MSEnormobtained from (7).\nAs R is the scalar 1, and we have\nMSEnorm(\u02c6Y (x)) < 1 \u2212 \u03c12+ (1 \u2212 \u03c1)2= 2(1 \u2212 \u03c1).\nIf \u03c1 is larger than about 0.95, or equivalently h is less than about 0.05, MSEnorm(\u02c6Y (x)) <\n(10)\n0.1, the target we have set.\nThe magnitudes of the correlations in R and r(x), which lead to MSEnorm in (7),\ndepend on \u03c4 =?d\npractical consequences. First, these two quantities are used to plan the simulations in\nj=1\u03b8j and \u03c8 =?d\nj=1\u03b82\njto a good approximation. There are two\nSection 5 and 6. We find that the behavior of the empirical analog, eavg, of MSEnorm\nis largely dependent on \u03c4 and \u03c8. Secondly, the distributions of the correlations in r(x)\n14"},{"page":16,"text":"(for random test points) and in R (between design points) are similar. The implication\nis that accuracy estimates will be similar from cross validation based on leaving out one\ndesign point at a time versus random test points.\nThere are many possible \u03b81,...,\u03b8dconfigurations, and we examine three special cases,\nordered from worst to best behavior in terms of the effect of dimensionality.\n1. Suppose \u03b81= \u00b7\u00b7\u00b7 = \u03b8d= \u03b8, i.e., as dimensionality increases, further equally active\nvariables are added. Then, \u03c4 = d\u03b8 and \u03c8 = d\u03b82. Thus, the mean of the distribution\nof h increases linearly with d, the standard deviation of the distribution increases as\n\u221ad, and the h distribution becomes stochastically larger with d. For large enough\nd, prediction accuracy will be poor, even if \u03b8 is small.\n2. Suppose \u03c4 is kept constant, i.e., a fixed amount of total sensitivity is spread across\nall dimensions. Clearly, \u03c8 takes its minimum value of \u03c8 = \u03c42\/d when \u03b81= \u00b7\u00b7\u00b7 =\n\u03b8d= \u03c4\/d. Thus, equally active factors are worst for prediction accuracy. Moreover,\nas \u03c8 = \u03c42\/d decreases with d, this effect becomes worse as d increases. For large\nenough d, the h distribution will become concentrated at its mean, m1(n)\u03c4, and\nthe limiting accuracy depends on \u03c4. In this sense, if the total amount of sensitivity\nis kept constant, the worst-case effect of dimensionality is small.\n3. Alternatively, suppose we keep \u03c4 and \u03c8 constant as d increases. Write\n\u03c8 =\nd\n?\nj=1\n\u03b82\nj=\nd\n?\nj=1\n(\u03b8j\u2212\u00af\u03b8)2+1\nd\u03c42. (11)\nBecause the second term on the right decreases with d,?d\nto keep \u03c8 constant. Another way of looking at the fact that \u03b81,...,\u03b8dmust become\nj=1(\u03b8j\u2212\u00af\u03b8)2must increase\nmore variable with d to maintain prediction accuracy is that some dimensions are\nmore active than others, or there is effect sparsity.\nThe argument that accuracy decreases as \u03c4 =?d\ndecreases is borne out by the simulations in Section 5 and 6.\nj=1\u03b8jincreases or as \u03c8 =?d\nj=1\u03b82\nj\n15"},{"page":17,"text":"The quantitative effect of n on accuracy is less obvious, however. The mean and\nvariance of the squared distance distribution in (8) and (9) do not depend on n in\nthe limit. Thus, R and r(x) in (7) have elements that depend only weakly on n in this\nstatistical sense. Rather, MSEnormdepends on n because R and r(x) have more elements.\nThe closest neighbor in the bound (10) will tend to become closer with larger n, thus\ndriving the bound down. A full analysis of the impact of using all n design points is\ncomplicated by the inverse of R in (7). All that we can say is that harder problems\n(larger \u03c4 and smaller \u03c8) will require larger sample sizes, regardless of dimensionality\nto a large extent. This insight greatly facilitates quantification of the impact of n by\nsimulation in Section 5 and 6.\nIf p1= \u00b7\u00b7\u00b7 = pd, but the common value is less than 2, m1(n) and m2(n) in Lemma 1\nwill change. The mean and variance of h in (8) and in (9) will still depend on \u03c4 and \u03c8,\nhowever.\n5 Simulation Results for Average Error\nThe arguments in Section 4 suggest that the effect of the correlation parameters on eavg\nis through \u03c4 and \u03c8, thereby diminishing the role of d. To investigate this further we will\nengage a simulation study that changes dimension but keeps \u03c4,\u03c8 fixed.\nBut before doing so we must decide on the configurations of the \u03b8 vectors to be\nexplored. Past experience has indicated that for well-behaved outputs there may be\na few large components of \u03b8, a few moderately sized, and the remainder small. For\nexample, for the G-protein model and the 33-run experiment,\u02c6\u03b8 = (1.71,0.29,0.27,0.25)\nhas one moderate value and the other three are small. From this point of view, we will\nadopt a two-parameter class of canonical configurations of \u03b8, defined by\n??\nHere \u03b8j decreases in j and?d\ncharacteristics we expect, especially as d gets large. Examples of \u03b8 configurations for\n\u03b8j= \u03c4\n1 \u2212j \u2212 1\nd\n?b\n\u2212\n?\n1 \u2212j\nd\n?b?\nfor j = 1,...,d,and b \u2265 1,\u03c4 > 0. (12)\nj=1\u03b8j = \u03c4. The generated \u03b8 vector tends to have the\n16"},{"page":18,"text":"d = 10 and \u03c4 = 1 are given in Table 1. When \u03c4 ?= 1, the value of \u03b8 is found by\nmultiplying each \u03b8jin the table by \u03c4.\nb\u03c8\u03b81\n\u03b82\n\u03b83\n\u03b84\n\u03b85\n\u03b86\n\u03b87\n\u03b88\n\u03b89\n\u03b810\n1 0.1 0.10.10.10.1 0.10.1 0.10.1 0.1 0.1\n3 0.18 0.2710.217 0.1690.127 0.0910.0610.037 0.019 0.0070.001\n9 0.45 0.6130.2530.094 0.0300.0080.0020000\nTable 1: Configurations of \u03b8 for d = 10\nData for the simulation study are generated as follows. Given d and n, select a\nmaximin LHD D of n points in [0,1]d. Fix values of \u00b5 = 0,\u03c32= 1, p = 2 and select\na canonical \u03b8 (as specified above) for the parameters of the GP given in (2). Gener-\nate 50 independent realizations of the GP resulting in 50 different sets of observations\n{y(x(i));x(i)\u2208 D}. Since, the measure of accuracy in (5) or (6) is standardized by the\nrange, the particular value of \u03c32= 1 is largely irrelevant.\nFor each data set form a predictor using (3) with the value of \u03b8 the same as that used\nto generate the simulated data. Alternatively, for each data set, we could estimate \u03b8\nand construct a predictor with\u02c6\u03b8. We found that there is no essential difference between\npredictors based on \u03b8 and\u02c6\u03b8 in terms of our summary measures of prediction accuracy,\nand using the fixed \u03b8 takes much less time in our extensive study. The predictor leads\nto a value of eavgin (5) for each data set.\nWe start with d = 5 and b = 1 in (12), which results in \u03b8j = \u03c4\/5,j = 1,...,5. As\nargued in Section 4, this choice of \u03b8 minimizes \u03c8 for a fixed \u03c4 and represent a \u201cworst\ncase\u201d starting point. For a given \u03c4 value, \u03c8 = \u03c42\/5 when d = 5. If \u03c4 and \u03c8 are kept\nconstant as d changes, then the canonical \u03b8 vector must satisfy?\u03b82\n15, and 20, this means that b = 3.445, 5.507, and 7.55, respectively, in (12). Values of\nj= \u03c42\/5. For d = 10,\n\u03c4 = 3, 10, 20, and 40 are chosen to cover problems from \u201ceasy\u201d to \u201cvery hard\u201d.\nThe arguments in Section 4 suggest that similar accuracy should be obtained in any\ndimension for fixed values of n, \u03c4 and \u03c8, but intuitively we expect that n must increase\n17"},{"page":19,"text":"with d. Our results for the first two moments may not fully explain the behavior of\nthe tails of the distribution of h, and small distances in particular play a prominent role.\nThus, we allow n to increase modestly with d, specifically linearly. We also allow different\nrates, i.e., n = kd, where k = 7, 10, 15, or 20.\nThe four panels in Figure 4 correspond to \u03c4 = 3, 10, 20, and 40, respectively. In each\npanel, four curves are plotted, for d = 5, 10, 15, and 20, respectively. A curve shows\nthe mean of eavgcomputed from the 50 realizations of the GP, which we denote by \u00af eavg,\nplotted against k (recall n = kd). Several features of these plots are worth singling out:\n\u2022 All curves lie below 0.20 suggesting that even in very hard problems (\u03c4 = 40) the\naverage error does not get extremely large.\n\u2022 The case of \u03c4 = 3 represents an \u201ceasy\u201d problem owing to the small components of\n\u03b8.\n\u2022 When \u03c8 is fixed, the curves for d = 5, 10, 15, and 20 are all quite close.\n\u2022 The choice of n = 10d leads to predictions that on average are accurate to within\n10% of the range of the data providing that \u03c4 \u2264 10; reliable fits are barely obtainable\nor not obtainable for \u03c4 \u2265 20.\n\u2022 The improvement in fit for sample sizes greater than n = 10d is marginal.\nSuppose \u00af eavgdecreases with n approximately at the convergence rate n\u2212c. The rate\nc can be estimated from the points shown in Figure 4 from the slope of the least squares\nfit of log(\u00af eavg) regressed on log(k). The estimated rates are in Table 2.\nThere are a few interesting things to notice in Table 2. For easy problems (\u03c4 = 3)\nconvergence rates close to 1 are achievable for dimensions as large as d = 20 so that\ndoubling sample size can reduce eavg by half. On the other hand, in hard problems\nthe rates of convergence can be very small. For example, when d = 15 and \u03c4 = 20,\nit takes about 8 times as many runs to reduce eavg by half. When \u03c4 = 40 it appears\nhopeless to reduce eavgsubstantially without enormous sample sizes. In such situations,\nthe computer experiment may have to be reformulated and restricted.\n18"},{"page":20,"text":"10 15 20\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nk\n\u03c4=3\ne\n1015 20\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nk\n\u03c4=10\ne\n101520\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nk\n\u03c4=20\ne\n101520\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\nk\n\u03c4=40\ne\nFigure 4: The four panels correspond to four values of \u03c4, and \u03c8 = \u03c42\/5. In each panel,\n\u00af eavgis plotted against k for d = 5 (solid line), d = 10 (dashed line), d = 15 (dotted line)\nand d = 20 (dot-dashed line).\n\u03c4\nd3 1020 40\n5 1.340.63 0.430.08\n100.97 0.570.31 0.14\n15 0.96 0.530.360.13\n20 0.870.51 0.28 0.19\nTable 2: Estimated convergence rates for \u00af eavg\nThe arguments in Section 4 suggest that for fixed total sensitivity \u03c4, dividing \u03c4 equally\nacross the d input variables is the worst case for prediction accuracy, i.e., \u03c8 = \u03c42\/d.\nFigure 5 explores worst-case problems by plotting eavg against \u03c4. There is a separate\n19"},{"page":21,"text":"plot for d = 5,10,15,20, and n = 10d throughout. Fifty simulated realizations are made\nfor each value of \u03c4. The lines in Figure 5 drawn through the averages of eavg show\n1020 3040\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n\u03c4\ne\nd=5\n10 2030 40\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n\u03c4\ne\nd=10\n10 2030 40\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n\u03c4\ne\nd=15\n10 20 3040\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n\u03c4\ne\nd=20\nFigure 5: The four panels correspond to d = 5, 10, 15, and 20, respectively. In each\npanel, eavg(squares) from 50 realizations and \u00af eavg(solid line) are plotted against \u03c4. The\nhorizontal line indicates accuracy to within 10% of the range of the data.\nlittle difference as d increases, as predicted in Section 4 for the worst case studied here.\nThere is a small dimensionality effect (and n = 10d is increasing with d), but the total\nsensitivity, \u03c4, is the important factor. For \u03c4 \u2265 20, eavg is above the target of 0.1 for\nall d studied, though it tends to remain below about 0.2 to 0.25. This latter fact is a\nsomewhat surprising feature and, in fact, holds for \u03c4 as large as 100 (not shown).\nTo investigate the more realistic situation where the problem has some degree of\nsparsity we allow \u03c8 to vary. We fix d = 10 and n = 100. As suggested by Figure 4, for\nfixed values of \u03c4 and \u03c8, results for other values of d (with n = 10d) are similar. For each\nfixed value of \u03c4 we increase the value of \u03c8 so that the sparsity is increased, and the total\nsensitivity of the function is shifted to fewer and fewer dimensions.\n20"},{"page":22,"text":"246\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n\u03c8\ne\n\u03c4=3\n2040 60\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n\u03c8\ne\n\u03c4=10\n50 100150200 250 300\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n\u03c8\ne\n\u03c4=20\n200400 600 800 10001200\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n\u03c8\ne\n\u03c4=40\nFigure 6: The four panels correspond to four values of \u03c4. In each panel, eavg(squares)\nfrom 50 realizations and \u00af eavg (solid line) are plotted against \u03c8. The horizontal line\nindicates accuracy to within 10% of the range of the data.\nEven a moderate degree of sparsity can result in drastic reduction of error. The\n\u03c4 = 40 panel in Figure 6 is interesting, since even in such a complex problem reasonable\naccuracy can be obtained when there is a degree of sparsity. In particular the last few\nvalues of \u03c8 represent situations where the 10-dimensional problem contains five or fewer\nactive dimensions.\n6 Simulation Results for Maximum Error\nIn Section 5 results were presented corresponding to \u00af eavg; in this section we discuss the\nsimilarities and differences that arise in using \u00af emax, i.e. the average of emaxin (6) across\nsimulations. The four panels in Figure 7 correspond to \u03c4 = 3, 10, 20, and 40, respectively.\nIn each panel, curves for \u00af emax(averaged across 50 simulations) are plotted for d = 5, 10,\n21"},{"page":23,"text":"15, and 20. Clearly, one would not expect to see the same level of accuracy as obtained\nusing \u00af eavgso we set a threshold of 0.2 as opposed to the threshold of 0.1 used for eavg.\nComparing this to the plots in Figure 4 there a few things to notice:\n101520\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nk\n\u03c4=3\ne\n101520\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nk\n\u03c4=10\ne\n101520\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nk\n\u03c4=20\ne\n101520\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nk\n\u03c4=40\ne\nFigure 7: The four panels correspond to four values of \u03c4 and \u03c8 = \u03c42\/5. In each panel\n\u00af emaxis plotted against k for d = 5 (solid line), d = 10 (dashed line), d = 15 (dotted line)\nand d = 20 (dot-dashed line).\n\u2022 The case of \u03c4 = 3 again represents an \u201ceasy\u201d problem owing to the small compo-\nnents of \u03b8.\n\u2022 When \u03c4 is fixed, the curves for d = 5, 10, 15, and 20 are all quite close.\n\u2022 The choice of n = 10d leads to predictions that on average are accurate to within\n20% to 30% of the range of the data providing that \u03c4 \u2264 10; reliable fits are barely\nobtainable or not obtainable for \u03c4 \u2265 20. When \u03c4 = 10 thresholds for emax are\nsomewhat harder to reach compared to those using eavg.\n22"},{"page":24,"text":"\u2022 The improvement in fit for sample sizes greater than n = 10d is marginal.\nThe analysis leading to Table 2 can be duplicated for \u00af emax; the result is Table 3. The\nconvergence rates, as expected, are lower than for \u00af eavgand are essentially 0 when \u03c4 = 40.\nTable 3 can be used in the same way as Table 2 is used in Section 7 to derive sample\nsizes needed to reduce \u00af emax.\n\u03c4\nd3 1020 40\n5 1.110.41 0.260.00\n10 0.740.370.140.03\n150.720.370.220.00\n200.640.280.110.04\nTable 3: Estimated convergence rates for \u00af emax\nFigure 8 explores worst-case problems (equal \u03b8j) by plotting emaxagainst \u03c4. There is\na separate plot for d = 5,10,15,20, and n = 10d throughout. Fifty simulated realizations\nare made for each value of \u03c4. The lines in Figure 8 drawn through the averages of emax\nshow little difference as d increases. Comparing this to the analogous plot in Figure 5\nwe see the same general trend using emax as opposed to eavg. There is again a small\ndimensionality effect, but the total sensitivity, \u03c4, is the important factor. For \u03c4 \u2265 20,\nemaxis above the target of 0.2 for all d.\nThe Figure 9 shows the effect of sparsity when using emaxand corresponds to the plot\nin Figure 6 for eavg. Even a moderate degree of sparsity can result in drastic reduction of\nerror. The \u03c4 = 40 panel in Figure 9 is interesting, since even in such a complex problem\nreasonable accuracy can be obtained when there is a degree of sparsity. In particular the\nlast few values of \u03c8 represent situations where the 10-dimensional problem contains five\nor fewer active dimensions. This is similar to what was found in Figure 6.\n23"},{"page":25,"text":"1020 3040\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n\u03c4\ne\nd=5\n1020 3040\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n\u03c4\ne\nd=10\n1020 3040\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n\u03c4\ne\nd=15\n102030 40\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n\u03c4\ne\nd=20\nFigure 8: The four panels correspond to d = 5, 10, 15, and 20, respectively. In each\npanel, emax(squares) from 50 realizations and \u00af emax(solid line) are plotted against \u03c4. The\nhorizontal line indicates accuracy to within 20% of the range of the data.\n7Follow-up Experiments\nSuppose an initial experiment of given sample size has been conducted. We now have\nreal data from running the code to fit a GP model following the methodology described\nin Section 2. Estimates of the correlation parameters, \u03b8 and p, are available, as well as\nvalues of eavg|codeand emax|codecomputed from (5) and (6).\nWhat should be done to augment the initial design, if anything? Set eAas a threshold\nvalue for acceptable eavg (for example, eA= 0.1). If eavg|code < eAthen nothing more\nneeds to be done to increase accuracy. If eavg|code> eAthen we propose the following\nfollow-up strategy:\n1. Do a simulation study using the estimated correlation parameters,\u02c6\u03b8 and \u02c6 p, and\nthe initial sample size. Compute eavg(and emax) for each realized data set.\n24"},{"page":26,"text":"246\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n\u03c8\ne\n\u03c4=3\n204060\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n\u03c8\ne\n\u03c4=10\n50 100150200250 300\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n\u03c8\ne\n\u03c4=20\n200 400600 800 10001200\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n\u03c8\ne\n\u03c4=40\nFigure 9: The four panels correspond to four values of \u03c4. In each panel, emax(squares)\nfrom 50 realizations and \u00af emax (solid line) are plotted against \u03c8. The horizontal line\nindicates accuracy to within 20% of the range of the data.\n2. If the distribution of the eavg values from the simulations suggests a non-trivial\nprobability of exceeding eA, it is plausible that the initial sample size is inadequate\nand we could go to Step 4. Otherwise, continue with Step 3.\n3. If the distribution of the eavg values from the simulations is inconsistent with\neavg|code, the appropriateness of the GP model is in question; see Section 8. Of\ncourse, it would be foolish to worry about small inconsistencies, and some subjec-\ntivity is inevitable in assessing what is substantial.\n4. To decide on a follow-up sample size, explore plausible choices of n through a\nsimulation study using the estimated correlation parameters,\u02c6\u03b8 and \u02c6 p.\nIn order to illustrate the points above, we revisit the G-protein example in Section 3.\nThe initial sample size of n = 33 led to eavg|code = 0.0221 (see Figure 1). For most\n25"},{"page":27,"text":"examples this would be considered small, and there would be nothing further to do.\nOn the other hand, if we wanted to reduce the error rate by a half, a simulation could\nbe performed using the outline above. Since \u02c6 p = 2, we could also consult Table 2 to\ncalculate the desired sample size. From the analysis of the n = 33 runs, \u02c6 \u03c4 = 2.52 and\n\u02c6\u03c8 = 3.15. For d = 5 and \u03c4 = 3, Table 2 gives a rate of convergence of 1.34, which is very\nlikely to be conservative. Thus, a sample size of at least n = 33(21\/1.34) = 55 is required.\nFrom Figure 1, eavg|codeat n = 60 is 0.0137, which is just slightly larger than half of the\nobserved value when n = 33.\nGough and Welch (1994) considered a model for ocean circulation with d = 7 inputs\nand seven different outputs, y1,...,y7. From an initial experiment yielding 36 good runs,\na GP was fit separately for each output. In each case p = 2 for each output y1,...,y7.\nEstimated values of \u03b8 are provided in Table 4. The \u02c6 \u03c4 and\u02c6\u03c8 rows in Table 4 summarize\ny1\ny2\ny3\ny4\ny5\ny6\ny7\n\u02c6\u03b81\n0.3190.5447.8990.3010.1001.249 2.512\n\u02c6\u03b82\n00.0260.0010.5520.8270.001 0.003\n\u02c6\u03b83\n0.6860.012 0.01200.115 0.066 0.037\n\u02c6\u03b84\n0.267 1.2020.02100.156 0.4780\n\u02c6\u03b85\n0.0290.197 0.0030.060 0.044 0.1191.463\n\u02c6\u03b86\n0 0.008 0.006 1.1360.6850.4330.665\n\u02c6\u03b87\n0.229 0.031 0.1290.069 0.0090.0821.001\n\u02c6 \u03c4 1.532.02 8.07 2.121.94 2.435.68\n\u02c6\u03c8 0.69 1.78 62.421.69 1.21 2.019.89\nTable 4: Estimates of \u03b8 for the ocean-circulation model.\nthe GP fits from the 36 runs.\nIf eA= 0.1, the values for eavg|codein Table 5 show that a reasonable approximation\nhas been obtained. Moreover, for all output variables, eavg|codeis within \u00af eavg\u00b1 2?sd(eavg),\ni.e., eavg|codelies within the support of the empirical distribution of simulated eavg. The\n26"},{"page":28,"text":"ny1\ny2\ny3\ny4\ny5\ny6\ny7\n36eavg|code\n0.0340.0410.0170.037 0.0340.039 0.078\n\u00af eavg\n0.028 0.033 0.0220.0300.042 0.051 0.072\n?sd(eavg)\n?sd(eavg)\n0.008 0.0110.0080.0090.0110.0260.021\n70\u00af eavg\n0.0080.011 0.0070.0090.0180.021 0.032\n0.0040.003 0.0030.0070.0060.0060.009\nTable 5: Actual and simulated accuracy measures for the ocean-circulation model\naccuracy for y7is lower than for the other output variables, however, and we consider\nreducing eavg|codefrom 0.078 to, say, 0.05. In this case \u02c6 p = 2, and again we can use Table\n2 for guidance in choosing an appropriate sample size.\nInterpolating the convergence rates in the table suggest that \u00af eavgshould be decreasing\nat rate roughly 1\/n, and reducing the sample size to 0.05 would require a sample size\nof approximately 56 runs. Alternatively, cutting \u00af eavgby half would require doubling the\nrun size. As no further code runs are available we investigate this strategy by simulating\nwhat would have happened if n = 70 runs had been performed: for y7, \u00af eavgis reduced by\njust over a factor of 2, as predicted.\nChapman et al. (1994) analyzed a computer code describing the seasonal growth and\ndecline of Arctic sea ice. The code had d = 13 input variables and four outputs, y1,...,y4.\nFrom an initial design of n1= 69 runs, GPs were fit separately for each output. Every\nfitted GP had at least one input variables with \u02c6 pj < 2. Estimated values of \u03b8j and\n\u03b1j= 2 \u2212 pjfor n1= 69 runs are provided in Table 6.\nEstimated vales of \u03b8jand \u03b1j= 2 \u2212 pjfor n = 157 runs are provided in Table 7.\nThe values of eavg|codeare in Table 8; each is below 0.1, and if eA= 0.1 we would be\ntempted to stop.\nThe eavg|codevalues for y3and y4are below 0.1 and within 2?sd(eavg) of \u00af eavgbut the\nsistent with the observed emax|codevalues, as evidenced by \u00af emaxand?sd(emax) in Table 8,\n27\nemax|codevalues of about 0.5 are of concern. The simulated emaxdistributions are incon-"},{"page":29,"text":"y1\ny2\ny3\ny4\nj\n\u02c6\u03b8j\n\u02c6 \u03b1j\n\u02c6\u03b8j\n\u02c6 \u03b1j\n\u02c6\u03b8j\n\u02c6 \u03b1j\n\u02c6\u03b8j\n\u02c6 \u03b1j\n10.0100.0000.0230.0000.021 0.0000.045 0.000\n20.069 0.0000.011 0.0000.0000.3440.000 1.000\n30.4550.0000.000 0.0000.5330.000 0.4450.000\n40.020 0.0000.0210.0000.2150.000 0.2660.000\n50.0010.000 0.126 0.0000.0001.0000.386 0.000\n60.024 0.000 0.1050.000 0.0000.0000.0530.038\n7 0.168 0.0000.0300.000 1.8830.000 0.8720.000\n80.3010.000 2.1710.0000.252 0.0940.1810.316\n9 0.058 0.000 0.3590.0000.0010.000 0.0280.000\n10 0.064 0.0000.0000.000 0.094 0.8640.5540.000\n11 0.0151.000 0.3860.000 0.990 0.5131.182 0.188\n120.000 0.907 0.0030.000 0.0000.344 0.0110.000\n130.0001.000 0.4160.135 0.000 0.0000.000 0.000\nTable 6: Estimates of \u03b8jand \u03b1j= 2 \u2212 pjfor the sea-ice code using n1= 69 runs.\nalthough for y3the range of the simulated emaxvalues covers emax|code. The sea-ice code\nfailed to converge for 12 of 81 attempted runs (hence the 69 good runs), a suggestion of\nerratic behavior of the code and a possible explanation of the difference between actual\nand simulated error in some regions of the input space.\nFaced by similar concerns about the approximation accuracy from the initial experi-\nment, Chapman et al. (1994) opted to make additional runs and ended up with a total\nof 157 good code runs. As these are the only follow-up runs available, we restrict our\nanalysis to seeing whether we can predict by simulation the impact of such a follow-up\nexperiment.\nThe accuracy measures for the n = 157 runs conducted and from simulation are\n28"},{"page":30,"text":"y1\ny2\ny3\ny4\nj\n\u02c6\u03b8j\n\u02c6 \u03b1j\n\u02c6\u03b8j\n\u02c6 \u03b1j\n\u02c6\u03b8j\n\u02c6 \u03b1j\n\u02c6\u03b8j\n\u02c6 \u03b1j\n10.0320.313 0.030 0.1320.048 0.5790.314 0.000\n20.015 0.1150.0220.0130.0031.0000.0670.000\n30.4210.0000.014 0.1580.3240.546 0.3160.503\n40.007 0.0000.0270.3750.001 0.0000.0000.000\n50.008 0.100 0.0330.5940.000 0.8200.2200.000\n6 0.043 0.0000.182 0.0000.0170.3510.2890.000\n70.216 0.0000.013 0.0002.272 0.0101.7110.015\n8 0.5630.0001.273 0.1410.2140.322 0.4760.085\n9 0.093 0.0000.3470.0000.000 0.0000.060 0.000\n10 0.1820.3220.006 0.536 0.4030.6520.331 0.255\n11 0.1840.0590.023 0.000 0.9080.3070.357 0.525\n12 0.000 0.9070.000 0.9070.000 0.9070.000 0.344\n130.003 1.0000.002 0.817 0.0000.9030.000 0.344\nTable 7: Estimates of \u03b8jand \u03b1j= 2 \u2212 pjfor the sea-ice code using n = 157 runs.\ncompared in Table 8. Relative to n = 69, simulation suggests only modest reduction in\neavg. For y4, even this modest reduction is not realized by eavg|code. With n = 157 runs, the\nsimulated values of emaxare again inconsistent with emax|codefor the troublesome y3and\ny4. Although the magnitude of the maximum error is underestimated, the simulations\ncorrectly predict that there will be little impact on emax|codefrom the further runs. Thus,\nthe simulation study leads to the same conclusion that Chapman et al. (1994) reached\nafter the follow-up experiment: Taking more runs is not effective. Alternative ways of\nproceeding are discussed in Section 8.\n29"},{"page":31,"text":"ny1\ny2\ny3\ny4\nAverage error\n69eavg|code\n0.0430.044 0.0930.099\n\u00af eavg\n0.048 0.0440.0790.089\n?sd(eavg)\n\u00af eavg\n0.011 0.0130.0190.018\n157eavg|code\n0.032 0.0310.0790.096\n0.0290.0290.056 0.062\n?sd(eavg)0.0080.009 0.0110.011\nMaximum Error\n69emax|code\n0.2490.1240.4660.559\n\u00af emax\n0.139 0.1280.2250.263\n?sd(emax)\n\u00af emax\n0.0390.0520.0710.079\n157emax|code\n0.189 0.1160.4460.494\n0.103 0.0960.1820.203\n?sd(emax) 0.0350.0330.045 0.055\nTable 8: Actual and simulated accuracy measures for the sea-ice code\n8 Comments and Open Issues\nThere are several open issues, concerned mainly with follow-up once an initial set of code\nruns has been collected and analyzed.\nEffective dimensionality\nThe ocean-circulation model (Gough and Welch, 1994) had an initial sample size of\nn = 36, about half the recommended value of n = 10d. Even so, a good fit was obtained.\nA closer look at this application shows that\u02c6\u03b8 has elements that are near zero for three\nof the input variables. Thus, the input space is effectively reduced to d = 4 dimensions,\nleading to a recommendation of n = 40. If there are good a priori reasons to expect that\n30"},{"page":32,"text":"the number of active dimensions, d0, is less than d then choosing n = 10d0could be a\nuseful complement to the recommended strategy, especially if there are serious budget\nconstraints.\nThe GP model is a poor fit\nGood general strategies to cope with lack of fit of the GP model are not readily\navailable. There is interesting work by Gramacy and Lee (2007) which could be useful\nwhen runs are plentiful. The approach used extensively by Aslett et al. (1998) and by\nGramacy and Lee (2007), of narrowing the space of inputs, better enables approximation\nof code output by a homogeneous GP; the assumption of homogeneity is less sustainable\nwhen the input space is too large. But how to do this in a measured way is not clear and\nneeds further research.\nCanonical configurations of \u03b8\nFor p = 2, we chose a simple two-parameter family in our analyses in Section 5 and 6.\nOther sets of values for \u03b8 can be explored, but we find little incentive to do so for the\npurpose of settling on initial sample size. We have found that even if \u03b8 is not a canonical\nconfiguration there is little to no difference in distributions of eavgor emaxrelative to a\ncanonical \u03b8 provided \u03c4 and \u03c8 are the same.\nTreating a GP with p ?= 2 (as in the sea-ice example)\nWe have not discussed the relevance, nor the use, of \u03c4 and \u03c8 when p ?= 2. The\ninterpretation of \u03c4 and \u03c8 values need to be reexamined.\nIn the case of the exponential correlation function (all pj = 1), the implied prior\ndistribution is on a much larger class of functions and achieving good accuracy is more\ndifficult. It is easy to work out the mean and variance of h1\njas in Lemma 1, and again\nwe find that \u03c4 and \u03c8 should be important. The exact values are given in Lemma 2.\nLemma 2: Let hjbe the distance between two randomly chosen points for variable xj\nin a random LHD. Then\nE(hj) =1\n3\n(n + 1)\n(n \u2212 1),\n31"},{"page":33,"text":"and\nVar(hj) =\n1\n18\n(n \u2212 2)(n + 1)\n(n \u2212 1)2\n.\nThe proof of Lemma 2 can be found in Appendix A. Note that the two moments converge\nto 1\/3 and 1\/18 as n \u2192 \u221e, i.e., they do not depend on n in the limit. The mean of\nh1\nj, is now approximately twice that for the case pj= 2, indicating that larger samples\ncould be needed to achieve desired accuracy. How this all plays out in analogues of the\nanalyses in Section 5 and 6, to enable follow-up recommendations has yet to be explored.\nWhen 1 < pj< 2, exact calculations of the mean and variance of hpj\nj are not avail-\nable. Approximations are obtainable as follows, however. Assume that xj and x\u2032\njare\napproximately independent and uniform on [0,1], and again let hj= |xj\u2212 x\u2032\nthat hpj\nj|. We find\njhas\nE(hpj\nj) =\n2\n(pj+ 1)(pj+ 2)\nand\nE(h2pj\nj ) =\n1\n(pj+ 1)(2pj+ 1).\nFor pj= 2 this produces E(h2\nj) = 1\/6 and Var(h2\nj) = 7\/180, which are the asymptotic\nvalues found in Lemma 1.\nFor the general case, with pj varying with xj, assume the design is a completely\nrandom LHD. Asymptotically, h(x,x\u2032) =?d\non both \u03b8 and p:\nd\n?\nSimilarly, it has asymptotic variance\nj=1\u03b8j|xj\u2212 x\u2032\nj|pjin (2) has mean depending\nE(h) =\nj=1\n\u03b8j\n2\n(pj+ 1)(pj+ 2).\nVar(h) =\nd\n?\nj=1\n\u03b82\nj\n?\n1\n(pj+ 1)(2pj+ 1)\u2212\n4\n(pj+ 1)2(pj+ 2)2\n?\n.\nDefining canonical sets of correlation parameters is now much more complicated. Some\npreliminary calculations for the sea-ice application suggest that the convergence rates for\np ?= 2 are different from those obtained when p = 2 and thus one must examine rates\nfor various combinations of both \u03b8 and p. This too calls for additional examination.\n32"},{"page":34,"text":"9 Discussion\nIn the introduction we raised a set of issues that should be treated. In the subsequent\nsections we have provided evidence that:\n\u2022 \u201cn = 10d\u201d is a viable and valuable rule-of-thumb for choosing an initial sample size\nfor a computer experiment.\n\u2022 Criteria can make a difference for post-experimental analysis but have less influence\non initial sample size. The sea-ice example shows that the conflict between the eavg\nand emaxcriteria has implications, as spelled out in Section 7. However, as seen in\nSection 6 both criteria support the \u201cn = 10d\u201d rule.\n\u2022 When p = 2 there is good information about rates at which error decreases with\nn and when feasible sample sizes are available. These depend on the parameters \u03c4\nand \u03c8, whose values are not known until the post-experimental stage and are then\nuseful for deciding how to follow-up. When p ?= 2 much remains to be done.\n\u2022 In the case that accuracy goals are not met with an initial sample size, a follow-up\nstrategy is needed, but a full analysis is lacking and is a topic for further inquiry.\nAPPENDIX A: Proof of Lemma 1\nProof of Lemma 1:\nLet D be an n \u00d7 d random LHD, and let xj and x\u2032\nof the design in dimension j. The construction of the LHD ensures that xj?= x\u2032\njbe any two randomly chosen runs\nj, and\nhence xj and x\u2032\njare dependent random variables. There are a total of\n?n\n2\n?\npossible\npairs of points and each pair is equally likely. Clearly, P(xj = i\/(n \u2212 1)) = 1\/n and\nP(x\u2032\nj= k\/(n \u2212 1)|xj = i\/(n \u2212 1)) = 1\/(n \u2212 1). Consider any two points that are an\nabsolute distance of i\/(n \u2212 1) apart. By a simple counting argument, there are n \u2212 i\n33"},{"page":35,"text":"pairs giving rise to this distance. This establishes\nP(hj= i\/(n \u2212 1)) =(n \u2212 i)\n?n\n2\n?\n=2(n \u2212 i)\nn(n \u2212 1),i = 1,...,n \u2212 1.\nThe expected value of h2\njis\nE(h2\nj) = E\n?\ni2\n(n \u2212 1)2\n2\nn(n \u2212 1)3\n?\n=\n1\n(n \u2212 1)2\n?\n2\nn(n \u2212 1)\n?\nn\u22121\n?\ni=1\ni2(n \u2212 i)\n?\n=\n?\nn\nn\u22121\n?\ni=1\ni2\u2212\nn\u22121\n?\ni=1\ni3\n=1\n6\nn(n + 1)\n(n \u2212 1)2.\nSimilarly,\nVar(h2\nj) = E(h4\nj) \u2212 E(h2\nj)2= E\n?\nn\u22121\n?\ni4\n(n \u2212 1)4\n?\n\u2212\n?\n?1\n?1\n6\nn(n + 1)\n(n \u2212 1)2\nn(n + 1)\n(n \u2212 1)2\n?2\n=\n1\n(n \u2212 1)4\n1\n180\n?\n2\nn(n \u2212 1)\ni=1\ni4(n \u2212 i)\u2212\n6\n?2\n=\nn(n \u2212 2)(n + 1)(7n + 9)\n(n \u2212 1)4\n.\nAlgebra was carried out in Maple. 2\nProof of Lemma 2:\nFollowing Lemma 1, the expected value is:\nE(hj) = E\n?\ni\n(n \u2212 1)\n2\n?\n=\n1\n(n \u2212 1)\nn\u22121\n?\n?\n2\nn(n \u2212 1)\n?\nn\u22121\n?\ni=1\ni(n \u2212 i)\n?\n=\nn(n \u2212 1)2\n?\nn\ni=1\ni \u2212\nn\u22121\n?\ni=1\ni2\n=1\n3\n(n + 1)\n(n \u2212 1)\nSimilarly, the variance is:\nVar(h2\nj) = E(h2\nj) \u2212 E(hj)2=1\n(n \u2212 2)(n + 1)\n(n \u2212 1)2\n6\nn(n + 1)\n(n \u2212 1)2\u2212\n?1\n3\n(n + 1)\n(n \u2212 1)\n?2\n=\n1\n18\n.\nAlgebra was carried out in Maple. 2\nACKNOWLEDGEMENTS\nThe research of Loeppky and Welch was supported by grants from the Natural Sci-\nences and Engineering Research Council of Canada.\n34"},{"page":36,"text":"References\nAslett, R., Buck, R. J., Duvall, S. G., Sacks, J., and Welch, W. J. (1998), \u201cCircuit\nOptimization via Sequential Computer Experiments: Design of an Ouput Buffer,\u201d\nApplied Statistics, 47, 31\u201348.\nBayarri, M. J., Berger, J. O., Garcia-Donato, G., Sacks, J., Walsh, D., Cafeo, J., and\nParthasarathy, R. (2007), \u201cComputer Model Validation with Function Output,\u201d An-\nnals of Statistics, 35, 1874\u20131906.\nChapman, W. L., Welch, W. J., Bowman, K. P., Sacks, J., and Walsh, J. E. (1994),\n\u201cArctic sea ice variability: Model sensitivities and a multidecadal simulation,\u201d Journal\nof Geophysical Research, 99, 919\u2013936.\nChen, X. (1996), \u201cProperties of Models for Computer Experiments,\u201d Ph.D. thesis, Uni-\nversity of Waterloo.\nCurrin, C., Mitchell, T., Morris, M., and Ylvisaker, D. (1991), \u201cBayesian Prediction of\nDeterministic Functions, With Applications to the Design and Analysis of Computer\nExperiments,\u201d Journal of the American Statistical Association, 86, 953\u2013963.\nFeeley, R., Frenklach, M., Paulo, R., and Sacks, J. (2007), \u201cA Study of the G-protein\nComputer Model,\u201d Tech. rep., Unpublished.\nGough, W. A. and Welch, W. J. (1994), \u201cParameter Space Exploration of an Ocean\nGeneral Circulation Model Using an Isopycnal Mixing Parameterization,\u201d Journal of\nMarine Research, 52, 773\u2013796.\nGramacy, R. B. and Lee, H. K. H. (2007), \u201cBayesian Treed Gaussian Process Mod-\nels with an Application to Computer Modeling,\u201d Journal of the American Statistical\nAssociation, to appear.\nHigdon, D., Kennedy, M., Cavendish, J. C., Cafeo, J. A., and Ryne, R. D. (2004),\n\u201cCombining Field Data and Computer Simulation for Calibration and Prediction,\u201d\nSIAM Journal on Scientific Computing, 26, 448\u2013466.\n35"},{"page":37,"text":"Higdon, D., Williams, R., Moore, L., McKay, M., and Keller-McNulty, S. (2005), \u201cUncer-\ntainty Quantification for Combining Experimental Data and Computer Simulations,\u201d\nin Society for Modeling and Simulation International, eds. Pace, D. and Stevenson, S.\nJones, D. R., Schonlau, M., and Welch, W. J. (1998), \u201cEfficient global optimization of\nexpensive black-box functions,\u201d Journal of Global Optimization, 13, 455\u2013492.\nLinkletter, C., Bingham, D., Hengartner, N., Higdon, D., and Ye, K. Q. (2006), \u201cVariable\nSelection for Gaussian Process Models in Computer Experiments,\u201d Technometrics, 48,\n478\u2013490.\nMcKay, M. D., Beckman, R. J., and Conover, W. J. (1979), \u201cA Comparison of Three\nMethods for Selecting Values of Input Variables in the Analysis of Ouput from a\nComputer Code,\u201d Technometrics, 21, 239\u2013245.\nMorris, M. D. and Mitchell, T. J. (1995), \u201cExploratory Designs for Computational Ex-\nperiments,\u201d Journal of Statistical Planning and Inference, 43, 381\u2013402.\nO\u2019Hagan, A. (1992), \u201cSome Bayesian Numerical Analysis,\u201d in Bayesian Statistics 4, eds.\nBernardo, J. M., Berger, J. O., Dawid, A. P., and Smith, A. F. M., Oxford University\nPress, pp. 345\u2013363.\nOwen, A. B. (1994), \u201cControlling Correlations in Latin Hypercube Samples,\u201d Journal of\nthe American Statistical Association, 89, 1517\u20131522.\nSacks, J., Schiller, S. B., and Welch, W. J. (1989a), \u201cDesigns for Computer Experiments,\u201d\nTechnometrics, 31, 41\u201347.\nSacks, J., Welch, W. J., Mitchell, T. J., and Wynn, H. P. (1989b), \u201cDesigns and Analysis\nof Computer Experiments (with Discussion),\u201d Statistical Science, 4, 409\u2013435.\nWelch, W. J., Buck, R. J., Sacks, J., Wynn, H. P., Mitchell, T. J., and Morris, M. D.\n(1992), \u201cScreening, Predicting, and Computer Experiments,\u201d Technometrics, 34, 15\u2013\n25.\n36"},{"page":38,"text":"Yi, T.-M., Fazel, M., Liu, X., Otitoju, T., Papachristodoulou, A., Prajna, S., and Doyle.,\nJ. (2005), \u201cApplication of Robust Model Validation Using SOSTOOLS to the Study of\nG-Protein Signaling in Yeast,\u201d in Proceedings of Foundations of Systems Biology and\nEngineering.\n37"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/William_Welch\/publication\/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide\/links\/0046352e6a4733d593000000.pdf","widgetId":"rgw29_56aba2472d207"},"id":"rgw29_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=237548813&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw30_56aba2472d207"},"id":"rgw30_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=237548813&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":237548813,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"0046352e6a4733d593000000","name":"William J. Welch","date":"Jan 27, 2014 ","nameLink":"profile\/William_Welch","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/William_Welch\/publication\/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide\/links\/0046352e6a4733d593000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/William_Welch\/publication\/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide\/links\/0046352e6a4733d593000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"ab73069d3001fa6c2f37b05d5916bffc","showFileSizeNote":false,"fileSize":"383.51 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"0046352e6a4733d593000000","name":"William J. Welch","date":"Jan 27, 2014 ","nameLink":"profile\/William_Welch","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/William_Welch\/publication\/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide\/links\/0046352e6a4733d593000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/William_Welch\/publication\/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide\/links\/0046352e6a4733d593000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"ab73069d3001fa6c2f37b05d5916bffc","showFileSizeNote":false,"fileSize":"383.51 KB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=OpDH6Yw4YqBobOoGDYW6WiBM2LiusmlZEfBJdA1QA5anEeIOy8Ag7yIg8cJF7f2eZ1Z6OXgOCdqhfoJI5ntKvw.5ZVui2Ng99CXlUGRF1RDML2hPqRamyyUlKgT1QWQKfwdWug5p_BI6-cpycnmPu2qKGenRF6kza8DnUQyDD8clw","clickOnPill":"publication.PublicationFigures.html?_sg=BI-H6s70tWJfZEqxVKzGndHsOJsY5lNvWHvQoP6pFvcDIrl_lYHeDHEYyvofbOK5eqDAhRZubwkRl5kSb_Wh8Q.MQ6SksdijwZ-603lGbJoT4QGynuav8DnFnltkyl6ErEPlW52tDSM5FTyhkbx3VFxrQMbSlqTET7cBKMNXlITNw"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FWilliam_Welch%2Fpublication%2F237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide%2Flinks%2F0046352e6a4733d593000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=yujFma010MOnmGAGMPIbKnWVL7NPnHWImke-4IUj14L_A_krmbqWFYh4VtHDYuPv93r_rfia5S_dLftlDzVa4A","urlHash":"3c4c8ef6228a9df78de182ccc4568dec","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=L-JKBfu0lXBQNXd-nOE9Ct6jhF0Mg-o2jH4UgJxCQjgAcwZlD1S_UPq4v-kYdWLy42szTRJVgg6YeQAOzJPJ28Ch6ZqubtqdZgSnSQ6L8o8.HC-UEWSqSsr7Ffg4bGDWZYlm2Hq9m6qUGCjTc2gkm8bFaarPeEg6UCXo67L3bG9EFGtlT-QGaIxeIUf892Y1mA.CLnaoYr3V4T6ou6JUbkR0Uy9uLer3UlPRoHQ1DN9S9f28RDPCajMXEqdwJ0bJgdoVDWRyKotaVq2sWsfeHh3xw","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"0046352e6a4733d593000000","trackedDownloads":{"0046352e6a4733d593000000":{"v":false,"d":false}},"assetId":"AS:101248611127297@1401150958171","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":237548813,"commentCursorPromo":null,"widgetId":"rgw32_56aba2472d207"},"id":"rgw32_56aba2472d207","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FWilliam_Welch%2Fpublication%2F237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide%2Flinks%2F0046352e6a4733d593000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A101248611127297%401401150958171&publicationUid=237548813&linkId=0046352e6a4733d593000000&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Choosing the Sample Size of a Computer Experiment: A Practical Guide","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=S2AJS84dGgnrowCVGTbiUoWwlPeZ1di4USieUr8VNN1WEIOLMXaQZALmq5ml75Kx-nbSzcVCjv7W5uPad7tytNmqmChbjmTliup1V8QmCrA.v5E1bJRQF60QnOm1Z5aZg9AXIKy-RQ-2WI8dx9NJuYcMUbaCFSzuhYsUGy8uo3dIMT83UZQBd-toCGG9UCZgTw.RTADweOit5fSmnzIrtjHGPj_3M7kRj4MmSgyaVgqC4dgJIV-KpBIcBotyivjNlVI2FprLev3GKGNMmZRAzPOdw","publicationUid":237548813,"trackedDownloads":{"0046352e6a4733d593000000":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw34_56aba2472d207"},"id":"rgw34_56aba2472d207","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw35_56aba2472d207"},"id":"rgw35_56aba2472d207","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw36_56aba2472d207"},"id":"rgw36_56aba2472d207","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw37_56aba2472d207"},"id":"rgw37_56aba2472d207","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw38_56aba2472d207"},"id":"rgw38_56aba2472d207","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw33_56aba2472d207"},"id":"rgw33_56aba2472d207","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw31_56aba2472d207"},"id":"rgw31_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56aba2472d207"},"id":"rgw2_56aba2472d207","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":237548813},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=237548813&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56aba2472d207"},"id":"rgw1_56aba2472d207","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"kvF43ui9A0czC+6tEEy+hfkFcyfnRJLV3ueKFOp1GrJenSJZEYhCgTPqhXhV5q3bLZ2ihnRQaj8GBH6jrnPIF1oYz3IHn8CjqiozUWl5xvt8kp5S0c7TFzl41nG43LO6xajnlcWdvn3uwP2I\/gfKtPx4m+LbguyQdPhB5AISetq+K6ZV725QtngdSOAHxO+5DaVrb5q173Xq9xEYzObR5xFBU8YpgAo4zhoULonnL+7Ebq187TiDLBOaO3QJImmhQbtuK4w3qgLQXDWB4iZDZTSXEs\/OZ\/0PJc7NstLWcAw=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Choosing the Sample Size of a Computer Experiment: A Practical Guide\" \/>\n<meta property=\"og:description\" content=\"We produce reasons and evidence supporting the informal rule that the number of runs for an effective initial computer experiment should be about 10 times the input dimension. Our arguments...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide\/links\/0046352e6a4733d593000000\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide\" \/>\n<meta property=\"rg:id\" content=\"PB:237548813\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1198\/TECH.2009.08040\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Choosing the Sample Size of a Computer Experiment: A Practical Guide\" \/>\n<meta name=\"citation_author\" content=\"Jason L. Loeppky\" \/>\n<meta name=\"citation_author\" content=\"Jerome Sacks\" \/>\n<meta name=\"citation_author\" content=\"William J. Welch\" \/>\n<meta name=\"citation_publication_date\" content=\"2009\/11\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"Technometrics\" \/>\n<meta name=\"citation_issn\" content=\"1537-2723\" \/>\n<meta name=\"citation_volume\" content=\"51\" \/>\n<meta name=\"citation_issue\" content=\"4\" \/>\n<meta name=\"citation_firstpage\" content=\"366\" \/>\n<meta name=\"citation_lastpage\" content=\"376\" \/>\n<meta name=\"citation_doi\" content=\"10.1198\/TECH.2009.08040\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/William_Welch\/publication\/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide\/links\/0046352e6a4733d593000000.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-f12ec667-6176-443a-b952-faceb5b7e5dd","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":694,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw39_56aba2472d207"},"id":"rgw39_56aba2472d207","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-f12ec667-6176-443a-b952-faceb5b7e5dd", "3bec609d1c01d0cbd68b7c58afe8e7f48a0fb0f7");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-f12ec667-6176-443a-b952-faceb5b7e5dd", "3bec609d1c01d0cbd68b7c58afe8e7f48a0fb0f7");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw40_56aba2472d207"},"id":"rgw40_56aba2472d207","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/237548813_Choosing_the_Sample_Size_of_a_Computer_Experiment_A_Practical_Guide","requestToken":"4yFYe47FiRXmn1fiWCTWaHXFHLZAIbHL+MiGZfKnfWe\/AOTOAda8FC1FqaPzGd9HDoEUKgHrMDV3VSEuh8q1WDjSLV4YUmXS3Aq4gzj2DYtToURP6wbtomyN5n+bJDJ2gVMz\/C1qRS5aV9A4pWAElRzmcTXgYYwlQfrWxEhBVxAqR42O60l39jfRKbuAEnOOQqfAkV8LYvCWEPxIxq9D\/ZaEradbFPZkEid9G+emEVbYid836Vb\/oqcGR5mcJ29YHhtymqfSSxSirmf8z7Pe9P4vbBAz9a7x8OMpD83D65k=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=3X8kdAldWFIdN6BRPdbJ1urbFsb568jXBDnKrAddEIDX8KYx5nWPEtaFXtRHi58d","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjM3NTQ4ODEzX0Nob29zaW5nX3RoZV9TYW1wbGVfU2l6ZV9vZl9hX0NvbXB1dGVyX0V4cGVyaW1lbnRfQV9QcmFjdGljYWxfR3VpZGU%3D","signupCallToAction":"Join for free","widgetId":"rgw42_56aba2472d207"},"id":"rgw42_56aba2472d207","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw41_56aba2472d207"},"id":"rgw41_56aba2472d207","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw43_56aba2472d207"},"id":"rgw43_56aba2472d207","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
