<!DOCTYPE html> <html lang="en" class="" id="rgw37_56aba05bc3db3"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="zX6VNvV8yxTgR0z1rEJjSywgr2VCGC+BlRqpwT/QQJZwXWN0JG5hKujD4nsNL1dwsX3rV+lgHEJO0Ym30tiRkHd2u6pXxQLcTDclp5K6V3gY/YVS218nVoroUsnHWuMcF5T6izi0m2LZBliZAfOMTZUnuvNvMiYu0yOoOHdXxB9Q5Ypjy5cjXNNC7q3xkUGPdjrXoOnxPUlbIMw57j0IRWdGqp7QsINiCnagriYq8YcPSTKDdhhnEcAdzBbY4C/Kj/nPPSsXrYl8znjpoEpaHnDy//YtL6UIVJaaSch6img="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-0d2bb7ae-4ad6-4f06-a217-f504070437e4",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors" />
<meta property="og:description" content="Abstract It is well known in the statistics literature that augmenting binary and polychotomous response models with Gaussian latent variables enables exact Bayesian analysis via Gibbs sampling..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors/links/00b7d516d1ebfbe178000000/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors" />
<meta property="rg:id" content="PB:220499817" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1162/neco.2006.18.8.1790" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors" />
<meta name="citation_author" content="Mark Girolami" />
<meta name="citation_author" content="Simon Rogers" />
<meta name="citation_publication_date" content="2006/08/01" />
<meta name="citation_journal_title" content="Neural Computation" />
<meta name="citation_issn" content="0899-7667" />
<meta name="citation_volume" content="18" />
<meta name="citation_issue" content="8" />
<meta name="citation_firstpage" content="1790" />
<meta name="citation_lastpage" content="1817" />
<meta name="citation_doi" content="10.1162/neco.2006.18.8.1790" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Simon_Rogers/publication/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors/links/00b7d516d1ebfbe178000000.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56aba05bc3db3" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56aba05bc3db3" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56aba05bc3db3">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1162%2Fneco.2006.18.8.1790&rft.atitle=Variational%20Bayesian%20Multinomial%20Probit%20Regression%20with%20Gaussian%20Process%20Priors&rft.title=Neural%20Computation&rft.jtitle=Neural%20Computation&rft.volume=18&rft.issue=8&rft.date=2006&rft.pages=1790-1817&rft.issn=0899-7667&rft.au=Mark%20Girolami%2CSimon%20Rogers&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors</h1> <meta itemprop="headline" content="Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors/links/00b7d516d1ebfbe178000000/smallpreview.png">  <div id="rgw8_56aba05bc3db3" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56aba05bc3db3" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Mark_Girolami" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Mark Girolami" alt="Mark Girolami" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Mark Girolami</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw10_56aba05bc3db3" data-account-key="Mark_Girolami">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Mark_Girolami"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Mark Girolami" alt="Mark Girolami" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Mark_Girolami" class="display-name">Mark Girolami</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_College_London" title="University College London">University College London</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56aba05bc3db3" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Simon_Rogers" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A317062914740226%401452605097502_m/Simon_Rogers.png" title="Simon Rogers" alt="Simon Rogers" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Simon Rogers</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw12_56aba05bc3db3" data-account-key="Simon_Rogers">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Simon_Rogers"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A317062914740226%401452605097502_l/Simon_Rogers.png" title="Simon Rogers" alt="Simon Rogers" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Simon_Rogers" class="display-name">Simon Rogers</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_of_Glasgow" title="University of Glasgow">University of Glasgow</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">  <div> School of Computing Science, University of Glasgow, Glasgow, Scotland, United Kingdom </div>      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/0899-7667_Neural_Computation"><span itemprop="name">Neural Computation</span></a> </span>    (Impact Factor: 2.21).     <meta itemprop="datePublished" content="2006-08">  08/2006;  18(8):1790-1817.    DOI:&nbsp;10.1162/neco.2006.18.8.1790           <div class="pub-source"> Source: <a href="http://dblp.uni-trier.de/db/journals/neco/neco18.html#GirolamiR06" rel="nofollow">DBLP</a> </div>  </div> <div id="rgw13_56aba05bc3db3" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>Abstract It is well known in the statistics literature that augmenting binary and polychotomous response models with Gaussian latent variables enables exact Bayesian analysis via Gibbs sampling from the parameter posterior. By adopting such a data augmentation strategy, dispensing with priors over regression coecien ts in favour of Gaussian Process (GP) priors over functions, and employing variational approximations to the full posterior we obtain ecien t computational methods for Gaussian Process classication in the multi-class setting,. The model augmentation with additional latent variables ensures full a posteriori class coupling whilst retaining the simple a priori independent GP covariance structure from which sparse approximations, such as multi-class Informative Vector Machines (IVM), emerge in a very natural and straightforward manner. This is the rst time that a fully Variational Bayesian treatment for multi-class GP classication has been developed without having to resort to additional explicit approximations to the non-Gaussian likelihood term. Empirical comparisons with exact analysis via MCMC and Laplace approximations illustrate the utility of the variational approximation as a computationally economic alternative to full MCMC and it is shown to be more accurate than the Laplace approximation.</div> </p>  </div>   </div>      <div class="action-container"> <div id="rgw14_56aba05bc3db3" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw28_56aba05bc3db3">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw29_56aba05bc3db3">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Simon_Rogers/publication/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors/links/00b7d516d1ebfbe178000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Simon_Rogers">Simon Rogers</a>   </span>  </div>  <div class="social-share-container"><div id="rgw31_56aba05bc3db3" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw32_56aba05bc3db3" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw33_56aba05bc3db3" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw34_56aba05bc3db3" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw35_56aba05bc3db3" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw36_56aba05bc3db3" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw30_56aba05bc3db3" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FSimon_Rogers%2Fpublication%2F220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors%2Flinks%2F00b7d516d1ebfbe178000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw27_56aba05bc3db3"  itemprop="articleBody">  <p>Page 1</p> <p>Girolami, M. and Rogers, S. (2006) Variational Bayesian multinomial <br />probit regression with Gaussian process priors. Neural Computation <br />18(8):pp. 1790-1817.<br /> <br /> <br /> <br /> <br /> <br /> <br />http://eprints.gla.ac.uk/3813/ <br /> <br />Deposited on: 9 November 2007 <br /> <br /> <br />Glasgow ePrints Service <br />http://eprints.gla.ac.uk</p>  <p>Page 2</p> <p>Variational Bayesian Multinomial Probit<br />Regression with Gaussian Process Priors<br />Mark Girolami &amp; Simon Rogers<br />{girolami,srogers}@dcs.gla.ac.uk<br />Department of Computing Science<br />University of Glasgow<br />Technical Report: TR-2005-205<br />{girolami,srogers}@dcs.gla.ac.uk.<br />November 9, 2005<br />1</p>  <p>Page 3</p> <p>Abstract<br />It is well known in the statistics literature that augmenting binary<br />and polychotomous response models with Gaussian latent variables en-<br />ables exact Bayesian analysis via Gibbs sampling from the parameter<br />posterior. By adopting such a data augmentation strategy, dispensing<br />with priors over regression coefficients in favour of Gaussian Process<br />(GP) priors over functions, and employing variational approximations<br />to the full posterior we obtain efficient computational methods for<br />Gaussian Process classification in the multi-class setting1. The model<br />augmentation with additional latent variables ensures full a posteri-<br />ori class coupling whilst retaining the simple a priori independent<br />GP covariance structure from which sparse approximations, such as<br />multi-class Informative Vector Machines (IVM), emerge in a very nat-<br />ural and straightforward manner. This is the first time that a fully<br />Variational Bayesian treatment for multi-class GP classification has<br />been developed without having to resort to additional explicit approx-<br />imations to the non-Gaussian likelihood term. Empirical comparisons<br />with exact analysis via MCMC and Laplace approximations illustrate<br />the utility of the variational approximation as a computationally eco-<br />nomic alternative to full MCMC and it is shown to be more accurate<br />than the Laplace approximation.<br />1Introduction<br />In (Albert and Chib, 1993) it was first shown that by augmenting binary<br />and multinomial probit regression models with a set of continuous latent<br />variables yk, corresponding to the k’th response value where yk = mk+ ?,<br />? ∼ N(0,1) and mk =?<br />ple consider binary probit regression on target variables tn ∈ {0,1}, the<br />probit likelihood for the nth data sample taking unit value (tn = 1) is<br />P(tn= 1|xn,β) = Φ(βTxn), where Φ is the standardised Normal Cumula-<br />tive Distribution Function (CDF). Now, this can be obtained by the follow-<br />ing marginalisation?P(tn= 1,yn|xn,β)dyn=?P(tn= 1|yn)p(yn|xn,β)dyn<br />marginal is simply the normalizing constant of a left truncated univariate<br />jβkjxj, an exact Bayesian analysis can be per-<br />formed by Gibbs sampling from the parameter posterior.As an exam-<br />and as by definition P(tn= 1|yn) = δ(yn&gt; 0) then we see that the required<br />1Matlab code to allow replication of the reported results is available at http://www.<br />dcs.gla.ac.uk/people/personal/girolami/pubs 2005/VBGP/index.htm<br />2</p>  <p>Page 4</p> <p>Gaussian so that P(tn= 1|xn,β) =?δ(yn&gt; 0)Nyn(βTxn,1)dyn= Φ(βTxn).<br />1,yn|xn,β) = δ(yn &gt; 0)Nyn(βTxn,1) provides a straightforward means of<br />Gibbs sampling from the parameter posterior which would not be the case if<br />the marginal term, Φ(βTxn), was employed in defining the joint distribution<br />over data and parameters.<br />This data augmentation strategy can be adopted in developing efficient<br />methods to obtain binary and multi-class Gaussian Process (GP) (Williams<br />and Rasmussen, 1996) classifiers as will be presented in this paper. With the<br />exception of (Neal, 1998), where a full Markov Chain Monte Carlo (MCMC)<br />treatment to GP based classification is provided, all other approaches have<br />focussed on methods to approximate the problematic form of the poste-<br />rior2which allow analytic marginalisation to proceed. Laplace approxima-<br />tions to the posterior were developed in (Williams and Barber, 1998) whilst<br />lower &amp; upper bound quadratic likelihood approximations were considered<br />in (Gibbs, 2000). Variational approximations for binary classification were<br />developed in (Seeger, 2000) where a logit likelihood was considered and mean<br />field approximations were applied to probit likelihood terms in (Opper and<br />Winther, 2000), (Csato et al, 2000) respectively. Additionally, incremen-<br />tal (Quinonero-Candela and Winther, 2003) or sparse approximations based<br />on Assumed Density Filtering (ADF) (Csato and Opper, 2002), Informative<br />Vector Machines (IVM) (Lawrence, et al 2003) and Expectation Propagation<br />(EP) (Minka, 2001; Kim, 2005) have been proposed. With the exceptions of<br />(Williams and Barber, 1998; Gibbs, 2000; Seeger and Jordan, 2004; Kim,<br />2005) the focus of most recent work has largely been on the binary GP clas-<br />sification problem. In (Seeger and Jordan, 2004) a multi-class generalisation<br />of the IVM is developed where the authors employ a multinomial-logit soft-<br />max likelihood. However, considerable representational effort is required to<br />ensure that the scaling of computation and storage required of the proposed<br />method matches that of the original IVM with linear scaling in the number<br />of classes. In contrast, by adopting the probabilistic representation of (Al-<br />bert and Chib, 1993) we will see that GP based K-class classification and<br />efficient sparse approximations (IVM generalisations with scaling linear in<br />the number of classes) can be realised by optimising a strict lower-bound<br />of the marginal likelihood of a multinomial probit regression model which<br />The key observation here is that working with the joint distribution P(tn=<br />2The likelihood is nonlinear in the parameters due to either the logistic or probit link<br />functions required in the classification setting<br />3</p>  <p>Page 5</p> <p>requires the solution of K computationally independent GP regression prob-<br />lems whilst still operating jointly (statistically) on the data. We will also<br />show that the accuracy of this approximation is comparable to that obtained<br />via MCMC.<br />The following section now introduces the multinomial-probit regression<br />model with Gaussian Process priors.<br />2 Multinomial Probit Regression<br />Define the data matrix as X = [x1,··· ,xN]Twhich has dimension N × D<br />and the N ×1 dimensional vector of associated target values as t where each<br />element tn∈ {1,··· ,K}. The N × K matrix of GP random variables mnk<br />is denoted by M. We represent the N ×1 dimensional columns of M by mk<br />and the corresponding K × 1 dimensional rows by mn. The N × K matrix<br />of auxiliary variables ynkis represented as Y, where the N × 1 dimensional<br />columns are denoted by ykand the corresponding K×1 dimensional rows as<br />yn. The M × 1 vector of covariance kernel hyper-parameters for each class3<br />is denoted by ϕkand associated hyper-parameters ψk&amp; αk complete the<br />model.<br />The graphical representation of the conditional dependency structure in<br />the auxiliary variable multinomial probit regression model with GP priors in<br />the most general case is shown in Figure (1).<br />ψ<br />ϕ<br />m<br />y<br />t<br />MN<br />K<br />α<br />Figure 1: Graphical representation of the conditional dependencies within the<br />general multinomial probit regression model with Gaussian Process priors.<br />3This is the most general setting, however it is more common to employ a single and<br />shared GP covariance function across classes.<br />4</p>  <p>Page 6</p> <p>3 Prior Probabilities<br />From the graphical model in Figure (1) a priori we can assume class specific<br />GP independence and define model priors such that mk|X,ϕk∼ GP(ϕk) =<br />Nmk(0,Cϕk), where the matrix Cϕk, of dimension N × N defines the class<br />specific GP covariance4. Typical examples of such GP covariance functions<br />are radial basis style functions such that the i,j’th element of each Cϕkis<br />defined as exp{−?M<br />within the GP function prior, see for example (McKay, 2003).<br />As in (Albert and Chib, 1993) we employ a standardised normal noise<br />model such that the prior on the auxilliary variables is ynk|mnk∼ Nynk(mnk,1)<br />to ensure appropriate matching with the probit function. Of course rather<br />than having this variance fixed it could also be made an additional free pa-<br />rameter of the model and therefore would yield a scaled probit function. For<br />the presentation here we restrict ourselves to the standardised model and<br />consider extensions to a scaled probit model as possible further work. The<br />relationship between the additional latent variables yn(denoting the n’th row<br />of Y) and the targets tnas defined in multinomial probit regression (Albert<br />and Chib, 1993) is adopted here, i.e.<br />d=1ϕkd(xid− xjd)2} where in this case M = D, however<br />there are many other forms of covariance functions which may be employed<br />tn= j ifynj=<br />max<br />1≤k≤K{ynk}<br />(1)<br />This has the effect of dividing RK(y space) into K non-overlapping K-<br />dimensional cones Ck= {y : yk&gt; yi,k ?= i} where RK= ∪kCkand so each<br />P(tn= i|yn) can be represented as δ(yni&gt; ynk∀ k ?= i). We then see that<br />similar to the binary case where the probit function emerges from explicitly<br />marginalising the auxiliary variable the multinomial probit takes the form<br />given below, where details are given in Appendix I.<br />?<br />?<br />j=1<br />P(tn= i|mn) =δ(yni&gt; ynk∀ k ?= i)<br />K<br />?<br />j=1<br />p(ynj|mnj)dy<br />??<br />=<br />Ci<br />K<br />?<br />p(ynj|mnj)dy = Ep(u)<br />j?=i<br />Φ(u + mni− mnj)<br />?<br />4The model can be defined by employing K −1 GP functions and an alternative trun-<br />cation of the Gaussian over the variables ynkhowever for the multi-class case we define a<br />GP for each class.<br />5</p>  <p>Page 7</p> <p>where the random variable u is standardised normal i.e. p(u) = N(0,1).<br />An hierarchic prior on the covariance function hyper-parameters is employed<br />such that each hyper-parameter has, for example, an independent exponen-<br />tial distribution ϕkd∼ Exp(ψkd) and a gamma distribution is placed on the<br />mean values of the exponential ψkd ∼ Γ(σk,τk) thus forming a conjugate<br />pair. Of course, as detailed in (Girolami and Rogers, 2005), a more general<br />form of covariance function can be employed that will allow the integration<br />of heterogeneous types of data which takes the form of a weighted combi-<br />nation of base covariance functions. The associated hyper-hyper-parameters<br />α = {σk=1,···,K,τk=1,···,K} can be estimated via type-II maximum likelihood<br />or set to reflect some prior knowledge of the data.<br />priors can be employed such that, for example, each σk = τk = 10−6.<br />Defining the parameter set as Θ = {Y,M} and the hyper-parameters as<br />Φ = {ϕk=1,···,K,ψk=1,···,K} the joint likelihood takes the form below.<br />Alternatively, vague<br />p(t,Θ,Φ|X,α) =<br />N<br />?<br />K<br />?<br />n=1<br />?<br />K<br />?<br />i=1<br />δ(yni&gt; ynk∀ k ?= i)δ(tn= i)<br />?<br />×<br />k=1<br />p(ynk|mnk)p(mk|X,ϕk)p(ϕk|ψk)p(ψk|αk) (2)<br />4Gaussian Process Multi-Class Classification<br />We now consider both exact and approximate Bayesian inference for GP clas-<br />sification with multiple classes employing the multinomial-probit regression<br />model.<br />4.1Exact Bayesian Inference: The Gibbs Sampler<br />The representation of the joint likelihood (Equation 2) is particularly con-<br />venient in that samples can be drawn from the full posterior over the model<br />parameters (given the hyper-parameter values) p(Θ|t,X,Φ,α) using a Gibbs<br />sampler in a very straightforward manner with scaling per sample of O(KN3).<br />Full details of the Gibbs sampler are provided in Appendix IV and this sam-<br />pler will be employed in the experimental section.<br />6</p>  <p>Page 8</p> <p>4.2Approximate Bayesian Inference: The Laplace Ap-<br />proximation<br />The Laplace approximation of the posterior over GP variables, p(M|t,X,Φ,α)<br />(where Y is marginalised), requires finding the mode of the unnormalised pos-<br />terior. Approximate Bayesian inference for GP classification with multiple-<br />classes employing a multinomial-logit (softmax) likelihood has been devel-<br />oped previously in (Williams and Barber, 1998). Due to the form of the<br />multinomial-logit likelihood a Newton iteration to obtain the posterior mode<br />will scale at best as O(KN3). Employing the multinomial-probit likelihood<br />we find that each Newton step will scale as O(K3N3) and details are provided<br />in Appendix V.<br />4.3Approximate Bayesian Inference:<br />and Sparse Approximation<br />A Variational<br />Employing a variational Bayes approximation (Beal, 2003; Jordan, et al 1999;<br />McKay, 2003) by using an approximating ensemble of factored posteriors<br />such that p(Θ|t,X,Φ,α) ≈<br />probit regression is more appealing from a computational perspective as a<br />sparse representation, with scaling O(KNS2) (where S is the subset of sam-<br />ples entering the model and S ? N), can be obtained in a straightforward<br />manner as will be shown in the following sections. The lower bound5, see<br />for example (Beal, 2003; Jordan, et al 1999; McKay, 2003), on the marginal<br />likelihood logp(t|X,Φ,α) ≥ EQ(Θ){logp(t,Θ|X,Φ,α)}−EQ(Θ){logQ(Θ)}<br />is minimised by distributions which take an unnormalised form of Q(Θi) ∝<br />exp?EQ(Θ\Θi){logP(t,Θ|X,Φ,α)}?where Q(Θ\Θi) denotes the ensemble<br />posterior components are given in the Appendix.<br />The approximate posterior over the GP random variables takes a factored<br />form such that<br />?<br />i=1Q(Θi) = Q(Y)Q(M) for multinomial-<br />distribution with the ithcomponent of Θ removed. Details of the required<br />Q(M) =<br />K<br />?<br />k=1<br />Q(mk) =<br />K<br />?<br />k=1<br />Nmk(? mk,Σk)(3)<br />5The bound follows from the application of Jensens inequality e.g.<br />log?p(t,Θ|X)<br />logp(t|X) =<br />Q(Θ)Q(Θ)dΘ ≥?Q(Θ)logp(t,Θ|X)<br />Q(Θ)dΘ<br />7</p>  <p>Page 9</p> <p>where the shorthand tilde notation denotes posterior expectation i.e.?<br />Σk? ykwhere Σk= Cϕk(I + Cϕk)−1(see Appendix I for full details). We will<br />ensuring that the appropriate class-conditional posterior dependencies will be<br />induced in?<br />each of N data samples induces posterior dependencies between each of the<br />K columns of?<br />obtaining sparse approximations (Lawrence, et al 2003) for the multi-class<br />GP in particular.<br />Due to the multinomial probit definition of the dependency between each<br />element of ynand tn(Equation.1) the posterior for the auxiliary variables<br />follows as<br />N<br />?<br />where Ntn<br />that if tn= i where i ∈ {1,··· ,K} then the i’th dimension has the largest<br />value. The required posterior expectations ? ynkfor all k ?= i and ? ynifollow as<br />? ynk =<br />? yni =<br />where Φn,i,k<br />u<br />tions with respect to p(u) which appear in Equation (5) can be obtained by<br />quadrature or straightforward sampling methods.<br />If we also consider the set of hyper-parameters, Φ, in this variational<br />treatment then the approximate posterior for the covariance kernel hyper-<br />parameters takes the form of<br />f(a) =<br />EQ(a){f(a)} and so the required posterior mean for each k is given as ? mk=<br />see that each row, ? yn, of?Y will have posterior correlation structure induced<br />M. It should be stressed here that whilst there are K a posteriori<br />independent GP processes the associated K-dimensional posterior means for<br />M due to the posterior coupling over each of the auxiliary<br />variables yn. We will see that this structure is particularly convenient in<br />Q(Y) =<br />n=1<br />Q(yn) =<br />N<br />?<br />n=1<br />Ntn<br />yn(? mn,I) (4)<br />yn(? mn,I) denotes a conic truncation of a multivariate Gaussian such<br />?Nu(? mnk− ? mni,1)Φn,i,k<br />??<br />=?<br />? mnk−<br />? mni−<br />Ep(u)<br />u<br />?<br />Ep(u)<br />?<br />? ynj− ? mnj<br />Φ(u + ? mni− ? mnk)Φn,i,k<br />u<br />?<br />(5)<br />j?=i<br />?<br />(6)<br />j?=i,kΦ(u + ? mni− ? mnj), and p(u) = Nu(0,1). The expecta-<br />Q(ϕk) ∝ N<br />?mk(0,Cϕk)<br />?M<br />d=1Exp(ϕkd|?ψkd)<br />and the required posterior expectations can be estimated employing impor-<br />tance sampling. Expectations can be approximated by drawing S samples<br />8</p>  <p>Page 10</p> <p>such that each ϕs<br />kd∼ Exp(?ψkd) and so<br />?<br />?<br />f(ϕk) ≈<br />S<br />s=1<br />f(ϕs<br />k)w(ϕs<br />k) where w(ϕs<br />k) =<br />N<br />?mk<br />?0,Cϕs<br />k<br />?<br />?S<br />s?=1N<br />?mk<br />?<br />0,Cϕs?<br />k<br />?<br />(7)<br />This form of importance sampling within a variational Bayes procedure has<br />been employed previously in (Lawrence, et al 2004). Clearly the scaling of the<br />above estimator per sample is similar to that required in the gradient based<br />methods which search for optima of the marginal likelihood as employed in<br />GP regression and classification e.g. (McKay, 2003).<br />Finally we have that each Q(ψkd) = Γψkd(σk+ 1,τk+ ? ϕkd) and the asso-<br />4.4 Summarising Variational Multi-Class GP Classifi-<br />cation<br />ciated posterior mean is simply?ψkd= (σk+ 1)/(τk+ ? ϕkd).<br />We can summarise what has been presented by the following iterations which,<br />in the general case, for all k and d, will optimise the bound on the marginal<br />likelihood (explicit expressions for the bound are provided in Appendix III).<br />? mk<br />← C<br />←<br />?ϕk(I + C<br />?<br />σk+ 1<br />τk+ ? ϕkd<br />?ϕk)−1(? mk+ pk)(8)<br />? ϕk<br />?ψkd ←<br />s<br />ϕs<br />kw(ϕs<br />k)(9)<br />(10)<br />where each ϕs<br />kthcolumn of the N × K matrix P whose elements pnkare defined by the<br />rightmost terms in Equations (5 &amp; 6) i.e. for tn = i then for all k ?= i<br />pnk= −<br />u<br />These iterations can be viewed as obtaining K One against All binary<br />classifiers, however, most importantly they are not statistically independent<br />of each other but are a posteriori coupled via the posterior mean estimates of<br />each of the auxiliary variables yn. The computational scaling will be linear<br />in the number of classes and cubic in the number of data points O(KN3).<br />It is worth noting that if the covariance function hyper-parameters are fixed<br />then the costly matrix inversion only requires to be computed once. The<br />kd∼ Exp(?ψkd), w(ϕs<br />Ep(u){Nu(<br />Ep(u){Φ(u+<br />k) is defined as previously and pk is the<br />?mnk−<br />?mni,1)Φn,i,k<br />u<br />}<br />?mni−<br />?mnk)Φn,i,k<br />}and pni= −?<br />j?=ipnj.<br />9</p>  <p>Page 11</p> <p>Laplace approximation will require a matrix inversion for each Newton step<br />when finding the mode of the posterior (Williams and Barber, 1998).<br />4.4.1 Binary Classification<br />Previous variational treatments of GP based binary classification include<br />(Seeger, 2000; Opper and Winther, 2000; Gibbs, 2000; Csato and Opper,<br />2002; Csato et al, 2000). It is however interesting to note in passing that for<br />binary classification, the outer plate in Figure (1) is removed and further sim-<br />plification follows as only K −1 i.e. one set of posterior mean values requires<br />to be estimated and as such the posterior expectations ? m = C<br />now a product of truncated univariate Gaussians and as such the expectation<br />for the latent variables ynhas an exact analytic form. For a unit-variance<br />Gaussian truncated below zero if tn= 1 and above zero if tn= −1 the re-<br />quired posterior mean ? y has elements which can be obtained by the following<br />mean of a Gaussian due to truncation6? yn= ? mn+tnN<br />likelihood<br />? m ← C<br />?ϕ(I+C<br />?ϕ)−1? y<br />now operate on N × 1 dimensional vectors ? m and ? y. The posterior Q(y) is<br />analytic expression derived from straightforward results for corrections to the<br />?mn(0,1)/ Φ(tn? mn). So<br />the following iteration will guarantee an increase in the bound of the marginal<br />?ϕ(I + C<br />?ϕ)−1(? m + p)(11)<br />where each element of the N×1 vector p is defined as pn= tnN<br />?mn(0,1)/Φ(tn? mn).<br />4.5Variational Predictive Distributions<br />The predictive distribution, P(tnew= k|xnew,X,t)7, for a new sample xnew<br />follows from results for standard GP regression. The N × 1 vector Cnew<br />contains the covariance function values between the new point and those<br />contained in X, and cnew<br />denotes the covariance function value for the new<br />point and itself. So the GP posterior p(mnew|xnew,X,t) is a product of K<br />Gaussians each with mean and variance<br />?ϕk<br />?ϕk<br />? mnew<br />k<br />= ? yT<br />yNy(? m,1)/{1 − Φ(−? m)}dy = ? m + N<br />k(I + C<br />?ϕk)−1Cnew<br />?ϕk<br />?σ2<br />−∞yNy(? m,1)/Φ(−? m)dy = ? m − N<br />k,new<br />= cnew<br />?ϕk− (Cnew<br />?ϕk)T(I + C<br />?ϕk)−1Cnew<br />?ϕk<br />6For t = +1 then ? y =?+∞<br />0<br />?m(0,1)/Φ(? m) and for<br />t = −1 then ? y =?0<br />?m(0,1)/Φ(−? m).<br />7Conditioning on?Y, ? ϕ,?ψ, and α is implicit.<br />10</p>  <p>Page 12</p> <p>using the following shorthand ? νnew<br />target values as<br />k<br />=<br />?<br />1 +?σ2<br />k,newthen it is straightforward<br />(details in Appendix II) to obtain the predictive distribution over possible<br />P(tnew= k|xnew,X,t) = Ep(u)<br />??<br />j?=k<br />Φ<br />?<br />1<br />? νnew<br />j<br />?u? νnew<br />k<br />+ ? mnew<br />k<br />− ? mnew<br />j<br />???<br />where, as before, u ∼ Nu(0,1). The expectation can be obtained numerically<br />employing sample estimates from a standardised Gaussian. For the binary<br />case then the standard result follows<br />?<br />= 1 − Φ<br />P(tnew= 1|xnew,X,t) =δ(ynew&gt; 0)Nynew (? mnew,? νnew)dynew<br />? νnew<br />?<br />−? mnew<br />?<br />= Φ<br />?? mnew<br />? νnew<br />?<br />5Sparse Variational Multi-Class GP Classi-<br />fication<br />The dominant O(N3) scaling of the matrix inversion required in the posterior<br />mean updates in GP regression has been the motivation behind a large body<br />of literature focusing on reducing this cost via reduced rank approximations<br />(Williams and Seeger, 2001) and sparse online learning (Csato and Opper,<br />2002; Quinonero-Candela and Winther, 2003) where Assumed Density Fil-<br />tering (ADF) forms the basis of online learning and sparse approximations<br />for GP’s. Likewise in (Lawrence, et al 2003) the Informative Vector Ma-<br />chine (IVM) (refer to (Lawrence, et al 2005) for comprehensive details) is<br />proposed which employs informative point selection criteria (Seeger, et al<br />2003) and ADF updating of the approximations of the GP posterior para-<br />meters. Only binary classification has been considered in (Lawrence, et al<br />2003; Csato and Opper, 2002; Quinonero-Candela and Winther, 2003) and it<br />is clear from (Seeger and Jordan, 2004) that extension of ADF based approx-<br />imations such as IVM to the multi-class problem is not at all straightforward<br />when a multinomial-logit softmax likelihood is adopted. However, we now<br />see that sparse GP based classification for multiple classes (multi-class IVM)<br />emerges as a simple by-product of online ADF approximations to the para-<br />meters of each Q(mk) (multivariate Gaussian). The ADF approximations<br />11</p>  <p>Page 13</p> <p>when adding the nthdata sample, selected at the lthof S iterations, for each<br />of the K GP posteriors, Q(mk), follow simply from details in (Lawrence, et<br />al 2005) as given below.<br />Σk,n ← Cn<br />sk<br />← sk−<br />ϕk− MT<br />1 + skndiag?Σk,nΣT<br />1<br />√1 + sknΣT<br />? mk+? ynk− ? mnk<br />kMk,n<br />(12)<br />1<br />k,n<br />?<br />(13)<br />Ml<br />k<br />←<br />k,n<br />(14)<br />? mk<br />←<br />1 + skn<br />Σk,n<br />(15)<br />Each ? ynk− ? mnk= pnkas defined in Section (4.4) and can be obtained from<br />(5 &amp; 6), Σk,n, an N × 1 vector, is the nthcolumn of the current estimate<br />of each Σk, likewise Cn<br />All elements of each Mk and mk are initialised to zero whilst each sk has<br />initial unit values. Of course there is no requirement to explicitly store each<br />N × N dimensional matrix Σk, only the S × N matrices Mk and N × 1<br />vectors skrequire storage and maintenance. We denote indexing into the lth<br />row of each Mkby Ml<br />estimated posterior variance.<br />The efficient Cholesky factor updating as detailed in (Lawrence, et al<br />2005) will ensure that for N data samples, K distinct GP priors, and a<br />maximum of S samples included in the model where S &lt;&lt; N then at most<br />O(KSN) storage and O(KNS2) compute scaling will be realised.<br />As an alternative to the entropic scoring heuristic of (Seeger, et al 2003;<br />Lawrence, et al 2003) we suggest that an appropriate criterion for point<br />inclusion assessment will be the posterior predictive probability of a target<br />value given the current model parameters for points which are currently not<br />included in the model i.e. P (tm|xm,{mk},{Σk}), where the subscript m<br />indexes such points. From the results of the previous section this is equal to<br />Pr(ym∈ Ctm=k) which is expressed as<br />??<br />where k is the value of tm, νjm=?1 + sjm, and so the data point with the<br />the current stored approximate values of each ? mn1,··· , ? mnK via equations<br />ϕkis the nthcolumn of each GP covariance matrix.<br />k, and the nthelement of each skby sknwhich is the<br />Ep(u)<br />j?=k<br />Φ<br />?<br />1<br />νjm[uνkm+ ? mmk− ? mmj]<br />??<br />(16)<br />smallest posterior target probability should be selected for inclusion. This<br />12</p>  <p>Page 14</p> <p>scoring criterion requires no additional storage overhead as all ? mk and sk<br />the model in, at most, O(KN) time8. Intuitively points in regions of low<br />target posterior certainty, i.e. class boundaries, will be the most influential in<br />updating the approximation of the target posteriors. And so the inclusion of<br />points with the most uncertain target posteriors will yield the largest possible<br />translation of each updated mkinto the interior of their respective cones Ck.<br />Experiments in the following section will demonstrate the effectiveness of this<br />multi-class IVM.<br />are already available and it can be computed for all m not currently in<br />6 Experiments<br />6.1Illustrative Multi-Class Toy Example<br />Ten dimensional data vectors, x, were generated such that if t = 1 then<br />0.5 &gt; x2<br />[x1,x2]T∼ N(0,0.01I) where I denotes an identity matrix of appropriate<br />dimension. Finally x3,··· ,x10are all distributed as N(0,1). Both the first<br />two dimensions are required to define the three class labels with the remaining<br />eight dimensions being irrelevant to the classification task. Each of the three<br />target values were sampled uniformly thus creating a balance of samples<br />drawn from the three target classes.<br />Two hundred and forty draws were made from the above distribution<br />and the sample was used in the proposed variational inference routine with a<br />further 4620 points being used to compute a 0-1 loss class prediction error. A<br />common radial basis covariance function of the form exp{−?<br />the length-scale hyper-parameters ψ1,··· ,ψ10. The posterior expectations<br />of the auxiliary variables ? y were obtained from Equations 5 &amp; 6 where the<br />N(0,1). The variational importance sampler employed 500 samples drawn<br />from each Exp(?ψd) in estimating the corresponding posterior means ? ϕdfor<br />and ϕ had unit initial values. In this example the variational iterations ran<br />for fifty steps where each step corresponds to the sequential posterior mean<br />updates of Equation (8,9,10). The value of the variational lower-bound was<br />1+ x2<br />2&gt; 0.1, for t = 2 then 1.0 &gt; x2<br />1+ x2<br />2&gt; 0.6 and for t = 3 then<br />dϕd|xid−xjd|2}<br />was employed and vague hyper-parameters, σ = τ = 10−3were placed on<br />Gaussian integrals were computed using 1000 samples drawn from p(u) =<br />the covariance function parameters. Each M and Y were initialised randomly<br />8Assuming constant time to approximate the expectation.<br />13</p>  <p>Page 15</p> <p>monitored during each step and as would be expected a steady convergence<br />in the improvement of the bound can be observed in Figure (2.a).<br />0 10 2030 4050<br />−10<br />−9<br />−8<br />−7<br />−6<br />−5<br />−4<br />Iteration Number<br />(a)<br />LB on Marginal Likelihood × 10−2<br />(b)<br />0 10 20 304050<br />20<br />40<br />60<br />80<br />100<br />Iteration Number<br />% Predictions Correct<br />(c)<br />Figure 2: (a) Convergence of the Lower Bound on the Marginal-Likelihood<br />for the toy data set considered. (b) Evolution of estimated posterior means<br />for the inverse squared length scale parameters (precision parameters) in the<br />RBF covariance function, (c) Evolution of out-of-sample predictive perfor-<br />mance on the toy data set.<br />Likewise the development of the estimated posterior mean values for the<br />covariance function parameters ? ϕd, Figure (2.b), shows Automatic Relevance<br />are effectively removed from the model.<br />From Figure (2.c) we can see that the development of the predictive<br />performance (out of sample) follows that of the lower-bound (Figure 2.a)<br />achieving a predictive performance of 99.37% at convergence. As a compari-<br />son to our multi-class GP classifier we use a Directed Acyclic Graph (DAG)<br />SVM (Platt, et al 2000) (assuming equal class distributions the scaling9is<br />O(N3K−1)) on this example. Employing the values of the posterior mean<br />values of the covariance function length scale parameters (one for each of<br />the ten dimensions) estimated by the proposed variational procedure in the<br />RBF kernel of the DAG SVM a predictive performance of 99.33% is obtained.<br />So, on this dataset, the proposed GP classifier has comparable performance,<br />under 0-1 loss, to the DAG SVM. However the estimation of the covariance<br />function parameters is a natural part of the approximate Bayesian inference<br />routines employed in GP classification. There is no natural method of ob-<br />taining estimates of the ten kernel parameters for the SVM without resorting<br />Detection (ARD) in progress (Neal, 1998) where the eight irrelevant features<br />9This assumes the use of standard quadratic optimisation routines.<br />14</p>  <p>Page 16</p> <p>to cross-validation (CV), which in the case of a single parameter, is feasible<br />but rapidly becomes infeasible as the number of parameters increases.<br />6.2 Comparing Laplace &amp; Variational Approximations<br />to Exact Inference via Gibbs Sampling<br />This section provides a brief empirical comparison of the Variational approxi-<br />mation, developed in previous sections, to a full MCMC treatment employing<br />the Gibbs sampler detailed in Appendix IV. In addition, a Laplace approxi-<br />mation is also considered in this short comparative study.<br />Variational approximations provide a strict lower-bound on the marginal<br />likelihood and it is this bound which is one of the approximations attractive<br />characteristics. However it is less well understood how much parameters ob-<br />tained from such approximations differ from those obtained via exact meth-<br />ods. Preliminary analysis of the asymptotic properties of variational estima-<br />tors is provided in (Wang and Titterington, 2004). A recent experimental<br />study of EP and Laplace approximations to binary GP classifiers has been<br />undertaken by (Kuss and Rasmussen, 2005) and it is motivating to consider<br />a similar comparison for the variational approximation in the multiple-class<br />setting. In (Kuss and Rasmussen, 2005) it was observed that the marginal<br />and predictive likelihoods, computed over a wide range of covariance kernel<br />hyper-parameter values, were less well preserved by the Laplace approxi-<br />mation than the EP approximation when compared to that obtained via<br />MCMC. We then consider the predictive likelihood obtained via the Gibbs<br />sampler and compare this to the variational and Laplace approximations of<br />the GP-based classifiers.<br />The toy dataset from the previous section is employed and, as in (Kuss<br />and Rasmussen, 2005), a covariance kernel of the form sexp{−ϕ?<br />at each pair of hyper-parameter values a multinomial-probit GP classifier is<br />induced using (a) MCMC via the Gibbs sampler, (b) the proposed variational<br />approximation, (c) a Laplace approximation of the probit model. For the<br />Gibbs sampler, after a burn-in of 2000 samples, the following 1000 samples<br />were used for inference purposes and the predictive likelihood (probability<br />of target values in the test set) and test error (0-1 error loss) was estimated<br />from the 1000 post-burn-in samples as detailed in Appendix IV.<br />We firstly consider a binary classification problem by merging classes 2<br />d?xid−<br />xjd?2} is adopted. Both s &amp; ϕ are varied in the range (log scale) -1 to +5 and<br />15</p>  <p>Page 17</p> <p>&amp; 3 of the toy data set into one class. The first thing to note from Figure<br />(3) is that the predictive likelihood response under the variational approxi-<br />mation preserves, to a rather good degree, the predictive likelihood response<br />obtained when using Gibbs sampling across the range of hyper-parameter<br />values. However the Laplace approximation does not do as good a job in<br />replicating the levels of the response profile obtained via MCMC over the<br />range of hyper-parameter values considered and this finding is consistent<br />with the results of (Kuss and Rasmussen, 2005).<br />−40<br />−60<br />−80<br />−160<br />−20<br />−101<br />log(ϕ)<br />(a)<br />2345<br />−1<br />0<br />1<br />2<br />3<br />4<br />5<br />log(s)<br />−40<br />−60<br />−80<br />−160<br />−20<br />−101<br />log(ϕ)<br />(b)<br />2345<br />−1<br />0<br />1<br />2<br />3<br />4<br />5<br />log(s)<br />−40<br />−60<br />−80<br />−160<br />−101<br />log(ϕ)<br />(c)<br />2345<br />−1<br />0<br />1<br />2<br />3<br />4<br />5<br />log(s)<br />Figure 3: Isocontours of predictive likelihood for binary classification problem<br />(a) Gibbs Sampler, (b) Variational Approximation, (c) Laplace Approxima-<br />tion.<br />The Laplace approximation to the multinomial-probit model has O(K3N3)<br />scaling (Appendix V) which limits its application to situations where the<br />number of classes is small. For this reason, in the following experiments we<br />instead consider the multinomial-logit Laplace approximation (Williams and<br />Barber, 1998). In Figure (4) the isocontours of predictive likelihood for the<br />toy dataset in the multi-class setting under various hyper-parameter settings<br />are provided.<br />As with the binary case the variational multinomial-probit approximation<br />provides predictive likelihood response levels which are good representations<br />of those obtained from the Gibbs sampler. The Laplace approximation for<br />the multinomial-logit suffers from the same distortion of the contours as does<br />the Laplace approximation for the binary probit, in addition the information<br />in the predictions is lower. We note, as in (Kuss and Rasmussen, 2005), that<br />for s = 1 (logs = 0) the Laplace approximation compares reasonably with<br />16</p>  <p>Page 18</p> <p>−10<br />−20<br />−30<br />−40<br />2 −101<br />log(ϕ)<br />(a)<br />345<br />−1<br />0<br />1<br />2<br />3<br />4<br />5<br />log(s)<br />−20<br />−30<br />−40<br />−50<br />−10<br />01<br />log(ϕ)<br />(b)<br />2345<br />−1<br />0<br />1<br />2<br />3<br />4<br />5<br />log(s)<br />−30<br />−40<br />−50<br />−60<br />−101<br />log(ϕ)<br />(c)<br />2345<br />−1<br />0<br />1<br />2<br />3<br />4<br />5<br />log(s)<br />Figure 4: Isocontours of predictive likelihood for multi-class classification<br />problem (a) Gibbs Sampler, (b) Variational Approximation, (c) Laplace Ap-<br />proximation.<br />results from both MCMC and variational approximations.<br />In the following experiment four standard multi-class datasets (Iris, Thy-<br />roid, Wine and Forensic Glass) from the UCI Machine Learning Data Reposi-<br />tory10along with the toy data previously described are used. For each dataset<br />a random 60% training / 40% testing split was used to assess the performance<br />of each of the classification methods being considered and 50 random splits<br />of each data set were used. For the toy dataset 50 random train and test<br />sets were generated. The hyper-parameters, for an RBF covariance function<br />taking the form of exp{−?<br />the classification methods considered. The marginal likelihood for the Gibbs<br />sampler was estimated simply by using 1000 samples from the GP prior.<br />For each dataset and each method (multinomial-logit Laplace approxima-<br />tion, Variational approximation &amp; Gibbs sampler) the marginal likelihood<br />(lower-bound in the case of the variational approximation), predictive error<br />(0-1 loss) and predictive likelihood were measured. The results, given as the<br />mean and standard deviation over the 50 data splits, are listed in Table (6.2).<br />The predictive likelihood obtained from the multinomial logit Laplace<br />approximation is consistently, across all datasets, lower than that of the<br />Variational approximation and the Gibbs sampler. This indicates that the<br />dϕd?xid−xjd?2}, were estimated employing the<br />Variational importance sampler and these were then fixed and employed in all<br />10http://www.ics.uci.edu/∼mlearn/MPRepository.html<br />17</p>  <p>Page 19</p> <p>Toy-Data<br />Marginal Likelihood<br />Predictive Error<br />Predictive Likelihood<br />Iris<br />Marginal Likelihood<br />Predictive Error<br />Predictive Likelihood<br />Thyroid<br />Marginal Likelihood<br />Predictive Error<br />Predictive Likelihood<br />Wine<br />Marginal Likelihood<br />Predictive Error<br />Predictive Likelihood<br />Forensic Glass<br />Marginal Likelihood<br />Predictive Error<br />Predictive Likelihood<br />Laplace<br />-169.27 ± 4.27<br />3.97 ± 2.00<br />-98.90 ± 8.22<br />Laplace<br />-143.87 ± 1.04<br />3.88 ± 2.00<br />-10.43 ± 1.12<br />Laplace<br />-158.18 ± 1.94<br />4.73 ± 2.36<br />-19.01 ± 2.55<br />Laplace<br />-152.22 ± 1.29<br />2.95 ± 2.16<br />-14.57 ± 1.29<br />Laplace<br />-275.11 ± 2.87<br />36.54 ± 4.74<br />-90.38 ± 3.25<br />Variational<br />-232.00 ± 17.13<br />3.65 ± 1.95<br />-72.27 ± 9.25<br />Variational<br />-202.98 ± 1.37<br />4.08 ± 2.16<br />-7.35 ± 1.27<br />Variational<br />-246.24 ± 1.63<br />3.86 ± 2.04<br />-14.62 ± 2.70<br />Variational<br />-253.90 ± 1.52<br />2.65 ± 1.87<br />-10.16 ± 1.47<br />Variational<br />-776.79 ± 5.75<br />32.79 ± 4.57<br />-77.60 ± 3.91<br />Gibbs Sampler<br />-94.07 ± 11.26<br />3.49 ± 1.69<br />-73.44 ± 7.67<br />Gibbs Sampler<br />-45.27 ± 6.17<br />4.08 ± 2.16<br />-7.26 ± 1.40<br />Gibbs Sampler<br />-68.82 ± 8.29<br />3.94 ± 2.02<br />-14.47 ± 2.39<br />Gibbs Sampler<br />-68.65 ± 6.19<br />2.78 ± 2.07<br />-10.47 ± 1.41<br />Gibbs Sampler<br />-268.21 ± 5.46<br />34.00 ± 4.62<br />-79.86 ± 4.80<br />Table 1: Results of comparison of Gibbs sampler, Variational and Laplace<br />approximations when applied to several UCI datasets. Best results for Pre-<br />dictive likelihood are highlighted in bold.<br />predictions from the Laplace approximation are less informative about the<br />target values than both other methods considered. In addition the Varia-<br />tional approximation yields predictive distributions which are as informative<br />as those provided by the Gibbs sampler, however the 0-1 prediction errors<br />obtained across all methods do not differ as significantly. In (Kuss and Ras-<br />mussen, 2005) a similar observation was made for the binary GP classification<br />problem when Laplace and EP approximations were compared to MCMC. It<br />will then be interesting to further compare EP and Variational approxima-<br />tions in this setting.<br />We have observed that the predictions obtained from the variational ap-<br />18</p>  <p>Page 20</p> <p>proximation are in close agreement with those of MCMC whilst the Laplace<br />approximation suffers from some inaccuracy and this has also been reported<br />for the binary classification setting in (Kuss and Rasmussen, 2005).<br />6.3Multi-Class Sparse Approximation<br />A further 1000 samples were drawn from the toy data generating process<br />already described and these were used to illustrate the sparse GP multi-class<br />classifier in operation. The posterior mean values of the shared covariance<br />kernel parameters estimated in the previous example were employed here<br />and so the covariance kernel parameters were not estimated. The predictive<br />posterior scoring criterion proposed in Section (5) was employed in selecting<br />points for inclusion in the overall model. To assess how effective this criterion<br />is random sampling was also employed to compare the rates of convergence<br />of both inclusion strategies in terms of predictive 0-1 loss on a held out test<br />set of 2385 samples. A maximum of S = 50 samples were to be included in<br />the model defining a 95% sparsity level.<br />In Figure (5.a) the first two dimensions of the 1000 samples are plot-<br />ted with the three different target classes denoted by ×,+,• symbols. The<br />isocontours of constant target posterior probability at a level of 1/3 (the de-<br />cision boundaries) for each of the three classes are shown by the solid and<br />dashed lines. What is interesting is that the 50 included points (circled) all sit<br />close to, or on, the corresponding decision boundaries as would be expected<br />given the selection criteria proposed. These can be considered as a proba-<br />bilistic analogue to the support vectors of an SVM. The rates of 0-1 error<br />convergence using both random and informative point sampling are shown in<br />Figure (5.b). The procedure was repeated twenty times, using the same data<br />samples, and the error bars show one standard deviation over these repeats.<br />It is clear that, on this example at least, random sampling has the slowest<br />convergence, and the informative point inclusion strategy achieves less than<br />1% predictive error after the inclusion of only 30 data points. Of course we<br />should bridle our enthusiasm by recalling that the estimated covariance ker-<br />nel parameters are already supplied. Nevertheless, multi-class IVM makes<br />Bayesian GP inference on large scale problems with multiple classes feasible<br />as will be demonstrated in the following example.<br />19</p>  <p>Page 21</p> <p>−1−0.500.51<br />−1  <br />−0.5<br />0   <br />0.5<br />1   <br />(a)<br />01020304050<br />20<br />30<br />40<br />50<br />60<br />70<br />80<br />90<br />100<br />Number Points Included<br />(b)<br />Percentage Predictions Correct<br />Figure 5: (a) Scatter plot of the first two dimensions of the 1000 available data<br />sample. Each class is denoted by ×,+,• and the decision boundaries denoted<br />by the contours of target posterior probability equal to 1/3 are plotted in solid<br />and dashed line. The fifty points selected based on the proposed criterion<br />are circled and it is clear that these sit close to the decision boundaries. (b)<br />The averaged predictive performance (percentage predictions correct) over<br />twenty random starts (dashed line denotes random sampling and solid line<br />denotes informative sampling) are shown with the slowest converging plot<br />characterizing what is achieved under a random sampling strategy.<br />6.4Large Scale Example of Sparse GP Multi-Class<br />Classification<br />The Isolet11dataset comprises of 6238 examples of letters from the alphabet<br />(26) spoken in isolation by 30 individual speakers, and each letter is rep-<br />resented by 617 features. An independent collection of 1559 spoken letters<br />is available for classification test purposes. The best reported test perfor-<br />mance over all 26 classes of letter was 3.27% error achieved using 30-bit<br />error-correcting codes with an artificial neural network. Here we employ<br />a single RBF covariance kernel with a common inverse length-scale of 0.001<br />(further fine tuning is of course possible) and a maximum of 2000 points from<br />11The dataset is available from http://www.ics.uci.edu/∼mlearn/databases/<br />isolet<br />20</p>  <p>Page 22</p> <p>the available 6238 are to be employed in the sparse multi-class GP classifier.<br />As in the previous example data is standardized, both random and infor-<br />mative sampling strategies were employed, with the results given in Figure<br />(6) illustrating the superior convergence of an informative sampling strategy.<br />After including 2000 of the available 6238 samples in the model, under the<br />informative sampling strategy, a test error rate of 3.52% is achieved. We are<br />unaware of any multi-class GP classification method which has been applied<br />to such a large scale problem both in terms of data samples available and<br />the number of classes.<br />050010001500<br />−6000<br />−5000<br />−4000<br />−3000<br />−2000<br />−1000<br />0<br />Number of Points Included<br />(a)<br />Log Predictive Likelihood<br />05001000 1500<br />30<br />40<br />50<br />60<br />70<br />80<br />90<br />100<br />Number of Points Included <br />Percentage Predictions Correct<br />(b)<br />Figure 6: (a) The predictive likelihood computed on held-out data for both<br />random sampling (solid line with ’+’ markers) and informative sampling<br />(solid line with ’?’ markers). The predictive likelihood is computed once<br />every 50 inclusion steps. (b) The predictive performance (percentage predic-<br />tions correct) achieved for both random sampling (solid line with ’+’ markers)<br />and informative sampling (solid line with ’?’ markers)<br />A recent paper (Qi, et al 2004) has presented an empirical study of ARD<br />when employed to select basis functions in Relevance Vector Machine (RVM)<br />(Tipping, 2000) classifiers. It was observed that a reliance on the marginal<br />likelihood alone as a criterion for model identification ran the risk of overfit-<br />ting the available data sample by producing an overly sparse representation.<br />The authors then employ an approximation to the leave-one-out error, which<br />emerges from the EP iterations, to counteract this problem. For Bayesian<br />methods which rely on optimising in-sample marginal likelihood (or an appro-<br />priate bound) then great care has to be taken when setting the convergence<br />21</p>  <p>Page 23</p> <p>tolerance which determines when the optimisation routine should halt. How-<br />ever, in the experiments we have conducted this phenomenon did not appear<br />to be such a problem with the exception of one dataset as will be discussed<br />in the following section.<br />6.5Comparison with Multi-class SVM<br />To briefly compare the performance of the proposed approach to multi-class<br />classification with a number of multi-class SVM methods we consider the<br />recent study of (Duan and Keerthi, 2005). In that work four forms of multi-<br />class classifier were considered; WTAS - one-versus-all SVM method with<br />winner takes all class selection; MWVS - one-versus-one SVM with a maxi-<br />mum votes class selection strategy; PWCK - one-versus-one SVM with prob-<br />abilistic outputs employing pairwise coupling (see (Duan and Keerthi, 2005)<br />for details); PWCK - Kernel logistic regression with pairwise coupling of<br />binary outputs. Five multi-class datasets from the UCI Machine Learning<br />Data Repository were employed: ABE (16 dimensions &amp; 3 classes) - a subset<br />of the Letters dataset using the letters ’A’, ’B’ &amp; ’E’; DNA (180 dimensions<br />&amp; 3 classes); SAT (36 dimensions &amp; 6 classes) - Satellite Image; SEG (18<br />dimensions &amp; 7 classes) - Image Segmentation; WAV (21 dimensions &amp; 3<br />classes) - Waveform. For each of these, (Duan and Keerthi, 2005) created<br />twenty random partitions into training and test sets for three different sizes of<br />training set, ranging from small to large. Here we consider only the smallest<br />training set sizes.<br />In (Duan and Keerthi, 2005) thorough and extensive cross-validation was<br />employed to select the length-scale parameters (single) of the Gaussian kernel<br />and the associated regularisation parameters which were used in each of the<br />SVM’s. The proposed importance sampler is employed to obtain the poste-<br />rior mean estimates for both single and multiple length scales (VBGPS - Vari-<br />ational Bayes Gaussian Process Classification - Single length scale) (VBGPM<br />- Variational Bayes Gaussian Process Classification - Multiple length scales)<br />for a common GP covariance shared across all classes. We monitor the bound<br />on the marginal and consider convergence has been achieved when less than a<br />1% increase in the bound is observed for all datasets except for ABE where a<br />10% convergence criterion was employed due to a degree of overfitting being<br />observed after this point. In all experiments, data was standardised to have<br />zero mean and unit variance.<br />The percentage test errors averaged over each of the 20 data splits (mean<br />22</p>  <p>Page 24</p> <p>WTAS<br />9.4±0.5<br />10.2±1.3<br />1.9±0.8<br />17.2±1.4<br />11.1±0.6<br />MWVS<br />7.9±1.2<br />9.9±0.9<br />1.9±0.6<br />17.8±1.4<br />11.0±0.7<br />PWCP<br />7.9±1.2<br />8.9±0.8<br />1.8±0.6<br />16.4±1.4<br />10.9±0.4<br />PWCK<br />7.5±1.2<br />9.7±0.7<br />1.8±0.6<br />15.6±1.1<br />11.2±0.6<br />VBGPM<br />?7.8±1.5<br />74.0±0.3<br />?1.8±0.8<br />25.2±1.2<br />12.0±0.4<br />VBGPS<br />11.5±1.2<br />13.3±1.3<br />2.4±0.8<br />?15.6±0.7<br />12.1±0.4<br />SEG<br />DNA<br />ABE<br />WAV<br />SAT<br />Table 2: SVM &amp; Variational Bayes GP Multi-class Classification Comparison<br />± standard deviation) are reported in Table. 2. For each dataset the clas-<br />sifiers which obtained the lowest prediction error and whose performances<br />were indistinguishable from each other at the 1% significance level using a<br />paired t-test are highlighted in bold. An asterisk, ?, highlights the cases<br />where the proposed GP-based multi-class classifiers were part of the best<br />performing set. We see that in three of the five datasets performance equal<br />to the best performing SVM’s is achieved by one of the GP-based classifiers<br />without recourse to any cross-validation or in-sample tuning with comparable<br />performance being achieved for SAT &amp; DNA. The performance of VBGPM<br />is particularly poor on DNA and this is possibly due to the large number<br />(180) of binary features.<br />7Conclusion &amp; Discussion<br />The main novelty of this work has been to adopt the data augmentation strat-<br />egy employed in obtaining an exact Bayesian analysis of binary &amp; multino-<br />mial probit regression models for GP based multi-class (of which binary is a<br />specific case) classification. Whilst a full Gibbs sampler can be straightfor-<br />wardly obtained from the joint likelihood of the model, approximate inference<br />employing a factored form for the posterior is appealing from the point of<br />view of computational effort &amp; efficiency. The variational Bayes procedures<br />developed provide simple iterations due to the inherent decoupling effect of<br />the auxiliary variable between the GP components related to each class. The<br />scaling is still of course dominated by an O(N3) term due to the matrix in-<br />version required in obtaining the posterior mean for the GP variables and<br />the repeated computing of multivariate Gaussians required for the weights<br />in the importance sampler. However with the simple decoupled form of the<br />23</p>  <p>Page 25</p> <p>posterior updates we have shown that ADF based online and sparse estima-<br />tion yields a full multi-class IVM which has linear scaling in the number of<br />classes and the number of available data points and this is achieved in a most<br />straightfoward manner. An empirical comparison with full MCMC suggests<br />that the variational approximation proposed is superior to a Laplace approx-<br />imation. Further ongoing work includes an investigation into the possible<br />equivalences between EP and variational based approximate inference for<br />the multi-class GP classification problem as well as developing a variational<br />treatment to GP based ordinal regression (Chu and Ghahramani, 2005).<br />Acknowledgments<br />This work is supported by Engineering &amp; Physical Sciences Research Council<br />grants GR/R55184/02 &amp; EP/C010620/1. The authors are grateful to Chris<br />Williams, Jim Kay and Joaquin Qui˜ nonero Candela for motivating discus-<br />sions regarding this work. In addition the comments and suggestions made<br />by the anonymous reviewers helped to significantly improve the manuscript.<br />8 Appendix I<br />8.1<br />We employ the shorthand Q(ϕ) =?<br />Q(M) ∝ exp<br />∝ exp<br />?<br />and so we have<br />Q(M)<br />kQ(ϕk) in the following.<br />Consider the Q(M) component of the approximate posterior. We have<br />?<br />EQ(Y)Q(ϕ)<br />?<br />EQ(Y)Q(ϕ)<br />?<br />kN<br />??<br />n<br />?<br />klogNyk(mk,I) + logNmk(0|Cϕk)<br />??<br />K<br />?<br />and ? mk = Σk? yk. Now each element of C−1<br />24<br />klogp(ynk|mnk) + logp(mk|ϕk)<br />??<br />?? ??<br />∝<br />?yk(mk,I)Nmk<br />0,C−1<br />ϕk<br />?−1?<br />Q(M) =<br />K<br />?<br />k=1<br />Q(mk) =<br />k=1<br />Nmk(? mk,Σk)<br />where Σk =<br />a nonlinear function of ϕkand so, if considered appropriate, a first-order<br />?<br />I +?<br />C−1<br />ϕk<br />?−1<br />ϕkis</p>  <p>Page 26</p> <p>approximation can be made to the expectation of the matrix inverse such<br />that?<br />8.2Q(Y)<br />C−1<br />ϕk≈ C−1<br />?ϕkin which case Σk= C<br />?ϕk<br />?I + C<br />?ϕk<br />?−1.<br />Q(Y) ∝ exp<br />?<br />EQ(M)<br />??<br />nNyn(? mn,I)δ(yni&gt; ynk∀ k ?= i)δ(tn= i)<br />??<br />nlogp(tn|yn) + logp(yn|mn)<br />??<br />∝ exp<br />∝<br />nlogp(tn|yn) + logNyn(? mn|I)<br />?<br />?<br />Each ynis then distributed as a truncated multivariate Gaussian such that<br />for tn= i the ithdimension of ynis always the largest and so we have,<br />Q(Y) =<br />N<br />?<br />n=1<br />Q(yn) =<br />N<br />?<br />n=1<br />Ntn<br />yn(? mn,I)<br />where Ntn<br />dimension indicated by the value of tnis always the largest.<br />The posterior expectation of each ynis now required. Note that<br />yn(.,.) denotes a K-dimensional Gaussian truncated such that the<br />Q(yn) = Z−1<br />n<br />?<br />k<br />Nynk(? mnk,1)<br />where Zn= Pr(yn∈ C) and C = {yn: ynj&lt; yni,j ?= i}. Now<br />Zn = Pr(yn∈ C)<br />=<br />−∞<br />??<br />Where u is a standardised Gaussian random variable such that p(u) =<br />?+∞<br />Nyni(? mni,1)<br />Φ(u + ? mni− ? mnj)<br />?<br />j?=i<br />?yni<br />−∞<br />Nynj(? mnj,1)dynidynj<br />= Ep(u)<br />j?=i<br />?<br />25</p>  <p>Page 27</p> <p>Nu(0,1). For all k ?= i the posterior expectation follows as<br />?+∞<br />j=1<br />?+∞<br />?<br />? ynk = Z−1<br />= Z−1<br />n<br />−∞<br />ynk<br />K<br />?<br />Nynj(? mnj,1)dynj<br />ynkNynk(? mnk,1)<br />nEp(u)<br />Nu(? mnk− ? mni,1)<br />n<br />−∞<br />?yni<br />−∞<br />?<br />j?=i,k<br />Nyni(? mni,1)Φ(yni− ? mnj)dynidynk<br />?<br />=<br />? mnk− Z−1<br />j?=i,k<br />Φ(u + ? mni− ? mnj)<br />?<br />The required expectation for the ithcomponent follows as<br />? yni = Z−1<br />=<br />n<br />?+∞<br />−∞<br />yniNyni(? mni,1)<br />nEp(u)<br />?<br />j?=i<br />Φ(yni− ? mnj)dyni<br />Φ(u + ? mni− ? mnj)<br />? mni+ Z−1<br />? mni+<br />?<br />u<br />?<br />j?=i<br />?<br />=<br />?<br />k?=i<br />(? mnk− ? ynk)<br />The final expression in the above follows from noting that for a random<br />variable u ∼ N(0,1) and any differentiable function g(u) then E{ug(u)} =<br />E{g?(u)} in which case<br />?<br />j?=ik?=i<br />Ep(u)<br />u<br />?<br />Φ(u + ? mni− ? mnj)<br />Q(ϕk)<br />?<br />=<br />?<br />Ep(u)<br />?<br />Nu(? mnk− ? mni,1)<br />?<br />j?=i<br />Φ(u + ? mni− ? mnj)<br />?<br />8.3<br />For each k we obtain the posterior component<br />Q(ϕk) ∝ exp?EQ(mk)Q(ψk)(logp(mk|ϕk) + logp(ϕk|ψk))?<br />= ZkN<br />?mk(0|Cϕk)<br />?<br />d<br />Expϕkd(?ψkd)<br />where Zkis the corresponding normalising constant for each posterior which<br />is unobtainable in closed form. As such the required expectations can be<br />obtained by importance sampling.<br />26</p>  <p>Page 28</p> <p>8.4Q(ψk)<br />The final posterior component required is<br />Q(ψk) ∝ exp?EQ(ϕk)(logp(ϕk|ψk) + logp(ψk|αk))?<br />∝<br />d<br />?<br />and the required posterior mean values follow as?ψkd=<br />9 Appendix II<br />?<br />Exp<br />?ϕkd(ψkd)Γψkd(σk,τk)<br />=<br />d<br />Γψkd(σk+ 1,τk+ ? ϕkd)<br />σk+1<br />τk+<br />?ϕkd<br />The predictive distribution for a new point xnewcan be obtained by firstly<br />marginalising the associated GP random variables such that<br />?<br />K<br />?<br />K<br />?<br />?<br />the predictive posterior for the auxilliary variable ynewthe appropriate conic<br />truncation of this spherical Gaussian yields the required distribution P(tnew=<br />k|xnew,X,t) as follows. Using the following shorthand P(tnew= k|ynew) =<br />p(ynew|xnew,X,t) =p(ynew|mnew)p(mnew|xnew,X,t)dmnew<br />?<br />=<br />k=1<br />Nmnew<br />k<br />(ynew<br />k<br />,1)Nmnew<br />k<br />(? mnew<br />k<br />,? σnew<br />k<br />)dmnew<br />k<br />=<br />k=1<br />Nynew<br />k<br />(? mnew<br />k,newis employed. Now that we have<br />k<br />,? νnew<br />k<br />)<br />where the shorthand ? νnew<br />k<br />=<br />1 +?σ2<br />27</p>  <p>Page 29</p> <p>δ(ynew<br />k<br />&gt; ynew<br />i<br />∀ i ?= k)δ(tnew= k) ≡ δk,newthen<br />?<br />=<br />Ck<br />?<br />P(tnew= k|xnew,X,t) =P(tnew= k|ynew)p(ynew|xnew,X,t)dynew<br />?<br />p(ynew|xnew,X,t)dynew<br />?<br />??<br />=δk,new<br />K<br />k=1<br />Nynew<br />?<br />k<br />(? mnew<br />?u? νnew<br />k<br />,? νnew<br />k<br />)dynew<br />k<br />= Ep(u)<br />j?=k<br />Φ<br />1<br />? νnew<br />j<br />k<br />+ ? mnew<br />k<br />− ? mnew<br />j<br />???<br />This is the probability that the auxiliary variable ynewis in the cone Ckso<br />?K<br />=<br />k=1P(tnew= k|xnew,X,t) =<br />?K<br />?<br />k=1<br />?<br />Ck<br />p(ynew|xnew,X,t)dynew<br />RKp(ynew|xnew,X,t)dynew= 1<br />thus yielding a properly normalised posterior distribution over classes 1,··· K.<br />10Appendix III<br />The variational bound conditioned on the current values of ϕk,ψk,αk(i.e.<br />assuming these are fixed values) can be obtained in the following manner<br />using the expansion of the relevant components of the lower-bound.<br />?<br />?<br />?<br />?<br />expanding each component in turn obtains<br />?<br />k<br />?<br />EQ(M){logp(mk|X,ϕk)} −<br />n<br />EQ(M)Q(Y){logp(ynk|mnk)} +(17)<br />k<br />(18)<br />k<br />EQ(mk){logQ(mk)} −<br />(19)<br />n<br />EQ(yn){logQ(yn)}<br />(20)<br />−1<br />2<br />k<br />?<br />n<br />??y2nk+?<br />m2nk− 2? ynk? mnk<br />28<br />?<br />−NK<br />2<br />log2π(21)</p>  <p>Page 30</p> <p>−<br />1<br />2<br />?<br />?<br />−NK<br />k<br />log|Cϕk| −1<br />?<br />2<br />?<br />?<br />k<br />? mT<br />−NK<br />kC−1<br />ϕk? mk<br />log2π<br />−<br />1<br />2<br />k<br />traceC−1<br />ϕkΣk<br />2<br />(22)<br />−NK<br />22<br />log2π −1<br />2<br />?<br />k<br />log|Σk|<br />(23)<br />−1<br />2<br />?<br />k<br />?<br />n<br />??y2nk+ ? m2<br />nk− 2? ynk? mnk<br />?<br />−<br />?<br />n<br />logZn−N<br />2log2π(24)<br />Combining and manipulating (21,22,23, and 24) gives the following ex-<br />pression for the lower-bound.<br />−NK<br />?<br />1<br />2<br />2<br />log2π +N<br />2log2π +NK<br />?<br />?<br />2<br />−1<br />2<br />?<br />k<br />trace{Σk} −<br />?<br />logZn<br />1<br />2<br />k<br />? mT<br />log|Cϕk| +1<br />kC−1<br />ϕk? mk−1<br />2<br />k<br />trace<br />?<br />C−1<br />ϕkΣk<br />−<br />?<br />k<br />2<br />k<br />log|Σk| +<br />?<br />?<br />n<br />where each Zn= Ep(u)<br />??<br />j?=iΦ(u + ? mni− ? mnj).<br />11 Appendix IV<br />Details of the Gibbs sampler required to obtain samples from the posterior<br />p(Θ|t,X,Φ,α) now follow. From the definition of the joint likelihood (Equa-<br />tion 2) it is straightforward to see that the conditional distribution for each<br />yn| mnwill be a truncated Gaussian defined in the cone Ctn, centered at mn<br />with identity covariance and denoted by Ntn<br />each mk| ykis multivariate Gaussian with covariance Σk= Cϕk(I+Cϕk)−1<br />y(mn,I). The distribution for<br />29</p>  <p>Page 31</p> <p>and mean Σkyk. Thus the Gibbs sampler, for each n and k, takes the simple<br />form below<br />y(i)<br />m(i+1)<br />k<br />n| m(i−1)<br />n<br />∼ Ntn<br />∼ Nm(Σky(i)<br />y(m(i−1)<br />n<br />,I)<br />| y(i)<br />kk,Σk)<br />where the superscript (i) denotes the ithsample drawn. The dominant<br />scaling will be O(KN3) per sample draw.<br />likelihood for a new data point defined as<br />??<br />the predictive distribution12is then obtained from<br />?<br />A Monte-Carlo estimate of the above required marginal posterior expecta-<br />tion can be obtained by drawing samples from the full posterior distribu-<br />tion, p(Θ|t,X,Φ,α), using the above sampler. Then for each Θ(i)sam-<br />pled an additional set of samples mnew,s<br />k<br />mnew,s<br />k<br />| y(i)<br />and the associated variance is σ2<br />The approximate predictive distribution can then be obtained by the follow-<br />ing Monte-Carlo estimate<br />??<br />An additional Metropolis-Hastings sub-sampler can be employed within the<br />above Gibbs sampler to draw samples from the posterior p(Θ,Φ|t,X,α) if<br />the covariance function hyper-parameters are to be integrated out.<br />With the multinomial probit<br />P(tnew= k|mnew) = Ep(u)<br />j?=k<br />Φ(u + mnew<br />k<br />− mnew<br />j<br />)<br />?<br />P(tnew= k|xnew,X,t) =<br />P(tnew= k|mnew)p(mnew|xnew,X,t) dmnew<br />are drawn, such that for each k,<br />k,new), where µnew,i<br />k<br />= (y(i)<br />k,new= cnew<br />k<br />∼ Nm(µnew,i<br />k<br />,σ2<br />k)T(I + Cϕk)−1Cnew<br />ϕk)T(I + Cϕk)−1Cnew<br />ϕk<br />ϕk.<br />ϕk− (Cnew<br />1<br />Nsamps<br />Nsamps<br />?<br />s=1<br />Ep(u)<br />j?=k<br />Φ(u + mnew,s<br />k<br />− mnew,s<br />j<br />)<br />?<br />12 Appendix V<br />The Laplace approximation requires the Hessian matrix of second-order deriv-<br />atives of the joint log-likelihood with respect to each mn. The derivatives of<br />12Conditioning on Φ and α is implicit.<br />30</p>  <p>Page 32</p> <p>the noise component, logP(tn= k|mn) = logEp(u)<br />follow as below, where we denote expectation with respect to a Gaussian<br />truncated in the cone Ckas ENk<br />??<br />j?=kΦ(u + ? mnk− ? mnj)<br />?<br />,<br />y{·}<br />∂<br />∂mni<br />logP(tn= k|mn) =<br />1<br />P(tn= k|mn)<br />= ENk<br />?<br />Ck<br />(yni− mni)Nyn(m,I)dy<br />y{yni} − mni<br />and<br />∂2<br />∂mnj∂mnilogP(tn= k|mn) = ENk<br />y{yniynj} − ENk<br />y{yni}ENk<br />y{ynj} − δij<br />This then defines an NK × NK dimensional Hessian matrix which, unlike<br />the Hessian of the multinomial-logit counterpart, cannot be decomposed into<br />a diagonal plus multiplicative form (refer to (Williams and Barber, 1998) for<br />details), due to the cross-diagonal elements ENk<br />matrix inversions of the Newton step and those required to obtain the pre-<br />dictive covariance will operate on a full NK × NK matrix.<br />y{yniynj}, and so the required<br />References<br />Albert, J. H., and Chib, S. (1993). Bayesian analysis of binary and poly-<br />chotomous response data. Journal of the American Statistcial Association,<br />88(422):669–679.<br />Beal, M. (2003). Variational Algorithms for Approximate Bayesian Inference.<br />PhD thesis, University College London.<br />Chu, W., and Ghahramani, Z. (2005). Gaussian Processes for Ordinal Re-<br />gression. Journal of Machine Learning Research, 6:1019–1041.<br />Csato, L., Fokue, E., Opper, M., Schottky, B., and Winther, O. (2000).<br />Efficient Approaches to Gaussian Process Classification. In Advances in<br />Neural Information Processing Systems 12. Eds: Solla, S. A., Leen, T. K.,<br />and M¨ uller, K. R., pp 252–257.<br />Csato, L., and Opper, M. (2002). Sparse online gaussian processes. Neural<br />Computation, 14:641–668.<br />31</p>  <p>Page 33</p> <p>Duan, K., and Keerthi, S. (2005).<br />Method? An Empirical Study. Proceedings of the Sixth International<br />Workshop on Multiple Classifier Systems, 278–285.<br />Which is the Best Multi-class SVM<br />Gibbs, M. N., and MacKay, D. J. C. (2000). Variational gaussian process<br />classifiers. IEEE Transactions on Neural Networks, 11(6):1458 – 1464.<br />Girolami, M., and Rogers, S. (2005). Hierarchic Bayesian models for kernel<br />learning. Proceedings of the 22nd International Conference on Machine<br />Learning, pages 241–248.<br />Platt, J. C., Cristianini, N., and Shawe-Taylor, J. (2000). Large margin dags<br />for multi-class classification. In Advances in Neural Information Processing<br />Systems 12, pages 547–553.<br />Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., and Saul, L. K. (1999).<br />An introduction to variational methods for graphical models. Machine<br />Learning, 37:183–233.<br />Kim,<br />fiers.<br />http://home.postech.ac.kr/∼grass/publication/<br />Kuss, M., and Rasmussen, C. E. (2005) Assessing Approximate Inference<br />for Binary Gaussian Process Classification . Journal of Machine Learning<br />Research, 6:1679–1704.<br />H.C.(2005).Bayesian andEnsemble KernelClassi-<br />PhD thesis, Pohang University of Science and Technology.<br />Lawrence, N. D., Milo, M., Niranjan, M., Rashbass, P., and Soullier, S.<br />(2004). Reducing the variability in cDNA microarray image processing by<br />Bayesian inference. Bioinformatics, 20(4):518–526.<br />Lawrence, N. D., Seeger, M., and Herbrich, R. (2003). Fast sparse gaussian<br />process methods: The informative vector machine. In Thrun, S., Becker,<br />S., and Obermayer, K., editors, Advances in Neural Information Processing<br />Systems 15. MIT Press.<br />Lawrence, N. D, Platt, J. C., and Jordan, M. I. (2005). Extensions of the<br />informative vector machine. In Winkler, J., Lawrence, N. D., and Niran-<br />jan, M., (eds), Proceedings of the Sheffield Machine Learning Workshop,<br />Springer-Verlag, Berlin.<br />32</p>  <p>Page 34</p> <p>MacKay, D. J. C (2003). Information Theory, Inference, and Learning Al-<br />gorithms. Cambridge University Press.<br />Minka, T. P. (2001). A family of algorithms for approximate Bayesian infer-<br />ence. PhD thesis, MIT.<br />Neal, R. (1998). Regression and classification using gaussian process priors.<br />In Dawid, A. P., Bernardo, M., Berger, J. O., and Smith, A. F. M., editors,<br />Bayesian Statistics 6, pages 475–501. Oxford University Press.<br />Opper, M., and Winther, O. (2000). Gaussian processes for classification:<br />Mean field algorithms. Neural Computation, 12:2655–2684.<br />Qi, Y., Minka, T. P., Picard, R. W., and Ghahramani, Z. (2004). Predic-<br />tive automatic relevance determination by expectation propagation. In<br />Greiner, R., and Schuurmans, D., editors, Proceedings of the twenty-first<br />International Conference on Machine Learning.<br />Quinonero-Candela, J., and Winther, O. (2003).<br />processes. In Becker, S., Thrun, S., and Obermayer, K., editors, Neural<br />Information Processing Systems 15. MIT Press.<br />Incremental gaussian<br />Seeger, M., Williams, C. K. I., and Lawrence, N. D. (2003). Fast forward<br />selection to speed up sparse gaussian process regression. In Bishop, C. M.,<br />and Frey, B. J., editors, Proceedings of the Ninth International Workshop<br />on Artificial Intelligence and Statistics.<br />Seeger, M., and Jordan, M. I. (2004). Sparse Gaussian Process Classification<br />With Multiple Classes. Department of Statistics, Technical Report 661,<br />University of California, Berkeley.<br />Seeger, M. (2000). Bayesian Model Selection for Support Vector Machines,<br />Gaussian Processes and Other Kernel Classifiers.<br />Processing Systems, 12, 603-609.<br />Neural Information<br />Tipping, M. (2000). Sparse Bayesian learning and the relevance vector ma-<br />chine. Journal of Machine Learning Research, 1:211–244.<br />Wang, B., and Titterington, D. M. (2004). Convergence and asymptotic<br />normality of variational Bayesian approximations for exponential family<br />models with missing values. Technical Report, No.04-02, Department of<br />Statistics, University of Glasgow.<br />33</p>  <p>Page 35</p> <p>Williams, C. K. I, and Barber, D. (1998).<br />gaussian processes. IEEE Transactions on Pattern Analysis and Machine<br />Intelligence, 20(12):1342–1351.<br />Bayesian classification with<br />Williams, C. K. I., and Rasmussen, C. E. (1996). Gaussian processes for<br />regression. In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E.,<br />editors, Advances in Neural Processing Systems 8, pages 598–604. MIT<br />Press, Cambridge, MA.<br />Williams, C. K. I, and Seeger, M. (2001). Using the Nystrom method to<br />speed up kernel machines. In Leen, T. K., Dietterich, T. G., and Tresp,<br />V., editors, Advances in Neural Information Processing Systems 13, pages<br />682-688, Cambridge, MA, MIT Press.<br />34</p>  <a href="https://www.researchgate.net/profile/Simon_Rogers/publication/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors/links/00b7d516d1ebfbe178000000.pdf">Download full-text</a> </div> <div id="rgw19_56aba05bc3db3" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw20_56aba05bc3db3">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw21_56aba05bc3db3"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Simon_Rogers/publication/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors/links/00b7d516d1ebfbe178000000.pdf" class="publication-viewer" title="download.pdf">download.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Simon_Rogers">Simon Rogers</a> &middot; Jan 20, 2016 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw22_56aba05bc3db3"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.7451&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors">Variational Bayesian Multinomial Probit Regression...</a> </div>  <div class="details">   Available from <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.7451&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow">psu.edu</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw24_56aba05bc3db3" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw25_56aba05bc3db3">  </ul> </div> </div>   <div id="rgw15_56aba05bc3db3" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw16_56aba05bc3db3"> <div> <h5> <a href="publication/285216515_Genetic_structure_and_molecular_divergence_among_samples_of_mandacaru_Cereus_spp_Cactaceae_as_revealed_by_microsatellite_markers" class="color-inherit ga-similar-publication-title"><span class="publication-title">Genetic structure and molecular divergence among samples of mandacaru (Cereus spp.; Cactaceae) as revealed by microsatellite markers</span></a>  </h5>  <div class="authors"> <a href="researcher/2086465184_Vanessa_Neves_de_Azevedo_Fernandes" class="authors ga-similar-publication-author">Vanessa Neves de Azevedo Fernandes</a>, <a href="researcher/2086478337_Andrea_Florindo_das_Neves" class="authors ga-similar-publication-author">Andréa Florindo das Neves</a>, <a href="researcher/2032902404_Paula_Garcia_Martin" class="authors ga-similar-publication-author">Paula Garcia Martin</a>, <a href="researcher/13979679_Claudete_Aparecida_Mangolin" class="authors ga-similar-publication-author">Claudete Aparecida Mangolin</a>, <a href="researcher/74264042_Maria_de_Fatima_PS_Machado" class="authors ga-similar-publication-author">Maria de Fátima P.S. Machado</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56aba05bc3db3"> <div> <h5> <a href="publication/284787272_Complete_mitochondrial_DNA_genome_of_bonnethead_shark_Sphyrna_tiburo_and_phylogenetic_relationships_among_main_superorders_of_modern_elasmobranchs" class="color-inherit ga-similar-publication-title"><span class="publication-title">Complete mitochondrial DNA genome of bonnethead shark, Sphyrna tiburo, and phylogenetic relationships among main superorders of modern elasmobranchs</span></a>  </h5>  <div class="authors"> <a href="researcher/989069_Pindaro_Diaz-Jaimes" class="authors ga-similar-publication-author">Píndaro Díaz-Jaimes</a>, <a href="researcher/2042093981_Natalia_J_Bayona-Vasquez" class="authors ga-similar-publication-author">Natalia J. Bayona-Vásquez</a>, <a href="researcher/719595_Douglas_H_Adams" class="authors ga-similar-publication-author">Douglas H. Adams</a>, <a href="researcher/47894951_Manuel_Uribe-Alcocer" class="authors ga-similar-publication-author">Manuel Uribe-Alcocer</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56aba05bc3db3"> <div> <h5> <a href="publication/290471649_Bayesian_analysis_of_a_linear_model_involving_structural_changes_in_either_regression_parameters_or_disturbances_precision" class="color-inherit ga-similar-publication-title"><span class="publication-title">Bayesian analysis of a linear model involving structural changes in either regression parameters or disturbances precision</span></a>  </h5>  <div class="authors"> <a href="researcher/69959053_Anoop_Chaturvedi" class="authors ga-similar-publication-author">Anoop Chaturvedi</a>, <a href="researcher/2085189366_Arvind_Shrivastava" class="authors ga-similar-publication-author">Arvind Shrivastava</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw38_56aba05bc3db3" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw39_56aba05bc3db3">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw40_56aba05bc3db3" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=c6xQRGkMWIoTmcvLQFCcF-4OHnikRvcw4O_avNfwH_Ebo7Oy7v3qu6Up33F_bWwz" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="VQQJBS07hZYEjxo4ejjwLkJD6t73vr1HWHsuKPmYDlDmeQNRa2zcvXZ88jZ+9IPWYYMIUpGtQ1InS/r2OqKgnjrkvAerRWHMeU/5KwTyM4gvLCD+Gqc9uhblVzaaQ7zN7Xvcc8LiRK6qhg0+bNsvd9N6+GGocaNxpNxLt98uo2KOa9PIqw3BCIHeBBUOFNrdm+/Xm5nFLqKBpOi2S8eggQaia9w/TOoEmSnALrYfJPyyd48LjRCVA9YO7lai/0KrLL7KEuvt6cAg/tAPqL1kKiTK/lzWGw8Rmnp285oPJiU="/> <input type="hidden" name="urlAfterLogin" value="publication/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjIwNDk5ODE3X1ZhcmlhdGlvbmFsX0JheWVzaWFuX011bHRpbm9taWFsX1Byb2JpdF9SZWdyZXNzaW9uX3dpdGhfR2F1c3NpYW5fUHJvY2Vzc19QcmlvcnM%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjIwNDk5ODE3X1ZhcmlhdGlvbmFsX0JheWVzaWFuX011bHRpbm9taWFsX1Byb2JpdF9SZWdyZXNzaW9uX3dpdGhfR2F1c3NpYW5fUHJvY2Vzc19QcmlvcnM%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjIwNDk5ODE3X1ZhcmlhdGlvbmFsX0JheWVzaWFuX011bHRpbm9taWFsX1Byb2JpdF9SZWdyZXNzaW9uX3dpdGhfR2F1c3NpYW5fUHJvY2Vzc19QcmlvcnM%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw41_56aba05bc3db3"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 452;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Mark Girolami","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Mark_Girolami","institution":"University College London","institutionUrl":false,"widgetId":"rgw4_56aba05bc3db3"},"id":"rgw4_56aba05bc3db3","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=1816679","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56aba05bc3db3"},"id":"rgw3_56aba05bc3db3","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=220499817","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":220499817,"title":"Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors","journalTitle":"Neural Computation","journalDetailsTooltip":{"data":{"journalTitle":"Neural Computation","journalAbbrev":"NEURAL COMPUT","publisher":"Massachusetts Institute of Technology Press (MIT Press)","issn":"0899-7667","impactFactor":"2.21","fiveYearImpactFactor":"2.20","citedHalfLife":">10.0","immediacyIndex":"0.57","eigenFactor":"0.01","articleInfluence":"0.96","widgetId":"rgw6_56aba05bc3db3"},"id":"rgw6_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=0899-7667","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":"School of Computing Science, University of Glasgow, Glasgow, Scotland, United Kingdom","type":"Article","details":{"doi":"10.1162\/neco.2006.18.8.1790","journalInfos":{"journal":"","publicationDate":"08\/2006;","publicationDateRobot":"2006-08","article":"18(8):1790-1817.","journalTitle":"Neural Computation","journalUrl":"journal\/0899-7667_Neural_Computation","impactFactor":2.21}},"source":{"sourceUrl":"http:\/\/dblp.uni-trier.de\/db\/journals\/neco\/neco18.html#GirolamiR06","sourceName":"DBLP"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1162\/neco.2006.18.8.1790"},{"key":"rft.atitle","value":"Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors"},{"key":"rft.title","value":"Neural Computation"},{"key":"rft.jtitle","value":"Neural Computation"},{"key":"rft.volume","value":"18"},{"key":"rft.issue","value":"8"},{"key":"rft.date","value":"2006"},{"key":"rft.pages","value":"1790-1817"},{"key":"rft.issn","value":"0899-7667"},{"key":"rft.au","value":"Mark Girolami,Simon Rogers"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56aba05bc3db3"},"id":"rgw7_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=220499817","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":220499817,"peopleItems":[{"data":{"authorNameOnPublication":"Mark Girolami","accountUrl":"profile\/Mark_Girolami","accountKey":"Mark_Girolami","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Mark Girolami","profile":{"professionalInstitution":{"professionalInstitutionName":"University College London","professionalInstitutionUrl":"institution\/University_College_London"}},"professionalInstitutionName":"University College London","professionalInstitutionUrl":"institution\/University_College_London","url":"profile\/Mark_Girolami","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Mark_Girolami","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw10_56aba05bc3db3"},"id":"rgw10_56aba05bc3db3","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1816679&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University College London","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":2,"accountCount":2,"publicationUid":220499817,"widgetId":"rgw9_56aba05bc3db3"},"id":"rgw9_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1816679&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=2&accountCount=2&publicationUid=220499817","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Simon Rogers","accountUrl":"profile\/Simon_Rogers","accountKey":"Simon_Rogers","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A317062914740226%401452605097502_m\/Simon_Rogers.png","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Simon Rogers","profile":{"professionalInstitution":{"professionalInstitutionName":"University of Glasgow","professionalInstitutionUrl":"institution\/University_of_Glasgow"}},"professionalInstitutionName":"University of Glasgow","professionalInstitutionUrl":"institution\/University_of_Glasgow","url":"profile\/Simon_Rogers","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A317062914740226%401452605097502_l\/Simon_Rogers.png","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Simon_Rogers","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw12_56aba05bc3db3"},"id":"rgw12_56aba05bc3db3","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1635064&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University of Glasgow","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":2,"accountCount":2,"publicationUid":220499817,"widgetId":"rgw11_56aba05bc3db3"},"id":"rgw11_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1635064&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=2&accountCount=2&publicationUid=220499817","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56aba05bc3db3"},"id":"rgw8_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=220499817&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":220499817,"abstract":"<noscript><\/noscript><div>Abstract It is well known in the statistics literature that augmenting binary and polychotomous response models with Gaussian latent variables enables exact Bayesian analysis via Gibbs sampling from the parameter posterior. By adopting such a data augmentation strategy, dispensing with priors over regression coecien ts in favour of Gaussian Process (GP) priors over functions, and employing variational approximations to the full posterior we obtain ecien t computational methods for Gaussian Process classication in the multi-class setting,. The model augmentation with additional latent variables ensures full a posteriori class coupling whilst retaining the simple a priori independent GP covariance structure from which sparse approximations, such as multi-class Informative Vector Machines (IVM), emerge in a very natural and straightforward manner. This is the rst time that a fully Variational Bayesian treatment for multi-class GP classication has been developed without having to resort to additional explicit approximations to the non-Gaussian likelihood term. Empirical comparisons with exact analysis via MCMC and Laplace approximations illustrate the utility of the variational approximation as a computationally economic alternative to full MCMC and it is shown to be more accurate than the Laplace approximation.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw13_56aba05bc3db3"},"id":"rgw13_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=220499817","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors\/links\/00b7d516d1ebfbe178000000\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw14_56aba05bc3db3"},"id":"rgw14_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56aba05bc3db3"},"id":"rgw5_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=220499817&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2086465184,"url":"researcher\/2086465184_Vanessa_Neves_de_Azevedo_Fernandes","fullname":"Vanessa Neves de Azevedo Fernandes","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2086478337,"url":"researcher\/2086478337_Andrea_Florindo_das_Neves","fullname":"Andr\u00e9a Florindo das Neves","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2032902404,"url":"researcher\/2032902404_Paula_Garcia_Martin","fullname":"Paula Garcia Martin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":13979679,"url":"researcher\/13979679_Claudete_Aparecida_Mangolin","fullname":"Claudete Aparecida Mangolin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2016","journal":"Biochemical Systematics and Ecology","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/285216515_Genetic_structure_and_molecular_divergence_among_samples_of_mandacaru_Cereus_spp_Cactaceae_as_revealed_by_microsatellite_markers","usePlainButton":true,"publicationUid":285216515,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"0.97","url":"publication\/285216515_Genetic_structure_and_molecular_divergence_among_samples_of_mandacaru_Cereus_spp_Cactaceae_as_revealed_by_microsatellite_markers","title":"Genetic structure and molecular divergence among samples of mandacaru (Cereus spp.; Cactaceae) as revealed by microsatellite markers","displayTitleAsLink":true,"authors":[{"id":2086465184,"url":"researcher\/2086465184_Vanessa_Neves_de_Azevedo_Fernandes","fullname":"Vanessa Neves de Azevedo Fernandes","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2086478337,"url":"researcher\/2086478337_Andrea_Florindo_das_Neves","fullname":"Andr\u00e9a Florindo das Neves","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2032902404,"url":"researcher\/2032902404_Paula_Garcia_Martin","fullname":"Paula Garcia Martin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":13979679,"url":"researcher\/13979679_Claudete_Aparecida_Mangolin","fullname":"Claudete Aparecida Mangolin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":74264042,"url":"researcher\/74264042_Maria_de_Fatima_PS_Machado","fullname":"Maria de F\u00e1tima P.S. Machado","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Biochemical Systematics and Ecology 02\/2016; 64:38-45. DOI:10.1016\/j.bse.2015.11.003"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/285216515_Genetic_structure_and_molecular_divergence_among_samples_of_mandacaru_Cereus_spp_Cactaceae_as_revealed_by_microsatellite_markers","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/285216515_Genetic_structure_and_molecular_divergence_among_samples_of_mandacaru_Cereus_spp_Cactaceae_as_revealed_by_microsatellite_markers\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56aba05bc3db3"},"id":"rgw16_56aba05bc3db3","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=285216515","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":989069,"url":"researcher\/989069_Pindaro_Diaz-Jaimes","fullname":"P\u00edndaro D\u00edaz-Jaimes","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2042093981,"url":"researcher\/2042093981_Natalia_J_Bayona-Vasquez","fullname":"Natalia J. Bayona-V\u00e1squez","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":719595,"url":"researcher\/719595_Douglas_H_Adams","fullname":"Douglas H. Adams","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":47894951,"url":"researcher\/47894951_Manuel_Uribe-Alcocer","fullname":"Manuel Uribe-Alcocer","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2016","journal":"Meta Gene","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/284787272_Complete_mitochondrial_DNA_genome_of_bonnethead_shark_Sphyrna_tiburo_and_phylogenetic_relationships_among_main_superorders_of_modern_elasmobranchs","usePlainButton":true,"publicationUid":284787272,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/284787272_Complete_mitochondrial_DNA_genome_of_bonnethead_shark_Sphyrna_tiburo_and_phylogenetic_relationships_among_main_superorders_of_modern_elasmobranchs","title":"Complete mitochondrial DNA genome of bonnethead shark, Sphyrna tiburo, and phylogenetic relationships among main superorders of modern elasmobranchs","displayTitleAsLink":true,"authors":[{"id":989069,"url":"researcher\/989069_Pindaro_Diaz-Jaimes","fullname":"P\u00edndaro D\u00edaz-Jaimes","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2042093981,"url":"researcher\/2042093981_Natalia_J_Bayona-Vasquez","fullname":"Natalia J. Bayona-V\u00e1squez","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":719595,"url":"researcher\/719595_Douglas_H_Adams","fullname":"Douglas H. Adams","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":47894951,"url":"researcher\/47894951_Manuel_Uribe-Alcocer","fullname":"Manuel Uribe-Alcocer","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Meta Gene 02\/2016; 7:48-55. DOI:10.1016\/j.mgene.2015.11.005"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/284787272_Complete_mitochondrial_DNA_genome_of_bonnethead_shark_Sphyrna_tiburo_and_phylogenetic_relationships_among_main_superorders_of_modern_elasmobranchs","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/284787272_Complete_mitochondrial_DNA_genome_of_bonnethead_shark_Sphyrna_tiburo_and_phylogenetic_relationships_among_main_superorders_of_modern_elasmobranchs\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56aba05bc3db3"},"id":"rgw17_56aba05bc3db3","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=284787272","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":69959053,"url":"researcher\/69959053_Anoop_Chaturvedi","fullname":"Anoop Chaturvedi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2085189366,"url":"researcher\/2085189366_Arvind_Shrivastava","fullname":"Arvind Shrivastava","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Communication in Statistics- Theory and Methods","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/290471649_Bayesian_analysis_of_a_linear_model_involving_structural_changes_in_either_regression_parameters_or_disturbances_precision","usePlainButton":true,"publicationUid":290471649,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"0.27","url":"publication\/290471649_Bayesian_analysis_of_a_linear_model_involving_structural_changes_in_either_regression_parameters_or_disturbances_precision","title":"Bayesian analysis of a linear model involving structural changes in either regression parameters or disturbances precision","displayTitleAsLink":true,"authors":[{"id":69959053,"url":"researcher\/69959053_Anoop_Chaturvedi","fullname":"Anoop Chaturvedi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2085189366,"url":"researcher\/2085189366_Arvind_Shrivastava","fullname":"Arvind Shrivastava","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Communication in Statistics- Theory and Methods 01\/2016; 45(2):307-320. DOI:10.1080\/03610926.2013.806666"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/290471649_Bayesian_analysis_of_a_linear_model_involving_structural_changes_in_either_regression_parameters_or_disturbances_precision","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/290471649_Bayesian_analysis_of_a_linear_model_involving_structural_changes_in_either_regression_parameters_or_disturbances_precision\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56aba05bc3db3"},"id":"rgw18_56aba05bc3db3","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=290471649","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw15_56aba05bc3db3"},"id":"rgw15_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=220499817&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":220499817,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":220499817,"publicationType":"article","linkId":"00b7d516d1ebfbe178000000","fileName":"download.pdf","fileUrl":"profile\/Simon_Rogers\/publication\/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors\/links\/00b7d516d1ebfbe178000000.pdf","name":"Simon Rogers","nameUrl":"profile\/Simon_Rogers","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Jan 20, 2016","fileSize":"462.59 KB","widgetId":"rgw21_56aba05bc3db3"},"id":"rgw21_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=220499817&linkId=00b7d516d1ebfbe178000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":220499817,"publicationType":"article","linkId":"0e606973f0c46d4f0abd7dae","fileName":"Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors","fileUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.101.7451&amp;rep=rep1&amp;type=pdf","name":"psu.edu","nameUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.101.7451&amp;rep=rep1&amp;type=pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw22_56aba05bc3db3"},"id":"rgw22_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=220499817&linkId=0e606973f0c46d4f0abd7dae&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw20_56aba05bc3db3"},"id":"rgw20_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=220499817&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":92,"valueFormatted":"92","widgetId":"rgw23_56aba05bc3db3"},"id":"rgw23_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=220499817","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw19_56aba05bc3db3"},"id":"rgw19_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=220499817&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":220499817,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw25_56aba05bc3db3"},"id":"rgw25_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=220499817&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":92,"valueFormatted":"92","widgetId":"rgw26_56aba05bc3db3"},"id":"rgw26_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=220499817","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw24_56aba05bc3db3"},"id":"rgw24_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=220499817&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Girolami, M. and Rogers, S. (2006) Variational Bayesian multinomial \nprobit regression with Gaussian process priors. Neural Computation \n18(8):pp. 1790-1817.\n \n \n \n \n \n \nhttp:\/\/eprints.gla.ac.uk\/3813\/ \n \nDeposited on: 9 November 2007 \n \n \nGlasgow ePrints Service \nhttp:\/\/eprints.gla.ac.uk"},{"page":2,"text":"Variational Bayesian Multinomial Probit\nRegression with Gaussian Process Priors\nMark Girolami & Simon Rogers\n{girolami,srogers}@dcs.gla.ac.uk\nDepartment of Computing Science\nUniversity of Glasgow\nTechnical Report: TR-2005-205\n{girolami,srogers}@dcs.gla.ac.uk.\nNovember 9, 2005\n1"},{"page":3,"text":"Abstract\nIt is well known in the statistics literature that augmenting binary\nand polychotomous response models with Gaussian latent variables en-\nables exact Bayesian analysis via Gibbs sampling from the parameter\nposterior. By adopting such a data augmentation strategy, dispensing\nwith priors over regression coefficients in favour of Gaussian Process\n(GP) priors over functions, and employing variational approximations\nto the full posterior we obtain efficient computational methods for\nGaussian Process classification in the multi-class setting1. The model\naugmentation with additional latent variables ensures full a posteri-\nori class coupling whilst retaining the simple a priori independent\nGP covariance structure from which sparse approximations, such as\nmulti-class Informative Vector Machines (IVM), emerge in a very nat-\nural and straightforward manner. This is the first time that a fully\nVariational Bayesian treatment for multi-class GP classification has\nbeen developed without having to resort to additional explicit approx-\nimations to the non-Gaussian likelihood term. Empirical comparisons\nwith exact analysis via MCMC and Laplace approximations illustrate\nthe utility of the variational approximation as a computationally eco-\nnomic alternative to full MCMC and it is shown to be more accurate\nthan the Laplace approximation.\n1Introduction\nIn (Albert and Chib, 1993) it was first shown that by augmenting binary\nand multinomial probit regression models with a set of continuous latent\nvariables yk, corresponding to the k\u2019th response value where yk = mk+ ?,\n? \u223c N(0,1) and mk =?\nple consider binary probit regression on target variables tn \u2208 {0,1}, the\nprobit likelihood for the nth data sample taking unit value (tn = 1) is\nP(tn= 1|xn,\u03b2) = \u03a6(\u03b2Txn), where \u03a6 is the standardised Normal Cumula-\ntive Distribution Function (CDF). Now, this can be obtained by the follow-\ning marginalisation?P(tn= 1,yn|xn,\u03b2)dyn=?P(tn= 1|yn)p(yn|xn,\u03b2)dyn\nmarginal is simply the normalizing constant of a left truncated univariate\nj\u03b2kjxj, an exact Bayesian analysis can be per-\nformed by Gibbs sampling from the parameter posterior.As an exam-\nand as by definition P(tn= 1|yn) = \u03b4(yn> 0) then we see that the required\n1Matlab code to allow replication of the reported results is available at http:\/\/www.\ndcs.gla.ac.uk\/people\/personal\/girolami\/pubs 2005\/VBGP\/index.htm\n2"},{"page":4,"text":"Gaussian so that P(tn= 1|xn,\u03b2) =?\u03b4(yn> 0)Nyn(\u03b2Txn,1)dyn= \u03a6(\u03b2Txn).\n1,yn|xn,\u03b2) = \u03b4(yn > 0)Nyn(\u03b2Txn,1) provides a straightforward means of\nGibbs sampling from the parameter posterior which would not be the case if\nthe marginal term, \u03a6(\u03b2Txn), was employed in defining the joint distribution\nover data and parameters.\nThis data augmentation strategy can be adopted in developing efficient\nmethods to obtain binary and multi-class Gaussian Process (GP) (Williams\nand Rasmussen, 1996) classifiers as will be presented in this paper. With the\nexception of (Neal, 1998), where a full Markov Chain Monte Carlo (MCMC)\ntreatment to GP based classification is provided, all other approaches have\nfocussed on methods to approximate the problematic form of the poste-\nrior2which allow analytic marginalisation to proceed. Laplace approxima-\ntions to the posterior were developed in (Williams and Barber, 1998) whilst\nlower & upper bound quadratic likelihood approximations were considered\nin (Gibbs, 2000). Variational approximations for binary classification were\ndeveloped in (Seeger, 2000) where a logit likelihood was considered and mean\nfield approximations were applied to probit likelihood terms in (Opper and\nWinther, 2000), (Csato et al, 2000) respectively. Additionally, incremen-\ntal (Quinonero-Candela and Winther, 2003) or sparse approximations based\non Assumed Density Filtering (ADF) (Csato and Opper, 2002), Informative\nVector Machines (IVM) (Lawrence, et al 2003) and Expectation Propagation\n(EP) (Minka, 2001; Kim, 2005) have been proposed. With the exceptions of\n(Williams and Barber, 1998; Gibbs, 2000; Seeger and Jordan, 2004; Kim,\n2005) the focus of most recent work has largely been on the binary GP clas-\nsification problem. In (Seeger and Jordan, 2004) a multi-class generalisation\nof the IVM is developed where the authors employ a multinomial-logit soft-\nmax likelihood. However, considerable representational effort is required to\nensure that the scaling of computation and storage required of the proposed\nmethod matches that of the original IVM with linear scaling in the number\nof classes. In contrast, by adopting the probabilistic representation of (Al-\nbert and Chib, 1993) we will see that GP based K-class classification and\nefficient sparse approximations (IVM generalisations with scaling linear in\nthe number of classes) can be realised by optimising a strict lower-bound\nof the marginal likelihood of a multinomial probit regression model which\nThe key observation here is that working with the joint distribution P(tn=\n2The likelihood is nonlinear in the parameters due to either the logistic or probit link\nfunctions required in the classification setting\n3"},{"page":5,"text":"requires the solution of K computationally independent GP regression prob-\nlems whilst still operating jointly (statistically) on the data. We will also\nshow that the accuracy of this approximation is comparable to that obtained\nvia MCMC.\nThe following section now introduces the multinomial-probit regression\nmodel with Gaussian Process priors.\n2 Multinomial Probit Regression\nDefine the data matrix as X = [x1,\u00b7\u00b7\u00b7 ,xN]Twhich has dimension N \u00d7 D\nand the N \u00d71 dimensional vector of associated target values as t where each\nelement tn\u2208 {1,\u00b7\u00b7\u00b7 ,K}. The N \u00d7 K matrix of GP random variables mnk\nis denoted by M. We represent the N \u00d71 dimensional columns of M by mk\nand the corresponding K \u00d7 1 dimensional rows by mn. The N \u00d7 K matrix\nof auxiliary variables ynkis represented as Y, where the N \u00d7 1 dimensional\ncolumns are denoted by ykand the corresponding K\u00d71 dimensional rows as\nyn. The M \u00d7 1 vector of covariance kernel hyper-parameters for each class3\nis denoted by \u03d5kand associated hyper-parameters \u03c8k& \u03b1k complete the\nmodel.\nThe graphical representation of the conditional dependency structure in\nthe auxiliary variable multinomial probit regression model with GP priors in\nthe most general case is shown in Figure (1).\n\u03c8\n\u03d5\nm\ny\nt\nMN\nK\n\u03b1\nFigure 1: Graphical representation of the conditional dependencies within the\ngeneral multinomial probit regression model with Gaussian Process priors.\n3This is the most general setting, however it is more common to employ a single and\nshared GP covariance function across classes.\n4"},{"page":6,"text":"3 Prior Probabilities\nFrom the graphical model in Figure (1) a priori we can assume class specific\nGP independence and define model priors such that mk|X,\u03d5k\u223c GP(\u03d5k) =\nNmk(0,C\u03d5k), where the matrix C\u03d5k, of dimension N \u00d7 N defines the class\nspecific GP covariance4. Typical examples of such GP covariance functions\nare radial basis style functions such that the i,j\u2019th element of each C\u03d5kis\ndefined as exp{\u2212?M\nwithin the GP function prior, see for example (McKay, 2003).\nAs in (Albert and Chib, 1993) we employ a standardised normal noise\nmodel such that the prior on the auxilliary variables is ynk|mnk\u223c Nynk(mnk,1)\nto ensure appropriate matching with the probit function. Of course rather\nthan having this variance fixed it could also be made an additional free pa-\nrameter of the model and therefore would yield a scaled probit function. For\nthe presentation here we restrict ourselves to the standardised model and\nconsider extensions to a scaled probit model as possible further work. The\nrelationship between the additional latent variables yn(denoting the n\u2019th row\nof Y) and the targets tnas defined in multinomial probit regression (Albert\nand Chib, 1993) is adopted here, i.e.\nd=1\u03d5kd(xid\u2212 xjd)2} where in this case M = D, however\nthere are many other forms of covariance functions which may be employed\ntn= j ifynj=\nmax\n1\u2264k\u2264K{ynk}\n(1)\nThis has the effect of dividing RK(y space) into K non-overlapping K-\ndimensional cones Ck= {y : yk> yi,k ?= i} where RK= \u222akCkand so each\nP(tn= i|yn) can be represented as \u03b4(yni> ynk\u2200 k ?= i). We then see that\nsimilar to the binary case where the probit function emerges from explicitly\nmarginalising the auxiliary variable the multinomial probit takes the form\ngiven below, where details are given in Appendix I.\n?\n?\nj=1\nP(tn= i|mn) =\u03b4(yni> ynk\u2200 k ?= i)\nK\n?\nj=1\np(ynj|mnj)dy\n??\n=\nCi\nK\n?\np(ynj|mnj)dy = Ep(u)\nj?=i\n\u03a6(u + mni\u2212 mnj)\n?\n4The model can be defined by employing K \u22121 GP functions and an alternative trun-\ncation of the Gaussian over the variables ynkhowever for the multi-class case we define a\nGP for each class.\n5"},{"page":7,"text":"where the random variable u is standardised normal i.e. p(u) = N(0,1).\nAn hierarchic prior on the covariance function hyper-parameters is employed\nsuch that each hyper-parameter has, for example, an independent exponen-\ntial distribution \u03d5kd\u223c Exp(\u03c8kd) and a gamma distribution is placed on the\nmean values of the exponential \u03c8kd \u223c \u0393(\u03c3k,\u03c4k) thus forming a conjugate\npair. Of course, as detailed in (Girolami and Rogers, 2005), a more general\nform of covariance function can be employed that will allow the integration\nof heterogeneous types of data which takes the form of a weighted combi-\nnation of base covariance functions. The associated hyper-hyper-parameters\n\u03b1 = {\u03c3k=1,\u00b7\u00b7\u00b7,K,\u03c4k=1,\u00b7\u00b7\u00b7,K} can be estimated via type-II maximum likelihood\nor set to reflect some prior knowledge of the data.\npriors can be employed such that, for example, each \u03c3k = \u03c4k = 10\u22126.\nDefining the parameter set as \u0398 = {Y,M} and the hyper-parameters as\n\u03a6 = {\u03d5k=1,\u00b7\u00b7\u00b7,K,\u03c8k=1,\u00b7\u00b7\u00b7,K} the joint likelihood takes the form below.\nAlternatively, vague\np(t,\u0398,\u03a6|X,\u03b1) =\nN\n?\nK\n?\nn=1\n?\nK\n?\ni=1\n\u03b4(yni> ynk\u2200 k ?= i)\u03b4(tn= i)\n?\n\u00d7\nk=1\np(ynk|mnk)p(mk|X,\u03d5k)p(\u03d5k|\u03c8k)p(\u03c8k|\u03b1k) (2)\n4Gaussian Process Multi-Class Classification\nWe now consider both exact and approximate Bayesian inference for GP clas-\nsification with multiple classes employing the multinomial-probit regression\nmodel.\n4.1Exact Bayesian Inference: The Gibbs Sampler\nThe representation of the joint likelihood (Equation 2) is particularly con-\nvenient in that samples can be drawn from the full posterior over the model\nparameters (given the hyper-parameter values) p(\u0398|t,X,\u03a6,\u03b1) using a Gibbs\nsampler in a very straightforward manner with scaling per sample of O(KN3).\nFull details of the Gibbs sampler are provided in Appendix IV and this sam-\npler will be employed in the experimental section.\n6"},{"page":8,"text":"4.2Approximate Bayesian Inference: The Laplace Ap-\nproximation\nThe Laplace approximation of the posterior over GP variables, p(M|t,X,\u03a6,\u03b1)\n(where Y is marginalised), requires finding the mode of the unnormalised pos-\nterior. Approximate Bayesian inference for GP classification with multiple-\nclasses employing a multinomial-logit (softmax) likelihood has been devel-\noped previously in (Williams and Barber, 1998). Due to the form of the\nmultinomial-logit likelihood a Newton iteration to obtain the posterior mode\nwill scale at best as O(KN3). Employing the multinomial-probit likelihood\nwe find that each Newton step will scale as O(K3N3) and details are provided\nin Appendix V.\n4.3Approximate Bayesian Inference:\nand Sparse Approximation\nA Variational\nEmploying a variational Bayes approximation (Beal, 2003; Jordan, et al 1999;\nMcKay, 2003) by using an approximating ensemble of factored posteriors\nsuch that p(\u0398|t,X,\u03a6,\u03b1) \u2248\nprobit regression is more appealing from a computational perspective as a\nsparse representation, with scaling O(KNS2) (where S is the subset of sam-\nples entering the model and S ? N), can be obtained in a straightforward\nmanner as will be shown in the following sections. The lower bound5, see\nfor example (Beal, 2003; Jordan, et al 1999; McKay, 2003), on the marginal\nlikelihood logp(t|X,\u03a6,\u03b1) \u2265 EQ(\u0398){logp(t,\u0398|X,\u03a6,\u03b1)}\u2212EQ(\u0398){logQ(\u0398)}\nis minimised by distributions which take an unnormalised form of Q(\u0398i) \u221d\nexp?EQ(\u0398\\\u0398i){logP(t,\u0398|X,\u03a6,\u03b1)}?where Q(\u0398\\\u0398i) denotes the ensemble\nposterior components are given in the Appendix.\nThe approximate posterior over the GP random variables takes a factored\nform such that\n?\ni=1Q(\u0398i) = Q(Y)Q(M) for multinomial-\ndistribution with the ithcomponent of \u0398 removed. Details of the required\nQ(M) =\nK\n?\nk=1\nQ(mk) =\nK\n?\nk=1\nNmk(? mk,\u03a3k)(3)\n5The bound follows from the application of Jensens inequality e.g.\nlog?p(t,\u0398|X)\nlogp(t|X) =\nQ(\u0398)Q(\u0398)d\u0398 \u2265?Q(\u0398)logp(t,\u0398|X)\nQ(\u0398)d\u0398\n7"},{"page":9,"text":"where the shorthand tilde notation denotes posterior expectation i.e.?\n\u03a3k? ykwhere \u03a3k= C\u03d5k(I + C\u03d5k)\u22121(see Appendix I for full details). We will\nensuring that the appropriate class-conditional posterior dependencies will be\ninduced in?\neach of N data samples induces posterior dependencies between each of the\nK columns of?\nobtaining sparse approximations (Lawrence, et al 2003) for the multi-class\nGP in particular.\nDue to the multinomial probit definition of the dependency between each\nelement of ynand tn(Equation.1) the posterior for the auxiliary variables\nfollows as\nN\n?\nwhere Ntn\nthat if tn= i where i \u2208 {1,\u00b7\u00b7\u00b7 ,K} then the i\u2019th dimension has the largest\nvalue. The required posterior expectations ? ynkfor all k ?= i and ? ynifollow as\n? ynk =\n? yni =\nwhere \u03a6n,i,k\nu\ntions with respect to p(u) which appear in Equation (5) can be obtained by\nquadrature or straightforward sampling methods.\nIf we also consider the set of hyper-parameters, \u03a6, in this variational\ntreatment then the approximate posterior for the covariance kernel hyper-\nparameters takes the form of\nf(a) =\nEQ(a){f(a)} and so the required posterior mean for each k is given as ? mk=\nsee that each row, ? yn, of?Y will have posterior correlation structure induced\nM. It should be stressed here that whilst there are K a posteriori\nindependent GP processes the associated K-dimensional posterior means for\nM due to the posterior coupling over each of the auxiliary\nvariables yn. We will see that this structure is particularly convenient in\nQ(Y) =\nn=1\nQ(yn) =\nN\n?\nn=1\nNtn\nyn(? mn,I) (4)\nyn(? mn,I) denotes a conic truncation of a multivariate Gaussian such\n?Nu(? mnk\u2212 ? mni,1)\u03a6n,i,k\n??\n=?\n? mnk\u2212\n? mni\u2212\nEp(u)\nu\n?\nEp(u)\n?\n? ynj\u2212 ? mnj\n\u03a6(u + ? mni\u2212 ? mnk)\u03a6n,i,k\nu\n?\n(5)\nj?=i\n?\n(6)\nj?=i,k\u03a6(u + ? mni\u2212 ? mnj), and p(u) = Nu(0,1). The expecta-\nQ(\u03d5k) \u221d N\n?mk(0,C\u03d5k)\n?M\nd=1Exp(\u03d5kd|?\u03c8kd)\nand the required posterior expectations can be estimated employing impor-\ntance sampling. Expectations can be approximated by drawing S samples\n8"},{"page":10,"text":"such that each \u03d5s\nkd\u223c Exp(?\u03c8kd) and so\n?\n?\nf(\u03d5k) \u2248\nS\ns=1\nf(\u03d5s\nk)w(\u03d5s\nk) where w(\u03d5s\nk) =\nN\n?mk\n?0,C\u03d5s\nk\n?\n?S\ns?=1N\n?mk\n?\n0,C\u03d5s?\nk\n?\n(7)\nThis form of importance sampling within a variational Bayes procedure has\nbeen employed previously in (Lawrence, et al 2004). Clearly the scaling of the\nabove estimator per sample is similar to that required in the gradient based\nmethods which search for optima of the marginal likelihood as employed in\nGP regression and classification e.g. (McKay, 2003).\nFinally we have that each Q(\u03c8kd) = \u0393\u03c8kd(\u03c3k+ 1,\u03c4k+ ? \u03d5kd) and the asso-\n4.4 Summarising Variational Multi-Class GP Classifi-\ncation\nciated posterior mean is simply?\u03c8kd= (\u03c3k+ 1)\/(\u03c4k+ ? \u03d5kd).\nWe can summarise what has been presented by the following iterations which,\nin the general case, for all k and d, will optimise the bound on the marginal\nlikelihood (explicit expressions for the bound are provided in Appendix III).\n? mk\n\u2190 C\n\u2190\n?\u03d5k(I + C\n?\n\u03c3k+ 1\n\u03c4k+ ? \u03d5kd\n?\u03d5k)\u22121(? mk+ pk)(8)\n? \u03d5k\n?\u03c8kd \u2190\ns\n\u03d5s\nkw(\u03d5s\nk)(9)\n(10)\nwhere each \u03d5s\nkthcolumn of the N \u00d7 K matrix P whose elements pnkare defined by the\nrightmost terms in Equations (5 & 6) i.e. for tn = i then for all k ?= i\npnk= \u2212\nu\nThese iterations can be viewed as obtaining K One against All binary\nclassifiers, however, most importantly they are not statistically independent\nof each other but are a posteriori coupled via the posterior mean estimates of\neach of the auxiliary variables yn. The computational scaling will be linear\nin the number of classes and cubic in the number of data points O(KN3).\nIt is worth noting that if the covariance function hyper-parameters are fixed\nthen the costly matrix inversion only requires to be computed once. The\nkd\u223c Exp(?\u03c8kd), w(\u03d5s\nEp(u){Nu(\nEp(u){\u03a6(u+\nk) is defined as previously and pk is the\n?mnk\u2212\n?mni,1)\u03a6n,i,k\nu\n}\n?mni\u2212\n?mnk)\u03a6n,i,k\n}and pni= \u2212?\nj?=ipnj.\n9"},{"page":11,"text":"Laplace approximation will require a matrix inversion for each Newton step\nwhen finding the mode of the posterior (Williams and Barber, 1998).\n4.4.1 Binary Classification\nPrevious variational treatments of GP based binary classification include\n(Seeger, 2000; Opper and Winther, 2000; Gibbs, 2000; Csato and Opper,\n2002; Csato et al, 2000). It is however interesting to note in passing that for\nbinary classification, the outer plate in Figure (1) is removed and further sim-\nplification follows as only K \u22121 i.e. one set of posterior mean values requires\nto be estimated and as such the posterior expectations ? m = C\nnow a product of truncated univariate Gaussians and as such the expectation\nfor the latent variables ynhas an exact analytic form. For a unit-variance\nGaussian truncated below zero if tn= 1 and above zero if tn= \u22121 the re-\nquired posterior mean ? y has elements which can be obtained by the following\nmean of a Gaussian due to truncation6? yn= ? mn+tnN\nlikelihood\n? m \u2190 C\n?\u03d5(I+C\n?\u03d5)\u22121? y\nnow operate on N \u00d7 1 dimensional vectors ? m and ? y. The posterior Q(y) is\nanalytic expression derived from straightforward results for corrections to the\n?mn(0,1)\/ \u03a6(tn? mn). So\nthe following iteration will guarantee an increase in the bound of the marginal\n?\u03d5(I + C\n?\u03d5)\u22121(? m + p)(11)\nwhere each element of the N\u00d71 vector p is defined as pn= tnN\n?mn(0,1)\/\u03a6(tn? mn).\n4.5Variational Predictive Distributions\nThe predictive distribution, P(tnew= k|xnew,X,t)7, for a new sample xnew\nfollows from results for standard GP regression. The N \u00d7 1 vector Cnew\ncontains the covariance function values between the new point and those\ncontained in X, and cnew\ndenotes the covariance function value for the new\npoint and itself. So the GP posterior p(mnew|xnew,X,t) is a product of K\nGaussians each with mean and variance\n?\u03d5k\n?\u03d5k\n? mnew\nk\n= ? yT\nyNy(? m,1)\/{1 \u2212 \u03a6(\u2212? m)}dy = ? m + N\nk(I + C\n?\u03d5k)\u22121Cnew\n?\u03d5k\n?\u03c32\n\u2212\u221eyNy(? m,1)\/\u03a6(\u2212? m)dy = ? m \u2212 N\nk,new\n= cnew\n?\u03d5k\u2212 (Cnew\n?\u03d5k)T(I + C\n?\u03d5k)\u22121Cnew\n?\u03d5k\n6For t = +1 then ? y =?+\u221e\n0\n?m(0,1)\/\u03a6(? m) and for\nt = \u22121 then ? y =?0\n?m(0,1)\/\u03a6(\u2212? m).\n7Conditioning on?Y, ? \u03d5,?\u03c8, and \u03b1 is implicit.\n10"},{"page":12,"text":"using the following shorthand ? \u03bdnew\ntarget values as\nk\n=\n?\n1 +?\u03c32\nk,newthen it is straightforward\n(details in Appendix II) to obtain the predictive distribution over possible\nP(tnew= k|xnew,X,t) = Ep(u)\n??\nj?=k\n\u03a6\n?\n1\n? \u03bdnew\nj\n?u? \u03bdnew\nk\n+ ? mnew\nk\n\u2212 ? mnew\nj\n???\nwhere, as before, u \u223c Nu(0,1). The expectation can be obtained numerically\nemploying sample estimates from a standardised Gaussian. For the binary\ncase then the standard result follows\n?\n= 1 \u2212 \u03a6\nP(tnew= 1|xnew,X,t) =\u03b4(ynew> 0)Nynew (? mnew,? \u03bdnew)dynew\n? \u03bdnew\n?\n\u2212? mnew\n?\n= \u03a6\n?? mnew\n? \u03bdnew\n?\n5Sparse Variational Multi-Class GP Classi-\nfication\nThe dominant O(N3) scaling of the matrix inversion required in the posterior\nmean updates in GP regression has been the motivation behind a large body\nof literature focusing on reducing this cost via reduced rank approximations\n(Williams and Seeger, 2001) and sparse online learning (Csato and Opper,\n2002; Quinonero-Candela and Winther, 2003) where Assumed Density Fil-\ntering (ADF) forms the basis of online learning and sparse approximations\nfor GP\u2019s. Likewise in (Lawrence, et al 2003) the Informative Vector Ma-\nchine (IVM) (refer to (Lawrence, et al 2005) for comprehensive details) is\nproposed which employs informative point selection criteria (Seeger, et al\n2003) and ADF updating of the approximations of the GP posterior para-\nmeters. Only binary classification has been considered in (Lawrence, et al\n2003; Csato and Opper, 2002; Quinonero-Candela and Winther, 2003) and it\nis clear from (Seeger and Jordan, 2004) that extension of ADF based approx-\nimations such as IVM to the multi-class problem is not at all straightforward\nwhen a multinomial-logit softmax likelihood is adopted. However, we now\nsee that sparse GP based classification for multiple classes (multi-class IVM)\nemerges as a simple by-product of online ADF approximations to the para-\nmeters of each Q(mk) (multivariate Gaussian). The ADF approximations\n11"},{"page":13,"text":"when adding the nthdata sample, selected at the lthof S iterations, for each\nof the K GP posteriors, Q(mk), follow simply from details in (Lawrence, et\nal 2005) as given below.\n\u03a3k,n \u2190 Cn\nsk\n\u2190 sk\u2212\n\u03d5k\u2212 MT\n1 + skndiag?\u03a3k,n\u03a3T\n1\n\u221a1 + skn\u03a3T\n? mk+? ynk\u2212 ? mnk\nkMk,n\n(12)\n1\nk,n\n?\n(13)\nMl\nk\n\u2190\nk,n\n(14)\n? mk\n\u2190\n1 + skn\n\u03a3k,n\n(15)\nEach ? ynk\u2212 ? mnk= pnkas defined in Section (4.4) and can be obtained from\n(5 & 6), \u03a3k,n, an N \u00d7 1 vector, is the nthcolumn of the current estimate\nof each \u03a3k, likewise Cn\nAll elements of each Mk and mk are initialised to zero whilst each sk has\ninitial unit values. Of course there is no requirement to explicitly store each\nN \u00d7 N dimensional matrix \u03a3k, only the S \u00d7 N matrices Mk and N \u00d7 1\nvectors skrequire storage and maintenance. We denote indexing into the lth\nrow of each Mkby Ml\nestimated posterior variance.\nThe efficient Cholesky factor updating as detailed in (Lawrence, et al\n2005) will ensure that for N data samples, K distinct GP priors, and a\nmaximum of S samples included in the model where S << N then at most\nO(KSN) storage and O(KNS2) compute scaling will be realised.\nAs an alternative to the entropic scoring heuristic of (Seeger, et al 2003;\nLawrence, et al 2003) we suggest that an appropriate criterion for point\ninclusion assessment will be the posterior predictive probability of a target\nvalue given the current model parameters for points which are currently not\nincluded in the model i.e. P (tm|xm,{mk},{\u03a3k}), where the subscript m\nindexes such points. From the results of the previous section this is equal to\nPr(ym\u2208 Ctm=k) which is expressed as\n??\nwhere k is the value of tm, \u03bdjm=?1 + sjm, and so the data point with the\nthe current stored approximate values of each ? mn1,\u00b7\u00b7\u00b7 , ? mnK via equations\n\u03d5kis the nthcolumn of each GP covariance matrix.\nk, and the nthelement of each skby sknwhich is the\nEp(u)\nj?=k\n\u03a6\n?\n1\n\u03bdjm[u\u03bdkm+ ? mmk\u2212 ? mmj]\n??\n(16)\nsmallest posterior target probability should be selected for inclusion. This\n12"},{"page":14,"text":"scoring criterion requires no additional storage overhead as all ? mk and sk\nthe model in, at most, O(KN) time8. Intuitively points in regions of low\ntarget posterior certainty, i.e. class boundaries, will be the most influential in\nupdating the approximation of the target posteriors. And so the inclusion of\npoints with the most uncertain target posteriors will yield the largest possible\ntranslation of each updated mkinto the interior of their respective cones Ck.\nExperiments in the following section will demonstrate the effectiveness of this\nmulti-class IVM.\nare already available and it can be computed for all m not currently in\n6 Experiments\n6.1Illustrative Multi-Class Toy Example\nTen dimensional data vectors, x, were generated such that if t = 1 then\n0.5 > x2\n[x1,x2]T\u223c N(0,0.01I) where I denotes an identity matrix of appropriate\ndimension. Finally x3,\u00b7\u00b7\u00b7 ,x10are all distributed as N(0,1). Both the first\ntwo dimensions are required to define the three class labels with the remaining\neight dimensions being irrelevant to the classification task. Each of the three\ntarget values were sampled uniformly thus creating a balance of samples\ndrawn from the three target classes.\nTwo hundred and forty draws were made from the above distribution\nand the sample was used in the proposed variational inference routine with a\nfurther 4620 points being used to compute a 0-1 loss class prediction error. A\ncommon radial basis covariance function of the form exp{\u2212?\nthe length-scale hyper-parameters \u03c81,\u00b7\u00b7\u00b7 ,\u03c810. The posterior expectations\nof the auxiliary variables ? y were obtained from Equations 5 & 6 where the\nN(0,1). The variational importance sampler employed 500 samples drawn\nfrom each Exp(?\u03c8d) in estimating the corresponding posterior means ? \u03d5dfor\nand \u03d5 had unit initial values. In this example the variational iterations ran\nfor fifty steps where each step corresponds to the sequential posterior mean\nupdates of Equation (8,9,10). The value of the variational lower-bound was\n1+ x2\n2> 0.1, for t = 2 then 1.0 > x2\n1+ x2\n2> 0.6 and for t = 3 then\nd\u03d5d|xid\u2212xjd|2}\nwas employed and vague hyper-parameters, \u03c3 = \u03c4 = 10\u22123were placed on\nGaussian integrals were computed using 1000 samples drawn from p(u) =\nthe covariance function parameters. Each M and Y were initialised randomly\n8Assuming constant time to approximate the expectation.\n13"},{"page":15,"text":"monitored during each step and as would be expected a steady convergence\nin the improvement of the bound can be observed in Figure (2.a).\n0 10 2030 4050\n\u221210\n\u22129\n\u22128\n\u22127\n\u22126\n\u22125\n\u22124\nIteration Number\n(a)\nLB on Marginal Likelihood \u00d7 10\u22122\n(b)\n0 10 20 304050\n20\n40\n60\n80\n100\nIteration Number\n% Predictions Correct\n(c)\nFigure 2: (a) Convergence of the Lower Bound on the Marginal-Likelihood\nfor the toy data set considered. (b) Evolution of estimated posterior means\nfor the inverse squared length scale parameters (precision parameters) in the\nRBF covariance function, (c) Evolution of out-of-sample predictive perfor-\nmance on the toy data set.\nLikewise the development of the estimated posterior mean values for the\ncovariance function parameters ? \u03d5d, Figure (2.b), shows Automatic Relevance\nare effectively removed from the model.\nFrom Figure (2.c) we can see that the development of the predictive\nperformance (out of sample) follows that of the lower-bound (Figure 2.a)\nachieving a predictive performance of 99.37% at convergence. As a compari-\nson to our multi-class GP classifier we use a Directed Acyclic Graph (DAG)\nSVM (Platt, et al 2000) (assuming equal class distributions the scaling9is\nO(N3K\u22121)) on this example. Employing the values of the posterior mean\nvalues of the covariance function length scale parameters (one for each of\nthe ten dimensions) estimated by the proposed variational procedure in the\nRBF kernel of the DAG SVM a predictive performance of 99.33% is obtained.\nSo, on this dataset, the proposed GP classifier has comparable performance,\nunder 0-1 loss, to the DAG SVM. However the estimation of the covariance\nfunction parameters is a natural part of the approximate Bayesian inference\nroutines employed in GP classification. There is no natural method of ob-\ntaining estimates of the ten kernel parameters for the SVM without resorting\nDetection (ARD) in progress (Neal, 1998) where the eight irrelevant features\n9This assumes the use of standard quadratic optimisation routines.\n14"},{"page":16,"text":"to cross-validation (CV), which in the case of a single parameter, is feasible\nbut rapidly becomes infeasible as the number of parameters increases.\n6.2 Comparing Laplace & Variational Approximations\nto Exact Inference via Gibbs Sampling\nThis section provides a brief empirical comparison of the Variational approxi-\nmation, developed in previous sections, to a full MCMC treatment employing\nthe Gibbs sampler detailed in Appendix IV. In addition, a Laplace approxi-\nmation is also considered in this short comparative study.\nVariational approximations provide a strict lower-bound on the marginal\nlikelihood and it is this bound which is one of the approximations attractive\ncharacteristics. However it is less well understood how much parameters ob-\ntained from such approximations differ from those obtained via exact meth-\nods. Preliminary analysis of the asymptotic properties of variational estima-\ntors is provided in (Wang and Titterington, 2004). A recent experimental\nstudy of EP and Laplace approximations to binary GP classifiers has been\nundertaken by (Kuss and Rasmussen, 2005) and it is motivating to consider\na similar comparison for the variational approximation in the multiple-class\nsetting. In (Kuss and Rasmussen, 2005) it was observed that the marginal\nand predictive likelihoods, computed over a wide range of covariance kernel\nhyper-parameter values, were less well preserved by the Laplace approxi-\nmation than the EP approximation when compared to that obtained via\nMCMC. We then consider the predictive likelihood obtained via the Gibbs\nsampler and compare this to the variational and Laplace approximations of\nthe GP-based classifiers.\nThe toy dataset from the previous section is employed and, as in (Kuss\nand Rasmussen, 2005), a covariance kernel of the form sexp{\u2212\u03d5?\nat each pair of hyper-parameter values a multinomial-probit GP classifier is\ninduced using (a) MCMC via the Gibbs sampler, (b) the proposed variational\napproximation, (c) a Laplace approximation of the probit model. For the\nGibbs sampler, after a burn-in of 2000 samples, the following 1000 samples\nwere used for inference purposes and the predictive likelihood (probability\nof target values in the test set) and test error (0-1 error loss) was estimated\nfrom the 1000 post-burn-in samples as detailed in Appendix IV.\nWe firstly consider a binary classification problem by merging classes 2\nd?xid\u2212\nxjd?2} is adopted. Both s & \u03d5 are varied in the range (log scale) -1 to +5 and\n15"},{"page":17,"text":"& 3 of the toy data set into one class. The first thing to note from Figure\n(3) is that the predictive likelihood response under the variational approxi-\nmation preserves, to a rather good degree, the predictive likelihood response\nobtained when using Gibbs sampling across the range of hyper-parameter\nvalues. However the Laplace approximation does not do as good a job in\nreplicating the levels of the response profile obtained via MCMC over the\nrange of hyper-parameter values considered and this finding is consistent\nwith the results of (Kuss and Rasmussen, 2005).\n\u221240\n\u221260\n\u221280\n\u2212160\n\u221220\n\u2212101\nlog(\u03d5)\n(a)\n2345\n\u22121\n0\n1\n2\n3\n4\n5\nlog(s)\n\u221240\n\u221260\n\u221280\n\u2212160\n\u221220\n\u2212101\nlog(\u03d5)\n(b)\n2345\n\u22121\n0\n1\n2\n3\n4\n5\nlog(s)\n\u221240\n\u221260\n\u221280\n\u2212160\n\u2212101\nlog(\u03d5)\n(c)\n2345\n\u22121\n0\n1\n2\n3\n4\n5\nlog(s)\nFigure 3: Isocontours of predictive likelihood for binary classification problem\n(a) Gibbs Sampler, (b) Variational Approximation, (c) Laplace Approxima-\ntion.\nThe Laplace approximation to the multinomial-probit model has O(K3N3)\nscaling (Appendix V) which limits its application to situations where the\nnumber of classes is small. For this reason, in the following experiments we\ninstead consider the multinomial-logit Laplace approximation (Williams and\nBarber, 1998). In Figure (4) the isocontours of predictive likelihood for the\ntoy dataset in the multi-class setting under various hyper-parameter settings\nare provided.\nAs with the binary case the variational multinomial-probit approximation\nprovides predictive likelihood response levels which are good representations\nof those obtained from the Gibbs sampler. The Laplace approximation for\nthe multinomial-logit suffers from the same distortion of the contours as does\nthe Laplace approximation for the binary probit, in addition the information\nin the predictions is lower. We note, as in (Kuss and Rasmussen, 2005), that\nfor s = 1 (logs = 0) the Laplace approximation compares reasonably with\n16"},{"page":18,"text":"\u221210\n\u221220\n\u221230\n\u221240\n2 \u2212101\nlog(\u03d5)\n(a)\n345\n\u22121\n0\n1\n2\n3\n4\n5\nlog(s)\n\u221220\n\u221230\n\u221240\n\u221250\n\u221210\n01\nlog(\u03d5)\n(b)\n2345\n\u22121\n0\n1\n2\n3\n4\n5\nlog(s)\n\u221230\n\u221240\n\u221250\n\u221260\n\u2212101\nlog(\u03d5)\n(c)\n2345\n\u22121\n0\n1\n2\n3\n4\n5\nlog(s)\nFigure 4: Isocontours of predictive likelihood for multi-class classification\nproblem (a) Gibbs Sampler, (b) Variational Approximation, (c) Laplace Ap-\nproximation.\nresults from both MCMC and variational approximations.\nIn the following experiment four standard multi-class datasets (Iris, Thy-\nroid, Wine and Forensic Glass) from the UCI Machine Learning Data Reposi-\ntory10along with the toy data previously described are used. For each dataset\na random 60% training \/ 40% testing split was used to assess the performance\nof each of the classification methods being considered and 50 random splits\nof each data set were used. For the toy dataset 50 random train and test\nsets were generated. The hyper-parameters, for an RBF covariance function\ntaking the form of exp{\u2212?\nthe classification methods considered. The marginal likelihood for the Gibbs\nsampler was estimated simply by using 1000 samples from the GP prior.\nFor each dataset and each method (multinomial-logit Laplace approxima-\ntion, Variational approximation & Gibbs sampler) the marginal likelihood\n(lower-bound in the case of the variational approximation), predictive error\n(0-1 loss) and predictive likelihood were measured. The results, given as the\nmean and standard deviation over the 50 data splits, are listed in Table (6.2).\nThe predictive likelihood obtained from the multinomial logit Laplace\napproximation is consistently, across all datasets, lower than that of the\nVariational approximation and the Gibbs sampler. This indicates that the\nd\u03d5d?xid\u2212xjd?2}, were estimated employing the\nVariational importance sampler and these were then fixed and employed in all\n10http:\/\/www.ics.uci.edu\/\u223cmlearn\/MPRepository.html\n17"},{"page":19,"text":"Toy-Data\nMarginal Likelihood\nPredictive Error\nPredictive Likelihood\nIris\nMarginal Likelihood\nPredictive Error\nPredictive Likelihood\nThyroid\nMarginal Likelihood\nPredictive Error\nPredictive Likelihood\nWine\nMarginal Likelihood\nPredictive Error\nPredictive Likelihood\nForensic Glass\nMarginal Likelihood\nPredictive Error\nPredictive Likelihood\nLaplace\n-169.27 \u00b1 4.27\n3.97 \u00b1 2.00\n-98.90 \u00b1 8.22\nLaplace\n-143.87 \u00b1 1.04\n3.88 \u00b1 2.00\n-10.43 \u00b1 1.12\nLaplace\n-158.18 \u00b1 1.94\n4.73 \u00b1 2.36\n-19.01 \u00b1 2.55\nLaplace\n-152.22 \u00b1 1.29\n2.95 \u00b1 2.16\n-14.57 \u00b1 1.29\nLaplace\n-275.11 \u00b1 2.87\n36.54 \u00b1 4.74\n-90.38 \u00b1 3.25\nVariational\n-232.00 \u00b1 17.13\n3.65 \u00b1 1.95\n-72.27 \u00b1 9.25\nVariational\n-202.98 \u00b1 1.37\n4.08 \u00b1 2.16\n-7.35 \u00b1 1.27\nVariational\n-246.24 \u00b1 1.63\n3.86 \u00b1 2.04\n-14.62 \u00b1 2.70\nVariational\n-253.90 \u00b1 1.52\n2.65 \u00b1 1.87\n-10.16 \u00b1 1.47\nVariational\n-776.79 \u00b1 5.75\n32.79 \u00b1 4.57\n-77.60 \u00b1 3.91\nGibbs Sampler\n-94.07 \u00b1 11.26\n3.49 \u00b1 1.69\n-73.44 \u00b1 7.67\nGibbs Sampler\n-45.27 \u00b1 6.17\n4.08 \u00b1 2.16\n-7.26 \u00b1 1.40\nGibbs Sampler\n-68.82 \u00b1 8.29\n3.94 \u00b1 2.02\n-14.47 \u00b1 2.39\nGibbs Sampler\n-68.65 \u00b1 6.19\n2.78 \u00b1 2.07\n-10.47 \u00b1 1.41\nGibbs Sampler\n-268.21 \u00b1 5.46\n34.00 \u00b1 4.62\n-79.86 \u00b1 4.80\nTable 1: Results of comparison of Gibbs sampler, Variational and Laplace\napproximations when applied to several UCI datasets. Best results for Pre-\ndictive likelihood are highlighted in bold.\npredictions from the Laplace approximation are less informative about the\ntarget values than both other methods considered. In addition the Varia-\ntional approximation yields predictive distributions which are as informative\nas those provided by the Gibbs sampler, however the 0-1 prediction errors\nobtained across all methods do not differ as significantly. In (Kuss and Ras-\nmussen, 2005) a similar observation was made for the binary GP classification\nproblem when Laplace and EP approximations were compared to MCMC. It\nwill then be interesting to further compare EP and Variational approxima-\ntions in this setting.\nWe have observed that the predictions obtained from the variational ap-\n18"},{"page":20,"text":"proximation are in close agreement with those of MCMC whilst the Laplace\napproximation suffers from some inaccuracy and this has also been reported\nfor the binary classification setting in (Kuss and Rasmussen, 2005).\n6.3Multi-Class Sparse Approximation\nA further 1000 samples were drawn from the toy data generating process\nalready described and these were used to illustrate the sparse GP multi-class\nclassifier in operation. The posterior mean values of the shared covariance\nkernel parameters estimated in the previous example were employed here\nand so the covariance kernel parameters were not estimated. The predictive\nposterior scoring criterion proposed in Section (5) was employed in selecting\npoints for inclusion in the overall model. To assess how effective this criterion\nis random sampling was also employed to compare the rates of convergence\nof both inclusion strategies in terms of predictive 0-1 loss on a held out test\nset of 2385 samples. A maximum of S = 50 samples were to be included in\nthe model defining a 95% sparsity level.\nIn Figure (5.a) the first two dimensions of the 1000 samples are plot-\nted with the three different target classes denoted by \u00d7,+,\u2022 symbols. The\nisocontours of constant target posterior probability at a level of 1\/3 (the de-\ncision boundaries) for each of the three classes are shown by the solid and\ndashed lines. What is interesting is that the 50 included points (circled) all sit\nclose to, or on, the corresponding decision boundaries as would be expected\ngiven the selection criteria proposed. These can be considered as a proba-\nbilistic analogue to the support vectors of an SVM. The rates of 0-1 error\nconvergence using both random and informative point sampling are shown in\nFigure (5.b). The procedure was repeated twenty times, using the same data\nsamples, and the error bars show one standard deviation over these repeats.\nIt is clear that, on this example at least, random sampling has the slowest\nconvergence, and the informative point inclusion strategy achieves less than\n1% predictive error after the inclusion of only 30 data points. Of course we\nshould bridle our enthusiasm by recalling that the estimated covariance ker-\nnel parameters are already supplied. Nevertheless, multi-class IVM makes\nBayesian GP inference on large scale problems with multiple classes feasible\nas will be demonstrated in the following example.\n19"},{"page":21,"text":"\u22121\u22120.500.51\n\u22121  \n\u22120.5\n0   \n0.5\n1   \n(a)\n01020304050\n20\n30\n40\n50\n60\n70\n80\n90\n100\nNumber Points Included\n(b)\nPercentage Predictions Correct\nFigure 5: (a) Scatter plot of the first two dimensions of the 1000 available data\nsample. Each class is denoted by \u00d7,+,\u2022 and the decision boundaries denoted\nby the contours of target posterior probability equal to 1\/3 are plotted in solid\nand dashed line. The fifty points selected based on the proposed criterion\nare circled and it is clear that these sit close to the decision boundaries. (b)\nThe averaged predictive performance (percentage predictions correct) over\ntwenty random starts (dashed line denotes random sampling and solid line\ndenotes informative sampling) are shown with the slowest converging plot\ncharacterizing what is achieved under a random sampling strategy.\n6.4Large Scale Example of Sparse GP Multi-Class\nClassification\nThe Isolet11dataset comprises of 6238 examples of letters from the alphabet\n(26) spoken in isolation by 30 individual speakers, and each letter is rep-\nresented by 617 features. An independent collection of 1559 spoken letters\nis available for classification test purposes. The best reported test perfor-\nmance over all 26 classes of letter was 3.27% error achieved using 30-bit\nerror-correcting codes with an artificial neural network. Here we employ\na single RBF covariance kernel with a common inverse length-scale of 0.001\n(further fine tuning is of course possible) and a maximum of 2000 points from\n11The dataset is available from http:\/\/www.ics.uci.edu\/\u223cmlearn\/databases\/\nisolet\n20"},{"page":22,"text":"the available 6238 are to be employed in the sparse multi-class GP classifier.\nAs in the previous example data is standardized, both random and infor-\nmative sampling strategies were employed, with the results given in Figure\n(6) illustrating the superior convergence of an informative sampling strategy.\nAfter including 2000 of the available 6238 samples in the model, under the\ninformative sampling strategy, a test error rate of 3.52% is achieved. We are\nunaware of any multi-class GP classification method which has been applied\nto such a large scale problem both in terms of data samples available and\nthe number of classes.\n050010001500\n\u22126000\n\u22125000\n\u22124000\n\u22123000\n\u22122000\n\u22121000\n0\nNumber of Points Included\n(a)\nLog Predictive Likelihood\n05001000 1500\n30\n40\n50\n60\n70\n80\n90\n100\nNumber of Points Included \nPercentage Predictions Correct\n(b)\nFigure 6: (a) The predictive likelihood computed on held-out data for both\nrandom sampling (solid line with \u2019+\u2019 markers) and informative sampling\n(solid line with \u2019?\u2019 markers). The predictive likelihood is computed once\nevery 50 inclusion steps. (b) The predictive performance (percentage predic-\ntions correct) achieved for both random sampling (solid line with \u2019+\u2019 markers)\nand informative sampling (solid line with \u2019?\u2019 markers)\nA recent paper (Qi, et al 2004) has presented an empirical study of ARD\nwhen employed to select basis functions in Relevance Vector Machine (RVM)\n(Tipping, 2000) classifiers. It was observed that a reliance on the marginal\nlikelihood alone as a criterion for model identification ran the risk of overfit-\nting the available data sample by producing an overly sparse representation.\nThe authors then employ an approximation to the leave-one-out error, which\nemerges from the EP iterations, to counteract this problem. For Bayesian\nmethods which rely on optimising in-sample marginal likelihood (or an appro-\npriate bound) then great care has to be taken when setting the convergence\n21"},{"page":23,"text":"tolerance which determines when the optimisation routine should halt. How-\never, in the experiments we have conducted this phenomenon did not appear\nto be such a problem with the exception of one dataset as will be discussed\nin the following section.\n6.5Comparison with Multi-class SVM\nTo briefly compare the performance of the proposed approach to multi-class\nclassification with a number of multi-class SVM methods we consider the\nrecent study of (Duan and Keerthi, 2005). In that work four forms of multi-\nclass classifier were considered; WTAS - one-versus-all SVM method with\nwinner takes all class selection; MWVS - one-versus-one SVM with a maxi-\nmum votes class selection strategy; PWCK - one-versus-one SVM with prob-\nabilistic outputs employing pairwise coupling (see (Duan and Keerthi, 2005)\nfor details); PWCK - Kernel logistic regression with pairwise coupling of\nbinary outputs. Five multi-class datasets from the UCI Machine Learning\nData Repository were employed: ABE (16 dimensions & 3 classes) - a subset\nof the Letters dataset using the letters \u2019A\u2019, \u2019B\u2019 & \u2019E\u2019; DNA (180 dimensions\n& 3 classes); SAT (36 dimensions & 6 classes) - Satellite Image; SEG (18\ndimensions & 7 classes) - Image Segmentation; WAV (21 dimensions & 3\nclasses) - Waveform. For each of these, (Duan and Keerthi, 2005) created\ntwenty random partitions into training and test sets for three different sizes of\ntraining set, ranging from small to large. Here we consider only the smallest\ntraining set sizes.\nIn (Duan and Keerthi, 2005) thorough and extensive cross-validation was\nemployed to select the length-scale parameters (single) of the Gaussian kernel\nand the associated regularisation parameters which were used in each of the\nSVM\u2019s. The proposed importance sampler is employed to obtain the poste-\nrior mean estimates for both single and multiple length scales (VBGPS - Vari-\national Bayes Gaussian Process Classification - Single length scale) (VBGPM\n- Variational Bayes Gaussian Process Classification - Multiple length scales)\nfor a common GP covariance shared across all classes. We monitor the bound\non the marginal and consider convergence has been achieved when less than a\n1% increase in the bound is observed for all datasets except for ABE where a\n10% convergence criterion was employed due to a degree of overfitting being\nobserved after this point. In all experiments, data was standardised to have\nzero mean and unit variance.\nThe percentage test errors averaged over each of the 20 data splits (mean\n22"},{"page":24,"text":"WTAS\n9.4\u00b10.5\n10.2\u00b11.3\n1.9\u00b10.8\n17.2\u00b11.4\n11.1\u00b10.6\nMWVS\n7.9\u00b11.2\n9.9\u00b10.9\n1.9\u00b10.6\n17.8\u00b11.4\n11.0\u00b10.7\nPWCP\n7.9\u00b11.2\n8.9\u00b10.8\n1.8\u00b10.6\n16.4\u00b11.4\n10.9\u00b10.4\nPWCK\n7.5\u00b11.2\n9.7\u00b10.7\n1.8\u00b10.6\n15.6\u00b11.1\n11.2\u00b10.6\nVBGPM\n?7.8\u00b11.5\n74.0\u00b10.3\n?1.8\u00b10.8\n25.2\u00b11.2\n12.0\u00b10.4\nVBGPS\n11.5\u00b11.2\n13.3\u00b11.3\n2.4\u00b10.8\n?15.6\u00b10.7\n12.1\u00b10.4\nSEG\nDNA\nABE\nWAV\nSAT\nTable 2: SVM & Variational Bayes GP Multi-class Classification Comparison\n\u00b1 standard deviation) are reported in Table. 2. For each dataset the clas-\nsifiers which obtained the lowest prediction error and whose performances\nwere indistinguishable from each other at the 1% significance level using a\npaired t-test are highlighted in bold. An asterisk, ?, highlights the cases\nwhere the proposed GP-based multi-class classifiers were part of the best\nperforming set. We see that in three of the five datasets performance equal\nto the best performing SVM\u2019s is achieved by one of the GP-based classifiers\nwithout recourse to any cross-validation or in-sample tuning with comparable\nperformance being achieved for SAT & DNA. The performance of VBGPM\nis particularly poor on DNA and this is possibly due to the large number\n(180) of binary features.\n7Conclusion & Discussion\nThe main novelty of this work has been to adopt the data augmentation strat-\negy employed in obtaining an exact Bayesian analysis of binary & multino-\nmial probit regression models for GP based multi-class (of which binary is a\nspecific case) classification. Whilst a full Gibbs sampler can be straightfor-\nwardly obtained from the joint likelihood of the model, approximate inference\nemploying a factored form for the posterior is appealing from the point of\nview of computational effort & efficiency. The variational Bayes procedures\ndeveloped provide simple iterations due to the inherent decoupling effect of\nthe auxiliary variable between the GP components related to each class. The\nscaling is still of course dominated by an O(N3) term due to the matrix in-\nversion required in obtaining the posterior mean for the GP variables and\nthe repeated computing of multivariate Gaussians required for the weights\nin the importance sampler. However with the simple decoupled form of the\n23"},{"page":25,"text":"posterior updates we have shown that ADF based online and sparse estima-\ntion yields a full multi-class IVM which has linear scaling in the number of\nclasses and the number of available data points and this is achieved in a most\nstraightfoward manner. An empirical comparison with full MCMC suggests\nthat the variational approximation proposed is superior to a Laplace approx-\nimation. Further ongoing work includes an investigation into the possible\nequivalences between EP and variational based approximate inference for\nthe multi-class GP classification problem as well as developing a variational\ntreatment to GP based ordinal regression (Chu and Ghahramani, 2005).\nAcknowledgments\nThis work is supported by Engineering & Physical Sciences Research Council\ngrants GR\/R55184\/02 & EP\/C010620\/1. The authors are grateful to Chris\nWilliams, Jim Kay and Joaquin Qui\u02dc nonero Candela for motivating discus-\nsions regarding this work. In addition the comments and suggestions made\nby the anonymous reviewers helped to significantly improve the manuscript.\n8 Appendix I\n8.1\nWe employ the shorthand Q(\u03d5) =?\nQ(M) \u221d exp\n\u221d exp\n?\nand so we have\nQ(M)\nkQ(\u03d5k) in the following.\nConsider the Q(M) component of the approximate posterior. We have\n?\nEQ(Y)Q(\u03d5)\n?\nEQ(Y)Q(\u03d5)\n?\nkN\n??\nn\n?\nklogNyk(mk,I) + logNmk(0|C\u03d5k)\n??\nK\n?\nand ? mk = \u03a3k? yk. Now each element of C\u22121\n24\nklogp(ynk|mnk) + logp(mk|\u03d5k)\n??\n?? ??\n\u221d\n?yk(mk,I)Nmk\n0,C\u22121\n\u03d5k\n?\u22121?\nQ(M) =\nK\n?\nk=1\nQ(mk) =\nk=1\nNmk(? mk,\u03a3k)\nwhere \u03a3k =\na nonlinear function of \u03d5kand so, if considered appropriate, a first-order\n?\nI +?\nC\u22121\n\u03d5k\n?\u22121\n\u03d5kis"},{"page":26,"text":"approximation can be made to the expectation of the matrix inverse such\nthat?\n8.2Q(Y)\nC\u22121\n\u03d5k\u2248 C\u22121\n?\u03d5kin which case \u03a3k= C\n?\u03d5k\n?I + C\n?\u03d5k\n?\u22121.\nQ(Y) \u221d exp\n?\nEQ(M)\n??\nnNyn(? mn,I)\u03b4(yni> ynk\u2200 k ?= i)\u03b4(tn= i)\n??\nnlogp(tn|yn) + logp(yn|mn)\n??\n\u221d exp\n\u221d\nnlogp(tn|yn) + logNyn(? mn|I)\n?\n?\nEach ynis then distributed as a truncated multivariate Gaussian such that\nfor tn= i the ithdimension of ynis always the largest and so we have,\nQ(Y) =\nN\n?\nn=1\nQ(yn) =\nN\n?\nn=1\nNtn\nyn(? mn,I)\nwhere Ntn\ndimension indicated by the value of tnis always the largest.\nThe posterior expectation of each ynis now required. Note that\nyn(.,.) denotes a K-dimensional Gaussian truncated such that the\nQ(yn) = Z\u22121\nn\n?\nk\nNynk(? mnk,1)\nwhere Zn= Pr(yn\u2208 C) and C = {yn: ynj< yni,j ?= i}. Now\nZn = Pr(yn\u2208 C)\n=\n\u2212\u221e\n??\nWhere u is a standardised Gaussian random variable such that p(u) =\n?+\u221e\nNyni(? mni,1)\n\u03a6(u + ? mni\u2212 ? mnj)\n?\nj?=i\n?yni\n\u2212\u221e\nNynj(? mnj,1)dynidynj\n= Ep(u)\nj?=i\n?\n25"},{"page":27,"text":"Nu(0,1). For all k ?= i the posterior expectation follows as\n?+\u221e\nj=1\n?+\u221e\n?\n? ynk = Z\u22121\n= Z\u22121\nn\n\u2212\u221e\nynk\nK\n?\nNynj(? mnj,1)dynj\nynkNynk(? mnk,1)\nnEp(u)\nNu(? mnk\u2212 ? mni,1)\nn\n\u2212\u221e\n?yni\n\u2212\u221e\n?\nj?=i,k\nNyni(? mni,1)\u03a6(yni\u2212 ? mnj)dynidynk\n?\n=\n? mnk\u2212 Z\u22121\nj?=i,k\n\u03a6(u + ? mni\u2212 ? mnj)\n?\nThe required expectation for the ithcomponent follows as\n? yni = Z\u22121\n=\nn\n?+\u221e\n\u2212\u221e\nyniNyni(? mni,1)\nnEp(u)\n?\nj?=i\n\u03a6(yni\u2212 ? mnj)dyni\n\u03a6(u + ? mni\u2212 ? mnj)\n? mni+ Z\u22121\n? mni+\n?\nu\n?\nj?=i\n?\n=\n?\nk?=i\n(? mnk\u2212 ? ynk)\nThe final expression in the above follows from noting that for a random\nvariable u \u223c N(0,1) and any differentiable function g(u) then E{ug(u)} =\nE{g?(u)} in which case\n?\nj?=ik?=i\nEp(u)\nu\n?\n\u03a6(u + ? mni\u2212 ? mnj)\nQ(\u03d5k)\n?\n=\n?\nEp(u)\n?\nNu(? mnk\u2212 ? mni,1)\n?\nj?=i\n\u03a6(u + ? mni\u2212 ? mnj)\n?\n8.3\nFor each k we obtain the posterior component\nQ(\u03d5k) \u221d exp?EQ(mk)Q(\u03c8k)(logp(mk|\u03d5k) + logp(\u03d5k|\u03c8k))?\n= ZkN\n?mk(0|C\u03d5k)\n?\nd\nExp\u03d5kd(?\u03c8kd)\nwhere Zkis the corresponding normalising constant for each posterior which\nis unobtainable in closed form. As such the required expectations can be\nobtained by importance sampling.\n26"},{"page":28,"text":"8.4Q(\u03c8k)\nThe final posterior component required is\nQ(\u03c8k) \u221d exp?EQ(\u03d5k)(logp(\u03d5k|\u03c8k) + logp(\u03c8k|\u03b1k))?\n\u221d\nd\n?\nand the required posterior mean values follow as?\u03c8kd=\n9 Appendix II\n?\nExp\n?\u03d5kd(\u03c8kd)\u0393\u03c8kd(\u03c3k,\u03c4k)\n=\nd\n\u0393\u03c8kd(\u03c3k+ 1,\u03c4k+ ? \u03d5kd)\n\u03c3k+1\n\u03c4k+\n?\u03d5kd\nThe predictive distribution for a new point xnewcan be obtained by firstly\nmarginalising the associated GP random variables such that\n?\nK\n?\nK\n?\n?\nthe predictive posterior for the auxilliary variable ynewthe appropriate conic\ntruncation of this spherical Gaussian yields the required distribution P(tnew=\nk|xnew,X,t) as follows. Using the following shorthand P(tnew= k|ynew) =\np(ynew|xnew,X,t) =p(ynew|mnew)p(mnew|xnew,X,t)dmnew\n?\n=\nk=1\nNmnew\nk\n(ynew\nk\n,1)Nmnew\nk\n(? mnew\nk\n,? \u03c3new\nk\n)dmnew\nk\n=\nk=1\nNynew\nk\n(? mnew\nk,newis employed. Now that we have\nk\n,? \u03bdnew\nk\n)\nwhere the shorthand ? \u03bdnew\nk\n=\n1 +?\u03c32\n27"},{"page":29,"text":"\u03b4(ynew\nk\n> ynew\ni\n\u2200 i ?= k)\u03b4(tnew= k) \u2261 \u03b4k,newthen\n?\n=\nCk\n?\nP(tnew= k|xnew,X,t) =P(tnew= k|ynew)p(ynew|xnew,X,t)dynew\n?\np(ynew|xnew,X,t)dynew\n?\n??\n=\u03b4k,new\nK\nk=1\nNynew\n?\nk\n(? mnew\n?u? \u03bdnew\nk\n,? \u03bdnew\nk\n)dynew\nk\n= Ep(u)\nj?=k\n\u03a6\n1\n? \u03bdnew\nj\nk\n+ ? mnew\nk\n\u2212 ? mnew\nj\n???\nThis is the probability that the auxiliary variable ynewis in the cone Ckso\n?K\n=\nk=1P(tnew= k|xnew,X,t) =\n?K\n?\nk=1\n?\nCk\np(ynew|xnew,X,t)dynew\nRKp(ynew|xnew,X,t)dynew= 1\nthus yielding a properly normalised posterior distribution over classes 1,\u00b7\u00b7\u00b7 K.\n10Appendix III\nThe variational bound conditioned on the current values of \u03d5k,\u03c8k,\u03b1k(i.e.\nassuming these are fixed values) can be obtained in the following manner\nusing the expansion of the relevant components of the lower-bound.\n?\n?\n?\n?\nexpanding each component in turn obtains\n?\nk\n?\nEQ(M){logp(mk|X,\u03d5k)} \u2212\nn\nEQ(M)Q(Y){logp(ynk|mnk)} +(17)\nk\n(18)\nk\nEQ(mk){logQ(mk)} \u2212\n(19)\nn\nEQ(yn){logQ(yn)}\n(20)\n\u22121\n2\nk\n?\nn\n??y2nk+?\nm2nk\u2212 2? ynk? mnk\n28\n?\n\u2212NK\n2\nlog2\u03c0(21)"},{"page":30,"text":"\u2212\n1\n2\n?\n?\n\u2212NK\nk\nlog|C\u03d5k| \u22121\n?\n2\n?\n?\nk\n? mT\n\u2212NK\nkC\u22121\n\u03d5k? mk\nlog2\u03c0\n\u2212\n1\n2\nk\ntraceC\u22121\n\u03d5k\u03a3k\n2\n(22)\n\u2212NK\n22\nlog2\u03c0 \u22121\n2\n?\nk\nlog|\u03a3k|\n(23)\n\u22121\n2\n?\nk\n?\nn\n??y2nk+ ? m2\nnk\u2212 2? ynk? mnk\n?\n\u2212\n?\nn\nlogZn\u2212N\n2log2\u03c0(24)\nCombining and manipulating (21,22,23, and 24) gives the following ex-\npression for the lower-bound.\n\u2212NK\n?\n1\n2\n2\nlog2\u03c0 +N\n2log2\u03c0 +NK\n?\n?\n2\n\u22121\n2\n?\nk\ntrace{\u03a3k} \u2212\n?\nlogZn\n1\n2\nk\n? mT\nlog|C\u03d5k| +1\nkC\u22121\n\u03d5k? mk\u22121\n2\nk\ntrace\n?\nC\u22121\n\u03d5k\u03a3k\n\u2212\n?\nk\n2\nk\nlog|\u03a3k| +\n?\n?\nn\nwhere each Zn= Ep(u)\n??\nj?=i\u03a6(u + ? mni\u2212 ? mnj).\n11 Appendix IV\nDetails of the Gibbs sampler required to obtain samples from the posterior\np(\u0398|t,X,\u03a6,\u03b1) now follow. From the definition of the joint likelihood (Equa-\ntion 2) it is straightforward to see that the conditional distribution for each\nyn| mnwill be a truncated Gaussian defined in the cone Ctn, centered at mn\nwith identity covariance and denoted by Ntn\neach mk| ykis multivariate Gaussian with covariance \u03a3k= C\u03d5k(I+C\u03d5k)\u22121\ny(mn,I). The distribution for\n29"},{"page":31,"text":"and mean \u03a3kyk. Thus the Gibbs sampler, for each n and k, takes the simple\nform below\ny(i)\nm(i+1)\nk\nn| m(i\u22121)\nn\n\u223c Ntn\n\u223c Nm(\u03a3ky(i)\ny(m(i\u22121)\nn\n,I)\n| y(i)\nkk,\u03a3k)\nwhere the superscript (i) denotes the ithsample drawn. The dominant\nscaling will be O(KN3) per sample draw.\nlikelihood for a new data point defined as\n??\nthe predictive distribution12is then obtained from\n?\nA Monte-Carlo estimate of the above required marginal posterior expecta-\ntion can be obtained by drawing samples from the full posterior distribu-\ntion, p(\u0398|t,X,\u03a6,\u03b1), using the above sampler. Then for each \u0398(i)sam-\npled an additional set of samples mnew,s\nk\nmnew,s\nk\n| y(i)\nand the associated variance is \u03c32\nThe approximate predictive distribution can then be obtained by the follow-\ning Monte-Carlo estimate\n??\nAn additional Metropolis-Hastings sub-sampler can be employed within the\nabove Gibbs sampler to draw samples from the posterior p(\u0398,\u03a6|t,X,\u03b1) if\nthe covariance function hyper-parameters are to be integrated out.\nWith the multinomial probit\nP(tnew= k|mnew) = Ep(u)\nj?=k\n\u03a6(u + mnew\nk\n\u2212 mnew\nj\n)\n?\nP(tnew= k|xnew,X,t) =\nP(tnew= k|mnew)p(mnew|xnew,X,t) dmnew\nare drawn, such that for each k,\nk,new), where \u00b5new,i\nk\n= (y(i)\nk,new= cnew\nk\n\u223c Nm(\u00b5new,i\nk\n,\u03c32\nk)T(I + C\u03d5k)\u22121Cnew\n\u03d5k)T(I + C\u03d5k)\u22121Cnew\n\u03d5k\n\u03d5k.\n\u03d5k\u2212 (Cnew\n1\nNsamps\nNsamps\n?\ns=1\nEp(u)\nj?=k\n\u03a6(u + mnew,s\nk\n\u2212 mnew,s\nj\n)\n?\n12 Appendix V\nThe Laplace approximation requires the Hessian matrix of second-order deriv-\natives of the joint log-likelihood with respect to each mn. The derivatives of\n12Conditioning on \u03a6 and \u03b1 is implicit.\n30"},{"page":32,"text":"the noise component, logP(tn= k|mn) = logEp(u)\nfollow as below, where we denote expectation with respect to a Gaussian\ntruncated in the cone Ckas ENk\n??\nj?=k\u03a6(u + ? mnk\u2212 ? mnj)\n?\n,\ny{\u00b7}\n\u2202\n\u2202mni\nlogP(tn= k|mn) =\n1\nP(tn= k|mn)\n= ENk\n?\nCk\n(yni\u2212 mni)Nyn(m,I)dy\ny{yni} \u2212 mni\nand\n\u22022\n\u2202mnj\u2202mnilogP(tn= k|mn) = ENk\ny{yniynj} \u2212 ENk\ny{yni}ENk\ny{ynj} \u2212 \u03b4ij\nThis then defines an NK \u00d7 NK dimensional Hessian matrix which, unlike\nthe Hessian of the multinomial-logit counterpart, cannot be decomposed into\na diagonal plus multiplicative form (refer to (Williams and Barber, 1998) for\ndetails), due to the cross-diagonal elements ENk\nmatrix inversions of the Newton step and those required to obtain the pre-\ndictive covariance will operate on a full NK \u00d7 NK matrix.\ny{yniynj}, and so the required\nReferences\nAlbert, J. H., and Chib, S. (1993). Bayesian analysis of binary and poly-\nchotomous response data. Journal of the American Statistcial Association,\n88(422):669\u2013679.\nBeal, M. (2003). Variational Algorithms for Approximate Bayesian Inference.\nPhD thesis, University College London.\nChu, W., and Ghahramani, Z. (2005). Gaussian Processes for Ordinal Re-\ngression. Journal of Machine Learning Research, 6:1019\u20131041.\nCsato, L., Fokue, E., Opper, M., Schottky, B., and Winther, O. (2000).\nEfficient Approaches to Gaussian Process Classification. In Advances in\nNeural Information Processing Systems 12. Eds: Solla, S. A., Leen, T. K.,\nand M\u00a8 uller, K. R., pp 252\u2013257.\nCsato, L., and Opper, M. (2002). Sparse online gaussian processes. Neural\nComputation, 14:641\u2013668.\n31"},{"page":33,"text":"Duan, K., and Keerthi, S. (2005).\nMethod? An Empirical Study. Proceedings of the Sixth International\nWorkshop on Multiple Classifier Systems, 278\u2013285.\nWhich is the Best Multi-class SVM\nGibbs, M. N., and MacKay, D. J. C. (2000). Variational gaussian process\nclassifiers. IEEE Transactions on Neural Networks, 11(6):1458 \u2013 1464.\nGirolami, M., and Rogers, S. (2005). Hierarchic Bayesian models for kernel\nlearning. Proceedings of the 22nd International Conference on Machine\nLearning, pages 241\u2013248.\nPlatt, J. C., Cristianini, N., and Shawe-Taylor, J. (2000). Large margin dags\nfor multi-class classification. In Advances in Neural Information Processing\nSystems 12, pages 547\u2013553.\nJordan, M. I., Ghahramani, Z., Jaakkola, T. S., and Saul, L. K. (1999).\nAn introduction to variational methods for graphical models. Machine\nLearning, 37:183\u2013233.\nKim,\nfiers.\nhttp:\/\/home.postech.ac.kr\/\u223cgrass\/publication\/\nKuss, M., and Rasmussen, C. E. (2005) Assessing Approximate Inference\nfor Binary Gaussian Process Classification . Journal of Machine Learning\nResearch, 6:1679\u20131704.\nH.C.(2005).Bayesian andEnsemble KernelClassi-\nPhD thesis, Pohang University of Science and Technology.\nLawrence, N. D., Milo, M., Niranjan, M., Rashbass, P., and Soullier, S.\n(2004). Reducing the variability in cDNA microarray image processing by\nBayesian inference. Bioinformatics, 20(4):518\u2013526.\nLawrence, N. D., Seeger, M., and Herbrich, R. (2003). Fast sparse gaussian\nprocess methods: The informative vector machine. In Thrun, S., Becker,\nS., and Obermayer, K., editors, Advances in Neural Information Processing\nSystems 15. MIT Press.\nLawrence, N. D, Platt, J. C., and Jordan, M. I. (2005). Extensions of the\ninformative vector machine. In Winkler, J., Lawrence, N. D., and Niran-\njan, M., (eds), Proceedings of the Sheffield Machine Learning Workshop,\nSpringer-Verlag, Berlin.\n32"},{"page":34,"text":"MacKay, D. J. C (2003). Information Theory, Inference, and Learning Al-\ngorithms. Cambridge University Press.\nMinka, T. P. (2001). A family of algorithms for approximate Bayesian infer-\nence. PhD thesis, MIT.\nNeal, R. (1998). Regression and classification using gaussian process priors.\nIn Dawid, A. P., Bernardo, M., Berger, J. O., and Smith, A. F. M., editors,\nBayesian Statistics 6, pages 475\u2013501. Oxford University Press.\nOpper, M., and Winther, O. (2000). Gaussian processes for classification:\nMean field algorithms. Neural Computation, 12:2655\u20132684.\nQi, Y., Minka, T. P., Picard, R. W., and Ghahramani, Z. (2004). Predic-\ntive automatic relevance determination by expectation propagation. In\nGreiner, R., and Schuurmans, D., editors, Proceedings of the twenty-first\nInternational Conference on Machine Learning.\nQuinonero-Candela, J., and Winther, O. (2003).\nprocesses. In Becker, S., Thrun, S., and Obermayer, K., editors, Neural\nInformation Processing Systems 15. MIT Press.\nIncremental gaussian\nSeeger, M., Williams, C. K. I., and Lawrence, N. D. (2003). Fast forward\nselection to speed up sparse gaussian process regression. In Bishop, C. M.,\nand Frey, B. J., editors, Proceedings of the Ninth International Workshop\non Artificial Intelligence and Statistics.\nSeeger, M., and Jordan, M. I. (2004). Sparse Gaussian Process Classification\nWith Multiple Classes. Department of Statistics, Technical Report 661,\nUniversity of California, Berkeley.\nSeeger, M. (2000). Bayesian Model Selection for Support Vector Machines,\nGaussian Processes and Other Kernel Classifiers.\nProcessing Systems, 12, 603-609.\nNeural Information\nTipping, M. (2000). Sparse Bayesian learning and the relevance vector ma-\nchine. Journal of Machine Learning Research, 1:211\u2013244.\nWang, B., and Titterington, D. M. (2004). Convergence and asymptotic\nnormality of variational Bayesian approximations for exponential family\nmodels with missing values. Technical Report, No.04-02, Department of\nStatistics, University of Glasgow.\n33"},{"page":35,"text":"Williams, C. K. I, and Barber, D. (1998).\ngaussian processes. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 20(12):1342\u20131351.\nBayesian classification with\nWilliams, C. K. I., and Rasmussen, C. E. (1996). Gaussian processes for\nregression. In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E.,\neditors, Advances in Neural Processing Systems 8, pages 598\u2013604. MIT\nPress, Cambridge, MA.\nWilliams, C. K. I, and Seeger, M. (2001). Using the Nystrom method to\nspeed up kernel machines. In Leen, T. K., Dietterich, T. G., and Tresp,\nV., editors, Advances in Neural Information Processing Systems 13, pages\n682-688, Cambridge, MA, MIT Press.\n34"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Simon_Rogers\/publication\/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors\/links\/00b7d516d1ebfbe178000000.pdf","widgetId":"rgw27_56aba05bc3db3"},"id":"rgw27_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=220499817&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw28_56aba05bc3db3"},"id":"rgw28_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=220499817&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":220499817,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"00b7d516d1ebfbe178000000","name":"Simon Rogers","date":null,"nameLink":"profile\/Simon_Rogers","filename":"download.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Simon_Rogers\/publication\/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors\/links\/00b7d516d1ebfbe178000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Simon_Rogers\/publication\/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors\/links\/00b7d516d1ebfbe178000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"d476cafb424ed0857ccc7f280e977563","showFileSizeNote":false,"fileSize":"462.59 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"00b7d516d1ebfbe178000000","name":"Simon Rogers","date":null,"nameLink":"profile\/Simon_Rogers","filename":"download.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Simon_Rogers\/publication\/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors\/links\/00b7d516d1ebfbe178000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Simon_Rogers\/publication\/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors\/links\/00b7d516d1ebfbe178000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"d476cafb424ed0857ccc7f280e977563","showFileSizeNote":false,"fileSize":"462.59 KB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=5elbCW35s-6sNxPfSDJxSY0JTnD8F39bE1ZZWNjFeTsq4X8J2wvJNinbT1PfdxVpMx4Pp43x721DnR06_wSGDQ.-1lYscsHcKHrwr9F7W40ZY6i-j_YKu2IWbpT65HIdpSePfnJGCFu3Ohdkdzo3nSqtqtZIC8VCoTod7GwYs9GOA","clickOnPill":"publication.PublicationFigures.html?_sg=e2FVfkx72szu4G3-pCEIYnsYrxrucfnzIMoUrH9v-OfZML528ydKxkanSL90-V7GIY4ep_Pj1FI8ScmxLgvOgQ.jVBAKhut4KbTURCIiEoJ230Jr6gNpAPGDlFzHnvJkvoKJGOVrs_sB7mKShfJWVbp-rBH1xBdxVrC_KUczI8qHw"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FSimon_Rogers%2Fpublication%2F220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors%2Flinks%2F00b7d516d1ebfbe178000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=FCDorjr9UjGPWHygqkSMynOHrMVFGDXPXQaf_0uvxZCEDhRR5Xi-hfNaeIXyw9rOT2DFzXU6DVkEvkC-Gw59FQ","urlHash":"031b42fa6303e310022432cdc18d675f","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=fOUZsOm-VzfxT8VQYwOPHeQhaKdteWeECBTdx6Yb1qIU3NonDmit7ykfsytZ2yM_f495GfbI2tLAxuKl-cgzmcWKlqmh6qrnGCQBdRY_-tM.3Zc2v4IV1tPZ2ICoSCcrpseq5amWnHFgWGDxRLJDP9oDyz1MRFDD8lGt1n9BIcVM4K3BDSB_aLBZ4Wb8ZwGy7A.iBhq4JuDx-mZCZJhFkKaxT5sCPKN6hplvb-WxCuNMWAMM-h65d6AXmeIXjk7pSOLiCw-nBXVSVqlJMGC7Tpi2A","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"00b7d516d1ebfbe178000000","trackedDownloads":{"00b7d516d1ebfbe178000000":{"v":false,"d":false}},"assetId":"AS:104447795531783@1401913705600","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":220499817,"commentCursorPromo":null,"widgetId":"rgw30_56aba05bc3db3"},"id":"rgw30_56aba05bc3db3","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FSimon_Rogers%2Fpublication%2F220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors%2Flinks%2F00b7d516d1ebfbe178000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A104447795531783%401401913705600&publicationUid=220499817&linkId=00b7d516d1ebfbe178000000&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=uLjdjcbPyxv11iXRkaxXlteaQC8ReBURncPqX_LEG6TfxYXXOiznphSLAcjsM58WdZu-UFUmk1-C4mi00sNfK38HqHxOV1rI3fbQ9vsFo1g.Fdijfoz5X-87S_VB6nUav-SeuyB9q7jfJhc-ZbK0AchUe9dN26ZDQW1gtiR9TgfEMuTgCpbYPEFdE6M32M6VZw.NZJIdNiPlUrHxH6rzqeG-2Osodez-FlApRmCm13o3WpD-JQozjllYjGRmEb5o6chbLvAT-K_4ka4_UNjHciszA","publicationUid":220499817,"trackedDownloads":{"00b7d516d1ebfbe178000000":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw32_56aba05bc3db3"},"id":"rgw32_56aba05bc3db3","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw33_56aba05bc3db3"},"id":"rgw33_56aba05bc3db3","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw34_56aba05bc3db3"},"id":"rgw34_56aba05bc3db3","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw35_56aba05bc3db3"},"id":"rgw35_56aba05bc3db3","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw36_56aba05bc3db3"},"id":"rgw36_56aba05bc3db3","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw31_56aba05bc3db3"},"id":"rgw31_56aba05bc3db3","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw29_56aba05bc3db3"},"id":"rgw29_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56aba05bc3db3"},"id":"rgw2_56aba05bc3db3","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":220499817},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=220499817&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56aba05bc3db3"},"id":"rgw1_56aba05bc3db3","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"zX6VNvV8yxTgR0z1rEJjSywgr2VCGC+BlRqpwT\/QQJZwXWN0JG5hKujD4nsNL1dwsX3rV+lgHEJO0Ym30tiRkHd2u6pXxQLcTDclp5K6V3gY\/YVS218nVoroUsnHWuMcF5T6izi0m2LZBliZAfOMTZUnuvNvMiYu0yOoOHdXxB9Q5Ypjy5cjXNNC7q3xkUGPdjrXoOnxPUlbIMw57j0IRWdGqp7QsINiCnagriYq8YcPSTKDdhhnEcAdzBbY4C\/Kj\/nPPSsXrYl8znjpoEpaHnDy\/\/YtL6UIVJaaSch6img=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors\" \/>\n<meta property=\"og:description\" content=\"Abstract It is well known in the statistics literature that augmenting binary and polychotomous response models with Gaussian latent variables enables exact Bayesian analysis via Gibbs sampling...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors\/links\/00b7d516d1ebfbe178000000\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors\" \/>\n<meta property=\"rg:id\" content=\"PB:220499817\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1162\/neco.2006.18.8.1790\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors\" \/>\n<meta name=\"citation_author\" content=\"Mark Girolami\" \/>\n<meta name=\"citation_author\" content=\"Simon Rogers\" \/>\n<meta name=\"citation_publication_date\" content=\"2006\/08\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"Neural Computation\" \/>\n<meta name=\"citation_issn\" content=\"0899-7667\" \/>\n<meta name=\"citation_volume\" content=\"18\" \/>\n<meta name=\"citation_issue\" content=\"8\" \/>\n<meta name=\"citation_firstpage\" content=\"1790\" \/>\n<meta name=\"citation_lastpage\" content=\"1817\" \/>\n<meta name=\"citation_doi\" content=\"10.1162\/neco.2006.18.8.1790\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Simon_Rogers\/publication\/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors\/links\/00b7d516d1ebfbe178000000.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-0d2bb7ae-4ad6-4f06-a217-f504070437e4","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":436,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw37_56aba05bc3db3"},"id":"rgw37_56aba05bc3db3","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-0d2bb7ae-4ad6-4f06-a217-f504070437e4", "b7e8e9954e4013e1c58f5dd4b40ec8e8af4beb3d");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-0d2bb7ae-4ad6-4f06-a217-f504070437e4", "b7e8e9954e4013e1c58f5dd4b40ec8e8af4beb3d");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw38_56aba05bc3db3"},"id":"rgw38_56aba05bc3db3","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors","requestToken":"VQQJBS07hZYEjxo4ejjwLkJD6t73vr1HWHsuKPmYDlDmeQNRa2zcvXZ88jZ+9IPWYYMIUpGtQ1InS\/r2OqKgnjrkvAerRWHMeU\/5KwTyM4gvLCD+Gqc9uhblVzaaQ7zN7Xvcc8LiRK6qhg0+bNsvd9N6+GGocaNxpNxLt98uo2KOa9PIqw3BCIHeBBUOFNrdm+\/Xm5nFLqKBpOi2S8eggQaia9w\/TOoEmSnALrYfJPyyd48LjRCVA9YO7lai\/0KrLL7KEuvt6cAg\/tAPqL1kKiTK\/lzWGw8Rmnp285oPJiU=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=c6xQRGkMWIoTmcvLQFCcF-4OHnikRvcw4O_avNfwH_Ebo7Oy7v3qu6Up33F_bWwz","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjIwNDk5ODE3X1ZhcmlhdGlvbmFsX0JheWVzaWFuX011bHRpbm9taWFsX1Byb2JpdF9SZWdyZXNzaW9uX3dpdGhfR2F1c3NpYW5fUHJvY2Vzc19QcmlvcnM%3D","signupCallToAction":"Join for free","widgetId":"rgw40_56aba05bc3db3"},"id":"rgw40_56aba05bc3db3","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw39_56aba05bc3db3"},"id":"rgw39_56aba05bc3db3","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw41_56aba05bc3db3"},"id":"rgw41_56aba05bc3db3","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
