<!DOCTYPE html> <html lang="en" class="" id="rgw38_56aba06f4cb75"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="JE/TofzPXxlynRSEqG4SNsCyYvp0O1TmHwPw7KaByIF+oAVtV9Fp61VR2cA4/sQXztyYNZiFJEhkE90IIox4lyPuF4qlVVwcCM0LzrhAH5IYlYOcZ1Dliy0b/oArFYiaOC5mHaBhoivkvcvfUVyCKvx862HyKtgZ+4YyIh7cyZD3nZhMZuKDGu+MhcuTfEdRwMs/shfVd7gw0zRujTq9v2d0XWJn7rcFozU1mfpg8lWjLedZoym6aFXnezWDCUeMx7KRSAqlnBffAmzW6OYDQbum9KpSsC4pc81T7ZmDnXE="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-34df5c8d-b0e1-47b7-a3ea-4f36f9dab5e4",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/234779817_Sparse_Spectrum_Gaussian_Process_Regression" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Sparse Spectrum Gaussian Process Regression" />
<meta property="og:description" content="We present a new sparse Gaussian Process (GP) model for regression. The key novel idea is to sparsify the spectral representation of the GP. This leads to a simple, practical algorithm for..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/234779817_Sparse_Spectrum_Gaussian_Process_Regression/links/0c96051ae1c817cdb7000000/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/234779817_Sparse_Spectrum_Gaussian_Process_Regression" />
<meta property="rg:id" content="PB:234779817" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Sparse Spectrum Gaussian Process Regression" />
<meta name="citation_author" content="Miguel LÃ¡zaro-Gredilla" />
<meta name="citation_author" content="Carl Edward Rasmussen" />
<meta name="citation_publication_date" content="2010/07/31" />
<meta name="citation_journal_title" content="Journal of Machine Learning Research" />
<meta name="citation_issn" content="1532-4435" />
<meta name="citation_volume" content="11" />
<meta name="citation_firstpage" content="1865" />
<meta name="citation_lastpage" content="1881" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Miguel_Lazaro-Gredilla/publication/234779817_Sparse_Spectrum_Gaussian_Process_Regression/links/0c96051ae1c817cdb7000000.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/234779817_Sparse_Spectrum_Gaussian_Process_Regression" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/234779817_Sparse_Spectrum_Gaussian_Process_Regression" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Sparse Spectrum Gaussian Process Regression (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Sparse Spectrum Gaussian Process Regression on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56aba06f4cb75" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56aba06f4cb75" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56aba06f4cb75">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Sparse%20Spectrum%20Gaussian%20Process%20Regression&rft.title=The%20Journal%20of%20Machine%20Learning%20Research&rft.jtitle=The%20Journal%20of%20Machine%20Learning%20Research&rft.volume=11&rft.date=2010&rft.pages=1865-1881&rft.issn=1532-4435&rft.au=Miguel%20L%C3%A1zaro-Gredilla%2CCarl%20Edward%20Rasmussen&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Sparse Spectrum Gaussian Process Regression</h1> <meta itemprop="headline" content="Sparse Spectrum Gaussian Process Regression">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/234779817_Sparse_Spectrum_Gaussian_Process_Regression/links/0c96051ae1c817cdb7000000/smallpreview.png">  <div id="rgw8_56aba06f4cb75" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56aba06f4cb75" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Miguel_Lazaro-Gredilla" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A276903991758850%401443030464156_m/Miguel_Lazaro-Gredilla.png" title="Miguel LÃ¡zaro-Gredilla" alt="Miguel LÃ¡zaro-Gredilla" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Miguel LÃ¡zaro-Gredilla</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw10_56aba06f4cb75" data-account-key="Miguel_Lazaro-Gredilla">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Miguel_Lazaro-Gredilla"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A276903991758850%401443030464156_l/Miguel_Lazaro-Gredilla.png" title="Miguel LÃ¡zaro-Gredilla" alt="Miguel LÃ¡zaro-Gredilla" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Miguel_Lazaro-Gredilla" class="display-name">Miguel LÃ¡zaro-Gredilla</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_Carlos_III_de_Madrid" title="University Carlos III de Madrid ">University Carlos III de Madrid </a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56aba06f4cb75"> <a href="researcher/43277170_Carl_Edward_Rasmussen" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Carl Edward Rasmussen" alt="Carl Edward Rasmussen" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Carl Edward Rasmussen</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56aba06f4cb75">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/43277170_Carl_Edward_Rasmussen"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Carl Edward Rasmussen" alt="Carl Edward Rasmussen" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/43277170_Carl_Edward_Rasmussen" class="display-name">Carl Edward Rasmussen</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/1532-4435_Journal_of_Machine_Learning_Research"><span itemprop="name">Journal of Machine Learning Research</span></a> </span>    (Impact Factor: 2.47).     <meta itemprop="datePublished" content="2010-07">  07/2010;  11:1865-1881.             <div class="pub-source"> Source: <a href="http://dblp.uni-trier.de/db/journals/jmlr/jmlr11.html#Lazaro-GredillaCRF10" rel="nofollow">DBLP</a> </div>  </div> <div id="rgw13_56aba06f4cb75" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>We present a new sparse Gaussian Process (GP) model for regression. The key novel idea is to sparsify the spectral representation of the GP. This leads to a simple, practical algorithm for regression tasks. We compare the achievable trade-offs between predictive accuracy and computational requirements, and show that these are typically superior to existing state-of-the-art sparse approximations. We discuss both the weight space and function space representations, and note that the new construction implies priors over functions which are always stationary, and can approximate any covariance function in this class.</div> </p>  </div>   </div>      <div class="action-container"> <div id="rgw14_56aba06f4cb75" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw29_56aba06f4cb75">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw30_56aba06f4cb75">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Miguel_Lazaro-Gredilla/publication/234779817_Sparse_Spectrum_Gaussian_Process_Regression/links/0c96051ae1c817cdb7000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Miguel_Lazaro-Gredilla">Miguel LÃ¡zaro-Gredilla</a>   </span>  </div>  <div class="social-share-container"><div id="rgw32_56aba06f4cb75" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw33_56aba06f4cb75" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw34_56aba06f4cb75" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw35_56aba06f4cb75" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw36_56aba06f4cb75" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw37_56aba06f4cb75" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw31_56aba06f4cb75" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMiguel_Lazaro-Gredilla%2Fpublication%2F234779817_Sparse_Spectrum_Gaussian_Process_Regression%2Flinks%2F0c96051ae1c817cdb7000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw28_56aba06f4cb75"  itemprop="articleBody">  <p>Page 1</p> <p>Journal of Machine Learning Research 11 (2010) 1865-1881Submitted 2/08; Revised 2/10; Published 6/10<br />Sparse Spectrum Gaussian Process Regression<br />Miguel LÃ¡zaro-Gredilla<br />Departamento de TeorÃ­a de la SeÃ±al y Comunicaciones<br />Universidad Carlos III de Madrid<br />28911 LeganÃ©s, Madrid, Spain<br />Joaquin QuiÃ±onero-Candela<br />Microsoft Research Ltd.<br />7 J J Thomson Avenue<br />Cambridge CB3 0FB, UK<br />Carl Edward Rasmussenâ<br />Deparment of Engineering<br />University of Cambridge, Trumpington st.<br />Cambridge CB2 1PZ, UK<br />MIGUEL@TSC.UC3M.ES<br />JOAQUINC@MICROSOFT.COM<br />CER54@CAM.AC.UK<br />AnÃ­bal R. Figueiras-Vidal<br />Departamento de TeorÃ­a de la SeÃ±al y Comunicaciones<br />Universidad Carlos III de Madrid<br />28911 LeganÃ©s, Madrid, Spain<br />ARFV@TSC.UC3M.ES<br />Editor: Tommi Jaakkola<br />Abstract<br />We present a new sparse Gaussian Process (GP) model for regression. The key novel idea is to<br />sparsify the spectral representation of the GP. This leads to a simple, practical algorithm for regres-<br />sion tasks. We compare the achievable trade-offs between predictive accuracy and computational<br />requirements, and show that these are typically superior to existing state-of-the-art sparse approx-<br />imations. We discuss both the weight space and function space representations, and note that the<br />new construction implies priors over functions which are always stationary, and can approximate<br />any covariance function in this class.<br />Keywords:<br />Gaussian process, probabilistic regression, sparse approximation, power spectrum,<br />computational efficiency<br />1. Introduction<br />One of the main practical limitations of Gaussian processes (GPs) for machine learning (Rasmussen<br />and Williams, 2006) is that in a direct implementation the computational and memory requirements<br />scale as O(n2) and O(n3), respectively. In practice this limits the applicability of exact GP imple-<br />mentations to data sets where the number of training samples n does not exceed a few thousand.<br />A number of computationally efficient approximations to GPs have been proposed, which re-<br />duce storage requirements to O(nm) and the number of computations to O(nm2), where m is much<br />smaller than n. One family of approximations, reviewed in QuiÃ±onero-Candela and Rasmussen<br />(2005), is based on assumptions of conditional independence given a reduced set of m inducing<br />â. Also at Max Planck Institute for Biological Cybernetics, Spemannstr. 38, 72076 TÃ¼bingen, Germany .<br />c ?2010 Miguel LÃ¡zaro-Gredilla, Joaquin QuiÃ±onero-Candela, Carl E. Rasmussen and AnÃ­bal R. Figueiras-Vidal.</p>  <p>Page 2</p> <p>LÃZARO-GREDILLA, QUIÃONERO-CANDELA, RASMUSSEN AND FIGUEIRAS-VIDAL<br />inputs. Examples of such models are those proposed in Seeger et al. (2003), Smola and Bartlett<br />(2001), Tresp (2000), Williams and Seeger (2001) and CsatÃ³ and Opper (2002), as well as the Fully<br />Independent Training Conditional (FITC) model, introduced as Sparse Pseudo-input GP (SPGP)<br />by Snelson and Ghahramani (2006). Walder et al. (2008) introduced the Sparse Multiscale GP<br />(SMGP), a modification of FITC that allows each basis function to have its own set of length-<br />scales.1This additional flexibility typically yields some performance improvement over FITC, but<br />it also requires learning twice as many parameters. SMGP can alternatively be derived within the<br />unifying framework of QuiÃ±onero-Candela and Rasmussen (2005) if we allow the inducing inputs<br />to lie in a transformed domain, as shown in LÃ¡zaro-Gredilla and Figueiras-Vidal (2010).<br />Another family of approximations is based on approximate matrix-vector-multiplications<br />(MVMs), where m is for example a reduced number of conjugate gradient steps to solve a sys-<br />tem of linear equations. Some of these methods have been briefly reviewed in QuiÃ±onero-Candela<br />et al. (2007). Local mixtures of GP have been used by Urtasun and Darrell (2008) for efficient<br />modelling of human poses.<br />In this paper we introduce a stationary trigonometric Bayesian model for regression that retains<br />the computational efficiency of the aforementioned approaches, while improving performance. The<br />modelconsistsofalinearcombinationoftrigonometricfunctionswherebothweightsandphasesare<br />integrated out. All hyperparameters of the model (frequencies and amplitudes) are learned jointly by<br />maximizing the marginal likelihood. This model is a stationary sparse GP that can approximate any<br />desired stationary full GP. Sparse trigonometric expansions have been proposed in several contexts,<br />for example, LÃ¡zaro-Gredilla et al. (2007) and Rahimi and Recht (2008), as discussed further in<br />Section 4.3.<br />FITC, SMGP, and the model introduced in this paper focus on predictive accuracy at low com-<br />putational cost, rather than on faithfully converging towards the full GP as the number of basis func-<br />tions grows. Performance-wise, FITC and the more recent SMGP can be regarded as the current<br />state-of-the-art sparse GP approximations, so we will use them as benchmarks in the performance<br />comparisons.<br />In Section 2 we give a brief review of GP regression. In Section 3 we introduce the trigono-<br />metric Bayesian model, and in Section 4 we present the Sparse Spectrum Gaussian Process (SSGP)<br />algorithm. Section 5 contains a comparative performance evaluation on several data sets.<br />2. Gaussian Process Regression<br />Regression is often formulated as the task of predicting the scalar output yâassociated to the D-<br />dimensional input xâ, given a training data set D â¡ {xj,yj|j = 1,...n} of n input-output pairs. A<br />common approach is to assume that the outputs have been generated by an unknown latent function<br />f(x) and independently corrupted by additive Gaussian noise of constant variance Ï2<br />n:<br />yj = f(xj)+Îµj,<br />Îµj â¼ N (0, Ï2<br />n).<br />The regression task boils down to making inference about f(x). Gaussian process (GP) regression<br />is a probabilistic, non-parametric Bayesian approach. A Gaussian process prior distribution on f(x)<br />allows us to encode assumptions about the smoothness (or other properties) of the latent function<br />1. Note that SMGP only extends FITC in the specific case of the anisotropic squared exponential covariance function,<br />whereas FITC can be applied to any covariance function.<br />1866</p>  <p>Page 3</p> <p>SPARSE SPECTRUM GAUSSIAN PROCESS REGRESSION<br />(Rasmussen and Williams, 2006). For any set of inputs {xi}n<br />evaluations f = [f(x1),..., f(xn)]â¤has a joint Gaussian distribution:<br />i=1the corresponding vector of function<br />p(f|{xi}n<br />i=1) = N (f|0,Kff).<br />This paper follows the common practice of setting the mean of the process to zero.2The properties<br />of the GP prior over functions are governed by the covariance function<br />?Kff<br />?<br />ij= k(xi,xj) = E[f(xi)f(xj)],<br />(1)<br />which determines how the similarity between a pair of function values varies as a function of the<br />corresponding pair of inputs. A covariance function is stationary if it only depends on the difference<br />between its inputs<br />k(xi,xj) = k(xiâxj) = k(Ï).<br />The elegance of the GP framework is that the properties of the function are conveniently expressed<br />directly in terms of the covariance function, rather than implicitly via basis functions.<br />To obtain the predictive distribution p(yâ|xâ,D) it is useful to express the model in matrix<br />notation by stacking the targets yjin vector y = [y1,...,yn]â¤and writing the joint distribution of<br />training and test targets:<br />?y<br />yâ<br />?<br />â¼ N<br />?<br />0,<br />?Kff+Ï2<br />nIn<br />kfâ<br />kâ¤<br />fâ<br />kââ+Ï2<br />n<br />??<br />,<br />where kfâis the vector of covariances between f(xâ) and the training latent function values, and<br />kââis the prior variance of f(xâ). Inis the nÃn identity. The predictive distribution is obtained by<br />conditioning on the observed training outputs:<br />p(yâ|xâ,D) = N (Âµâ,Ï2<br />â), where<br />?<br />Âµâ= kâf(Kff+Ï2<br />Ï2<br />nIn)â1y<br />â= Ï2<br />n+kâââkâf(Kff+Ï2<br />nIn)â1kfâ.<br />(2)<br />The covariance function is parameterized by hyperparameters. Consider for example the sta-<br />tionary anisotropic squared exponential covariance function<br />kARD(Ï) = Ï2<br />0exp(â1<br />2Ïâ¤Îâ1Ï), where Î = diag([â2<br />1, â2<br />2, ... â2<br />D]).<br />(3)<br />The hyperparameters are the prior variance Ï2<br />the covariance decays with the distance between inputs. This covariance function is also known<br />as the ARD (Automatic Relevance Determination) squared exponential, because it can effectively<br />prune input dimensions by growing the corresponding lengthscales.<br />It is convenient to denote all hyperparameters including the noise variance by Î¸. These can be<br />learned by maximizing the evidence, or log marginal likelihood:<br />0and the lengthscales {âd} that determine how rapidly<br />logp(y|Î¸) = ân<br />2log(2Ï)â1<br />2|Kff+Ï2<br />nIn|â1<br />2yâ¤?Kff+Ï2<br />nIn<br />?â1y.<br />(4)<br />Provided there exist analytic forms for the gradients of the covariance function with respect to the<br />hyperparameters, the evidence can be maximized by using a gradient-based search. Unfortunately,<br />computing the evidence and the gradients requires the inversion of the covariance matrix Kff+Ï2<br />at a cost of O(n3) operations, which is prohibitive for large data sets.<br />nIn<br />2. The extension to GPs with general mean functions is straightforward.<br />1867</p>  <p>Page 4</p> <p>LÃZARO-GREDILLA, QUIÃONERO-CANDELA, RASMUSSEN AND FIGUEIRAS-VIDAL<br />3. Trigonometric Bayesian Regression<br />In this section we present a Bayesian linear regression model with trigonometric basis functions,<br />and related it to a full GP in the next section. Consider the model<br />f(x)=<br />m<br />â<br />r=1<br />arcos(2Ïsâ¤<br />rx)+brsin(2Ïsâ¤<br />rx) ,<br />(5)<br />where each of the m pairs of basis functions is parametrized by a D-dimensional vector srof spectral<br />frequencies. Note that each pair of basis functions share frequencies, but each have independent<br />amplitude parameters, ar and br. We treat the frequencies as deterministic parameters and the<br />amplitudes in a Bayesian way. The priors are independent Gaussian<br />ar â¼ N?0,Ï2<br />0<br />m<br />?,<br />br â¼ N?0,Ï2<br />0<br />m<br />?,<br />where the variances are scaled down linearly by the number of basis functions. Under the prior, the<br />distribution over functions from Equation (5) is Gaussian with mean function zero and covariance<br />function (from Equation (1))<br />k(xi,xj) =Ï2<br />0<br />mÏ(xi)â¤Ï(xj) =Ï2<br />0<br />m<br />m<br />â<br />r=1<br />cos?2Ïsâ¤<br />r(xiâxj)?,<br />(6)<br />where we define the column vector of length 2m containing the evaluation of the m pairs of trigono-<br />metric functions at x<br />Ï(x) =<br />?cos(2Ïsâ¤<br />1x) sin(2Ïsâ¤<br />1x) ... cos(2Ïsâ¤<br />mx)<br />sin(2Ïsâ¤<br />mx)?â¤.<br />Sparse linear models generally induce priors over functions whose variance depends on the input.<br />In contrast, the covariance function in Equation (6) is stationary, that is, the prior variance is inde-<br />pendent of the input and equal to Ï2<br />functions and implies that the predictive variances cannot be âhealedâ, as proposed in Rasmussen<br />and QuiÃ±onero-Candela (2005) for the case of the Relevance Vector Machine.<br />The predictions and marginal likelihood can be evaluated using Equations (2) and (4), although<br />direct evaluation is computationally inefficient when 2m &lt; n. For the predictive distribution we use<br />the more efficient<br />0. This is due to the particular nature of the trigonometric basis<br />E[yâ] = Ï(xâ)â¤Aâ1Î¦fy,<br />V[yâ] = Ï2<br />n+Ï2<br />nÏ(xâ)â¤Aâ1Ï(xâ),<br />(7)<br />where we have defined the 2m by n design matrix Î¦f= [Ï(x1),...,Ï(xn)] and A = Î¦fÎ¦â¤<br />Similarly, for the log marginal likelihood<br />f+mÏ2<br />n<br />Ï2<br />0I2m.<br />logp(y|Î¸) = â?yâ¤yâyâ¤Î¦â¤<br />A stable and efficient implementation uses Cholesky decompositions, Appendix A. Both the predic-<br />tive distribution and the marginal likelihood can be computed in O(nm2). The predictive mean and<br />variance at an additional test point can be computed in O(m) and O(m2) respectively. The storage<br />costs are also reduced, since we no longer store the full covariance matrix (of size nÃn), but only<br />the design matrix (of size nÃ2m).<br />fAâ1Î¦fy?/(2Ï2<br />n)â1<br />2log|A|+mlogmÏ2<br />n<br />Ï2<br />0<br />ân<br />2log2ÏÏ2<br />n.<br />(8)<br />1868</p>  <p>Page 5</p> <p>SPARSE SPECTRUM GAUSSIAN PROCESS REGRESSION<br />3.1 Periodicity<br />One might be tempted to assume that this model would only be useful for modeling periodic func-<br />tions, since strictly speaking a linear combination of periodic signals is itself periodic. However,<br />if the individual frequencies are not all multiples of a common base frequency, then the period<br />of the resulting signal will be very long, typically exceeding the range of the inputs by orders of<br />magnitude. Thus, the model based on trigonometric basis functions has practical use for modeling<br />non-periodic functions. The same principle is used (interchanging input and frequency domains)<br />in uneven sampling to space apart frequency replicas an avoid aliasing, see for instance Bretthorst<br />(2000). As our experimental results suggest, the model provides satisfactory predictive variances.<br />3.2 Representation<br />An alternative and equivalent representation of the model in Equation (5), which only uses half<br />the number of trigonometric basis functions is possible, by writing the linear combination of a<br />sine and a cosine as a cosine with an amplitude and a phase. Although these two representations<br />are equivalent, inference based on them differs. Whereas we have been able to integrate out the<br />amplitudes to arrive at the GP in Equation (6), this would not be possible analytically using the<br />more parsimonious representation.<br />Optimization instead of marginalization of the phases has two important consequences. Firstly,<br />we lose the property of stationarity of the prior over functions. Secondly we may expect that the<br />model becomes more prone to overfitting. When considering the contribution from a basis func-<br />tion (pair) with a specific frequency, the optimization based scheme could fit arbitrarily the phase,<br />whereas the integration based inference is constrained to use a flat prior over phases. In Section 5.3<br />we empirically verify that the computation vs accuracy tradeoff typically favors the less compact<br />representation.<br />4. The Sparse Spectrum Gaussian Process<br />In the previous section we presented an explicit basis function regression model, but we did not dis-<br />cuss how to select the frequencies defining the basis functions. In this section, we present a sparse<br />GP approximation view of this model, which shows how it can be understood as a computation-<br />ally efficient approximation to any GP with stationary covariance function. In the next section we<br />present experimental results showing that dramatic improvements over other state-of-the-art sparse<br />GP regression algorithms are possible.<br />We will now take a generic GP with stationary covariance function and sparsify its power spec-<br />traldensitytoobtainasparseGPthatapproximatesthefullGP.Thepowerspectraldensity(orpower<br />spectrum) S(s) of a stationary random process expresses how the power is distributed over the fre-<br />quency domain. For a stationary GP, the power is equal to the prior variance k(x,x) = k(0) = Ï2<br />The frequency vector s has the same length D as the input vector x. The d-th element of s can be in-<br />terpreted as the frequency associated to the d-th input dimension. The Wiener-Khintchine theorem<br />(see for example Carlson, 1986, p. 162) states that the power spectrum and the autocorrelation of<br />the random process constitute a Fourier pair. In our case, given that f(Â·) is drawn from a stationary<br />Gaussian process, the autocorrelation function is equal to the stationary covariance function, and<br />we have:<br />k(Ï) =<br />0.<br />?<br />RDe2Ïisâ¤ÏS(s)ds,<br />S(s) =<br />?<br />RDeâ2Ïisâ¤Ïk(Ï)dÏ.<br />(9)<br />1869</p>  <p>Page 6</p> <p>LÃZARO-GREDILLA, QUIÃONERO-CANDELA, RASMUSSEN AND FIGUEIRAS-VIDAL<br />We thus see that there are two equivalent representations for a stationary Gaussian process: the<br />traditional one in terms of the covariance function in the (input) space domain, and a perhaps less<br />usual one as the power spectrum in the frequency domain.<br />Bochnerâs theorem (Stein, 1999, p. 24) states that any stationary covariance function k(Ï) can<br />be represented as the Fourier transform of a positive finite measure. This means that the power<br />spectrum in (9) is a positive finite measure, and in particular that it is proportional to a probability<br />measure, S(s) â pS(s). The proportionality constant can be directly obtained by evaluating the<br />covariance function in (9) at Ï = 0. We obtain the relation:<br />S(s) = k(0)pS(s) = Ï2<br />0pS(s).<br />(10)<br />We can use the fact that S(s) is proportional to a multivariate probability density in s to rewrite the<br />covariance function in (9) as an expectation:<br />k(xi,xj)=<br />k(Ï) =<br />?<br />e2Ïisâ¤xi?<br />RDe2Ïisâ¤(xiâxj)S(s)ds = Ï2<br />e2Ïisâ¤xj?â?<br />0<br />?<br />RDe2Ïisâ¤xi?<br />e2Ïisâ¤xj?âpS(s)ds<br />=<br />Ï2<br />0EpS<br />?<br />,<br />(11)<br />where EpSdenotes expectation wrt. pS(s) and superscript asterisk3denotes complex conjugation.<br />This last expression is an exact expansion of the covariance function as the expectation of a product<br />of complex exponentials with respect to a particular distribution over their frequencies. This integral<br />can be approximated by simple Monte Carlo by taking an average of a few samples corresponding<br />to a finite set of frequencies, which we call spectral points.<br />Since the power spectrum is symmetric around zero, a valid Monte Carlo procedure is to sample<br />frequencies always as a pair {sr,âsr}. This has the advantage of preserving the property of the exact<br />expansion, Equation (11) that the imaginary terms cancel:<br />k(xi,xj)â<br />Ï2<br />2m<br />Ï2<br />0<br />mRe<br />0<br />m<br />â<br />r=1<br />?<br />â<br />r=1<br />e2Ïisâ¤<br />rxi?<br />e2Ïisâ¤<br />rxj?â+<br />?<br />e2Ïisâ¤<br />rxi?âe2Ïisâ¤<br />=Ï2<br />m<br />â<br />r=1<br />rxj?<br />=<br />? m<br />e2Ïisâ¤<br />rxi?<br />e2Ïisâ¤<br />rxj?â?<br />0<br />m<br />cos?2Ïsâ¤<br />r(xiâxj)?,<br />where sris drawn from pS(s) and Re[Â·] denotes the real part of a complex number. Notice, that<br />we have recovered exactly the expression for the covariance function induced by the trigonometric<br />basis functions model, Equation (6). Further, we have given an interpretation of the frequencies as<br />spectral Monte Carlo samples, approximating any stationary covariance function. This is a more<br />general result than that of (MacKay, 2003, Ch. 45), which only applies to Gaussian covariances.<br />The approximation is equivalent to replacing the original spectrum S(s) by a set of Dirac deltas of<br />amplitude Ï2<br />This convergence result can also be stated as follows: A stationary GP can be seen as a neural<br />network with infinitely many hidden units and trigonometric activations if independent priors fol-<br />lowing Gaussian and pS(s) distributions are placed on the output and input weights, respectively.<br />This is analogous to the result of Williams (1997) for the non-stationary multilayer perceptron co-<br />variance function.<br />0distributed according to pS(s). Thus, we âsparsifyâ the spectrum of the GP.<br />1870</p>  <p>Page 7</p> <p>SPARSE SPECTRUM GAUSSIAN PROCESS REGRESSION<br />â8â6 â4â20<br />Ï<br />2468<br />â0.4<br />â0.2<br />0<br />0.2<br />0.4<br />0.6<br />0.8<br />1<br />1.2<br />Covariance function<br /> <br /> <br />SE covariance function<br />10 spectral points approx.<br />â8â6 â4 â20<br />Ï<br />2468<br />â0.4<br />â0.2<br />0<br />0.2<br />0.4<br />0.6<br />0.8<br />1<br />1.2<br />Covariance function<br /> <br /> <br />SE covariance function<br />50 spectral points approx.<br />(a) (b)<br />Figure 1: Squared exponential covariance function and its approximation with (a) 10 and (b) 50<br />random spectral points respectively.<br />4.1 Example: The Squared Exponential Covariance Function<br />The probability density associated to the squared exponential covariance function of Equation (3)<br />can be obtained from the Fourier transform<br />pARD<br />S<br />(s) =<br />1<br />kARD(0)<br />?<br />RDeâ2Ïisâ¤ÏkARD(Ï)dÏ =<br />?<br />|2ÏÎ|exp(â2Ï2sâ¤Îs),<br />(12)<br />which also has the form of a multivariate Gaussian distribution. For illustration purposes, we com-<br />pare the exact squared exponential covariance function with its spectral approximation in Figure 1,<br />where the spectral points are sampled from Equation (12). As expected, the quality of the approxi-<br />mation improves with the number of samples.<br />4.2 The SSGP Algorithm<br />One of the main goals of sparse approximations is to reduce the computational burden while re-<br />taining as much predictive accuracy as possible. Sampling from the spectral density constitutes a<br />way of building a sparse approximation. However, we may suspect that we can obtain much sparser<br />models if the spectral frequencies are learned by optimizing the marginal likelihood, an idea which<br />we pursue in the following.<br />The algorithm we propose uses conjugate gradients to optimize the marginal likelihood (8)<br />with respect to the spectral points {sr} and the hyperparameters Ï2<br />Optimizing with respect to the lengthscales in addition to the spectral points is effectively an over-<br />parametrization, but in our experience this redundancy proves helpful in avoiding undesired local<br />minima. As is usual with this kind of optimization, the problem is non-convex and we cannot expect<br />to find the global optimum. The goal of the optimization is to find a reasonable local optimum.<br />In detail, model selection for the SSGP algorithm consists in:<br />0, Ï2<br />n, and {â1, â2, ... âD}.<br />1. Initialize {âd}, Ï2<br />dimensions, the variance of {yj} and Ï2<br />0, and Ï2<br />nto some sensible values. We use one half of the ranges of the input<br />0/4, respectively.<br />2. Initialize the {sr} by sampling from (10).<br />3. The superscript asterisk denotes complex conjugate and the subscript asterisk indicates test quantity.<br />1871</p>  <p>Page 8</p> <p>LÃZARO-GREDILLA, QUIÃONERO-CANDELA, RASMUSSEN AND FIGUEIRAS-VIDAL<br />3. Jointly optimize the marginal likelihood wrt. spectral points and hyperparameters.<br />The computational cost of training the SSGP algorithm is O(nm2) per conjugate gradient step.<br />At prediction time, the cost is O(m) for the predictive mean and O(m2) for the predictive vari-<br />ance per test point. These computational costs are of the same order as those of the majority of<br />the sparse GP approximations that have recently been proposed (see QuiÃ±onero-Candela and Ras-<br />mussen, 2005, for a review).<br />Learning the spectral frequencies by optimization departs from the original motivation of ap-<br />proximating a full GP. The optimization stage poses a risk of overfitting, which we assess in the<br />experimental section that follows. However, the additional flexibility can potentially improve per-<br />formance since it allows learning a covariance function suitable to the problem at hand.<br />4.3 Related Algorithms<br />Finite decompositions in terms of harmonic basis functions, such as Fourier series, are a classic<br />idea. In the context of kernel machines recent work include LÃ¡zaro-Gredilla et al. (2007) for GPs<br />and Rahimi and Recht (2008) for Support Vector Machines (SVMs). As we show in the experimen-<br />tal section, the details of the implementation turn out to have a critical impact on the performance<br />of the algorithms. The SVM based approach uses projections onto a random set of harmonic func-<br />tions, whereas the approach used in this paper uses the evidence framework to carefully craft an<br />optimized sparse harmonic representation. As is revealed in the experimental section, optimization<br />of the frequencies, amplitudes and noise offers dramatic performance improvements for comparable<br />sparseness.<br />5. Experiments<br />In this section we investigate properties of the SSGP algorithm, and evaluate the computational<br />complexity vs. accuracy tradeoff. We first relate the FITC and SSGP approximations. We then<br />present empirical comparisons on several data sets, using FITC and SMGP as benchmarks. Finally,<br />we revisit the alternative more compact representation of SSGP using phases, and discuss a data set<br />where SSGP performs badly.<br />OurimplementationofSSGPinmatlabisavailablefrom http://www.tsc.uc3m.es/~miguel/<br />simpletutorialssgp.php together with a simple usage tutorial and the data sets from this sec-<br />tion. An implementation of FITC is available from Snelsonâs web page at http://www.gatsby.<br />ucl.ac.uk/~snelson .<br />5.1 Comparing Predictive Distributions for SSGP and FITC<br />Whereas SSGP relies on a sparse approximation to the spectrum, the FITC approximation is sparse<br />in a spatial sense: A set of pseudo-inputs is used as an information bottleneck. The only evaluations<br />of the covariance function allowed are those involving a function value at a pseudo-input. For a set<br />of m pseudo-inputs the computational complexity of FITC is of the same order as that of SSGP with<br />m spectral points.<br />The covariance function induced by FITC has a constant prior variance, but it is not stationary.<br />The original covariance of the full GP is only approximated faithfully in the vicinity of the pseudo-<br />inputs and the covariance between any two function values that are both far apart from any pseudo-<br />1872</p>  <p>Page 9</p> <p>SPARSE SPECTRUM GAUSSIAN PROCESS REGRESSION<br />05 1015<br />â1<br />â0.5<br />0<br />0.5<br />1<br />input, x<br />(a)<br />output, f(x)<br />05 10 15<br />â1<br />â0.5<br />0<br />0.5<br />1<br />input, x<br />(b)<br />output, f(x)<br />Figure 2: Learning the sinc(x) function from 100 noisy observations (plusses) using 40 basis func-<br />tions with shaded area showing 95% (noise free) posterior confidence area. In panel (a)<br />the SSGP method with three functions drawn from the posterior is shown. In (b) the same<br />data for the FITC method with samples (dots) drawn from the joint posterior.<br />2450<br />Number of basis functions<br />(a)<br />100200300 500 7501250<br />0.001<br />0.005<br />0.01<br />0.05<br />0.1<br />0.5<br />Normalized Mean Squared Error<br /> <br /> <br />FITC<br />SMGP<br />SSGP fixed spectral points<br />SSGP<br />Full GP on 10000 data points<br />24 50<br />Number of basis functions<br />(b)<br />100 200300 500750 1250<br />â1.5<br />â1<br />â0.5<br />0<br />0.5<br />1<br />1.5<br />2<br />2.5<br />Mean Negative LogâProbability<br /> <br /> <br />FITC<br />SMGP<br />SSGP fixed spectral points<br />SSGP<br />Full GP on 10000 data points<br />Figure 3: Kin-40k data set. (a) NMSE and (b) MNLP as a function of the number of basis functions.<br />input decays to zero. As a result, functions sampled from the GP prior induced by FITC tend to<br />white Gaussian noise away from the pseudo-inputs.<br />1873</p>  <p>Page 10</p> <p>LÃZARO-GREDILLA, QUIÃONERO-CANDELA, RASMUSSEN AND FIGUEIRAS-VIDAL<br />1024 5074 100<br />0.04<br />0.05<br />0.1<br />Number of basis functions<br />(a)<br />Normalized Mean Squared Error<br /> <br /> <br />FITC<br />SMGP<br />SSGP fixed spectral points<br />SSGP<br />Full GP on 7168 data points<br />102450 74100<br />â0.2<br />â0.15<br />â0.1<br />â0.05<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />Number of basis functions<br />(b)<br />Mean Negative LogâProbability<br /> <br /> <br />FITC<br />SMGP<br />SSGP fixed spectral points<br />SSGP<br />Full GP on 7168 data points<br />Figure 4: Pumadyn-32nm data set. (a) NMSE and (b) MNLP as a function of the number of basis<br />functions.<br />Figure2comparesthepredictiveposteriordistributionsofSSGPandFITCforasimplesynthetic<br />data set. The training data is generated by evaluating the sinc function on 100 random inputs x â<br />[â1,5] and adding white, zero-mean Gaussian noise of variance Ï2<br />spectral points sampled from the spectrum of a squared exponential covariance function, and FITC<br />is given 40 fixed pseudo-inputs sampled uniformly from the range of the training inputs. The rest<br />of the hyperparameters are optimized in both cases by maximizing the marginal likelihood. We plot<br />the 95% confidence interval for both predictive distributions (mean Â± two standard deviations), and<br />draw three samples from the SSGP posterior and one sample from the FITC posterior.<br />Despite the different nature of the approximations, the figure shows that for an equal number<br />of basis functions both predictive distributions are qualitatively very similar: the uncertainty grows<br />away from the training data. In the following section, we verify empirically that the SSGP is a<br />practical approximation for modelling non-periodic data.<br />n=0.052. SSGP is given 20 fixed<br />5.2 Performance Evaluation<br />Wewillusetwoquantitativeperformancemeasures: thetestNormalizedMeanSquareError(NMSE)<br />and the test Mean Negative Log Probability (MNLP) , defined as:<br />NMSE =?(yâjâÂµâj)2?<br />?(yâjây)2?<br />and MNLP =1<br />2<br />??yâjâÂµâj<br />Ïâj<br />?2+logÏ2<br />âj+log2Ï<br />?<br />,<br />(13)<br />where Âµâjand Ï2<br />the actual test value for that sample. The average output value for training data is y. We denote<br />âjare, respectively, the predictive mean and variance for test sample j and yâjis<br />1874</p>  <p>Page 11</p> <p>SPARSE SPECTRUM GAUSSIAN PROCESS REGRESSION<br />the average over test cases by ?Â·?. For all experiments the values reported are averages over ten<br />repetitions.<br />For each data set we report the performance of five different methods: first, the SSGP algorithm<br />as presented in Section 4.2; second, a version of SSGP where the spectral points are âfixedâ to sam-<br />ples from the spectral density of a squared exponential covariance function whose lengthscales are<br />learned (SSGP fixed spectral points);4third, the FITC approximation, learning the pseudo-inputs;<br />fourth, SMGP, trained as described in Walder et al. (2008); and finally as a base line comparison<br />we report the result of a full GP trained on the entire training set. We plot the performance as a<br />function of the number of basis functions. For FITC this is equivalent to the number of pseudo-<br />inputs, whereas for SSGP a spectral point corresponds to two basis functions. The number of basis<br />functions is a good proxy for computational cost.<br />We consider four data sets of size moderate enough to be tractable by a full GP, but still large<br />enough that there is a motivation for computationally efficient approximations.<br />The two first data sets are both artificially generated using a robot arm simulator and are highly<br />non-linear and have very low noise. They were both used in Seeger et al. (2003) and Snelson and<br />Ghahramani (2006), but note that their definition of the NMSE measure differs by a factor of 2<br />from our definition in (13). We follow precisely their preprocessing and use the original splits. The<br />first data set is Kin-40k (8 dimensions, 10000 training and 30000 testing samples) and the results<br />are displayed in Figure 3. For both error measures SSGP outperforms FITC and SMGP by a large<br />margin, and even improves on the performance of the full GP. The SSGP with fixed spectral points<br />is inferior, proving that a greater sparsity vs. accuracy tradeoff can be achieved by optimizing the<br />spectral points.<br />The Pumadyn-32nm problem (32 dimensions, 7168 training and 1024 testing samples) can be<br />seen as a test of the ARD capabilities of a regression model, since only 4 out of the 32 input<br />dimensions are relevant. Following Snelson and Ghahramani (2006), to avoid getting stuck at an<br />undesirable bad local optimum, lengthscales are initialized from a full GP on a subset of 1024<br />training data points, for all compared methods. The results are shown in figure 4.<br />The conclusions are similar as for the Kin-40k data set. SSGP matches the full GP for a surpris-<br />ingly small number of basis functions.<br />The Pole Telecomm and the Elevators data sets are taken from http://www.liaad.up.pt/<br />~ltorgo/Regression/DataSets.html . In the Pole Telecomm data set we retain 26 dimensions,<br />removing constants. We use the original split, 10000 data for training and 5000 for testing. Both the<br />inputs and the outputs take a discrete set of values. In particular, the outputs take values between<br />0 and 100, in multiples of 10. We take into account the output quantization by lower bounding the<br />value of Ï2<br />all the compared methods. The effect is to provide a better estimation for Ï2<br />MNLP measures, but we have observed that this modification has no noticeable effect on NMSE<br />values. Resulting plots are in Figure 5.<br />SSGP is superior in terms of NMSE, getting very close to the full GP for more than 200 basis<br />functions. In terms of MNLP, SSGP is between FITC and SMGP for small m, but slightly worse<br />for more than 100 basis functions. This may be an indication that SSGP produces better predictive<br />means than variances. We also see that SSGP with fixed spectral points is uniformly worse.<br />nto the value of the quantization noise, bin2<br />spacing/12. This lower bounding is applied to<br />nand therefore, better<br />4. In practice the spectral points are sampled from the spectral density of a squared exponential covariance function,<br />and scaled as the lengthscales adapt.<br />1875</p>  <p>Page 12</p> <p>LÃZARO-GREDILLA, QUIÃONERO-CANDELA, RASMUSSEN AND FIGUEIRAS-VIDAL<br />1024<br />Number of basis functions<br />(a)<br />50100 250500 1000<br />0.01<br />0.02<br />0.03<br />0.04<br />0.05<br />0.1<br />0.15<br />0.2<br />Normalized Mean Squared Error<br /> <br /> <br />FITC<br />SMGP<br />SSGP fixed spectral points<br />SSGP<br />Full GP on 10000 data points<br />10 24<br />Number of basis functions<br />(b)<br />50100 250500 1000<br />2.5<br />3<br />3.5<br />4<br />4.5<br />5<br />5.5<br />Mean Negative LogâProbability<br /> <br /> <br />FITC<br />SMGP<br />SSGP fixed spectral points<br />SSGP<br />Full GP on 10000 data points<br />Figure 5: Pole Telecomm data set. (a) NMSE and (b) MNLP as a function of the number of basis<br />functions.<br />The fourth data set, Elevators, relates to controlling the elevators of an F16 aircraft. After<br />removing some constant inputs the data is 17-dimensional. We use the original split with 8752 data<br />for training and 7847 for testing. Results are displayed in Figure 6. SSGP consistently outperforms<br />FITC and SMGP and gets very close to the full GP using a very low number of basis functions. The<br />large NMSE average errors incurred by SSGP with fixed spectral points for small numbers of basis<br />functions are due to outliers that are present in a small number (about 10 out of 7847) of the test<br />inputs, in some of the 10 repeated runs. The predictive variances for these few points are also big,<br />so their impact on the MNLP score is small. Such an effect has not been observed in any of the<br />other data sets.<br />5.3 Explicit Phase Representation<br />In Section 3.2 we considered an alternative representation of the SSGP model using only half the<br />basis functions, but explicitly representing the phases. Bayesian inference in this representation is<br />intractable, but one can optimize the phases instead, at the possibly increased risk of overfitting.<br />As an example, we evaluate the performance of the cosine only expansion with explicit phases<br />on the Pole-Telecomm data set in Figure 7. Whereas the performance for the two variants are<br />comparable for small numbers of basis functions, the cosine only representation becomes worse<br />when the number of basis functions gets larger, confirming our suspicion that optimization of the<br />phases increases the risk of overfitting.<br />1876</p>  <p>Page 13</p> <p>SPARSE SPECTRUM GAUSSIAN PROCESS REGRESSION<br />1024<br />Number of basis functions<br />(a)<br />50100 250500 750 1000<br />0.1<br />0.15<br />0.2<br />0.25<br />Normalized Mean Squared Error<br /> <br /> <br />FITC<br />SMGP<br />SSGP fixed spectral points<br />SSGP<br />Full GP on 8752 data points<br />1024<br />Number of basis functions<br />(b)<br />50 100250 500 750 1000<br />â4.8<br />â4.6<br />â4.4<br />â4.2<br />â4<br />â3.8<br />Mean Negative LogâProbability<br /> <br /> <br />FITC<br />SMGP<br />SSGP fixed spectral points<br />SSGP<br />Full GP on 8752 data points<br />Figure 6: Elevators data set. (a) NMSE and (b) MNLP as a function of the number of basis func-<br />tions.<br />5.4 The Pendulum Data Set<br />So far we have seen data sets where SSGP consistently outperforms FITC and SMGP, and often<br />approaches the performance of a full GP for quite small numbers of basis functions. In this section<br />we present a counter example, showing that SSGP may occasionally fail, although we suspect that<br />this is the exception rather than the norm.<br />The small data set Pendulum (9 dimensions, 315 training and 315 testing samples) represents<br />the problem of predicting the change in angular velocity of a simulated mechanical pendulum over<br />a short time frame (50 ms) as a function of various parameters of the dynamical system. The target<br />variable depends heavily on all inputs and the targets are almost noise free. Figure 8 shows the<br />results of our experiments. Note that we use up to 800 basis functions for investigation, although<br />for computational reasons it would make sense to use the full GP rather than an approximation<br />with more than 315 basis functions. Although the SSGP NMSE performance is good, we see that<br />especially for large number of basis functions, the MNLP performance is spectacularly bad. A<br />closer inspection shows that the mean predictions are quite accurate, the predictive variances are<br />excessively small. This SSGP model thus exhibits overfitting in the form of being overconfident.<br />Note, that the SSGP with fixed spectral points seems to suffer much less from this effect, as would<br />be expected. Interestingly, re-running the SSGP algorithm with different random initializations<br />gives very different predictions, the predictive distributions from separate runs disagreeing wildly.<br />One could perhaps diagnose the occurrence of the problem in this way. The bottom line is that<br />any algorithm which optimizes the marginal likelihood over a large number of parameters, will risk<br />1877</p>  <p>Page 14</p> <p>LÃZARO-GREDILLA, QUIÃONERO-CANDELA, RASMUSSEN AND FIGUEIRAS-VIDAL<br />1024<br />Number of basis functions<br />(a)<br />50100 250500 1000<br />0.01<br />0.02<br />0.03<br />0.04<br />0.05<br />0.1<br />0.15<br />0.2<br />Normalized Mean Squared Error<br /> <br /> <br />FITC<br />SSGP Cosines Only<br />SSGP<br />Full GP on 10000 data points<br />1024<br />Number of basis functions<br />(b)<br />50100250500 1000<br />2.5<br />3<br />3.5<br />4<br />4.5<br />Mean Negative LogâProbability<br /> <br /> <br />FITC<br />SSGP Cosines Only<br />SSGP<br />Full GP on 10000 data points<br />Figure 7: Pole Telecomm data set. (a) NMSE and (b) MNLP as a function of the number of basis<br />functions, comparing SSGP with the version with cosines only and explicit phases.<br />falling in the overfitting trap. We nevertheless think that the SSGP algorithm will often have very<br />good performance, and will be a practically important algorithm, although one must use it with care.<br />6. Discussion<br />We have introduced the Sparse Spectrum Gaussian Process (SSGP) algorithm, a novel perspective<br />on sparse GP approximations where rather than the usual sparsity approximation in the spatial do-<br />main, it is the spectrum of the covariance function that is subject to a sparse approximation by<br />means of a discrete set of samples, the spectral points. We have provided a detailed comparison<br />of the computational complexity vs. accuracy tradeoff of SSGP to that of the state of the art GP<br />sparse approximation FITC and its extension SMGP. SSGP shows a dramatic improvement in four<br />commonly used benchmark regression data sets, including the two data sets used for evaluation<br />in the paper where FITC was originally proposed (Snelson and Ghahramani, 2006). However, we<br />found a small data set where SSGP badly fails, with good predictive means but with overconfident<br />predictive variances. This indicates that although SSGP is practically a very appealing algorithm,<br />care must be taken to avoid the occasional risk of overfitting.<br />Other algorithms, such as the variational approach of Titsias (2009) which focus on approaching<br />the full GP in the limit of large numbers of basis functions are to a large degree safeguarded from<br />overfitting. However, algorithms derived from GPs whose focus is on achieving good predictive<br />accuracy on a limited computational budget, such as FITC, SMGP and the currently proposed SSGP,<br />1878</p>  <p>Page 15</p> <p>SPARSE SPECTRUM GAUSSIAN PROCESS REGRESSION<br />1024<br />Number of basis functions<br />(a)<br />50 100200400800<br />0.25<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />Normalized Mean Squared Error<br /> <br /> <br />FITC<br />SMGP<br />SSGP fixed spectral points<br />SSGP<br />Full GP on 315 data points<br />1024<br />Number of basis functions<br />(b)<br />50 100 200400 800<br />2<br />4<br />6<br />8<br />10<br />12<br />14<br />16<br />18<br />Mean Negative LogâProbability<br /> <br /> <br />FITC<br />SMGP<br />SSGP fixed spectral points<br />SSGP<br />Full GP on 315 data points<br />Figure 8: Pendulum data set. (a) NMSE and (b) MNLP as a function of the number of basis func-<br />tions.<br />typically achieve superior performance, see Figure 3 in Titsias (2009), with some risk of overfitting.<br />Note, that these algorithms donât generally converge toward the full GP.<br />An equivalent view of SSGP is as a sparse Bayesian linear combination of pairs of trigonometric<br />basis functions, a sine and a cosine for each spectral point. The weights are integrated out, and at<br />the price of having two basis functions per frequency, the phases are effectively integrated out as<br />well. We have shown that although a representation in terms of a single basis function per frequency<br />and an explicit phase is possible, learning the phases poses an increased risk of overfitting. If the<br />spectral points are sampled from the power spectrum of a stationary GP, then SSGP approximates<br />its covariance function. However, much sparser solutions can be achieved by learning the spectral<br />points, which effectively implies learning the covariance function. The SSGP model is to the best<br />of our knowledge the only sparse GP approximation that induces a stationary covariance function.<br />SSGP has been presented here as a Gaussian process prior for regression with a tractable like-<br />lihood function from the assumption of Gaussian observation noise. Extending to other types of<br />analytically intractable likelihood functions, such as sigmoid for classification or Laplace for robust<br />regression is possible by using the same approximation techniques as for full GPs. An example<br />is the use of Expectation Propagation in the derivation of generalized FITC (Naish-Guzman and<br />Holden, 2008). Further modifications and extensions of SSGP are discussed in LÃ¡zaro-Gredilla<br />(2010).<br />The main differences between SSGP and most previous approaches to sparse GP regression is<br />the stationarity of the prior and the non-local nature of the basis functions. It will be interesting to<br />1879</p>  <p>Page 16</p> <p>LÃZARO-GREDILLA, QUIÃONERO-CANDELA, RASMUSSEN AND FIGUEIRAS-VIDAL<br />investigate more carefully in the future the exact conditions under which these spectacular sparsity<br />vs. accuracy improvements can be expected.<br />Acknowledgments<br />This work has been partly supported by an FPU grant (first author) from the Spanish Ministry<br />of Education and CICYT project TEC-2005-00992 (first and last author). Part of this work was<br />developed while the first author was a visitor at the Computational and Biological Learning Lab,<br />Department of Engineering, University of Cambridge.<br />Appendix A. Details of the Implementation<br />In practice, to improve numerical accuracy and speed, Equations (7) and (8) should be implemented<br />using the Cholesky decomposition R = chol(A). Thus the predictive distribution is computed as<br />E[yâ] = Ï(xâ)â¤R\(Râ¤\(Î¦fy))<br />V[yâ] = Ï2<br />n+Ï2<br />n||Râ¤\Ï(xâ)||2,<br />and the log evidence as<br />logp(y|Î¸) = â<br />1<br />2Ï2n<br />?<br />||y||2â||Râ¤\(Î¦fy)||2?<br />â1<br />2â<br />i<br />logR2<br />ii+mlogmÏ2<br />n<br />Ï2<br />0<br />ân<br />2log2ÏÏ2<br />n,<br />where Riirefers to the diagonal elements of R.<br />References<br />G. L. Bretthorst.<br />Bayesian Methods, pages 1â28. Kluwer, 2000.<br />Nonuniform sampling: Bandwidth and aliasing.In Maximum Entropy and<br />A. B. Carlson. Communication Systems. McGraw-Hill, 3rd edition, 1986.<br />L. CsatÃ³ and M. Opper. Sparse online Gaussian processes. Neural Computation, 14(3):641â669,<br />2002.<br />M. LÃ¡zaro-Gredilla. Sparse Gaussian Processes for Large-Scale Machine Learning. PhD the-<br />sis, Universidad Carlos III de Madrid, 2010.<br />publications.php.<br />URL http://www.tsc.uc3m.es/~miguel/<br />M. LÃ¡zaro-Gredilla and A.R. Figueiras-Vidal. Inter-domain Gaussian processes for sparse inference<br />using inducing features. In Advances in Neural Information Processing Systems 22, pages 1087â<br />1095. MIT Press, 2010.<br />M. LÃ¡zaro-Gredilla, J. QuiÃ±onero-Candela, and A. Figueiras-Vidal. Sparse spectral sampling Gaus-<br />sian processes. Technical report, Microsoft Research, 2007.<br />D. J. C. MacKay. Information Theory, Inference, and Learning Algorithms. Cambridge University<br />Press, 2003.<br />1880</p>  <p>Page 17</p> <p>SPARSE SPECTRUM GAUSSIAN PROCESS REGRESSION<br />A. Naish-Guzman and S. Holden. The generalized FITC approximation. In Advances in Neural<br />Information Processing Systems 20, pages 1057â1064. MIT Press, 2008.<br />J. QuiÃ±onero-Candela and C. E. Rasmussen. A unifying view of sparse approximate Gaussian<br />process regression. Journal of Machine Learning Research, 6:1939â1959, 2005.<br />J. QuiÃ±onero-Candela, C. E. Rasmussen, and C. K. I. Williams. Approximation methods for Gaus-<br />sian process regression. In L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, editors, Large-<br />Scale Kernel Machines, pages 203â223. MIT Press, 2007.<br />A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in Neural<br />Information Processing Systems 20, pages 1177â1184. MIT Press, Cambridge, MA, 2008.<br />C. E. Rasmussen and Joaquin QuiÃ±onero-Candela. Healing the relevance vector machine through<br />augmentation. In Proceedings of the 22nd International Conference on Machine Learning, pages<br />689â696, 2005.<br />C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press,<br />2006.<br />M. Seeger, C. K. I. Williams, and N. D. Lawrence. Fast forward selection to speed up sparse<br />Gaussian process regression. In Proceedings of the 9th International Workshop on AI Stats,<br />2003.<br />A. J. Smola and P. Bartlett. Sparse greedy Gaussian process regression. In Advances in Neural<br />Information Processing Systems 13, pages 619â625. MIT Press, 2001.<br />E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances in<br />Neural Information Processing Systems 18, pages 1259ËUâ1266. MIT Press, 2006.<br />M. L. Stein. Interpolation of Spatial Data. Springer Verlag, 1999.<br />M. K. Titsias. Variational learning of inducing variables in sparse Gaussian processes. In Proceed-<br />ings of the 12th International Workshop on AI Stats, 2009.<br />V. Tresp. A Bayesian committee machine. Neural Computation, 12:2719â2741, 2000.<br />R. Urtasun and T. Darrell. Sparse probabilistic regression for activity-independent human pose<br />inference. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on,<br />pages 1â8, 2008.<br />C. Walder, K. I. Kim, and B. SchÃ¶lkopf. Sparse multiscale Gaussian process regression. In 25th<br />International Conference on Machine Learning. ACM Press, New York, 2008.<br />C. K. I. Williams. Computing with infinite networks. In Advances in Neural Information Processing<br />Systems 9, pages 1069â1072. MIT Press, 1997.<br />C. K. I. Williams and M. Seeger. Using the NystrÃ¶m method to speed up kernel machines. In<br />Advances in Neural Information Processing Systems 13, pages 682â688. MIT Press, 2001.<br />1881</p>  <a href="https://www.researchgate.net/profile/Miguel_Lazaro-Gredilla/publication/234779817_Sparse_Spectrum_Gaussian_Process_Regression/links/0c96051ae1c817cdb7000000.pdf">Download full-text</a> </div> <div id="rgw19_56aba06f4cb75" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw20_56aba06f4cb75">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw21_56aba06f4cb75"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Miguel_Lazaro-Gredilla/publication/234779817_Sparse_Spectrum_Gaussian_Process_Regression/links/0c96051ae1c817cdb7000000.pdf" class="publication-viewer" title="0c96051ae1c817cdb7000000.pdf">0c96051ae1c817cdb7000000.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Miguel_Lazaro-Gredilla">Miguel LÃ¡zaro-Gredilla</a> &middot; May 23, 2014 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw22_56aba06f4cb75"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.168.5864&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Sparse Spectrum Gaussian Process Regression">Sparse Spectrum Gaussian Process Regression</a> </div>  <div class="details">   Available from <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.168.5864&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow">citeseerx.ist.psu.edu</a>  </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw23_56aba06f4cb75"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.168.5864&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Sparse Spectrum Gaussian Process Regression">Sparse Spectrum Gaussian Process Regression</a> </div>  <div class="details">   Available from <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.168.5864&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow">psu.edu</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw25_56aba06f4cb75" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw26_56aba06f4cb75">  </ul> </div> </div>   <div id="rgw15_56aba06f4cb75" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw16_56aba06f4cb75"> <div> <h5> <a href="publication/230869907_Real-time_model_learning_using_Incremental_Sparse_Spectrum_Gaussian_Process_Regression" class="color-inherit ga-similar-publication-title"><span class="publication-title">Real-time model learning using Incremental Sparse Spectrum Gaussian Process Regression</span></a>  </h5>  <div class="authors"> <a href="researcher/70325656_Arjan_Gijsberts" class="authors ga-similar-publication-author">Arjan Gijsberts</a>, <a href="researcher/3227665_Giorgio_Metta" class="authors ga-similar-publication-author">Giorgio Metta</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56aba06f4cb75"> <div> <h5> <a href="publication/237082504_Variational_inference_for_sparse_spectrum_Gaussian_process_regression" class="color-inherit ga-similar-publication-title"><span class="publication-title">Variational inference for sparse spectrum Gaussian process regression</span></a>  </h5>  <div class="authors"> <a href="researcher/2067864256_Linda_S_L_Tan" class="authors ga-similar-publication-author">Linda S. L. Tan</a>, <a href="researcher/8484000_David_J_Nott" class="authors ga-similar-publication-author">David J. Nott</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56aba06f4cb75"> <div> <h5> <a href="publication/238595208_Multiuser_Detection_with_Sparse_Spectrum_Gaussian_Process_Regression" class="color-inherit ga-similar-publication-title"><span class="publication-title">Multiuser Detection with Sparse Spectrum Gaussian Process Regression</span></a>  </h5>  <div class="authors"> <a href="researcher/69535421_Shaowei_Wang" class="authors ga-similar-publication-author">Shaowei Wang</a>, <a href="researcher/69684310_Hualai_Gu" class="authors ga-similar-publication-author">Hualai Gu</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw39_56aba06f4cb75" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw40_56aba06f4cb75">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw41_56aba06f4cb75" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=_OIHrfyGzCo9rQtWiWCquYwN0G6JuYy0AC8MqtOCFwlPD3XMQMe3wwFuBJ8LqE3J" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="nR/ty4HJe6wofdzHY2kpT+R/39JJHx/mJKmqzsKKytCOUA9p8SPSvm5m4ImahDZaYeeWpL5tgZGK5Sl7BBuyrdQsALm55xAwuP6BnBuD/UpWQjXh2xHXsaQFRhEnIVrCQ5dtGkJvzfa8iPuFO49jyj3EWq5U+J+y3Zs0JVlHwVI5pCIMcC8B3cCkRLzoidSKW3ag7TeCZSutowAbXsz1mwhiTpWWhRr4SjrrkcSpaqBgVpm2BWdQFF8hs+9C2JzOtQKhw/cXlhguMFDU2q0qDj+m+AV9kUAz7wrY3wOT470="/> <input type="hidden" name="urlAfterLogin" value="publication/234779817_Sparse_Spectrum_Gaussian_Process_Regression"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjM0Nzc5ODE3X1NwYXJzZV9TcGVjdHJ1bV9HYXVzc2lhbl9Qcm9jZXNzX1JlZ3Jlc3Npb24%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjM0Nzc5ODE3X1NwYXJzZV9TcGVjdHJ1bV9HYXVzc2lhbl9Qcm9jZXNzX1JlZ3Jlc3Npb24%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjM0Nzc5ODE3X1NwYXJzZV9TcGVjdHJ1bV9HYXVzc2lhbl9Qcm9jZXNzX1JlZ3Jlc3Npb24%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw42_56aba06f4cb75"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 521;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Miguel L\u00e1zaro-Gredilla","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A276903991758850%401443030464156_m\/Miguel_Lazaro-Gredilla.png","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Miguel_Lazaro-Gredilla","institution":"University Carlos III de Madrid ","institutionUrl":false,"widgetId":"rgw4_56aba06f4cb75"},"id":"rgw4_56aba06f4cb75","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=3117333","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56aba06f4cb75"},"id":"rgw3_56aba06f4cb75","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=234779817","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":234779817,"title":"Sparse Spectrum Gaussian Process Regression","journalTitle":"Journal of Machine Learning Research","journalDetailsTooltip":{"data":{"journalTitle":"Journal of Machine Learning Research","journalAbbrev":"J MACH LEARN RES","publisher":false,"issn":"1532-4435","impactFactor":"2.47","fiveYearImpactFactor":"4.77","citedHalfLife":"8.30","immediacyIndex":"0.31","eigenFactor":"0.03","articleInfluence":"3.23","widgetId":"rgw6_56aba06f4cb75"},"id":"rgw6_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=1532-4435","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"07\/2010;","publicationDateRobot":"2010-07","article":"11:1865-1881.","journalTitle":"Journal of Machine Learning Research","journalUrl":"journal\/1532-4435_Journal_of_Machine_Learning_Research","impactFactor":2.47}},"source":{"sourceUrl":"http:\/\/dblp.uni-trier.de\/db\/journals\/jmlr\/jmlr11.html#Lazaro-GredillaCRF10","sourceName":"DBLP"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Sparse Spectrum Gaussian Process Regression"},{"key":"rft.title","value":"The Journal of Machine Learning Research"},{"key":"rft.jtitle","value":"The Journal of Machine Learning Research"},{"key":"rft.volume","value":"11"},{"key":"rft.date","value":"2010"},{"key":"rft.pages","value":"1865-1881"},{"key":"rft.issn","value":"1532-4435"},{"key":"rft.au","value":"Miguel L\u00e1zaro-Gredilla,Carl Edward Rasmussen"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56aba06f4cb75"},"id":"rgw7_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=234779817","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":234779817,"peopleItems":[{"data":{"authorNameOnPublication":"Miguel L\u00e1zaro-Gredilla","accountUrl":"profile\/Miguel_Lazaro-Gredilla","accountKey":"Miguel_Lazaro-Gredilla","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A276903991758850%401443030464156_m\/Miguel_Lazaro-Gredilla.png","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Miguel L\u00e1zaro-Gredilla","profile":{"professionalInstitution":{"professionalInstitutionName":"University Carlos III de Madrid ","professionalInstitutionUrl":"institution\/University_Carlos_III_de_Madrid"}},"professionalInstitutionName":"University Carlos III de Madrid ","professionalInstitutionUrl":"institution\/University_Carlos_III_de_Madrid","url":"profile\/Miguel_Lazaro-Gredilla","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A276903991758850%401443030464156_l\/Miguel_Lazaro-Gredilla.png","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Miguel_Lazaro-Gredilla","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw10_56aba06f4cb75"},"id":"rgw10_56aba06f4cb75","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=3117333&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University Carlos III de Madrid ","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":2,"accountCount":1,"publicationUid":234779817,"widgetId":"rgw9_56aba06f4cb75"},"id":"rgw9_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=3117333&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=2&accountCount=1&publicationUid=234779817","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/43277170_Carl_Edward_Rasmussen","authorNameOnPublication":"Carl Edward Rasmussen","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Carl Edward Rasmussen","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/43277170_Carl_Edward_Rasmussen","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56aba06f4cb75"},"id":"rgw12_56aba06f4cb75","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=43277170&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56aba06f4cb75"},"id":"rgw11_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=43277170&authorNameOnPublication=Carl%20Edward%20Rasmussen","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56aba06f4cb75"},"id":"rgw8_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=234779817&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":234779817,"abstract":"<noscript><\/noscript><div>We present a new sparse Gaussian Process (GP) model for regression. The key novel idea is to sparsify the spectral representation of the GP. This leads to a simple, practical algorithm for regression tasks. We compare the achievable trade-offs between predictive accuracy and computational requirements, and show that these are typically superior to existing state-of-the-art sparse approximations. We discuss both the weight space and function space representations, and note that the new construction implies priors over functions which are always stationary, and can approximate any covariance function in this class.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw13_56aba06f4cb75"},"id":"rgw13_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=234779817","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/234779817_Sparse_Spectrum_Gaussian_Process_Regression\/links\/0c96051ae1c817cdb7000000\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw14_56aba06f4cb75"},"id":"rgw14_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56aba06f4cb75"},"id":"rgw5_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=234779817&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":70325656,"url":"researcher\/70325656_Arjan_Gijsberts","fullname":"Arjan Gijsberts","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":3227665,"url":"researcher\/3227665_Giorgio_Metta","fullname":"Giorgio Metta","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Sep 2012","journal":"Neural networks: the official journal of the International Neural Network Society","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/230869907_Real-time_model_learning_using_Incremental_Sparse_Spectrum_Gaussian_Process_Regression","usePlainButton":true,"publicationUid":230869907,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.71","url":"publication\/230869907_Real-time_model_learning_using_Incremental_Sparse_Spectrum_Gaussian_Process_Regression","title":"Real-time model learning using Incremental Sparse Spectrum Gaussian Process Regression","displayTitleAsLink":true,"authors":[{"id":70325656,"url":"researcher\/70325656_Arjan_Gijsberts","fullname":"Arjan Gijsberts","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":3227665,"url":"researcher\/3227665_Giorgio_Metta","fullname":"Giorgio Metta","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Neural networks: the official journal of the International Neural Network Society 09\/2012; 41. DOI:10.1016\/j.neunet.2012.08.011"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/230869907_Real-time_model_learning_using_Incremental_Sparse_Spectrum_Gaussian_Process_Regression","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/230869907_Real-time_model_learning_using_Incremental_Sparse_Spectrum_Gaussian_Process_Regression\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56aba06f4cb75"},"id":"rgw16_56aba06f4cb75","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=230869907","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2067864256,"url":"researcher\/2067864256_Linda_S_L_Tan","fullname":"Linda S. L. Tan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8484000,"url":"researcher\/8484000_David_J_Nott","fullname":"David J. Nott","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jun 2013","journal":"Statistics and Computing","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/237082504_Variational_inference_for_sparse_spectrum_Gaussian_process_regression","usePlainButton":true,"publicationUid":237082504,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.62","url":"publication\/237082504_Variational_inference_for_sparse_spectrum_Gaussian_process_regression","title":"Variational inference for sparse spectrum Gaussian process regression","displayTitleAsLink":true,"authors":[{"id":2067864256,"url":"researcher\/2067864256_Linda_S_L_Tan","fullname":"Linda S. L. Tan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8484000,"url":"researcher\/8484000_David_J_Nott","fullname":"David J. Nott","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Statistics and Computing 06\/2013;  DOI:10.1007\/s11222-015-9600-7"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/237082504_Variational_inference_for_sparse_spectrum_Gaussian_process_regression","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/237082504_Variational_inference_for_sparse_spectrum_Gaussian_process_regression\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56aba06f4cb75"},"id":"rgw17_56aba06f4cb75","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=237082504","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":69535421,"url":"researcher\/69535421_Shaowei_Wang","fullname":"Shaowei Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69684310,"url":"researcher\/69684310_Hualai_Gu","fullname":"Hualai Gu","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2012","journal":"IEEE Communications Letters","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/238595208_Multiuser_Detection_with_Sparse_Spectrum_Gaussian_Process_Regression","usePlainButton":true,"publicationUid":238595208,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.27","url":"publication\/238595208_Multiuser_Detection_with_Sparse_Spectrum_Gaussian_Process_Regression","title":"Multiuser Detection with Sparse Spectrum Gaussian Process Regression","displayTitleAsLink":true,"authors":[{"id":69535421,"url":"researcher\/69535421_Shaowei_Wang","fullname":"Shaowei Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69684310,"url":"researcher\/69684310_Hualai_Gu","fullname":"Hualai Gu","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["IEEE Communications Letters 02\/2012; 16(2):164-167. DOI:10.1109\/LCOMM.2011.120211.111508"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/238595208_Multiuser_Detection_with_Sparse_Spectrum_Gaussian_Process_Regression","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/238595208_Multiuser_Detection_with_Sparse_Spectrum_Gaussian_Process_Regression\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56aba06f4cb75"},"id":"rgw18_56aba06f4cb75","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=238595208","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw15_56aba06f4cb75"},"id":"rgw15_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=234779817&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":234779817,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":234779817,"publicationType":"article","linkId":"0c96051ae1c817cdb7000000","fileName":"0c96051ae1c817cdb7000000.pdf","fileUrl":"profile\/Miguel_Lazaro-Gredilla\/publication\/234779817_Sparse_Spectrum_Gaussian_Process_Regression\/links\/0c96051ae1c817cdb7000000.pdf","name":"Miguel L\u00e1zaro-Gredilla","nameUrl":"profile\/Miguel_Lazaro-Gredilla","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"May 23, 2014","fileSize":"174.21 KB","widgetId":"rgw21_56aba06f4cb75"},"id":"rgw21_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=234779817&linkId=0c96051ae1c817cdb7000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":234779817,"publicationType":"article","linkId":"01010bac0cf26c366b889207","fileName":"Sparse Spectrum Gaussian Process Regression","fileUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.168.5864&amp;rep=rep1&amp;type=pdf","name":"citeseerx.ist.psu.edu","nameUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.168.5864&amp;rep=rep1&amp;type=pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw22_56aba06f4cb75"},"id":"rgw22_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=234779817&linkId=01010bac0cf26c366b889207&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":234779817,"publicationType":"article","linkId":"00b6d58c0cf245659d03c0af","fileName":"Sparse Spectrum Gaussian Process Regression","fileUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.168.5864&amp;rep=rep1&amp;type=pdf","name":"psu.edu","nameUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.168.5864&amp;rep=rep1&amp;type=pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw23_56aba06f4cb75"},"id":"rgw23_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=234779817&linkId=00b6d58c0cf245659d03c0af&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw20_56aba06f4cb75"},"id":"rgw20_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=234779817&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":3,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":72,"valueFormatted":"72","widgetId":"rgw24_56aba06f4cb75"},"id":"rgw24_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=234779817","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw19_56aba06f4cb75"},"id":"rgw19_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=234779817&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":234779817,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw26_56aba06f4cb75"},"id":"rgw26_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=234779817&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":72,"valueFormatted":"72","widgetId":"rgw27_56aba06f4cb75"},"id":"rgw27_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=234779817","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw25_56aba06f4cb75"},"id":"rgw25_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=234779817&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Journal of Machine Learning Research 11 (2010) 1865-1881Submitted 2\/08; Revised 2\/10; Published 6\/10\nSparse Spectrum Gaussian Process Regression\nMiguel L\u00e1zaro-Gredilla\nDepartamento de Teor\u00eda de la Se\u00f1al y Comunicaciones\nUniversidad Carlos III de Madrid\n28911 Legan\u00e9s, Madrid, Spain\nJoaquin Qui\u00f1onero-Candela\nMicrosoft Research Ltd.\n7 J J Thomson Avenue\nCambridge CB3 0FB, UK\nCarl Edward Rasmussen\u2217\nDeparment of Engineering\nUniversity of Cambridge, Trumpington st.\nCambridge CB2 1PZ, UK\nMIGUEL@TSC.UC3M.ES\nJOAQUINC@MICROSOFT.COM\nCER54@CAM.AC.UK\nAn\u00edbal R. Figueiras-Vidal\nDepartamento de Teor\u00eda de la Se\u00f1al y Comunicaciones\nUniversidad Carlos III de Madrid\n28911 Legan\u00e9s, Madrid, Spain\nARFV@TSC.UC3M.ES\nEditor: Tommi Jaakkola\nAbstract\nWe present a new sparse Gaussian Process (GP) model for regression. The key novel idea is to\nsparsify the spectral representation of the GP. This leads to a simple, practical algorithm for regres-\nsion tasks. We compare the achievable trade-offs between predictive accuracy and computational\nrequirements, and show that these are typically superior to existing state-of-the-art sparse approx-\nimations. We discuss both the weight space and function space representations, and note that the\nnew construction implies priors over functions which are always stationary, and can approximate\nany covariance function in this class.\nKeywords:\nGaussian process, probabilistic regression, sparse approximation, power spectrum,\ncomputational efficiency\n1. Introduction\nOne of the main practical limitations of Gaussian processes (GPs) for machine learning (Rasmussen\nand Williams, 2006) is that in a direct implementation the computational and memory requirements\nscale as O(n2) and O(n3), respectively. In practice this limits the applicability of exact GP imple-\nmentations to data sets where the number of training samples n does not exceed a few thousand.\nA number of computationally efficient approximations to GPs have been proposed, which re-\nduce storage requirements to O(nm) and the number of computations to O(nm2), where m is much\nsmaller than n. One family of approximations, reviewed in Qui\u00f1onero-Candela and Rasmussen\n(2005), is based on assumptions of conditional independence given a reduced set of m inducing\n\u2217. Also at Max Planck Institute for Biological Cybernetics, Spemannstr. 38, 72076 T\u00fcbingen, Germany .\nc ?2010 Miguel L\u00e1zaro-Gredilla, Joaquin Qui\u00f1onero-Candela, Carl E. Rasmussen and An\u00edbal R. Figueiras-Vidal."},{"page":2,"text":"L\u00c1ZARO-GREDILLA, QUI\u00d1ONERO-CANDELA, RASMUSSEN AND FIGUEIRAS-VIDAL\ninputs. Examples of such models are those proposed in Seeger et al. (2003), Smola and Bartlett\n(2001), Tresp (2000), Williams and Seeger (2001) and Csat\u00f3 and Opper (2002), as well as the Fully\nIndependent Training Conditional (FITC) model, introduced as Sparse Pseudo-input GP (SPGP)\nby Snelson and Ghahramani (2006). Walder et al. (2008) introduced the Sparse Multiscale GP\n(SMGP), a modification of FITC that allows each basis function to have its own set of length-\nscales.1This additional flexibility typically yields some performance improvement over FITC, but\nit also requires learning twice as many parameters. SMGP can alternatively be derived within the\nunifying framework of Qui\u00f1onero-Candela and Rasmussen (2005) if we allow the inducing inputs\nto lie in a transformed domain, as shown in L\u00e1zaro-Gredilla and Figueiras-Vidal (2010).\nAnother family of approximations is based on approximate matrix-vector-multiplications\n(MVMs), where m is for example a reduced number of conjugate gradient steps to solve a sys-\ntem of linear equations. Some of these methods have been briefly reviewed in Qui\u00f1onero-Candela\net al. (2007). Local mixtures of GP have been used by Urtasun and Darrell (2008) for efficient\nmodelling of human poses.\nIn this paper we introduce a stationary trigonometric Bayesian model for regression that retains\nthe computational efficiency of the aforementioned approaches, while improving performance. The\nmodelconsistsofalinearcombinationoftrigonometricfunctionswherebothweightsandphasesare\nintegrated out. All hyperparameters of the model (frequencies and amplitudes) are learned jointly by\nmaximizing the marginal likelihood. This model is a stationary sparse GP that can approximate any\ndesired stationary full GP. Sparse trigonometric expansions have been proposed in several contexts,\nfor example, L\u00e1zaro-Gredilla et al. (2007) and Rahimi and Recht (2008), as discussed further in\nSection 4.3.\nFITC, SMGP, and the model introduced in this paper focus on predictive accuracy at low com-\nputational cost, rather than on faithfully converging towards the full GP as the number of basis func-\ntions grows. Performance-wise, FITC and the more recent SMGP can be regarded as the current\nstate-of-the-art sparse GP approximations, so we will use them as benchmarks in the performance\ncomparisons.\nIn Section 2 we give a brief review of GP regression. In Section 3 we introduce the trigono-\nmetric Bayesian model, and in Section 4 we present the Sparse Spectrum Gaussian Process (SSGP)\nalgorithm. Section 5 contains a comparative performance evaluation on several data sets.\n2. Gaussian Process Regression\nRegression is often formulated as the task of predicting the scalar output y\u2217associated to the D-\ndimensional input x\u2217, given a training data set D \u2261 {xj,yj|j = 1,...n} of n input-output pairs. A\ncommon approach is to assume that the outputs have been generated by an unknown latent function\nf(x) and independently corrupted by additive Gaussian noise of constant variance \u03c32\nn:\nyj = f(xj)+\u03b5j,\n\u03b5j \u223c N (0, \u03c32\nn).\nThe regression task boils down to making inference about f(x). Gaussian process (GP) regression\nis a probabilistic, non-parametric Bayesian approach. A Gaussian process prior distribution on f(x)\nallows us to encode assumptions about the smoothness (or other properties) of the latent function\n1. Note that SMGP only extends FITC in the specific case of the anisotropic squared exponential covariance function,\nwhereas FITC can be applied to any covariance function.\n1866"},{"page":3,"text":"SPARSE SPECTRUM GAUSSIAN PROCESS REGRESSION\n(Rasmussen and Williams, 2006). For any set of inputs {xi}n\nevaluations f = [f(x1),..., f(xn)]\u22a4has a joint Gaussian distribution:\ni=1the corresponding vector of function\np(f|{xi}n\ni=1) = N (f|0,Kff).\nThis paper follows the common practice of setting the mean of the process to zero.2The properties\nof the GP prior over functions are governed by the covariance function\n?Kff\n?\nij= k(xi,xj) = E[f(xi)f(xj)],\n(1)\nwhich determines how the similarity between a pair of function values varies as a function of the\ncorresponding pair of inputs. A covariance function is stationary if it only depends on the difference\nbetween its inputs\nk(xi,xj) = k(xi\u2212xj) = k(\u03c4).\nThe elegance of the GP framework is that the properties of the function are conveniently expressed\ndirectly in terms of the covariance function, rather than implicitly via basis functions.\nTo obtain the predictive distribution p(y\u2217|x\u2217,D) it is useful to express the model in matrix\nnotation by stacking the targets yjin vector y = [y1,...,yn]\u22a4and writing the joint distribution of\ntraining and test targets:\n?y\ny\u2217\n?\n\u223c N\n?\n0,\n?Kff+\u03c32\nnIn\nkf\u2217\nk\u22a4\nf\u2217\nk\u2217\u2217+\u03c32\nn\n??\n,\nwhere kf\u2217is the vector of covariances between f(x\u2217) and the training latent function values, and\nk\u2217\u2217is the prior variance of f(x\u2217). Inis the n\u00d7n identity. The predictive distribution is obtained by\nconditioning on the observed training outputs:\np(y\u2217|x\u2217,D) = N (\u00b5\u2217,\u03c32\n\u2217), where\n?\n\u00b5\u2217= k\u2217f(Kff+\u03c32\n\u03c32\nnIn)\u22121y\n\u2217= \u03c32\nn+k\u2217\u2217\u2212k\u2217f(Kff+\u03c32\nnIn)\u22121kf\u2217.\n(2)\nThe covariance function is parameterized by hyperparameters. Consider for example the sta-\ntionary anisotropic squared exponential covariance function\nkARD(\u03c4) = \u03c32\n0exp(\u22121\n2\u03c4\u22a4\u039b\u22121\u03c4), where \u039b = diag([\u21132\n1, \u21132\n2, ... \u21132\nD]).\n(3)\nThe hyperparameters are the prior variance \u03c32\nthe covariance decays with the distance between inputs. This covariance function is also known\nas the ARD (Automatic Relevance Determination) squared exponential, because it can effectively\nprune input dimensions by growing the corresponding lengthscales.\nIt is convenient to denote all hyperparameters including the noise variance by \u03b8. These can be\nlearned by maximizing the evidence, or log marginal likelihood:\n0and the lengthscales {\u2113d} that determine how rapidly\nlogp(y|\u03b8) = \u2212n\n2log(2\u03c0)\u22121\n2|Kff+\u03c32\nnIn|\u22121\n2y\u22a4?Kff+\u03c32\nnIn\n?\u22121y.\n(4)\nProvided there exist analytic forms for the gradients of the covariance function with respect to the\nhyperparameters, the evidence can be maximized by using a gradient-based search. Unfortunately,\ncomputing the evidence and the gradients requires the inversion of the covariance matrix Kff+\u03c32\nat a cost of O(n3) operations, which is prohibitive for large data sets.\nnIn\n2. The extension to GPs with general mean functions is straightforward.\n1867"},{"page":4,"text":"L\u00c1ZARO-GREDILLA, QUI\u00d1ONERO-CANDELA, RASMUSSEN AND FIGUEIRAS-VIDAL\n3. Trigonometric Bayesian Regression\nIn this section we present a Bayesian linear regression model with trigonometric basis functions,\nand related it to a full GP in the next section. Consider the model\nf(x)=\nm\n\u2211\nr=1\narcos(2\u03c0s\u22a4\nrx)+brsin(2\u03c0s\u22a4\nrx) ,\n(5)\nwhere each of the m pairs of basis functions is parametrized by a D-dimensional vector srof spectral\nfrequencies. Note that each pair of basis functions share frequencies, but each have independent\namplitude parameters, ar and br. We treat the frequencies as deterministic parameters and the\namplitudes in a Bayesian way. The priors are independent Gaussian\nar \u223c N?0,\u03c32\n0\nm\n?,\nbr \u223c N?0,\u03c32\n0\nm\n?,\nwhere the variances are scaled down linearly by the number of basis functions. Under the prior, the\ndistribution over functions from Equation (5) is Gaussian with mean function zero and covariance\nfunction (from Equation (1))\nk(xi,xj) =\u03c32\n0\nm\u03c6(xi)\u22a4\u03c6(xj) =\u03c32\n0\nm\nm\n\u2211\nr=1\ncos?2\u03c0s\u22a4\nr(xi\u2212xj)?,\n(6)\nwhere we define the column vector of length 2m containing the evaluation of the m pairs of trigono-\nmetric functions at x\n\u03c6(x) =\n?cos(2\u03c0s\u22a4\n1x) sin(2\u03c0s\u22a4\n1x) ... cos(2\u03c0s\u22a4\nmx)\nsin(2\u03c0s\u22a4\nmx)?\u22a4.\nSparse linear models generally induce priors over functions whose variance depends on the input.\nIn contrast, the covariance function in Equation (6) is stationary, that is, the prior variance is inde-\npendent of the input and equal to \u03c32\nfunctions and implies that the predictive variances cannot be \u201chealed\u201d, as proposed in Rasmussen\nand Qui\u00f1onero-Candela (2005) for the case of the Relevance Vector Machine.\nThe predictions and marginal likelihood can be evaluated using Equations (2) and (4), although\ndirect evaluation is computationally inefficient when 2m < n. For the predictive distribution we use\nthe more efficient\n0. This is due to the particular nature of the trigonometric basis\nE[y\u2217] = \u03c6(x\u2217)\u22a4A\u22121\u03a6fy,\nV[y\u2217] = \u03c32\nn+\u03c32\nn\u03c6(x\u2217)\u22a4A\u22121\u03c6(x\u2217),\n(7)\nwhere we have defined the 2m by n design matrix \u03a6f= [\u03c6(x1),...,\u03c6(xn)] and A = \u03a6f\u03a6\u22a4\nSimilarly, for the log marginal likelihood\nf+m\u03c32\nn\n\u03c32\n0I2m.\nlogp(y|\u03b8) = \u2212?y\u22a4y\u2212y\u22a4\u03a6\u22a4\nA stable and efficient implementation uses Cholesky decompositions, Appendix A. Both the predic-\ntive distribution and the marginal likelihood can be computed in O(nm2). The predictive mean and\nvariance at an additional test point can be computed in O(m) and O(m2) respectively. The storage\ncosts are also reduced, since we no longer store the full covariance matrix (of size n\u00d7n), but only\nthe design matrix (of size n\u00d72m).\nfA\u22121\u03a6fy?\/(2\u03c32\nn)\u22121\n2log|A|+mlogm\u03c32\nn\n\u03c32\n0\n\u2212n\n2log2\u03c0\u03c32\nn.\n(8)\n1868"},{"page":5,"text":"SPARSE SPECTRUM GAUSSIAN PROCESS REGRESSION\n3.1 Periodicity\nOne might be tempted to assume that this model would only be useful for modeling periodic func-\ntions, since strictly speaking a linear combination of periodic signals is itself periodic. However,\nif the individual frequencies are not all multiples of a common base frequency, then the period\nof the resulting signal will be very long, typically exceeding the range of the inputs by orders of\nmagnitude. Thus, the model based on trigonometric basis functions has practical use for modeling\nnon-periodic functions. The same principle is used (interchanging input and frequency domains)\nin uneven sampling to space apart frequency replicas an avoid aliasing, see for instance Bretthorst\n(2000). As our experimental results suggest, the model provides satisfactory predictive variances.\n3.2 Representation\nAn alternative and equivalent representation of the model in Equation (5), which only uses half\nthe number of trigonometric basis functions is possible, by writing the linear combination of a\nsine and a cosine as a cosine with an amplitude and a phase. Although these two representations\nare equivalent, inference based on them differs. Whereas we have been able to integrate out the\namplitudes to arrive at the GP in Equation (6), this would not be possible analytically using the\nmore parsimonious representation.\nOptimization instead of marginalization of the phases has two important consequences. Firstly,\nwe lose the property of stationarity of the prior over functions. Secondly we may expect that the\nmodel becomes more prone to overfitting. When considering the contribution from a basis func-\ntion (pair) with a specific frequency, the optimization based scheme could fit arbitrarily the phase,\nwhereas the integration based inference is constrained to use a flat prior over phases. In Section 5.3\nwe empirically verify that the computation vs accuracy tradeoff typically favors the less compact\nrepresentation.\n4. The Sparse Spectrum Gaussian Process\nIn the previous section we presented an explicit basis function regression model, but we did not dis-\ncuss how to select the frequencies defining the basis functions. In this section, we present a sparse\nGP approximation view of this model, which shows how it can be understood as a computation-\nally efficient approximation to any GP with stationary covariance function. In the next section we\npresent experimental results showing that dramatic improvements over other state-of-the-art sparse\nGP regression algorithms are possible.\nWe will now take a generic GP with stationary covariance function and sparsify its power spec-\ntraldensitytoobtainasparseGPthatapproximatesthefullGP.Thepowerspectraldensity(orpower\nspectrum) S(s) of a stationary random process expresses how the power is distributed over the fre-\nquency domain. For a stationary GP, the power is equal to the prior variance k(x,x) = k(0) = \u03c32\nThe frequency vector s has the same length D as the input vector x. The d-th element of s can be in-\nterpreted as the frequency associated to the d-th input dimension. The Wiener-Khintchine theorem\n(see for example Carlson, 1986, p. 162) states that the power spectrum and the autocorrelation of\nthe random process constitute a Fourier pair. In our case, given that f(\u00b7) is drawn from a stationary\nGaussian process, the autocorrelation function is equal to the stationary covariance function, and\nwe have:\nk(\u03c4) =\n0.\n?\nRDe2\u03c0is\u22a4\u03c4S(s)ds,\nS(s) =\n?\nRDe\u22122\u03c0is\u22a4\u03c4k(\u03c4)d\u03c4.\n(9)\n1869"},{"page":6,"text":"L\u00c1ZARO-GREDILLA, QUI\u00d1ONERO-CANDELA, RASMUSSEN AND FIGUEIRAS-VIDAL\nWe thus see that there are two equivalent representations for a stationary Gaussian process: the\ntraditional one in terms of the covariance function in the (input) space domain, and a perhaps less\nusual one as the power spectrum in the frequency domain.\nBochner\u2019s theorem (Stein, 1999, p. 24) states that any stationary covariance function k(\u03c4) can\nbe represented as the Fourier transform of a positive finite measure. This means that the power\nspectrum in (9) is a positive finite measure, and in particular that it is proportional to a probability\nmeasure, S(s) \u221d pS(s). The proportionality constant can be directly obtained by evaluating the\ncovariance function in (9) at \u03c4 = 0. We obtain the relation:\nS(s) = k(0)pS(s) = \u03c32\n0pS(s).\n(10)\nWe can use the fact that S(s) is proportional to a multivariate probability density in s to rewrite the\ncovariance function in (9) as an expectation:\nk(xi,xj)=\nk(\u03c4) =\n?\ne2\u03c0is\u22a4xi?\nRDe2\u03c0is\u22a4(xi\u2212xj)S(s)ds = \u03c32\ne2\u03c0is\u22a4xj?\u2217?\n0\n?\nRDe2\u03c0is\u22a4xi?\ne2\u03c0is\u22a4xj?\u2217pS(s)ds\n=\n\u03c32\n0EpS\n?\n,\n(11)\nwhere EpSdenotes expectation wrt. pS(s) and superscript asterisk3denotes complex conjugation.\nThis last expression is an exact expansion of the covariance function as the expectation of a product\nof complex exponentials with respect to a particular distribution over their frequencies. This integral\ncan be approximated by simple Monte Carlo by taking an average of a few samples corresponding\nto a finite set of frequencies, which we call spectral points.\nSince the power spectrum is symmetric around zero, a valid Monte Carlo procedure is to sample\nfrequencies always as a pair {sr,\u2212sr}. This has the advantage of preserving the property of the exact\nexpansion, Equation (11) that the imaginary terms cancel:\nk(xi,xj)\u2243\n\u03c32\n2m\n\u03c32\n0\nmRe\n0\nm\n\u2211\nr=1\n?\n\u2211\nr=1\ne2\u03c0is\u22a4\nrxi?\ne2\u03c0is\u22a4\nrxj?\u2217+\n?\ne2\u03c0is\u22a4\nrxi?\u2217e2\u03c0is\u22a4\n=\u03c32\nm\n\u2211\nr=1\nrxj?\n=\n? m\ne2\u03c0is\u22a4\nrxi?\ne2\u03c0is\u22a4\nrxj?\u2217?\n0\nm\ncos?2\u03c0s\u22a4\nr(xi\u2212xj)?,\nwhere sris drawn from pS(s) and Re[\u00b7] denotes the real part of a complex number. Notice, that\nwe have recovered exactly the expression for the covariance function induced by the trigonometric\nbasis functions model, Equation (6). Further, we have given an interpretation of the frequencies as\nspectral Monte Carlo samples, approximating any stationary covariance function. This is a more\ngeneral result than that of (MacKay, 2003, Ch. 45), which only applies to Gaussian covariances.\nThe approximation is equivalent to replacing the original spectrum S(s) by a set of Dirac deltas of\namplitude \u03c32\nThis convergence result can also be stated as follows: A stationary GP can be seen as a neural\nnetwork with infinitely many hidden units and trigonometric activations if independent priors fol-\nlowing Gaussian and pS(s) distributions are placed on the output and input weights, respectively.\nThis is analogous to the result of Williams (1997) for the non-stationary multilayer perceptron co-\nvariance function.\n0distributed according to pS(s). Thus, we \u201csparsify\u201d the spectrum of the GP.\n1870"},{"page":7,"text":"SPARSE SPECTRUM GAUSSIAN PROCESS REGRESSION\n\u22128\u22126 \u22124\u221220\n\u03c4\n2468\n\u22120.4\n\u22120.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\nCovariance function\n \n \nSE covariance function\n10 spectral points approx.\n\u22128\u22126 \u22124 \u221220\n\u03c4\n2468\n\u22120.4\n\u22120.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\nCovariance function\n \n \nSE covariance function\n50 spectral points approx.\n(a) (b)\nFigure 1: Squared exponential covariance function and its approximation with (a) 10 and (b) 50\nrandom spectral points respectively.\n4.1 Example: The Squared Exponential Covariance Function\nThe probability density associated to the squared exponential covariance function of Equation (3)\ncan be obtained from the Fourier transform\npARD\nS\n(s) =\n1\nkARD(0)\n?\nRDe\u22122\u03c0is\u22a4\u03c4kARD(\u03c4)d\u03c4 =\n?\n|2\u03c0\u039b|exp(\u22122\u03c02s\u22a4\u039bs),\n(12)\nwhich also has the form of a multivariate Gaussian distribution. For illustration purposes, we com-\npare the exact squared exponential covariance function with its spectral approximation in Figure 1,\nwhere the spectral points are sampled from Equation (12). As expected, the quality of the approxi-\nmation improves with the number of samples.\n4.2 The SSGP Algorithm\nOne of the main goals of sparse approximations is to reduce the computational burden while re-\ntaining as much predictive accuracy as possible. Sampling from the spectral density constitutes a\nway of building a sparse approximation. However, we may suspect that we can obtain much sparser\nmodels if the spectral frequencies are learned by optimizing the marginal likelihood, an idea which\nwe pursue in the following.\nThe algorithm we propose uses conjugate gradients to optimize the marginal likelihood (8)\nwith respect to the spectral points {sr} and the hyperparameters \u03c32\nOptimizing with respect to the lengthscales in addition to the spectral points is effectively an over-\nparametrization, but in our experience this redundancy proves helpful in avoiding undesired local\nminima. As is usual with this kind of optimization, the problem is non-convex and we cannot expect\nto find the global optimum. The goal of the optimization is to find a reasonable local optimum.\nIn detail, model selection for the SSGP algorithm consists in:\n0, \u03c32\nn, and {\u21131, \u21132, ... \u2113D}.\n1. Initialize {\u2113d}, \u03c32\ndimensions, the variance of {yj} and \u03c32\n0, and \u03c32\nnto some sensible values. We use one half of the ranges of the input\n0\/4, respectively.\n2. Initialize the {sr} by sampling from (10).\n3. The superscript asterisk denotes complex conjugate and the subscript asterisk indicates test quantity.\n1871"},{"page":8,"text":"L\u00c1ZARO-GREDILLA, QUI\u00d1ONERO-CANDELA, RASMUSSEN AND FIGUEIRAS-VIDAL\n3. Jointly optimize the marginal likelihood wrt. spectral points and hyperparameters.\nThe computational cost of training the SSGP algorithm is O(nm2) per conjugate gradient step.\nAt prediction time, the cost is O(m) for the predictive mean and O(m2) for the predictive vari-\nance per test point. These computational costs are of the same order as those of the majority of\nthe sparse GP approximations that have recently been proposed (see Qui\u00f1onero-Candela and Ras-\nmussen, 2005, for a review).\nLearning the spectral frequencies by optimization departs from the original motivation of ap-\nproximating a full GP. The optimization stage poses a risk of overfitting, which we assess in the\nexperimental section that follows. However, the additional flexibility can potentially improve per-\nformance since it allows learning a covariance function suitable to the problem at hand.\n4.3 Related Algorithms\nFinite decompositions in terms of harmonic basis functions, such as Fourier series, are a classic\nidea. In the context of kernel machines recent work include L\u00e1zaro-Gredilla et al. (2007) for GPs\nand Rahimi and Recht (2008) for Support Vector Machines (SVMs). As we show in the experimen-\ntal section, the details of the implementation turn out to have a critical impact on the performance\nof the algorithms. The SVM based approach uses projections onto a random set of harmonic func-\ntions, whereas the approach used in this paper uses the evidence framework to carefully craft an\noptimized sparse harmonic representation. As is revealed in the experimental section, optimization\nof the frequencies, amplitudes and noise offers dramatic performance improvements for comparable\nsparseness.\n5. Experiments\nIn this section we investigate properties of the SSGP algorithm, and evaluate the computational\ncomplexity vs. accuracy tradeoff. We first relate the FITC and SSGP approximations. We then\npresent empirical comparisons on several data sets, using FITC and SMGP as benchmarks. Finally,\nwe revisit the alternative more compact representation of SSGP using phases, and discuss a data set\nwhere SSGP performs badly.\nOurimplementationofSSGPinmatlabisavailablefrom http:\/\/www.tsc.uc3m.es\/~miguel\/\nsimpletutorialssgp.php together with a simple usage tutorial and the data sets from this sec-\ntion. An implementation of FITC is available from Snelson\u2019s web page at http:\/\/www.gatsby.\nucl.ac.uk\/~snelson .\n5.1 Comparing Predictive Distributions for SSGP and FITC\nWhereas SSGP relies on a sparse approximation to the spectrum, the FITC approximation is sparse\nin a spatial sense: A set of pseudo-inputs is used as an information bottleneck. The only evaluations\nof the covariance function allowed are those involving a function value at a pseudo-input. For a set\nof m pseudo-inputs the computational complexity of FITC is of the same order as that of SSGP with\nm spectral points.\nThe covariance function induced by FITC has a constant prior variance, but it is not stationary.\nThe original covariance of the full GP is only approximated faithfully in the vicinity of the pseudo-\ninputs and the covariance between any two function values that are both far apart from any pseudo-\n1872"},{"page":9,"text":"SPARSE SPECTRUM GAUSSIAN PROCESS REGRESSION\n05 1015\n\u22121\n\u22120.5\n0\n0.5\n1\ninput, x\n(a)\noutput, f(x)\n05 10 15\n\u22121\n\u22120.5\n0\n0.5\n1\ninput, x\n(b)\noutput, f(x)\nFigure 2: Learning the sinc(x) function from 100 noisy observations (plusses) using 40 basis func-\ntions with shaded area showing 95% (noise free) posterior confidence area. In panel (a)\nthe SSGP method with three functions drawn from the posterior is shown. In (b) the same\ndata for the FITC method with samples (dots) drawn from the joint posterior.\n2450\nNumber of basis functions\n(a)\n100200300 500 7501250\n0.001\n0.005\n0.01\n0.05\n0.1\n0.5\nNormalized Mean Squared Error\n \n \nFITC\nSMGP\nSSGP fixed spectral points\nSSGP\nFull GP on 10000 data points\n24 50\nNumber of basis functions\n(b)\n100 200300 500750 1250\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n2\n2.5\nMean Negative Log\u2212Probability\n \n \nFITC\nSMGP\nSSGP fixed spectral points\nSSGP\nFull GP on 10000 data points\nFigure 3: Kin-40k data set. (a) NMSE and (b) MNLP as a function of the number of basis functions.\ninput decays to zero. As a result, functions sampled from the GP prior induced by FITC tend to\nwhite Gaussian noise away from the pseudo-inputs.\n1873"},{"page":10,"text":"L\u00c1ZARO-GREDILLA, QUI\u00d1ONERO-CANDELA, RASMUSSEN AND FIGUEIRAS-VIDAL\n1024 5074 100\n0.04\n0.05\n0.1\nNumber of basis functions\n(a)\nNormalized Mean Squared Error\n \n \nFITC\nSMGP\nSSGP fixed spectral points\nSSGP\nFull GP on 7168 data points\n102450 74100\n\u22120.2\n\u22120.15\n\u22120.1\n\u22120.05\n0\n0.05\n0.1\n0.15\n0.2\nNumber of basis functions\n(b)\nMean Negative Log\u2212Probability\n \n \nFITC\nSMGP\nSSGP fixed spectral points\nSSGP\nFull GP on 7168 data points\nFigure 4: Pumadyn-32nm data set. (a) NMSE and (b) MNLP as a function of the number of basis\nfunctions.\nFigure2comparesthepredictiveposteriordistributionsofSSGPandFITCforasimplesynthetic\ndata set. The training data is generated by evaluating the sinc function on 100 random inputs x \u2208\n[\u22121,5] and adding white, zero-mean Gaussian noise of variance \u03c32\nspectral points sampled from the spectrum of a squared exponential covariance function, and FITC\nis given 40 fixed pseudo-inputs sampled uniformly from the range of the training inputs. The rest\nof the hyperparameters are optimized in both cases by maximizing the marginal likelihood. We plot\nthe 95% confidence interval for both predictive distributions (mean \u00b1 two standard deviations), and\ndraw three samples from the SSGP posterior and one sample from the FITC posterior.\nDespite the different nature of the approximations, the figure shows that for an equal number\nof basis functions both predictive distributions are qualitatively very similar: the uncertainty grows\naway from the training data. In the following section, we verify empirically that the SSGP is a\npractical approximation for modelling non-periodic data.\nn=0.052. SSGP is given 20 fixed\n5.2 Performance Evaluation\nWewillusetwoquantitativeperformancemeasures: thetestNormalizedMeanSquareError(NMSE)\nand the test Mean Negative Log Probability (MNLP) , defined as:\nNMSE =?(y\u2217j\u2212\u00b5\u2217j)2?\n?(y\u2217j\u2212y)2?\nand MNLP =1\n2\n??y\u2217j\u2212\u00b5\u2217j\n\u03c3\u2217j\n?2+log\u03c32\n\u2217j+log2\u03c0\n?\n,\n(13)\nwhere \u00b5\u2217jand \u03c32\nthe actual test value for that sample. The average output value for training data is y. We denote\n\u2217jare, respectively, the predictive mean and variance for test sample j and y\u2217jis\n1874"},{"page":11,"text":"SPARSE SPECTRUM GAUSSIAN PROCESS REGRESSION\nthe average over test cases by ?\u00b7?. For all experiments the values reported are averages over ten\nrepetitions.\nFor each data set we report the performance of five different methods: first, the SSGP algorithm\nas presented in Section 4.2; second, a version of SSGP where the spectral points are \u201cfixed\u201d to sam-\nples from the spectral density of a squared exponential covariance function whose lengthscales are\nlearned (SSGP fixed spectral points);4third, the FITC approximation, learning the pseudo-inputs;\nfourth, SMGP, trained as described in Walder et al. (2008); and finally as a base line comparison\nwe report the result of a full GP trained on the entire training set. We plot the performance as a\nfunction of the number of basis functions. For FITC this is equivalent to the number of pseudo-\ninputs, whereas for SSGP a spectral point corresponds to two basis functions. The number of basis\nfunctions is a good proxy for computational cost.\nWe consider four data sets of size moderate enough to be tractable by a full GP, but still large\nenough that there is a motivation for computationally efficient approximations.\nThe two first data sets are both artificially generated using a robot arm simulator and are highly\nnon-linear and have very low noise. They were both used in Seeger et al. (2003) and Snelson and\nGhahramani (2006), but note that their definition of the NMSE measure differs by a factor of 2\nfrom our definition in (13). We follow precisely their preprocessing and use the original splits. The\nfirst data set is Kin-40k (8 dimensions, 10000 training and 30000 testing samples) and the results\nare displayed in Figure 3. For both error measures SSGP outperforms FITC and SMGP by a large\nmargin, and even improves on the performance of the full GP. The SSGP with fixed spectral points\nis inferior, proving that a greater sparsity vs. accuracy tradeoff can be achieved by optimizing the\nspectral points.\nThe Pumadyn-32nm problem (32 dimensions, 7168 training and 1024 testing samples) can be\nseen as a test of the ARD capabilities of a regression model, since only 4 out of the 32 input\ndimensions are relevant. Following Snelson and Ghahramani (2006), to avoid getting stuck at an\nundesirable bad local optimum, lengthscales are initialized from a full GP on a subset of 1024\ntraining data points, for all compared methods. The results are shown in figure 4.\nThe conclusions are similar as for the Kin-40k data set. SSGP matches the full GP for a surpris-\ningly small number of basis functions.\nThe Pole Telecomm and the Elevators data sets are taken from http:\/\/www.liaad.up.pt\/\n~ltorgo\/Regression\/DataSets.html . In the Pole Telecomm data set we retain 26 dimensions,\nremoving constants. We use the original split, 10000 data for training and 5000 for testing. Both the\ninputs and the outputs take a discrete set of values. In particular, the outputs take values between\n0 and 100, in multiples of 10. We take into account the output quantization by lower bounding the\nvalue of \u03c32\nall the compared methods. The effect is to provide a better estimation for \u03c32\nMNLP measures, but we have observed that this modification has no noticeable effect on NMSE\nvalues. Resulting plots are in Figure 5.\nSSGP is superior in terms of NMSE, getting very close to the full GP for more than 200 basis\nfunctions. In terms of MNLP, SSGP is between FITC and SMGP for small m, but slightly worse\nfor more than 100 basis functions. This may be an indication that SSGP produces better predictive\nmeans than variances. We also see that SSGP with fixed spectral points is uniformly worse.\nnto the value of the quantization noise, bin2\nspacing\/12. This lower bounding is applied to\nnand therefore, better\n4. In practice the spectral points are sampled from the spectral density of a squared exponential covariance function,\nand scaled as the lengthscales adapt.\n1875"},{"page":12,"text":"L\u00c1ZARO-GREDILLA, QUI\u00d1ONERO-CANDELA, RASMUSSEN AND FIGUEIRAS-VIDAL\n1024\nNumber of basis functions\n(a)\n50100 250500 1000\n0.01\n0.02\n0.03\n0.04\n0.05\n0.1\n0.15\n0.2\nNormalized Mean Squared Error\n \n \nFITC\nSMGP\nSSGP fixed spectral points\nSSGP\nFull GP on 10000 data points\n10 24\nNumber of basis functions\n(b)\n50100 250500 1000\n2.5\n3\n3.5\n4\n4.5\n5\n5.5\nMean Negative Log\u2212Probability\n \n \nFITC\nSMGP\nSSGP fixed spectral points\nSSGP\nFull GP on 10000 data points\nFigure 5: Pole Telecomm data set. (a) NMSE and (b) MNLP as a function of the number of basis\nfunctions.\nThe fourth data set, Elevators, relates to controlling the elevators of an F16 aircraft. After\nremoving some constant inputs the data is 17-dimensional. We use the original split with 8752 data\nfor training and 7847 for testing. Results are displayed in Figure 6. SSGP consistently outperforms\nFITC and SMGP and gets very close to the full GP using a very low number of basis functions. The\nlarge NMSE average errors incurred by SSGP with fixed spectral points for small numbers of basis\nfunctions are due to outliers that are present in a small number (about 10 out of 7847) of the test\ninputs, in some of the 10 repeated runs. The predictive variances for these few points are also big,\nso their impact on the MNLP score is small. Such an effect has not been observed in any of the\nother data sets.\n5.3 Explicit Phase Representation\nIn Section 3.2 we considered an alternative representation of the SSGP model using only half the\nbasis functions, but explicitly representing the phases. Bayesian inference in this representation is\nintractable, but one can optimize the phases instead, at the possibly increased risk of overfitting.\nAs an example, we evaluate the performance of the cosine only expansion with explicit phases\non the Pole-Telecomm data set in Figure 7. Whereas the performance for the two variants are\ncomparable for small numbers of basis functions, the cosine only representation becomes worse\nwhen the number of basis functions gets larger, confirming our suspicion that optimization of the\nphases increases the risk of overfitting.\n1876"},{"page":13,"text":"SPARSE SPECTRUM GAUSSIAN PROCESS REGRESSION\n1024\nNumber of basis functions\n(a)\n50100 250500 750 1000\n0.1\n0.15\n0.2\n0.25\nNormalized Mean Squared Error\n \n \nFITC\nSMGP\nSSGP fixed spectral points\nSSGP\nFull GP on 8752 data points\n1024\nNumber of basis functions\n(b)\n50 100250 500 750 1000\n\u22124.8\n\u22124.6\n\u22124.4\n\u22124.2\n\u22124\n\u22123.8\nMean Negative Log\u2212Probability\n \n \nFITC\nSMGP\nSSGP fixed spectral points\nSSGP\nFull GP on 8752 data points\nFigure 6: Elevators data set. (a) NMSE and (b) MNLP as a function of the number of basis func-\ntions.\n5.4 The Pendulum Data Set\nSo far we have seen data sets where SSGP consistently outperforms FITC and SMGP, and often\napproaches the performance of a full GP for quite small numbers of basis functions. In this section\nwe present a counter example, showing that SSGP may occasionally fail, although we suspect that\nthis is the exception rather than the norm.\nThe small data set Pendulum (9 dimensions, 315 training and 315 testing samples) represents\nthe problem of predicting the change in angular velocity of a simulated mechanical pendulum over\na short time frame (50 ms) as a function of various parameters of the dynamical system. The target\nvariable depends heavily on all inputs and the targets are almost noise free. Figure 8 shows the\nresults of our experiments. Note that we use up to 800 basis functions for investigation, although\nfor computational reasons it would make sense to use the full GP rather than an approximation\nwith more than 315 basis functions. Although the SSGP NMSE performance is good, we see that\nespecially for large number of basis functions, the MNLP performance is spectacularly bad. A\ncloser inspection shows that the mean predictions are quite accurate, the predictive variances are\nexcessively small. This SSGP model thus exhibits overfitting in the form of being overconfident.\nNote, that the SSGP with fixed spectral points seems to suffer much less from this effect, as would\nbe expected. Interestingly, re-running the SSGP algorithm with different random initializations\ngives very different predictions, the predictive distributions from separate runs disagreeing wildly.\nOne could perhaps diagnose the occurrence of the problem in this way. The bottom line is that\nany algorithm which optimizes the marginal likelihood over a large number of parameters, will risk\n1877"},{"page":14,"text":"L\u00c1ZARO-GREDILLA, QUI\u00d1ONERO-CANDELA, RASMUSSEN AND FIGUEIRAS-VIDAL\n1024\nNumber of basis functions\n(a)\n50100 250500 1000\n0.01\n0.02\n0.03\n0.04\n0.05\n0.1\n0.15\n0.2\nNormalized Mean Squared Error\n \n \nFITC\nSSGP Cosines Only\nSSGP\nFull GP on 10000 data points\n1024\nNumber of basis functions\n(b)\n50100250500 1000\n2.5\n3\n3.5\n4\n4.5\nMean Negative Log\u2212Probability\n \n \nFITC\nSSGP Cosines Only\nSSGP\nFull GP on 10000 data points\nFigure 7: Pole Telecomm data set. (a) NMSE and (b) MNLP as a function of the number of basis\nfunctions, comparing SSGP with the version with cosines only and explicit phases.\nfalling in the overfitting trap. We nevertheless think that the SSGP algorithm will often have very\ngood performance, and will be a practically important algorithm, although one must use it with care.\n6. Discussion\nWe have introduced the Sparse Spectrum Gaussian Process (SSGP) algorithm, a novel perspective\non sparse GP approximations where rather than the usual sparsity approximation in the spatial do-\nmain, it is the spectrum of the covariance function that is subject to a sparse approximation by\nmeans of a discrete set of samples, the spectral points. We have provided a detailed comparison\nof the computational complexity vs. accuracy tradeoff of SSGP to that of the state of the art GP\nsparse approximation FITC and its extension SMGP. SSGP shows a dramatic improvement in four\ncommonly used benchmark regression data sets, including the two data sets used for evaluation\nin the paper where FITC was originally proposed (Snelson and Ghahramani, 2006). However, we\nfound a small data set where SSGP badly fails, with good predictive means but with overconfident\npredictive variances. This indicates that although SSGP is practically a very appealing algorithm,\ncare must be taken to avoid the occasional risk of overfitting.\nOther algorithms, such as the variational approach of Titsias (2009) which focus on approaching\nthe full GP in the limit of large numbers of basis functions are to a large degree safeguarded from\noverfitting. However, algorithms derived from GPs whose focus is on achieving good predictive\naccuracy on a limited computational budget, such as FITC, SMGP and the currently proposed SSGP,\n1878"},{"page":15,"text":"SPARSE SPECTRUM GAUSSIAN PROCESS REGRESSION\n1024\nNumber of basis functions\n(a)\n50 100200400800\n0.25\n0.3\n0.4\n0.5\n0.6\n0.7\nNormalized Mean Squared Error\n \n \nFITC\nSMGP\nSSGP fixed spectral points\nSSGP\nFull GP on 315 data points\n1024\nNumber of basis functions\n(b)\n50 100 200400 800\n2\n4\n6\n8\n10\n12\n14\n16\n18\nMean Negative Log\u2212Probability\n \n \nFITC\nSMGP\nSSGP fixed spectral points\nSSGP\nFull GP on 315 data points\nFigure 8: Pendulum data set. (a) NMSE and (b) MNLP as a function of the number of basis func-\ntions.\ntypically achieve superior performance, see Figure 3 in Titsias (2009), with some risk of overfitting.\nNote, that these algorithms don\u2019t generally converge toward the full GP.\nAn equivalent view of SSGP is as a sparse Bayesian linear combination of pairs of trigonometric\nbasis functions, a sine and a cosine for each spectral point. The weights are integrated out, and at\nthe price of having two basis functions per frequency, the phases are effectively integrated out as\nwell. We have shown that although a representation in terms of a single basis function per frequency\nand an explicit phase is possible, learning the phases poses an increased risk of overfitting. If the\nspectral points are sampled from the power spectrum of a stationary GP, then SSGP approximates\nits covariance function. However, much sparser solutions can be achieved by learning the spectral\npoints, which effectively implies learning the covariance function. The SSGP model is to the best\nof our knowledge the only sparse GP approximation that induces a stationary covariance function.\nSSGP has been presented here as a Gaussian process prior for regression with a tractable like-\nlihood function from the assumption of Gaussian observation noise. Extending to other types of\nanalytically intractable likelihood functions, such as sigmoid for classification or Laplace for robust\nregression is possible by using the same approximation techniques as for full GPs. An example\nis the use of Expectation Propagation in the derivation of generalized FITC (Naish-Guzman and\nHolden, 2008). Further modifications and extensions of SSGP are discussed in L\u00e1zaro-Gredilla\n(2010).\nThe main differences between SSGP and most previous approaches to sparse GP regression is\nthe stationarity of the prior and the non-local nature of the basis functions. It will be interesting to\n1879"},{"page":16,"text":"L\u00c1ZARO-GREDILLA, QUI\u00d1ONERO-CANDELA, RASMUSSEN AND FIGUEIRAS-VIDAL\ninvestigate more carefully in the future the exact conditions under which these spectacular sparsity\nvs. accuracy improvements can be expected.\nAcknowledgments\nThis work has been partly supported by an FPU grant (first author) from the Spanish Ministry\nof Education and CICYT project TEC-2005-00992 (first and last author). Part of this work was\ndeveloped while the first author was a visitor at the Computational and Biological Learning Lab,\nDepartment of Engineering, University of Cambridge.\nAppendix A. Details of the Implementation\nIn practice, to improve numerical accuracy and speed, Equations (7) and (8) should be implemented\nusing the Cholesky decomposition R = chol(A). Thus the predictive distribution is computed as\nE[y\u2217] = \u03c6(x\u2217)\u22a4R\\(R\u22a4\\(\u03a6fy))\nV[y\u2217] = \u03c32\nn+\u03c32\nn||R\u22a4\\\u03c6(x\u2217)||2,\nand the log evidence as\nlogp(y|\u03b8) = \u2212\n1\n2\u03c32n\n?\n||y||2\u2212||R\u22a4\\(\u03a6fy)||2?\n\u22121\n2\u2211\ni\nlogR2\nii+mlogm\u03c32\nn\n\u03c32\n0\n\u2212n\n2log2\u03c0\u03c32\nn,\nwhere Riirefers to the diagonal elements of R.\nReferences\nG. L. Bretthorst.\nBayesian Methods, pages 1\u201328. Kluwer, 2000.\nNonuniform sampling: Bandwidth and aliasing.In Maximum Entropy and\nA. B. Carlson. Communication Systems. McGraw-Hill, 3rd edition, 1986.\nL. Csat\u00f3 and M. Opper. Sparse online Gaussian processes. Neural Computation, 14(3):641\u2013669,\n2002.\nM. L\u00e1zaro-Gredilla. Sparse Gaussian Processes for Large-Scale Machine Learning. PhD the-\nsis, Universidad Carlos III de Madrid, 2010.\npublications.php.\nURL http:\/\/www.tsc.uc3m.es\/~miguel\/\nM. L\u00e1zaro-Gredilla and A.R. Figueiras-Vidal. Inter-domain Gaussian processes for sparse inference\nusing inducing features. In Advances in Neural Information Processing Systems 22, pages 1087\u2013\n1095. MIT Press, 2010.\nM. L\u00e1zaro-Gredilla, J. Qui\u00f1onero-Candela, and A. Figueiras-Vidal. Sparse spectral sampling Gaus-\nsian processes. Technical report, Microsoft Research, 2007.\nD. J. C. MacKay. Information Theory, Inference, and Learning Algorithms. Cambridge University\nPress, 2003.\n1880"},{"page":17,"text":"SPARSE SPECTRUM GAUSSIAN PROCESS REGRESSION\nA. Naish-Guzman and S. Holden. The generalized FITC approximation. In Advances in Neural\nInformation Processing Systems 20, pages 1057\u20131064. MIT Press, 2008.\nJ. Qui\u00f1onero-Candela and C. E. Rasmussen. A unifying view of sparse approximate Gaussian\nprocess regression. Journal of Machine Learning Research, 6:1939\u20131959, 2005.\nJ. Qui\u00f1onero-Candela, C. E. Rasmussen, and C. K. I. Williams. Approximation methods for Gaus-\nsian process regression. In L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, editors, Large-\nScale Kernel Machines, pages 203\u2013223. MIT Press, 2007.\nA. Rahimi and B. Recht. Random features for large-scale kernel machines. In Advances in Neural\nInformation Processing Systems 20, pages 1177\u20131184. MIT Press, Cambridge, MA, 2008.\nC. E. Rasmussen and Joaquin Qui\u00f1onero-Candela. Healing the relevance vector machine through\naugmentation. In Proceedings of the 22nd International Conference on Machine Learning, pages\n689\u2013696, 2005.\nC. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. The MIT Press,\n2006.\nM. Seeger, C. K. I. Williams, and N. D. Lawrence. Fast forward selection to speed up sparse\nGaussian process regression. In Proceedings of the 9th International Workshop on AI Stats,\n2003.\nA. J. Smola and P. Bartlett. Sparse greedy Gaussian process regression. In Advances in Neural\nInformation Processing Systems 13, pages 619\u2013625. MIT Press, 2001.\nE. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs. In Advances in\nNeural Information Processing Systems 18, pages 1259\u02ddU\u20131266. MIT Press, 2006.\nM. L. Stein. Interpolation of Spatial Data. Springer Verlag, 1999.\nM. K. Titsias. Variational learning of inducing variables in sparse Gaussian processes. In Proceed-\nings of the 12th International Workshop on AI Stats, 2009.\nV. Tresp. A Bayesian committee machine. Neural Computation, 12:2719\u20132741, 2000.\nR. Urtasun and T. Darrell. Sparse probabilistic regression for activity-independent human pose\ninference. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on,\npages 1\u20138, 2008.\nC. Walder, K. I. Kim, and B. Sch\u00f6lkopf. Sparse multiscale Gaussian process regression. In 25th\nInternational Conference on Machine Learning. ACM Press, New York, 2008.\nC. K. I. Williams. Computing with infinite networks. In Advances in Neural Information Processing\nSystems 9, pages 1069\u20131072. MIT Press, 1997.\nC. K. I. Williams and M. Seeger. Using the Nystr\u00f6m method to speed up kernel machines. In\nAdvances in Neural Information Processing Systems 13, pages 682\u2013688. MIT Press, 2001.\n1881"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Miguel_Lazaro-Gredilla\/publication\/234779817_Sparse_Spectrum_Gaussian_Process_Regression\/links\/0c96051ae1c817cdb7000000.pdf","widgetId":"rgw28_56aba06f4cb75"},"id":"rgw28_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=234779817&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw29_56aba06f4cb75"},"id":"rgw29_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=234779817&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":234779817,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"0c96051ae1c817cdb7000000","name":"Miguel L\u00e1zaro-Gredilla","date":null,"nameLink":"profile\/Miguel_Lazaro-Gredilla","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Miguel_Lazaro-Gredilla\/publication\/234779817_Sparse_Spectrum_Gaussian_Process_Regression\/links\/0c96051ae1c817cdb7000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Miguel_Lazaro-Gredilla\/publication\/234779817_Sparse_Spectrum_Gaussian_Process_Regression\/links\/0c96051ae1c817cdb7000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"39223c691946b1e03e2383fab847be5d","showFileSizeNote":false,"fileSize":"174.21 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"0c96051ae1c817cdb7000000","name":"Miguel L\u00e1zaro-Gredilla","date":null,"nameLink":"profile\/Miguel_Lazaro-Gredilla","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Miguel_Lazaro-Gredilla\/publication\/234779817_Sparse_Spectrum_Gaussian_Process_Regression\/links\/0c96051ae1c817cdb7000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Miguel_Lazaro-Gredilla\/publication\/234779817_Sparse_Spectrum_Gaussian_Process_Regression\/links\/0c96051ae1c817cdb7000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"39223c691946b1e03e2383fab847be5d","showFileSizeNote":false,"fileSize":"174.21 KB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=XhPavkOtUIBCJi7kCxNCakvZOiSfGJLc_OKGzZFnz84XRyeylWKQFx8bTkKKltEUQraB5r32lEqNG7lbUUXGmw.s330SGhn4w7y8chvoMkXIn-RcCmoVTBukr2Zt9_6wdfLGsP6D8M7b2wh-Zy9cHBdQ3ETHVJyFzLth83zgZ0hpA","clickOnPill":"publication.PublicationFigures.html?_sg=zixuwmspUVwaG6Kg_bQFugptZFlELJVgUXIOHz1sHfUETOkV9BWsZI7d-ZZka1cktHBZjHN9hIFCgpUDPwYexQ.FIxkWXPxRYJQ_Vd9agRHn1sd8nFyAjtS3h8xLSw0qfrOfajmMIi1wB5N0VXLLo3KC6dbCFqijgHcVHo3SPmVyg"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMiguel_Lazaro-Gredilla%2Fpublication%2F234779817_Sparse_Spectrum_Gaussian_Process_Regression%2Flinks%2F0c96051ae1c817cdb7000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=C1SY6PSEDjZAX-W_HL6QN5pmPbsXJJRBG2D5HbAHOiwP9zNBEYqWRiph_aiyFtW90db4kzn-X0_4ac_hYll3vA","urlHash":"fcc9ca5385a234428013f76dbc73e853","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=pn_db8tfeoL2E57nIKsfr0Jac0sF1f3ONBFvjrLmYh6R2EJubRGoXoT2DrUZChH5wLMG6B15s9HEAwjN0doLjn8ww-jjI-qlA5HRVHmaDmc.PieljRtR9DJbbyQlLsM_bmY4kxv_tNOOAceiqTxRMjBg4vBKX-Eu2BnjxKDGAOct1aEdyPiEOOaELIOy-n_JlA.UgRB4l3BBez0pHJyrV7SxDypPbnijOx0HPRPRULE2CLYqd6Npq7LRoPYBLyU1fKx70ypjIn5NdfVWayZqOehOw","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"0c96051ae1c817cdb7000000","trackedDownloads":{"0c96051ae1c817cdb7000000":{"v":false,"d":false}},"assetId":"AS:99859960958977@1400819878274","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":234779817,"commentCursorPromo":null,"widgetId":"rgw31_56aba06f4cb75"},"id":"rgw31_56aba06f4cb75","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMiguel_Lazaro-Gredilla%2Fpublication%2F234779817_Sparse_Spectrum_Gaussian_Process_Regression%2Flinks%2F0c96051ae1c817cdb7000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A99859960958977%401400819878274&publicationUid=234779817&linkId=0c96051ae1c817cdb7000000&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Sparse Spectrum Gaussian Process Regression","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=0MvFzbPWm8ceG5koWeubsrhwOl4oCo1R7ikV9m8xUpX6UutiVGiS4hIS06GNBdnsWseg9g94Z5F4JKaB3wTe5GGn27llntLva1Bz3ICV52o.wERwDg82uXkd7ZpDm5KlJ3XG_x1DYSeow0UoQzA9yLUeQU9zXdA1MM_zCCvTNtnLIgDg_bJ6GS8sc8fo_wqPVw.GXIU8KBHKtKSUzbKNBowDBm8Huo1rzlWFuBkHpZ-fJAhwnh42n3SR7KaPqOTlr0mdkzWDeGU6Z-gsU5jhCVazg","publicationUid":234779817,"trackedDownloads":{"0c96051ae1c817cdb7000000":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw33_56aba06f4cb75"},"id":"rgw33_56aba06f4cb75","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw34_56aba06f4cb75"},"id":"rgw34_56aba06f4cb75","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw35_56aba06f4cb75"},"id":"rgw35_56aba06f4cb75","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw36_56aba06f4cb75"},"id":"rgw36_56aba06f4cb75","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw37_56aba06f4cb75"},"id":"rgw37_56aba06f4cb75","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw32_56aba06f4cb75"},"id":"rgw32_56aba06f4cb75","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw30_56aba06f4cb75"},"id":"rgw30_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/234779817_Sparse_Spectrum_Gaussian_Process_Regression","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56aba06f4cb75"},"id":"rgw2_56aba06f4cb75","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":234779817},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=234779817&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56aba06f4cb75"},"id":"rgw1_56aba06f4cb75","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"JE\/TofzPXxlynRSEqG4SNsCyYvp0O1TmHwPw7KaByIF+oAVtV9Fp61VR2cA4\/sQXztyYNZiFJEhkE90IIox4lyPuF4qlVVwcCM0LzrhAH5IYlYOcZ1Dliy0b\/oArFYiaOC5mHaBhoivkvcvfUVyCKvx862HyKtgZ+4YyIh7cyZD3nZhMZuKDGu+MhcuTfEdRwMs\/shfVd7gw0zRujTq9v2d0XWJn7rcFozU1mfpg8lWjLedZoym6aFXnezWDCUeMx7KRSAqlnBffAmzW6OYDQbum9KpSsC4pc81T7ZmDnXE=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/234779817_Sparse_Spectrum_Gaussian_Process_Regression\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Sparse Spectrum Gaussian Process Regression\" \/>\n<meta property=\"og:description\" content=\"We present a new sparse Gaussian Process (GP) model for regression. The key novel idea is to sparsify the spectral representation of the GP. This leads to a simple, practical algorithm for...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/234779817_Sparse_Spectrum_Gaussian_Process_Regression\/links\/0c96051ae1c817cdb7000000\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/234779817_Sparse_Spectrum_Gaussian_Process_Regression\" \/>\n<meta property=\"rg:id\" content=\"PB:234779817\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Sparse Spectrum Gaussian Process Regression\" \/>\n<meta name=\"citation_author\" content=\"Miguel L\u00e1zaro-Gredilla\" \/>\n<meta name=\"citation_author\" content=\"Carl Edward Rasmussen\" \/>\n<meta name=\"citation_publication_date\" content=\"2010\/07\/31\" \/>\n<meta name=\"citation_journal_title\" content=\"Journal of Machine Learning Research\" \/>\n<meta name=\"citation_issn\" content=\"1532-4435\" \/>\n<meta name=\"citation_volume\" content=\"11\" \/>\n<meta name=\"citation_firstpage\" content=\"1865\" \/>\n<meta name=\"citation_lastpage\" content=\"1881\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Miguel_Lazaro-Gredilla\/publication\/234779817_Sparse_Spectrum_Gaussian_Process_Regression\/links\/0c96051ae1c817cdb7000000.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/234779817_Sparse_Spectrum_Gaussian_Process_Regression\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/234779817_Sparse_Spectrum_Gaussian_Process_Regression\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-34df5c8d-b0e1-47b7-a3ea-4f36f9dab5e4","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":505,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw38_56aba06f4cb75"},"id":"rgw38_56aba06f4cb75","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-34df5c8d-b0e1-47b7-a3ea-4f36f9dab5e4", "bbb1b08ee347620c603a887c9f3c1449e1693d5d");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-34df5c8d-b0e1-47b7-a3ea-4f36f9dab5e4", "bbb1b08ee347620c603a887c9f3c1449e1693d5d");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw39_56aba06f4cb75"},"id":"rgw39_56aba06f4cb75","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/234779817_Sparse_Spectrum_Gaussian_Process_Regression","requestToken":"nR\/ty4HJe6wofdzHY2kpT+R\/39JJHx\/mJKmqzsKKytCOUA9p8SPSvm5m4ImahDZaYeeWpL5tgZGK5Sl7BBuyrdQsALm55xAwuP6BnBuD\/UpWQjXh2xHXsaQFRhEnIVrCQ5dtGkJvzfa8iPuFO49jyj3EWq5U+J+y3Zs0JVlHwVI5pCIMcC8B3cCkRLzoidSKW3ag7TeCZSutowAbXsz1mwhiTpWWhRr4SjrrkcSpaqBgVpm2BWdQFF8hs+9C2JzOtQKhw\/cXlhguMFDU2q0qDj+m+AV9kUAz7wrY3wOT470=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=_OIHrfyGzCo9rQtWiWCquYwN0G6JuYy0AC8MqtOCFwlPD3XMQMe3wwFuBJ8LqE3J","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjM0Nzc5ODE3X1NwYXJzZV9TcGVjdHJ1bV9HYXVzc2lhbl9Qcm9jZXNzX1JlZ3Jlc3Npb24%3D","signupCallToAction":"Join for free","widgetId":"rgw41_56aba06f4cb75"},"id":"rgw41_56aba06f4cb75","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw40_56aba06f4cb75"},"id":"rgw40_56aba06f4cb75","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw42_56aba06f4cb75"},"id":"rgw42_56aba06f4cb75","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
