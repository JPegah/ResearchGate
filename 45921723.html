<!DOCTYPE html> <html lang="en" class="" id="rgw29_56aba0688b41c"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="ivEtjxYvU6TfUyxGivKIj35lHlKSitT0DJ8dAikeCvptHoKP3F/Y+FXHjrKP9lsPahdl7+rpWeXFQMS7MNr5c3AAnlzPFdBVOpUCUtefvyI+akFwNGFpW8S77BqYpdxVGwKnLJbzp8ddmz8A/EQnuWisIcaE9F8SXJAYhic5UAYaFwe0H8i/5XyZJ1kIydEL9toCL1lJXmXqFcTqfeAGlNeW/krb8SGa4gqgwdedxPC/w2U/LwzgPdED8e8KZNTL8buUMukZdqkQjJxPkc0KwVrtM0c53xKX4n0dDFAKjTQ="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-43a02883-e92f-493d-b50d-36a3b74e02b3",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/45921723_Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Slice sampling covariance hyperparameters of latent Gaussian models" />
<meta property="og:description" content="The Gaussian process (GP) is a popular way to specify dependencies between random variables in a probabilistic model. In the Bayesian framework the covariance structure can be specified using..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/45921723_Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models/links/0f64908538294e886aa206c9/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/45921723_Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models" />
<meta property="rg:id" content="PB:45921723" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Slice sampling covariance hyperparameters of latent Gaussian models" />
<meta name="citation_author" content="Iain Murray" />
<meta name="citation_author" content="Ryan Prescott Adams" />
<meta name="citation_publication_date" content="2010/06/04" />
<meta name="citation_journal_title" content="Advances in neural information processing systems" />
<meta name="citation_issn" content="1049-5258" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/45921723_Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/45921723_Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Slice sampling covariance hyperparameters of latent Gaussian models</title>
<meta name="description" content="Slice sampling covariance hyperparameters of latent Gaussian models on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56aba0688b41c" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56aba0688b41c" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw7_56aba0688b41c">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Slice%20sampling%20covariance%20hyperparameters%20of%20latent%20Gaussian%20models&rft.title=Advances%20in%2C%20Neural%20Information%20Processing%20Systems%2023&rft.jtitle=Advances%20in%2C%20Neural%20Information%20Processing%20Systems%2023&rft.date=2010&rft.issn=1049-5258&rft.au=Iain%20Murray%2CRyan%20Prescott%20Adams&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Slice sampling covariance hyperparameters of latent Gaussian models</h1> <meta itemprop="headline" content="Slice sampling covariance hyperparameters of latent Gaussian models">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/45921723_Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models/links/0f64908538294e886aa206c9/smallpreview.png">  <div id="rgw10_56aba0688b41c" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw11_56aba0688b41c"> <a href="researcher/45470150_Iain_Murray" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Iain Murray" alt="Iain Murray" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Iain Murray</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56aba0688b41c">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/45470150_Iain_Murray"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Iain Murray" alt="Iain Murray" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/45470150_Iain_Murray" class="display-name">Iain Murray</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw13_56aba0688b41c"> <a href="researcher/32349423_Ryan_Prescott_Adams" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Ryan Prescott Adams" alt="Ryan Prescott Adams" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Ryan Prescott Adams</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw14_56aba0688b41c">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/32349423_Ryan_Prescott_Adams"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Ryan Prescott Adams" alt="Ryan Prescott Adams" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/32349423_Ryan_Prescott_Adams" class="display-name">Ryan Prescott Adams</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/1049-5258_Advances_in_neural_information_processing_systems"><span itemprop="name">Advances in neural information processing systems</span></a> </span>        <meta itemprop="datePublished" content="2010-06">  06/2010;               <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1006.0868" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw15_56aba0688b41c" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>The Gaussian process (GP) is a popular way to specify dependencies between random variables in a probabilistic model. In the Bayesian framework the covariance structure can be specified using unknown hyperparameters. Integrating over these hyperparameters considers different possible explanations for the data when making predictions. This integration is often performed using Markov chain Monte Carlo (MCMC) sampling. However, with non-Gaussian observations standard hyperparameter sampling approaches require careful tuning and may converge slowly. In this paper we present a slice sampling approach that requires little tuning while mixing well in both strong- and weak-data regimes. Comment: 9 pages, 4 figures, 4 algorithms. Minor corrections to previous version. This version to appear in Advances in Neural Information Processing Systems (NIPS) 23, 2010</div> </p>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw28_56aba0688b41c">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw27_56aba0688b41c"  itemprop="articleBody">  <p>Page 1</p> <p>Slice sampling covariance hyperparameters<br />of latent Gaussian models<br />Iain Murray<br />School of Informatics<br />University of Edinburgh<br />Ryan Prescott Adams<br />Dept. Computer Science<br />University of Toronto<br />Abstract<br />The Gaussian process (GP) is a popular way to specify dependencies be-<br />tween random variables in a probabilistic model. In the Bayesian framework<br />the covariance structure can be specified using unknown hyperparameters.<br />Integrating over these hyperparameters considers different possible expla-<br />nations for the data when making predictions. This integration is often<br />performed using Markov chain Monte Carlo (MCMC) sampling.<br />ever, with non-Gaussian observations standard hyperparameter sampling<br />approaches require careful tuning and may converge slowly. In this paper<br />present a slice sampling approach that requires little tuning while mixing<br />well in both strong- and weak-data regimes.<br />How-<br />1Introduction<br />Many probabilistic models incorporate multivariate Gaussian distributions to explain de-<br />pendencies between variables. Gaussian process (GP) models and generalized linear mixed<br />models are common examples. For non-Gaussian observation models, inferring the parame-<br />ters that specify the covariance structure can be difficult. Existing computational methods<br />can be split into two complementary classes: deterministic approximations and Monte Carlo<br />simulation. This work presents a method to make the sampling approach easier to apply.<br />In recent work Murray et al. [1] developed a slice sampling [2] variant, elliptical slice sam-<br />pling, for updating strongly coupled a-priori Gaussian variates given non-Gaussian obser-<br />vations. Previously, Agarwal and Gelfand [3] demonstrated the utility of slice sampling for<br />updating covariance parameters, conventionally called hyperparameters, with a Gaussian<br />observation model, and questioned the possibility of slice sampling in more general settings.<br />In this work we develop a new slice sampler for updating covariance hyperparameters. Our<br />method uses a robust representation that should work well on a wide variety of problems,<br />has very few technical requirements, little need for tuning and so should be easy to apply.<br />1.1Latent Gaussian models<br />We consider generative models of data that depend on a vector of latent variables f that are<br />Gaussian distributed with covariance Σθset by unknown hyperparameters θ. These models<br />are common in the machine learning Gaussian process literature [e.g. 4] and throughout the<br />statistical sciences. We use standard notation for a Gaussian distribution with mean m and<br />covariance Σ,<br />N(f;m,Σ) ≡ |2πΣ|−1/2exp?<br />and use f ∼ N(m,Σ) to indicate that f is drawn from a distribution with the density in (1).<br />−1<br />2(f−m)?Σ−1(f−m)?, (1)<br />1<br />arXiv:1006.0868v1  [stat.CO]  4 Jun 2010</p>  <p>Page 2</p> <p>0 0.20.4<br />Input Space, x<br />0.60.81<br />−2<br />−1.5<br />−1<br />−0.5<br />0<br />0.5<br />1<br />Latent values, f<br /> <br /> <br />l = 0.1<br />l = 0.5<br />l = 2<br />(a) Prior draws<br />10<br />−2<br />10<br />−1<br />10<br />0<br />10<br />1<br />0<br />0.02<br />0.04<br />0.06<br />0.08<br />0.1<br />lengthscale, l<br />p(log l | f)<br /> <br /> <br />l = 0.1<br />l = 0.5<br />l = 2<br />(b) Lengthscale given f<br />Figure 1: (a) Shows draws from the prior over f using three different lengthscales in the squared<br />exponential covariance (2). (b) Shows the posteriors over log-lengthscale for these three draws.<br />The generic form of the generative models we consider is summarized by<br />covariance hyperparameters θ ∼ ph,<br />latent variables f ∼ N(0,Σθ),<br />and a conditional likelihood P(data|f) = L(f).<br />The methods discussed in this paper apply to covariances Σθ that are arbitrary positive<br />definite functions of the hyperparameters θ. However, our experiments focus on the popular<br />case where the covariance is associated with N input vectors {xn}N<br />exponential kernel,<br />n=1through the squared-<br />(Σθ)ij=k(xi,xj)=σ2<br />fexp<br />?<br />−1<br />2<br />?D<br />d=1<br />(xd,i−xd,j)2<br />?2<br />d<br />?<br />,(2)<br />with hyperparameters θ={σ2<br />scale of the latent variables f. The ?d give characteristic lengthscales for converting the<br />distances between inputs into covariances between the corresponding latent values f.<br />f,{?d}}. Here σ2<br />fis the ‘signal variance’ controlling the overall<br />For non-Gaussian likelihoods we wish to sample from the joint posterior over unknowns,<br />P(f,θ|data) =1<br />ZL(f)N(f;0,Σθ)ph(θ).(3)<br />We would like to avoid implementing new code or tuning algorithms for different covariances<br />Σθand conditional likelihood functions L(f).<br />2Markov chain inference<br />A Markov chain transition operator T(z?←z) defines a conditional distribution on a new<br />position z?given an initial position z. The operator is said to leave a target distribution π<br />invariant if π(z?) =?T(z?← z)π(z) dz. A standard way to sample from the joint poste-<br />and P(θ|f), invariant. Under fairly mild conditions the Markov chain will equilibrate to-<br />wards the target distribution [e.g. 5].<br />rior (3) is to alternately apply transition operators that leave its conditionals, P(f |data,θ)<br />Recent work has focused on transition operators for updating the latent variables f given<br />data and a fixed covariance Σθ [6, 1]. Updates to the hyperparameters for fixed latent<br />variables f need to leave the conditional posterior,<br />P(θ|f) ∝ N(f;0,Σθ)ph(θ),(4)<br />invariant. The simplest algorithm for this is the Metropolis–Hastings operator, see Algo-<br />rithm 1. Other possibilities include slice sampling [2] and Hamiltonian Monte Carlo [7].<br />2</p>  <p>Page 3</p> <p>Algorithm 1 M–H transition for fixed f<br />Input: Current f and hyperparameters θ;<br />proposal dist. q; covariance function Σ().<br />Output:Next hyperparameters<br />1: Propose: θ?∼ q(θ?; θ)<br />2: Draw u ∼ Uniform(0,1)<br />3: if u &lt;<br />N(f;0,Σθ)ph(θ)q(θ?;θ)<br />4:<br />return θ?<br />5: else<br />6:<br />return θ<br />N(f;0,Σθ?)ph(θ?)q(θ ;θ?)<br />? Accept new state<br />? Keep current state<br />Algorithm 2 M–H transition for fixed ν<br />Input:Current state θ, f; proposal dist. q;<br />covariance function Σ(); likelihood L().<br />Output:Next θ, f<br />1: Solve for N(0,I) variate: ν=L−1<br />2: Propose θ?∼ q(θ?; θ)<br />3: Compute implied values: f?=LΣθ?ν<br />4: Draw u ∼ Uniform(0,1)<br />5: if u &lt;L(f?)ph(θ?)q(θ ;θ?)<br />L(f)ph(θ)q(θ?;θ)<br />6:<br />return θ?, f?<br />7: else<br />8:<br />return θ, f<br />Σθf<br />? Accept new state<br />? Keep current state<br />Alternately fixing the unknowns f and θ is appealing from an implementation standpoint.<br />However, the resulting Markov chain can be very slow in exploring the joint posterior distri-<br />bution. Figure 1a shows latent vector samples using squared-exponential covariances with<br />different lengthscales. These samples are highly informative about the lengthscale hyperpa-<br />rameter that was used, especially for short lengthscales. The sharpness of P(θ|f), Figure 1b,<br />dramatically limits the amount that any Markov chain can update the hyperparameters θ<br />for fixed latent values f.<br />2.1Whitening the prior<br />Often the conditional likelihood is quite weak; this is why strong prior smoothing as-<br />sumptions are often introduced in latent Gaussian models.<br />which there is no data, i.e. L is constant, the target distribution is the prior model,<br />P(f,θ) = N(f;0,Σθ)ph(θ). Sampling from the prior should be easy, but alternately fix-<br />ing f and θ does not work well because they are strongly coupled. One strategy is to<br />reparameterize the model so that the unknown variables are independent under the prior.<br />In the extreme limit in<br />Independent random variables can be identified from a common generative procedure for<br />the multivariate Gaussian distribution. A vector of independent normals, ν, is drawn inde-<br />pendently of the hyperparameters and then deterministically transformed:<br />ν ∼ N(0,I),f = LΣθν,where LΣθL?<br />Σθ=Σθ. (5)<br />Notation: Throughout this paper LC will be any user-chosen square root of covariance<br />matrix C. While any matrix square root can be used, the lower-diagonal Cholesky decom-<br />position is often the most convenient. We would reserve C1/2for the principal square root,<br />because other square roots do not behave like powers: for example, chol(C)−1?= chol(C−1).<br />We can choose to update the hyperparameters θ for fixed ν instead of fixed f. As the<br />original latent variables f are deterministically linked to the hyperparameters θ in (5), these<br />updates will actually change both θ and f. The samples in Figure 1a resulted from using<br />the same whitened variable ν with different hyperparameters. They follow the same general<br />trend, but vary over the lengthscales used to construct them.<br />The posterior over hyperparameters for fixed ν is apparent by applying Bayes rule to the<br />generative procedure in (5), or one can laboriously obtain it by changing variables in (3):<br />P(θ|ν,data) ∝ P(θ,ν,data) = P(θ,f =LΣθν,data)|LΣθ| ∝ ··· ∝ L(f(θ,ν))ph(θ).<br />Algorithm 2 is the Metropolis–Hastings operator for this distribution. The acceptance rule<br />now depends on the latent variables through the conditional likelihood L(f) instead of the<br />prior N(f;0,Σθ) and these variables are automatically updated to respect the prior. In the<br />no-data limit, new hyperparameters proposed from the prior are always accepted.<br />(6)<br />3Surrogate data model<br />Neither of the two algorithms are ideal for statistical applications, which is illustrated in<br />Figure 2. Algorithm 2 is ideal in the “weak data” limit where the latent variables f are<br />3</p>  <p>Page 4</p> <p>00.2 0.4<br />Input Space, x<br />0.60.81<br />−2<br />−1.5<br />−1<br />−0.5<br />0<br />0.5<br />Observations, y<br /> <br /> <br />current state f<br />whitened prior proposal<br />surrogate data proposal<br />Figure 2: A regression problem with Gaussian observations illustrated by 2σ gray bars. The<br />current state of the sampler has a short lengthscale hyperparameter (?=0.3), a longer lengthscale<br />(?=1.5) is being proposed. The current latent variables do not lie on a straight enough line for the<br />long lengthscale to be plausible. Whitening the prior (Section 2.1) updates the latent variables to<br />a straighter line, but ignores the observations. A proposal using surrogate data (Section 3, with Sθ<br />set to the observation noise) sets the latent variables to a draw that is plausible for the proposed<br />lengthscale while being close to the current state.<br />distributed according to the prior. In the example the likelihoods are too restrictive for<br />Algorithm 2’s proposal to be acceptable.In the “strong data” limit, where the latent<br />variables f are fixed by the likelihood L, Algorithm 1 would be ideal. However, the likelihood<br />terms in the example are not so weak that the prior can be ignored.<br />For regression problems with Gaussian noise the latent variables can be marginalised out an-<br />alytically, allowing hyperparameters to be accepted or rejected according to their marginal<br />posterior P(θ|data). If latent variables are required they can be sampled directly from<br />the conditional posterior P(f |θ,data). To build a method that applies to non-Gaussian<br />likelihoods, we create an auxiliary variable model that introduces surrogate Gaussian ob-<br />servations that will guide joint proposals of the hyperparameters and latent variables.<br />We augment the latent Gaussian model with auxiliary variables, g, a noisy version of the<br />true latent variables:<br />P(g|f,θ) = N(g; f,Sθ).<br />For now Sθ is an arbitrary free parameter that could be set by hand to either a fixed<br />value or a value that depends on the current hyperparameters θ. We will discuss how to<br />automatically set the auxiliary noise covariance Sθin Section 3.2.<br />(7)<br />The original model, f ∼ N(0,Σθ) and (7) define a joint auxiliary distribution P(f,g|θ)<br />given the hyperparameters. It is possible to sample from this distribution in the opposite<br />order, by first drawing the auxiliary values from their marginal distribution<br />P(g|θ) = N(g; 0,Σθ+Sθ),<br />and then sampling the model’s latent values conditioned on the auxiliary values from<br />P(f |g,θ) = N(f; mθ,g,Rθ),where some standard manipulations give:<br />Rθ= (Σ−1<br />mθ,g= Σθ(Σθ+Sθ)−1g = RθS−1<br />That is, under the auxiliary model the latent variables of interest are drawn from their<br />posterior given the surrogate data g. Again we can describe the sampling process via a<br />draw from a spherical Gaussian:<br />η ∼ N(0,I),<br />We then condition on the “whitened” variables η and the surrogate data g while updating<br />the hyperparameters θ. The implied latent variables (10) will remain a plausible draw from<br />the surrogate posterior for the current hyperparameters. This is illustrated in Figure 2.<br />(8)<br />θ+S−1<br />θ)−1= Σθ−Σθ(Σθ+Sθ)−1Σθ= Sθ−Sθ(Sθ+Σθ)−1Sθ,<br />θg.(9)<br />f = LRθη + mθ,g,where LRθL?<br />Rθ=Rθ.(10)<br />We can leave the joint distribution (3) invariant by updating the following conditional<br />distribution derived from the above generative model:<br />P(θ|η,g,data) ∝ P(θ,η,g,data) ∝ L(f(η,θ))N(g;0,Σθ+Sθ)ph(θ). (11)<br />4</p>  <p>Page 5</p> <p>Algorithm 3 Surrogate data M–H<br />Input:<br />Output:<br />1: Draw surrogate data: g ∼ N(f,Sθ)<br />2: Compute implied latent variates:<br />η=L−1<br />Rθ(f − mθ,g)<br />3: Propose θ?∼ q(θ?; θ)<br />4: Compute function f?=LRθ?η + mθ?,g<br />5: Draw u ∼ Uniform(0,1)<br />6: if u &lt;<br />L(f)N(g;0,Σθ+Sθ)ph(θ)q(θ?;θ)<br />7:<br />return θ?, f?<br />8: else<br />9:<br />return θ, f<br />θ, f; prop. dist. q; model of Sec. 3.<br />Next θ, f<br />L(f?)N(g;0,Σθ?+Sθ?)ph(θ?)q(θ ;θ?)<br />? Accept new state<br />? Keep current state<br />Algorithm 4 Surrogate data slice sampling<br />Input:<br />Output:<br />1: Draw surrogate data: g ∼ N(f,Sθ)<br />2: Compute implied latent variates:<br />η=L−1<br />Rθ(f − mθ,g)<br />3: Randomly center a bracket:<br />v ∼ Uniform(0,σ), θmin=θ−v, θmax=θmin+σ<br />4: Draw u ∼ Uniform(0,1)<br />5: Determine threshold:<br />y = uL(f)N(g;0,Σθ+Sθ)ph(θ)<br />6: Draw proposal: θ?∼ Uniform(θmin,θmax)<br />7: Compute function f?=LRθ?η + mθ?,g<br />8: if L(f?)N(g;0,Σθ?+Sθ?)ph(θ?) &gt; y<br />9:<br />return f?, θ?<br />10: else if θ?&lt; θ<br />11:<br />Shrink bracket minimum: θmin= θ?<br />12: else<br />13:<br />Shrink bracket maximum: θmax= θ?<br />14: goto 6<br />θ, f; scale σ; model of Sec. 3.<br />Next f, θ<br />The Metropolis–Hastings Algorithm 3 contains a ratio of these terms in the acceptance rule.<br />3.1 Slice sampling<br />The Metropolis–Hastings algorithms discussed so far have a proposal distribution q(θ?;θ)<br />that must be set and tuned. The efficiency of the algorithms depend crucially on careful<br />choice of the scale σ of the proposal distribution. Slice sampling [2] is a family of adaptive<br />search procedures that are much more robust to the choice of scale parameter.<br />Algorithm 3 applies one possible slice sampling algorithm to a scalar hyperparameter θ in<br />the surrogate data model of this section. It has a free parameter σ, the scale of the initial<br />proposal distribution. However, careful tuning of this parameter is not required. If the initial<br />scale is set to a large value, such as the width of the prior, then the width of the proposals<br />will shrink to an acceptable range exponentially quickly. Stepping-out procedures [2] can<br />also adapt initial scales that are too small.<br />3.2 The auxiliary noise covariance Sθ<br />The surrogate data g and noise covariance Sθ define a pseudo-posterior distribution that<br />softly specifies a plausible region within which the latent variables f are updated. The noise<br />covariance determines the size of this region. The first two baseline algorithms of Section 2<br />result from limiting cases of Sθ=αI: 1) if α=0 the surrogate data and the current latent<br />variables are equal then the acceptance ratio reduces to that of Algorithm 1. 2) as α→∞<br />the observations are uninformative about the current state and the pseudo-posterior tends<br />to the prior. In the limit, the acceptance ratio reduces to that of Algorithm 2. One could<br />choose α based on preliminary runs, but such tuning would be burdensome.<br />For likelihood terms that factorize, L(f)=?<br />P(fi|Li,θ) ∝ Li(fi) N(fi;0,(Σθ)ii).<br />A Gaussian can be fitted by moment matching or a Laplace approximation (matching sec-<br />ond derivatives at the mode). Such fits, or close approximations, are often possible analyti-<br />cally and can always be performed numerically as the distribution is only one-dimensional.<br />Given a Gaussian fit to the site-posterior (12) with variance vi, we can set the auxil-<br />iary noise to a level that would result in the same posterior variance at that site alone:<br />(Sθ)ii=(v−1<br />i<br />−(Σθ)ii<br />ing procedure is a grossly simplified first step of “assumed density filtering” or “expectation<br />propagation” [8], which are too expensive for our use in the inner-loop of a Markov chain.<br />iLi(fi), we can measure how much the likeli-<br />hood restricts each variable individually:<br />(12)<br />−1)−1. (Any negative (Σθ)iimust be thresholded.) The moment match-<br />5</p>  <p>Page 6</p> <p>4Related work<br />We have discussed samplers that jointly update strongly-coupled latent variables and hy-<br />perparameters. The hyperparameters can move further in joint moves than their narrow<br />conditional posteriors (e.g., Figure 1b) would allow. A generic way of jointly sampling real-<br />valued variables is Hamiltonian/Hybrid Monte Carlo (HMC) [7]. However, this method is<br />cumbersome to implement and tune; in practice it is difficult to gain a significant advantage<br />by applying HMC to hierarchical models [9].<br />Christensen et al. [10] have also proposed a robust representation for sampling in latent<br />Gaussian models. They use an approximation to the target posterior distribution to con-<br />struct a reparameterization where the unknown variables are close to independent. The<br />approximation replaces the likelihood with a Gaussian form proportional to N(f;ˆf,Λ(ˆf)):<br />ˆf = argmaxfL(f),<br />where Λ is often diagonal, or it was suggested one would only take the diagonal part.<br />This Taylor approximation looks like a Laplace approximation, except that the likelihood<br />function is not a probability density in f. This likelihood fit results in an approximate<br />Gaussian posterior N(f;mθ,g=ˆf,Rθ) as found in (9), with noise Sθ=Λ(ˆf)−1and data g=ˆf.<br />Thinking of the current latent variables as a draw from this approximate posterior,<br />ω∼N(0,I), f = LRθω+mθ,ˆf, suggests using the reparameterization ω = L−1<br />We can then fix the new variables and update the hyperparameters under<br />Λij(ˆf) =<br />∂2log L(f)<br />∂fi∂fj<br />???ˆf,(13)<br />Rθ(f − mθ,ˆf).<br />P(θ|ω,data) ∝ L(f(ω,θ))N(f(ω,θ);0,Σθ)ph(θ)|LRθ|.<br />When the likelihood is Gaussian, the reparameterized variables ω are independent of each<br />other and the hyperparameters. The hope is that approximating non-Gaussian likelihoods<br />will result in nearly-independent parameterizations on which Markov chains will mix rapidly.<br />(14)<br />Taylor expanding some common log-likelihoods around the maximum is not well defined,<br />for example approximating probit or logistic likelihoods for binary classification, or Pois-<br />son observations with zero counts. These Taylor expansions could be seen as giving flat or<br />undefined Gaussian approximations that do not reweight the prior. When all of the like-<br />lihood terms are flat the reparameterization approach reduces to that of Section 2.1. The<br />alternative Sθauxiliary covariances that we have proposed could be used instead.<br />The surrogate data samplers can also be viewed as using reparameterizations, by ignoring<br />the structure of the surrogate data model and treating η =L−1<br />random reparameterization for making proposals. A proposal density q(η?,θ?;η,θ) in the<br />reparameterized space must be multiplied by the Jacobian |L−1<br />in the original parameterization. The probability of proposing the reparameterization must<br />also be included in the Metropolis–Hastings acceptance probability:<br />?<br />A few lines of linear algebra confirms that, as it must do, the same acceptance ratio results<br />as before. However, just substituting (3) into (15) shows that the acceptance probability is<br />very similar to that from the direct reparameterization approach. The differences are that<br />the new latent variables f?are computed using a different pseudo-posterior mean and there<br />is an extra term for the random, rather than fixed, choice of reparameterization.<br />Rθ(f − mθ,g) as an arbitrary<br />Rθ?| to give a proposal density<br />min1,<br />P(θ?,f?|data)·P(g|f?,Sθ?)·q(θ;θ?)|L−1<br />P(θ,f |data)·P(g|f,Sθ)·q(θ?;θ)|L−1<br />Rθ|<br />Rθ?|<br />?<br />.(15)<br />The surrogate data sampler is easier to implement than the previous reparameterization<br />work because the surrogate posterior is centred around the current latent variables. This<br />means that 1) no point estimate, such as the maximum likelihoodˆf, is required. 2) picking<br />the noise covariance Sθpoorly may still produce a workable method, whereas a fixed repa-<br />rameterized can work badly if the true posterior distribution is in the tails of the Gaussian<br />approximation. Christensen et al. [10] pointed out that centering the approximate Gaus-<br />sian likelihood in their reparameterization around the current state is tempting, but that<br />computing the Jacobian of the transformation is then intractable. By construction, the<br />surrogate data model centers the reparameterization near to the current state.<br />6</p>  <p>Page 7</p> <p>5Experiments<br />We empirically compare the performance of the various approaches to GP hyperparameter<br />sampling on four data sets: one regression, one classification, and two Cox process inference<br />problems. Further details are in the rest of this section, with full code as supplementary<br />material. The results are summarized in Figure 3 followed by a discussion section.<br />In each of the experimental configurations, we ran ten independent chains with different<br />random seeds, burning in for 1000 iterations and sampling for 5000 iterations. We quantify<br />the mixing of the chain by estimating the effective number of samples of the complete<br />data likelihood trace using R-CODA [11], and compare that with three cost metrics: the<br />number of hyperparameter settings considered (each requiring a small number of covariance<br />decompositions with O(n3) time complexity), the number of likelihood evaluations, and the<br />total elapsed time on a single core of an Intel Xeon 3GHz CPU.<br />The experiments are designed to test the mixing of hyperparameters θ while sampling from<br />the joint posterior (3). All of the discussed approaches except Algorithm 1 update the latent<br />variables f as a side-effect. However, further transition operators for the latent variables for<br />fixed hyperparameters are required. In Algorithm 2 the “whitened” variables ν remain fixed;<br />the latent variables and hyperparameters are constrained to satisfy f =LΣθν. The surrogate<br />data samplers are ergodic: the full joint posterior distribution will eventually be explored.<br />However, each update changes the hyperparameters and requires expensive computations<br />involving covariances. After computing the covariances for one set of hyperparameters, it<br />makes sense to apply several cheap updates to the latent variables. For every method we<br />applied ten updates of elliptical slice sampling [1] to the latent variables f between each<br />hyperparameter update. One could also consider applying elliptical slice sampling to a<br />reparameterized representation, for simplicity of comparison we do not.<br />Methods<br />eters. Each method used the same slice sampler, as in Algorithm 4, applied to the following<br />model representations. fixed: fixing the latent function f [12]. prior-white: whitening<br />with the prior. surr-site: using surrogate data with the noise level set to match the site<br />posterior (12). We used Laplace approximations for the Poisson likelihood. For classifi-<br />cation problems we used moment matching, because Laplace approximations do not work<br />well [13]. surr-taylor: using surrogate data with noise variance set via Taylor expansion<br />of the log-likelihood. Infinite variances were truncated to a large value. post-taylor and<br />post-site: as for the surr- methods but a fixed reparameterization based on a posterior<br />approximation. The approximations, Laplace and moment matching, refer to Gaussian fits<br />to the ‘site posteriors’ (12).<br />We implemented six 5 methods for updating Gaussian covariance hyperparam-<br />Binary Classification (Ionosphere)<br />binary GP classification: fixed, prior-white, surr-kl and post-white-kl. We applied<br />these methods to the Ionosphere dataset [14], using 200 training data and 34 dimensions.<br />We used a logistic likelihood with zero-mean prior, inferring lengthscales as well as sig-<br />nal variance. The -taylor methods reduce to other methods or don’t apply because the<br />maximum of the log-likelihood is at plus or minus infinity.<br />We evaluated five different methods for performing<br />Gaussian Regression (Synthetic)<br />rameterization of Christensen et al. [10] makes the hyperparameters and latent variables<br />exactly independent. The random centering of the surrogate data model will yield a less<br />effective sampler. We used a Gaussian regression problem to assess how much worse the sur-<br />rogate data method is compared to an ideal reparameterization. The synthetic data set had<br />200 input points in 10-D drawn uniformly within a unit hypercube. The GP had zero mean,<br />unit signal variance and its ten lengthscales in (2) drawn from Uniform(0,√10). Observation<br />noise had variance 0.09. We applied the fixed, prior-white, surr-site/surr-taylor,<br />and post-site/post-taylor methods. The -site and -taylor methods coincide with<br />auxiliary noise matching the observation noise (Sθ= 0.09I).<br />When the observations have Gaussian noise the repa-<br />7</p>  <p>Page 8</p> <p>ionosphere synthetic<br />x1.6e−04   x3.3e−04   x4.3e−05   x4.8e−04   <br />miningredwoods<br />0<br />1<br />2<br />3<br />4<br />Effective samples per likelihood evaluation<br />ionosphere synthetic<br />x2.9e−04   x1.1e−03   x7.4e−04   x3.7e−03   <br />mining redwoods<br />0<br />1<br />2<br />3<br />4<br />Effective samples per covariance construction<br /> <br /> <br />fixedprior−whitesurr−sitepost−sitesurr−taylorpost−taylor<br />ionosphere synthetic<br />x7.7e−03   x5.4e−02   x1.2e−01   x1.5e−02   <br />mining redwoods<br />0<br />1<br />2<br />3<br />4<br />Effective samples per second<br />Figure 3: The results of experimental comparisons of six MCMC methods for GP hyperparameter<br />inference on four data sets. Each figure shows four groups of bars (one for each experiment) and the<br />vertical axis shows the effective number of samples of the complete data likelihood per unit cost.<br />The costs are per likelihood evaluation (left), per covariance construction (center), and per second<br />(right). Means and standard errors for 10 runs are shown. Each group of bars has been rescaled for<br />readability: the number beneath each group gives the effective samples for the surr-site method,<br />which always has bars of height 1. Bars are missing where methods are inapplicable (see text).<br />Cox process inference<br />with a Gaussian process prior for the log-rate. We sampled the hyperparameters in (2) and<br />a mean offset to the log-rate. The model was applied to two point process datasets: 1) a<br />record of mining disasters [15] with 191 events in 112 bins of 365 days. 2) 195 redwood tree<br />locations in a region scaled to the unit square [16] split into 25×25=625 bins. The results<br />for the mining problem were initially highly variable. As the mining experiments were also<br />the quickest we re-ran each chain for 20,000 iterations.<br />We tested all methods on an inhomogeneous Poisson process<br />6Discussion<br />On the Ionosphere classification problem both of the -site methods worked much better<br />than the two baselines. We slightly prefer surr-site as it involves less problem-specific<br />derivations than post-site.<br />On the synthetic test the post- and surr- methods perform very similarly. We had expected<br />the existing post- method to have an advantage of perhaps up to 2–3×, but that was not<br />realized on this particular dataset. The post- methods had a slight time advantage, but<br />this is down to implementation details and is not notable.<br />On the mining problem the Poisson likelihoods are often close to Gaussian, so the existing<br />post-taylor approximation works well, as do all of our new proposed methods. With<br />the Poisson likelihood Gaussian approximations fit most poorly to sites with zero counts.<br />The redwood dataset discretizes two-dimensional space, leading to a large number of bins.<br />The majority of these bins have zero counts, many more than the mining dataset. Taylor<br />expanding the likelihood gives no likelihood contribution for bins with zero counts, so it<br />is unsurprising that post-taylor performs similarly to prior-white. While surr-taylor<br />works better, the best results come from using approximations to the site-posterior (12).<br />The surrogate data method and reparameterization using a site-posterior and our surrogate<br />data methods offer state-of-the-art performance and are the first such advanced methods to<br />be applicable to Gaussian process classification.<br />In any case, an important message from our results is that fixing the latent variables and<br />updating hyperparameters according to the conditional posterior—as used by the vast<br />majority of GP practitioners—can work exceedingly poorly. Even the simple reparameter-<br />ization of “whitening the prior” discussed in Section 2.1 works much better on problems<br />where smoothness is important in the posterior. Even if site approximations are difficult<br />and the more advanced methods presented are inapplicable, the simple whitening repa-<br />rameterization should be given serious consideration when performing MCMC inference of<br />hyperparameters.<br />8</p>  <p>Page 9</p> <p>Acknowledgements<br />This work was supported in part by the IST Programme of the European Community, under<br />the PASCAL2 Network of Excellence, IST-2007-216886. This publication only reflects the<br />authors’ views. RPA is a junior fellow of the Canadian Institute for Advanced Research.<br />References<br />[1] Iain Murray, Ryan Prescott Adams, and David J.C. MacKay. Elliptical slice sampling.<br />In Proceedings of the 13th International Conference on Artificial Intelligence and Statis-<br />tics (AISTATS)., 2010. Volume 9, JMLR: W&amp;CP.<br />http://arxiv.org/abs/1001.0175.<br />[2] Radford M. Neal. Slice sampling. Annals of Statistics, 31(3):705–767, 2003.<br />[3] Deepak K. Agarwal and Alan E. Gelfand. Slice sampling for simulation based fitting<br />of spatial data models. Statistics and Computing, 15(1):61–69, 2005.<br />[4] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for ma-<br />chine learning. MIT Press, 2006.<br />[5] Luke Tierney. Markov chains for exploring posterior distributions. The Annals of<br />Statistics, 22(4):1701–1728, 1994.<br />[6] Michalis Titsias, Neil D Lawrence, and Magnus Rattray. Efficient sampling for Gaussian<br />process inference using control variables. In Advances in Neural Information Processing<br />Systems 21, pages 1681–1688. MIT Press, 2009.<br />[7] Simon Duane, A. D. Kennedy, Brian J. Pendleton, and Duncan Roweth. Hybrid Monte<br />Carlo. Physics Letters B, 195(2):216–222, September 1987.<br />[8] Thomas Minka. Expectation propagation for approximate Bayesian inference, 2001.<br />Corrected version available from<br />http://research.microsoft.com/~minka/papers/ep/.<br />[9] Kiam Choo. Learning hyperparameters for neural network models using Hamiltonian<br />dynamics. Master’s thesis, Department of Computer Science, University of Toronto,<br />2000. Available from http://www.cs.toronto.edu/~radford/ftp/kiam-thesis.ps.<br />[10] Ole F. Christensen, Gareth O. Roberts, and Martin Sk˜ ald. Robust Markov chain Monte<br />Carlo methods for spatial generalized linear mixed models. Journal of Computational<br />and Graphical Statistics, 15(1):1–17, 2006.<br />[11] Mary Kathryn Cowles, Nicky Best, Karen Vines, and Martyn Plummer. R-CODA<br />0.10-5, 2006. http://www-fis.iarc.fr/coda/.<br />[12] Radford M. Neal. Regression and classification using Gaussian process priors. In J. M.<br />Bernardo et al., editors, Bayesian Statistics 6, pages 475–501. OU Press, 1999.<br />[13] Malte Kuss and Carl Edward Rasmussen. Assessing approximate inference for binary<br />Gaussian process classification. Journal of Machine Learning Research, 6:1679–1704,<br />2005.<br />[14] V. G. Sigillito, S. P. Wing, L. V. Hutton, and K. B. Baker. Classification of radar<br />returns from the ionosphere using neural networks. Johns Hopkins APL Technical<br />Digest, 10:262–266, 1989.<br />[15] R. G. Jarrett. A note on the intervals between coal-mining disasters. Biometrika, 66<br />(1):191–193, 1979.<br />[16] Brian D. Ripley. Modelling spatial patterns. Journal of the Royal Statistical Society,<br />Series B, 39:172–212, 1977.<br />9</p>   </div> <div id="rgw20_56aba0688b41c" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw21_56aba0688b41c">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw22_56aba0688b41c"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://arxiv.org/pdf/1006.0868.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Slice sampling covariance hyperparameters of latent Gaussian models">Slice sampling covariance hyperparameters of laten...</a> </div>  <div class="details">   Available from <a href="http://arxiv.org/pdf/1006.0868.pdf" target="_blank" rel="nofollow">ArXiv</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw24_56aba0688b41c" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw25_56aba0688b41c">  </ul> </div> </div>   <div id="rgw16_56aba0688b41c" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw17_56aba0688b41c"> <div> <h5> <a href="publication/291340346_Temperature-Dependent_Emission_Kinetics_of_Colloidal_Semiconductor_Nanoplatelets_Strongly_Modified_by_Stacking" class="color-inherit ga-similar-publication-title"><span class="publication-title">Temperature-Dependent Emission Kinetics of Colloidal Semiconductor Nanoplatelets Strongly Modified by Stacking</span></a>  </h5>  <div class="authors"> <a href="researcher/2059777116_Onur_Erdem" class="authors ga-similar-publication-author">Onur Erdem</a>, <a href="researcher/31320601_Murat_Olutas" class="authors ga-similar-publication-author">Murat Olutas</a>, <a href="researcher/60001925_Burak_Guzelturk" class="authors ga-similar-publication-author">Burak Guzelturk</a>, <a href="researcher/2008924001_Yusuf_Kelestemur" class="authors ga-similar-publication-author">Yusuf Kelestemur</a>, <a href="researcher/38633698_Hilmi_Volkan_Demir" class="authors ga-similar-publication-author">Hilmi Volkan Demir</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56aba0688b41c"> <div> <h5> <a href="publication/282897577_A_Retinex_model_based_on_Absorbing_Markov_Chains" class="color-inherit ga-similar-publication-title"><span class="publication-title">A Retinex model based on Absorbing Markov Chains</span></a>  </h5>  <div class="authors"> <a href="researcher/12000465_Gabriele_Gianini" class="authors ga-similar-publication-author">Gabriele Gianini</a>, <a href="researcher/12409286_Alessandro_Rizzi" class="authors ga-similar-publication-author">Alessandro Rizzi</a>, <a href="researcher/6725194_Ernesto_Damiani" class="authors ga-similar-publication-author">Ernesto Damiani</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56aba0688b41c"> <div> <h5> <a href="publication/283656157_Wind_Power_Forecast_Error_Probabilistic_Model_Using_Markov_Chains" class="color-inherit ga-similar-publication-title"><span class="publication-title">Wind Power Forecast Error Probabilistic Model Using Markov Chains</span></a>  </h5>  <div class="authors"> <a href="researcher/2084342649_S_Martin_Martinez" class="authors ga-similar-publication-author">S. Martín Martinez</a>, <a href="researcher/2084370777_A_Honrubia_Escribano" class="authors ga-similar-publication-author">A. Honrubia Escribano</a>, <a href="researcher/2084353056_M_Canas_Carreton" class="authors ga-similar-publication-author">M. Cañas Carretón</a>, <a href="researcher/2084376137_V_Guerrero_Mestre" class="authors ga-similar-publication-author">V. Guerrero Mestre</a>, <a href="researcher/2084375230_E_Gomez_Lazaro" class="authors ga-similar-publication-author">E. Gómez Lázaro</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw30_56aba0688b41c" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw31_56aba0688b41c">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw32_56aba0688b41c" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=aaMwev2tfuG6Aivn1wefgyCyub8e1jOs2V2iQ5q20ifYQYSAAVY5jx6bhEFLO4nk" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="2GPOyCpE1fu0v8xgG+sN7aSC+hzxfLXSKSNHUy47sxC3EOcSZBq3eJ+kEDqAye6wOhsqbmvlYjkpCXBEQ204RAmD0zqWgCzwEqG7YX1o9UFhkA43x2rnGqYDXlxCQ27OX+lhdysG8FI88ZGLQbzf0QkCI86KzbqhJ5KZVgZKC0uRsjksZ3ulNiZFWlRpy1purzQC8VeSmpRaI4fOM3F3BLC7wufXUgxtbGc3TuCHdTsjgH8uz/oJTiI9qBUxFI9ah/2WSTwYxOXC0TgvMszC9ewqo3qqDyESSIPxO1qLoV4="/> <input type="hidden" name="urlAfterLogin" value="publication/45921723_Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vNDU5MjE3MjNfU2xpY2Vfc2FtcGxpbmdfY292YXJpYW5jZV9oeXBlcnBhcmFtZXRlcnNfb2ZfbGF0ZW50X0dhdXNzaWFuX21vZGVscw%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vNDU5MjE3MjNfU2xpY2Vfc2FtcGxpbmdfY292YXJpYW5jZV9oeXBlcnBhcmFtZXRlcnNfb2ZfbGF0ZW50X0dhdXNzaWFuX21vZGVscw%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vNDU5MjE3MjNfU2xpY2Vfc2FtcGxpbmdfY292YXJpYW5jZV9oeXBlcnBhcmFtZXRlcnNfb2ZfbGF0ZW50X0dhdXNzaWFuX21vZGVscw%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw33_56aba0688b41c"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 389;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"profileUrl":"researcher\/45470150_Iain_Murray","fullname":"Iain Murray","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2549355721578\/images\/template\/default\/profile\/profile_default_m.png","profileStats":[{"data":{"impactPoints":"0.51","widgetId":"rgw5_56aba0688b41c"},"id":"rgw5_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorImpactPoints.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorImpactPoints.html?authorUid=45470150","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationCount":19,"widgetId":"rgw6_56aba0688b41c"},"id":"rgw6_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorPublicationCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorPublicationCount.html?authorUid=45470150","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},null],"widgetId":"rgw4_56aba0688b41c"},"id":"rgw4_56aba0688b41c","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorBadge.html?authorUid=45470150","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw3_56aba0688b41c"},"id":"rgw3_56aba0688b41c","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=45921723","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":45921723,"title":"Slice sampling covariance hyperparameters of latent Gaussian models","journalTitle":"Advances in neural information processing systems","journalDetailsTooltip":{"data":{"journalTitle":"Advances in neural information processing systems","journalAbbrev":"Adv Neural Inform Process Syst","publisher":"IEEE Conference on Neural Information Processing Systems--Natural and Synthetic, Massachusetts Institute of Technology Press","issn":"1049-5258","impactFactor":"0.00","fiveYearImpactFactor":"0.00","citedHalfLife":"0.00","immediacyIndex":"0.00","eigenFactor":"0.00","articleInfluence":"0.00","widgetId":"rgw8_56aba0688b41c"},"id":"rgw8_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=1049-5258","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"06\/2010;","publicationDateRobot":"2010-06","article":"","journalTitle":"Advances in neural information processing systems","journalUrl":"journal\/1049-5258_Advances_in_neural_information_processing_systems"}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1006.0868","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Slice sampling covariance hyperparameters of latent Gaussian models"},{"key":"rft.title","value":"Advances in, Neural Information Processing Systems 23"},{"key":"rft.jtitle","value":"Advances in, Neural Information Processing Systems 23"},{"key":"rft.date","value":"2010"},{"key":"rft.issn","value":"1049-5258"},{"key":"rft.au","value":"Iain Murray,Ryan Prescott Adams"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw9_56aba0688b41c"},"id":"rgw9_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=45921723","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":45921723,"peopleItems":[{"data":{"authorUrl":"researcher\/45470150_Iain_Murray","authorNameOnPublication":"Iain Murray","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Iain Murray","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/45470150_Iain_Murray","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56aba0688b41c"},"id":"rgw12_56aba0688b41c","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=45470150&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56aba0688b41c"},"id":"rgw11_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=45470150&authorNameOnPublication=Iain%20Murray","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/32349423_Ryan_Prescott_Adams","authorNameOnPublication":"Ryan Prescott Adams","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Ryan Prescott Adams","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/32349423_Ryan_Prescott_Adams","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw14_56aba0688b41c"},"id":"rgw14_56aba0688b41c","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=32349423&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw13_56aba0688b41c"},"id":"rgw13_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=32349423&authorNameOnPublication=Ryan%20Prescott%20Adams","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw10_56aba0688b41c"},"id":"rgw10_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=45921723&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":45921723,"abstract":"<noscript><\/noscript><div>The Gaussian process (GP) is a popular way to specify dependencies between random variables in a probabilistic model. In the Bayesian framework the covariance structure can be specified using unknown hyperparameters. Integrating over these hyperparameters considers different possible explanations for the data when making predictions. This integration is often performed using Markov chain Monte Carlo (MCMC) sampling. However, with non-Gaussian observations standard hyperparameter sampling approaches require careful tuning and may converge slowly. In this paper we present a slice sampling approach that requires little tuning while mixing well in both strong- and weak-data regimes. Comment: 9 pages, 4 figures, 4 algorithms. Minor corrections to previous version. This version to appear in Advances in Neural Information Processing Systems (NIPS) 23, 2010<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw15_56aba0688b41c"},"id":"rgw15_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=45921723","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/45921723_Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models\/links\/0f64908538294e886aa206c9\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw7_56aba0688b41c"},"id":"rgw7_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=45921723&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2059777116,"url":"researcher\/2059777116_Onur_Erdem","fullname":"Onur Erdem","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":31320601,"url":"researcher\/31320601_Murat_Olutas","fullname":"Murat Olutas","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":60001925,"url":"researcher\/60001925_Burak_Guzelturk","fullname":"Burak Guzelturk","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2008924001,"url":"researcher\/2008924001_Yusuf_Kelestemur","fullname":"Yusuf Kelestemur","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Journal of Physical Chemistry Letters","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291340346_Temperature-Dependent_Emission_Kinetics_of_Colloidal_Semiconductor_Nanoplatelets_Strongly_Modified_by_Stacking","usePlainButton":true,"publicationUid":291340346,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"7.46","url":"publication\/291340346_Temperature-Dependent_Emission_Kinetics_of_Colloidal_Semiconductor_Nanoplatelets_Strongly_Modified_by_Stacking","title":"Temperature-Dependent Emission Kinetics of Colloidal Semiconductor Nanoplatelets Strongly Modified by Stacking","displayTitleAsLink":true,"authors":[{"id":2059777116,"url":"researcher\/2059777116_Onur_Erdem","fullname":"Onur Erdem","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":31320601,"url":"researcher\/31320601_Murat_Olutas","fullname":"Murat Olutas","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":60001925,"url":"researcher\/60001925_Burak_Guzelturk","fullname":"Burak Guzelturk","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2008924001,"url":"researcher\/2008924001_Yusuf_Kelestemur","fullname":"Yusuf Kelestemur","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38633698,"url":"researcher\/38633698_Hilmi_Volkan_Demir","fullname":"Hilmi Volkan Demir","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of Physical Chemistry Letters 01\/2016;  DOI:10.1021\/acs.jpclett.5b02763"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291340346_Temperature-Dependent_Emission_Kinetics_of_Colloidal_Semiconductor_Nanoplatelets_Strongly_Modified_by_Stacking","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291340346_Temperature-Dependent_Emission_Kinetics_of_Colloidal_Semiconductor_Nanoplatelets_Strongly_Modified_by_Stacking\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56aba0688b41c"},"id":"rgw17_56aba0688b41c","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291340346","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":12000465,"url":"researcher\/12000465_Gabriele_Gianini","fullname":"Gabriele Gianini","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":12409286,"url":"researcher\/12409286_Alessandro_Rizzi","fullname":"Alessandro Rizzi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":6725194,"url":"researcher\/6725194_Ernesto_Damiani","fullname":"Ernesto Damiani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Information Sciences","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/282897577_A_Retinex_model_based_on_Absorbing_Markov_Chains","usePlainButton":true,"publicationUid":282897577,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"4.04","url":"publication\/282897577_A_Retinex_model_based_on_Absorbing_Markov_Chains","title":"A Retinex model based on Absorbing Markov Chains","displayTitleAsLink":true,"authors":[{"id":12000465,"url":"researcher\/12000465_Gabriele_Gianini","fullname":"Gabriele Gianini","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":12409286,"url":"researcher\/12409286_Alessandro_Rizzi","fullname":"Alessandro Rizzi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":6725194,"url":"researcher\/6725194_Ernesto_Damiani","fullname":"Ernesto Damiani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Information Sciences 01\/2016; 327:149-174. DOI:10.1016\/j.ins.2015.08.015"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/282897577_A_Retinex_model_based_on_Absorbing_Markov_Chains","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/282897577_A_Retinex_model_based_on_Absorbing_Markov_Chains\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56aba0688b41c"},"id":"rgw18_56aba0688b41c","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=282897577","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2084342649,"url":"researcher\/2084342649_S_Martin_Martinez","fullname":"S. Mart\u00edn Martinez","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084370777,"url":"researcher\/2084370777_A_Honrubia_Escribano","fullname":"A. Honrubia Escribano","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084353056,"url":"researcher\/2084353056_M_Canas_Carreton","fullname":"M. Ca\u00f1as Carret\u00f3n","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2084376137,"url":"researcher\/2084376137_V_Guerrero_Mestre","fullname":"V. Guerrero Mestre","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283656157_Wind_Power_Forecast_Error_Probabilistic_Model_Using_Markov_Chains","usePlainButton":true,"publicationUid":283656157,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/283656157_Wind_Power_Forecast_Error_Probabilistic_Model_Using_Markov_Chains","title":"Wind Power Forecast Error Probabilistic Model Using Markov Chains","displayTitleAsLink":true,"authors":[{"id":2084342649,"url":"researcher\/2084342649_S_Martin_Martinez","fullname":"S. Mart\u00edn Martinez","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084370777,"url":"researcher\/2084370777_A_Honrubia_Escribano","fullname":"A. Honrubia Escribano","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084353056,"url":"researcher\/2084353056_M_Canas_Carreton","fullname":"M. Ca\u00f1as Carret\u00f3n","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084376137,"url":"researcher\/2084376137_V_Guerrero_Mestre","fullname":"V. Guerrero Mestre","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084375230,"url":"researcher\/2084375230_E_Gomez_Lazaro","fullname":"E. G\u00f3mez L\u00e1zaro","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283656157_Wind_Power_Forecast_Error_Probabilistic_Model_Using_Markov_Chains","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283656157_Wind_Power_Forecast_Error_Probabilistic_Model_Using_Markov_Chains\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56aba0688b41c"},"id":"rgw19_56aba0688b41c","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=283656157","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw16_56aba0688b41c"},"id":"rgw16_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=45921723&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":45921723,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":45921723,"publicationType":"article","linkId":"0f64908538294e886aa206c9","fileName":"Slice sampling covariance hyperparameters of latent Gaussian models","fileUrl":"http:\/\/arxiv.org\/pdf\/1006.0868.pdf","name":"ArXiv","nameUrl":"http:\/\/arxiv.org\/pdf\/1006.0868.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw22_56aba0688b41c"},"id":"rgw22_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=45921723&linkId=0f64908538294e886aa206c9&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw21_56aba0688b41c"},"id":"rgw21_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=45921723&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":0,"valueFormatted":"0","widgetId":"rgw23_56aba0688b41c"},"id":"rgw23_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=45921723","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw20_56aba0688b41c"},"id":"rgw20_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=45921723&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":45921723,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw25_56aba0688b41c"},"id":"rgw25_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=45921723&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":0,"valueFormatted":"0","widgetId":"rgw26_56aba0688b41c"},"id":"rgw26_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=45921723","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw24_56aba0688b41c"},"id":"rgw24_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=45921723&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Slice sampling covariance hyperparameters\nof latent Gaussian models\nIain Murray\nSchool of Informatics\nUniversity of Edinburgh\nRyan Prescott Adams\nDept. Computer Science\nUniversity of Toronto\nAbstract\nThe Gaussian process (GP) is a popular way to specify dependencies be-\ntween random variables in a probabilistic model. In the Bayesian framework\nthe covariance structure can be specified using unknown hyperparameters.\nIntegrating over these hyperparameters considers different possible expla-\nnations for the data when making predictions. This integration is often\nperformed using Markov chain Monte Carlo (MCMC) sampling.\never, with non-Gaussian observations standard hyperparameter sampling\napproaches require careful tuning and may converge slowly. In this paper\npresent a slice sampling approach that requires little tuning while mixing\nwell in both strong- and weak-data regimes.\nHow-\n1Introduction\nMany probabilistic models incorporate multivariate Gaussian distributions to explain de-\npendencies between variables. Gaussian process (GP) models and generalized linear mixed\nmodels are common examples. For non-Gaussian observation models, inferring the parame-\nters that specify the covariance structure can be difficult. Existing computational methods\ncan be split into two complementary classes: deterministic approximations and Monte Carlo\nsimulation. This work presents a method to make the sampling approach easier to apply.\nIn recent work Murray et al. [1] developed a slice sampling [2] variant, elliptical slice sam-\npling, for updating strongly coupled a-priori Gaussian variates given non-Gaussian obser-\nvations. Previously, Agarwal and Gelfand [3] demonstrated the utility of slice sampling for\nupdating covariance parameters, conventionally called hyperparameters, with a Gaussian\nobservation model, and questioned the possibility of slice sampling in more general settings.\nIn this work we develop a new slice sampler for updating covariance hyperparameters. Our\nmethod uses a robust representation that should work well on a wide variety of problems,\nhas very few technical requirements, little need for tuning and so should be easy to apply.\n1.1Latent Gaussian models\nWe consider generative models of data that depend on a vector of latent variables f that are\nGaussian distributed with covariance \u03a3\u03b8set by unknown hyperparameters \u03b8. These models\nare common in the machine learning Gaussian process literature [e.g. 4] and throughout the\nstatistical sciences. We use standard notation for a Gaussian distribution with mean m and\ncovariance \u03a3,\nN(f;m,\u03a3) \u2261 |2\u03c0\u03a3|\u22121\/2exp?\nand use f \u223c N(m,\u03a3) to indicate that f is drawn from a distribution with the density in (1).\n\u22121\n2(f\u2212m)?\u03a3\u22121(f\u2212m)?, (1)\n1\narXiv:1006.0868v1  [stat.CO]  4 Jun 2010"},{"page":2,"text":"0 0.20.4\nInput Space, x\n0.60.81\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\nLatent values, f\n \n \nl = 0.1\nl = 0.5\nl = 2\n(a) Prior draws\n10\n\u22122\n10\n\u22121\n10\n0\n10\n1\n0\n0.02\n0.04\n0.06\n0.08\n0.1\nlengthscale, l\np(log l | f)\n \n \nl = 0.1\nl = 0.5\nl = 2\n(b) Lengthscale given f\nFigure 1: (a) Shows draws from the prior over f using three different lengthscales in the squared\nexponential covariance (2). (b) Shows the posteriors over log-lengthscale for these three draws.\nThe generic form of the generative models we consider is summarized by\ncovariance hyperparameters \u03b8 \u223c ph,\nlatent variables f \u223c N(0,\u03a3\u03b8),\nand a conditional likelihood P(data|f) = L(f).\nThe methods discussed in this paper apply to covariances \u03a3\u03b8 that are arbitrary positive\ndefinite functions of the hyperparameters \u03b8. However, our experiments focus on the popular\ncase where the covariance is associated with N input vectors {xn}N\nexponential kernel,\nn=1through the squared-\n(\u03a3\u03b8)ij=k(xi,xj)=\u03c32\nfexp\n?\n\u22121\n2\n?D\nd=1\n(xd,i\u2212xd,j)2\n?2\nd\n?\n,(2)\nwith hyperparameters \u03b8={\u03c32\nscale of the latent variables f. The ?d give characteristic lengthscales for converting the\ndistances between inputs into covariances between the corresponding latent values f.\nf,{?d}}. Here \u03c32\nfis the \u2018signal variance\u2019 controlling the overall\nFor non-Gaussian likelihoods we wish to sample from the joint posterior over unknowns,\nP(f,\u03b8|data) =1\nZL(f)N(f;0,\u03a3\u03b8)ph(\u03b8).(3)\nWe would like to avoid implementing new code or tuning algorithms for different covariances\n\u03a3\u03b8and conditional likelihood functions L(f).\n2Markov chain inference\nA Markov chain transition operator T(z?\u2190z) defines a conditional distribution on a new\nposition z?given an initial position z. The operator is said to leave a target distribution \u03c0\ninvariant if \u03c0(z?) =?T(z?\u2190 z)\u03c0(z) dz. A standard way to sample from the joint poste-\nand P(\u03b8|f), invariant. Under fairly mild conditions the Markov chain will equilibrate to-\nwards the target distribution [e.g. 5].\nrior (3) is to alternately apply transition operators that leave its conditionals, P(f |data,\u03b8)\nRecent work has focused on transition operators for updating the latent variables f given\ndata and a fixed covariance \u03a3\u03b8 [6, 1]. Updates to the hyperparameters for fixed latent\nvariables f need to leave the conditional posterior,\nP(\u03b8|f) \u221d N(f;0,\u03a3\u03b8)ph(\u03b8),(4)\ninvariant. The simplest algorithm for this is the Metropolis\u2013Hastings operator, see Algo-\nrithm 1. Other possibilities include slice sampling [2] and Hamiltonian Monte Carlo [7].\n2"},{"page":3,"text":"Algorithm 1 M\u2013H transition for fixed f\nInput: Current f and hyperparameters \u03b8;\nproposal dist. q; covariance function \u03a3().\nOutput:Next hyperparameters\n1: Propose: \u03b8?\u223c q(\u03b8?; \u03b8)\n2: Draw u \u223c Uniform(0,1)\n3: if u <\nN(f;0,\u03a3\u03b8)ph(\u03b8)q(\u03b8?;\u03b8)\n4:\nreturn \u03b8?\n5: else\n6:\nreturn \u03b8\nN(f;0,\u03a3\u03b8?)ph(\u03b8?)q(\u03b8 ;\u03b8?)\n? Accept new state\n? Keep current state\nAlgorithm 2 M\u2013H transition for fixed \u03bd\nInput:Current state \u03b8, f; proposal dist. q;\ncovariance function \u03a3(); likelihood L().\nOutput:Next \u03b8, f\n1: Solve for N(0,I) variate: \u03bd=L\u22121\n2: Propose \u03b8?\u223c q(\u03b8?; \u03b8)\n3: Compute implied values: f?=L\u03a3\u03b8?\u03bd\n4: Draw u \u223c Uniform(0,1)\n5: if u <L(f?)ph(\u03b8?)q(\u03b8 ;\u03b8?)\nL(f)ph(\u03b8)q(\u03b8?;\u03b8)\n6:\nreturn \u03b8?, f?\n7: else\n8:\nreturn \u03b8, f\n\u03a3\u03b8f\n? Accept new state\n? Keep current state\nAlternately fixing the unknowns f and \u03b8 is appealing from an implementation standpoint.\nHowever, the resulting Markov chain can be very slow in exploring the joint posterior distri-\nbution. Figure 1a shows latent vector samples using squared-exponential covariances with\ndifferent lengthscales. These samples are highly informative about the lengthscale hyperpa-\nrameter that was used, especially for short lengthscales. The sharpness of P(\u03b8|f), Figure 1b,\ndramatically limits the amount that any Markov chain can update the hyperparameters \u03b8\nfor fixed latent values f.\n2.1Whitening the prior\nOften the conditional likelihood is quite weak; this is why strong prior smoothing as-\nsumptions are often introduced in latent Gaussian models.\nwhich there is no data, i.e. L is constant, the target distribution is the prior model,\nP(f,\u03b8) = N(f;0,\u03a3\u03b8)ph(\u03b8). Sampling from the prior should be easy, but alternately fix-\ning f and \u03b8 does not work well because they are strongly coupled. One strategy is to\nreparameterize the model so that the unknown variables are independent under the prior.\nIn the extreme limit in\nIndependent random variables can be identified from a common generative procedure for\nthe multivariate Gaussian distribution. A vector of independent normals, \u03bd, is drawn inde-\npendently of the hyperparameters and then deterministically transformed:\n\u03bd \u223c N(0,I),f = L\u03a3\u03b8\u03bd,where L\u03a3\u03b8L?\n\u03a3\u03b8=\u03a3\u03b8. (5)\nNotation: Throughout this paper LC will be any user-chosen square root of covariance\nmatrix C. While any matrix square root can be used, the lower-diagonal Cholesky decom-\nposition is often the most convenient. We would reserve C1\/2for the principal square root,\nbecause other square roots do not behave like powers: for example, chol(C)\u22121?= chol(C\u22121).\nWe can choose to update the hyperparameters \u03b8 for fixed \u03bd instead of fixed f. As the\noriginal latent variables f are deterministically linked to the hyperparameters \u03b8 in (5), these\nupdates will actually change both \u03b8 and f. The samples in Figure 1a resulted from using\nthe same whitened variable \u03bd with different hyperparameters. They follow the same general\ntrend, but vary over the lengthscales used to construct them.\nThe posterior over hyperparameters for fixed \u03bd is apparent by applying Bayes rule to the\ngenerative procedure in (5), or one can laboriously obtain it by changing variables in (3):\nP(\u03b8|\u03bd,data) \u221d P(\u03b8,\u03bd,data) = P(\u03b8,f =L\u03a3\u03b8\u03bd,data)|L\u03a3\u03b8| \u221d \u00b7\u00b7\u00b7 \u221d L(f(\u03b8,\u03bd))ph(\u03b8).\nAlgorithm 2 is the Metropolis\u2013Hastings operator for this distribution. The acceptance rule\nnow depends on the latent variables through the conditional likelihood L(f) instead of the\nprior N(f;0,\u03a3\u03b8) and these variables are automatically updated to respect the prior. In the\nno-data limit, new hyperparameters proposed from the prior are always accepted.\n(6)\n3Surrogate data model\nNeither of the two algorithms are ideal for statistical applications, which is illustrated in\nFigure 2. Algorithm 2 is ideal in the \u201cweak data\u201d limit where the latent variables f are\n3"},{"page":4,"text":"00.2 0.4\nInput Space, x\n0.60.81\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\nObservations, y\n \n \ncurrent state f\nwhitened prior proposal\nsurrogate data proposal\nFigure 2: A regression problem with Gaussian observations illustrated by 2\u03c3 gray bars. The\ncurrent state of the sampler has a short lengthscale hyperparameter (?=0.3), a longer lengthscale\n(?=1.5) is being proposed. The current latent variables do not lie on a straight enough line for the\nlong lengthscale to be plausible. Whitening the prior (Section 2.1) updates the latent variables to\na straighter line, but ignores the observations. A proposal using surrogate data (Section 3, with S\u03b8\nset to the observation noise) sets the latent variables to a draw that is plausible for the proposed\nlengthscale while being close to the current state.\ndistributed according to the prior. In the example the likelihoods are too restrictive for\nAlgorithm 2\u2019s proposal to be acceptable.In the \u201cstrong data\u201d limit, where the latent\nvariables f are fixed by the likelihood L, Algorithm 1 would be ideal. However, the likelihood\nterms in the example are not so weak that the prior can be ignored.\nFor regression problems with Gaussian noise the latent variables can be marginalised out an-\nalytically, allowing hyperparameters to be accepted or rejected according to their marginal\nposterior P(\u03b8|data). If latent variables are required they can be sampled directly from\nthe conditional posterior P(f |\u03b8,data). To build a method that applies to non-Gaussian\nlikelihoods, we create an auxiliary variable model that introduces surrogate Gaussian ob-\nservations that will guide joint proposals of the hyperparameters and latent variables.\nWe augment the latent Gaussian model with auxiliary variables, g, a noisy version of the\ntrue latent variables:\nP(g|f,\u03b8) = N(g; f,S\u03b8).\nFor now S\u03b8 is an arbitrary free parameter that could be set by hand to either a fixed\nvalue or a value that depends on the current hyperparameters \u03b8. We will discuss how to\nautomatically set the auxiliary noise covariance S\u03b8in Section 3.2.\n(7)\nThe original model, f \u223c N(0,\u03a3\u03b8) and (7) define a joint auxiliary distribution P(f,g|\u03b8)\ngiven the hyperparameters. It is possible to sample from this distribution in the opposite\norder, by first drawing the auxiliary values from their marginal distribution\nP(g|\u03b8) = N(g; 0,\u03a3\u03b8+S\u03b8),\nand then sampling the model\u2019s latent values conditioned on the auxiliary values from\nP(f |g,\u03b8) = N(f; m\u03b8,g,R\u03b8),where some standard manipulations give:\nR\u03b8= (\u03a3\u22121\nm\u03b8,g= \u03a3\u03b8(\u03a3\u03b8+S\u03b8)\u22121g = R\u03b8S\u22121\nThat is, under the auxiliary model the latent variables of interest are drawn from their\nposterior given the surrogate data g. Again we can describe the sampling process via a\ndraw from a spherical Gaussian:\n\u03b7 \u223c N(0,I),\nWe then condition on the \u201cwhitened\u201d variables \u03b7 and the surrogate data g while updating\nthe hyperparameters \u03b8. The implied latent variables (10) will remain a plausible draw from\nthe surrogate posterior for the current hyperparameters. This is illustrated in Figure 2.\n(8)\n\u03b8+S\u22121\n\u03b8)\u22121= \u03a3\u03b8\u2212\u03a3\u03b8(\u03a3\u03b8+S\u03b8)\u22121\u03a3\u03b8= S\u03b8\u2212S\u03b8(S\u03b8+\u03a3\u03b8)\u22121S\u03b8,\n\u03b8g.(9)\nf = LR\u03b8\u03b7 + m\u03b8,g,where LR\u03b8L?\nR\u03b8=R\u03b8.(10)\nWe can leave the joint distribution (3) invariant by updating the following conditional\ndistribution derived from the above generative model:\nP(\u03b8|\u03b7,g,data) \u221d P(\u03b8,\u03b7,g,data) \u221d L(f(\u03b7,\u03b8))N(g;0,\u03a3\u03b8+S\u03b8)ph(\u03b8). (11)\n4"},{"page":5,"text":"Algorithm 3 Surrogate data M\u2013H\nInput:\nOutput:\n1: Draw surrogate data: g \u223c N(f,S\u03b8)\n2: Compute implied latent variates:\n\u03b7=L\u22121\nR\u03b8(f \u2212 m\u03b8,g)\n3: Propose \u03b8?\u223c q(\u03b8?; \u03b8)\n4: Compute function f?=LR\u03b8?\u03b7 + m\u03b8?,g\n5: Draw u \u223c Uniform(0,1)\n6: if u <\nL(f)N(g;0,\u03a3\u03b8+S\u03b8)ph(\u03b8)q(\u03b8?;\u03b8)\n7:\nreturn \u03b8?, f?\n8: else\n9:\nreturn \u03b8, f\n\u03b8, f; prop. dist. q; model of Sec. 3.\nNext \u03b8, f\nL(f?)N(g;0,\u03a3\u03b8?+S\u03b8?)ph(\u03b8?)q(\u03b8 ;\u03b8?)\n? Accept new state\n? Keep current state\nAlgorithm 4 Surrogate data slice sampling\nInput:\nOutput:\n1: Draw surrogate data: g \u223c N(f,S\u03b8)\n2: Compute implied latent variates:\n\u03b7=L\u22121\nR\u03b8(f \u2212 m\u03b8,g)\n3: Randomly center a bracket:\nv \u223c Uniform(0,\u03c3), \u03b8min=\u03b8\u2212v, \u03b8max=\u03b8min+\u03c3\n4: Draw u \u223c Uniform(0,1)\n5: Determine threshold:\ny = uL(f)N(g;0,\u03a3\u03b8+S\u03b8)ph(\u03b8)\n6: Draw proposal: \u03b8?\u223c Uniform(\u03b8min,\u03b8max)\n7: Compute function f?=LR\u03b8?\u03b7 + m\u03b8?,g\n8: if L(f?)N(g;0,\u03a3\u03b8?+S\u03b8?)ph(\u03b8?) > y\n9:\nreturn f?, \u03b8?\n10: else if \u03b8?< \u03b8\n11:\nShrink bracket minimum: \u03b8min= \u03b8?\n12: else\n13:\nShrink bracket maximum: \u03b8max= \u03b8?\n14: goto 6\n\u03b8, f; scale \u03c3; model of Sec. 3.\nNext f, \u03b8\nThe Metropolis\u2013Hastings Algorithm 3 contains a ratio of these terms in the acceptance rule.\n3.1 Slice sampling\nThe Metropolis\u2013Hastings algorithms discussed so far have a proposal distribution q(\u03b8?;\u03b8)\nthat must be set and tuned. The efficiency of the algorithms depend crucially on careful\nchoice of the scale \u03c3 of the proposal distribution. Slice sampling [2] is a family of adaptive\nsearch procedures that are much more robust to the choice of scale parameter.\nAlgorithm 3 applies one possible slice sampling algorithm to a scalar hyperparameter \u03b8 in\nthe surrogate data model of this section. It has a free parameter \u03c3, the scale of the initial\nproposal distribution. However, careful tuning of this parameter is not required. If the initial\nscale is set to a large value, such as the width of the prior, then the width of the proposals\nwill shrink to an acceptable range exponentially quickly. Stepping-out procedures [2] can\nalso adapt initial scales that are too small.\n3.2 The auxiliary noise covariance S\u03b8\nThe surrogate data g and noise covariance S\u03b8 define a pseudo-posterior distribution that\nsoftly specifies a plausible region within which the latent variables f are updated. The noise\ncovariance determines the size of this region. The first two baseline algorithms of Section 2\nresult from limiting cases of S\u03b8=\u03b1I: 1) if \u03b1=0 the surrogate data and the current latent\nvariables are equal then the acceptance ratio reduces to that of Algorithm 1. 2) as \u03b1\u2192\u221e\nthe observations are uninformative about the current state and the pseudo-posterior tends\nto the prior. In the limit, the acceptance ratio reduces to that of Algorithm 2. One could\nchoose \u03b1 based on preliminary runs, but such tuning would be burdensome.\nFor likelihood terms that factorize, L(f)=?\nP(fi|Li,\u03b8) \u221d Li(fi) N(fi;0,(\u03a3\u03b8)ii).\nA Gaussian can be fitted by moment matching or a Laplace approximation (matching sec-\nond derivatives at the mode). Such fits, or close approximations, are often possible analyti-\ncally and can always be performed numerically as the distribution is only one-dimensional.\nGiven a Gaussian fit to the site-posterior (12) with variance vi, we can set the auxil-\niary noise to a level that would result in the same posterior variance at that site alone:\n(S\u03b8)ii=(v\u22121\ni\n\u2212(\u03a3\u03b8)ii\ning procedure is a grossly simplified first step of \u201cassumed density filtering\u201d or \u201cexpectation\npropagation\u201d [8], which are too expensive for our use in the inner-loop of a Markov chain.\niLi(fi), we can measure how much the likeli-\nhood restricts each variable individually:\n(12)\n\u22121)\u22121. (Any negative (\u03a3\u03b8)iimust be thresholded.) The moment match-\n5"},{"page":6,"text":"4Related work\nWe have discussed samplers that jointly update strongly-coupled latent variables and hy-\nperparameters. The hyperparameters can move further in joint moves than their narrow\nconditional posteriors (e.g., Figure 1b) would allow. A generic way of jointly sampling real-\nvalued variables is Hamiltonian\/Hybrid Monte Carlo (HMC) [7]. However, this method is\ncumbersome to implement and tune; in practice it is difficult to gain a significant advantage\nby applying HMC to hierarchical models [9].\nChristensen et al. [10] have also proposed a robust representation for sampling in latent\nGaussian models. They use an approximation to the target posterior distribution to con-\nstruct a reparameterization where the unknown variables are close to independent. The\napproximation replaces the likelihood with a Gaussian form proportional to N(f;\u02c6f,\u039b(\u02c6f)):\n\u02c6f = argmaxfL(f),\nwhere \u039b is often diagonal, or it was suggested one would only take the diagonal part.\nThis Taylor approximation looks like a Laplace approximation, except that the likelihood\nfunction is not a probability density in f. This likelihood fit results in an approximate\nGaussian posterior N(f;m\u03b8,g=\u02c6f,R\u03b8) as found in (9), with noise S\u03b8=\u039b(\u02c6f)\u22121and data g=\u02c6f.\nThinking of the current latent variables as a draw from this approximate posterior,\n\u03c9\u223cN(0,I), f = LR\u03b8\u03c9+m\u03b8,\u02c6f, suggests using the reparameterization \u03c9 = L\u22121\nWe can then fix the new variables and update the hyperparameters under\n\u039bij(\u02c6f) =\n\u22022log L(f)\n\u2202fi\u2202fj\n???\u02c6f,(13)\nR\u03b8(f \u2212 m\u03b8,\u02c6f).\nP(\u03b8|\u03c9,data) \u221d L(f(\u03c9,\u03b8))N(f(\u03c9,\u03b8);0,\u03a3\u03b8)ph(\u03b8)|LR\u03b8|.\nWhen the likelihood is Gaussian, the reparameterized variables \u03c9 are independent of each\nother and the hyperparameters. The hope is that approximating non-Gaussian likelihoods\nwill result in nearly-independent parameterizations on which Markov chains will mix rapidly.\n(14)\nTaylor expanding some common log-likelihoods around the maximum is not well defined,\nfor example approximating probit or logistic likelihoods for binary classification, or Pois-\nson observations with zero counts. These Taylor expansions could be seen as giving flat or\nundefined Gaussian approximations that do not reweight the prior. When all of the like-\nlihood terms are flat the reparameterization approach reduces to that of Section 2.1. The\nalternative S\u03b8auxiliary covariances that we have proposed could be used instead.\nThe surrogate data samplers can also be viewed as using reparameterizations, by ignoring\nthe structure of the surrogate data model and treating \u03b7 =L\u22121\nrandom reparameterization for making proposals. A proposal density q(\u03b7?,\u03b8?;\u03b7,\u03b8) in the\nreparameterized space must be multiplied by the Jacobian |L\u22121\nin the original parameterization. The probability of proposing the reparameterization must\nalso be included in the Metropolis\u2013Hastings acceptance probability:\n?\nA few lines of linear algebra confirms that, as it must do, the same acceptance ratio results\nas before. However, just substituting (3) into (15) shows that the acceptance probability is\nvery similar to that from the direct reparameterization approach. The differences are that\nthe new latent variables f?are computed using a different pseudo-posterior mean and there\nis an extra term for the random, rather than fixed, choice of reparameterization.\nR\u03b8(f \u2212 m\u03b8,g) as an arbitrary\nR\u03b8?| to give a proposal density\nmin1,\nP(\u03b8?,f?|data)\u00b7P(g|f?,S\u03b8?)\u00b7q(\u03b8;\u03b8?)|L\u22121\nP(\u03b8,f |data)\u00b7P(g|f,S\u03b8)\u00b7q(\u03b8?;\u03b8)|L\u22121\nR\u03b8|\nR\u03b8?|\n?\n.(15)\nThe surrogate data sampler is easier to implement than the previous reparameterization\nwork because the surrogate posterior is centred around the current latent variables. This\nmeans that 1) no point estimate, such as the maximum likelihood\u02c6f, is required. 2) picking\nthe noise covariance S\u03b8poorly may still produce a workable method, whereas a fixed repa-\nrameterized can work badly if the true posterior distribution is in the tails of the Gaussian\napproximation. Christensen et al. [10] pointed out that centering the approximate Gaus-\nsian likelihood in their reparameterization around the current state is tempting, but that\ncomputing the Jacobian of the transformation is then intractable. By construction, the\nsurrogate data model centers the reparameterization near to the current state.\n6"},{"page":7,"text":"5Experiments\nWe empirically compare the performance of the various approaches to GP hyperparameter\nsampling on four data sets: one regression, one classification, and two Cox process inference\nproblems. Further details are in the rest of this section, with full code as supplementary\nmaterial. The results are summarized in Figure 3 followed by a discussion section.\nIn each of the experimental configurations, we ran ten independent chains with different\nrandom seeds, burning in for 1000 iterations and sampling for 5000 iterations. We quantify\nthe mixing of the chain by estimating the effective number of samples of the complete\ndata likelihood trace using R-CODA [11], and compare that with three cost metrics: the\nnumber of hyperparameter settings considered (each requiring a small number of covariance\ndecompositions with O(n3) time complexity), the number of likelihood evaluations, and the\ntotal elapsed time on a single core of an Intel Xeon 3GHz CPU.\nThe experiments are designed to test the mixing of hyperparameters \u03b8 while sampling from\nthe joint posterior (3). All of the discussed approaches except Algorithm 1 update the latent\nvariables f as a side-effect. However, further transition operators for the latent variables for\nfixed hyperparameters are required. In Algorithm 2 the \u201cwhitened\u201d variables \u03bd remain fixed;\nthe latent variables and hyperparameters are constrained to satisfy f =L\u03a3\u03b8\u03bd. The surrogate\ndata samplers are ergodic: the full joint posterior distribution will eventually be explored.\nHowever, each update changes the hyperparameters and requires expensive computations\ninvolving covariances. After computing the covariances for one set of hyperparameters, it\nmakes sense to apply several cheap updates to the latent variables. For every method we\napplied ten updates of elliptical slice sampling [1] to the latent variables f between each\nhyperparameter update. One could also consider applying elliptical slice sampling to a\nreparameterized representation, for simplicity of comparison we do not.\nMethods\neters. Each method used the same slice sampler, as in Algorithm 4, applied to the following\nmodel representations. fixed: fixing the latent function f [12]. prior-white: whitening\nwith the prior. surr-site: using surrogate data with the noise level set to match the site\nposterior (12). We used Laplace approximations for the Poisson likelihood. For classifi-\ncation problems we used moment matching, because Laplace approximations do not work\nwell [13]. surr-taylor: using surrogate data with noise variance set via Taylor expansion\nof the log-likelihood. Infinite variances were truncated to a large value. post-taylor and\npost-site: as for the surr- methods but a fixed reparameterization based on a posterior\napproximation. The approximations, Laplace and moment matching, refer to Gaussian fits\nto the \u2018site posteriors\u2019 (12).\nWe implemented six 5 methods for updating Gaussian covariance hyperparam-\nBinary Classification (Ionosphere)\nbinary GP classification: fixed, prior-white, surr-kl and post-white-kl. We applied\nthese methods to the Ionosphere dataset [14], using 200 training data and 34 dimensions.\nWe used a logistic likelihood with zero-mean prior, inferring lengthscales as well as sig-\nnal variance. The -taylor methods reduce to other methods or don\u2019t apply because the\nmaximum of the log-likelihood is at plus or minus infinity.\nWe evaluated five different methods for performing\nGaussian Regression (Synthetic)\nrameterization of Christensen et al. [10] makes the hyperparameters and latent variables\nexactly independent. The random centering of the surrogate data model will yield a less\neffective sampler. We used a Gaussian regression problem to assess how much worse the sur-\nrogate data method is compared to an ideal reparameterization. The synthetic data set had\n200 input points in 10-D drawn uniformly within a unit hypercube. The GP had zero mean,\nunit signal variance and its ten lengthscales in (2) drawn from Uniform(0,\u221a10). Observation\nnoise had variance 0.09. We applied the fixed, prior-white, surr-site\/surr-taylor,\nand post-site\/post-taylor methods. The -site and -taylor methods coincide with\nauxiliary noise matching the observation noise (S\u03b8= 0.09I).\nWhen the observations have Gaussian noise the repa-\n7"},{"page":8,"text":"ionosphere synthetic\nx1.6e\u221204   x3.3e\u221204   x4.3e\u221205   x4.8e\u221204   \nminingredwoods\n0\n1\n2\n3\n4\nEffective samples per likelihood evaluation\nionosphere synthetic\nx2.9e\u221204   x1.1e\u221203   x7.4e\u221204   x3.7e\u221203   \nmining redwoods\n0\n1\n2\n3\n4\nEffective samples per covariance construction\n \n \nfixedprior\u2212whitesurr\u2212sitepost\u2212sitesurr\u2212taylorpost\u2212taylor\nionosphere synthetic\nx7.7e\u221203   x5.4e\u221202   x1.2e\u221201   x1.5e\u221202   \nmining redwoods\n0\n1\n2\n3\n4\nEffective samples per second\nFigure 3: The results of experimental comparisons of six MCMC methods for GP hyperparameter\ninference on four data sets. Each figure shows four groups of bars (one for each experiment) and the\nvertical axis shows the effective number of samples of the complete data likelihood per unit cost.\nThe costs are per likelihood evaluation (left), per covariance construction (center), and per second\n(right). Means and standard errors for 10 runs are shown. Each group of bars has been rescaled for\nreadability: the number beneath each group gives the effective samples for the surr-site method,\nwhich always has bars of height 1. Bars are missing where methods are inapplicable (see text).\nCox process inference\nwith a Gaussian process prior for the log-rate. We sampled the hyperparameters in (2) and\na mean offset to the log-rate. The model was applied to two point process datasets: 1) a\nrecord of mining disasters [15] with 191 events in 112 bins of 365 days. 2) 195 redwood tree\nlocations in a region scaled to the unit square [16] split into 25\u00d725=625 bins. The results\nfor the mining problem were initially highly variable. As the mining experiments were also\nthe quickest we re-ran each chain for 20,000 iterations.\nWe tested all methods on an inhomogeneous Poisson process\n6Discussion\nOn the Ionosphere classification problem both of the -site methods worked much better\nthan the two baselines. We slightly prefer surr-site as it involves less problem-specific\nderivations than post-site.\nOn the synthetic test the post- and surr- methods perform very similarly. We had expected\nthe existing post- method to have an advantage of perhaps up to 2\u20133\u00d7, but that was not\nrealized on this particular dataset. The post- methods had a slight time advantage, but\nthis is down to implementation details and is not notable.\nOn the mining problem the Poisson likelihoods are often close to Gaussian, so the existing\npost-taylor approximation works well, as do all of our new proposed methods. With\nthe Poisson likelihood Gaussian approximations fit most poorly to sites with zero counts.\nThe redwood dataset discretizes two-dimensional space, leading to a large number of bins.\nThe majority of these bins have zero counts, many more than the mining dataset. Taylor\nexpanding the likelihood gives no likelihood contribution for bins with zero counts, so it\nis unsurprising that post-taylor performs similarly to prior-white. While surr-taylor\nworks better, the best results come from using approximations to the site-posterior (12).\nThe surrogate data method and reparameterization using a site-posterior and our surrogate\ndata methods offer state-of-the-art performance and are the first such advanced methods to\nbe applicable to Gaussian process classification.\nIn any case, an important message from our results is that fixing the latent variables and\nupdating hyperparameters according to the conditional posterior\u2014as used by the vast\nmajority of GP practitioners\u2014can work exceedingly poorly. Even the simple reparameter-\nization of \u201cwhitening the prior\u201d discussed in Section 2.1 works much better on problems\nwhere smoothness is important in the posterior. Even if site approximations are difficult\nand the more advanced methods presented are inapplicable, the simple whitening repa-\nrameterization should be given serious consideration when performing MCMC inference of\nhyperparameters.\n8"},{"page":9,"text":"Acknowledgements\nThis work was supported in part by the IST Programme of the European Community, under\nthe PASCAL2 Network of Excellence, IST-2007-216886. This publication only reflects the\nauthors\u2019 views. RPA is a junior fellow of the Canadian Institute for Advanced Research.\nReferences\n[1] Iain Murray, Ryan Prescott Adams, and David J.C. MacKay. Elliptical slice sampling.\nIn Proceedings of the 13th International Conference on Artificial Intelligence and Statis-\ntics (AISTATS)., 2010. Volume 9, JMLR: W&CP.\nhttp:\/\/arxiv.org\/abs\/1001.0175.\n[2] Radford M. Neal. Slice sampling. Annals of Statistics, 31(3):705\u2013767, 2003.\n[3] Deepak K. Agarwal and Alan E. Gelfand. Slice sampling for simulation based fitting\nof spatial data models. Statistics and Computing, 15(1):61\u201369, 2005.\n[4] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for ma-\nchine learning. MIT Press, 2006.\n[5] Luke Tierney. Markov chains for exploring posterior distributions. The Annals of\nStatistics, 22(4):1701\u20131728, 1994.\n[6] Michalis Titsias, Neil D Lawrence, and Magnus Rattray. Efficient sampling for Gaussian\nprocess inference using control variables. In Advances in Neural Information Processing\nSystems 21, pages 1681\u20131688. MIT Press, 2009.\n[7] Simon Duane, A. D. Kennedy, Brian J. Pendleton, and Duncan Roweth. Hybrid Monte\nCarlo. Physics Letters B, 195(2):216\u2013222, September 1987.\n[8] Thomas Minka. Expectation propagation for approximate Bayesian inference, 2001.\nCorrected version available from\nhttp:\/\/research.microsoft.com\/~minka\/papers\/ep\/.\n[9] Kiam Choo. Learning hyperparameters for neural network models using Hamiltonian\ndynamics. Master\u2019s thesis, Department of Computer Science, University of Toronto,\n2000. Available from http:\/\/www.cs.toronto.edu\/~radford\/ftp\/kiam-thesis.ps.\n[10] Ole F. Christensen, Gareth O. Roberts, and Martin Sk\u02dc ald. Robust Markov chain Monte\nCarlo methods for spatial generalized linear mixed models. Journal of Computational\nand Graphical Statistics, 15(1):1\u201317, 2006.\n[11] Mary Kathryn Cowles, Nicky Best, Karen Vines, and Martyn Plummer. R-CODA\n0.10-5, 2006. http:\/\/www-fis.iarc.fr\/coda\/.\n[12] Radford M. Neal. Regression and classification using Gaussian process priors. In J. M.\nBernardo et al., editors, Bayesian Statistics 6, pages 475\u2013501. OU Press, 1999.\n[13] Malte Kuss and Carl Edward Rasmussen. Assessing approximate inference for binary\nGaussian process classification. Journal of Machine Learning Research, 6:1679\u20131704,\n2005.\n[14] V. G. Sigillito, S. P. Wing, L. V. Hutton, and K. B. Baker. Classification of radar\nreturns from the ionosphere using neural networks. Johns Hopkins APL Technical\nDigest, 10:262\u2013266, 1989.\n[15] R. G. Jarrett. A note on the intervals between coal-mining disasters. Biometrika, 66\n(1):191\u2013193, 1979.\n[16] Brian D. Ripley. Modelling spatial patterns. Journal of the Royal Statistical Society,\nSeries B, 39:172\u2013212, 1977.\n9"}],"widgetId":"rgw27_56aba0688b41c"},"id":"rgw27_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=45921723&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw28_56aba0688b41c"},"id":"rgw28_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=45921723&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":45921723,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/45921723_Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56aba0688b41c"},"id":"rgw2_56aba0688b41c","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":45921723},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=45921723&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56aba0688b41c"},"id":"rgw1_56aba0688b41c","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"ivEtjxYvU6TfUyxGivKIj35lHlKSitT0DJ8dAikeCvptHoKP3F\/Y+FXHjrKP9lsPahdl7+rpWeXFQMS7MNr5c3AAnlzPFdBVOpUCUtefvyI+akFwNGFpW8S77BqYpdxVGwKnLJbzp8ddmz8A\/EQnuWisIcaE9F8SXJAYhic5UAYaFwe0H8i\/5XyZJ1kIydEL9toCL1lJXmXqFcTqfeAGlNeW\/krb8SGa4gqgwdedxPC\/w2U\/LwzgPdED8e8KZNTL8buUMukZdqkQjJxPkc0KwVrtM0c53xKX4n0dDFAKjTQ=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/45921723_Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Slice sampling covariance hyperparameters of latent Gaussian models\" \/>\n<meta property=\"og:description\" content=\"The Gaussian process (GP) is a popular way to specify dependencies between random variables in a probabilistic model. In the Bayesian framework the covariance structure can be specified using...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/45921723_Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models\/links\/0f64908538294e886aa206c9\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/45921723_Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models\" \/>\n<meta property=\"rg:id\" content=\"PB:45921723\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Slice sampling covariance hyperparameters of latent Gaussian models\" \/>\n<meta name=\"citation_author\" content=\"Iain Murray\" \/>\n<meta name=\"citation_author\" content=\"Ryan Prescott Adams\" \/>\n<meta name=\"citation_publication_date\" content=\"2010\/06\/04\" \/>\n<meta name=\"citation_journal_title\" content=\"Advances in neural information processing systems\" \/>\n<meta name=\"citation_issn\" content=\"1049-5258\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/45921723_Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/45921723_Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-43a02883-e92f-493d-b50d-36a3b74e02b3","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":374,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw29_56aba0688b41c"},"id":"rgw29_56aba0688b41c","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-43a02883-e92f-493d-b50d-36a3b74e02b3", "5b4b78fbef82d651c346d52c97e17d8388902487");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-43a02883-e92f-493d-b50d-36a3b74e02b3", "5b4b78fbef82d651c346d52c97e17d8388902487");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw30_56aba0688b41c"},"id":"rgw30_56aba0688b41c","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/45921723_Slice_sampling_covariance_hyperparameters_of_latent_Gaussian_models","requestToken":"2GPOyCpE1fu0v8xgG+sN7aSC+hzxfLXSKSNHUy47sxC3EOcSZBq3eJ+kEDqAye6wOhsqbmvlYjkpCXBEQ204RAmD0zqWgCzwEqG7YX1o9UFhkA43x2rnGqYDXlxCQ27OX+lhdysG8FI88ZGLQbzf0QkCI86KzbqhJ5KZVgZKC0uRsjksZ3ulNiZFWlRpy1purzQC8VeSmpRaI4fOM3F3BLC7wufXUgxtbGc3TuCHdTsjgH8uz\/oJTiI9qBUxFI9ah\/2WSTwYxOXC0TgvMszC9ewqo3qqDyESSIPxO1qLoV4=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=aaMwev2tfuG6Aivn1wefgyCyub8e1jOs2V2iQ5q20ifYQYSAAVY5jx6bhEFLO4nk","encodedUrlAfterLogin":"cHVibGljYXRpb24vNDU5MjE3MjNfU2xpY2Vfc2FtcGxpbmdfY292YXJpYW5jZV9oeXBlcnBhcmFtZXRlcnNfb2ZfbGF0ZW50X0dhdXNzaWFuX21vZGVscw%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw32_56aba0688b41c"},"id":"rgw32_56aba0688b41c","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw31_56aba0688b41c"},"id":"rgw31_56aba0688b41c","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw33_56aba0688b41c"},"id":"rgw33_56aba0688b41c","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
