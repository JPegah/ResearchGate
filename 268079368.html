<!DOCTYPE html> <html lang="en" class="" id="rgw38_56ab9fb5b3228"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="m4VihjdqcCeseZSdjdnVTzzk6EJRkkacVF/MN5BWMxSNGsgRe8jrUR1zdLr93ZDzYIg5fZPiu/mVQ0ryIXKqnPouMGEGDsorCo44q4xqoQG3yA8/Obw7SdWGIj/xuxwqOq+wm10zLTmaPESUz3r9NGEzrZWnBA8OechX8KeN2VCgmM2ltKLbg2rjnRk9kiXW28p53u+IrUA1k4AzOTQlBG/+N/7jBvttRfYf5hQR8pZFnPPLyr5KGZQ9Oxbv8ID8FcfJngSiT9VQMXBe8gYw5mghCQUC2cCPsJtnu+UJNKs="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-ecd3039d-514e-4ac4-aa9f-8cd2edeb1ae0",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/268079368_Scalable_Variational_Gaussian_Process_Classification" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Scalable Variational Gaussian Process Classification" />
<meta property="og:description" content="Gaussian process classification is a popular method with a number of
appealing properties. We show how to scale the model within a variational
inducing point framework, outperforming the state of..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/268079368_Scalable_Variational_Gaussian_Process_Classification/links/55adfd9508aed9b7dcdb08a5/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/268079368_Scalable_Variational_Gaussian_Process_Classification" />
<meta property="rg:id" content="PB:268079368" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Scalable Variational Gaussian Process Classification" />
<meta name="citation_author" content="James Hensman" />
<meta name="citation_author" content="Alex Matthews" />
<meta name="citation_author" content="Zoubin Ghahramani" />
<meta name="citation_publication_date" content="2014/11/07" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/James_Hensman/publication/268079368_Scalable_Variational_Gaussian_Process_Classification/links/55adfd9508aed9b7dcdb08a5.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/268079368_Scalable_Variational_Gaussian_Process_Classification" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/268079368_Scalable_Variational_Gaussian_Process_Classification" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Scalable Variational Gaussian Process Classification (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Scalable Variational Gaussian Process Classification on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab9fb5b3228" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab9fb5b3228" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab9fb5b3228">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Scalable%20Variational%20Gaussian%20Process%20Classification&rft.date=2014&rft.au=James%20Hensman%2CAlex%20Matthews%2CZoubin%20Ghahramani&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Scalable Variational Gaussian Process Classification</h1> <meta itemprop="headline" content="Scalable Variational Gaussian Process Classification">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/268079368_Scalable_Variational_Gaussian_Process_Classification/links/55adfd9508aed9b7dcdb08a5/smallpreview.png">  <div id="rgw7_56ab9fb5b3228" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab9fb5b3228" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/James_Hensman" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A277099177889798%401443077000994_m" title="James Hensman" alt="James Hensman" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">James Hensman</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw9_56ab9fb5b3228" data-account-key="James_Hensman">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/James_Hensman"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A277099177889798%401443077000994_l" title="James Hensman" alt="James Hensman" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/James_Hensman" class="display-name">James Hensman</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Lancaster_University" title="Lancaster University">Lancaster University</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56ab9fb5b3228"> <a href="researcher/2058019585_Alex_Matthews" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Alex Matthews" alt="Alex Matthews" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Alex Matthews</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw11_56ab9fb5b3228">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2058019585_Alex_Matthews"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Alex Matthews" alt="Alex Matthews" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2058019585_Alex_Matthews" class="display-name">Alex Matthews</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56ab9fb5b3228"> <a href="researcher/8159937_Zoubin_Ghahramani" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Zoubin Ghahramani</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw13_56ab9fb5b3228">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/8159937_Zoubin_Ghahramani"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/8159937_Zoubin_Ghahramani" class="display-name">Zoubin Ghahramani</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">        <meta itemprop="datePublished" content="2014-11">  11/2014;               <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1411.2005" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw14_56ab9fb5b3228" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>Gaussian process classification is a popular method with a number of<br />
appealing properties. We show how to scale the model within a variational<br />
inducing point framework, outperforming the state of the art on benchmark<br />
datasets. Importantly, the variational formulation can be exploited to allow<br />
classification in problems with millions of data points, as we demonstrate in<br />
experiments.</div> </p>  </div>  </div>   </div>      <div class="action-container"> <div id="rgw15_56ab9fb5b3228" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw29_56ab9fb5b3228">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw30_56ab9fb5b3228">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/James_Hensman/publication/268079368_Scalable_Variational_Gaussian_Process_Classification/links/55adfd9508aed9b7dcdb08a5.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/James_Hensman">James Hensman</a>, <span class="js-publication-date"> Jul 21, 2015 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw32_56ab9fb5b3228" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw33_56ab9fb5b3228" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw34_56ab9fb5b3228" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw35_56ab9fb5b3228" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw36_56ab9fb5b3228" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw37_56ab9fb5b3228" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw31_56ab9fb5b3228" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FJames_Hensman%2Fpublication%2F268079368_Scalable_Variational_Gaussian_Process_Classification%2Flinks%2F55adfd9508aed9b7dcdb08a5.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw28_56ab9fb5b3228"  itemprop="articleBody">  <p>Page 1</p> <p>Scalable Variational Gaussian Process Classification<br />James Hensman<br />University of Sheffield<br />Alex Matthews<br />University of Cambridge<br />Zoubin Ghahramani<br />University of Cambridge<br />Abstract<br />Gaussian process classification is a popular<br />method with a number of appealing proper-<br />ties. We show how to scale the model within<br />a variational inducing point framework, out-<br />performing the state of the art on benchmark<br />datasets. Importantly, the variational formu-<br />lation can be exploited to allow classification<br />in problems with millions of data points, as<br />we demonstrate in experiments.<br />1 Introduction<br />Gaussian processes (GPs) provide priors over functions<br />that can be used for many machine learning tasks. In<br />the regression setting, when the likelihood is Gaussian,<br />inference can be performed in closed-form using linear<br />algebra. When the likelihood is non-Gaussian, such as<br />in GP classification, the posterior and marginal like-<br />lihood must be approximated. Kuss and Rasmussen<br />[2005] and Nickisch and Rasmussen [2008] provide ex-<br />cellent comparisons of several approximate inference<br />methods for GP classification. More recently, Opper<br />and Archambeau [2009] and Khan et al. [2012] consid-<br />ered algorithmic improvements to variational approx-<br />imations in non-conjugate GP models.<br />The computational cost of inference in GPs is O(N3)<br />in general, where N is the number of data. In the<br />regression setting, there has been much interest in<br />low-rank or sparse approaches to reduce this computa-<br />tional complexity: Qui˜ nonero-Candela and Rasmussen<br />[2005] provides a review. Many of these approximation<br />schemes rely on the use of a series of inducing points,<br />which can be difficult to select. Titsias [2009] sug-<br />gests a variational approach (see section 2) which pro-<br />vides an objective function for optimizing these points.<br />This variational idea was extended by Hensman et al.<br />[2013], who showed how the variational objective could<br />be reformulated with additional parameters to enable<br />stochastic optimization, which allows GPs to be fitted<br />to millions of data.<br />Despite much interest in approximate GP classifica-<br />tion and sparse approximations, there has been lit-<br />tle overlap of the two.The approximate inference<br />schemes which deal with the non-conjugacy of the like-<br />lihood generally scale with O(N3), as they require fac-<br />torization of the covariance matrix. Two approaches<br />which have addressed these issues simultaneously are<br />the IVM [Lawrence et al., 2003] and Generalized FITC<br />[Naish-Guzman and Holden, 2007] which both have<br />shortcomings as we shall discuss.<br />It is tempting to think that sparse GP classification is<br />simply a case of combining a low-rank approximation<br />to the covariance with one’s preferred non-conjugate<br />approximation, but as we shall show this does not nec-<br />essarily lead to an effective method: it can be very<br />difficult to place the inducing input points, and scala-<br />bility of the method is usually restricted by the com-<br />plexity of matrix-matrix multiplications.<br />For these reasons, there is a strong case for a non-<br />conjugate sparse GP scheme which provides a vari-<br />ational bound on the marginal likelihood, combining<br />the scalability of the stochastic optimization approach<br />with the ability to optimize the positions of the induc-<br />ing inputs. Furthermore, a variational approach would<br />allow for integration of the approximation within other<br />GP models such as GP regression networks [Wilson<br />et al., 2012], latent variable models [Lawrence, 2004]<br />and deep GPs [Damianou and Lawrence, 2013], as the<br />variational objective could be optimized as part of such<br />a model without fear of overfitting.<br />The rest of the paper is arranged as follows. In sec-<br />tion 2, we briefly cover some background material<br />and existing work. In section 3, we show how vari-<br />ational approximations to the covariance matrix can<br />be used post-hoc to provide variational bounds with<br />non-Gaussian likelihoods. These approaches are not<br />entirely satisfactory, and so in section 4 we provide a<br />variational approach which does not first approximate<br />the covariance matrix, but delivers a variational bound<br />directly. In section 5 we compare our proposals with<br />the state of the art, as well as demonstrating empiri-<br />cally that our preferred method is applicable to very<br />large datasets through stochastic variational optimiza-<br />tion. Section 6 concludes.<br />arXiv:1411.2005v1  [stat.ML]  7 Nov 2014</p>  <p>Page 2</p> <p>Scalable Variational Gaussian Process Classification<br />2Background<br />Gaussian Process Classification<br />cess priors provide rich nonparametric models of func-<br />tions. To perform classification with this prior, the<br />process is ‘squashed’ through a sigmoidal inverse-link<br />function, and a Bernoulli likelihood conditions the<br />data on the transformed function values.<br />mussen and Williams [2006] for a review.<br />Gaussian pro-<br />See Ras-<br />We denote the binary class observations as y =<br />{yn}N<br />sign matrix X = {xn}N<br />function at all pairs of input vectors to build the co-<br />variance matrix Knnin the usual way, and arrive at<br />a prior for the values of the GP function at the input<br />points: p(f) = N(f |0,Knn).<br />We denote the probit inverse link function as<br />φ(x) =?x<br />distribution of data and latent variables becomes<br />n=1, and then collect the input data into a de-<br />n=1. We evaluate the covariance<br />−∞N(a|0, 1)da and the Bernoulli distribu-<br />tion B(yn|φ(fn)) = φ(fn)yn(1−φ(fn))1−yn. The joint<br />p(y,f) =<br />N<br />?<br />n=1<br />B(yn|φ(fn)) N(f |0,Knn) .(1)<br />The main object of interest is the posterior over func-<br />tion values p(f |y), which must be approximated. We<br />also require an approximation to the marginal likeli-<br />hood p(y) in order to optimize (or marginalize) pa-<br />rameters of the covariance function. An assortment of<br />approximation schemes have been proposed (see Nick-<br />isch and Rasmussen [2008] for a comparison), but they<br />all require O(N3) computation.<br />Sparse Gaussian Processes for Regression<br />computational complexity of any Gaussian process<br />method scales with O(N3) because of the need to in-<br />vert the covariance matrix K.<br />putational complexity, many approximation schemes<br />have been proposed, though most focus on regression<br />tasks, see Qui˜ nonero-Candela and Rasmussen [2005]<br />for a review. Here we focus on inducing point meth-<br />ods [Snelson and Ghahramani, 2005], where the latent<br />variables are augmented with additional input-output<br />pairs Z,u, known as ‘inducing inputs’ and ‘inducing<br />variables’.<br />The<br />To reduce the com-<br />The random variables u are points on the function in<br />exactly the same way as f, and so the joint distribution<br />can be written<br />??<br />where Kmm is formed by evaluating the covariance<br />function at all pairs of inducing inputs points zm,zm?,<br />and Knmis formed by evaluating the covariance func-<br />tion across the data input points and inducing inputs<br />p(f,u) = N<br />f<br />u<br />????0,<br />?<br />Knn<br />K?<br />nm<br />Knm<br />Kmm<br />??<br />(2)<br />points similarly. Using the properties of a multivariate<br />normal distribution, the joint can be re-written as<br />p(f,u) = p(f |u)p(u)<br />= N(f |KnmK−1<br />(3)<br />mmu,Knn− Qnn)N(u|0, Kmm)<br />with Qnn = KnmK−1<br />now takes the form<br />mmK?<br />nm. The joint distribution<br />p(y,f,u) = p(y|f)p(f |u)p(u) .(4)<br />To obtain computationally efficient inference, integra-<br />tion over f is approximated. To obtain the popular<br />FITC method (in the case of Gaussian likelihood), a<br />factorization is enforced: p(y|u) ≈?<br />ity is used<br />np(yn|u). To<br />get a variational approximation, the following inequal-<br />logp(y|u) ≥ Ep(f |u)[logp(y|f)] ? log ˜ p(y|u) . (5)<br />Substituting this bound on the conditional into the<br />standard expression p(y) =<br />tractable bound on the marginal likelihood for the<br />Gaussian case [Titsias, 2009]:<br />?p(y|u)p(u)du gives a<br />logp(y) ≥logN(y|0,KnmK−1<br />−<br />mmK?<br />nm+ σ2I)<br />1<br />2σ2tr(Knn− Qnn) ,<br />(6)<br />where σ2is the variance of the Gaussian likelihood<br />term. This bound on the marginal likelihood can then<br />be used as an objective function in optimizing the<br />covariance function parameters as well as the induc-<br />ing input points Z. The bound becomes tight when<br />the inducing points are the data points: Z = X so<br />Knm= Kmm= Knn and (6) becomes equal the true<br />marginal likelihood logN(y|0, Knn+ σ2I).<br />Computing this bound (6) and its derivatives costs<br />O(NM2).<br />can be achieved by introducing additional variational<br />parameters [Hensman et al., 2013]. Noting that (6) im-<br />plies an approximate posterior ˜ p(u|y), we introduce<br />a variational distribution q(u) = N(u|m,S) to ap-<br />proximate this distribution, and applying a standard<br />variational bound, obtain:<br />A more computationally scalable bound<br />logp(y) ≥logN(y|KnmK−1<br />−<br />1<br />2σ2tr(Knn− Qnn) − KL[q(u)||p(u)] .<br />mmm, σ2I)<br />1<br />2σ2tr(KnmK−1<br />mmSK−1<br />mmKmn)(7)<br />−<br />This bound has a unique optimum in terms of the vari-<br />ational parameters m,S, at which point it is tight to<br />the original sparse GP bound (6). The advantage of<br />the representation in (7) is that it can be optimized in</p>  <p>Page 3</p> <p>James Hensman, Alex Matthews, Zoubin Ghahramani<br />a stochastic [Hensman et al., 2013] or distributed [Dai<br />et al., 2014, Gal et al., 2015] fashion.<br />Of course for the Bernoulli likelihood, the required in-<br />tegrals for (6) and (7) are not tractable, but we will<br />build on them both in subsequent sections to build<br />sparse GP classifiers.<br />Related work<br />(IVM) [Lawrence et al., 2003] is the first work to ap-<br />proach sparse GP classification to our knowledge. The<br />idea is to combine assumed density filtering with a se-<br />lection heuristic to pick points from the data X to act<br />as inducing points Z: the inducing variables u are then<br />a subset of the latent function variables f.<br />The informative vector machine<br />The IVM offers superior performance to support vector<br />machines [Lawrence et al., 2003], along with a proba-<br />bilistic interpretation. However we might expect bet-<br />ter performance by relaxing the condition that the in-<br />ducing points be a sub-set of the data, as is the case for<br />regression [Qui˜ nonero-Candela and Rasmussen, 2005].<br />Subsequent work on sparse GP classification [Naish-<br />Guzman and Holden, 2007] removed the restriction of<br />selecting Z to be a subset of the data X, and ostensibly<br />improved over the assumed density filtering scheme by<br />using expectation propagation (EP) for inference.<br />Naish-Guzman and Holden [2007] noted that when us-<br />ing the FITC approximation for a Gaussian likelihood,<br />the equivalent prior (see also Qui˜ nonero-Candela and<br />Rasmussen [2005]) is<br />p(f) ≈ N(f |0, Qnn+ diag(Knn− Qnn)) .<br />The Generalized FITC method combines this approx-<br />imate prior with a Bernoulli likelihood and uses EP<br />to approximate the posterior. The form of the prior<br />means that the linear algebra within the EP updates is<br />simplified, and a round of updates costs O(NM2). EP<br />is nested inside an optimization loop, where the covari-<br />ance hyper-parameters and inducing inputs are opti-<br />mized against the EP approximation to the marginal<br />likelihood. Computing the marginal likelihood approx-<br />imation and gradients costs O(NM2).<br />The generalized FITC method works well in practise,<br />often finding solutions which are as good as or bet-<br />ter than the IVM, with fewer inducing inputs points<br />[Naish-Guzman and Holden, 2007].<br />(8)<br />Discussion<br />than the IVM, GFITC does not satisfy our require-<br />ments for a scalable GP classifier. There is no clear<br />way to distribute computation or use stochastic op-<br />timization in GFITC: we find that it is limited to a<br />few thousand data. Further, the positions of the in-<br />ducing input points Z can be optimized against the<br />Despite performing significantly better<br />approximation to the marginal likelihood, but there is<br />no guarantee that this will provide a good solution: in-<br />deed, our experiments in section 5 show that this can<br />lead to strange pathologies.<br />To obtain the ability to place inducing inputs as Tit-<br />sias [2009] and to scale as Hensman et al. [2013], we de-<br />sire a bound on the marginal likelihood against which<br />to optimize Z. In the next section, we attempt to build<br />such a variational approximation in the same fashion<br />as FITC, by first using existing variational methods<br />to approximate the covariance, and then using further<br />variational approximate methods to deal with non-<br />conjugacy.<br />3Two stage approaches<br />A straightforward approach to building sparse GP<br />classifiers is to separate the low-rank approximation<br />to the covariance from the non-Gaussian likelihood.<br />In other words, simply treat the approximation to the<br />covariance matrix as the prior, and then select an ap-<br />proximate inference algorithm (e.g. from one of those<br />compared by Nickisch and Rasmussen [2008]), and<br />then proceed with approximate inference, exploiting<br />the form of the approximate covariance where possi-<br />ble for computation saving.<br />This is how the generalized FITC approximation was<br />derived. However, as described above we aim to con-<br />struct variational approximations.<br />It’s possible to construct a variational approach in this<br />mould by using the fact that the probit likelihood can<br />be written as a convolution of a unit Gaussian and a<br />step function. Without modifying our original model,<br />we can introduce a set of additional latent variables g<br />which relate to the original latent variables f through<br />a unit variance isotropic Gaussian:<br />p(y,f,g) =<br />N<br />?<br />is<br />n=1<br />B(yn|θ(gn))N(g|f,I)N(f|0,K) (9)<br />where<br />The<br />marginalization<br />tor:<br />?N<br />We can now proceed by using a variational sparse GP<br />bound on g, followed by a further variational approx-<br />imation to deal with the non-Gaussian likelihood.<br />θastep-function<br />(1)<br />the additional<br />n=1B(yn|θ(gn))N(g|f,I)dg<br />n=1B(yn|φ(fn)).<br />inverse-link.<br />recovered<br />latent<br />originalmodel<br />of<br />isby<br />vec-<br />? ?N<br />=<br />3.1Sparse mean field approach<br />Encouraged by the success of a (non-sparse) factoriz-<br />ing approximation made by [Hensman et al., 2014], we</p>  <p>Page 4</p> <p>Scalable Variational Gaussian Process Classification<br />couple a variational bound on p(g) with a mean-field<br />approximation. Substituting the variational bound for<br />a Gaussian sparse GP (6) with our augmented model<br />(9) (where g in (9) replaces y in (6)), we arrive at a<br />bound on the joint distribution:<br />p(y, g) ≥<br />N<br />?<br />exp{−1<br />n=1<br />B(yn|θ(gn))N(g|0,Qnn+ I)<br />2tr(Knn− Qnn)} .<br />(10)<br />Assuming a factorizing distribution q(g) =?<br />the usual variational way, and the optimal form of the<br />approximating distribution is a truncated Gaussian<br />nq(gn),<br />we obtain a lower bound on the marginal likelihood in<br />q?(gn) = B(yn|θ(gn))N(gn|an, ˜ σ2<br />where ˜ σ−2<br />n<br />is given by the nthdiagonal element of<br />[Qnn+ I]−1, γnare the required normalizers and an<br />are free variational parameters. Some cancellation in<br />the bound leads to a tractable expression:<br />n)/γn , (11)<br />logp(y) ≥<br />N<br />?<br />−1<br />n=1<br />logγn−1<br />2log|Qnn+ I|<br />2tr([Qnn+ I]−1Eq(g)[gg?])<br />N<br />?<br />−1<br />All the components of this bound can be computed<br />in maximum O(NM2) time. The expectations under<br />the factorizing variational distribution (and their gra-<br />dients) are available in closed form.<br />+1<br />2<br />n=1<br />?log ˜ σ2<br />n+Eq(gn)[(an− gn)2]<br />˜ σ2<br />n<br />?<br />(12)<br />2tr(Knn− Qnn).<br />Approximate inference can now proceed by optimizing<br />this bound with respect to the variational parameters<br />a = {an}N<br />covariance function and the inducing points Z. Em-<br />pirically, we find it useful to optimize the variational<br />parameters on an ‘inner loop’, which costs O(NM)<br />per iteration. Computing the relevant gradients out-<br />side this loop costs O(NM2).<br />n=1, alongside the hyper-parameters of the<br />Predictions<br />function value g?at a test input point x?, we would<br />like to marginalize across the variational distribution:<br />?<br />Since this is in general intractable, we approximate it<br />by Monte Carlo. Since the approximate posterior q is<br />a factorized series of truncated normal distributions,<br />it is straight-forward to sample from. Prediction of a<br />single test point costs O(N).<br />To make a prediction for a new latent<br />p(g?|y) ≈<br />p(g?|g)q(g)dg .(13)<br />Alternatively, we can employ a Gaussian approxima-<br />tion to the posterior as suggested by Nickisch and<br />Rasmussen [2008]. In our sparse formulation, this re-<br />sults in p(g|y) ≈ N(g|ΣK−1<br />Σ = Kmm− Kmn[Qnn+ I]−1Knm. Substituting in<br />to (13) results in a computational cost of O(M2) to<br />predict a single test point.<br />mmKmnEq(g)[g],Σ), with<br />3.2A more scalable method?<br />The mean-field method in section 3.1 is somewhat un-<br />satisfactory since the number of variational parameters<br />scales with N. The variational parameters are also de-<br />pendent on each other, and so application of stochastic<br />optimization or distributed computing is difficult. For<br />the Gaussian likelihood case, Hensman et al. [2013]<br />proposes a bound on logp(y) (7) which can be opti-<br />mized effectively with O(M2) parameters. Perhaps it<br />is possible to obtain a scalable algorithm by substitut-<br />ing this bound for p(g) as in the above?<br />Substituting (7) into (9) (again replacing y with g)<br />results in a tractable integral to obtain a bound on the<br />marginal likelihood, as Hensman et al. [2013] points<br />out. The result is<br />?<br />≥<br />n<br />−1<br />−1<br />where k?<br />optimized in a stochastic or distributed way [Tolva-<br />nen, 2014], but has been found to be less effective than<br />might be expected (Owen Thomas, personal commu-<br />nication). To understand why, consider the case where<br />the inducing points are set to the data points Z = X,<br />so that u = f. The bound reduces to<br />?<br />−1<br />and the optimum occurs where S−1= K−1<br />m is the maximum a posteriori (MAP) point.<br />logp(y) = logp(y|g)p(g)dg<br />B(yn|φ(k?<br />?<br />nK−1<br />mmm))<br />2tr(KnmK−1<br />2tr(Knn− Qnn) − KL[q(u)||p(u)] ,<br />mmSK−1<br />mmKmn)<br />nis the nthrow of Knm. This bound can be<br />logp(y) ≥<br />n<br />B(yn|φ(mn))<br />2tr(S) − KL[q(f)||p(f)] .<br />(14)<br />nn+ I, and<br />This approximation is reminiscent of the Laplace ap-<br />proximation, which also places the mean of the pos-<br />terior at the MAP point, and approximates the co-<br />variance with S−1= K−1<br />sian of the log likelihood evaluated at the MAP point.<br />The Laplace approximation is known to be relatively<br />ineffective for classification [Nickisch and Rasmussen,<br />2008], so it is no surprise that this variational approx-<br />imation should be ineffective. From here we abandon<br />nn+ W, where W is the Hes-</p>  <p>Page 5</p> <p>James Hensman, Alex Matthews, Zoubin Ghahramani<br />this bound: the next section sees the construction of a<br />variational bound which is scalable and (empirically)<br />effective.<br />4A single variational bound<br />Here we obtain a bound on the marginal likelihood<br />without introducing the additional latent variables as<br />above, and without making factorizing assumptions.<br />We first return to the bound (5) on the conditional<br />used to construct the variational bounds for the Gaus-<br />sian case:<br />logp(y|u) ≥ Ep(f |u)[logp(y|f)]<br />which is in general intractable for the non-conjugate<br />case. We nevertheless persist, recalling the standard<br />variational equation<br />(15)<br />logp(y) ≥ Eq(u)[logp(y|u)] − KL[q(u)||p(u)] .<br />(16)<br />Substituting (15) into (16) results in a (further) bound<br />on the marginal likelihood:<br />logp(y) ≥ Eq(u)[logp(y|u)] − KL[q(u)||p(u)]<br />≥ Eq(u)<br />= Eq(f)[logp(y|f)] − KL[q(u)||p(u)]<br />where we have defined:<br />?<br />?Ep(f|u)[logp(y|f)]?− KL[q(u)||p(u)]<br />(17)<br />q(f) :=p(f |u)q(u)du .(18)<br />Consider the case where q(u) = N(u|m,S).<br />gives the following functional form for q(f):<br />This<br />q(f) = N(f |Am,Knn+ A(S − Kmm)A?)<br />with A = KnmK−1<br />(19)<br />mm.<br />Since in the classification case the likelihood factors as<br />p(y|f) =<br />N<br />?<br />i=1<br />p(yi|fi) ,(20)<br />we only require the marginals of q(f) in order to com-<br />pute the expectations in (17). We are left with some<br />one-dimensional integrals of the log-likelihood, which<br />can be computed by e.g. Gauss-Hermite quadrature:<br />logp(y) ≥<br />N<br />?<br />n=1<br />Eq(fn)[logp(yn|fn)] − KL[q(u)||p(u)] .<br />(21)<br />Our algorithm then consists of maximizing the pa-<br />rameters of q(u) with respect to this bound on the<br />marginal likelihood using gradient based optimization.<br />To maintain positive-definiteness of S, we represent it<br />using a lower triangular form S = LL?, which allows<br />us to perform unconstrained optimization.<br />Computations and Scalability<br />KL divergence of the bound (21) requires O(M3) com-<br />putations. Since we expect the number of required in-<br />ducing points M to be much smaller than the number<br />of data N, most of the work will be in computing the<br />expected likelihood terms. To compute the derivatives<br />of these, we use the Gaussian identities made familiar<br />to us by Opper and Archambeau [2009]:<br />Computing the<br />∂<br />∂µEN(x|µ,σ2)<br />∂<br />∂σ2EN(x|µ,σ2)<br />?f(x)?= EN(x|µ,σ2)<br />?f(x)?=1<br />? ∂<br />? ∂2<br />∂xf(x)?<br />∂x2f(x)?.<br />2EN(x|µ,σ2)<br />(22)<br />We can make use of these by substituting f for<br />logp(yn|fn) and µ,σ2for the marginals of q(f) in (21).<br />These derivatives also have to be computed by quadra-<br />ture methods, after which derivatives with respect to<br />m,L,Z and any covariance function parameters re-<br />quires the application of straight-forward algebra.<br />We also have the option to optimize the objective in<br />a distributed fashion due to the ease of parallelizing<br />the simple sum over N, or in a stochastic fashion by<br />selecting mini-batches of the data at random as we<br />shall show in the following.<br />Predictions<br />q(f,u) = p(f |u)q(u). To make predictions at a set<br />of test points X? for the new latent function values<br />f?, we substitute our approximate posterior into the<br />standard probabilistic rule:<br />?<br />≈<br />?<br />where the last line occurs due to the consistency rules<br />of the GP. The integral is tractable similarly to (19),<br />and we can compute the mean and variance of a test-<br />latent f?in O(M2), from which the distribution of the<br />test label y?is easily computed.<br />Our approximate posterior is given as<br />p(f?|y) =p(f?|f,u)p(f,u|y)dfdu(23)<br />?<br />p(f?|f,u)p(f |u)q(u)dfdu (24)<br />=p(f?|u)q(u)du(25)<br />Limiting cases<br />work, consider two limiting cases of this bound. First,<br />when the inducing variables are equal to the data<br />points Z=X, and second, where the likelihood is re-<br />placed with Gaussian noise.<br />To relate this method to existing<br />When Z = X, the approximate posterior in equation<br />(18) reduces to q(f) = N(f |m, S). In this special case<br />the number of parameters required to represent the<br />covariance can be reduced to 2N [Opper and Archam-<br />beau, 2009], and we have recovered the full-Gaussian</p>  <p>Page 6</p> <p>Scalable Variational Gaussian Process Classification<br />KLSP<br />M=4 M=8M=16M=32M=64Full<br />MF<br />GFITC<br />Figure 1:<br />KL method, the mean field method and Generalized FITC, whilst columns show increasing numbers of inducing<br />points. In each pane, the colored points represent training data, the inducing inputs are black dots and the<br />decision boundaries are black lines. The rightmost column shows the result of the equivalent non-sparse methods<br />The effect of increasing the number of inducing points for the banana dataset. Rows represent the<br />approximation (the ‘KL’ method described by Nick-<br />isch and Rasmussen [2008]).<br />If the likelihood were Gaussian, the expectations in<br />equation (21) would be computable in closed form, and<br />after a little re-arranging the result is as [Hensman<br />et al., 2013], equation (7). In this case the bound has<br />a unique solution for m and S, which recovers the<br />variational bound of Titsias [2009], equation (6).<br />In the case where Z=X and the likelihood is Gaussian,<br />exact inference is recovered.<br />5Experiments<br />We have proposed two variational approximations for<br />GP classification. The first in section 3 comprises a<br />mean-field approximation after making a variational<br />approximation to the prior over an augmented latent<br />vector. The second in section 4 proposes to minimize<br />the KL divergence using a Gaussian approximation at<br />a set of inducing points. We henceforth refer to these<br />as the MF (mean-field) and KL methods respectively.<br />Increasing the number of inducing points<br />compare the methods with the state-of-the-art Gen-<br />eralized FITC method, we first turn to the two-<br />dimensional Banana dataset. For all three methods,<br />we initialized the inducing points using k-means clus-<br />tering. For the generalized FITC method we used the<br />implementation provided by Rasmussen and Nickisch<br />[2010]. For all the methods we used the L-BFGS-B<br />optimizer [Zhu et al., 1997].<br />To<br />With the expectation that increasing the number of<br />inducing points should improve all three methods, we<br />applied 4 to 64 inducing points, as shown in Figure<br />1. The KL method pulls the inducing points positions<br />toward the decision boundary, and provides a near-<br />optimal solution with 16 inducing points. The MF<br />method is less able to adapt the inducing input posi-<br />tions, but provides good solutions at the same number<br />of inducing points.The Generalized FITC method<br />appears to pull inducing points towards the decision<br />boundary, but is unable to make good use of 64 induc-<br />ing points, moving some to the decision boundary and<br />some toward the origin.<br />Numerical comparison<br />mance of the classifiers for a number of commonly used<br />classification data sets. We took ten folds of the data<br />and report the median hold out negative log probabil-<br />ity and 2-σ confidence intervals. For comparison we<br />used the EP FITC implementation from the GPML<br />toolbox [Rasmussen and Nickisch, 2010] which is gen-<br />erally considered to be amongst the best implementa-<br />tions of this algorithm. For the KL and sparse KL<br />methods we found that the optimization behaviour<br />was improved by freezing the kernel hyper parameters<br />at the beginning of optimization and then unfreezing<br />them once a reasonable set of variational parameters<br />has been attained. Table 1 shows the result of the<br />experiments. In the case where a classifier gives a ex-<br />tremely confident wrong prediction for one or more<br />test points one can obtain a numerically high negative<br />log probability. These cases are denoted as ‘large’.<br />We compared the perfor-</p>  <p>Page 7</p> <p>James Hensman, Alex Matthews, Zoubin Ghahramani<br />Table 1: comparison of the performance of our proposed methods and Generalized FITC on benchmark datasets.<br />EPFitc<br />M=3.0%<br />thyroid.11 ± .05<br />heart.46 ± .14<br />twonorm.17 ± .43<br />ringnorm.21 ± .22<br />german.51 ± .09<br />waveform.23 ± .09<br />cancer.56 ± .09<br />flare solar.59 ± .02<br />diabetes.48 ± .04<br />DatasetKLEP<br />EPFitc<br />M=8<br />.15 ± .04<br />.42 ± .08<br />Large<br />.34 ± .01<br />.49 ± .04<br />.22 ± .01<br />.58 ± .06<br />.57 ± .02<br />.50 ± .02<br />KLSp<br />M=8<br />.13 ± .06<br />.42 ± .11<br />.08 ± .01<br />.41 ± .09<br />.49 ± .05<br />.23 ± .01<br />.56 ± .07<br />.59 ± .02<br />.47 ± .02<br />KLSp<br />M=3.0%<br />.09 ± .05<br />.47 ± .18<br />.09 ± .02<br />.15 ± .03<br />.51 ± .08<br />.25 ± .04<br />.58 ± .13<br />.58 ± .02<br />.51 ± .04<br />MFSp<br />M=8<br />.22 ± .16<br />.41 ± .15<br />Large<br />.46 ± .00<br />.52 ± .06<br />.28 ± .01<br />.58 ± .11<br />.64 ± .05<br />Large<br />MFSp<br />M=3.0%<br />.15 ± .14<br />.43 ± .19<br />Large<br />Large<br />.51 ± .09<br />Large<br />.59 ± .11<br />.60 ± .04<br />.50 ± .04<br />.11 ± .05<br />.43 ± .11<br />.08 ± .01<br />.18 ± .02<br />.48 ± .06<br />.23 ± .02<br />.55 ± .08<br />.60 ± .02<br />.48 ± .03<br />.13 ± .04<br />.44 ± .08<br />.08 ± .01<br />.20 ± .01<br />.50 ± .04<br />.22 ± .01<br />.56 ± .07<br />.57 ± .02<br />.51 ± .02<br />The table shows that mean field based methods of-<br />ten give over confident predictions. The results show<br />similar performance between the sparse KL and FITC<br />methods. The confidence intervals of the two methods<br />either overlap or sparse KL is better. As we shall see,<br />Sparse KL runs faster in the timed experiments that<br />follow and can be run on very large datasets using<br />stochastic gradient descent.<br />Time-performance trade-off<br />rithms used perform optimization there is a trade-off<br />for amount of time spent on optimization against the<br />classification performance achieved. The whole trade-<br />off curve can be used to characterize the efficiency of<br />a given algorithm.<br />Since all the algo-<br />Naish-Guzman and Holden [2007] consider the image<br />dataset amongst their benchmarks. Of the datasets<br />they consider it is one of the most demanding in terms<br />of the number of inducing points that are required. We<br />ran timed experiments on this dataset recording the<br />wall clock time after each function call and computing<br />the hold out negative log probability of the associated<br />parameters at each step.<br />We profiled the MFSp and KLSp algorithms for a va-<br />riety of inducing point numbers. Although GPML is<br />implemented in MATLAB rather than Python both<br />have access to fast numerical linear algebra libraries.<br />Optimization for the sparse KL and FITC methods<br />was carried out using the LBFGS-B Zhu et al. [1997]<br />algorithm. The mean field optimization surface is chal-<br />lenging numerically and we found that performance<br />was improved by using the scaled conjugate gradients<br />algorithm. We found no significant qualitative differ-<br />ence between the wall clock time and CPU times.<br />Figure 2 shows the results. The efficient frontier of the<br />comparison is defined by the algorithm that for any<br />given time achieves the lowest negative log probabil-<br />ity. In this case the efficient frontier is occupied by the<br />sparse KL method. Each of the algorithms is showing<br />better performance as the number of inducing points<br />increases. The challenging optimization behaviour of<br />the sparse mean-field algorithm can be seen by the un-<br />predictable changes in hold out probability as the opti-<br />mization proceeds. The supplementary material shows<br />that in terms of classification error rate this algorithm<br />is much better behaved, suggesting that MFSp finds it<br />difficult to produce well calibrated predictions.<br />The supplementary material contains plots of the hold<br />out classification accuracy for the image dataset. It<br />also contains similar plots for the banana dataset.<br />Stochastic optimization<br />proposed KL method can be optimized effectively in<br />a stochastic fashion, we first turn to the MNIST data<br />set. Selecting the odd digits and even digits to make<br />a binary problem, results in a training set with 60000<br />points. We used ADADELTA method [Zeiler, 2012]<br />from the climin toolbox [Osendorfer et al., 2014]. Se-<br />lecting a step-rate of 0.1, a mini-batch size of 10 with<br />200 inducing points resulted in a hold-out accuracy of<br />97.8%. The mean negative-log-probability of the test-<br />ing set was 0.069. It is encouraging that we are able to<br />fit highly nonlinear, high-dimensional decision bound-<br />aries with good accuracy, on a dataset size that is out<br />of the range of existing methods.<br />To demonstrate that the<br />Stochastic optimization of our bound allows fitting of<br />GP classifiers to datasets that are larger than previ-<br />ously possible. We downloaded the flight arrival and<br />departure times for every commercial flight in the USA<br />from January 2008 to April 2008.1This dataset con-<br />tains information about 5.9 million flights, including<br />the delay in reaching the destination. We build a clas-<br />sifier which was to predict whether any flight was to<br />subject to delay.<br />As a benchmark, we first fitted a linear model using<br />1Hensman et al. use this data to illustrate regression,<br />here we simply classify whether the delay ≤ zero.</p>  <p>Page 8</p> <p>Scalable Variational Gaussian Process Classification<br />100<br />101<br />102<br />103<br />104<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />Time (seconds)<br />Holdout negative log probability<br />KLSp M=4<br />KLSp M=50<br />KLSp M=200<br />MFSp M=4<br />MFSp M=50<br />MFSp M=200<br />EPFitc M=4<br />EPFitc M=50<br />EPFitc M=200<br />Figure 2: Temporal performance of the different methods on the image dataset.<br />scikits-learn [Pedregosa et al., 2011] which in turns<br />uses LIBLINEAR [Fan et al., 2008]. On our randomly<br />selected hold-out set of 100000 points, this achieved<br />a surprisingly low error rate of 37%, the negative-log<br />probability of the held-out data was 0.642.<br />We built a Gaussian process kernel using the sum of<br />a Matern-3<br />2and a linear kernel. For each, we intro-<br />duced parameters which allowed for the scaling of the<br />inputs (sometimes called Automatic Relevance Deter-<br />mination, ARD). Using a similar optimization scheme<br />to the MNIST data above, our method was able to ex-<br />ceed the performance of a linear model in a few min-<br />utes, as shown in Figure 3. The kernel parameters at<br />convergence suggested that the problem is highly non-<br />linear: the relative variance of the linear kernel was<br />negligible. The optimized lengthscales for the Matern<br />part of the covariance suggested that the most useful<br />features were the time of day and time of year.<br />6Discussion<br />We have presented two novel variational bounds for<br />performing sparse GP classification.<br />the existing GFITC method, makes an approximation<br />to the covariance matrix before introducing the non-<br />conjugate likelihood. These approaches are somewhat<br />unsatisfactory since in performing approximate infer-<br />ence, we necessarily introduce additional parameters<br />(variational means and variances, or the parameters<br />of EP factors), which naturally scale linearly with N.<br />The first, like<br />Our proposed KLSP bound outperforms the state-of-<br />the art GFITC method on benchmark datasets, and is<br />capable of being optimized in a stochastic fashion as<br />we have shown, making GP classification applicable to<br />big data for the first time.<br />102<br />103<br />104<br />105<br />0.32<br />0.34<br />0.36<br />0.38<br />0.4<br />error (%)<br />102<br />103<br />Time (seconds)<br />104<br />105<br />0.6<br />0.62<br />0.64<br />0.66<br />-log p(y)<br />Figure 3:<br />The red horizontal line depicts performance of a linear<br />classifier, whilst the blue line shows performance of the<br />stochastically optimized KLsparse method.<br />Performance of the airline delay dataset.<br />In future work, we note that this work opens the door<br />for several other GP models: if the likelihood factorizes<br />in N, then our method is applicable through Gauss-<br />Hermite quadrature of the log likelihood. We also note<br />that it is possible to relax the restriction of q(u) to<br />a Gaussian form, and mixture model approximations<br />follow straightforwardly, allowing scalable extensions<br />of Nguyen and Bonilla [2014].</p>  <p>Page 9</p> <p>James Hensman, Alex Matthews, Zoubin Ghahramani<br />References<br />C. Cortes and N. D. Lawrence, editors. Advances in<br />Neural Information Processing Systems, Cambridge,<br />MA, 2014. MIT Press.<br />Z. Dai, A. Damianou, J. Hensman, and N. Lawrence.<br />Gaussian process models with parallelization and<br />gpu acceleration. arXiv, 2014.<br />A. Damianou and N. Lawrence. Deep gaussian pro-<br />cesses. In Proceedings of the Sixteenth International<br />Conference on Artificial Intelligence and Statistics,<br />pages 207–215, 2013.<br />R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang,<br />and C.-J. Lin. Liblinear: A library for large lin-<br />ear classification. The Journal of Machine Learning<br />Research, 9:1871–1874, 2008.<br />Y. Gal, M. van der Wilk, and C. E. Rasmussen. Dis-<br />tributed variational inference in sparse Gaussian<br />process regression and latent variable models. In<br />Cortes and Lawrence [2014].<br />J. Hensman, N. Fusi, and N. D. Lawrence.<br />sian processes for big data. In A. Nicholson and<br />P. Smyth, editors, Uncertainty in Artificial Intelli-<br />gence, volume 29. AUAI Press, 2013.<br />Gaus-<br />J. Hensman, M. Zwießele, and N. D. Lawrence. Tilted<br />variational bayes. In S. Kaski and J. Corander, ed-<br />itors, Proceedings of the Seventeenth International<br />Conference on Artificial Intelligence and Statistics,<br />volume 33, pages 356–364. JMLR W&amp;CP, 2014.<br />E. Khan, S. Mohamed, and K. P. Murphy.<br />bayesian inference for non-conjugate gaussian pro-<br />cess regression.In P. Bartlett, editor, Advances<br />in Neural Information Processing Systems, pages<br />3140–3148, Cambridge, MA, 2012. MIT Press.<br />Fast<br />M. Kuss and C. E. Rasmussen. Assessing approximate<br />inference for binary Gaussian process classification.<br />The Journal of Machine Learning Research, 6:1679–<br />1704, 2005.<br />N. Lawrence, M. Seeger, and R. Herbrich. Fast sparse<br />Gaussian process methods: The informative vector<br />machine. In Proceedings of the 16th Annual Con-<br />ference on Neural Information Processing Systems,<br />number EPFL-CONF-161319, pages 609–616, 2003.<br />N. D. Lawrence. Gaussian process latent variable mod-<br />els for visualisation of high dimensional data. Ad-<br />vances in neural information processing systems, 16:<br />329–336, 2004.<br />A. Naish-Guzman and S. Holden. The generalized fitc<br />approximation. In Advances in Neural Information<br />Processing Systems, pages 1057–1064, 2007.<br />T. Nguyen and E. Bonilla. Automated variational in-<br />ference for Gaussian process models. In Cortes and<br />Lawrence [2014].<br />H. Nickisch and C. E. Rasmussen. Approximations for<br />binary Gaussian process classification. Journal of<br />Machine Learning Research, 9:2035–2078, 2008.<br />M. Opper and C. Archambeau. The variational Gaus-<br />sian approximation revisited. Neural computation,<br />21(3):786–792, 2009.<br />C. Osendorfer, J. Bayer, S. Diot-Girard, T. Rueck-<br />stiess, and S. Urban. Climin. http://github.com/<br />BRML/climin, 2014. Accessed: 2014-10-19.<br />F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,<br />B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,<br />R. Weiss, V. Dubourg, et al. Scikit-learn: Machine<br />learning in python. The Journal of Machine Learn-<br />ing Research, 12:2825–2830, 2011.<br />J. Qui˜ nonero-Candela and C. E. Rasmussen. A uni-<br />fying view of sparse approximate Gaussian process<br />regression. The Journal of Machine Learning Re-<br />search, 6:1939–1959, 2005.<br />C. E. Rasmussen and H. Nickisch. Gaussian processes<br />for machine learning (gpml) toolbox. The Journal<br />of Machine Learning Research, 11:3011–3015, 2010.<br />C. E. Rasmussen and C. K. I. Williams. Gaussian pro-<br />cesses for machine learning. MIT Press, Cambridge,<br />MA, 2006.<br />E. Snelson and Z. Ghahramani. Sparse Gaussian pro-<br />cesses using pseudo-inputs. In Advances in Neural<br />Information Processing Systems, pages 1257–1264,<br />2005.<br />M. Titsias. Variational learning of inducing variables<br />in sparse Gaussian processes. In D. van Dyk and<br />M. Welling, editors, Proceedings of the Twelfth In-<br />ternational Workshop on Artificial Intelligence and<br />Statistics, volume 5, pages 567–574, Clearwater<br />Beach, FL, 16-18 April 2009. JMLR W&amp;CP 5.<br />V. Tolvanen. Gaussian processes with monotonicity<br />constraints for big data. Master’s thesis, Aalto Uni-<br />veristy, June 2014.<br />A. G. Wilson, D. A. Knowles, and Z. Ghahramani.<br />Gaussian process regression networks. In J. Lang-<br />ford and J. Pineau, editors, Proceedings of the<br />29th International Conference on Machine Learning<br />(ICML), Edinburgh, June 2012. Omnipress.<br />M. D. Zeiler. Adadelta: An adaptive learning rate<br />method. arXiv preprint arXiv:1212.5701, 2012.<br />C. Zhu, R. H. Byrd, P. Lu, and J. Nocedal. Algo-<br />rithm 778: L-bfgs-b: Fortran subroutines for large-<br />scale bound-constrained optimization. ACM Trans-<br />actions on Mathematical Software (TOMS), 23(4):<br />550–560, 1997.</p>  <p>Page 10</p> <p>Scalable Variational Gaussian Process Classification<br />Supplementary Material for :<br />Scalable Variational Gaussian Process<br />Classification<br />anonymous authors</p>  <p>Page 11</p> <p>James Hensman, Alex Matthews, Zoubin Ghahramani<br />10−2<br />10−1<br />100<br />101<br />102<br />103<br />104<br />0<br />5 · 10−2<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />0.35<br />0.4<br />0.45<br />0.5<br />0.55<br />0.6<br />0.65<br />0.7<br />Time (seconds))<br />Holdout negative log probability<br />KLSp M=4<br />KLSp M=50<br />KLSp M=200<br />MFSp M=4<br />MFSp M=50<br />MFSp M=200<br />EPFitc M=4<br />EPFitc M=50<br />EPFitc M=200<br />Figure S.1 : Hold out predictive densities of the different methods on the image dataset.</p>  <p>Page 12</p> <p>Scalable Variational Gaussian Process Classification<br />10−2<br />10−1<br />100<br />101<br />102<br />103<br />104<br />0<br />2 · 10−2<br />4 · 10−2<br />6 · 10−2<br />8 · 10−2<br />0.1<br />0.12<br />0.14<br />0.16<br />0.18<br />0.2<br />0.22<br />0.24<br />0.26<br />0.28<br />0.3<br />0.32<br />0.34<br />0.36<br />0.38<br />0.4<br />0.42<br />0.44<br />Time (seconds)<br />Holdout error rate<br />KLSp M=4<br />KLSp M=50<br />KLSp M=200<br />MFSp M=4<br />MFSp M=50<br />MFSp M=200<br />EPFitc M=4<br />EPFitc M=50<br />EPFitc M=200<br />Figure S.2 : Hold out errors of the different methods on the image dataset.</p>  <p>Page 13</p> <p>James Hensman, Alex Matthews, Zoubin Ghahramani<br />10−2<br />10−1<br />100<br />101<br />102<br />103<br />0.22<br />0.24<br />0.26<br />0.28<br />0.3<br />0.32<br />0.34<br />0.36<br />0.38<br />0.4<br />0.42<br />0.44<br />0.46<br />0.48<br />0.5<br />0.52<br />0.54<br />0.56<br />0.58<br />0.6<br />Time (seconds)<br />Holdout negative log probability<br />KLSp M=4<br />KLSp M=8<br />KLSp M=16<br />KLSp M=32<br />EP<br />KL<br />MF<br />MFSp M=4<br />MFSp M=8<br />MFSp M=16<br />MFSp M=32<br />EPFitc M=4<br />EPFitc M=8<br />EPFitc M=16<br />EPFitc M=32<br />Figure S.3 : Hold out predictive densities of the different methods on the banana dataset.</p>  <p>Page 14</p> <p>Scalable Variational Gaussian Process Classification<br />10−1<br />100<br />101<br />Time (seconds)<br />102<br />103<br />0.24<br />0.25<br />0.25<br />0.26<br />0.26<br />0.27<br />0.27<br />0.28<br />0.28<br />0.29<br />0.29<br />0.3<br />0.3<br />0.31<br />0.31<br />Holdout negative log probability<br />KLSp M=4<br />KLSp M=8<br />KLSp M=16<br />KLSp M=32<br />EP<br />KL<br />MF<br />MFSp M=4<br />MFSp M=8<br />MFSp M=16<br />MFSp M=32<br />EPFitc M=4<br />EPFitc M=8<br />EPFitc M=16<br />EPFitc M=32<br />Figure S.4 : Hold out predictive densities of the different methods on the banana dataset.</p>  <p>Page 15</p> <p>James Hensman, Alex Matthews, Zoubin Ghahramani<br />10−3<br />10−2<br />10−1<br />100<br />Time (seconds)<br />101<br />102<br />103<br />104<br />0.10<br />0.15<br />0.20<br />0.25<br />0.30<br />0.35<br />0.40<br />0.45<br />0.50<br />Holdout error rate<br />KLSp M=4<br />KLSp M=8<br />KLSp M=16<br />KLSp M=32<br />EP<br />KL<br />MF<br />MFSp M=4<br />MFSp M=8<br />MFSp M=16<br />MFSp M=32<br />EPFitc M=4<br />EPFitc M=8<br />EPFitc M=16<br />EPFitc M=32<br />Figure S.5 : Hold out errors of the different methods on the banana dataset.</p>  <p>Page 16</p> <p>Scalable Variational Gaussian Process Classification<br />10−2<br />10−1<br />100<br />101<br />102<br />103<br />104<br />0.10<br />0.15<br />Time (seconds)<br />Holdout error rate<br />KLSp M=4<br />KLSp M=8<br />KLSp M=16<br />KLSp M=32<br />EP<br />KL<br />MF<br />MFSp M=4<br />MFSp M=8<br />MFSp M=16<br />MFSp M=32<br />EPFitc M=4<br />EPFitc M=8<br />EPFitc M=16<br />EPFitc M=32<br />Figure S.6 : Hold out errors of the different methods on the banana dataset.</p>  <a href="https://www.researchgate.net/profile/James_Hensman/publication/268079368_Scalable_Variational_Gaussian_Process_Classification/links/55adfd9508aed9b7dcdb08a5.pdf">Download full-text</a> </div> <div id="rgw20_56ab9fb5b3228" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw21_56ab9fb5b3228">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw22_56ab9fb5b3228"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/James_Hensman/publication/268079368_Scalable_Variational_Gaussian_Process_Classification/links/55adfd9508aed9b7dcdb08a5.pdf" class="publication-viewer" title="55adfd9508aed9b7dcdb08a5.pdf">55adfd9508aed9b7dcdb08a5.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/James_Hensman">James Hensman</a> &middot; Jul 21, 2015 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw23_56ab9fb5b3228"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://de.arxiv.org/pdf/1411.2005" target="_blank" rel="nofollow" class="publication-viewer" title="Scalable Variational Gaussian Process Classification">Scalable Variational Gaussian Process Classificati...</a> </div>  <div class="details">   Available from <a href="http://de.arxiv.org/pdf/1411.2005" target="_blank" rel="nofollow">de.arxiv.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw25_56ab9fb5b3228" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw26_56ab9fb5b3228">  </ul> </div> </div>   <div id="rgw16_56ab9fb5b3228" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw17_56ab9fb5b3228"> <div> <h5> <a href="publication/289762582_Phoneme_classification_using_constrained_variational_Gaussian_process_dynamical_system" class="color-inherit ga-similar-publication-title"><span class="publication-title">Phoneme classification using constrained variational Gaussian process dynamical system</span></a>  </h5>  <div class="authors"> <a href="researcher/2093098424_H_Park" class="authors ga-similar-publication-author">H. Park</a>, <a href="researcher/2093031797_S_Yun" class="authors ga-similar-publication-author">S. Yun</a>, <a href="researcher/2004746478_S_Park" class="authors ga-similar-publication-author">S. Park</a>, <a href="researcher/2074748306_J_Kim" class="authors ga-similar-publication-author">J. Kim</a>, <a href="researcher/2093001123_CD_Yoo" class="authors ga-similar-publication-author">C.D. Yoo</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56ab9fb5b3228"> <div> <h5> <a href="publication/221619721_Variational_Gaussian-process_factor_analysis_for_modeling_spatio-temporal_data" class="color-inherit ga-similar-publication-title"><span class="publication-title">Variational Gaussian-process factor analysis for modeling spatio-temporal data</span></a>  </h5>  <div class="authors"> <a href="researcher/70145370_Jaakko_Luttinen" class="authors ga-similar-publication-author">Jaakko Luttinen</a>, <a href="researcher/11320919_Alexander_Ilin" class="authors ga-similar-publication-author">Alexander Ilin</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56ab9fb5b3228"> <div> <h5> <a href="publication/263281044_Variational_Gaussian_Process_State-Space_Models" class="color-inherit ga-similar-publication-title"><span class="publication-title">Variational Gaussian Process State-Space Models</span></a>  </h5>  <div class="authors"> <a href="researcher/2006925236_Roger_Frigola" class="authors ga-similar-publication-author">Roger Frigola</a>, <a href="researcher/2050105511_Yutian_Chen" class="authors ga-similar-publication-author">Yutian Chen</a>, <a href="researcher/43277170_Carl_E_Rasmussen" class="authors ga-similar-publication-author">Carl E. Rasmussen</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw39_56ab9fb5b3228" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw40_56ab9fb5b3228">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw41_56ab9fb5b3228" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=suQ8hY9MMRGsGnf2Ri_TuKbxzvpq3ye8DYvrYQVMUP6NUEA8ISUqGeeWcQO1SvSg" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="hAWLp2/vIQxp6aD3wvN9LVXvU36fF5m100cijKrNLdyK0x4HJCTdddt7o8jh+gqABRWIcIE022chE7wfpP7gUZED4JUIhRzwQ0ZDRO/gW2P0b8IepQxWcFDiaod59903oUNcStI8KqwDYZvBW/d0CVHYCI1zGgCNAZo7xvRbkGKZEW5AhBIcUplMCPZbEcFfhI5tamRrbKiF6Sm1pRqntbrXw7k1DDuNdZqI1lMMIXGEHOEkIyyYFVveuwLrRZPI7Crsf68CaZmtEyQ0D5TSa/j23q+9Y9ZtwHGT4lo2XbA="/> <input type="hidden" name="urlAfterLogin" value="publication/268079368_Scalable_Variational_Gaussian_Process_Classification"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjY4MDc5MzY4X1NjYWxhYmxlX1ZhcmlhdGlvbmFsX0dhdXNzaWFuX1Byb2Nlc3NfQ2xhc3NpZmljYXRpb24%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjY4MDc5MzY4X1NjYWxhYmxlX1ZhcmlhdGlvbmFsX0dhdXNzaWFuX1Byb2Nlc3NfQ2xhc3NpZmljYXRpb24%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjY4MDc5MzY4X1NjYWxhYmxlX1ZhcmlhdGlvbmFsX0dhdXNzaWFuX1Byb2Nlc3NfQ2xhc3NpZmljYXRpb24%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw42_56ab9fb5b3228"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 1007;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"James Hensman","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277099177889798%401443077000994_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/James_Hensman","institution":"Lancaster University","institutionUrl":false,"widgetId":"rgw4_56ab9fb5b3228"},"id":"rgw4_56ab9fb5b3228","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=3686694","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab9fb5b3228"},"id":"rgw3_56ab9fb5b3228","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=268079368","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":268079368,"title":"Scalable Variational Gaussian Process Classification","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"11\/2014;","publicationDateRobot":"2014-11","article":""}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1411.2005","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Scalable Variational Gaussian Process Classification"},{"key":"rft.date","value":"2014"},{"key":"rft.au","value":"James Hensman,Alex Matthews,Zoubin Ghahramani"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56ab9fb5b3228"},"id":"rgw6_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=268079368","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":268079368,"peopleItems":[{"data":{"authorNameOnPublication":"James Hensman","accountUrl":"profile\/James_Hensman","accountKey":"James_Hensman","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277099177889798%401443077000994_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"James Hensman","profile":{"professionalInstitution":{"professionalInstitutionName":"Lancaster University","professionalInstitutionUrl":"institution\/Lancaster_University"}},"professionalInstitutionName":"Lancaster University","professionalInstitutionUrl":"institution\/Lancaster_University","url":"profile\/James_Hensman","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277099177889798%401443077000994_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"James_Hensman","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw9_56ab9fb5b3228"},"id":"rgw9_56ab9fb5b3228","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=3686694&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Lancaster University","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":1,"publicationUid":268079368,"widgetId":"rgw8_56ab9fb5b3228"},"id":"rgw8_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=3686694&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=1&publicationUid=268079368","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/2058019585_Alex_Matthews","authorNameOnPublication":"Alex Matthews","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Alex Matthews","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2058019585_Alex_Matthews","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw11_56ab9fb5b3228"},"id":"rgw11_56ab9fb5b3228","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2058019585&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw10_56ab9fb5b3228"},"id":"rgw10_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2058019585&authorNameOnPublication=Alex%20Matthews","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/8159937_Zoubin_Ghahramani","authorNameOnPublication":"Zoubin Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Zoubin Ghahramani","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/8159937_Zoubin_Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw13_56ab9fb5b3228"},"id":"rgw13_56ab9fb5b3228","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=8159937&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw12_56ab9fb5b3228"},"id":"rgw12_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=8159937&authorNameOnPublication=Zoubin%20Ghahramani","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab9fb5b3228"},"id":"rgw7_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=268079368&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":268079368,"abstract":"<noscript><\/noscript><div>Gaussian process classification is a popular method with a number of<br \/>\nappealing properties. We show how to scale the model within a variational<br \/>\ninducing point framework, outperforming the state of the art on benchmark<br \/>\ndatasets. Importantly, the variational formulation can be exploited to allow<br \/>\nclassification in problems with millions of data points, as we demonstrate in<br \/>\nexperiments.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw14_56ab9fb5b3228"},"id":"rgw14_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=268079368","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/268079368_Scalable_Variational_Gaussian_Process_Classification\/links\/55adfd9508aed9b7dcdb08a5\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw15_56ab9fb5b3228"},"id":"rgw15_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab9fb5b3228"},"id":"rgw5_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=268079368&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2093098424,"url":"researcher\/2093098424_H_Park","fullname":"H. Park","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2093031797,"url":"researcher\/2093031797_S_Yun","fullname":"S. Yun","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2004746478,"url":"researcher\/2004746478_S_Park","fullname":"S. Park","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2074748306,"url":"researcher\/2074748306_J_Kim","fullname":"J. Kim","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2012","journal":"Advances in neural information processing systems","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/289762582_Phoneme_classification_using_constrained_variational_Gaussian_process_dynamical_system","usePlainButton":true,"publicationUid":289762582,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/289762582_Phoneme_classification_using_constrained_variational_Gaussian_process_dynamical_system","title":"Phoneme classification using constrained variational Gaussian process dynamical system","displayTitleAsLink":true,"authors":[{"id":2093098424,"url":"researcher\/2093098424_H_Park","fullname":"H. Park","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2093031797,"url":"researcher\/2093031797_S_Yun","fullname":"S. Yun","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2004746478,"url":"researcher\/2004746478_S_Park","fullname":"S. Park","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2074748306,"url":"researcher\/2074748306_J_Kim","fullname":"J. Kim","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2093001123,"url":"researcher\/2093001123_CD_Yoo","fullname":"C.D. Yoo","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Advances in neural information processing systems 01\/2012; 3:2006-2014."],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/289762582_Phoneme_classification_using_constrained_variational_Gaussian_process_dynamical_system","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/289762582_Phoneme_classification_using_constrained_variational_Gaussian_process_dynamical_system\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab9fb5b3228"},"id":"rgw17_56ab9fb5b3228","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=289762582","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":70145370,"url":"researcher\/70145370_Jaakko_Luttinen","fullname":"Jaakko Luttinen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":11320919,"url":"researcher\/11320919_Alexander_Ilin","fullname":"Alexander Ilin","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Conference Paper","publicationDate":"Jan 2009","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/221619721_Variational_Gaussian-process_factor_analysis_for_modeling_spatio-temporal_data","usePlainButton":true,"publicationUid":221619721,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/221619721_Variational_Gaussian-process_factor_analysis_for_modeling_spatio-temporal_data","title":"Variational Gaussian-process factor analysis for modeling spatio-temporal data","displayTitleAsLink":true,"authors":[{"id":70145370,"url":"researcher\/70145370_Jaakko_Luttinen","fullname":"Jaakko Luttinen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":11320919,"url":"researcher\/11320919_Alexander_Ilin","fullname":"Alexander Ilin","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009. Proceedings of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada.; 01\/2009"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/221619721_Variational_Gaussian-process_factor_analysis_for_modeling_spatio-temporal_data","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/221619721_Variational_Gaussian-process_factor_analysis_for_modeling_spatio-temporal_data\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab9fb5b3228"},"id":"rgw18_56ab9fb5b3228","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=221619721","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2006925236,"url":"researcher\/2006925236_Roger_Frigola","fullname":"Roger Frigola","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2050105511,"url":"researcher\/2050105511_Yutian_Chen","fullname":"Yutian Chen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":43277170,"url":"researcher\/43277170_Carl_E_Rasmussen","fullname":"Carl E. Rasmussen","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jun 2014","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/263281044_Variational_Gaussian_Process_State-Space_Models","usePlainButton":true,"publicationUid":263281044,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/263281044_Variational_Gaussian_Process_State-Space_Models","title":"Variational Gaussian Process State-Space Models","displayTitleAsLink":true,"authors":[{"id":2006925236,"url":"researcher\/2006925236_Roger_Frigola","fullname":"Roger Frigola","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2050105511,"url":"researcher\/2050105511_Yutian_Chen","fullname":"Yutian Chen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":43277170,"url":"researcher\/43277170_Carl_E_Rasmussen","fullname":"Carl E. Rasmussen","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/263281044_Variational_Gaussian_Process_State-Space_Models","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/263281044_Variational_Gaussian_Process_State-Space_Models\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab9fb5b3228"},"id":"rgw19_56ab9fb5b3228","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=263281044","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw16_56ab9fb5b3228"},"id":"rgw16_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=268079368&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":268079368,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":268079368,"publicationType":"article","linkId":"55adfd9508aed9b7dcdb08a5","fileName":"55adfd9508aed9b7dcdb08a5.pdf","fileUrl":"profile\/James_Hensman\/publication\/268079368_Scalable_Variational_Gaussian_Process_Classification\/links\/55adfd9508aed9b7dcdb08a5.pdf","name":"James Hensman","nameUrl":"profile\/James_Hensman","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Jul 21, 2015","fileSize":"1.6 MB","widgetId":"rgw22_56ab9fb5b3228"},"id":"rgw22_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=268079368&linkId=55adfd9508aed9b7dcdb08a5&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":268079368,"publicationType":"article","linkId":"54b7770c0cf2e68eb28017ce","fileName":"Scalable Variational Gaussian Process Classification","fileUrl":"http:\/\/de.arxiv.org\/pdf\/1411.2005","name":"de.arxiv.org","nameUrl":"http:\/\/de.arxiv.org\/pdf\/1411.2005","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw23_56ab9fb5b3228"},"id":"rgw23_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=268079368&linkId=54b7770c0cf2e68eb28017ce&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw21_56ab9fb5b3228"},"id":"rgw21_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=268079368&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":4,"valueFormatted":"4","widgetId":"rgw24_56ab9fb5b3228"},"id":"rgw24_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=268079368","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw20_56ab9fb5b3228"},"id":"rgw20_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=268079368&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":268079368,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw26_56ab9fb5b3228"},"id":"rgw26_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=268079368&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":4,"valueFormatted":"4","widgetId":"rgw27_56ab9fb5b3228"},"id":"rgw27_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=268079368","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw25_56ab9fb5b3228"},"id":"rgw25_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=268079368&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Scalable Variational Gaussian Process Classification\nJames Hensman\nUniversity of Sheffield\nAlex Matthews\nUniversity of Cambridge\nZoubin Ghahramani\nUniversity of Cambridge\nAbstract\nGaussian process classification is a popular\nmethod with a number of appealing proper-\nties. We show how to scale the model within\na variational inducing point framework, out-\nperforming the state of the art on benchmark\ndatasets. Importantly, the variational formu-\nlation can be exploited to allow classification\nin problems with millions of data points, as\nwe demonstrate in experiments.\n1 Introduction\nGaussian processes (GPs) provide priors over functions\nthat can be used for many machine learning tasks. In\nthe regression setting, when the likelihood is Gaussian,\ninference can be performed in closed-form using linear\nalgebra. When the likelihood is non-Gaussian, such as\nin GP classification, the posterior and marginal like-\nlihood must be approximated. Kuss and Rasmussen\n[2005] and Nickisch and Rasmussen [2008] provide ex-\ncellent comparisons of several approximate inference\nmethods for GP classification. More recently, Opper\nand Archambeau [2009] and Khan et al. [2012] consid-\nered algorithmic improvements to variational approx-\nimations in non-conjugate GP models.\nThe computational cost of inference in GPs is O(N3)\nin general, where N is the number of data. In the\nregression setting, there has been much interest in\nlow-rank or sparse approaches to reduce this computa-\ntional complexity: Qui\u02dc nonero-Candela and Rasmussen\n[2005] provides a review. Many of these approximation\nschemes rely on the use of a series of inducing points,\nwhich can be difficult to select. Titsias [2009] sug-\ngests a variational approach (see section 2) which pro-\nvides an objective function for optimizing these points.\nThis variational idea was extended by Hensman et al.\n[2013], who showed how the variational objective could\nbe reformulated with additional parameters to enable\nstochastic optimization, which allows GPs to be fitted\nto millions of data.\nDespite much interest in approximate GP classifica-\ntion and sparse approximations, there has been lit-\ntle overlap of the two.The approximate inference\nschemes which deal with the non-conjugacy of the like-\nlihood generally scale with O(N3), as they require fac-\ntorization of the covariance matrix. Two approaches\nwhich have addressed these issues simultaneously are\nthe IVM [Lawrence et al., 2003] and Generalized FITC\n[Naish-Guzman and Holden, 2007] which both have\nshortcomings as we shall discuss.\nIt is tempting to think that sparse GP classification is\nsimply a case of combining a low-rank approximation\nto the covariance with one\u2019s preferred non-conjugate\napproximation, but as we shall show this does not nec-\nessarily lead to an effective method: it can be very\ndifficult to place the inducing input points, and scala-\nbility of the method is usually restricted by the com-\nplexity of matrix-matrix multiplications.\nFor these reasons, there is a strong case for a non-\nconjugate sparse GP scheme which provides a vari-\national bound on the marginal likelihood, combining\nthe scalability of the stochastic optimization approach\nwith the ability to optimize the positions of the induc-\ning inputs. Furthermore, a variational approach would\nallow for integration of the approximation within other\nGP models such as GP regression networks [Wilson\net al., 2012], latent variable models [Lawrence, 2004]\nand deep GPs [Damianou and Lawrence, 2013], as the\nvariational objective could be optimized as part of such\na model without fear of overfitting.\nThe rest of the paper is arranged as follows. In sec-\ntion 2, we briefly cover some background material\nand existing work. In section 3, we show how vari-\national approximations to the covariance matrix can\nbe used post-hoc to provide variational bounds with\nnon-Gaussian likelihoods. These approaches are not\nentirely satisfactory, and so in section 4 we provide a\nvariational approach which does not first approximate\nthe covariance matrix, but delivers a variational bound\ndirectly. In section 5 we compare our proposals with\nthe state of the art, as well as demonstrating empiri-\ncally that our preferred method is applicable to very\nlarge datasets through stochastic variational optimiza-\ntion. Section 6 concludes.\narXiv:1411.2005v1  [stat.ML]  7 Nov 2014"},{"page":2,"text":"Scalable Variational Gaussian Process Classification\n2Background\nGaussian Process Classification\ncess priors provide rich nonparametric models of func-\ntions. To perform classification with this prior, the\nprocess is \u2018squashed\u2019 through a sigmoidal inverse-link\nfunction, and a Bernoulli likelihood conditions the\ndata on the transformed function values.\nmussen and Williams [2006] for a review.\nGaussian pro-\nSee Ras-\nWe denote the binary class observations as y =\n{yn}N\nsign matrix X = {xn}N\nfunction at all pairs of input vectors to build the co-\nvariance matrix Knnin the usual way, and arrive at\na prior for the values of the GP function at the input\npoints: p(f) = N(f |0,Knn).\nWe denote the probit inverse link function as\n\u03c6(x) =?x\ndistribution of data and latent variables becomes\nn=1, and then collect the input data into a de-\nn=1. We evaluate the covariance\n\u2212\u221eN(a|0, 1)da and the Bernoulli distribu-\ntion B(yn|\u03c6(fn)) = \u03c6(fn)yn(1\u2212\u03c6(fn))1\u2212yn. The joint\np(y,f) =\nN\n?\nn=1\nB(yn|\u03c6(fn)) N(f |0,Knn) .(1)\nThe main object of interest is the posterior over func-\ntion values p(f |y), which must be approximated. We\nalso require an approximation to the marginal likeli-\nhood p(y) in order to optimize (or marginalize) pa-\nrameters of the covariance function. An assortment of\napproximation schemes have been proposed (see Nick-\nisch and Rasmussen [2008] for a comparison), but they\nall require O(N3) computation.\nSparse Gaussian Processes for Regression\ncomputational complexity of any Gaussian process\nmethod scales with O(N3) because of the need to in-\nvert the covariance matrix K.\nputational complexity, many approximation schemes\nhave been proposed, though most focus on regression\ntasks, see Qui\u02dc nonero-Candela and Rasmussen [2005]\nfor a review. Here we focus on inducing point meth-\nods [Snelson and Ghahramani, 2005], where the latent\nvariables are augmented with additional input-output\npairs Z,u, known as \u2018inducing inputs\u2019 and \u2018inducing\nvariables\u2019.\nThe\nTo reduce the com-\nThe random variables u are points on the function in\nexactly the same way as f, and so the joint distribution\ncan be written\n??\nwhere Kmm is formed by evaluating the covariance\nfunction at all pairs of inducing inputs points zm,zm?,\nand Knmis formed by evaluating the covariance func-\ntion across the data input points and inducing inputs\np(f,u) = N\nf\nu\n????0,\n?\nKnn\nK?\nnm\nKnm\nKmm\n??\n(2)\npoints similarly. Using the properties of a multivariate\nnormal distribution, the joint can be re-written as\np(f,u) = p(f |u)p(u)\n= N(f |KnmK\u22121\n(3)\nmmu,Knn\u2212 Qnn)N(u|0, Kmm)\nwith Qnn = KnmK\u22121\nnow takes the form\nmmK?\nnm. The joint distribution\np(y,f,u) = p(y|f)p(f |u)p(u) .(4)\nTo obtain computationally efficient inference, integra-\ntion over f is approximated. To obtain the popular\nFITC method (in the case of Gaussian likelihood), a\nfactorization is enforced: p(y|u) \u2248?\nity is used\nnp(yn|u). To\nget a variational approximation, the following inequal-\nlogp(y|u) \u2265 Ep(f |u)[logp(y|f)] ? log \u02dc p(y|u) . (5)\nSubstituting this bound on the conditional into the\nstandard expression p(y) =\ntractable bound on the marginal likelihood for the\nGaussian case [Titsias, 2009]:\n?p(y|u)p(u)du gives a\nlogp(y) \u2265logN(y|0,KnmK\u22121\n\u2212\nmmK?\nnm+ \u03c32I)\n1\n2\u03c32tr(Knn\u2212 Qnn) ,\n(6)\nwhere \u03c32is the variance of the Gaussian likelihood\nterm. This bound on the marginal likelihood can then\nbe used as an objective function in optimizing the\ncovariance function parameters as well as the induc-\ning input points Z. The bound becomes tight when\nthe inducing points are the data points: Z = X so\nKnm= Kmm= Knn and (6) becomes equal the true\nmarginal likelihood logN(y|0, Knn+ \u03c32I).\nComputing this bound (6) and its derivatives costs\nO(NM2).\ncan be achieved by introducing additional variational\nparameters [Hensman et al., 2013]. Noting that (6) im-\nplies an approximate posterior \u02dc p(u|y), we introduce\na variational distribution q(u) = N(u|m,S) to ap-\nproximate this distribution, and applying a standard\nvariational bound, obtain:\nA more computationally scalable bound\nlogp(y) \u2265logN(y|KnmK\u22121\n\u2212\n1\n2\u03c32tr(Knn\u2212 Qnn) \u2212 KL[q(u)||p(u)] .\nmmm, \u03c32I)\n1\n2\u03c32tr(KnmK\u22121\nmmSK\u22121\nmmKmn)(7)\n\u2212\nThis bound has a unique optimum in terms of the vari-\national parameters m,S, at which point it is tight to\nthe original sparse GP bound (6). The advantage of\nthe representation in (7) is that it can be optimized in"},{"page":3,"text":"James Hensman, Alex Matthews, Zoubin Ghahramani\na stochastic [Hensman et al., 2013] or distributed [Dai\net al., 2014, Gal et al., 2015] fashion.\nOf course for the Bernoulli likelihood, the required in-\ntegrals for (6) and (7) are not tractable, but we will\nbuild on them both in subsequent sections to build\nsparse GP classifiers.\nRelated work\n(IVM) [Lawrence et al., 2003] is the first work to ap-\nproach sparse GP classification to our knowledge. The\nidea is to combine assumed density filtering with a se-\nlection heuristic to pick points from the data X to act\nas inducing points Z: the inducing variables u are then\na subset of the latent function variables f.\nThe informative vector machine\nThe IVM offers superior performance to support vector\nmachines [Lawrence et al., 2003], along with a proba-\nbilistic interpretation. However we might expect bet-\nter performance by relaxing the condition that the in-\nducing points be a sub-set of the data, as is the case for\nregression [Qui\u02dc nonero-Candela and Rasmussen, 2005].\nSubsequent work on sparse GP classification [Naish-\nGuzman and Holden, 2007] removed the restriction of\nselecting Z to be a subset of the data X, and ostensibly\nimproved over the assumed density filtering scheme by\nusing expectation propagation (EP) for inference.\nNaish-Guzman and Holden [2007] noted that when us-\ning the FITC approximation for a Gaussian likelihood,\nthe equivalent prior (see also Qui\u02dc nonero-Candela and\nRasmussen [2005]) is\np(f) \u2248 N(f |0, Qnn+ diag(Knn\u2212 Qnn)) .\nThe Generalized FITC method combines this approx-\nimate prior with a Bernoulli likelihood and uses EP\nto approximate the posterior. The form of the prior\nmeans that the linear algebra within the EP updates is\nsimplified, and a round of updates costs O(NM2). EP\nis nested inside an optimization loop, where the covari-\nance hyper-parameters and inducing inputs are opti-\nmized against the EP approximation to the marginal\nlikelihood. Computing the marginal likelihood approx-\nimation and gradients costs O(NM2).\nThe generalized FITC method works well in practise,\noften finding solutions which are as good as or bet-\nter than the IVM, with fewer inducing inputs points\n[Naish-Guzman and Holden, 2007].\n(8)\nDiscussion\nthan the IVM, GFITC does not satisfy our require-\nments for a scalable GP classifier. There is no clear\nway to distribute computation or use stochastic op-\ntimization in GFITC: we find that it is limited to a\nfew thousand data. Further, the positions of the in-\nducing input points Z can be optimized against the\nDespite performing significantly better\napproximation to the marginal likelihood, but there is\nno guarantee that this will provide a good solution: in-\ndeed, our experiments in section 5 show that this can\nlead to strange pathologies.\nTo obtain the ability to place inducing inputs as Tit-\nsias [2009] and to scale as Hensman et al. [2013], we de-\nsire a bound on the marginal likelihood against which\nto optimize Z. In the next section, we attempt to build\nsuch a variational approximation in the same fashion\nas FITC, by first using existing variational methods\nto approximate the covariance, and then using further\nvariational approximate methods to deal with non-\nconjugacy.\n3Two stage approaches\nA straightforward approach to building sparse GP\nclassifiers is to separate the low-rank approximation\nto the covariance from the non-Gaussian likelihood.\nIn other words, simply treat the approximation to the\ncovariance matrix as the prior, and then select an ap-\nproximate inference algorithm (e.g. from one of those\ncompared by Nickisch and Rasmussen [2008]), and\nthen proceed with approximate inference, exploiting\nthe form of the approximate covariance where possi-\nble for computation saving.\nThis is how the generalized FITC approximation was\nderived. However, as described above we aim to con-\nstruct variational approximations.\nIt\u2019s possible to construct a variational approach in this\nmould by using the fact that the probit likelihood can\nbe written as a convolution of a unit Gaussian and a\nstep function. Without modifying our original model,\nwe can introduce a set of additional latent variables g\nwhich relate to the original latent variables f through\na unit variance isotropic Gaussian:\np(y,f,g) =\nN\n?\nis\nn=1\nB(yn|\u03b8(gn))N(g|f,I)N(f|0,K) (9)\nwhere\nThe\nmarginalization\ntor:\n?N\nWe can now proceed by using a variational sparse GP\nbound on g, followed by a further variational approx-\nimation to deal with the non-Gaussian likelihood.\n\u03b8astep-function\n(1)\nthe additional\nn=1B(yn|\u03b8(gn))N(g|f,I)dg\nn=1B(yn|\u03c6(fn)).\ninverse-link.\nrecovered\nlatent\noriginalmodel\nof\nisby\nvec-\n? ?N\n=\n3.1Sparse mean field approach\nEncouraged by the success of a (non-sparse) factoriz-\ning approximation made by [Hensman et al., 2014], we"},{"page":4,"text":"Scalable Variational Gaussian Process Classification\ncouple a variational bound on p(g) with a mean-field\napproximation. Substituting the variational bound for\na Gaussian sparse GP (6) with our augmented model\n(9) (where g in (9) replaces y in (6)), we arrive at a\nbound on the joint distribution:\np(y, g) \u2265\nN\n?\nexp{\u22121\nn=1\nB(yn|\u03b8(gn))N(g|0,Qnn+ I)\n2tr(Knn\u2212 Qnn)} .\n(10)\nAssuming a factorizing distribution q(g) =?\nthe usual variational way, and the optimal form of the\napproximating distribution is a truncated Gaussian\nnq(gn),\nwe obtain a lower bound on the marginal likelihood in\nq?(gn) = B(yn|\u03b8(gn))N(gn|an, \u02dc \u03c32\nwhere \u02dc \u03c3\u22122\nn\nis given by the nthdiagonal element of\n[Qnn+ I]\u22121, \u03b3nare the required normalizers and an\nare free variational parameters. Some cancellation in\nthe bound leads to a tractable expression:\nn)\/\u03b3n , (11)\nlogp(y) \u2265\nN\n?\n\u22121\nn=1\nlog\u03b3n\u22121\n2log|Qnn+ I|\n2tr([Qnn+ I]\u22121Eq(g)[gg?])\nN\n?\n\u22121\nAll the components of this bound can be computed\nin maximum O(NM2) time. The expectations under\nthe factorizing variational distribution (and their gra-\ndients) are available in closed form.\n+1\n2\nn=1\n?log \u02dc \u03c32\nn+Eq(gn)[(an\u2212 gn)2]\n\u02dc \u03c32\nn\n?\n(12)\n2tr(Knn\u2212 Qnn).\nApproximate inference can now proceed by optimizing\nthis bound with respect to the variational parameters\na = {an}N\ncovariance function and the inducing points Z. Em-\npirically, we find it useful to optimize the variational\nparameters on an \u2018inner loop\u2019, which costs O(NM)\nper iteration. Computing the relevant gradients out-\nside this loop costs O(NM2).\nn=1, alongside the hyper-parameters of the\nPredictions\nfunction value g?at a test input point x?, we would\nlike to marginalize across the variational distribution:\n?\nSince this is in general intractable, we approximate it\nby Monte Carlo. Since the approximate posterior q is\na factorized series of truncated normal distributions,\nit is straight-forward to sample from. Prediction of a\nsingle test point costs O(N).\nTo make a prediction for a new latent\np(g?|y) \u2248\np(g?|g)q(g)dg .(13)\nAlternatively, we can employ a Gaussian approxima-\ntion to the posterior as suggested by Nickisch and\nRasmussen [2008]. In our sparse formulation, this re-\nsults in p(g|y) \u2248 N(g|\u03a3K\u22121\n\u03a3 = Kmm\u2212 Kmn[Qnn+ I]\u22121Knm. Substituting in\nto (13) results in a computational cost of O(M2) to\npredict a single test point.\nmmKmnEq(g)[g],\u03a3), with\n3.2A more scalable method?\nThe mean-field method in section 3.1 is somewhat un-\nsatisfactory since the number of variational parameters\nscales with N. The variational parameters are also de-\npendent on each other, and so application of stochastic\noptimization or distributed computing is difficult. For\nthe Gaussian likelihood case, Hensman et al. [2013]\nproposes a bound on logp(y) (7) which can be opti-\nmized effectively with O(M2) parameters. Perhaps it\nis possible to obtain a scalable algorithm by substitut-\ning this bound for p(g) as in the above?\nSubstituting (7) into (9) (again replacing y with g)\nresults in a tractable integral to obtain a bound on the\nmarginal likelihood, as Hensman et al. [2013] points\nout. The result is\n?\n\u2265\nn\n\u22121\n\u22121\nwhere k?\noptimized in a stochastic or distributed way [Tolva-\nnen, 2014], but has been found to be less effective than\nmight be expected (Owen Thomas, personal commu-\nnication). To understand why, consider the case where\nthe inducing points are set to the data points Z = X,\nso that u = f. The bound reduces to\n?\n\u22121\nand the optimum occurs where S\u22121= K\u22121\nm is the maximum a posteriori (MAP) point.\nlogp(y) = logp(y|g)p(g)dg\nB(yn|\u03c6(k?\n?\nnK\u22121\nmmm))\n2tr(KnmK\u22121\n2tr(Knn\u2212 Qnn) \u2212 KL[q(u)||p(u)] ,\nmmSK\u22121\nmmKmn)\nnis the nthrow of Knm. This bound can be\nlogp(y) \u2265\nn\nB(yn|\u03c6(mn))\n2tr(S) \u2212 KL[q(f)||p(f)] .\n(14)\nnn+ I, and\nThis approximation is reminiscent of the Laplace ap-\nproximation, which also places the mean of the pos-\nterior at the MAP point, and approximates the co-\nvariance with S\u22121= K\u22121\nsian of the log likelihood evaluated at the MAP point.\nThe Laplace approximation is known to be relatively\nineffective for classification [Nickisch and Rasmussen,\n2008], so it is no surprise that this variational approx-\nimation should be ineffective. From here we abandon\nnn+ W, where W is the Hes-"},{"page":5,"text":"James Hensman, Alex Matthews, Zoubin Ghahramani\nthis bound: the next section sees the construction of a\nvariational bound which is scalable and (empirically)\neffective.\n4A single variational bound\nHere we obtain a bound on the marginal likelihood\nwithout introducing the additional latent variables as\nabove, and without making factorizing assumptions.\nWe first return to the bound (5) on the conditional\nused to construct the variational bounds for the Gaus-\nsian case:\nlogp(y|u) \u2265 Ep(f |u)[logp(y|f)]\nwhich is in general intractable for the non-conjugate\ncase. We nevertheless persist, recalling the standard\nvariational equation\n(15)\nlogp(y) \u2265 Eq(u)[logp(y|u)] \u2212 KL[q(u)||p(u)] .\n(16)\nSubstituting (15) into (16) results in a (further) bound\non the marginal likelihood:\nlogp(y) \u2265 Eq(u)[logp(y|u)] \u2212 KL[q(u)||p(u)]\n\u2265 Eq(u)\n= Eq(f)[logp(y|f)] \u2212 KL[q(u)||p(u)]\nwhere we have defined:\n?\n?Ep(f|u)[logp(y|f)]?\u2212 KL[q(u)||p(u)]\n(17)\nq(f) :=p(f |u)q(u)du .(18)\nConsider the case where q(u) = N(u|m,S).\ngives the following functional form for q(f):\nThis\nq(f) = N(f |Am,Knn+ A(S \u2212 Kmm)A?)\nwith A = KnmK\u22121\n(19)\nmm.\nSince in the classification case the likelihood factors as\np(y|f) =\nN\n?\ni=1\np(yi|fi) ,(20)\nwe only require the marginals of q(f) in order to com-\npute the expectations in (17). We are left with some\none-dimensional integrals of the log-likelihood, which\ncan be computed by e.g. Gauss-Hermite quadrature:\nlogp(y) \u2265\nN\n?\nn=1\nEq(fn)[logp(yn|fn)] \u2212 KL[q(u)||p(u)] .\n(21)\nOur algorithm then consists of maximizing the pa-\nrameters of q(u) with respect to this bound on the\nmarginal likelihood using gradient based optimization.\nTo maintain positive-definiteness of S, we represent it\nusing a lower triangular form S = LL?, which allows\nus to perform unconstrained optimization.\nComputations and Scalability\nKL divergence of the bound (21) requires O(M3) com-\nputations. Since we expect the number of required in-\nducing points M to be much smaller than the number\nof data N, most of the work will be in computing the\nexpected likelihood terms. To compute the derivatives\nof these, we use the Gaussian identities made familiar\nto us by Opper and Archambeau [2009]:\nComputing the\n\u2202\n\u2202\u00b5EN(x|\u00b5,\u03c32)\n\u2202\n\u2202\u03c32EN(x|\u00b5,\u03c32)\n?f(x)?= EN(x|\u00b5,\u03c32)\n?f(x)?=1\n? \u2202\n? \u22022\n\u2202xf(x)?\n\u2202x2f(x)?.\n2EN(x|\u00b5,\u03c32)\n(22)\nWe can make use of these by substituting f for\nlogp(yn|fn) and \u00b5,\u03c32for the marginals of q(f) in (21).\nThese derivatives also have to be computed by quadra-\nture methods, after which derivatives with respect to\nm,L,Z and any covariance function parameters re-\nquires the application of straight-forward algebra.\nWe also have the option to optimize the objective in\na distributed fashion due to the ease of parallelizing\nthe simple sum over N, or in a stochastic fashion by\nselecting mini-batches of the data at random as we\nshall show in the following.\nPredictions\nq(f,u) = p(f |u)q(u). To make predictions at a set\nof test points X? for the new latent function values\nf?, we substitute our approximate posterior into the\nstandard probabilistic rule:\n?\n\u2248\n?\nwhere the last line occurs due to the consistency rules\nof the GP. The integral is tractable similarly to (19),\nand we can compute the mean and variance of a test-\nlatent f?in O(M2), from which the distribution of the\ntest label y?is easily computed.\nOur approximate posterior is given as\np(f?|y) =p(f?|f,u)p(f,u|y)dfdu(23)\n?\np(f?|f,u)p(f |u)q(u)dfdu (24)\n=p(f?|u)q(u)du(25)\nLimiting cases\nwork, consider two limiting cases of this bound. First,\nwhen the inducing variables are equal to the data\npoints Z=X, and second, where the likelihood is re-\nplaced with Gaussian noise.\nTo relate this method to existing\nWhen Z = X, the approximate posterior in equation\n(18) reduces to q(f) = N(f |m, S). In this special case\nthe number of parameters required to represent the\ncovariance can be reduced to 2N [Opper and Archam-\nbeau, 2009], and we have recovered the full-Gaussian"},{"page":6,"text":"Scalable Variational Gaussian Process Classification\nKLSP\nM=4 M=8M=16M=32M=64Full\nMF\nGFITC\nFigure 1:\nKL method, the mean field method and Generalized FITC, whilst columns show increasing numbers of inducing\npoints. In each pane, the colored points represent training data, the inducing inputs are black dots and the\ndecision boundaries are black lines. The rightmost column shows the result of the equivalent non-sparse methods\nThe effect of increasing the number of inducing points for the banana dataset. Rows represent the\napproximation (the \u2018KL\u2019 method described by Nick-\nisch and Rasmussen [2008]).\nIf the likelihood were Gaussian, the expectations in\nequation (21) would be computable in closed form, and\nafter a little re-arranging the result is as [Hensman\net al., 2013], equation (7). In this case the bound has\na unique solution for m and S, which recovers the\nvariational bound of Titsias [2009], equation (6).\nIn the case where Z=X and the likelihood is Gaussian,\nexact inference is recovered.\n5Experiments\nWe have proposed two variational approximations for\nGP classification. The first in section 3 comprises a\nmean-field approximation after making a variational\napproximation to the prior over an augmented latent\nvector. The second in section 4 proposes to minimize\nthe KL divergence using a Gaussian approximation at\na set of inducing points. We henceforth refer to these\nas the MF (mean-field) and KL methods respectively.\nIncreasing the number of inducing points\ncompare the methods with the state-of-the-art Gen-\neralized FITC method, we first turn to the two-\ndimensional Banana dataset. For all three methods,\nwe initialized the inducing points using k-means clus-\ntering. For the generalized FITC method we used the\nimplementation provided by Rasmussen and Nickisch\n[2010]. For all the methods we used the L-BFGS-B\noptimizer [Zhu et al., 1997].\nTo\nWith the expectation that increasing the number of\ninducing points should improve all three methods, we\napplied 4 to 64 inducing points, as shown in Figure\n1. The KL method pulls the inducing points positions\ntoward the decision boundary, and provides a near-\noptimal solution with 16 inducing points. The MF\nmethod is less able to adapt the inducing input posi-\ntions, but provides good solutions at the same number\nof inducing points.The Generalized FITC method\nappears to pull inducing points towards the decision\nboundary, but is unable to make good use of 64 induc-\ning points, moving some to the decision boundary and\nsome toward the origin.\nNumerical comparison\nmance of the classifiers for a number of commonly used\nclassification data sets. We took ten folds of the data\nand report the median hold out negative log probabil-\nity and 2-\u03c3 confidence intervals. For comparison we\nused the EP FITC implementation from the GPML\ntoolbox [Rasmussen and Nickisch, 2010] which is gen-\nerally considered to be amongst the best implementa-\ntions of this algorithm. For the KL and sparse KL\nmethods we found that the optimization behaviour\nwas improved by freezing the kernel hyper parameters\nat the beginning of optimization and then unfreezing\nthem once a reasonable set of variational parameters\nhas been attained. Table 1 shows the result of the\nexperiments. In the case where a classifier gives a ex-\ntremely confident wrong prediction for one or more\ntest points one can obtain a numerically high negative\nlog probability. These cases are denoted as \u2018large\u2019.\nWe compared the perfor-"},{"page":7,"text":"James Hensman, Alex Matthews, Zoubin Ghahramani\nTable 1: comparison of the performance of our proposed methods and Generalized FITC on benchmark datasets.\nEPFitc\nM=3.0%\nthyroid.11 \u00b1 .05\nheart.46 \u00b1 .14\ntwonorm.17 \u00b1 .43\nringnorm.21 \u00b1 .22\ngerman.51 \u00b1 .09\nwaveform.23 \u00b1 .09\ncancer.56 \u00b1 .09\nflare solar.59 \u00b1 .02\ndiabetes.48 \u00b1 .04\nDatasetKLEP\nEPFitc\nM=8\n.15 \u00b1 .04\n.42 \u00b1 .08\nLarge\n.34 \u00b1 .01\n.49 \u00b1 .04\n.22 \u00b1 .01\n.58 \u00b1 .06\n.57 \u00b1 .02\n.50 \u00b1 .02\nKLSp\nM=8\n.13 \u00b1 .06\n.42 \u00b1 .11\n.08 \u00b1 .01\n.41 \u00b1 .09\n.49 \u00b1 .05\n.23 \u00b1 .01\n.56 \u00b1 .07\n.59 \u00b1 .02\n.47 \u00b1 .02\nKLSp\nM=3.0%\n.09 \u00b1 .05\n.47 \u00b1 .18\n.09 \u00b1 .02\n.15 \u00b1 .03\n.51 \u00b1 .08\n.25 \u00b1 .04\n.58 \u00b1 .13\n.58 \u00b1 .02\n.51 \u00b1 .04\nMFSp\nM=8\n.22 \u00b1 .16\n.41 \u00b1 .15\nLarge\n.46 \u00b1 .00\n.52 \u00b1 .06\n.28 \u00b1 .01\n.58 \u00b1 .11\n.64 \u00b1 .05\nLarge\nMFSp\nM=3.0%\n.15 \u00b1 .14\n.43 \u00b1 .19\nLarge\nLarge\n.51 \u00b1 .09\nLarge\n.59 \u00b1 .11\n.60 \u00b1 .04\n.50 \u00b1 .04\n.11 \u00b1 .05\n.43 \u00b1 .11\n.08 \u00b1 .01\n.18 \u00b1 .02\n.48 \u00b1 .06\n.23 \u00b1 .02\n.55 \u00b1 .08\n.60 \u00b1 .02\n.48 \u00b1 .03\n.13 \u00b1 .04\n.44 \u00b1 .08\n.08 \u00b1 .01\n.20 \u00b1 .01\n.50 \u00b1 .04\n.22 \u00b1 .01\n.56 \u00b1 .07\n.57 \u00b1 .02\n.51 \u00b1 .02\nThe table shows that mean field based methods of-\nten give over confident predictions. The results show\nsimilar performance between the sparse KL and FITC\nmethods. The confidence intervals of the two methods\neither overlap or sparse KL is better. As we shall see,\nSparse KL runs faster in the timed experiments that\nfollow and can be run on very large datasets using\nstochastic gradient descent.\nTime-performance trade-off\nrithms used perform optimization there is a trade-off\nfor amount of time spent on optimization against the\nclassification performance achieved. The whole trade-\noff curve can be used to characterize the efficiency of\na given algorithm.\nSince all the algo-\nNaish-Guzman and Holden [2007] consider the image\ndataset amongst their benchmarks. Of the datasets\nthey consider it is one of the most demanding in terms\nof the number of inducing points that are required. We\nran timed experiments on this dataset recording the\nwall clock time after each function call and computing\nthe hold out negative log probability of the associated\nparameters at each step.\nWe profiled the MFSp and KLSp algorithms for a va-\nriety of inducing point numbers. Although GPML is\nimplemented in MATLAB rather than Python both\nhave access to fast numerical linear algebra libraries.\nOptimization for the sparse KL and FITC methods\nwas carried out using the LBFGS-B Zhu et al. [1997]\nalgorithm. The mean field optimization surface is chal-\nlenging numerically and we found that performance\nwas improved by using the scaled conjugate gradients\nalgorithm. We found no significant qualitative differ-\nence between the wall clock time and CPU times.\nFigure 2 shows the results. The efficient frontier of the\ncomparison is defined by the algorithm that for any\ngiven time achieves the lowest negative log probabil-\nity. In this case the efficient frontier is occupied by the\nsparse KL method. Each of the algorithms is showing\nbetter performance as the number of inducing points\nincreases. The challenging optimization behaviour of\nthe sparse mean-field algorithm can be seen by the un-\npredictable changes in hold out probability as the opti-\nmization proceeds. The supplementary material shows\nthat in terms of classification error rate this algorithm\nis much better behaved, suggesting that MFSp finds it\ndifficult to produce well calibrated predictions.\nThe supplementary material contains plots of the hold\nout classification accuracy for the image dataset. It\nalso contains similar plots for the banana dataset.\nStochastic optimization\nproposed KL method can be optimized effectively in\na stochastic fashion, we first turn to the MNIST data\nset. Selecting the odd digits and even digits to make\na binary problem, results in a training set with 60000\npoints. We used ADADELTA method [Zeiler, 2012]\nfrom the climin toolbox [Osendorfer et al., 2014]. Se-\nlecting a step-rate of 0.1, a mini-batch size of 10 with\n200 inducing points resulted in a hold-out accuracy of\n97.8%. The mean negative-log-probability of the test-\ning set was 0.069. It is encouraging that we are able to\nfit highly nonlinear, high-dimensional decision bound-\naries with good accuracy, on a dataset size that is out\nof the range of existing methods.\nTo demonstrate that the\nStochastic optimization of our bound allows fitting of\nGP classifiers to datasets that are larger than previ-\nously possible. We downloaded the flight arrival and\ndeparture times for every commercial flight in the USA\nfrom January 2008 to April 2008.1This dataset con-\ntains information about 5.9 million flights, including\nthe delay in reaching the destination. We build a clas-\nsifier which was to predict whether any flight was to\nsubject to delay.\nAs a benchmark, we first fitted a linear model using\n1Hensman et al. use this data to illustrate regression,\nhere we simply classify whether the delay \u2264 zero."},{"page":8,"text":"Scalable Variational Gaussian Process Classification\n100\n101\n102\n103\n104\n0.1\n0.2\n0.3\n0.4\n0.5\nTime (seconds)\nHoldout negative log probability\nKLSp M=4\nKLSp M=50\nKLSp M=200\nMFSp M=4\nMFSp M=50\nMFSp M=200\nEPFitc M=4\nEPFitc M=50\nEPFitc M=200\nFigure 2: Temporal performance of the different methods on the image dataset.\nscikits-learn [Pedregosa et al., 2011] which in turns\nuses LIBLINEAR [Fan et al., 2008]. On our randomly\nselected hold-out set of 100000 points, this achieved\na surprisingly low error rate of 37%, the negative-log\nprobability of the held-out data was 0.642.\nWe built a Gaussian process kernel using the sum of\na Matern-3\n2and a linear kernel. For each, we intro-\nduced parameters which allowed for the scaling of the\ninputs (sometimes called Automatic Relevance Deter-\nmination, ARD). Using a similar optimization scheme\nto the MNIST data above, our method was able to ex-\nceed the performance of a linear model in a few min-\nutes, as shown in Figure 3. The kernel parameters at\nconvergence suggested that the problem is highly non-\nlinear: the relative variance of the linear kernel was\nnegligible. The optimized lengthscales for the Matern\npart of the covariance suggested that the most useful\nfeatures were the time of day and time of year.\n6Discussion\nWe have presented two novel variational bounds for\nperforming sparse GP classification.\nthe existing GFITC method, makes an approximation\nto the covariance matrix before introducing the non-\nconjugate likelihood. These approaches are somewhat\nunsatisfactory since in performing approximate infer-\nence, we necessarily introduce additional parameters\n(variational means and variances, or the parameters\nof EP factors), which naturally scale linearly with N.\nThe first, like\nOur proposed KLSP bound outperforms the state-of-\nthe art GFITC method on benchmark datasets, and is\ncapable of being optimized in a stochastic fashion as\nwe have shown, making GP classification applicable to\nbig data for the first time.\n102\n103\n104\n105\n0.32\n0.34\n0.36\n0.38\n0.4\nerror (%)\n102\n103\nTime (seconds)\n104\n105\n0.6\n0.62\n0.64\n0.66\n-log p(y)\nFigure 3:\nThe red horizontal line depicts performance of a linear\nclassifier, whilst the blue line shows performance of the\nstochastically optimized KLsparse method.\nPerformance of the airline delay dataset.\nIn future work, we note that this work opens the door\nfor several other GP models: if the likelihood factorizes\nin N, then our method is applicable through Gauss-\nHermite quadrature of the log likelihood. We also note\nthat it is possible to relax the restriction of q(u) to\na Gaussian form, and mixture model approximations\nfollow straightforwardly, allowing scalable extensions\nof Nguyen and Bonilla [2014]."},{"page":9,"text":"James Hensman, Alex Matthews, Zoubin Ghahramani\nReferences\nC. Cortes and N. D. Lawrence, editors. Advances in\nNeural Information Processing Systems, Cambridge,\nMA, 2014. MIT Press.\nZ. Dai, A. Damianou, J. Hensman, and N. Lawrence.\nGaussian process models with parallelization and\ngpu acceleration. arXiv, 2014.\nA. Damianou and N. Lawrence. Deep gaussian pro-\ncesses. In Proceedings of the Sixteenth International\nConference on Artificial Intelligence and Statistics,\npages 207\u2013215, 2013.\nR.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang,\nand C.-J. Lin. Liblinear: A library for large lin-\near classification. The Journal of Machine Learning\nResearch, 9:1871\u20131874, 2008.\nY. Gal, M. van der Wilk, and C. E. Rasmussen. Dis-\ntributed variational inference in sparse Gaussian\nprocess regression and latent variable models. In\nCortes and Lawrence [2014].\nJ. Hensman, N. Fusi, and N. D. Lawrence.\nsian processes for big data. In A. Nicholson and\nP. Smyth, editors, Uncertainty in Artificial Intelli-\ngence, volume 29. AUAI Press, 2013.\nGaus-\nJ. Hensman, M. Zwie\u00dfele, and N. D. Lawrence. Tilted\nvariational bayes. In S. Kaski and J. Corander, ed-\nitors, Proceedings of the Seventeenth International\nConference on Artificial Intelligence and Statistics,\nvolume 33, pages 356\u2013364. JMLR W&CP, 2014.\nE. Khan, S. Mohamed, and K. P. Murphy.\nbayesian inference for non-conjugate gaussian pro-\ncess regression.In P. Bartlett, editor, Advances\nin Neural Information Processing Systems, pages\n3140\u20133148, Cambridge, MA, 2012. MIT Press.\nFast\nM. Kuss and C. E. Rasmussen. Assessing approximate\ninference for binary Gaussian process classification.\nThe Journal of Machine Learning Research, 6:1679\u2013\n1704, 2005.\nN. Lawrence, M. Seeger, and R. Herbrich. Fast sparse\nGaussian process methods: The informative vector\nmachine. In Proceedings of the 16th Annual Con-\nference on Neural Information Processing Systems,\nnumber EPFL-CONF-161319, pages 609\u2013616, 2003.\nN. D. Lawrence. Gaussian process latent variable mod-\nels for visualisation of high dimensional data. Ad-\nvances in neural information processing systems, 16:\n329\u2013336, 2004.\nA. Naish-Guzman and S. Holden. The generalized fitc\napproximation. In Advances in Neural Information\nProcessing Systems, pages 1057\u20131064, 2007.\nT. Nguyen and E. Bonilla. Automated variational in-\nference for Gaussian process models. In Cortes and\nLawrence [2014].\nH. Nickisch and C. E. Rasmussen. Approximations for\nbinary Gaussian process classification. Journal of\nMachine Learning Research, 9:2035\u20132078, 2008.\nM. Opper and C. Archambeau. The variational Gaus-\nsian approximation revisited. Neural computation,\n21(3):786\u2013792, 2009.\nC. Osendorfer, J. Bayer, S. Diot-Girard, T. Rueck-\nstiess, and S. Urban. Climin. http:\/\/github.com\/\nBRML\/climin, 2014. Accessed: 2014-10-19.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V. Dubourg, et al. Scikit-learn: Machine\nlearning in python. The Journal of Machine Learn-\ning Research, 12:2825\u20132830, 2011.\nJ. Qui\u02dc nonero-Candela and C. E. Rasmussen. A uni-\nfying view of sparse approximate Gaussian process\nregression. The Journal of Machine Learning Re-\nsearch, 6:1939\u20131959, 2005.\nC. E. Rasmussen and H. Nickisch. Gaussian processes\nfor machine learning (gpml) toolbox. The Journal\nof Machine Learning Research, 11:3011\u20133015, 2010.\nC. E. Rasmussen and C. K. I. Williams. Gaussian pro-\ncesses for machine learning. MIT Press, Cambridge,\nMA, 2006.\nE. Snelson and Z. Ghahramani. Sparse Gaussian pro-\ncesses using pseudo-inputs. In Advances in Neural\nInformation Processing Systems, pages 1257\u20131264,\n2005.\nM. Titsias. Variational learning of inducing variables\nin sparse Gaussian processes. In D. van Dyk and\nM. Welling, editors, Proceedings of the Twelfth In-\nternational Workshop on Artificial Intelligence and\nStatistics, volume 5, pages 567\u2013574, Clearwater\nBeach, FL, 16-18 April 2009. JMLR W&CP 5.\nV. Tolvanen. Gaussian processes with monotonicity\nconstraints for big data. Master\u2019s thesis, Aalto Uni-\nveristy, June 2014.\nA. G. Wilson, D. A. Knowles, and Z. Ghahramani.\nGaussian process regression networks. In J. Lang-\nford and J. Pineau, editors, Proceedings of the\n29th International Conference on Machine Learning\n(ICML), Edinburgh, June 2012. Omnipress.\nM. D. Zeiler. Adadelta: An adaptive learning rate\nmethod. arXiv preprint arXiv:1212.5701, 2012.\nC. Zhu, R. H. Byrd, P. Lu, and J. Nocedal. Algo-\nrithm 778: L-bfgs-b: Fortran subroutines for large-\nscale bound-constrained optimization. ACM Trans-\nactions on Mathematical Software (TOMS), 23(4):\n550\u2013560, 1997."},{"page":10,"text":"Scalable Variational Gaussian Process Classification\nSupplementary Material for :\nScalable Variational Gaussian Process\nClassification\nanonymous authors"},{"page":11,"text":"James Hensman, Alex Matthews, Zoubin Ghahramani\n10\u22122\n10\u22121\n100\n101\n102\n103\n104\n0\n5 \u00b7 10\u22122\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\nTime (seconds))\nHoldout negative log probability\nKLSp M=4\nKLSp M=50\nKLSp M=200\nMFSp M=4\nMFSp M=50\nMFSp M=200\nEPFitc M=4\nEPFitc M=50\nEPFitc M=200\nFigure S.1 : Hold out predictive densities of the different methods on the image dataset."},{"page":12,"text":"Scalable Variational Gaussian Process Classification\n10\u22122\n10\u22121\n100\n101\n102\n103\n104\n0\n2 \u00b7 10\u22122\n4 \u00b7 10\u22122\n6 \u00b7 10\u22122\n8 \u00b7 10\u22122\n0.1\n0.12\n0.14\n0.16\n0.18\n0.2\n0.22\n0.24\n0.26\n0.28\n0.3\n0.32\n0.34\n0.36\n0.38\n0.4\n0.42\n0.44\nTime (seconds)\nHoldout error rate\nKLSp M=4\nKLSp M=50\nKLSp M=200\nMFSp M=4\nMFSp M=50\nMFSp M=200\nEPFitc M=4\nEPFitc M=50\nEPFitc M=200\nFigure S.2 : Hold out errors of the different methods on the image dataset."},{"page":13,"text":"James Hensman, Alex Matthews, Zoubin Ghahramani\n10\u22122\n10\u22121\n100\n101\n102\n103\n0.22\n0.24\n0.26\n0.28\n0.3\n0.32\n0.34\n0.36\n0.38\n0.4\n0.42\n0.44\n0.46\n0.48\n0.5\n0.52\n0.54\n0.56\n0.58\n0.6\nTime (seconds)\nHoldout negative log probability\nKLSp M=4\nKLSp M=8\nKLSp M=16\nKLSp M=32\nEP\nKL\nMF\nMFSp M=4\nMFSp M=8\nMFSp M=16\nMFSp M=32\nEPFitc M=4\nEPFitc M=8\nEPFitc M=16\nEPFitc M=32\nFigure S.3 : Hold out predictive densities of the different methods on the banana dataset."},{"page":14,"text":"Scalable Variational Gaussian Process Classification\n10\u22121\n100\n101\nTime (seconds)\n102\n103\n0.24\n0.25\n0.25\n0.26\n0.26\n0.27\n0.27\n0.28\n0.28\n0.29\n0.29\n0.3\n0.3\n0.31\n0.31\nHoldout negative log probability\nKLSp M=4\nKLSp M=8\nKLSp M=16\nKLSp M=32\nEP\nKL\nMF\nMFSp M=4\nMFSp M=8\nMFSp M=16\nMFSp M=32\nEPFitc M=4\nEPFitc M=8\nEPFitc M=16\nEPFitc M=32\nFigure S.4 : Hold out predictive densities of the different methods on the banana dataset."},{"page":15,"text":"James Hensman, Alex Matthews, Zoubin Ghahramani\n10\u22123\n10\u22122\n10\u22121\n100\nTime (seconds)\n101\n102\n103\n104\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\nHoldout error rate\nKLSp M=4\nKLSp M=8\nKLSp M=16\nKLSp M=32\nEP\nKL\nMF\nMFSp M=4\nMFSp M=8\nMFSp M=16\nMFSp M=32\nEPFitc M=4\nEPFitc M=8\nEPFitc M=16\nEPFitc M=32\nFigure S.5 : Hold out errors of the different methods on the banana dataset."},{"page":16,"text":"Scalable Variational Gaussian Process Classification\n10\u22122\n10\u22121\n100\n101\n102\n103\n104\n0.10\n0.15\nTime (seconds)\nHoldout error rate\nKLSp M=4\nKLSp M=8\nKLSp M=16\nKLSp M=32\nEP\nKL\nMF\nMFSp M=4\nMFSp M=8\nMFSp M=16\nMFSp M=32\nEPFitc M=4\nEPFitc M=8\nEPFitc M=16\nEPFitc M=32\nFigure S.6 : Hold out errors of the different methods on the banana dataset."}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/James_Hensman\/publication\/268079368_Scalable_Variational_Gaussian_Process_Classification\/links\/55adfd9508aed9b7dcdb08a5.pdf","widgetId":"rgw28_56ab9fb5b3228"},"id":"rgw28_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=268079368&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw29_56ab9fb5b3228"},"id":"rgw29_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=268079368&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":268079368,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"55adfd9508aed9b7dcdb08a5","name":"James Hensman","date":"Jul 21, 2015 ","nameLink":"profile\/James_Hensman","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/James_Hensman\/publication\/268079368_Scalable_Variational_Gaussian_Process_Classification\/links\/55adfd9508aed9b7dcdb08a5.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/James_Hensman\/publication\/268079368_Scalable_Variational_Gaussian_Process_Classification\/links\/55adfd9508aed9b7dcdb08a5.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"1cf12f1c3a09d77375042bff1a46be1c","showFileSizeNote":false,"fileSize":"1.6 MB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"55adfd9508aed9b7dcdb08a5","name":"James Hensman","date":"Jul 21, 2015 ","nameLink":"profile\/James_Hensman","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/James_Hensman\/publication\/268079368_Scalable_Variational_Gaussian_Process_Classification\/links\/55adfd9508aed9b7dcdb08a5.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/James_Hensman\/publication\/268079368_Scalable_Variational_Gaussian_Process_Classification\/links\/55adfd9508aed9b7dcdb08a5.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"1cf12f1c3a09d77375042bff1a46be1c","showFileSizeNote":false,"fileSize":"1.6 MB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=_I0LxOpN06HHngt1yRoiuaSj0d3L_QPNiOvTO5JqlSdMnH3s6wS8xwh9gobD7IlZBCvtiwfcipIBQIkp9FtGTg.FaRnWrtw1nZ2xqeA9-b18xUlWMcQ4I_alS78L5maq3eqQaZPL2bCswSydcjg_VbuDePSOBFk8hqaNbsrKtJE0Q","clickOnPill":"publication.PublicationFigures.html?_sg=orzwSIU1llHU71IPMSs2o9dEl2fsge7sTmOrpcf79p6-OxMDaF3NaWxxf7bE-f-evPJgcSWhMOadm3_483HYzA.6PaCRfgQzvTNv4oD6WjoN4ltlYbjKYr7UmSUmj3gTVT0Cwpdz1gCo4H8LOk5h0zISzafxnW5p4yqsJPGXs_VZg"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FJames_Hensman%2Fpublication%2F268079368_Scalable_Variational_Gaussian_Process_Classification%2Flinks%2F55adfd9508aed9b7dcdb08a5.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=FxXlIhdYF1FsyH3KAd6_ioQxcDdwYb9cDC0T_jN4pw-r5Eq5vNU45PJmD8d9Q1j_kHljSBay1w5qCAVspU-Llg","urlHash":"3cdd3f0453257ea188a6c5db897ce1d3","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=OR99rgrXRq4b1dakXhD83D9MJ-FThdyXpAbf4PS97g02lgPS2EGdaeNDHTr2su0D_BcodV7qEN0e5NJzCGCK7e9nPL-yUgpGEeKG0Upmi8E.klb7tD7lzhzNdQR8CTv9PfOxVRHxjUsGUHINdF5Rzd1CK2IqW5nDhGvOQMxv0Aig4TvPhhT4scd--nuRV7Cm3w.TfYWEL31kQ7rucc9nfBm-LofhVuKbPtwNJHmzda3vQ1AWWQGmlxLAl8k3ij8eur_RZRPML5I86e7L29B3XY5qA","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"55adfd9508aed9b7dcdb08a5","trackedDownloads":{"55adfd9508aed9b7dcdb08a5":{"v":false,"d":false}},"assetId":"AS:253564949889025@1437466004037","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":268079368,"commentCursorPromo":null,"widgetId":"rgw31_56ab9fb5b3228"},"id":"rgw31_56ab9fb5b3228","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FJames_Hensman%2Fpublication%2F268079368_Scalable_Variational_Gaussian_Process_Classification%2Flinks%2F55adfd9508aed9b7dcdb08a5.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A253564949889025%401437466004037&publicationUid=268079368&linkId=55adfd9508aed9b7dcdb08a5&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Scalable Variational Gaussian Process Classification","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=MGeXdzdIsCRc7Gtr84mmBMx8jZwHaoM095EyCiA-9VG1xSzoBWRdRjDQEVyFENMi1wJaAkI6Ol6EcM2hPfvlUQ9HQR-cEpmLB20-et02Qz8.N7uxM2Cl_sS-J9iNLcMs-Rlbu9KVVXg2Phm2_Z4OaWL8gVhNfkTBqZlI9RTdmwz1fOQQt_5lBdBGx34g1AgkOg._XJ2A0IOasrVIhmxxCxGWfabs-65bWizHgRNjSr2QH2eWf8Mhvy3lKrN1cHdc5DNDZaYnzXCVoGhEzY4CwRFtw","publicationUid":268079368,"trackedDownloads":{"55adfd9508aed9b7dcdb08a5":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw33_56ab9fb5b3228"},"id":"rgw33_56ab9fb5b3228","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw34_56ab9fb5b3228"},"id":"rgw34_56ab9fb5b3228","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw35_56ab9fb5b3228"},"id":"rgw35_56ab9fb5b3228","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw36_56ab9fb5b3228"},"id":"rgw36_56ab9fb5b3228","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw37_56ab9fb5b3228"},"id":"rgw37_56ab9fb5b3228","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw32_56ab9fb5b3228"},"id":"rgw32_56ab9fb5b3228","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw30_56ab9fb5b3228"},"id":"rgw30_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/268079368_Scalable_Variational_Gaussian_Process_Classification","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab9fb5b3228"},"id":"rgw2_56ab9fb5b3228","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":268079368},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=268079368&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab9fb5b3228"},"id":"rgw1_56ab9fb5b3228","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"m4VihjdqcCeseZSdjdnVTzzk6EJRkkacVF\/MN5BWMxSNGsgRe8jrUR1zdLr93ZDzYIg5fZPiu\/mVQ0ryIXKqnPouMGEGDsorCo44q4xqoQG3yA8\/Obw7SdWGIj\/xuxwqOq+wm10zLTmaPESUz3r9NGEzrZWnBA8OechX8KeN2VCgmM2ltKLbg2rjnRk9kiXW28p53u+IrUA1k4AzOTQlBG\/+N\/7jBvttRfYf5hQR8pZFnPPLyr5KGZQ9Oxbv8ID8FcfJngSiT9VQMXBe8gYw5mghCQUC2cCPsJtnu+UJNKs=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/268079368_Scalable_Variational_Gaussian_Process_Classification\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Scalable Variational Gaussian Process Classification\" \/>\n<meta property=\"og:description\" content=\"Gaussian process classification is a popular method with a number of\nappealing properties. We show how to scale the model within a variational\ninducing point framework, outperforming the state of...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/268079368_Scalable_Variational_Gaussian_Process_Classification\/links\/55adfd9508aed9b7dcdb08a5\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/268079368_Scalable_Variational_Gaussian_Process_Classification\" \/>\n<meta property=\"rg:id\" content=\"PB:268079368\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Scalable Variational Gaussian Process Classification\" \/>\n<meta name=\"citation_author\" content=\"James Hensman\" \/>\n<meta name=\"citation_author\" content=\"Alex Matthews\" \/>\n<meta name=\"citation_author\" content=\"Zoubin Ghahramani\" \/>\n<meta name=\"citation_publication_date\" content=\"2014\/11\/07\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/James_Hensman\/publication\/268079368_Scalable_Variational_Gaussian_Process_Classification\/links\/55adfd9508aed9b7dcdb08a5.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/268079368_Scalable_Variational_Gaussian_Process_Classification\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/268079368_Scalable_Variational_Gaussian_Process_Classification\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-ecd3039d-514e-4ac4-aa9f-8cd2edeb1ae0","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":991,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw38_56ab9fb5b3228"},"id":"rgw38_56ab9fb5b3228","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-ecd3039d-514e-4ac4-aa9f-8cd2edeb1ae0", "4cb8d9cee01a7584bc6ed9d45fafd645d08339bf");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-ecd3039d-514e-4ac4-aa9f-8cd2edeb1ae0", "4cb8d9cee01a7584bc6ed9d45fafd645d08339bf");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw39_56ab9fb5b3228"},"id":"rgw39_56ab9fb5b3228","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/268079368_Scalable_Variational_Gaussian_Process_Classification","requestToken":"hAWLp2\/vIQxp6aD3wvN9LVXvU36fF5m100cijKrNLdyK0x4HJCTdddt7o8jh+gqABRWIcIE022chE7wfpP7gUZED4JUIhRzwQ0ZDRO\/gW2P0b8IepQxWcFDiaod59903oUNcStI8KqwDYZvBW\/d0CVHYCI1zGgCNAZo7xvRbkGKZEW5AhBIcUplMCPZbEcFfhI5tamRrbKiF6Sm1pRqntbrXw7k1DDuNdZqI1lMMIXGEHOEkIyyYFVveuwLrRZPI7Crsf68CaZmtEyQ0D5TSa\/j23q+9Y9ZtwHGT4lo2XbA=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=suQ8hY9MMRGsGnf2Ri_TuKbxzvpq3ye8DYvrYQVMUP6NUEA8ISUqGeeWcQO1SvSg","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjY4MDc5MzY4X1NjYWxhYmxlX1ZhcmlhdGlvbmFsX0dhdXNzaWFuX1Byb2Nlc3NfQ2xhc3NpZmljYXRpb24%3D","signupCallToAction":"Join for free","widgetId":"rgw41_56ab9fb5b3228"},"id":"rgw41_56ab9fb5b3228","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw40_56ab9fb5b3228"},"id":"rgw40_56ab9fb5b3228","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw42_56ab9fb5b3228"},"id":"rgw42_56ab9fb5b3228","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
