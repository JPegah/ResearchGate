<!DOCTYPE html> <html lang="en" class="" id="rgw34_56aba11b5d889"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="2ejAW7XCsG2LoQEBQuVqBmaRkUAyGx2SAotp0HlDPn188HTMHrY4DODs57GFQyaqjjqKCOiqueVP3qYXmPyj49IlvanUWhE2O+ArRJbsOTer+yVCZUTE/Mh5oTt++gBxioa1c0UJ5aJEe6DVOmwcsQ8td/wjYD3PvdtUBzUsPybvWQ2RfXAQhsMYWdlnTii5lCFnCWIsrkE/NQjszF3f24qUcpPqgBf0QvfprvbenYV6AYKFyYQe/mMhWOCa01XTJ+n1rnjWXrMyAaN2cfQbviODVD8Z5aLUfUutwLhRM1k="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-1329184d-8778-4a0e-ad25-c84d15abf628",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/259367763_Playing_Atari_with_Deep_Reinforcement_Learning" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Playing Atari with Deep Reinforcement Learning" />
<meta property="og:description" content="We present the first deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is a convolutional neural..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/259367763_Playing_Atari_with_Deep_Reinforcement_Learning/links/03578a2b0cf2db8d39ddb13d/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/259367763_Playing_Atari_with_Deep_Reinforcement_Learning" />
<meta property="rg:id" content="PB:259367763" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Playing Atari with Deep Reinforcement Learning" />
<meta name="citation_author" content="Volodymyr Mnih" />
<meta name="citation_author" content="Koray Kavukcuoglu" />
<meta name="citation_author" content="David Silver" />
<meta name="citation_author" content="Alex Graves" />
<meta name="citation_author" content="Ioannis Antonoglou" />
<meta name="citation_author" content="Daan Wierstra" />
<meta name="citation_author" content="Martin Riedmiller" />
<meta name="citation_publication_date" content="2013/12/19" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/259367763_Playing_Atari_with_Deep_Reinforcement_Learning" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/259367763_Playing_Atari_with_Deep_Reinforcement_Learning" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Playing Atari with Deep Reinforcement Learning</title>
<meta name="description" content="Playing Atari with Deep Reinforcement Learning on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56aba11b5d889" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol has-leaderboard" id="rgw2_56aba11b5d889" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div > <div class="clearfix">  <div class="publication-detail-dfp-leaderboard-container"> <div id="rgw33_56aba11b5d889">  </div> </div>  <div class="publication-header"> <div id="rgw5_56aba11b5d889">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Playing%20Atari%20with%20Deep%20Reinforcement%20Learning&rft.date=2013&rft.au=Volodymyr%20Mnih%2CKoray%20Kavukcuoglu%2CDavid%20Silver%2CAlex%20Graves%2CIoannis%20Antonoglou%2CDaan%20Wierstra%2CMartin%20Riedmiller&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Playing Atari with Deep Reinforcement Learning</h1> <meta itemprop="headline" content="Playing Atari with Deep Reinforcement Learning">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/259367763_Playing_Atari_with_Deep_Reinforcement_Learning/links/03578a2b0cf2db8d39ddb13d/smallpreview.png">  <div id="rgw7_56aba11b5d889" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56aba11b5d889"> <a href="researcher/70047966_Volodymyr_Mnih" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Volodymyr Mnih" alt="Volodymyr Mnih" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Volodymyr Mnih</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw9_56aba11b5d889">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/70047966_Volodymyr_Mnih"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Volodymyr Mnih" alt="Volodymyr Mnih" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/70047966_Volodymyr_Mnih" class="display-name">Volodymyr Mnih</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56aba11b5d889"> <a href="researcher/2040167286_Koray_Kavukcuoglu" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Koray Kavukcuoglu" alt="Koray Kavukcuoglu" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Koray Kavukcuoglu</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw11_56aba11b5d889">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2040167286_Koray_Kavukcuoglu"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Koray Kavukcuoglu" alt="Koray Kavukcuoglu" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2040167286_Koray_Kavukcuoglu" class="display-name">Koray Kavukcuoglu</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56aba11b5d889"> <a href="researcher/2089221891_David_Silver" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="David Silver" alt="David Silver" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">David Silver</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw13_56aba11b5d889">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2089221891_David_Silver"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="David Silver" alt="David Silver" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2089221891_David_Silver" class="display-name">David Silver</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw14_56aba11b5d889"> <a href="researcher/2040157445_Alex_Graves" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Alex Graves" alt="Alex Graves" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Alex Graves</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw15_56aba11b5d889">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2040157445_Alex_Graves"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Alex Graves" alt="Alex Graves" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2040157445_Alex_Graves" class="display-name">Alex Graves</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw16_56aba11b5d889" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Ioannis_Antonoglou" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Ioannis Antonoglou" alt="Ioannis Antonoglou" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Ioannis Antonoglou</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw17_56aba11b5d889" data-account-key="Ioannis_Antonoglou">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Ioannis_Antonoglou"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Ioannis Antonoglou" alt="Ioannis Antonoglou" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Ioannis_Antonoglou" class="display-name">Ioannis Antonoglou</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Google_Inc" title="Google Inc.">Google Inc.</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw18_56aba11b5d889"> <a href="researcher/12447169_Daan_Wierstra" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Daan Wierstra" alt="Daan Wierstra" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Daan Wierstra</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw19_56aba11b5d889">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/12447169_Daan_Wierstra"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Daan Wierstra" alt="Daan Wierstra" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/12447169_Daan_Wierstra" class="display-name">Daan Wierstra</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>     </ul> <div class="js-loading"></div>   <a class="js-show-more rf text-gray-lighter show-more">[more]</a>   </div> </div> <div class="pub-details js-pub-details">        <meta itemprop="datePublished" content="2013-12">  12/2013;               <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1312.5602" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw20_56aba11b5d889" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>We present the first deep learning model to successfully learn control<br />
policies directly from high-dimensional sensory input using reinforcement<br />
learning. The model is a convolutional neural network, trained with a variant<br />
of Q-learning, whose input is raw pixels and whose output is a value function<br />
estimating future rewards. We apply our method to seven Atari 2600 games from<br />
the Arcade Learning Environment, with no adjustment of the architecture or<br />
learning algorithm. We find that it outperforms all previous approaches on six<br />
of the games and surpasses a human expert on three of them.</div> </p>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw32_56aba11b5d889"  itemprop="articleBody">  <p>Page 1</p> <p>Playing Atari with Deep Reinforcement Learning<br />Volodymyr Mnih Koray KavukcuogluDavid SilverAlex Graves Ioannis Antonoglou<br />Daan WierstraMartin Riedmiller<br />DeepMind Technologies<br />{vlad,koray,david,alex.graves,ioannis,daan,martin.riedmiller} @ deepmind.com<br />Abstract<br />We present the first deep learning model to successfully learn control policies di-<br />rectly from high-dimensional sensory input using reinforcement learning. The<br />model is a convolutional neural network, trained with a variant of Q-learning,<br />whose input is raw pixels and whose output is a value function estimating future<br />rewards. We apply our method to seven Atari 2600 games from the Arcade Learn-<br />ing Environment, with no adjustment of the architecture or learning algorithm. We<br />find that it outperforms all previous approaches on six of the games and surpasses<br />a human expert on three of them.<br />1 Introduction<br />Learning to control agents directly from high-dimensional sensory inputs like vision and speech is<br />one of the long-standing challenges of reinforcement learning (RL). Most successful RL applica-<br />tions that operate on these domains have relied on hand-crafted features combined with linear value<br />functions or policy representations. Clearly, the performance of such systems heavily relies on the<br />quality of the feature representation.<br />Recent advances in deep learning have made it possible to extract high-level features from raw sen-<br />sory data, leading to breakthroughs in computer vision [11, 22, 16] and speech recognition [6, 7].<br />These methods utilise a range of neural network architectures, including convolutional networks,<br />multilayer perceptrons, restricted Boltzmann machines and recurrent neural networks, and have ex-<br />ploited both supervised and unsupervised learning. It seems natural to ask whether similar tech-<br />niques could also be beneficial for RL with sensory data.<br />However reinforcement learning presents several challenges from a deep learning perspective.<br />Firstly, most successful deep learning applications to date have required large amounts of hand-<br />labelled training data. RL algorithms, on the other hand, must be able to learn from a scalar reward<br />signal that is frequently sparse, noisy and delayed. The delay between actions and resulting rewards,<br />which can be thousands of timesteps long, seems particularly daunting when compared to the direct<br />association between inputs and targets found in supervised learning. Another issue is that most deep<br />learning algorithms assume the data samples to be independent, while in reinforcement learning one<br />typically encounters sequences of highly correlated states. Furthermore, in RL the data distribu-<br />tion changes as the algorithm learns new behaviours, which can be problematic for deep learning<br />methods that assume a fixed underlying distribution.<br />This paper demonstrates that a convolutional neural network can overcome these challenges to learn<br />successful control policies from raw video data in complex RL environments. The network is<br />trained with a variant of the Q-learning [26] algorithm, with stochastic gradient descent to update<br />the weights. To alleviate the problems of correlated data and non-stationary distributions, we use<br />1<br />arXiv:1312.5602v1  [cs.LG]  19 Dec 2013</p>  <p>Page 2</p> <p>Figure 1: Screen shots from five Atari 2600 Games: (Left-to-right) Pong, Breakout, Space Invaders,<br />Seaquest, Beam Rider<br />an experience replay mechanism [13] which randomly samples previous transitions, and thereby<br />smooths the training distribution over many past behaviors.<br />We apply our approach to a range of Atari 2600 games implemented in The Arcade Learning Envi-<br />ronment (ALE) [3]. Atari 2600 is a challenging RL testbed that presents agents with a high dimen-<br />sional visual input (210 × 160 RGB video at 60Hz) and a diverse and interesting set of tasks that<br />were designed to be difficult for humans players. Our goal is to create a single neural network agent<br />that is able to successfully learn to play as many of the games as possible. The network was not pro-<br />vided with any game-specific information or hand-designed visual features, and was not privy to the<br />internal state of the emulator; it learned from nothing but the video input, the reward and terminal<br />signals, and the set of possible actions—just as a human player would. Furthermore the network ar-<br />chitecture and all hyperparameters used for training were kept constant across the games. So far the<br />network has outperformed all previous RL algorithms on six of the seven games we have attempted<br />and surpassed an expert human player on three of them. Figure 1 provides sample screenshots from<br />five of the games used for training.<br />2 Background<br />We consider tasks in which an agent interacts with an environment E, in this case the Atari emulator,<br />in a sequence of actions, observations and rewards. At each time-step the agent selects an action<br />atfrom the set of legal game actions, A = {1,...,K}. The action is passed to the emulator and<br />modifies its internal state and the game score. In general E may be stochastic. The emulator’s<br />internal state is not observed by the agent; instead it observes an image xt∈ Rdfrom the emulator,<br />which is a vector of raw pixel values representing the current screen. In addition it receives a reward<br />rtrepresenting the change in game score. Note that in general the game score may depend on the<br />whole prior sequence of actions and observations; feedback about an action may only be received<br />after many thousands of time-steps have elapsed.<br />Since the agent only observes images of the current screen, the task is partially observed and many<br />emulator states are perceptually aliased, i.e. it is impossible to fully understand the current situation<br />from only the current screen xt. We therefore consider sequences of actions and observations, st=<br />x1,a1,x2,...,at−1,xt, and learn game strategies that depend upon these sequences. All sequences<br />in the emulator are assumed to terminate in a finite number of time-steps. This formalism gives<br />rise to a large but finite Markov decision process (MDP) in which each sequence is a distinct state.<br />As a result, we can apply standard reinforcement learning methods for MDPs, simply by using the<br />complete sequence stas the state representation at time t.<br />The goal of the agent is to interact with the emulator by selecting actions in a way that maximises<br />future rewards. We make the standard assumption that future rewards are discounted by a factor of<br />γ per time-step, and define the future discounted return at time t as Rt=?T<br />as the maximum expected return achievable by following any strategy, after seeing some sequence<br />s and then taking some action a, Q∗(s,a) = maxπE[Rt|st= s,at= a,π], where π is a policy<br />mapping sequences to actions (or distributions over actions).<br />The optimal action-value function obeys an important identity known as the Bellman equation. This<br />is based on the following intuition: if the optimal value Q∗(s?,a?) of the sequence s?at the next<br />time-step was known for all possible actions a?, then the optimal strategy is to select the action a?<br />t?=tγt?−trt?, where T<br />is the time-step at which the game terminates. We define the optimal action-value function Q∗(s,a)<br />2</p>  <p>Page 3</p> <p>maximising the expected value of r + γQ∗(s?,a?),<br />Q∗(s,a) = Es?∼E<br />?<br />r + γ max<br />a?<br />Q∗(s?,a?)<br />???s,a<br />?<br />(1)<br />The basic idea behind many reinforcement learning algorithms is to estimate the action-<br />value function, by using the Bellman equation as an iterative update, Qi+1(s,a)<br />E[r + γ maxa? Qi(s?,a?)|s,a]. Such value iteration algorithms converge to the optimal action-<br />value function, Qi → Q∗as i → ∞ [23]. In practice, this basic approach is totally impractical,<br />because the action-value function is estimated separately for each sequence, without any generali-<br />sation. Instead, it is common to use a function approximator to estimate the action-value function,<br />Q(s,a;θ) ≈ Q∗(s,a). In the reinforcement learning community this is typically a linear function<br />approximator, but sometimes a non-linear function approximator is used instead, such as a neural<br />network. We refer to a neural network function approximator with weights θ as a Q-network. A<br />Q-network can be trained by minimising a sequence of loss functions Li(θi) that changes at each<br />iteration i,<br />?<br />where yi = Es?∼E[r + γ maxa? Q(s?,a?;θi−1)|s,a] is the target for iteration i and ρ(s,a) is a<br />probability distribution over sequences s and actions a that we refer to as the behaviour distribution.<br />The parameters from the previous iteration θi−1are held fixed when optimising the loss function<br />Li(θi). Note that the targets depend on the network weights; this is in contrast with the targets used<br />for supervised learning, which are fixed before learning begins. Differentiating the loss function<br />with respect to the weights we arrive at the following gradient,<br />??<br />Rather than computing the full expectations in the above gradient, it is often computationally expe-<br />dient to optimise the loss function by stochastic gradient descent. If the weights are updated after<br />every time-step, and the expectations are replaced by single samples from the behaviour distribution<br />ρ and the emulator E respectively, then we arrive at the familiar Q-learning algorithm [26].<br />Note that this algorithm is model-free: it solves the reinforcement learning task directly using sam-<br />ples from the emulator E, without explicitly constructing an estimate of E. It is also off-policy: it<br />learns about the greedy strategy a = maxaQ(s,a;θ), while following a behaviour distribution that<br />ensures adequate exploration of the state space. In practice, the behaviour distribution is often se-<br />lected by an ?-greedy strategy that follows the greedy strategy with probability 1 − ? and selects a<br />random action with probability ?.<br />=<br />Li(θi) = Es,a∼ρ(·)<br />(yi− Q(s,a;θi))2?<br />,<br />(2)<br />∇θiLi(θi) = Es,a∼ρ(·);s?∼E<br />r + γ max<br />a?<br />Q(s?,a?;θi−1) − Q(s,a;θi)<br />?<br />∇θiQ(s,a;θi)<br />?<br />.<br />(3)<br />3 Related Work<br />Perhaps the best-known success story of reinforcement learning is TD-gammon, a backgammon-<br />playing program which learnt entirely by reinforcement learning and self-play, and achieved a super-<br />human level of play [24]. TD-gammon used a model-free reinforcement learning algorithm similar<br />to Q-learning, and approximated the value function using a multi-layer perceptron with one hidden<br />layer1.<br />However, early attempts to follow up on TD-gammon, including applications of the same method to<br />chess, Go and checkers were less successful. This led to a widespread belief that the TD-gammon<br />approach was a special case that only worked in backgammon, perhaps because the stochasticity in<br />the dice rolls helps explore the state space and also makes the value function particularly smooth<br />[19].<br />Furthermore, it was shown that combining model-free reinforcement learning algorithms such as Q-<br />learning with non-linear function approximators [25], or indeed with off-policy learning [1] could<br />cause the Q-network to diverge. Subsequently, the majority of work in reinforcement learning fo-<br />cused on linear function approximators with better convergence guarantees [25].<br />1In fact TD-Gammon approximated the state value function V (s) rather than the action-value function<br />Q(s,a), and learnt on-policy directly from the self-play games<br />3</p>  <p>Page 4</p> <p>More recently, there has been a revival of interest in combining deep learning with reinforcement<br />learning. Deep neural networks have been used to estimate the environment E; restricted Boltzmann<br />machines have been used to estimate the value function [21]; or the policy [9]. In addition, the<br />divergence issues with Q-learning have been partially addressed by gradient temporal-difference<br />methods. These methods are proven to converge when evaluating a fixed policy with a nonlinear<br />function approximator [14]; or when learning a control policy with linear function approximation<br />using a restricted variant of Q-learning [15]. However, these methods have not yet been extended to<br />nonlinear control.<br />Perhaps the most similar prior work to our own approach is neural fitted Q-learning (NFQ) [20].<br />NFQ optimises the sequence of loss functions in Equation 2, using the RPROP algorithm to update<br />the parameters of the Q-network. However, it uses a batch update that has a computational cost<br />per iteration that is proportional to the size of the data set, whereas we consider stochastic gradient<br />updates that have a low constant cost per iteration and scale to large data-sets. NFQ has also been<br />successfully applied to simple real-world control tasks using purely visual input, by first using deep<br />autoencoders to learn a low dimensional representation of the task, and then applying NFQ to this<br />representation [12]. In contrast our approach applies reinforcement learning end-to-end, directly<br />from the visual inputs; as a result it may learn features that are directly relevant to discriminating<br />action-values. Q-learning has also previously been combined with experience replay and a simple<br />neural network [13], but again starting with a low-dimensional state rather than raw visual inputs.<br />The use of the Atari 2600 emulator as a reinforcement learning platform was introduced by [3], who<br />applied standard reinforcement learning algorithms with linear function approximation and generic<br />visual features. Subsequently, results were improved by using a larger number of features, and<br />using tug-of-war hashing to randomly project the features into a lower-dimensional space [2]. The<br />HyperNEAT evolutionary architecture [8] has also been applied to the Atari platform, where it was<br />used to evolve (separately, for each distinct game) a neural network representing a strategy for that<br />game. When trained repeatedly against deterministic sequences using the emulator’s reset facility,<br />these strategies were able to exploit design flaws in several Atari games.<br />4 Deep Reinforcement Learning<br />Recent breakthroughs in computer vision and speech recognition have relied on efficiently training<br />deep neural networks on very large training sets. The most successful approaches are trained directly<br />from the raw inputs, using lightweight updates based on stochastic gradient descent. By feeding<br />sufficient data into deep neural networks, it is often possible to learn better representations than<br />handcrafted features [11]. These successes motivate our approach to reinforcement learning. Our<br />goal is to connect a reinforcement learning algorithm to a deep neural network which operates<br />directly on RGB images and efficiently process training data by using stochastic gradient updates.<br />Tesauro’s TD-Gammon architecture provides a starting point for such an approach. This architec-<br />ture updates the parameters of a network that estimates the value function, directly from on-policy<br />samples of experience, st,at,rt,st+1,at+1, drawn from the algorithm’s interactions with the envi-<br />ronment (or by self-play, in the case of backgammon). Since this approach was able to outperform<br />the best human backgammon players 20 years ago, it is natural to wonder whether two decades of<br />hardware improvements, coupled with modern deep neural network architectures and scalable RL<br />algorithms might produce significant progress.<br />In contrast to TD-Gammon and similar online approaches, we utilize a technique known as expe-<br />rience replay [13] where we store the agent’s experiences at each time-step, et= (st,at,rt,st+1)<br />in a data-set D = e1,...,eN, pooled over many episodes into a replay memory. During the inner<br />loop of the algorithm, we apply Q-learning updates, or minibatch updates, to samples of experience,<br />e ∼ D, drawn at random from the pool of stored samples. After performing experience replay,<br />the agent selects and executes an action according to an ?-greedy policy. Since using histories of<br />arbitrary length as inputs to a neural network can be difficult, our Q-function instead works on fixed<br />length representation of histories produced by a function φ. The full algorithm, which we call deep<br />Q-learning, is presented in Algorithm 1.<br />This approach has several advantages over standard online Q-learning [23]. First, each step of<br />experience is potentially used in many weight updates, which allows for greater data efficiency.<br />4</p>  <p>Page 5</p> <p>Algorithm 1 Deep Q-learning with Experience Replay<br />Initialize replay memory D to capacity N<br />Initialize action-value function Q with random weights<br />for episode = 1,M do<br />Initialise sequence s1= {x1} and preprocessed sequenced φ1= φ(s1)<br />for t = 1,T do<br />With probability ? select a random action at<br />otherwise select at= maxaQ∗(φ(st),a;θ)<br />Execute action atin emulator and observe reward rtand image xt+1<br />Set st+1= st,at,xt+1and preprocess φt+1= φ(st+1)<br />Store transition (φt,at,rt,φt+1) in D<br />Sample random minibatch of transitions (φj,aj,rj,φj+1) from D<br />Set yj=<br />rj+ γ maxa? Q(φj+1,a?;θ)<br />Perform a gradient descent step on (yj− Q(φj,aj;θ))2according to equation 3<br />end for<br />end for<br />?<br />rj<br />for terminal φj+1<br />for non-terminal φj+1<br />Second, learning directly from consecutive samples is inefficient, due to the strong correlations<br />between the samples; randomizing the samples breaks these correlations and therefore reduces the<br />variance of the updates. Third, when learning on-policy the current parameters determine the next<br />data sample that the parameters are trained on. For example, if the maximizing action is to move left<br />then the training samples will be dominated by samples from the left-hand side; if the maximizing<br />action then switches to the right then the training distribution will also switch. It is easy to see how<br />unwanted feedback loops may arise and the parameters could get stuck in a poor local minimum, or<br />even diverge catastrophically [25]. By using experience replay the behavior distribution is averaged<br />over many of its previous states, smoothing out learning and avoiding oscillations or divergence in<br />the parameters. Note that when learning by experience replay, it is necessary to learn off-policy<br />(because our current parameters are different to those used to generate the sample), which motivates<br />the choice of Q-learning.<br />In practice, our algorithm only stores the last N experience tuples in the replay memory, and samples<br />uniformly at random from D when performing updates. This approach is in some respects limited<br />since the memory buffer does not differentiate important transitions and always overwrites with<br />recent transitions due to the finite memory size N. Similarly, the uniform sampling gives equal<br />importance to all transitions in the replay memory. A more sophisticated sampling strategy might<br />emphasize transitions from which we can learn the most, similar to prioritized sweeping [17].<br />4.1Preprocessing and Model Architecture<br />Working directly with raw Atari frames, which are 210×160 pixel images with a 128 color palette,<br />can be computationally demanding, so we apply a basic preprocessing step aimed at reducing the<br />input dimensionality. The raw frames are preprocessed by first converting their RGB representation<br />to gray-scale and down-sampling it to a 110×84 image. The final input representation is obtained by<br />cropping an 84 × 84 region of the image that roughly captures the playing area. The final cropping<br />stage is only required because we use the GPU implementation of 2D convolutions from [11], which<br />expects square inputs. For the experiments in this paper, the function φ from algorithm 1 applies this<br />preprocessing to the last 4 frames of a history and stacks them to produce the input to the Q-function.<br />There are several possible ways of parameterizing Q using a neural network. Since Q maps history-<br />action pairs to scalar estimates of their Q-value, the history and the action have been used as inputs<br />to the neural network by some previous approaches [20, 12]. The main drawback of this type<br />of architecture is that a separate forward pass is required to compute the Q-value of each action,<br />resulting in a cost that scales linearly with the number of actions. We instead use an architecture<br />in which there is a separate output unit for each possible action, and only the state representation is<br />an input to the neural network. The outputs correspond to the predicted Q-values of the individual<br />action for the input state. The main advantage of this type of architecture is the ability to compute<br />Q-values for all possible actions in a given state with only a single forward pass through the network.<br />5</p>  <p>Page 6</p> <p>We now describe the exact architecture used for all seven Atari games. The input to the neural<br />network consists is an 84 × 84 × 4 image produced by φ. The first hidden layer convolves 16 8 × 8<br />filters with stride 4 with the input image and applies a rectifier nonlinearity [10, 18]. The second<br />hidden layer convolves 32 4 × 4 filters with stride 2, again followed by a rectifier nonlinearity. The<br />final hidden layer is fully-connected and consists of 256 rectifier units. The output layer is a fully-<br />connected linear layer with a single output for each valid action. The number of valid actions varied<br />between 4 and 18 on the games we considered. We refer to convolutional networks trained with our<br />approach as Deep Q-Networks (DQN).<br />5 Experiments<br />So far, we have performed experiments on seven popular ATARI games – Beam Rider, Breakout,<br />Enduro, Pong, Q*bert, Seaquest, Space Invaders. We use the same network architecture, learning<br />algorithm and hyperparameters settings across all seven games, showing that our approach is robust<br />enough to work on a variety of games without incorporating game-specific information. While we<br />evaluated our agents on the real and unmodified games, we made one change to the reward structure<br />of the games during training only. Since the scale of scores varies greatly from game to game, we<br />fixed all positive rewards to be 1 and all negative rewards to be −1, leaving 0 rewards unchanged.<br />Clipping the rewards in this manner limits the scale of the error derivatives and makes it easier to<br />use the same learning rate across multiple games. At the same time, it could affect the performance<br />of our agent since it cannot differentiate between rewards of different magnitude.<br />In these experiments, we used the RMSProp algorithm with minibatches of size 32. The behavior<br />policy during training was ?-greedy with ? annealed linearly from 1 to 0.1 over the first million<br />frames, and fixed at 0.1 thereafter. We trained for a total of 10 million frames and used a replay<br />memory of one million most recent frames.<br />Following previous approaches to playing Atari games, we also use a simple frame-skipping tech-<br />nique [3]. More precisely, the agent sees and selects actions on every kthframe instead of every<br />frame, and its last action is repeated on skipped frames. Since running the emulator forward for one<br />step requires much less computation than having the agent select an action, this technique allows<br />the agent to play roughly k times more games without significantly increasing the runtime. We use<br />k = 4 for all games except Space Invaders where we noticed that using k = 4 makes the lasers<br />invisible because of the period at which they blink. We used k = 3 to make the lasers visible and<br />this change was the only difference in hyperparameter values between any of the games.<br />5.1 Training and Stability<br />In supervised learning, one can easily track the performance of a model during training by evaluating<br />it on the training and validation sets. In reinforcement learning, however, accurately evaluating the<br />progress of an agent during training can be challenging. Since our evaluation metric, as suggested<br />by [3], is the total reward the agent collects in an episode or game averaged over a number of<br />games, we periodically compute it during training. The average total reward metric tends to be very<br />noisy because small changes to the weights of a policy can lead to large changes in the distribution of<br />states the policy visits . The leftmost two plots in figure 2 show how the average total reward evolves<br />during training on the games Seaquest and Breakout. Both averaged reward plots are indeed quite<br />noisy, giving one the impression that the learning algorithm is not making steady progress. Another,<br />more stable, metric is the policy’s estimated action-value function Q, which provides an estimate of<br />how much discounted reward the agent can obtain by following its policy from any given state. We<br />collect a fixed set of states by running a random policy before training starts and track the average<br />of the maximum2predicted Q for these states. The two rightmost plots in figure 2 show that average<br />predicted Q increases much more smoothly than the average total reward obtained by the agent and<br />plotting the same metrics on the other five games produces similarly smooth curves. In addition<br />to seeing relatively smooth improvement to predicted Q during training we did not experience any<br />divergence issues in any of our experiments. This suggests that, despite lacking any theoretical<br />convergence guarantees, our method is able to train large neural networks using a reinforcement<br />learning signal and stochastic gradient descent in a stable manner.<br />2The maximum for each state is taken over the possible actions.<br />6</p>  <p>Page 7</p> <p>0<br /> 50<br /> 100<br /> 150<br /> 200<br /> 250<br />Average Reward per Episode<br /> 0  10  20  30  40  50  60  70  80  90  100<br />Training Epochs<br />Average Reward on Breakout<br /> 0<br /> 200<br /> 400<br /> 600<br /> 800<br /> 1000<br /> 1200<br /> 1400<br /> 1600<br /> 1800<br />Average Reward per Episode<br /> 0  10  20  30  40  50  60  70  80  90  100<br />Training Epochs<br />Average Reward on Seaquest<br /> 0<br /> 0.5<br /> 1<br /> 1.5<br /> 2<br /> 2.5<br /> 3<br /> 3.5<br /> 4<br /> 0  10  20  30  40  50  60  70  80  90  100<br />Training Epochs<br />Average Action Value (Q)<br />Average Q on Breakout<br /> 0<br /> 1<br /> 2<br /> 3<br /> 4<br /> 5<br /> 6<br /> 7<br /> 8<br /> 9<br /> 0  10  20  30  40  50  60  70  80  90  100<br />Training Epochs<br />Average Action Value (Q)<br />Average Q on Seaquest<br />Figure 2:<br />respectively during training. The statistics were computed by running an ?-greedy policy with ? =<br />0.05 for 10000 steps. The two plots on the right show the average maximum predicted action-value<br />of a held out set of states on Breakout and Seaquest respectively. One epoch corresponds to 50000<br />minibatch weight updates or roughly 30 minutes of training time.<br />The two plots on the left show average reward per episode on Breakout and Seaquest<br />Figure 3: The leftmost plot shows the predicted value function for a 30 frame segment of the game<br />Seaquest. The three screenshots correspond to the frames labeled by A, B, and C respectively.<br />5.2 Visualizing the Value Function<br />Figure 3 shows a visualization of the learned value function on the game Seaquest. The figure shows<br />that the predicted value jumps after an enemy appears on the left of the screen (point A). The agent<br />then fires a torpedo at the enemy and the predicted value peaks as the torpedo is about to hit the<br />enemy (point B). Finally, the value falls to roughly its original value after the enemy disappears<br />(point C). Figure 3 demonstrates that our method is able to learn how the value function evolves for<br />a reasonably complex sequence of events.<br />5.3 Main Evaluation<br />We compare our results with the best performing methods from the RL literature [3, 4]. The method<br />labeled Sarsa used the Sarsa algorithm to learn linear policies on several different feature sets hand-<br />engineered for the Atari task and we report the score for the best performing feature set [3]. Con-<br />tingency used the same basic approach as Sarsa but augmented the feature sets with a learned<br />representation of the parts of the screen that are under the agent’s control [4]. Note that both of these<br />methods incorporate significant prior knowledge about the visual problem by using background sub-<br />traction and treating each of the 128 colors as a separate channel. Since many of the Atari games use<br />one distinct color for each type of object, treating each color as a separate channel can be similar to<br />producing a separate binary map encoding the presence of each object type. In contrast, our agents<br />only receive the raw RGB screenshots as input and must learn to detect objects on their own.<br />In addition to the learned agents, we also report scores for an expert human game player and a policy<br />that selects actions uniformly at random. The human performance is the median reward achieved<br />after around two hours of playing each game. Note that our reported human scores are much higher<br />than the ones in Bellemare et al. [3]. For the learned methods, we follow the evaluation strategy used<br />in Bellemare et al. [3, 5] and report the average score obtained by running an ?-greedy policy with<br />? = 0.05 for a fixed number of steps. The first five rows of table 1 show the per-game average scores<br />on all games. Our approach (labeled DQN) outperforms the other learning methods by a substantial<br />margin on all seven games despite incorporating almost no prior knowledge about the inputs.<br />We also include a comparison to the evolutionary policy search approach from [8] in the last three<br />rows of table 1. We report two sets of results for this method. The HNeat Best score reflects the<br />results obtained by using a hand-engineered object detector algorithm that outputs the locations and<br />7</p>  <p>Page 8</p> <p>B. Rider<br />354<br />996<br />1743<br />4092<br />7456<br />3616<br />1332<br />5184<br />Breakout<br />1.2<br />5.2<br />6<br />168<br />31<br />52<br />4<br />225<br />Enduro<br />0<br />129<br />159<br />470<br />368<br />106<br />91<br />661<br />Pong<br />−20.4<br />−19<br />−17<br />20<br />−3<br />19<br />−16<br />21<br />Q*bert<br />157<br />614<br />960<br />1952<br />18900<br />1800<br />1325<br />4500<br />Seaquest<br />110<br />665<br />723<br />1705<br />28010<br />920<br />800<br />1740<br />S. Invaders<br />179<br />271<br />268<br />581<br />3690<br />1720<br />1145<br />1075<br />Random<br />Sarsa [3]<br />Contingency [4]<br />DQN<br />Human<br />HNeat Best [8]<br />HNeat Pixel [8]<br />DQN Best<br />Table 1: The upper table compares average total reward for various learning methods by running<br />an ?-greedy policy with ? = 0.05 for a fixed number of steps. The lower table reports results of<br />the single best performing episode for HNeat and DQN. HNeat produces deterministic policies that<br />always get the same score while DQN used an ?-greedy policy with ? = 0.05.<br />types of objects on the Atari screen. The HNeat Pixel score is obtained by using the special 8 color<br />channel representation of the Atari emulator that represents an object label map at each channel.<br />This method relies heavily on finding a deterministic sequence of states that represents a successful<br />exploit. It is unlikely that strategies learnt in this way will generalize to random perturbations;<br />therefore the algorithm was only evaluated on the highest scoring single episode. In contrast, our<br />algorithm is evaluated on ?-greedy control sequences, and must therefore generalize across a wide<br />variety of possible situations. Nevertheless, we show that on all the games, except Space Invaders,<br />not only our max evaluation results (row 8), but also our average results (row 4) achieve better<br />performance.<br />Finally, we show that our method achieves better performance than an expert human player on<br />Breakout, Enduro and Pong and it achieves close to human performance on Beam Rider. The games<br />Q*bert, Seaquest, Space Invaders, on which we are far from human performance, are more chal-<br />lenging because they require the network to find a strategy that extends over long time scales.<br />6 Conclusion<br />This paper introduced a new deep learning model for reinforcement learning, and demonstrated its<br />ability to master difficult control policies for Atari 2600 computer games, using only raw pixels<br />as input. We also presented a variant of online Q-learning that combines stochastic minibatch up-<br />dates with experience replay memory to ease the training of deep networks for RL. Our approach<br />gave state-of-the-art results in six of the seven games it was tested on, with no adjustment of the<br />architecture or hyperparameters.<br />References<br />[1] Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In<br />Proceedings of the 12th International Conference on Machine Learning (ICML 1995), pages<br />30–37. Morgan Kaufmann, 1995.<br />[2] Marc Bellemare, Joel Veness, and Michael Bowling. Sketch-based linear value function ap-<br />proximation. In Advances in Neural Information Processing Systems 25, pages 2222–2230,<br />2012.<br />[3] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning<br />environment: An evaluation platform for general agents. Journal of Artificial Intelligence<br />Research, 47:253–279, 2013.<br />[4] Marc G Bellemare, Joel Veness, and Michael Bowling. Investigating contingency awareness<br />using atari 2600 games. In AAAI, 2012.<br />[5] Marc G. Bellemare, Joel Veness, and Michael Bowling. Bayesian learning of recursively fac-<br />tored environments. In Proceedings of the Thirtieth International Conference on Machine<br />Learning (ICML 2013), pages 1211–1219, 2013.<br />8</p>  <p>Page 9</p> <p>[6] George E. Dahl, Dong Yu, Li Deng, and Alex Acero. Context-dependent pre-trained deep<br />neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Pro-<br />cessing, IEEE Transactions on, 20(1):30 –42, January 2012.<br />[7] Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. Speech recognition with deep<br />recurrent neural networks. In Proc. ICASSP, 2013.<br />[8] Matthew Hausknecht, Risto Miikkulainen, and Peter Stone. A neuro-evolution approach to<br />general atari game playing. 2013.<br />[9] Nicolas Heess, David Silver, and Yee Whye Teh. Actor-critic reinforcement learning with<br />energy-based policies. In European Workshop on Reinforcement Learning, page 43, 2012.<br />[10] Kevin Jarrett, Koray Kavukcuoglu, MarcAurelio Ranzato, and Yann LeCun. What is the best<br />multi-stage architecture for object recognition? In Proc. International Conference on Com-<br />puter Vision and Pattern Recognition (CVPR 2009), pages 2146–2153. IEEE, 2009.<br />[11] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classification with deep con-<br />volutional neural networks. In Advances in Neural Information Processing Systems 25, pages<br />1106–1114, 2012.<br />[12] Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement<br />learning. In Neural Networks (IJCNN), The 2010 International Joint Conference on, pages<br />1–8. IEEE, 2010.<br />[13] Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report, DTIC<br />Document, 1993.<br />[14] Hamid Maei, Csaba Szepesvari, Shalabh Bhatnagar, Doina Precup, David Silver, and Rich<br />Sutton. Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approxi-<br />mation. In Advances in Neural Information Processing Systems 22, pages 1204–1212, 2009.<br />[15] Hamid Maei, Csaba Szepesv´ ari, Shalabh Bhatnagar, and Richard S. Sutton. Toward off-policy<br />learning control with function approximation. In Proceedings of the 27th International Con-<br />ference on Machine Learning (ICML 2010), pages 719–726, 2010.<br />[16] Volodymyr Mnih. Machine Learning for Aerial Image Labeling. PhD thesis, University of<br />Toronto, 2013.<br />[17] Andrew Moore and Chris Atkeson. Prioritized sweeping: Reinforcement learning with less<br />data and less real time. Machine Learning, 13:103–130, 1993.<br />[18] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann ma-<br />chines. In Proceedings of the 27th International Conference on Machine Learning (ICML<br />2010), pages 807–814, 2010.<br />[19] Jordan B. Pollack and Alan D. Blair. Why did td-gammon work. In Advances in Neural<br />Information Processing Systems 9, pages 10–16, 1996.<br />[20] Martin Riedmiller. Neural fitted q iteration–first experiences with a data efficient neural re-<br />inforcement learning method. In Machine Learning: ECML 2005, pages 317–328. Springer,<br />2005.<br />[21] Brian Sallans and Geoffrey E. Hinton. Reinforcement learning with factored states and actions.<br />Journal of Machine Learning Research, 5:1063–1088, 2004.<br />[22] Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, and Yann LeCun. Pedestrian de-<br />tection with unsupervised multi-stage feature learning. In Proc. International Conference on<br />Computer Vision and Pattern Recognition (CVPR 2013). IEEE, 2013.<br />[23] Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction. MIT Press,<br />1998.<br />[24] Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM,<br />38(3):58–68, 1995.<br />[25] John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with<br />function approximation. Automatic Control, IEEE Transactions on, 42(5):674–690, 1997.<br />[26] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292,<br />1992.<br />9</p>   </div> <div id="rgw25_56aba11b5d889" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw26_56aba11b5d889">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw27_56aba11b5d889"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://export.arxiv.org/pdf/1312.5602" target="_blank" rel="nofollow" class="publication-viewer" title="Playing Atari with Deep Reinforcement Learning">Playing Atari with Deep Reinforcement Learning</a> </div>  <div class="details">   Available from <a href="http://export.arxiv.org/pdf/1312.5602" target="_blank" rel="nofollow">export.arxiv.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw29_56aba11b5d889" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw30_56aba11b5d889">  </ul> </div> </div>   <div id="rgw21_56aba11b5d889" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw22_56aba11b5d889"> <div> <h5> <a href="publication/291389017_Deep_learning_from_speech_recognition_to_language_and_multimodal_processing" class="color-inherit ga-similar-publication-title"><span class="publication-title">Deep learning: from speech recognition to language and multimodal processing</span></a>  </h5>  <div class="authors"> <a href="researcher/2095255873_Li_Deng" class="authors ga-similar-publication-author">Li Deng</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw23_56aba11b5d889"> <div> <h5> <a href="publication/269997600_Automatic_Photo_Adjustment_Using_Deep_Neural_Networks" class="color-inherit ga-similar-publication-title"><span class="publication-title">Automatic Photo Adjustment Using Deep Neural Networks</span></a>  </h5>  <div class="authors"> <a href="researcher/2045940732_Zhicheng_Yan" class="authors ga-similar-publication-author">Zhicheng Yan</a>, <a href="researcher/2061443303_Hao_Zhang" class="authors ga-similar-publication-author">Hao Zhang</a>, <a href="researcher/69864821_Baoyuan_Wang" class="authors ga-similar-publication-author">Baoyuan Wang</a>, <a href="researcher/2061474644_Sylvain_Paris" class="authors ga-similar-publication-author">Sylvain Paris</a>, <a href="researcher/7708340_Yizhou_Yu" class="authors ga-similar-publication-author">Yizhou Yu</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw24_56aba11b5d889"> <div> <h5> <a href="publication/279313323_Transfer_learning_for_short-term_wind_speed_prediction_with_deep_neural_networks" class="color-inherit ga-similar-publication-title"><span class="publication-title">Transfer learning for short-term wind speed prediction with deep neural networks</span></a>  </h5>  <div class="authors"> <a href="researcher/10917384_Qinghua_Hu" class="authors ga-similar-publication-author">Qinghua Hu</a>, <a href="researcher/2076874501_Rujia_Zhang" class="authors ga-similar-publication-author">Rujia Zhang</a>, <a href="researcher/2046160574_Yucan_Zhou" class="authors ga-similar-publication-author">Yucan Zhou</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw35_56aba11b5d889" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw36_56aba11b5d889">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw37_56aba11b5d889" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=8S0GZtN_weP9g4cQhl08ptojpk5ObC65Z8XRRbfK-GIjnwGkK5F9iiP6bVi0SFyv" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="xmtSmWt4sKoc+eZrjFrIilmGpO6APVzt/CnS35bQUyw7268cgDUVbww6HYAKgqsj6Wne6j/YVzmxyq6UUz6b1xxNY1HOGfZtsKGvMp+1y7jl9X+6fqArT9Xw4f6XHWVUnGjGUsYbAg4GApJvjLfbXWzCi5NWj2h19CUp0L5iiWGMHYIyBVEI+QxgRIaCWwQfNVJGmMCYFW5OfDlGqSt8ZhSvds6etsTY/hQ3+kbAGeMQcZ+Fhzkeh3VwuyaPGWNryP06jjchqL6HIwZPlyFANXCoLbBtPEO0b8vVMNPJ50U="/> <input type="hidden" name="urlAfterLogin" value="publication/259367763_Playing_Atari_with_Deep_Reinforcement_Learning"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjU5MzY3NzYzX1BsYXlpbmdfQXRhcmlfd2l0aF9EZWVwX1JlaW5mb3JjZW1lbnRfTGVhcm5pbmc%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjU5MzY3NzYzX1BsYXlpbmdfQXRhcmlfd2l0aF9EZWVwX1JlaW5mb3JjZW1lbnRfTGVhcm5pbmc%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjU5MzY3NzYzX1BsYXlpbmdfQXRhcmlfd2l0aF9EZWVwX1JlaW5mb3JjZW1lbnRfTGVhcm5pbmc%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw38_56aba11b5d889"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 450;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Ioannis Antonoglou","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Ioannis_Antonoglou","institution":"Google Inc.","institutionUrl":false,"widgetId":"rgw4_56aba11b5d889"},"id":"rgw4_56aba11b5d889","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=7371418","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56aba11b5d889"},"id":"rgw3_56aba11b5d889","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=259367763","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":259367763,"title":"Playing Atari with Deep Reinforcement Learning","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"12\/2013;","publicationDateRobot":"2013-12","article":""}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1312.5602","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Playing Atari with Deep Reinforcement Learning"},{"key":"rft.date","value":"2013"},{"key":"rft.au","value":"Volodymyr Mnih,Koray Kavukcuoglu,David Silver,Alex Graves,Ioannis Antonoglou,Daan Wierstra,Martin Riedmiller"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56aba11b5d889"},"id":"rgw6_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=259367763","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":259367763,"peopleItems":[{"data":{"authorUrl":"researcher\/70047966_Volodymyr_Mnih","authorNameOnPublication":"Volodymyr Mnih","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Volodymyr Mnih","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/70047966_Volodymyr_Mnih","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw9_56aba11b5d889"},"id":"rgw9_56aba11b5d889","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=70047966&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw8_56aba11b5d889"},"id":"rgw8_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=70047966&authorNameOnPublication=Volodymyr%20Mnih","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/2040167286_Koray_Kavukcuoglu","authorNameOnPublication":"Koray Kavukcuoglu","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Koray Kavukcuoglu","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2040167286_Koray_Kavukcuoglu","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw11_56aba11b5d889"},"id":"rgw11_56aba11b5d889","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2040167286&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw10_56aba11b5d889"},"id":"rgw10_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2040167286&authorNameOnPublication=Koray%20Kavukcuoglu","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/2089221891_David_Silver","authorNameOnPublication":"David Silver","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"David Silver","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2089221891_David_Silver","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw13_56aba11b5d889"},"id":"rgw13_56aba11b5d889","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2089221891&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw12_56aba11b5d889"},"id":"rgw12_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2089221891&authorNameOnPublication=David%20Silver","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/2040157445_Alex_Graves","authorNameOnPublication":"Alex Graves","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Alex Graves","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2040157445_Alex_Graves","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw15_56aba11b5d889"},"id":"rgw15_56aba11b5d889","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2040157445&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw14_56aba11b5d889"},"id":"rgw14_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2040157445&authorNameOnPublication=Alex%20Graves","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Ioannis Antonoglou","accountUrl":"profile\/Ioannis_Antonoglou","accountKey":"Ioannis_Antonoglou","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Ioannis Antonoglou","profile":{"professionalInstitution":{"professionalInstitutionName":"Google Inc.","professionalInstitutionUrl":"institution\/Google_Inc"}},"professionalInstitutionName":"Google Inc.","professionalInstitutionUrl":"institution\/Google_Inc","url":"profile\/Ioannis_Antonoglou","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Ioannis_Antonoglou","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw17_56aba11b5d889"},"id":"rgw17_56aba11b5d889","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=7371418&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Google Inc.","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":6,"accountCount":1,"publicationUid":259367763,"widgetId":"rgw16_56aba11b5d889"},"id":"rgw16_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=7371418&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=6&accountCount=1&publicationUid=259367763","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/12447169_Daan_Wierstra","authorNameOnPublication":"Daan Wierstra","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Daan Wierstra","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/12447169_Daan_Wierstra","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw19_56aba11b5d889"},"id":"rgw19_56aba11b5d889","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=12447169&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw18_56aba11b5d889"},"id":"rgw18_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=12447169&authorNameOnPublication=Daan%20Wierstra","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":true,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56aba11b5d889"},"id":"rgw7_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=259367763&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":259367763,"abstract":"<noscript><\/noscript><div>We present the first deep learning model to successfully learn control<br \/>\npolicies directly from high-dimensional sensory input using reinforcement<br \/>\nlearning. The model is a convolutional neural network, trained with a variant<br \/>\nof Q-learning, whose input is raw pixels and whose output is a value function<br \/>\nestimating future rewards. We apply our method to seven Atari 2600 games from<br \/>\nthe Arcade Learning Environment, with no adjustment of the architecture or<br \/>\nlearning algorithm. We find that it outperforms all previous approaches on six<br \/>\nof the games and surpasses a human expert on three of them.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw20_56aba11b5d889"},"id":"rgw20_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=259367763","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/259367763_Playing_Atari_with_Deep_Reinforcement_Learning\/links\/03578a2b0cf2db8d39ddb13d\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw5_56aba11b5d889"},"id":"rgw5_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=259367763&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2095255873,"url":"researcher\/2095255873_Li_Deng","fullname":"Li Deng","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"APSIPA Transactions on Signal and Information Processing","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291389017_Deep_learning_from_speech_recognition_to_language_and_multimodal_processing","usePlainButton":true,"publicationUid":291389017,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/291389017_Deep_learning_from_speech_recognition_to_language_and_multimodal_processing","title":"Deep learning: from speech recognition to language and multimodal processing","displayTitleAsLink":true,"authors":[{"id":2095255873,"url":"researcher\/2095255873_Li_Deng","fullname":"Li Deng","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["APSIPA Transactions on Signal and Information Processing 01\/2016; 5. DOI:10.1017\/atsip.2015.22"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291389017_Deep_learning_from_speech_recognition_to_language_and_multimodal_processing","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291389017_Deep_learning_from_speech_recognition_to_language_and_multimodal_processing\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw22_56aba11b5d889"},"id":"rgw22_56aba11b5d889","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291389017","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2045940732,"url":"researcher\/2045940732_Zhicheng_Yan","fullname":"Zhicheng Yan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2061443303,"url":"researcher\/2061443303_Hao_Zhang","fullname":"Hao Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69864821,"url":"researcher\/69864821_Baoyuan_Wang","fullname":"Baoyuan Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2061474644,"url":"researcher\/2061474644_Sylvain_Paris","fullname":"Sylvain Paris","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2016","journal":"ACM Transactions on Graphics","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/269997600_Automatic_Photo_Adjustment_Using_Deep_Neural_Networks","usePlainButton":true,"publicationUid":269997600,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"4.10","url":"publication\/269997600_Automatic_Photo_Adjustment_Using_Deep_Neural_Networks","title":"Automatic Photo Adjustment Using Deep Neural Networks","displayTitleAsLink":true,"authors":[{"id":2045940732,"url":"researcher\/2045940732_Zhicheng_Yan","fullname":"Zhicheng Yan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2061443303,"url":"researcher\/2061443303_Hao_Zhang","fullname":"Hao Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69864821,"url":"researcher\/69864821_Baoyuan_Wang","fullname":"Baoyuan Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2061474644,"url":"researcher\/2061474644_Sylvain_Paris","fullname":"Sylvain Paris","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7708340,"url":"researcher\/7708340_Yizhou_Yu","fullname":"Yizhou Yu","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["ACM Transactions on Graphics 02\/2016; 35(1)."],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/269997600_Automatic_Photo_Adjustment_Using_Deep_Neural_Networks","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/269997600_Automatic_Photo_Adjustment_Using_Deep_Neural_Networks\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw23_56aba11b5d889"},"id":"rgw23_56aba11b5d889","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=269997600","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":10917384,"url":"researcher\/10917384_Qinghua_Hu","fullname":"Qinghua Hu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2076874501,"url":"researcher\/2076874501_Rujia_Zhang","fullname":"Rujia Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2046160574,"url":"researcher\/2046160574_Yucan_Zhou","fullname":"Yucan Zhou","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Renewable Energy","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/279313323_Transfer_learning_for_short-term_wind_speed_prediction_with_deep_neural_networks","usePlainButton":true,"publicationUid":279313323,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"3.48","url":"publication\/279313323_Transfer_learning_for_short-term_wind_speed_prediction_with_deep_neural_networks","title":"Transfer learning for short-term wind speed prediction with deep neural networks","displayTitleAsLink":true,"authors":[{"id":10917384,"url":"researcher\/10917384_Qinghua_Hu","fullname":"Qinghua Hu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2076874501,"url":"researcher\/2076874501_Rujia_Zhang","fullname":"Rujia Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2046160574,"url":"researcher\/2046160574_Yucan_Zhou","fullname":"Yucan Zhou","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Renewable Energy 01\/2016; 85. DOI:10.1016\/j.renene.2015.06.034"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/279313323_Transfer_learning_for_short-term_wind_speed_prediction_with_deep_neural_networks","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/279313323_Transfer_learning_for_short-term_wind_speed_prediction_with_deep_neural_networks\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw24_56aba11b5d889"},"id":"rgw24_56aba11b5d889","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=279313323","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw21_56aba11b5d889"},"id":"rgw21_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=259367763&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":259367763,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":259367763,"publicationType":"article","linkId":"03578a2b0cf2db8d39ddb13d","fileName":"Playing Atari with Deep Reinforcement Learning","fileUrl":"http:\/\/export.arxiv.org\/pdf\/1312.5602","name":"export.arxiv.org","nameUrl":"http:\/\/export.arxiv.org\/pdf\/1312.5602","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw27_56aba11b5d889"},"id":"rgw27_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=259367763&linkId=03578a2b0cf2db8d39ddb13d&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw26_56aba11b5d889"},"id":"rgw26_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=259367763&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":7,"valueFormatted":"7","widgetId":"rgw28_56aba11b5d889"},"id":"rgw28_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=259367763","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw25_56aba11b5d889"},"id":"rgw25_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=259367763&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":259367763,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw30_56aba11b5d889"},"id":"rgw30_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=259367763&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":7,"valueFormatted":"7","widgetId":"rgw31_56aba11b5d889"},"id":"rgw31_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=259367763","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw29_56aba11b5d889"},"id":"rgw29_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=259367763&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Playing Atari with Deep Reinforcement Learning\nVolodymyr Mnih Koray KavukcuogluDavid SilverAlex Graves Ioannis Antonoglou\nDaan WierstraMartin Riedmiller\nDeepMind Technologies\n{vlad,koray,david,alex.graves,ioannis,daan,martin.riedmiller} @ deepmind.com\nAbstract\nWe present the first deep learning model to successfully learn control policies di-\nrectly from high-dimensional sensory input using reinforcement learning. The\nmodel is a convolutional neural network, trained with a variant of Q-learning,\nwhose input is raw pixels and whose output is a value function estimating future\nrewards. We apply our method to seven Atari 2600 games from the Arcade Learn-\ning Environment, with no adjustment of the architecture or learning algorithm. We\nfind that it outperforms all previous approaches on six of the games and surpasses\na human expert on three of them.\n1 Introduction\nLearning to control agents directly from high-dimensional sensory inputs like vision and speech is\none of the long-standing challenges of reinforcement learning (RL). Most successful RL applica-\ntions that operate on these domains have relied on hand-crafted features combined with linear value\nfunctions or policy representations. Clearly, the performance of such systems heavily relies on the\nquality of the feature representation.\nRecent advances in deep learning have made it possible to extract high-level features from raw sen-\nsory data, leading to breakthroughs in computer vision [11, 22, 16] and speech recognition [6, 7].\nThese methods utilise a range of neural network architectures, including convolutional networks,\nmultilayer perceptrons, restricted Boltzmann machines and recurrent neural networks, and have ex-\nploited both supervised and unsupervised learning. It seems natural to ask whether similar tech-\nniques could also be beneficial for RL with sensory data.\nHowever reinforcement learning presents several challenges from a deep learning perspective.\nFirstly, most successful deep learning applications to date have required large amounts of hand-\nlabelled training data. RL algorithms, on the other hand, must be able to learn from a scalar reward\nsignal that is frequently sparse, noisy and delayed. The delay between actions and resulting rewards,\nwhich can be thousands of timesteps long, seems particularly daunting when compared to the direct\nassociation between inputs and targets found in supervised learning. Another issue is that most deep\nlearning algorithms assume the data samples to be independent, while in reinforcement learning one\ntypically encounters sequences of highly correlated states. Furthermore, in RL the data distribu-\ntion changes as the algorithm learns new behaviours, which can be problematic for deep learning\nmethods that assume a fixed underlying distribution.\nThis paper demonstrates that a convolutional neural network can overcome these challenges to learn\nsuccessful control policies from raw video data in complex RL environments. The network is\ntrained with a variant of the Q-learning [26] algorithm, with stochastic gradient descent to update\nthe weights. To alleviate the problems of correlated data and non-stationary distributions, we use\n1\narXiv:1312.5602v1  [cs.LG]  19 Dec 2013"},{"page":2,"text":"Figure 1: Screen shots from five Atari 2600 Games: (Left-to-right) Pong, Breakout, Space Invaders,\nSeaquest, Beam Rider\nan experience replay mechanism [13] which randomly samples previous transitions, and thereby\nsmooths the training distribution over many past behaviors.\nWe apply our approach to a range of Atari 2600 games implemented in The Arcade Learning Envi-\nronment (ALE) [3]. Atari 2600 is a challenging RL testbed that presents agents with a high dimen-\nsional visual input (210 \u00d7 160 RGB video at 60Hz) and a diverse and interesting set of tasks that\nwere designed to be difficult for humans players. Our goal is to create a single neural network agent\nthat is able to successfully learn to play as many of the games as possible. The network was not pro-\nvided with any game-specific information or hand-designed visual features, and was not privy to the\ninternal state of the emulator; it learned from nothing but the video input, the reward and terminal\nsignals, and the set of possible actions\u2014just as a human player would. Furthermore the network ar-\nchitecture and all hyperparameters used for training were kept constant across the games. So far the\nnetwork has outperformed all previous RL algorithms on six of the seven games we have attempted\nand surpassed an expert human player on three of them. Figure 1 provides sample screenshots from\nfive of the games used for training.\n2 Background\nWe consider tasks in which an agent interacts with an environment E, in this case the Atari emulator,\nin a sequence of actions, observations and rewards. At each time-step the agent selects an action\natfrom the set of legal game actions, A = {1,...,K}. The action is passed to the emulator and\nmodifies its internal state and the game score. In general E may be stochastic. The emulator\u2019s\ninternal state is not observed by the agent; instead it observes an image xt\u2208 Rdfrom the emulator,\nwhich is a vector of raw pixel values representing the current screen. In addition it receives a reward\nrtrepresenting the change in game score. Note that in general the game score may depend on the\nwhole prior sequence of actions and observations; feedback about an action may only be received\nafter many thousands of time-steps have elapsed.\nSince the agent only observes images of the current screen, the task is partially observed and many\nemulator states are perceptually aliased, i.e. it is impossible to fully understand the current situation\nfrom only the current screen xt. We therefore consider sequences of actions and observations, st=\nx1,a1,x2,...,at\u22121,xt, and learn game strategies that depend upon these sequences. All sequences\nin the emulator are assumed to terminate in a finite number of time-steps. This formalism gives\nrise to a large but finite Markov decision process (MDP) in which each sequence is a distinct state.\nAs a result, we can apply standard reinforcement learning methods for MDPs, simply by using the\ncomplete sequence stas the state representation at time t.\nThe goal of the agent is to interact with the emulator by selecting actions in a way that maximises\nfuture rewards. We make the standard assumption that future rewards are discounted by a factor of\n\u03b3 per time-step, and define the future discounted return at time t as Rt=?T\nas the maximum expected return achievable by following any strategy, after seeing some sequence\ns and then taking some action a, Q\u2217(s,a) = max\u03c0E[Rt|st= s,at= a,\u03c0], where \u03c0 is a policy\nmapping sequences to actions (or distributions over actions).\nThe optimal action-value function obeys an important identity known as the Bellman equation. This\nis based on the following intuition: if the optimal value Q\u2217(s?,a?) of the sequence s?at the next\ntime-step was known for all possible actions a?, then the optimal strategy is to select the action a?\nt?=t\u03b3t?\u2212trt?, where T\nis the time-step at which the game terminates. We define the optimal action-value function Q\u2217(s,a)\n2"},{"page":3,"text":"maximising the expected value of r + \u03b3Q\u2217(s?,a?),\nQ\u2217(s,a) = Es?\u223cE\n?\nr + \u03b3 max\na?\nQ\u2217(s?,a?)\n???s,a\n?\n(1)\nThe basic idea behind many reinforcement learning algorithms is to estimate the action-\nvalue function, by using the Bellman equation as an iterative update, Qi+1(s,a)\nE[r + \u03b3 maxa? Qi(s?,a?)|s,a]. Such value iteration algorithms converge to the optimal action-\nvalue function, Qi \u2192 Q\u2217as i \u2192 \u221e [23]. In practice, this basic approach is totally impractical,\nbecause the action-value function is estimated separately for each sequence, without any generali-\nsation. Instead, it is common to use a function approximator to estimate the action-value function,\nQ(s,a;\u03b8) \u2248 Q\u2217(s,a). In the reinforcement learning community this is typically a linear function\napproximator, but sometimes a non-linear function approximator is used instead, such as a neural\nnetwork. We refer to a neural network function approximator with weights \u03b8 as a Q-network. A\nQ-network can be trained by minimising a sequence of loss functions Li(\u03b8i) that changes at each\niteration i,\n?\nwhere yi = Es?\u223cE[r + \u03b3 maxa? Q(s?,a?;\u03b8i\u22121)|s,a] is the target for iteration i and \u03c1(s,a) is a\nprobability distribution over sequences s and actions a that we refer to as the behaviour distribution.\nThe parameters from the previous iteration \u03b8i\u22121are held fixed when optimising the loss function\nLi(\u03b8i). Note that the targets depend on the network weights; this is in contrast with the targets used\nfor supervised learning, which are fixed before learning begins. Differentiating the loss function\nwith respect to the weights we arrive at the following gradient,\n??\nRather than computing the full expectations in the above gradient, it is often computationally expe-\ndient to optimise the loss function by stochastic gradient descent. If the weights are updated after\nevery time-step, and the expectations are replaced by single samples from the behaviour distribution\n\u03c1 and the emulator E respectively, then we arrive at the familiar Q-learning algorithm [26].\nNote that this algorithm is model-free: it solves the reinforcement learning task directly using sam-\nples from the emulator E, without explicitly constructing an estimate of E. It is also off-policy: it\nlearns about the greedy strategy a = maxaQ(s,a;\u03b8), while following a behaviour distribution that\nensures adequate exploration of the state space. In practice, the behaviour distribution is often se-\nlected by an ?-greedy strategy that follows the greedy strategy with probability 1 \u2212 ? and selects a\nrandom action with probability ?.\n=\nLi(\u03b8i) = Es,a\u223c\u03c1(\u00b7)\n(yi\u2212 Q(s,a;\u03b8i))2?\n,\n(2)\n\u2207\u03b8iLi(\u03b8i) = Es,a\u223c\u03c1(\u00b7);s?\u223cE\nr + \u03b3 max\na?\nQ(s?,a?;\u03b8i\u22121) \u2212 Q(s,a;\u03b8i)\n?\n\u2207\u03b8iQ(s,a;\u03b8i)\n?\n.\n(3)\n3 Related Work\nPerhaps the best-known success story of reinforcement learning is TD-gammon, a backgammon-\nplaying program which learnt entirely by reinforcement learning and self-play, and achieved a super-\nhuman level of play [24]. TD-gammon used a model-free reinforcement learning algorithm similar\nto Q-learning, and approximated the value function using a multi-layer perceptron with one hidden\nlayer1.\nHowever, early attempts to follow up on TD-gammon, including applications of the same method to\nchess, Go and checkers were less successful. This led to a widespread belief that the TD-gammon\napproach was a special case that only worked in backgammon, perhaps because the stochasticity in\nthe dice rolls helps explore the state space and also makes the value function particularly smooth\n[19].\nFurthermore, it was shown that combining model-free reinforcement learning algorithms such as Q-\nlearning with non-linear function approximators [25], or indeed with off-policy learning [1] could\ncause the Q-network to diverge. Subsequently, the majority of work in reinforcement learning fo-\ncused on linear function approximators with better convergence guarantees [25].\n1In fact TD-Gammon approximated the state value function V (s) rather than the action-value function\nQ(s,a), and learnt on-policy directly from the self-play games\n3"},{"page":4,"text":"More recently, there has been a revival of interest in combining deep learning with reinforcement\nlearning. Deep neural networks have been used to estimate the environment E; restricted Boltzmann\nmachines have been used to estimate the value function [21]; or the policy [9]. In addition, the\ndivergence issues with Q-learning have been partially addressed by gradient temporal-difference\nmethods. These methods are proven to converge when evaluating a fixed policy with a nonlinear\nfunction approximator [14]; or when learning a control policy with linear function approximation\nusing a restricted variant of Q-learning [15]. However, these methods have not yet been extended to\nnonlinear control.\nPerhaps the most similar prior work to our own approach is neural fitted Q-learning (NFQ) [20].\nNFQ optimises the sequence of loss functions in Equation 2, using the RPROP algorithm to update\nthe parameters of the Q-network. However, it uses a batch update that has a computational cost\nper iteration that is proportional to the size of the data set, whereas we consider stochastic gradient\nupdates that have a low constant cost per iteration and scale to large data-sets. NFQ has also been\nsuccessfully applied to simple real-world control tasks using purely visual input, by first using deep\nautoencoders to learn a low dimensional representation of the task, and then applying NFQ to this\nrepresentation [12]. In contrast our approach applies reinforcement learning end-to-end, directly\nfrom the visual inputs; as a result it may learn features that are directly relevant to discriminating\naction-values. Q-learning has also previously been combined with experience replay and a simple\nneural network [13], but again starting with a low-dimensional state rather than raw visual inputs.\nThe use of the Atari 2600 emulator as a reinforcement learning platform was introduced by [3], who\napplied standard reinforcement learning algorithms with linear function approximation and generic\nvisual features. Subsequently, results were improved by using a larger number of features, and\nusing tug-of-war hashing to randomly project the features into a lower-dimensional space [2]. The\nHyperNEAT evolutionary architecture [8] has also been applied to the Atari platform, where it was\nused to evolve (separately, for each distinct game) a neural network representing a strategy for that\ngame. When trained repeatedly against deterministic sequences using the emulator\u2019s reset facility,\nthese strategies were able to exploit design flaws in several Atari games.\n4 Deep Reinforcement Learning\nRecent breakthroughs in computer vision and speech recognition have relied on efficiently training\ndeep neural networks on very large training sets. The most successful approaches are trained directly\nfrom the raw inputs, using lightweight updates based on stochastic gradient descent. By feeding\nsufficient data into deep neural networks, it is often possible to learn better representations than\nhandcrafted features [11]. These successes motivate our approach to reinforcement learning. Our\ngoal is to connect a reinforcement learning algorithm to a deep neural network which operates\ndirectly on RGB images and efficiently process training data by using stochastic gradient updates.\nTesauro\u2019s TD-Gammon architecture provides a starting point for such an approach. This architec-\nture updates the parameters of a network that estimates the value function, directly from on-policy\nsamples of experience, st,at,rt,st+1,at+1, drawn from the algorithm\u2019s interactions with the envi-\nronment (or by self-play, in the case of backgammon). Since this approach was able to outperform\nthe best human backgammon players 20 years ago, it is natural to wonder whether two decades of\nhardware improvements, coupled with modern deep neural network architectures and scalable RL\nalgorithms might produce significant progress.\nIn contrast to TD-Gammon and similar online approaches, we utilize a technique known as expe-\nrience replay [13] where we store the agent\u2019s experiences at each time-step, et= (st,at,rt,st+1)\nin a data-set D = e1,...,eN, pooled over many episodes into a replay memory. During the inner\nloop of the algorithm, we apply Q-learning updates, or minibatch updates, to samples of experience,\ne \u223c D, drawn at random from the pool of stored samples. After performing experience replay,\nthe agent selects and executes an action according to an ?-greedy policy. Since using histories of\narbitrary length as inputs to a neural network can be difficult, our Q-function instead works on fixed\nlength representation of histories produced by a function \u03c6. The full algorithm, which we call deep\nQ-learning, is presented in Algorithm 1.\nThis approach has several advantages over standard online Q-learning [23]. First, each step of\nexperience is potentially used in many weight updates, which allows for greater data efficiency.\n4"},{"page":5,"text":"Algorithm 1 Deep Q-learning with Experience Replay\nInitialize replay memory D to capacity N\nInitialize action-value function Q with random weights\nfor episode = 1,M do\nInitialise sequence s1= {x1} and preprocessed sequenced \u03c61= \u03c6(s1)\nfor t = 1,T do\nWith probability ? select a random action at\notherwise select at= maxaQ\u2217(\u03c6(st),a;\u03b8)\nExecute action atin emulator and observe reward rtand image xt+1\nSet st+1= st,at,xt+1and preprocess \u03c6t+1= \u03c6(st+1)\nStore transition (\u03c6t,at,rt,\u03c6t+1) in D\nSample random minibatch of transitions (\u03c6j,aj,rj,\u03c6j+1) from D\nSet yj=\nrj+ \u03b3 maxa? Q(\u03c6j+1,a?;\u03b8)\nPerform a gradient descent step on (yj\u2212 Q(\u03c6j,aj;\u03b8))2according to equation 3\nend for\nend for\n?\nrj\nfor terminal \u03c6j+1\nfor non-terminal \u03c6j+1\nSecond, learning directly from consecutive samples is inefficient, due to the strong correlations\nbetween the samples; randomizing the samples breaks these correlations and therefore reduces the\nvariance of the updates. Third, when learning on-policy the current parameters determine the next\ndata sample that the parameters are trained on. For example, if the maximizing action is to move left\nthen the training samples will be dominated by samples from the left-hand side; if the maximizing\naction then switches to the right then the training distribution will also switch. It is easy to see how\nunwanted feedback loops may arise and the parameters could get stuck in a poor local minimum, or\neven diverge catastrophically [25]. By using experience replay the behavior distribution is averaged\nover many of its previous states, smoothing out learning and avoiding oscillations or divergence in\nthe parameters. Note that when learning by experience replay, it is necessary to learn off-policy\n(because our current parameters are different to those used to generate the sample), which motivates\nthe choice of Q-learning.\nIn practice, our algorithm only stores the last N experience tuples in the replay memory, and samples\nuniformly at random from D when performing updates. This approach is in some respects limited\nsince the memory buffer does not differentiate important transitions and always overwrites with\nrecent transitions due to the finite memory size N. Similarly, the uniform sampling gives equal\nimportance to all transitions in the replay memory. A more sophisticated sampling strategy might\nemphasize transitions from which we can learn the most, similar to prioritized sweeping [17].\n4.1Preprocessing and Model Architecture\nWorking directly with raw Atari frames, which are 210\u00d7160 pixel images with a 128 color palette,\ncan be computationally demanding, so we apply a basic preprocessing step aimed at reducing the\ninput dimensionality. The raw frames are preprocessed by first converting their RGB representation\nto gray-scale and down-sampling it to a 110\u00d784 image. The final input representation is obtained by\ncropping an 84 \u00d7 84 region of the image that roughly captures the playing area. The final cropping\nstage is only required because we use the GPU implementation of 2D convolutions from [11], which\nexpects square inputs. For the experiments in this paper, the function \u03c6 from algorithm 1 applies this\npreprocessing to the last 4 frames of a history and stacks them to produce the input to the Q-function.\nThere are several possible ways of parameterizing Q using a neural network. Since Q maps history-\naction pairs to scalar estimates of their Q-value, the history and the action have been used as inputs\nto the neural network by some previous approaches [20, 12]. The main drawback of this type\nof architecture is that a separate forward pass is required to compute the Q-value of each action,\nresulting in a cost that scales linearly with the number of actions. We instead use an architecture\nin which there is a separate output unit for each possible action, and only the state representation is\nan input to the neural network. The outputs correspond to the predicted Q-values of the individual\naction for the input state. The main advantage of this type of architecture is the ability to compute\nQ-values for all possible actions in a given state with only a single forward pass through the network.\n5"},{"page":6,"text":"We now describe the exact architecture used for all seven Atari games. The input to the neural\nnetwork consists is an 84 \u00d7 84 \u00d7 4 image produced by \u03c6. The first hidden layer convolves 16 8 \u00d7 8\nfilters with stride 4 with the input image and applies a rectifier nonlinearity [10, 18]. The second\nhidden layer convolves 32 4 \u00d7 4 filters with stride 2, again followed by a rectifier nonlinearity. The\nfinal hidden layer is fully-connected and consists of 256 rectifier units. The output layer is a fully-\nconnected linear layer with a single output for each valid action. The number of valid actions varied\nbetween 4 and 18 on the games we considered. We refer to convolutional networks trained with our\napproach as Deep Q-Networks (DQN).\n5 Experiments\nSo far, we have performed experiments on seven popular ATARI games \u2013 Beam Rider, Breakout,\nEnduro, Pong, Q*bert, Seaquest, Space Invaders. We use the same network architecture, learning\nalgorithm and hyperparameters settings across all seven games, showing that our approach is robust\nenough to work on a variety of games without incorporating game-specific information. While we\nevaluated our agents on the real and unmodified games, we made one change to the reward structure\nof the games during training only. Since the scale of scores varies greatly from game to game, we\nfixed all positive rewards to be 1 and all negative rewards to be \u22121, leaving 0 rewards unchanged.\nClipping the rewards in this manner limits the scale of the error derivatives and makes it easier to\nuse the same learning rate across multiple games. At the same time, it could affect the performance\nof our agent since it cannot differentiate between rewards of different magnitude.\nIn these experiments, we used the RMSProp algorithm with minibatches of size 32. The behavior\npolicy during training was ?-greedy with ? annealed linearly from 1 to 0.1 over the first million\nframes, and fixed at 0.1 thereafter. We trained for a total of 10 million frames and used a replay\nmemory of one million most recent frames.\nFollowing previous approaches to playing Atari games, we also use a simple frame-skipping tech-\nnique [3]. More precisely, the agent sees and selects actions on every kthframe instead of every\nframe, and its last action is repeated on skipped frames. Since running the emulator forward for one\nstep requires much less computation than having the agent select an action, this technique allows\nthe agent to play roughly k times more games without significantly increasing the runtime. We use\nk = 4 for all games except Space Invaders where we noticed that using k = 4 makes the lasers\ninvisible because of the period at which they blink. We used k = 3 to make the lasers visible and\nthis change was the only difference in hyperparameter values between any of the games.\n5.1 Training and Stability\nIn supervised learning, one can easily track the performance of a model during training by evaluating\nit on the training and validation sets. In reinforcement learning, however, accurately evaluating the\nprogress of an agent during training can be challenging. Since our evaluation metric, as suggested\nby [3], is the total reward the agent collects in an episode or game averaged over a number of\ngames, we periodically compute it during training. The average total reward metric tends to be very\nnoisy because small changes to the weights of a policy can lead to large changes in the distribution of\nstates the policy visits . The leftmost two plots in figure 2 show how the average total reward evolves\nduring training on the games Seaquest and Breakout. Both averaged reward plots are indeed quite\nnoisy, giving one the impression that the learning algorithm is not making steady progress. Another,\nmore stable, metric is the policy\u2019s estimated action-value function Q, which provides an estimate of\nhow much discounted reward the agent can obtain by following its policy from any given state. We\ncollect a fixed set of states by running a random policy before training starts and track the average\nof the maximum2predicted Q for these states. The two rightmost plots in figure 2 show that average\npredicted Q increases much more smoothly than the average total reward obtained by the agent and\nplotting the same metrics on the other five games produces similarly smooth curves. In addition\nto seeing relatively smooth improvement to predicted Q during training we did not experience any\ndivergence issues in any of our experiments. This suggests that, despite lacking any theoretical\nconvergence guarantees, our method is able to train large neural networks using a reinforcement\nlearning signal and stochastic gradient descent in a stable manner.\n2The maximum for each state is taken over the possible actions.\n6"},{"page":7,"text":"0\n 50\n 100\n 150\n 200\n 250\nAverage Reward per Episode\n 0  10  20  30  40  50  60  70  80  90  100\nTraining Epochs\nAverage Reward on Breakout\n 0\n 200\n 400\n 600\n 800\n 1000\n 1200\n 1400\n 1600\n 1800\nAverage Reward per Episode\n 0  10  20  30  40  50  60  70  80  90  100\nTraining Epochs\nAverage Reward on Seaquest\n 0\n 0.5\n 1\n 1.5\n 2\n 2.5\n 3\n 3.5\n 4\n 0  10  20  30  40  50  60  70  80  90  100\nTraining Epochs\nAverage Action Value (Q)\nAverage Q on Breakout\n 0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n 0  10  20  30  40  50  60  70  80  90  100\nTraining Epochs\nAverage Action Value (Q)\nAverage Q on Seaquest\nFigure 2:\nrespectively during training. The statistics were computed by running an ?-greedy policy with ? =\n0.05 for 10000 steps. The two plots on the right show the average maximum predicted action-value\nof a held out set of states on Breakout and Seaquest respectively. One epoch corresponds to 50000\nminibatch weight updates or roughly 30 minutes of training time.\nThe two plots on the left show average reward per episode on Breakout and Seaquest\nFigure 3: The leftmost plot shows the predicted value function for a 30 frame segment of the game\nSeaquest. The three screenshots correspond to the frames labeled by A, B, and C respectively.\n5.2 Visualizing the Value Function\nFigure 3 shows a visualization of the learned value function on the game Seaquest. The figure shows\nthat the predicted value jumps after an enemy appears on the left of the screen (point A). The agent\nthen fires a torpedo at the enemy and the predicted value peaks as the torpedo is about to hit the\nenemy (point B). Finally, the value falls to roughly its original value after the enemy disappears\n(point C). Figure 3 demonstrates that our method is able to learn how the value function evolves for\na reasonably complex sequence of events.\n5.3 Main Evaluation\nWe compare our results with the best performing methods from the RL literature [3, 4]. The method\nlabeled Sarsa used the Sarsa algorithm to learn linear policies on several different feature sets hand-\nengineered for the Atari task and we report the score for the best performing feature set [3]. Con-\ntingency used the same basic approach as Sarsa but augmented the feature sets with a learned\nrepresentation of the parts of the screen that are under the agent\u2019s control [4]. Note that both of these\nmethods incorporate significant prior knowledge about the visual problem by using background sub-\ntraction and treating each of the 128 colors as a separate channel. Since many of the Atari games use\none distinct color for each type of object, treating each color as a separate channel can be similar to\nproducing a separate binary map encoding the presence of each object type. In contrast, our agents\nonly receive the raw RGB screenshots as input and must learn to detect objects on their own.\nIn addition to the learned agents, we also report scores for an expert human game player and a policy\nthat selects actions uniformly at random. The human performance is the median reward achieved\nafter around two hours of playing each game. Note that our reported human scores are much higher\nthan the ones in Bellemare et al. [3]. For the learned methods, we follow the evaluation strategy used\nin Bellemare et al. [3, 5] and report the average score obtained by running an ?-greedy policy with\n? = 0.05 for a fixed number of steps. The first five rows of table 1 show the per-game average scores\non all games. Our approach (labeled DQN) outperforms the other learning methods by a substantial\nmargin on all seven games despite incorporating almost no prior knowledge about the inputs.\nWe also include a comparison to the evolutionary policy search approach from [8] in the last three\nrows of table 1. We report two sets of results for this method. The HNeat Best score reflects the\nresults obtained by using a hand-engineered object detector algorithm that outputs the locations and\n7"},{"page":8,"text":"B. Rider\n354\n996\n1743\n4092\n7456\n3616\n1332\n5184\nBreakout\n1.2\n5.2\n6\n168\n31\n52\n4\n225\nEnduro\n0\n129\n159\n470\n368\n106\n91\n661\nPong\n\u221220.4\n\u221219\n\u221217\n20\n\u22123\n19\n\u221216\n21\nQ*bert\n157\n614\n960\n1952\n18900\n1800\n1325\n4500\nSeaquest\n110\n665\n723\n1705\n28010\n920\n800\n1740\nS. Invaders\n179\n271\n268\n581\n3690\n1720\n1145\n1075\nRandom\nSarsa [3]\nContingency [4]\nDQN\nHuman\nHNeat Best [8]\nHNeat Pixel [8]\nDQN Best\nTable 1: The upper table compares average total reward for various learning methods by running\nan ?-greedy policy with ? = 0.05 for a fixed number of steps. The lower table reports results of\nthe single best performing episode for HNeat and DQN. HNeat produces deterministic policies that\nalways get the same score while DQN used an ?-greedy policy with ? = 0.05.\ntypes of objects on the Atari screen. The HNeat Pixel score is obtained by using the special 8 color\nchannel representation of the Atari emulator that represents an object label map at each channel.\nThis method relies heavily on finding a deterministic sequence of states that represents a successful\nexploit. It is unlikely that strategies learnt in this way will generalize to random perturbations;\ntherefore the algorithm was only evaluated on the highest scoring single episode. In contrast, our\nalgorithm is evaluated on ?-greedy control sequences, and must therefore generalize across a wide\nvariety of possible situations. Nevertheless, we show that on all the games, except Space Invaders,\nnot only our max evaluation results (row 8), but also our average results (row 4) achieve better\nperformance.\nFinally, we show that our method achieves better performance than an expert human player on\nBreakout, Enduro and Pong and it achieves close to human performance on Beam Rider. The games\nQ*bert, Seaquest, Space Invaders, on which we are far from human performance, are more chal-\nlenging because they require the network to find a strategy that extends over long time scales.\n6 Conclusion\nThis paper introduced a new deep learning model for reinforcement learning, and demonstrated its\nability to master difficult control policies for Atari 2600 computer games, using only raw pixels\nas input. We also presented a variant of online Q-learning that combines stochastic minibatch up-\ndates with experience replay memory to ease the training of deep networks for RL. Our approach\ngave state-of-the-art results in six of the seven games it was tested on, with no adjustment of the\narchitecture or hyperparameters.\nReferences\n[1] Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In\nProceedings of the 12th International Conference on Machine Learning (ICML 1995), pages\n30\u201337. Morgan Kaufmann, 1995.\n[2] Marc Bellemare, Joel Veness, and Michael Bowling. Sketch-based linear value function ap-\nproximation. In Advances in Neural Information Processing Systems 25, pages 2222\u20132230,\n2012.\n[3] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning\nenvironment: An evaluation platform for general agents. Journal of Artificial Intelligence\nResearch, 47:253\u2013279, 2013.\n[4] Marc G Bellemare, Joel Veness, and Michael Bowling. Investigating contingency awareness\nusing atari 2600 games. In AAAI, 2012.\n[5] Marc G. Bellemare, Joel Veness, and Michael Bowling. Bayesian learning of recursively fac-\ntored environments. In Proceedings of the Thirtieth International Conference on Machine\nLearning (ICML 2013), pages 1211\u20131219, 2013.\n8"},{"page":9,"text":"[6] George E. Dahl, Dong Yu, Li Deng, and Alex Acero. Context-dependent pre-trained deep\nneural networks for large-vocabulary speech recognition. Audio, Speech, and Language Pro-\ncessing, IEEE Transactions on, 20(1):30 \u201342, January 2012.\n[7] Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. Speech recognition with deep\nrecurrent neural networks. In Proc. ICASSP, 2013.\n[8] Matthew Hausknecht, Risto Miikkulainen, and Peter Stone. A neuro-evolution approach to\ngeneral atari game playing. 2013.\n[9] Nicolas Heess, David Silver, and Yee Whye Teh. Actor-critic reinforcement learning with\nenergy-based policies. In European Workshop on Reinforcement Learning, page 43, 2012.\n[10] Kevin Jarrett, Koray Kavukcuoglu, MarcAurelio Ranzato, and Yann LeCun. What is the best\nmulti-stage architecture for object recognition? In Proc. International Conference on Com-\nputer Vision and Pattern Recognition (CVPR 2009), pages 2146\u20132153. IEEE, 2009.\n[11] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classification with deep con-\nvolutional neural networks. In Advances in Neural Information Processing Systems 25, pages\n1106\u20131114, 2012.\n[12] Sascha Lange and Martin Riedmiller. Deep auto-encoder neural networks in reinforcement\nlearning. In Neural Networks (IJCNN), The 2010 International Joint Conference on, pages\n1\u20138. IEEE, 2010.\n[13] Long-Ji Lin. Reinforcement learning for robots using neural networks. Technical report, DTIC\nDocument, 1993.\n[14] Hamid Maei, Csaba Szepesvari, Shalabh Bhatnagar, Doina Precup, David Silver, and Rich\nSutton. Convergent Temporal-Difference Learning with Arbitrary Smooth Function Approxi-\nmation. In Advances in Neural Information Processing Systems 22, pages 1204\u20131212, 2009.\n[15] Hamid Maei, Csaba Szepesv\u00b4 ari, Shalabh Bhatnagar, and Richard S. Sutton. Toward off-policy\nlearning control with function approximation. In Proceedings of the 27th International Con-\nference on Machine Learning (ICML 2010), pages 719\u2013726, 2010.\n[16] Volodymyr Mnih. Machine Learning for Aerial Image Labeling. PhD thesis, University of\nToronto, 2013.\n[17] Andrew Moore and Chris Atkeson. Prioritized sweeping: Reinforcement learning with less\ndata and less real time. Machine Learning, 13:103\u2013130, 1993.\n[18] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann ma-\nchines. In Proceedings of the 27th International Conference on Machine Learning (ICML\n2010), pages 807\u2013814, 2010.\n[19] Jordan B. Pollack and Alan D. Blair. Why did td-gammon work. In Advances in Neural\nInformation Processing Systems 9, pages 10\u201316, 1996.\n[20] Martin Riedmiller. Neural fitted q iteration\u2013first experiences with a data efficient neural re-\ninforcement learning method. In Machine Learning: ECML 2005, pages 317\u2013328. Springer,\n2005.\n[21] Brian Sallans and Geoffrey E. Hinton. Reinforcement learning with factored states and actions.\nJournal of Machine Learning Research, 5:1063\u20131088, 2004.\n[22] Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, and Yann LeCun. Pedestrian de-\ntection with unsupervised multi-stage feature learning. In Proc. International Conference on\nComputer Vision and Pattern Recognition (CVPR 2013). IEEE, 2013.\n[23] Richard Sutton and Andrew Barto. Reinforcement Learning: An Introduction. MIT Press,\n1998.\n[24] Gerald Tesauro. Temporal difference learning and td-gammon. Communications of the ACM,\n38(3):58\u201368, 1995.\n[25] John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with\nfunction approximation. Automatic Control, IEEE Transactions on, 42(5):674\u2013690, 1997.\n[26] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279\u2013292,\n1992.\n9"}],"widgetId":"rgw32_56aba11b5d889"},"id":"rgw32_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=259367763&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw33_56aba11b5d889"},"id":"rgw33_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=259367763&slotId=728x90_Publications_ATF_Top&collapseSlotMode=beforeFetch&fallbackContainerEnabled=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":259367763,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/259367763_Playing_Atari_with_Deep_Reinforcement_Learning","isLeaderboardAd":true,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56aba11b5d889"},"id":"rgw2_56aba11b5d889","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":259367763},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=259367763&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56aba11b5d889"},"id":"rgw1_56aba11b5d889","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"2ejAW7XCsG2LoQEBQuVqBmaRkUAyGx2SAotp0HlDPn188HTMHrY4DODs57GFQyaqjjqKCOiqueVP3qYXmPyj49IlvanUWhE2O+ArRJbsOTer+yVCZUTE\/Mh5oTt++gBxioa1c0UJ5aJEe6DVOmwcsQ8td\/wjYD3PvdtUBzUsPybvWQ2RfXAQhsMYWdlnTii5lCFnCWIsrkE\/NQjszF3f24qUcpPqgBf0QvfprvbenYV6AYKFyYQe\/mMhWOCa01XTJ+n1rnjWXrMyAaN2cfQbviODVD8Z5aLUfUutwLhRM1k=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/259367763_Playing_Atari_with_Deep_Reinforcement_Learning\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Playing Atari with Deep Reinforcement Learning\" \/>\n<meta property=\"og:description\" content=\"We present the first deep learning model to successfully learn control\npolicies directly from high-dimensional sensory input using reinforcement\nlearning. The model is a convolutional neural...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/259367763_Playing_Atari_with_Deep_Reinforcement_Learning\/links\/03578a2b0cf2db8d39ddb13d\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/259367763_Playing_Atari_with_Deep_Reinforcement_Learning\" \/>\n<meta property=\"rg:id\" content=\"PB:259367763\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Playing Atari with Deep Reinforcement Learning\" \/>\n<meta name=\"citation_author\" content=\"Volodymyr Mnih\" \/>\n<meta name=\"citation_author\" content=\"Koray Kavukcuoglu\" \/>\n<meta name=\"citation_author\" content=\"David Silver\" \/>\n<meta name=\"citation_author\" content=\"Alex Graves\" \/>\n<meta name=\"citation_author\" content=\"Ioannis Antonoglou\" \/>\n<meta name=\"citation_author\" content=\"Daan Wierstra\" \/>\n<meta name=\"citation_author\" content=\"Martin Riedmiller\" \/>\n<meta name=\"citation_publication_date\" content=\"2013\/12\/19\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/259367763_Playing_Atari_with_Deep_Reinforcement_Learning\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/259367763_Playing_Atari_with_Deep_Reinforcement_Learning\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-1329184d-8778-4a0e-ad25-c84d15abf628","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":434,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw34_56aba11b5d889"},"id":"rgw34_56aba11b5d889","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-1329184d-8778-4a0e-ad25-c84d15abf628", "bd6a038c32502dcd6a7a0ad58ad497f4e49c36a9");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-1329184d-8778-4a0e-ad25-c84d15abf628", "bd6a038c32502dcd6a7a0ad58ad497f4e49c36a9");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw35_56aba11b5d889"},"id":"rgw35_56aba11b5d889","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/259367763_Playing_Atari_with_Deep_Reinforcement_Learning","requestToken":"xmtSmWt4sKoc+eZrjFrIilmGpO6APVzt\/CnS35bQUyw7268cgDUVbww6HYAKgqsj6Wne6j\/YVzmxyq6UUz6b1xxNY1HOGfZtsKGvMp+1y7jl9X+6fqArT9Xw4f6XHWVUnGjGUsYbAg4GApJvjLfbXWzCi5NWj2h19CUp0L5iiWGMHYIyBVEI+QxgRIaCWwQfNVJGmMCYFW5OfDlGqSt8ZhSvds6etsTY\/hQ3+kbAGeMQcZ+Fhzkeh3VwuyaPGWNryP06jjchqL6HIwZPlyFANXCoLbBtPEO0b8vVMNPJ50U=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=8S0GZtN_weP9g4cQhl08ptojpk5ObC65Z8XRRbfK-GIjnwGkK5F9iiP6bVi0SFyv","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjU5MzY3NzYzX1BsYXlpbmdfQXRhcmlfd2l0aF9EZWVwX1JlaW5mb3JjZW1lbnRfTGVhcm5pbmc%3D","signupCallToAction":"Join for free","widgetId":"rgw37_56aba11b5d889"},"id":"rgw37_56aba11b5d889","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw36_56aba11b5d889"},"id":"rgw36_56aba11b5d889","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw38_56aba11b5d889"},"id":"rgw38_56aba11b5d889","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
