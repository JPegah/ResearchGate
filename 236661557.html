<!DOCTYPE html> <html lang="en" class="" id="rgw37_56ab1cac69bfa"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="p3Y9dIQrhaW/8HHJeTLUZI59DFpHTIcBIBc0/utAiTLMvD8JOBM4mm7P8E4jNu9IQiNalfuX5MbtBJjSehl7zFGmYJ4ia7YKI0YHRPj5EqIySEklolizfehXoNqIFebzmUSajVSD8n+2ccd0KVtQzDLNBnITs+MIFKheAARZA/Kf7cMn9y9Ei8Clac0jHnyOhppiq8dCDOXfDEiqwj5hD7baBNoPTD9ED6biM0N5H64gPUiQeF/27iQR/ZlgmiK68CftA+mWEEUIzkk4paQIMHh10R5A3U0IznveB7R0G9Y="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-3722ef69-f6e5-415b-a098-098486e6a1e3",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/236661557_Scaling_the_Indian_Buffet_Process_via_Submodular_Maximization" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Scaling the Indian Buffet Process via Submodular Maximization" />
<meta property="og:description" content="Inference for latent feature models is inherently difficult as the inference
space grows exponentially with the size of the input data and number of latent
features. In this work, we use Kurihara..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/236661557_Scaling_the_Indian_Buffet_Process_via_Submodular_Maximization/links/034fa80a0cf2ac15472e918a/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/236661557_Scaling_the_Indian_Buffet_Process_via_Submodular_Maximization" />
<meta property="rg:id" content="PB:236661557" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Scaling the Indian Buffet Process via Submodular Maximization" />
<meta name="citation_author" content="Colorado Reed" />
<meta name="citation_author" content="Zoubin Ghahramani" />
<meta name="citation_publication_date" content="2013/04/11" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/236661557_Scaling_the_Indian_Buffet_Process_via_Submodular_Maximization" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/236661557_Scaling_the_Indian_Buffet_Process_via_Submodular_Maximization" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Scaling the Indian Buffet Process via Submodular Maximization</title>
<meta name="description" content="Scaling the Indian Buffet Process via Submodular Maximization on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1cac69bfa" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1cac69bfa" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw6_56ab1cac69bfa">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Scaling%20the%20Indian%20Buffet%20Process%20via%20Submodular%20Maximization&rft.title=30th%20International%20Conference%20on%20Machine%20Learning%2C%20ICML%202013&rft.jtitle=30th%20International%20Conference%20on%20Machine%20Learning%2C%20ICML%202013&rft.date=2013&rft.au=Colorado%20Reed%2CZoubin%20Ghahramani&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Scaling the Indian Buffet Process via Submodular Maximization</h1> <meta itemprop="headline" content="Scaling the Indian Buffet Process via Submodular Maximization">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/236661557_Scaling_the_Indian_Buffet_Process_via_Submodular_Maximization/links/034fa80a0cf2ac15472e918a/smallpreview.png">  <div id="rgw8_56ab1cac69bfa" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56ab1cac69bfa"> <a href="researcher/2008466791_Colorado_Reed" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Colorado Reed" alt="Colorado Reed" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Colorado Reed</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw10_56ab1cac69bfa">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2008466791_Colorado_Reed"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Colorado Reed" alt="Colorado Reed" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2008466791_Colorado_Reed" class="display-name">Colorado Reed</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56ab1cac69bfa"> <a href="researcher/8159937_Zoubin_Ghahramani" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Zoubin Ghahramani</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56ab1cac69bfa">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/8159937_Zoubin_Ghahramani"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/8159937_Zoubin_Ghahramani" class="display-name">Zoubin Ghahramani</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">     30th International Conference on Machine Learning, ICML 2013   <meta itemprop="datePublished" content="2013-04">  04/2013;               <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1304.3285" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw13_56ab1cac69bfa" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>Inference for latent feature models is inherently difficult as the inference<br />
space grows exponentially with the size of the input data and number of latent<br />
features. In this work, we use Kurihara &amp; Welling (2008)'s<br />
maximization-expectation framework to perform approximate MAP inference for<br />
linear-Gaussian latent feature models with an Indian Buffet Process (IBP)<br />
prior. This formulation yields a submodular function of the features that<br />
corresponds to a lower bound on the model evidence. By adding a constant to<br />
this function, we obtain a nonnegative submodular function that can be<br />
maximized via a greedy algorithm that obtains at least a one-third<br />
approximation to the optimal solution. Our inference method scales linearly<br />
with the size of the input data, and we show the efficacy of our method on the<br />
largest datasets currently analyzed using an IBP model.</div> </p>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw26_56ab1cac69bfa">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw25_56ab1cac69bfa"  itemprop="articleBody">  <p>Page 1</p> <p>Scaling the Indian Buffet Process via Submodular Maximization<br />Colorado Reed<br />Zoubin Ghahramani<br />Engineering Department, Cambridge University, Cambridge UK<br />cr478@cam.ac.uk<br />zoubin@eng.cam.ac.uk<br />Abstract<br />Inference for latent feature models is inher-<br />ently difficult as the inference space grows<br />exponentially with the size of the input data<br />and number of latent features.<br />work, we use Kurihara &amp; Welling (2008)’s<br />maximization-expectation framework to per-<br />form approximate MAP inference for linear-<br />Gaussian latent feature models with an In-<br />dian Buffet Process (IBP) prior. This for-<br />mulation yields a submodular function of the<br />features that corresponds to a lower bound on<br />the model evidence. By adding a constant to<br />this function, we obtain a nonnegative sub-<br />modular function that can be maximized via<br />a greedy algorithm that obtains at least a1<br />approximation to the optimal solution. Our<br />inference method scales linearly with the size<br />of the input data, and we show the efficacy of<br />our method on the largest datasets currently<br />analyzed using an IBP model.<br />In this<br />3-<br />1. Introduction<br />Nonparametric latent feature models experienced a<br />surge of interest in the machine learning community<br />following Griffiths &amp; Ghahramani (2006)’s formula-<br />tion of the Indian Buffet Process (IBP)—a nonpara-<br />metric prior for equivalence classes of sparse binary<br />matrices. These binary matrices have a finite num-<br />ber of exchangeable rows and an unbounded number<br />of columns, where a 1 in row n and column k indicates<br />that observation n expresses latent feature k. For ex-<br />ample, given an image dataset of human faces, each<br />observation is an image, and the latent features might<br />be “is smiling,” “is wearing glasses,” etc. More gener-<br />ally, feature models can be viewed as a generalization<br />of unsupervised clustering, see Broderick et al. (2012).<br />Proceedings of the 30thInternational Conference on Ma-<br />chine Learning, Atlanta, Georgia, USA, 2013.<br />W&amp;CP volume 28. Copyright 2013 by the author(s).<br />JMLR:<br />The IBP prior is often used in sparse matrix factoriza-<br />tion models where a data matrix of N D-dimensional<br />observations is expressed as a product of two matrices<br />that factor over K latent factors plus a noise term:<br />X = ZA + E. Formally, this model has a binary fea-<br />ture matrix Z ∈ {0,1}N×Kthat linearly combines a<br />latent factor matrix A ∈ RK×Dplus a noise matrix<br />E ∈ RN×Dto form the observed data X ∈ RN×D.<br />Placing an IBP prior on Z lets K be unbounded and<br />allows the number of active features K+(those with<br />non-zero Z column sums) to be learned from the data<br />while remaining finite with probability one. The IBP<br />inspired several infinite-limit versions of classic matrix<br />factorization models, e.g. infinite independent compo-<br />nent analysis models (Knowles &amp; Ghahramani, 2007).<br />Inference with IBP models is challenging as its discrete<br />state space has 2NK+possible assignments. In turn,<br />the IBP has found limited application to large data in<br />comparison to the Chinese Restaurant Process, which<br />assigns one feature to each observation. In this paper,<br />we use Kurihara &amp; Welling (2008)’s Maximization-<br />Expectation (ME) framework to perform approximate<br />MAP inference with IBP matrix factorization models,<br />termed MEIBP inference. For nonnegative A, we show<br />that we can obtain approximate MAP solutions for Z<br />by maximizing N submodular cost functions. The sub-<br />modularity property enables the use of a simple greedy<br />algorithm that obtains at least a<br />the optimal solution. While the worst-case complex-<br />ity of MEIBP inference is comparable to sampling and<br />variational approaches, in §5 we show that MEIBP in-<br />ference often converges to better solutions than varia-<br />tional methods and similar solutions as the best sam-<br />pling techniques but in a fraction of the time.<br />1<br />3-approximation to<br />This paper is structured as follows: in §2 we present<br />background material that sets the foundation for our<br />presentation of MEIBP inference in §3 and the re-<br />sulting submodular maximization problem that arises,<br />then in §4 we discuss related work, and in §5 we com-<br />pare the MEIBP with other IBP inference techniques<br />using both synthetic and real-world datasets.<br />arXiv:1304.3285v4  [stat.ML]  24 Jul 2013</p>  <p>Page 2</p> <p>Scaling the Indian Buffet Process via Submodular Maximization<br />2. Background<br />2.1. The Indian Buffet Process<br />Griffiths &amp; Ghahramani (2006) derived the IBP prior<br />by placing independent beta priors on Bernoulli gener-<br />ated entries of an N ×K binary matrix Z, marginaliz-<br />ing over the beta priors, and letting K go to inifinity.<br />In this infinite limit, however, P(Z) is zero for any<br />particular Z. Griffiths &amp; Ghahramani (2006) there-<br />fore take the limit of an equivalence classes of binary<br />matrices, [Z], defined by the “left-order form” (lof )<br />ordering of the columns and show that P([Z]lof) has a<br />non-zero probability as K goes to infinity.<br />The lof ordering arranges the columns of Z such that<br />the binary values of the columns are non-increasing,<br />where the first row is the most significant bit. Ding<br />et al. (2010) examine different “shifted” equivalence<br />classes formed by shifting all-zero columns to the right<br />of non-zero columns while maintaining the non-zero<br />column ordering.Given K+ non-zero columns, the<br />IBP prior for the shifted equivalence classes is<br />P([Z]|α) =αK+<br />K+!e−αHN<br />K+<br />?<br />k=1<br />(N − mk)!(mk− 1)!<br />N!<br />(1)<br />where α is a hyperparameter, HNis the Nthharmonic<br />number, and mk =<br />material has a derivation of P([Z]|α) as well as a com-<br />parison to the lof equivalence classes. The derivations<br />in §3 can be applied using either equivalence class.<br />However, the shifted equivalence classes simplify the<br />mathematics and produce the same results in practice.<br />?N<br />n=1znk. The supplementary<br />2.2. Maximization-Expectation<br />Kurihara &amp; Welling (2008) presented the ME algo-<br />rithm:an inference algorithm that exchanges the<br />expectation and maximization variables in the EM<br />algorithm. Consider a general probabilistic model<br />p(X,Z,A), where X are the observed random vari-<br />ables (RVs), Z are the local latent RVs, and A are the<br />global latent RVs. RVs are qualified as “local” if there<br />is one RV for each observation, and RVs are “global” if<br />their multiplicity is constant or inferred from the data.<br />ME can be viewed as a special case of a Mean-Field<br />Variational Bayes (MFVB) approximation to a poste-<br />rior that cannot be computed analytically, p(Z,A|X).<br />MFVB operates by approximating the posterior dis-<br />tribution of a given probabilistic model by assuming<br />independent variational distributions, p(Z,A|X) ≈<br />q(Z)q(A) (Attias, 2000; Ghahramani &amp; Beal, 2001).<br />The independence constraint lets us compute the vari-<br />ational distribution q that minimizes the KL diver-<br />gence between the variational distribution and true<br />posterior. Without this constraint, the distribution<br />that minimizes the KL-divergence is the true posterior,<br />returning us to our original problem. In MFVB, we de-<br />termine the variational distributions and their param-<br />eters using coordinate ascent optimization in which we<br />iteratively update:<br />q(Z) ∝ exp E<br />q(A) ∝ exp E<br />q(A)[lnp(X,Z,A)](2)<br />q(Z)[lnp(X,Z,A)], (3)<br />which commonly has closed-form solutions.<br />The EM algorithm can be viewed as a special case<br />of MFVB that obtains MAP values of the global RVs<br />by letting q(A) = δ(A − A∗), where δ(·) is the delta<br />function and A∗is the MAP assignment. The ME al-<br />gorithm instead maximizes the local RVs Z and com-<br />putes the expectation over the global RVs A, which<br />can be viewed as MFVB with q(Z) = δ(Z − Z∗).<br />In the limit of large N, the ME algorithm recovers<br />a Bayesian information criterion regularization term<br />(Kurihara &amp; Welling, 2008). Also, maintaining a vari-<br />ational distribution over the global RVs retains the<br />model selection ability of MFVB, while using point es-<br />timates of the local RVs allows the use of efficient data<br />structures and optimization techniques.<br />show, the ME algorithm leads to a scalable submodu-<br />lar optimization problem for latent feature models.<br />As we will<br />2.3. Submodularity<br />Submodularity is a set function property that makes<br />optimization of the function tractable or approx-<br />imable. Given ground set V and set function f : 2V→<br />R, f is submodular if for all A ⊆ B ⊆ V and e ∈ V \B:<br />f(A ∪ {e}) − f(A) ≥ f(B ∪ {e}) − f(B),<br />which expresses a “diminishing returns” property,<br />where the incremental benefit of element e diminishes<br />as we include it in larger solution sets. Submodularity<br />is desirable in discrete optimization because submod-<br />ular functions are discrete analogs of convex functions<br />and can be globally minimized in polynomial time<br />(Lov´ asz, 1983).However, global submodular maxi-<br />mization is NP-hard, but submodularity often enables<br />approximation bounds via greedy algorithms. In the<br />next section, we show that determining a MAP esti-<br />mate of Z in the ME algorithm is a scalable submod-<br />ular maximization problem.<br />(4)<br />3. Maximization-Expectation IBP<br />Here we present the ME algorithm for nonnegative<br />linear-Gaussian IBP models and show that approxi-</p>  <p>Page 3</p> <p>Scaling the Indian Buffet Process via Submodular Maximization<br />mate MAP inference arises as a submodular maximiza-<br />tion problem. Boldface variables are matrices with<br />(row, column) subscripts; a dot indicates all elements<br />of the dimension, and lowercase variables are scalars.<br />3.1. Nonnegative Linear-Gaussian IBP Model<br />We consider the following probabilistic model:<br />p(X,Z,A|θ) = p(X|Z,A,σ2<br />X)p(A|σ2<br />A)p(Z|α) (5)<br />p(X|Z,A,σ2<br />A) =<br />N<br />?<br />K<br />?<br />n=1<br />N(Xn·;Zi·A,σ2<br />AI) (6)<br />p(A|0,σ2<br />A) =<br />k=1<br />D<br />?<br />d=1<br />TN(akd;0,σ2<br />A) (7)<br />with p([Z]|α) specified in Eq. 1. This is a nonnegative<br />linear-Gaussian IBP model, where the prior over the<br />latent factors, p(A|0,σ2<br />cated Gaussian with nonnegative support, denoted<br />TN. As we show below, this nonnegative prior yields<br />a submodular maximization problem when optimizing<br />Z. We use a truncated Gaussian as it is conjugate to<br />the Gaussian likelihood, but other nonnegative priors<br />(e.g. exponential) can be used. For brevity we assume<br />the hyperparameters, θ = {α,σ2<br />discuss θ inference in the supplementary material.<br />A), is a zero-mean i.i.d. trun-<br />A,σ2<br />X}, are known and<br />3.2. MEIBP Evidence<br />In the ME framework, we approximate the true pos-<br />terior distribution via a MFVB assumption:<br />p(Z,A|X,θ) ≈ q(A)δ(Z − Z∗).<br />That is, we maintain a variational distribution over the<br />latent factors A and optimize the latent features Z.<br />Given the MFVB constraint, we determine the varia-<br />tional distributions by minimizing the KL-divergence<br />between the variational distributions and the true<br />posterior, which is equivalent to maximizing a lower<br />bound on the evidence (Attias, 2000):<br />(8)<br />lnp(X|θ) = E<br />q[lnp(X,A,Z|θ)] + H[q] + D(q?p)<br />≥ E<br />where H[q] is the entropy of q and D(q?p) represents<br />the KL-divergence between the variational distribution<br />and the true posterior. The evidence lower bound, F,<br />for the nonnegative linear-Gaussian IBP model is:<br />?<br />K+<br />?<br />q[lnp(X,A,Z|θ)] + H[q] ≡ F<br />(9)<br />1<br />σ2<br />X<br />N<br />?<br />n=1<br />−1<br />2Zn·ΦΦTZT<br />n·+ Zn·ξT<br />n·<br />?<br />− lnK+!<br />+<br />k=1<br />?<br />ln(N − mk)!(mk− 1)!<br />N!<br />+ ηk<br />?<br />+ const(10)<br />with<br />ξnk= Φk·XT<br />n·+1<br />2<br />D<br />?<br />d=1<br />?E[akd]2− E[a2<br />kd]?<br />(11)<br />and<br />ηk=<br />D<br />?<br />d=1<br />?<br />−ln<br />πσ2<br />2α2/D<br />2<br />A<br />−E[a2<br />kd]<br />2σ2<br />A<br />+ H(q(akd))<br />?<br />(12)<br />where Φk· = (E[ak1],...,E[akD]), and all expecta-<br />tions are with respect to q(A), which is defined in the<br />next subsection. In §3.5 we show that maximizing this<br />lower bound with respect to Z can be formulated as a<br />submodular maximization problem.<br />3.3. Variational Factor Updates<br />Maximizing Eq. 10 with respect to q(A) yields<br />q(A) =<br />K<br />?<br />k=1<br />D<br />?<br />d=1<br />TN(akd; ˜ µkd, ˜ σ2<br />kd), (13)<br />with parameter updates<br />˜ µkd= ρk<br />N<br />?<br />X,<br />n=1<br />znk<br />?<br />xnd−<br />?<br />k??=k<br />znk? E[ak?d]<br />?<br />(14)<br />˜ σ2<br />kd= ρkσ2<br />(15)<br />where ρk =<br />O(NK2D), and the relevant moments are:<br />?mk +<br />σ2<br />σ2<br />X<br />A<br />?−1.<br />?2/π<br />erfcx(℘kd)<br />These updates take<br />E[akd] = ˜ µkd+ ˜ σkd<br />(16)<br />E?a2<br />kd<br />?= ˜ µ2<br />kd+ ˜ σ2<br />kd+ ˜ σkd˜ µkd<br />?2/π<br />erfcx(℘kd)<br />(17)<br />with ℘kd = −<br />representing the scaled complementary error function.<br />˜ µkd<br />˜ σkd<br />√2and erfcx(y) = ey2(1 − erf(y))<br />3.4. Evidence Lower Bound as K → ∞<br />Here we show that the evidence lower bound [Eq. 10] is<br />well-defined in the limit K → ∞; in fact, all instances<br />of K are simply replaced by K+. Therefore, similar to<br />variational IBP methods, a user must specify a maxi-<br />mum model complexity K+. A benefit over variational<br />IBP methods, however, is that the q(Z) updates are<br />not affected by inactive features—see §4.<br />We take this limit by breaking the evidence into com-<br />ponents 1,...,K+ and K++ 1,...,K and note that</p>  <p>Page 4</p> <p>Scaling the Indian Buffet Process via Submodular Maximization<br />when mk = 0: ˜ µkd = 0, ˜ σ2<br />1<br />2lnπeσ2<br />2<br />. After some algebra, the evidence becomes:<br />?<br />kd= σ2<br />A, and H(akd) =<br />A<br />ψK++1<br />2<br />K<br />?<br />k=K++1<br />D<br />?<br />d=1<br />−lnπσ2<br />A<br />2<br />−E[a2<br />kd]<br />σ2<br />A<br />+ lnπeσ2<br />A<br />2<br />?<br />(18)<br />where ψK+is Eq. 10 but with K+ replacing all K.<br />From Eq. 25, we see that E[a2<br />which causes all terms to cancel in Eq. 18 except ψK+.<br />kd] = σ2<br />Awhen mk= 0,<br />The evidence lower bound remains well-defined be-<br />cause both the likelihood and IBP prior terms do not<br />depend on inactive features, so for inactive features<br />the KL-divergence between the posterior and varia-<br />tional distributions is simply the KL-divergence be-<br />tween p(A) and q(A). For inactive features, p(A) =<br />q(A), and as a result, the KL-divergence is zero.<br />3.5. Z Objective Function<br />Given q(A), we compute MAP estimates of Z by max-<br />imizing the evidence [Eq. 10] for each n ∈ {1,...,N}<br />while holding constant all n?∈ {1,...,N}\n. Decom-<br />posing Eq. 10 into terms that depend on Zn·and those<br />that do not yields (see the supplementary material):<br />F(Zn·) = −<br />1<br />2σ2<br />X<br />?<br />Zn·ΦΦTZT<br />n·+ Zn·ωT<br />n·+ const<br />− ln<br />?<br />K+\n+<br />K+<br />?<br />k=1<br />?<br />1{mk\n=0}znk<br />?<br />D<br />?<br />??<br />! (19)<br />Φk·=<br />E[ak1],...,E[akD]<br />ωnk=1<br />σ2<br />X<br />?<br />Φk·XT<br />n·+1<br />2<br />d=1<br />?E[akd]2− E[a2<br />kd]??<br />+ ν(znk= 1) − ν(znk= 0) + 1{mk\n=0}ηk,<br />which is a quadratic pseudo-Boolean function plus a<br />term that penalizes K+, where 1{·}is the indicator<br />function, a “\n” subscript indicates the given variable<br />is determined after removing the nthrow from Z, and<br /><br /><br />We can prove F(Zn·) is submodular given the follow-<br />ing two well-known propositions, see Fujishige (2005):<br />Proposition 1. Nonnegative linear combinations of<br />submodular functions are submodular.<br />Proposition 2. A quadratic pseudo-Boolean function<br />with quadratic weight matrix W is submodular if and<br />only if Wij≤ 0 for all i,j.<br />ν(znk) =<br /><br /><br /><br />0, if mk\n= 0 and znk= 0<br />ln(N − mk\n− znk)!/N!<br />+ ln(mk\n+ znk− 1)!, otherwise<br />Via Proposition 2, we see that −<br />Zn·ωT<br />Proposition 1, Eq. 19 is submodular if and only if<br />1<br />2σ2<br />XZn·ΦΦTZT<br />n·+<br />n·is submodular when Φ is nonnegative. From<br />G(Zn·) = −ln<br />?<br />K+\n+<br />K+<br />?<br />k=1<br />?<br />1{mk\n=0}znk<br />??<br />!(20)<br />is submodular. We prove this property by rephras-<br />ing G(Zn·) as a set function and using the defini-<br />tion of submodularity given by Eq. 4.<br />Bn ⊆ V where V = {1,...,K+} and An,Bn ∈ 2V<br />with G(An) = −ln(K+\n+ KAn)!.<br />KAn=?K+<br />G(An∪ {e}) − G(An) ≥ G(Bn∪ {e}) − G(Bn)<br />?K+\n+ KAn<br />Let An ⊆<br />Here we let<br />k=11{mk\n=0}1{k∈An}where k ∈ An indi-<br />cates znk= 1. G is submodular if for all e ∈ V \ Bn:<br />ln<br />?!<br />?K+\n+ KAn∪{e}<br />Eq. 21 has two cases: (1) me\n&gt; 0 so KBn∪{e}= KBn<br />and KAn∪{e}= KAn, yielding 0 ≥ 0 for Eq. 21, which<br />is true for all e ∈ V \ Bnand An⊆ Bn, (2) me\n=<br />0 so KBn∪{e}= KBn+ 1 and KAn∪{e}= KAn+ 1.<br />After some algebra this yields KBn∪{e}≥ KAn∪{e}for<br />Eq. 21, which is again true for all e ∈ V \ Bn and<br />An⊆ Bn. As a result, both components of Eq. 19 are<br />submodular, and by Proposition 1, adding these terms<br />yields a submodular function.<br />?!≥ ln<br />?K+\n+ KBn<br />?!<br />?K+\n+ KBn∪{e}<br />?!<br />(21)<br />3.6. Z Optimization<br />Eq. 19 is an unconstrained nonmonotone submodular<br />function. Feige et al. (2011) prove that an approx-<br />imibility guarantee is NP-hard for this class of func-<br />tions. However, Feige et al. (2011) also show that a<br />local-search (ls) algorithm obtains a constant-factor<br />approximation to the optimal solution, provided the<br />submodular objective function is nonnegative. For a<br />submodular function F : 2V→ R with ground set<br />V = {1,...,K+} and solution set A ⊆ V , the ls-<br />algorithm operates as follows:<br />1. initialize: let A = {argmaxw∈VF({w})}<br />2. grow: while there is an element w ∈ V \ A s.t.<br />F(A ∪ {w}) &gt; (1 +<br />3. prune: if there is an element w ∈ A s.t. F(A \<br />{w}) &gt; (1+<br />4. return: maximum of F(A) and F(V \ A).<br />?<br />|V |2)F(A): let A := A ∪ {w}<br />?<br />|V |2)F(A): let A := A\{w}, goto 2.<br />The ls-algorithm obtains a solution that is greater than<br />1<br />3(1 −<br />?<br />|V |)OPT—where ? is a parameter and OPT is</p>  <p>Page 5</p> <p>Scaling the Indian Buffet Process via Submodular Maximization<br />the maximum value of F. The ls-algorithm performs<br />O(1<br />steps.<br />?|V |3log|V |) function calls in the grow/prune<br />2468 1012<br />0<br />0.2<br />0.4<br />0.6<br />0.8<br />1<br />latent features (K)<br />fraction of opts<br />LS ≥ 95% Optimum<br />LS Finds Optimum<br />Random ≥ 95% Optimum<br />Random Finds Optimum<br />Figure 1. Fraction of ls-algorithm and random solutions<br />that obtain [within 95% of] the true optimum using data<br />generated from the nonnegative linear-Gaussian model<br />with N = 500,D = 50,σX = 1.0.<br />Since Eq. 19 is not strictly nonnegative, we use<br />its normalized cost function to interpret the ls-<br />approximability guarantee: F(Zn·) − Fn0, where Fn0<br />is the minimum value of F(Zn·). Using the normal-<br />ized cost function, we obtain the following optimality<br />guarantee:<br />?<br />where the superscript “ls” denotes the solution from<br />the greedy ls-algorithm and an asterisk denotes the<br />set that obtains the true maximum. This inequality<br />states that the ls-algorithm solution is guaranteed to<br />perform better than the minimum by an amount pro-<br />portional to the difference between the optimum and<br />the minimum. However, we emphasize that this in-<br />equality does not provide an optimality guarantee for<br />the global MAP solution.<br />F(Zls<br />n·) ≥ Fn0+1<br />3<br />1 −<br />?<br />|V |<br />?<br />(F(Z∗<br />n·) − Fn0) (22)<br />We studied the empirical performance of the ls-<br />algorithm by generating high noise data (σX = 1)<br />from the nonnegative linear-Gaussian model with N =<br />500,D = 50 and compared the ls-algorithm with the<br />brute-force optimal solution as K varied from 2 to 12,<br />performing 1000K total optimizations for each of ten<br />randomly generated datasets. Furthermore, we com-<br />pared the ls-algorithm with randomly sampled Zn·so-<br />lutions to demonstrate that the optimization space was<br />not skewed to favor solutions near the optimal value.<br />Figure 1 shows the fraction of solutions that obtain the<br />true optimum as well as the fraction of solutions that<br />were greater than 95% of F(Z∗<br />error bars indicate the combined standard deviation<br />over the 10 × 1000K optimizations. The ls-algorithm<br />found the optimal solution roughly 70% of the time<br />n·) − Fn0, where the<br />for K = 12 and obtained within 95% of the optimal<br />solution over 99.9% of the time for all K—meaning we<br />could empirically replace the1<br />random sampling comparison indicated that the opti-<br />mization space did not favor nearly-optimal solutions:<br />its convergence to 5% for within-95% optimal solutions<br />was characteristic of a uniform solution space.<br />3in Eq. 22 with19<br />20. The<br />By precomputing ΦΦTand maintaining an auxil-<br />iary vector of K+ weights, we can evaluate Eq.<br />19 in constant time when adding/removing elements<br />to the solution set. In turn, the ls-algorithm op-<br />timizes F(Zn·) in K2<br />tions. The O(1<br />+logK+) component arises from the<br />add/removal operations, but as we show in Figure 2,<br />it is a loose upper bound that scales sub-quadratically<br />in practice.<br />+D + O(1<br />?K3<br />+logK+) opera-<br />?K3<br />0 100<br />number of latent features (K)<br />200300 400500<br />0<br />2<br />4<br />·104<br />O(1) updates<br />K*logK fit<br />data<br />Figure 2. Number of O(1) updates per ls-optimization us-<br />ing data generated from the nonnegative linear-Gaussian<br />model with N = 1000,D = 1000,σX = 1.0.<br />4. Related Work<br />Several proposals have been made for efficient infer-<br />ence with latent feature models. Table 1 summarizes<br />the per-iteration complexity of the methods discussed<br />below. In the next section we compare these methods<br />on two synthetic and three real-world datasets.<br />Doshi-Velez et al. (2009) formulated a coordinate as-<br />cent variational inference technique for IBP models<br />(VIBP). This method used the “stick breaking” for-<br />mulation of the IBP, which maintained coupled beta-<br />distributed priors on the entries of Z—marginalizing<br />these priors does not allow closed-form MFVB up-<br />dates. Unlike MEIBP inference, maintaining the beta<br />priors has the undesirable consequence that inactive<br />features contribute to the evidence lower bound and<br />must be ignored when updating the variational distri-<br />butions. This was not a problem for Doshi-Velez et al.<br />(2009)’s finite variational IBP, which computes vari-<br />ational distributions for a linear-Gaussian likelihood<br />with a parametric beta-Bernoulli prior on the latent<br />features. The inference complexity for both methods</p>  <p>Page 6</p> <p>Scaling the Indian Buffet Process via Submodular Maximization<br />is O(NK2<br />+D), which is dominated by updating q(Z).<br />Ding et al. (2010) used mixed expectation-propagation<br />style updates with MFVB inference in order to per-<br />form variational inference for a nonnegative linear-<br />Gaussian IBP model (INMF). The expectation-<br />propagation style updates are more complicated than<br />MFVB updates and have per-iteration complexity<br />O(N(K3D+KD2)). Ding et al. (2010) motivated this<br />framework by stating that the evidence lower bound<br />of a linear-Gaussian likelihood with a truncated Gaus-<br />sian prior on the latent factors is negative infinity. This<br />is only true if the variational distribution is a Gaus-<br />sian, however the free-form variational distribution for<br />their model is a truncated Gaussian, which has a well-<br />defined evidence lower bound.<br />Doshi-Velez &amp; Ghahramani (2009) presented a linear-<br />time “accelerated” Gibbs sampler for conjugate IBP<br />models that effectively marginalized over the latent<br />factors (AIBP). The per-iteration complexity was<br />O(N(K2+ KD)). This is comparable to the uncol-<br />lapsed IBP sampler (UGibbs) that has per-iteration<br />complexity O(NDK2) but does not marginalize over<br />the latent factors, and as a result, takes longer to<br />mix. In terms of both complexity and empirical per-<br />formance, the accelerated Gibbs sampler is the most<br />scalable sampling-based IBP inference technique cur-<br />rently available. One constraint of the accelerated IBP<br />is that the latent factor distribution must be conjugate<br />to the likelihood, which for instance, does not allow<br />nonnegative priors on the latent factors.<br />Rai &amp; Daume III (2011) introduced a beam-search<br />heuristic for locating approximate MAP solutions to<br />linear-Gaussian IBP models (BS-IBP). This heuris-<br />tic sequentially adds a single data point to the model<br />and determines the latent feature assignments by scor-<br />ing all 2K+latent feature combinations.<br />ing heuristic uses an estimate of the joint probability,<br />P(X,Z) to score assignments, which evaluates the col-<br />lapsed likelihood P(X|Z) for all 2K+possible assign-<br />ments: an expensive N3(K++ D) operation, yielding<br />a per-iteration complexity of O(N3(K++ D)2K+).<br />The scor-<br />5. Experiments<br />We evaluated the inference quality and efficiency of<br />MEIBP inference on two synthetic and three real-<br />world datasets. We used the runtime and predic-<br />tive likelihood of held-out observations as our perfor-<br />mance criteria and compared MEIBP inference with<br />the methods listed in Table 1 (the finite and infinite<br />VIBP are differentiated with an “f-” and “i-” prefix).<br />We used a truncated Gaussian prior on the latent fac-<br />Table 1. Worst-case<br />linear-Gaussian likelihood model for N D-dimensional ob-<br />servations and K+ active latent features.<br />per-iterationcomplexitygivena<br />Algorithm<br />MEIBP<br />VIBP (Doshi-Velez et al.,<br />2009)<br />AIBP (Doshi-Velez &amp;<br />Ghahramani, 2009)<br />UGibbs (Doshi-Velez &amp;<br />Ghahramani, 2009)<br />BS-IBP (Rai &amp;<br />Daume III, 2011)<br />INMF (Ding et al., 2010)<br />Iteration Complexity<br />O(N(K2<br />+D+K3<br />+lnK+))<br />O(NK2<br />+D)<br />O(N(K2<br />++ K+D))<br />O(NK2<br />+D)<br />O(N3(K++ D)2K+)<br />O(N(K3<br />+D + K+D2))<br />tors for UGibbs and INMF, and Gaussian priors for<br />the AIBP and variational methods.<br />tions, we also included Schmidt et al. (2009)’s iterated<br />conditional modes algorithm, which computes a MAP<br />estimate of a parametric nonnegative matrix factoriza-<br />tion model: X = BA+E, where B and A have expo-<br />nential priors and E is zero-mean Gaussian noise. We<br />abbreviate this model “BNMF”; it has a per-iteration<br />complexity of O(N(K2<br />++ K+D)).<br />In our evalua-<br />The VIBP and MEIBP inference methods specify a<br />maximum K value, while the sampling methods are<br />unbounded. Therefore, we also included truncated<br />versions of the sampling methods (indicated by a “t-”<br />prefix) for a fairer comparison. We centered all in-<br />put data to have a 0-mean for the models with 0-<br />mean Gaussian priors and a 0-minimum for nonneg-<br />ative models, and all inferred matrices were initial-<br />ized randomly from their respective priors. Following<br />Doshi-Velez &amp; Ghahramani (2009), we fixed the hyper-<br />parameters σX and σAto3<br />dard deviation across all dimensions of the data, and<br />set α = 3. We ran each algorithm until the multiplica-<br />tive difference of the average training log-likelihood<br />differed by less than 10−4between blocks of five it-<br />erations with a maximum runtime of 36 hours. Our<br />experiments used MATLAB implementations of the<br />algorithms, as provided by the respective authors, on<br />3.20 GHz processors.<br />4σ, where σ was the stan-<br />Synthetic Data We created high-noise synthetic<br />datasets in the following way:<br />Bernoulli(p = 0.4), (2) generate A with K random,<br />potentially overlapping binary factors, (3) let X =<br />ZA + E, where E ∼ N(0,1). We evaluated the pre-<br />dictive likelihood on 20% of the dimensions from the<br />last half of the data (see supplementary information).<br />(1) sample zn,k ∼<br />Figure 3 shows the evolution of the test log-likelihood<br />over time for a small dataset with N = 500,D =</p>  <p>Page 7</p> <p>Scaling the Indian Buffet Process via Submodular Maximization<br />10−1<br />100<br />101<br />102<br />103<br />−3.5<br />−3<br />−2.5<br />·104<br />bnmf<br />meibp<br />t-ugibbs<br />ugibbs<br />aibp<br />t-aibp<br />f-vibp<br />i-vibp<br />seconds<br />test log-likelihood<br />ugibbs<br />t-ugibbs<br />t-aibp<br />aibp<br />bnmf<br />f-vibp<br />i-vibp<br />meibp<br />05 10 15<br />hours<br />20 2530 35<br />−1.4<br />−1.2<br />−1<br />−0.8·107<br />bnmf<br />meibp<br />t-ugibbs<br />ugibbs<br />aibp<br />t-aibp<br />f-vibp<br />i-vibp<br />Figure 3. Evolution of test log-liklihood over time for a synthetic dataset; Left: dataset with N = 500,D = 500,K = 20<br />(NB: x-axis is log-scale) Right: dataset with N = 105,D = 103,K = 50 (NB: x-axis is linear-scale).<br />500,K = 20 and a large dataset with N = 105,D =<br />103,K = 50. All models were initialized randomly<br />with the true number of latent features, and the error<br />regions display the standard deviation over five ran-<br />dom restarts. The BS-IBP and INMF methods were<br />removed from our experiments following the synthetic<br />dataset tests as both methods took at least an order<br />of magnitude longer than the other methods: in 36<br />hours, the BS-IBP did not complete a single iteration<br />on the small dataset, and the INMF did not complete<br />a single iteration on the large dataset.<br />MEIBP converged quickest among the IBP models<br />for both the small and large dataset, while the para-<br />metric BNMF model converged much faster than all<br />IBP models. However, the IBP models captured the<br />sparsity of the latent features and the MEIBP and<br />UGibbs eventually outperformed the BNMF on the<br />small dataset, while only the MEIBP outperformed the<br />BNMF on the large dataset. The VIBP methods con-<br />verged quicker than the sampling counterparts but had<br />trouble escaping local optima. The uncollapsed sam-<br />plers eventually performed as well as the MEIBP on<br />the small dataset but did not mix to a well-performing<br />distribution for the large dataset.<br />Real<br />datasets used in our experiments.<br />BC are dense real-valued datasets, whereas the Flickr<br />dataset is a sparse binary dataset (0.81% filled). For<br />the Piano and Flickr datasets, we evaluated the pre-<br />dictive likelihood on a held-out portion of 20% of the<br />dimensions from the last half of the datasets.<br />Yale-BC dataset had roughly sixty-four facial images<br />of thirty-eight subjects, and we removed the bottom<br />half of five images from each subject for testing.<br />Data Table 2 summarizes the real-world<br />Piano and Yale-<br />The<br />Figure 4 shows the test log-likelihood and convergence<br />time for all inference methods applied to the real-world<br />datasets, averaged over five random restarts. All in-<br />ference methods were initialized with K = {10,25,50}<br />Table 2. Summary of real-world datasets.<br />Dataset<br />Piano (Poliner<br />&amp; Ellis, 2006)<br />Flickr (Kollar &amp;<br />Roy, 2009)<br />Yale-BC (Lee<br />et al., 2005)<br />Size (N × D)<br />16000 × 161<br />Details<br />DFT of piano<br />recordings<br />binary image-tag<br />indicators<br />face images with<br />various lightings<br />25000 × 1500<br />2414 × 32256<br />as indicated by the size of the marker (the smallest<br />marker shows the K = 10 results).<br />methods (AIBP, UGibbs) also include a large faded<br />marker that shows the results for unbounded K.<br />The sampling<br />The Piano results were similar to the small synthetic<br />dataset. The BNMF converged much faster than the<br />IBP models, and the MEIBP performed best among<br />the IBP models in terms of runtime and test log-<br />likelihood—it converged to a similar solution as the<br />AIBP in one-third the time. Though UGibbs has the<br />best per-iteration complexity, it got stuck in poor local<br />optima when randomly initialized. The VIBP meth-<br />ods and MEIBP expressed large uncertainty about the<br />latent factors early on and overcame these poor lo-<br />cal optima. By using hard latent feature assignments,<br />the MEIBP took larger steps in the inference space<br />than the VIBP methods, which was beneficial for this<br />dataset, and achieved similar results to the AIBP.<br />MEIBP inference performed comparable to the best<br />IBP sampling technique for the sparse binary Flickr<br />dataset and converged over an order of magnitude<br />faster. Surprisingly, the dense BNMF inference per-<br />formed very well on this dataset even though the<br />dataset was sparse and binary.<br />verged slower than the MEIBP because it inferred a<br />sparse matrix from a dense prior, which took over four<br />times as many iterations to converge compared to the<br />dense datasets. While the t-AIBP converged to a bet-<br />ter solution than the MEIBP, it took over an order-<br />The BNMF con-</p>  <p>Page 8</p> <p>Scaling the Indian Buffet Process via Submodular Maximization<br />101<br />102<br />103<br />104<br />105<br />−4<br />−3.5<br />−3<br />−2.5<br />·105<br />convergence time (s)<br />test log-likelihood<br />Piano<br />meibp<br />ugibbs<br />aibp<br />bnmf<br />f-vibp<br />i-vibp<br />103<br />104<br />105<br />−4.95<br />−4.9<br />−4.85<br />·106<br />meibp+aibp<br />convergence time (s)<br />Flickr<br />103<br />104<br />105<br />−2.8<br />−2.7<br />−2.6<br />·106<br />convergence time (s)<br />Yale-BC<br />Figure 4. Inference results on real-world datasets. The size of the marker indicates the K value for K = {10,25,50},<br />with larger markers indicating a larger K. AIBP and UGibbs also include a larger faded marker that shows the inference<br />results for unbounded K. The Flickr plot also shows the result of initializing the AIBP using the MEIBP result. The<br />error-bars indicate the standard deviation of convergence time and test log-likelihood over five random restarts.<br />of-magnitude longer to surpass the MEIBP’s perfor-<br />mance. As we demonstrate with the Flickr results, ini-<br />tializing the AIBP with the MEIBP outcome obtained<br />a similar solution in a fraction of the time (indicated<br />as “meibp+aibp” on the figure).<br />The MEIBP converged faster than the other IBP<br />methods for the Yale-BC dataset but to a lower test<br />likelihood. The UGibbs and BNMF also experienced<br />difficulty for this dataset, where BNMF converged to<br />a test log-likelihood around −3.6 × 106(not visible<br />in the figure). These linear-Gaussian models with<br />nonnegative priors performed worse than the models<br />with Gaussian priors because the dataset contained<br />many images with dark shadows covering part of the<br />face.The nonnegative priors appeared to struggle<br />with reconstructing these shadows, because unlike the<br />Gaussian priors, they could not infer negative-valued<br />“shadow” factors that obscured part of the image.<br />In the above experiments, the MEIBP consistently ex-<br />hibited a sudden convergence whereby it obtained a lo-<br />cal optima and the ls-algorithm did not change any Z<br />assignments. This is a characteristic of using hard as-<br />signments with a greedy algorithm: at a certain point,<br />changing any latent feature assignments decreased the<br />objective function. This abrupt convergence, in combi-<br />nation with the speed of the ls-algorithm, helped the<br />MEIBP consistently converge faster than other IBP<br />methods. Furthermore, the submodular maximization<br />algorithm converged to local optima that were compa-<br />rable or better than the sampling or variational results,<br />though at the cost of only obtaining a MAP solution.<br />Like the variational methods, it maintained a distri-<br />bution over A that prevented it from getting stuck in<br />local optima early on, and like the sampling methods,<br />the MEIBP used hard Z assignments to take larger<br />steps in the inference space and obtain better optima.<br />6. Summary and Future Work<br />We presented a new inference technique for IBP mod-<br />els that used Kurihara &amp; Welling (2008)’s ME frame-<br />work to perform approximate MAP inference via<br />submodular maximization.<br />exploit the submodularity inherent in the evidence<br />lower bound formulated in §3, which arose from the<br />quadratic pseudo-Boolean component of the linear-<br />Gaussian model. MEIBP inference converged faster<br />than competing IBP methods and obtained compara-<br />ble solutions on various datasets.<br />Our key insight was to<br />There are many discrete Bayesian nonparametric pri-<br />ors, such as the Dirichlet process, and an interest-<br />ing area for future research will be to generalize our<br />results in order to phrase inference with these pri-<br />ors as submodular optimization problems. Further-<br />more, we used a simple local-search algorithm to<br />obtain a<br />3-approximation bound, but concurrently<br />with this work, Buchbinder et al. (2012) proposed a<br />simpler stochastic algorithm for unconstrained sub-<br />modular maximization that obtains an expected<br />approximation bound. Using this algorithm, MEIBP<br />inference has an improved worst case complexity of<br />O(NK2<br />+D). We will investigate this algorithm in an<br />extended technical version of this paper.<br />1<br />1<br />2-<br />Code:<br />available at https://github.com/cjrd/MEIBP.<br />A MATLAB implementation of MEIBP is<br />Acknowledgements: CR was supported by the Win-<br />ston Churchill Foundation of the United States, and</p>  <p>Page 9</p> <p>Scaling the Indian Buffet Process via Submodular Maximization<br />ZG was supported by EPSRC grant EP/I036575/1<br />and grants from Google and Microsoft. We thank the<br />anonymous reviewers for their helpful comments.<br />References<br />Attias, H. A variational bayesian framework for graph-<br />ical models. Advances in Neural Information Pro-<br />cessing Systems, 12:209–215, 2000.<br />Broderick, T., Jordan, M. I., and Pitman, J. Clus-<br />ters and features from combinatorial stochastic pro-<br />cesses. arXiv:1206.5862, 2012.<br />Buchbinder, N., Feldman, M., Naor, J., and Schwartz,<br />R. A tight linear time (1/2)-approximation for un-<br />constrained submodular maximization. In 53rd An-<br />nual Symposium on Foundations of Computer Sci-<br />ence, pp. 649–658. IEEE, 2012.<br />Ding, N., Qi, Y.A., Xiang, R., Molloy, I., and Li,<br />N. Nonparametric Bayesian matrix factorization by<br />Power-EP. In 14th Int’l Conf. on AISTATS, vol-<br />ume 9, pp. 169–176, 2010.<br />Doshi-Velez, F. and Ghahramani, Z. Accelerated sam-<br />pling for the Indian buffet process. In Proceedings of<br />the 26th Annual Int’l Conference on Machine Learn-<br />ing, pp. 273–280, 2009.<br />Doshi-Velez, F., Miller, K. T., Van Gael, J., and Teh,<br />Y. W. Variational inference for the Indian buffet<br />process. In 13th Int’l Conf. on AISTATS, pp. 137–<br />144, 2009.<br />Feige, U., Mirrokni, V.S., and Vondrak, J. Maximizing<br />non-monotone submodular functions. SIAM Jour-<br />nal on Computing, 40(4):1133–1153, 2011.<br />Fujishige, S. Submodular functions and optimization,<br />volume 58. Elsevier Science Limited, 2005.<br />Ghahramani, Z. and Beal, M.J.<br />rithms for variational bayesian learning.<br />vances in Neural Information Processing Systems,<br />volume 13, 2001.<br />Propagation algo-<br />In Ad-<br />Griffiths, T. and Ghahramani, Z. Infinite latent fea-<br />ture models and the Indian buffet process. Technical<br />report, Gatsby Unit, UCL, London, UK, 2005.<br />Griffiths, T. L. and Ghahramani, Z. Infinite latent fea-<br />ture models and the Indian buffet process. In Ad-<br />vances in Neural Information Processing Systems,<br />volume 18, 2006.<br />Knowles, D. and Ghahramani, Z. Infinite sparse factor<br />analysis and infinite independent components anal-<br />ysis. Independent Component Analysis and Signal<br />Separation, pp. 381–388, 2007.<br />Kollar, T. and Roy, N. Utilizing object-object and<br />object-scene context when planning to find things.<br />In IEEE International Conference on Robotics and<br />Automation, pp. 2168 –2173, 2009.<br />Kurihara, K. and Welling, M. Bayesian k-means as a<br />maximization-expectation algorithm. Neural Com-<br />putation, 21(4):1145–1172, 2008.<br />Lee, K.C., Ho, J., and Kriegman, D. Acquiring linear<br />subspaces for face recognition under variable light-<br />ing. IEEE Trans. Pattern Anal. Mach. Intelligence,<br />27(5):684–698, 2005.<br />Lov´ asz, L. Submodular functions and convexity. Math-<br />ematical programming: the state of the art, pp. 235–<br />257, 1983.<br />Papaspiliopoulos, O. and Roberts, G. O. Retrospec-<br />tive Markov chain Monte Carlo methods for Dirich-<br />let process hierarchical models. Biometrika, 95(1):<br />169–186, 2008.<br />Poliner, G.E. and Ellis, D.P.W.<br />tive model for polyphonic piano transcription.<br />EURASIP Journal on Advances in Signal Process-<br />ing, 2006.<br />A discrimina-<br />Rai, P. and Daume III, H. Beam search based map<br />estimates for the Indian buffet process. In Proceed-<br />ings of the 28th Annual Int’l Conference on Machine<br />Learning, 2011.<br />Schmidt, M. N., Winther, O., and Hansen, L. K.<br />Bayesian non-negative matrix factorization.<br />Int’l Conference on Independent Component Anal-<br />ysis and Signal Separation, volume 5441 of Lecture<br />Notes in Computer Science (LNCS), pp. 540–547.<br />Springer, 2009.<br />In<br />Teh, Y.W., Gorur, D., and Ghahramani, Z. Stick-<br />breaking construction for the indian buffet process.<br />In Int’l Conference on AISTATS, volume 11, 2007.<br />Wang, C. and Blei, D. Truncation-free online varia-<br />tional inference for bayesian nonparametric models.<br />In Advances in Neural Information Processing Sys-<br />tems, volume 25, 2012.</p>  <p>Page 10</p> <p>Scaling the Indian Buffet Process via Submodular Maximization<br />Supplementary Material<br />S.1. Truncated Gaussian Properties<br />In the main text we examined a truncated Gaussian<br />of the form:<br />TN(˜ µkd, ˜ σ2<br />kd) =<br />2<br />erfc<br />?<br />−<br />˜ µkd<br />˜ σkd<br />√2<br />?N(˜ µkd, ˜ σ2<br />kd) (23)<br />with N representing a Gaussian distribution. The first<br />two moments of TN(˜ µkd, ˜ σ2<br />kd) are:<br />?2/π<br />erfcx(℘kd)<br />E[akd] = ˜ µkd+ ˜ σkd<br />(24)<br />E?a2<br />kd<br />?= ˜ µ2<br />kd+ ˜ σ2<br />kd+ ˜ σkd˜ µkd<br />?2/π<br />erfcx(℘kd)<br />(25)<br />with ℘kd = −<br />representing the scaled complementary error function.<br />The entropy is<br />˜ µkd<br />˜ σkd<br />√2and erfcx(y) = ey2(1 − erf(y))<br />H(q(akd)) =1<br />2lnπe˜ σ2<br />kd<br />2<br />?<br />+ lnerfc<br />?<br />?<br />−<br />˜ µkd<br />˜ σkd<br />˜ µkd<br />˜ σkd<br />√2<br />?<br />??−1<br />(26)<br />+˜ µkd<br />˜ σkd<br />1<br />2π<br />?<br />erfcx<br />−<br />√2<br />.<br />(27)<br />S.2. Shifted Equivalence Classes<br />Here we discuss the “shifted” equivalence class of bi-<br />nary matrices first proposed by Ding et al. (2010). For<br />a given N ×K binary matrix Z, the equivalence class<br />for this binary matrix [Z] is obtained by shifting all-<br />zero columns to the right of the non-zero columns while<br />maintaining the non-zero column orderings, see Fig-<br />ure 5. Placing independent Beta(α<br />Bernoulli entries of Z and integrating over these pri-<br />ors yields the following probability for Z, see Eq. 27<br />in Griffiths &amp; Ghahramani (2005):<br />K,1) priors on the<br />P(Z) =<br />K<br />?<br />k=1<br />α<br />KΓ(mk+α<br />K)Γ(N − mk+ 1)<br />Γ(N + 1 +α<br />K)<br />(28)<br />where mk =<br />P(Z) = 0 for all Z. However, the probability of cer-<br />tain equivalence classes of binary matrices, P([Z]), can<br />remain non-zero as K → ∞. Specifically, Griffiths &amp;<br />Ghahramani (2005) show P([Z]) remains non-zero for<br />the “left-ordered form” equivalence class of binary ma-<br />trices, whereby the columns of Z are ordered such that<br />the binary values of the columns are non-increasing,<br />where the first row is the most significant bit. Here<br />?N<br />n=1znk. Letting K → ∞ yields<br />we outline a similar result for the shifted equivalence<br />class.1<br />We obtain the probability of the shifted equivalence<br />class by multiplying the multiplicity of the equivalence<br />class by the probability of a matrix within the class.<br />For a given matrix with K columns and K+non-zero<br />columns, each shifted equivalence class has?K<br />?K<br />k=1<br />K+<br />?ma-<br />trices that map to it, yielding:<br />P ([Z]) =<br />K+<br />? K<br />?<br />α<br />KΓ(mk+α<br />K)Γ(N − mk+ 1)<br />Γ(N + 1 +α<br />K)<br />.<br />(29)<br />Following a similar algebraic rearrangement as Grif-<br />fiths &amp; Ghahramani (2005) Eqs. 30-33, except replac-<br />ing the<br />?2N−1<br />cause of the different equivalence class multiplicities—<br />results in:<br />K!<br />h=0<br />Kh!term with?K<br />K+<br />?—which occurs be-<br />P([Z]) =αK+<br />K+!·<br />K!<br />(K − K+)!KK+·<br />(N − mk)!?mk−1<br />?<br />N!<br />?2N−1<br />K)<br />j=1(j +α<br />K)<br />?K<br />·<br />K+<br />?<br />k=1<br />j=1(j +α<br />N!<br />. (30)<br />We then take the limit K → ∞ for each of the four<br />terms. The first term has no K dependence and does<br />not change in the infinite limit. For the second term we<br />let K0= K − K+ and have<br />in Griffiths &amp; Ghahramani (2005) show that this term<br />becomes 1 as K → ∞. The infinite limit of the third<br />and fourth terms are determined in the Appendix of<br />Griffiths &amp; Ghahramani (2005). Combining all four<br />terms together yields:<br />K!<br />K0!KK+. Equations 60-62<br />P([Z]) =αK+<br />K+!e−αHN<br />K+<br />?<br />k=1<br />(N − mk)!(mk− 1)!<br />N!<br />(31)<br />where HN is the Nthharmonic number.<br />The probability of the shifted equivalence class is<br />nearly identical to the probability of the left-ordered-<br />form equivalence class:<br />P([Z]lof) =<br />αK+<br />?2N−1<br />h=1Kh<br />e−αHN<br />K+<br />?<br />k=1<br />(N − mk)!(mk− 1)!<br />N!<br />,<br />(32)<br />where Khis the number of columns of Z with binary<br />value h ∈ {1,...,2N−1} when the first row is taken<br />1Ding et al. (2010) proposed this equivalence class but<br />did not explicitly show that it remains well defined as K →<br />∞. Furthermore, they did not discuss the collapsed case<br />where we first marginalize over the beta priors on Z.</p>  <p>Page 11</p> <p>Scaling the Indian Buffet Process via Submodular Maximization<br />shifted<br />− − − − →<br />Figure 5. Example of a binary matrix (left) and its shifted equivalence matrix (dark squares are 1, white squares are<br />0)—placing the two all-zero columns anywhere in the matrix will yield the same equivalence matrix.<br />to be the most significant bit. The only difference be-<br />tween Eq. 31 and Eq. 32 is the denominator of the first<br />fraction. For the left-ordered-form, this term penalizes<br />Z matrices with identical columns. In the feature as-<br />signment view, this term penalizes features that are<br />assigned to the exact same set of observations. The<br />K+! term in the shifted equivalence class prior does<br />not distinguish between identical and distinct columns<br />of Z, and in turn, does not penalize repeated feature<br />assignments. These two equivalence class probabilities<br />are proportional in the limit of large N as the proba-<br />bility of two columns being identical approaches 0.<br />S.3. Hyperparameter Inference<br />In the main text we assumed the hyperparameters<br />θ = {σX,σA,α} were known (i.e. estimated from the<br />data). Placing conjugate gamma hyperpriors on these<br />parameters allows for a straightforward extension in<br />which we infer their values. Formally, let<br />p(τX) = Gamma(τX;aX,bX)<br />p(τA) = Gamma(τA;aA,bA)<br />p(α) = Gamma(α;aα,bα)<br />(33)<br />(34)<br />(35)<br />where τ represents the precision, equivalent to the in-<br />verse variance<br />σ2, for the variance parameter indicated<br />in the subscript. Update equations for the variational<br />distributions follow from standard update equations<br />for variational inference in exponential families, cf. At-<br />tias (2000), and yield:<br />1<br />q(τX) = Gamma(? aX,?bX)<br />q(α) = Gamma(? aα,?bα)<br />? aA= aA+KD<br />?bA= bA+1<br />(36)<br />q(τA) = Gamma(? aA,?bA) (37)<br />(38)<br />with variance updates<br />2<br />K+<br />?<br />(39)<br />2<br />k=1<br />D<br />?<br />d=1<br />E?a2<br />kd<br />?<br />(40)<br />and<br />? aX= aX+ND<br />?bX= bX+1<br />− 2E[akd]znkxnd+ 2<br />2<br />?<br />(41)<br />2<br />N<br />n=1<br />D<br />?<br />d=1<br />?<br />x2<br />nd+<br />K+<br />?<br />k=1<br />?<br />E?a2<br />kd<br />?znk<br />(42)<br />K+<br />?<br />k?=k+1<br />znkznk?akdak?d<br />??<br />(43)<br />and q(α) updates<br />? aα= aα+ K+<br />(44)<br />?bα= bα+ HN.(45)<br />MEIBP inference is carried out exactly as discussed<br />in the main text except all instances of σX,σA, and α<br />are replaced with the expectation from their respective<br />variational distribution. Furthermore the variational<br />lower bound also has three additional entropy terms<br />for gamma distributions, one for each hyperparameter.<br />S.4. Evidence as a function of Zn·<br />As shown in the main text, we obtain a submodular<br />objective function for each Zn·, n ∈ {1,...,N} by ex-<br />amining the evidence as a function of Zn·while holding<br />constant all n?∈ {1,...,N} \ n. The evidence is<br />1<br />σ2<br />X<br />n=1<br />?<br />N<br />?<br />?<br />?<br />−1<br />2Zn·ΦΦTZT<br />n·+ Zn·ξT<br />n·<br />?<br />− lnK+!<br />+<br />K+<br />k=1<br />ln(N − mk)!(mk− 1)!<br />N!<br />+ ηk<br />?<br />+ const<br />(46)<br />ξnk= Φk·XT<br />n·+1<br />2<br />D<br />?<br />A<br />d=1<br />?E[akd]2− E[a2<br />−E[a2<br />2σ2<br />A<br />kd]?<br />(47)<br />ηk=<br />D<br />?<br />d=1<br />?<br />−ln<br />πσ2<br />2α2/D<br />2<br />kd]<br />+ H(q(akd))<br />?<br />, (48)</p>  <p>Page 12</p> <p>Scaling the Indian Buffet Process via Submodular Maximization<br />which nearly factorizes over the Zn·because the like-<br />lihood component and parts of the prior components<br />naturally fit into a quadratic function of Zn·. The<br />lnK+! and ηk only couple the rows of Z when K+<br />changes, while the log-factorial term couples the rows<br />of Z through the sums of the columns. Both of these<br />terms only depend on statistics of Z (the mk values<br />and K+), not the Z matrix itself, e.g. permuting the<br />rows of Z would not affect these terms. Furthermore,<br />lnK+and ηkhave no N dependence and become in-<br />significant as N increases. These observations, in con-<br />junction with the MEIBP performance in the exper-<br />imental section of the main text, indicate that opti-<br />mizing Eq. 46 for Zn· is a reasonable surrogate for<br />optimizing Z.<br />Here we explicitly decompose Eq. 46 to show its<br />Zn· dependency.<br />straightforward if we first define the function:<br />?<br />0, if mk\n= 0 and znk= 0.<br />Decomposing ln(N−mk)!(mk−1)!<br />N!<br />is<br />ν(znk) =<br />ln(N − mk\n− znk)!(mk\n+ znk− 1)!/N!<br />(49)<br />where the “\n” subscript indicates the variable with<br />the nthrow removed from Z. For a given n we have:<br />K+<br />?<br />k=1<br />ν(znk) =<br />K+<br />?<br />K+<br />?<br />+ ν(znk= 0),<br />k=1<br />ln(N − mk)!(mk− 1)!/N!<br />=<br />k=1<br />znk(ν(znk= 1) − ν(znk= 0))<br />(50)<br />which makes the Zn·dependency explicit and lets us<br />add ν(znk = 1) − ν(znk = 0) into the inner-product<br />term, ξn·, and place ν(znk= 0) into a constant term.<br />We can incorporate ηkinto the inner-product term in<br />a similar manner for a given n ∈ {1,...,N} :<br />K+<br />?<br />k=1<br />ηk=<br />?<br />k:mk\n&gt;0<br />ηk+<br />K+<br />?<br />k=1<br />1{mk\n=0}znkηk, (51)<br />where the first term does not depend on Zn· and is<br />added to the constant term, while the second term is<br />added to the inner-product term. Finally, for a given<br />n ∈ {1,...,N} the lnK! term becomes<br /><br />k=1<br />lnK+! = ln<br />K+\n+<br />K+<br />?<br />?<br />1{mk\n=0}znk<br />?<br /><br />!, (52)<br />where 1{·} is the indicator function.<br />the main text, combining the above terms yields<br />As stated in<br />the following submodular objective function for n =<br />1,...,N:<br />F(Zn·) = −<br />1<br />2σ2<br />X<br /><br />Zn·ΦΦTZT<br />n·+ Zn·ωT<br />n·+ const<br />− ln<br />K+\n+<br />?<br />K+<br />?<br />k=1<br />?<br />1{mk\n=0}znk<br />?<br />! (53)<br />(54)Φk·=(E[ak1],...,E[akD])<br />ωnk=1<br />σ2<br />X<br />Φk·XT<br />n·+1<br />2<br />D<br />?<br />d=1<br />?E[akd]2− E[a2<br />kd]?<br />?<br />+ ν(znk= 1) − ν(znk= 0) + 1{mk\n=0}ηk,<br />(55)<br />1{·} is the indicator function, and the subscript “\n”<br />is the value of the given variable after removing the<br />nthrow from Z.<br />S.5. Additional MEIBP Characterization<br />In this section, we will maintain a growing list of<br />additional MEIBP characterization experiments. See<br />http://arxiv.org/abs/1304.3285 for the current<br />version.<br />S.5.1. Learning K+<br />An ostensible advantage of using Bayesian nonpara-<br />metric priors is that a user does not need to specify<br />the multiplicity of the prior parameters. Clever sam-<br />pling techniques such as slice sampling and retrospec-<br />tive sampling allow samples to be drawn from these<br />nonparametric priors, c.f. Teh et al. (2007) and Pa-<br />paspiliopoulos &amp; Roberts (2008). However variational<br />methods are not directly amenable to Bayesian non-<br />parametric priors as the variational optimization can-<br />not be performed over an unbounded prior space. In-<br />stead, variational methods must specify a maximum<br />model complexity (parameter multiplicity).<br />heuristics have been proposed to address this limita-<br />tion: Wang &amp; Blei (2012) sampled from the variational<br />distribution for the local parameters—which included<br />sampling from the unbounded prior— and used the<br />empirical distributions of the local samples to update<br />the global parameters, while Ding et al. (2010) simply<br />started with K+= 1 and greedily added features. We<br />did not address these techniques in this work as the<br />MEIBP performed competitively with the unbounded<br />sampling techniques without employing these types of<br />heuristics. Furthermore, here we demonstrate that the<br />MEIBP can robustly infer the true number of latent<br />features when the K+bound is greater than the true<br />number of latent features.<br />Several</p>  <p>Page 13</p> <p>Scaling the Indian Buffet Process via Submodular Maximization<br />For this experiment we generated the binary images<br />dataset used in Griffiths &amp; Ghahramani (2005), where<br />the dataset, X, consisted of 2000 6 × 6 images. Each<br />row of X was a 36 dimensional vector of pixel inten-<br />sity values that was generated by using Z to linearly<br />combine a subset of the four binary factors shown in<br />Figure 6. Gaussian white noise, N(0,σX), was then<br />added to each image, yielding X = ZA + E. The<br />feature vectors, Zn·were sampled from a distribution<br />in which each factor was present with probability 0.5.<br />Figure 7 shows four of these images with different σX<br />values.<br />Figure 6. The four binary latent factors used in the sensi-<br />tivity analysis in this section. The white squares are ones<br />and the dark squares are zeros.<br />We initialized the MEIBP with K = 20, σX=1.0,<br />σA= 1.0, α = 2, ˜ µkd∼ |N(0,0.05)| (variational fac-<br />tor means), ˜ σkd∼ |N(0,0.1)| (variational factor stan-<br />dard deviations), znk ∼ Bernoulli(1<br />tialization, we tested the MEIBP robustness by per-<br />forming MEIBP inference on X for σX= 0.1,...,1.0<br />in 100 evenly spaced increments with all hyperparam-<br />eters and algorithm options unchanged during the ex-<br />periment. MEIBP convergence was determined in the<br />same way as the main experimental section. Figure 8<br />(top) shows a histogram of the final number of MEIBP<br />features (Ktrue= 4) and Figure 8 (bottom) shows the<br />final number of MEIBP features as a function of σX.<br />3). With this ini-<br />Figure 7. Example data used in the sensitivity analysis dis-<br />cussed in §S.5.1. Each column contains the same combina-<br />tion of latent factors, where the top row has a data noise<br />term of σX = 0.1, the middle row has σX = 0.5, and the<br />bottom row has σX = 1.0. Top: histogram of final K+<br />value. Bottom: final K+ value as a function of σX.<br />These results indicate that the regularizing nature of<br />the IBP prior tends to lead to the correct number of<br />latent features even when the K+bound is much larger<br />than the true K+. Furthermore this experiment indi-<br />cates that MEIBP inference is robust to model noise,<br />at least, for the simple data used in this experiment.<br />At a medium level of data noise, the inference occa-<br />sionally finished with K+ = 3, which resulted from<br />two true latent factors collapsing to the same inferred<br />latent feature. Once this occurred, MEIBP did not<br />have a mechanism for splitting the features. For σX<br />comparable to the latent factors, σX ≥ 0.9, MEIBP<br />often inferred “noise features,” which were essentially<br />whitenoise and were typically active for less than 4%<br />of the data instances. In future experiments we will<br />attempt to flesh out the practical differences between<br />unbounded priors and priors that operate in a large<br />bounded latent space.<br />12345678<br />0<br />20<br />40<br />60<br />K+<br />count<br />00.20.40.60.81<br />2<br />3<br />4<br />5<br />6<br />7<br />σX<br />K+<br />Figure 8. Final feature count (K+ value) for MEIBP infer-<br />ence where Ktrue = 4 for the binary image data with K+<br />initialized to 20 for σX = 0.1,...,1.0 in 100 evenly spaced<br />increments with all hyperparameters and algorithm options<br />fixed during the experiment.</p>   </div> <div id="rgw18_56ab1cac69bfa" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw19_56ab1cac69bfa">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw20_56ab1cac69bfa"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://export.arxiv.org/pdf/1304.3285" target="_blank" rel="nofollow" class="publication-viewer" title="Scaling the Indian Buffet Process via Submodular Maximization">Scaling the Indian Buffet Process via Submodular M...</a> </div>  <div class="details">   Available from <a href="http://export.arxiv.org/pdf/1304.3285" target="_blank" rel="nofollow">export.arxiv.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw27_56ab1cac69bfa" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (6) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw28_56ab1cac69bfa" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw29_56ab1cac69bfa" >  <div class="indent-left">  <div id="rgw30_56ab1cac69bfa" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/265444271_Parallel_Double_Greedy_Submodular_Maximization">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Xinghao_Pan" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Xinghao Pan </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw31_56ab1cac69bfa">  <li class="citation-context-item"> "However, many important applications in machine learning lead to non-monotone submodular functions. For example, graphical model inference [5] [17], or trading off any submodular gain maximization with costs (functions of the form F (S) = G(S) − λM (S), where G(S) is monotone submodular and M (S) a linear (modular) cost function), such as for utility-privacy tradeoffs [18], require maximizing non-monotone submodular functions. For non-monotone functions, the simple greedy algorithm in [8] can perform arbitrarily poorly (see Appendix H.1 for an example). " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Conference Paper:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/265444271_Parallel_Double_Greedy_Submodular_Maximization"> <span class="publication-title js-publication-title">Parallel Double Greedy Submodular Maximization</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/70339848_Xinghao_Pan" class="authors js-author-name ga-publications-authors">Xinghao Pan</a> &middot;     <a href="researcher/70262224_Stefanie_Jegelka" class="authors js-author-name ga-publications-authors">Stefanie Jegelka</a> &middot;     <a href="researcher/2043680896_Joseph_E_Gonzalez" class="authors js-author-name ga-publications-authors">Joseph E Gonzalez</a> &middot;     <a href="researcher/2053857884_Joseph_K_Bradley" class="authors js-author-name ga-publications-authors">Joseph K Bradley</a> &middot;     <a href="researcher/65912024_Michael_I_Jordan" class="authors js-author-name ga-publications-authors">Michael I Jordan</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Many machine learning algorithms can be reduced to the maximization of submodular functions. Although well understood in the serial setting, the parallel maximization of submodular functions remains an open area of research with recent results only addressing monotone functions. The optimal algorithm for maximizing the more general class of non-monotone submodular functions was introduced by Buchbinder et al. (2012) and follows a strongly serial double-greedy logic and program analysis. In this work, we propose two methods to parallelize the double-greedy algorithm. The first, coordination-free approach emphasizes speed at the cost of a weaker approximation guarantee. The second, concurrency control approach guarantees a tight 1/2-approximation, at the quantifiable cost of additional coordination and reduced parallelism. As a consequence we explore the tradeoff space between guaranteed performance and objective optimality. We implement and evaluate both algorithms on multi-core hardware and billion edge graphs, demonstrating both the scalability and tradeoffs of each approach. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Conference Paper &middot; Dec 2014  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Xinghao_Pan/publication/265444271_Parallel_Double_Greedy_Submodular_Maximization/links/5457acde0cf26d5090ab4e45.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw32_56ab1cac69bfa" >  <div class="indent-left">  <div id="rgw33_56ab1cac69bfa" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/264654316_Streaming_Submodular_Maximization_Massive_Data_Summarization_on_the_Fly">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Baharan_Mirzasoleiman" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Baharan Mirzasoleiman </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw34_56ab1cac69bfa">  <li class="citation-context-item"> "Over the recent years, submodular optimization has been identified as a powerful tool for numerous data mining and machine learning applications including viral marketing [17], network monitoring [22], news article recommendation [10], nonparametric learning [14] [29], document and corpus summarization [23] [7] [33], network inference [30], and Determinantal Point Processes [13]. A problem of key importance in all these applications is to maximize a monotone submodular function subject to a cardinality constraint (i.e., a bound on the number k of elements that can be selected). " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Conference Paper:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/264654316_Streaming_Submodular_Maximization_Massive_Data_Summarization_on_the_Fly"> <span class="publication-title js-publication-title">Streaming Submodular Maximization: Massive Data Summarization on the Fly</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/59370900_Ashwinkumar_Badanidiyuru" class="authors js-author-name ga-publications-authors">Ashwinkumar Badanidiyuru</a> &middot;     <a href="researcher/58006559_Baharan_Mirzasoleiman" class="authors js-author-name ga-publications-authors">Baharan Mirzasoleiman</a> &middot;     <a href="researcher/22792717_Amin_Karbasi" class="authors js-author-name ga-publications-authors">Amin Karbasi</a> &middot;     <a href="researcher/310456_Andreas_Krause" class="authors js-author-name ga-publications-authors">Andreas Krause</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> How can one summarize a massive data set &quot;on the fly&quot;, i.e., without even having seen it in its entirety? In this paper, we address the problem of extracting representative elements from a large stream of data. I.e., we would like to select a subset of say k data points from the stream that are most representative according to some objective function. Many natural notions of &quot;representativeness&quot; satisfy submodular-ity, an intuitive notion of diminishing returns. Thus, such problems can be reduced to maximizing a submodular set function subject to a cardinality constraint. Classical ap-proaches to submodular maximization require full access to the data set. We develop the first efficient streaming algo-rithm with constant factor 1/2 − ε approximation guaran-tee to the optimum solution, requiring only a single pass through the data, and memory independent of data size. In our experiments, we extensively evaluate the effectiveness of our approach on several applications, including training large-scale kernel methods and exemplar-based clustering, on millions of data points. We observe that our streaming method, while achieving practically the same utility value, runs about 100 times faster than previous work. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Conference Paper &middot; Jan 2014  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Baharan_Mirzasoleiman/publication/264654316_Streaming_Submodular_Maximization_Massive_Data_Summarization_on_the_Fly/links/53ea2f4c0cf28f342f41887e.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   "  id="rgw35_56ab1cac69bfa" >  <div class="indent-left">  <div id="rgw36_56ab1cac69bfa" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/260946406_Communication_Communities_in_MOOCs">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Stephen_Roberts9" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Stephen J. Roberts </div> </div>   </div>  </div>  <div class="indent-right">      </div>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/260946406_Communication_Communities_in_MOOCs"> <span class="publication-title js-publication-title">Communication Communities in MOOCs</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2045639709_Nabeel_Gillani" class="authors js-author-name ga-publications-authors">Nabeel Gillani</a> &middot;     <a href="researcher/49508295_Rebecca_Eynon" class="authors js-author-name ga-publications-authors">Rebecca Eynon</a> &middot;     <a href="researcher/2051939493_Michael_Osborne" class="authors js-author-name ga-publications-authors">Michael Osborne</a> &middot;     <a href="researcher/2046440867_Isis_Hjorth" class="authors js-author-name ga-publications-authors">Isis Hjorth</a> &middot;     <a href="researcher/7326333_Stephen_Roberts" class="authors js-author-name ga-publications-authors">Stephen Roberts</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Massive Open Online Courses (MOOCs) bring together thousands of people from
different geographies and demographic backgrounds -- but to date, little is
known about how they learn or communicate. We introduce a new content-analysed
MOOC dataset and use Bayesian Non-negative Matrix Factorization (BNMF) to
extract communities of learners based on the nature of their online forum
posts. We see that BNMF yields a superior probabilistic generative model for
online discussions when compared to other models, and that the communities it
learns are differentiated by their composite students&#39; demographic and course
performance indicators. These findings suggest that computationally efficient
probabilistic generative modelling of MOOCs can reveal important insights for
educational researchers and practitioners and help to develop more intelligent
and responsive online learning environments. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Mar 2014  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Stephen_Roberts9/publication/260946406_Communication_Communities_in_MOOCs/links/00b495346a7e4970c3000000.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw22_56ab1cac69bfa" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw23_56ab1cac69bfa">  </ul> </div> </div>   <div id="rgw14_56ab1cac69bfa" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw15_56ab1cac69bfa"> <div> <h5> <a href="publication/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process" class="color-inherit ga-similar-publication-title"><span class="publication-title">Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process</span></a>  </h5>  <div class="authors"> <a href="researcher/69839119_Finale_Doshi-Velez" class="authors ga-similar-publication-author">Finale Doshi-Velez</a>, <a href="researcher/54244192_David_Knowles" class="authors ga-similar-publication-author">David Knowles</a>, <a href="researcher/59315559_Shakir_Mohamed" class="authors ga-similar-publication-author">Shakir Mohamed</a>, <a href="researcher/8159937_Zoubin_Ghahramani" class="authors ga-similar-publication-author">Zoubin Ghahramani</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw16_56ab1cac69bfa"> <div> <h5> <a href="publication/220320085_The_Indian_Buffet_Process_An_Introduction_and_Review" class="color-inherit ga-similar-publication-title"><span class="publication-title">The Indian Buffet Process: An Introduction and Review</span></a>  </h5>  <div class="authors"> <a href="researcher/38420871_Thomas_L_Griffiths" class="authors ga-similar-publication-author">Thomas L. Griffiths</a>, <a href="researcher/8159937_Zoubin_Ghahramani" class="authors ga-similar-publication-author">Zoubin Ghahramani</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56ab1cac69bfa"> <div> <h5> <a href="publication/226423227_The_Phylogenetic_Indian_Buffet_Process_A_Non-Exchangeable_NonparametricPrior_for_Latent_Features" class="color-inherit ga-similar-publication-title"><span class="publication-title">The Phylogenetic Indian Buffet Process: A Non-Exchangeable Nonparametric
Prior for Latent Features</span></a>  </h5>  <div class="authors"> <a href="researcher/70879321_Kurt_T_Miller" class="authors ga-similar-publication-author">Kurt T. Miller</a>, <a href="researcher/38420871_Thomas_L_Griffiths" class="authors ga-similar-publication-author">Thomas L. Griffiths</a>, <a href="researcher/65912024_Michael_I_Jordan" class="authors ga-similar-publication-author">Michael I. Jordan</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw38_56ab1cac69bfa" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw39_56ab1cac69bfa">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw40_56ab1cac69bfa" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=J2kO1uBnuC0nKfby9AkLSO1a2A1KtUr6s-qPRir3q2h4Qv42o3-8z3uBvcKX2_kA" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="HESzZHwM7D+WaBMRwVMfUIilHrNAjjUzNO3mq+vGk7csCIdgBCaxmD/VojBfyG/qN1XyNfNJtHDn7J7pkRp2YdjyywP7NKqFSNEM4lk1qnUCCxe8NgBeYBJjf1uPE9xDRwSnj+pETAIDFA8eodim0OOy1Un3V6UgwuJvR3WiHJLvzR3Tbx3M9r9c2WFdHl13/7DRrSmADdl09NDSYOROMVj1F3CRFipEFaXgohLWcTr+WHYSWXVJmHp5zDmRdM/l5qZtcMJLAgjulPa7Xe1alc2yZND+QIzdKRiVKH8pd4Q="/> <input type="hidden" name="urlAfterLogin" value="publication/236661557_Scaling_the_Indian_Buffet_Process_via_Submodular_Maximization"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjM2NjYxNTU3X1NjYWxpbmdfdGhlX0luZGlhbl9CdWZmZXRfUHJvY2Vzc192aWFfU3VibW9kdWxhcl9NYXhpbWl6YXRpb24%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjM2NjYxNTU3X1NjYWxpbmdfdGhlX0luZGlhbl9CdWZmZXRfUHJvY2Vzc192aWFfU3VibW9kdWxhcl9NYXhpbWl6YXRpb24%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjM2NjYxNTU3X1NjYWxpbmdfdGhlX0luZGlhbl9CdWZmZXRfUHJvY2Vzc192aWFfU3VibW9kdWxhcl9NYXhpbWl6YXRpb24%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw41_56ab1cac69bfa"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 648;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"profileUrl":"researcher\/2008466791_Colorado_Reed","fullname":"Colorado Reed","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2549355721578\/images\/template\/default\/profile\/profile_default_m.png","profileStats":[null,{"data":{"publicationCount":1,"widgetId":"rgw5_56ab1cac69bfa"},"id":"rgw5_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorPublicationCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorPublicationCount.html?authorUid=2008466791","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},null],"widgetId":"rgw4_56ab1cac69bfa"},"id":"rgw4_56ab1cac69bfa","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorBadge.html?authorUid=2008466791","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw3_56ab1cac69bfa"},"id":"rgw3_56ab1cac69bfa","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=236661557","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":236661557,"title":"Scaling the Indian Buffet Process via Submodular Maximization","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"30th International Conference on Machine Learning, ICML 2013","publicationDate":"04\/2013;","publicationDateRobot":"2013-04","article":""}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1304.3285","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Scaling the Indian Buffet Process via Submodular Maximization"},{"key":"rft.title","value":"30th International Conference on Machine Learning, ICML 2013"},{"key":"rft.jtitle","value":"30th International Conference on Machine Learning, ICML 2013"},{"key":"rft.date","value":"2013"},{"key":"rft.au","value":"Colorado Reed,Zoubin Ghahramani"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56ab1cac69bfa"},"id":"rgw7_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=236661557","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":236661557,"peopleItems":[{"data":{"authorUrl":"researcher\/2008466791_Colorado_Reed","authorNameOnPublication":"Colorado Reed","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Colorado Reed","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2008466791_Colorado_Reed","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw10_56ab1cac69bfa"},"id":"rgw10_56ab1cac69bfa","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2008466791&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw9_56ab1cac69bfa"},"id":"rgw9_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2008466791&authorNameOnPublication=Colorado%20Reed","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/8159937_Zoubin_Ghahramani","authorNameOnPublication":"Zoubin Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Zoubin Ghahramani","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/8159937_Zoubin_Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56ab1cac69bfa"},"id":"rgw12_56ab1cac69bfa","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=8159937&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56ab1cac69bfa"},"id":"rgw11_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=8159937&authorNameOnPublication=Zoubin%20Ghahramani","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab1cac69bfa"},"id":"rgw8_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=236661557&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":236661557,"abstract":"<noscript><\/noscript><div>Inference for latent feature models is inherently difficult as the inference<br \/>\nspace grows exponentially with the size of the input data and number of latent<br \/>\nfeatures. In this work, we use Kurihara &amp; Welling (2008)'s<br \/>\nmaximization-expectation framework to perform approximate MAP inference for<br \/>\nlinear-Gaussian latent feature models with an Indian Buffet Process (IBP)<br \/>\nprior. This formulation yields a submodular function of the features that<br \/>\ncorresponds to a lower bound on the model evidence. By adding a constant to<br \/>\nthis function, we obtain a nonnegative submodular function that can be<br \/>\nmaximized via a greedy algorithm that obtains at least a one-third<br \/>\napproximation to the optimal solution. Our inference method scales linearly<br \/>\nwith the size of the input data, and we show the efficacy of our method on the<br \/>\nlargest datasets currently analyzed using an IBP model.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw13_56ab1cac69bfa"},"id":"rgw13_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=236661557","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/236661557_Scaling_the_Indian_Buffet_Process_via_Submodular_Maximization\/links\/034fa80a0cf2ac15472e918a\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw6_56ab1cac69bfa"},"id":"rgw6_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=236661557&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":69839119,"url":"researcher\/69839119_Finale_Doshi-Velez","fullname":"Finale Doshi-Velez","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":54244192,"url":"researcher\/54244192_David_Knowles","fullname":"David Knowles","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":59315559,"url":"researcher\/59315559_Shakir_Mohamed","fullname":"Shakir Mohamed","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Conference Paper","publicationDate":"Dec 2009","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process","usePlainButton":true,"publicationUid":221620392,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process","title":"Large Scale Nonparametric Bayesian Inference: Data Parallelisation in the Indian Buffet Process","displayTitleAsLink":true,"authors":[{"id":69839119,"url":"researcher\/69839119_Finale_Doshi-Velez","fullname":"Finale Doshi-Velez","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":54244192,"url":"researcher\/54244192_David_Knowles","fullname":"David Knowles","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":59315559,"url":"researcher\/59315559_Shakir_Mohamed","fullname":"Shakir Mohamed","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Processing Systems 2009. Proceedings of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada.; 12\/2009"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/221620392_Large_Scale_Nonparametric_Bayesian_Inference_Data_Parallelisation_in_the_Indian_Buffet_Process\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw15_56ab1cac69bfa"},"id":"rgw15_56ab1cac69bfa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=221620392","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":38420871,"url":"researcher\/38420871_Thomas_L_Griffiths","fullname":"Thomas L. Griffiths","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Apr 2011","journal":"Journal of Machine Learning Research","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/220320085_The_Indian_Buffet_Process_An_Introduction_and_Review","usePlainButton":true,"publicationUid":220320085,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.47","url":"publication\/220320085_The_Indian_Buffet_Process_An_Introduction_and_Review","title":"The Indian Buffet Process: An Introduction and Review","displayTitleAsLink":true,"authors":[{"id":38420871,"url":"researcher\/38420871_Thomas_L_Griffiths","fullname":"Thomas L. Griffiths","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of Machine Learning Research 04\/2011; 12:1185-1224."],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/220320085_The_Indian_Buffet_Process_An_Introduction_and_Review","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/220320085_The_Indian_Buffet_Process_An_Introduction_and_Review\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56ab1cac69bfa"},"id":"rgw16_56ab1cac69bfa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=220320085","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":70879321,"url":"researcher\/70879321_Kurt_T_Miller","fullname":"Kurt T. Miller","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38420871,"url":"researcher\/38420871_Thomas_L_Griffiths","fullname":"Thomas L. Griffiths","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":65912024,"url":"researcher\/65912024_Michael_I_Jordan","fullname":"Michael I. Jordan","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jun 2012","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/226423227_The_Phylogenetic_Indian_Buffet_Process_A_Non-Exchangeable_NonparametricPrior_for_Latent_Features","usePlainButton":true,"publicationUid":226423227,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/226423227_The_Phylogenetic_Indian_Buffet_Process_A_Non-Exchangeable_NonparametricPrior_for_Latent_Features","title":"The Phylogenetic Indian Buffet Process: A Non-Exchangeable Nonparametric\nPrior for Latent Features","displayTitleAsLink":true,"authors":[{"id":70879321,"url":"researcher\/70879321_Kurt_T_Miller","fullname":"Kurt T. Miller","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38420871,"url":"researcher\/38420871_Thomas_L_Griffiths","fullname":"Thomas L. Griffiths","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":65912024,"url":"researcher\/65912024_Michael_I_Jordan","fullname":"Michael I. Jordan","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/226423227_The_Phylogenetic_Indian_Buffet_Process_A_Non-Exchangeable_NonparametricPrior_for_Latent_Features","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/226423227_The_Phylogenetic_Indian_Buffet_Process_A_Non-Exchangeable_NonparametricPrior_for_Latent_Features\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab1cac69bfa"},"id":"rgw17_56ab1cac69bfa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=226423227","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw14_56ab1cac69bfa"},"id":"rgw14_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=236661557&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":236661557,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":236661557,"publicationType":"article","linkId":"034fa80a0cf2ac15472e918a","fileName":"Scaling the Indian Buffet Process via Submodular Maximization","fileUrl":"http:\/\/export.arxiv.org\/pdf\/1304.3285","name":"export.arxiv.org","nameUrl":"http:\/\/export.arxiv.org\/pdf\/1304.3285","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw20_56ab1cac69bfa"},"id":"rgw20_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=236661557&linkId=034fa80a0cf2ac15472e918a&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw19_56ab1cac69bfa"},"id":"rgw19_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=236661557&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":0,"valueFormatted":"0","widgetId":"rgw21_56ab1cac69bfa"},"id":"rgw21_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=236661557","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw18_56ab1cac69bfa"},"id":"rgw18_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=236661557&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":236661557,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw23_56ab1cac69bfa"},"id":"rgw23_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=236661557&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":0,"valueFormatted":"0","widgetId":"rgw24_56ab1cac69bfa"},"id":"rgw24_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=236661557","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw22_56ab1cac69bfa"},"id":"rgw22_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=236661557&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Scaling the Indian Buffet Process via Submodular Maximization\nColorado Reed\nZoubin Ghahramani\nEngineering Department, Cambridge University, Cambridge UK\ncr478@cam.ac.uk\nzoubin@eng.cam.ac.uk\nAbstract\nInference for latent feature models is inher-\nently difficult as the inference space grows\nexponentially with the size of the input data\nand number of latent features.\nwork, we use Kurihara & Welling (2008)\u2019s\nmaximization-expectation framework to per-\nform approximate MAP inference for linear-\nGaussian latent feature models with an In-\ndian Buffet Process (IBP) prior. This for-\nmulation yields a submodular function of the\nfeatures that corresponds to a lower bound on\nthe model evidence. By adding a constant to\nthis function, we obtain a nonnegative sub-\nmodular function that can be maximized via\na greedy algorithm that obtains at least a1\napproximation to the optimal solution. Our\ninference method scales linearly with the size\nof the input data, and we show the efficacy of\nour method on the largest datasets currently\nanalyzed using an IBP model.\nIn this\n3-\n1. Introduction\nNonparametric latent feature models experienced a\nsurge of interest in the machine learning community\nfollowing Griffiths & Ghahramani (2006)\u2019s formula-\ntion of the Indian Buffet Process (IBP)\u2014a nonpara-\nmetric prior for equivalence classes of sparse binary\nmatrices. These binary matrices have a finite num-\nber of exchangeable rows and an unbounded number\nof columns, where a 1 in row n and column k indicates\nthat observation n expresses latent feature k. For ex-\nample, given an image dataset of human faces, each\nobservation is an image, and the latent features might\nbe \u201cis smiling,\u201d \u201cis wearing glasses,\u201d etc. More gener-\nally, feature models can be viewed as a generalization\nof unsupervised clustering, see Broderick et al. (2012).\nProceedings of the 30thInternational Conference on Ma-\nchine Learning, Atlanta, Georgia, USA, 2013.\nW&CP volume 28. Copyright 2013 by the author(s).\nJMLR:\nThe IBP prior is often used in sparse matrix factoriza-\ntion models where a data matrix of N D-dimensional\nobservations is expressed as a product of two matrices\nthat factor over K latent factors plus a noise term:\nX = ZA + E. Formally, this model has a binary fea-\nture matrix Z \u2208 {0,1}N\u00d7Kthat linearly combines a\nlatent factor matrix A \u2208 RK\u00d7Dplus a noise matrix\nE \u2208 RN\u00d7Dto form the observed data X \u2208 RN\u00d7D.\nPlacing an IBP prior on Z lets K be unbounded and\nallows the number of active features K+(those with\nnon-zero Z column sums) to be learned from the data\nwhile remaining finite with probability one. The IBP\ninspired several infinite-limit versions of classic matrix\nfactorization models, e.g. infinite independent compo-\nnent analysis models (Knowles & Ghahramani, 2007).\nInference with IBP models is challenging as its discrete\nstate space has 2NK+possible assignments. In turn,\nthe IBP has found limited application to large data in\ncomparison to the Chinese Restaurant Process, which\nassigns one feature to each observation. In this paper,\nwe use Kurihara & Welling (2008)\u2019s Maximization-\nExpectation (ME) framework to perform approximate\nMAP inference with IBP matrix factorization models,\ntermed MEIBP inference. For nonnegative A, we show\nthat we can obtain approximate MAP solutions for Z\nby maximizing N submodular cost functions. The sub-\nmodularity property enables the use of a simple greedy\nalgorithm that obtains at least a\nthe optimal solution. While the worst-case complex-\nity of MEIBP inference is comparable to sampling and\nvariational approaches, in \u00a75 we show that MEIBP in-\nference often converges to better solutions than varia-\ntional methods and similar solutions as the best sam-\npling techniques but in a fraction of the time.\n1\n3-approximation to\nThis paper is structured as follows: in \u00a72 we present\nbackground material that sets the foundation for our\npresentation of MEIBP inference in \u00a73 and the re-\nsulting submodular maximization problem that arises,\nthen in \u00a74 we discuss related work, and in \u00a75 we com-\npare the MEIBP with other IBP inference techniques\nusing both synthetic and real-world datasets.\narXiv:1304.3285v4  [stat.ML]  24 Jul 2013"},{"page":2,"text":"Scaling the Indian Buffet Process via Submodular Maximization\n2. Background\n2.1. The Indian Buffet Process\nGriffiths & Ghahramani (2006) derived the IBP prior\nby placing independent beta priors on Bernoulli gener-\nated entries of an N \u00d7K binary matrix Z, marginaliz-\ning over the beta priors, and letting K go to inifinity.\nIn this infinite limit, however, P(Z) is zero for any\nparticular Z. Griffiths & Ghahramani (2006) there-\nfore take the limit of an equivalence classes of binary\nmatrices, [Z], defined by the \u201cleft-order form\u201d (lof )\nordering of the columns and show that P([Z]lof) has a\nnon-zero probability as K goes to infinity.\nThe lof ordering arranges the columns of Z such that\nthe binary values of the columns are non-increasing,\nwhere the first row is the most significant bit. Ding\net al. (2010) examine different \u201cshifted\u201d equivalence\nclasses formed by shifting all-zero columns to the right\nof non-zero columns while maintaining the non-zero\ncolumn ordering.Given K+ non-zero columns, the\nIBP prior for the shifted equivalence classes is\nP([Z]|\u03b1) =\u03b1K+\nK+!e\u2212\u03b1HN\nK+\n?\nk=1\n(N \u2212 mk)!(mk\u2212 1)!\nN!\n(1)\nwhere \u03b1 is a hyperparameter, HNis the Nthharmonic\nnumber, and mk =\nmaterial has a derivation of P([Z]|\u03b1) as well as a com-\nparison to the lof equivalence classes. The derivations\nin \u00a73 can be applied using either equivalence class.\nHowever, the shifted equivalence classes simplify the\nmathematics and produce the same results in practice.\n?N\nn=1znk. The supplementary\n2.2. Maximization-Expectation\nKurihara & Welling (2008) presented the ME algo-\nrithm:an inference algorithm that exchanges the\nexpectation and maximization variables in the EM\nalgorithm. Consider a general probabilistic model\np(X,Z,A), where X are the observed random vari-\nables (RVs), Z are the local latent RVs, and A are the\nglobal latent RVs. RVs are qualified as \u201clocal\u201d if there\nis one RV for each observation, and RVs are \u201cglobal\u201d if\ntheir multiplicity is constant or inferred from the data.\nME can be viewed as a special case of a Mean-Field\nVariational Bayes (MFVB) approximation to a poste-\nrior that cannot be computed analytically, p(Z,A|X).\nMFVB operates by approximating the posterior dis-\ntribution of a given probabilistic model by assuming\nindependent variational distributions, p(Z,A|X) \u2248\nq(Z)q(A) (Attias, 2000; Ghahramani & Beal, 2001).\nThe independence constraint lets us compute the vari-\national distribution q that minimizes the KL diver-\ngence between the variational distribution and true\nposterior. Without this constraint, the distribution\nthat minimizes the KL-divergence is the true posterior,\nreturning us to our original problem. In MFVB, we de-\ntermine the variational distributions and their param-\neters using coordinate ascent optimization in which we\niteratively update:\nq(Z) \u221d exp E\nq(A) \u221d exp E\nq(A)[lnp(X,Z,A)](2)\nq(Z)[lnp(X,Z,A)], (3)\nwhich commonly has closed-form solutions.\nThe EM algorithm can be viewed as a special case\nof MFVB that obtains MAP values of the global RVs\nby letting q(A) = \u03b4(A \u2212 A\u2217), where \u03b4(\u00b7) is the delta\nfunction and A\u2217is the MAP assignment. The ME al-\ngorithm instead maximizes the local RVs Z and com-\nputes the expectation over the global RVs A, which\ncan be viewed as MFVB with q(Z) = \u03b4(Z \u2212 Z\u2217).\nIn the limit of large N, the ME algorithm recovers\na Bayesian information criterion regularization term\n(Kurihara & Welling, 2008). Also, maintaining a vari-\national distribution over the global RVs retains the\nmodel selection ability of MFVB, while using point es-\ntimates of the local RVs allows the use of efficient data\nstructures and optimization techniques.\nshow, the ME algorithm leads to a scalable submodu-\nlar optimization problem for latent feature models.\nAs we will\n2.3. Submodularity\nSubmodularity is a set function property that makes\noptimization of the function tractable or approx-\nimable. Given ground set V and set function f : 2V\u2192\nR, f is submodular if for all A \u2286 B \u2286 V and e \u2208 V \\B:\nf(A \u222a {e}) \u2212 f(A) \u2265 f(B \u222a {e}) \u2212 f(B),\nwhich expresses a \u201cdiminishing returns\u201d property,\nwhere the incremental benefit of element e diminishes\nas we include it in larger solution sets. Submodularity\nis desirable in discrete optimization because submod-\nular functions are discrete analogs of convex functions\nand can be globally minimized in polynomial time\n(Lov\u00b4 asz, 1983).However, global submodular maxi-\nmization is NP-hard, but submodularity often enables\napproximation bounds via greedy algorithms. In the\nnext section, we show that determining a MAP esti-\nmate of Z in the ME algorithm is a scalable submod-\nular maximization problem.\n(4)\n3. Maximization-Expectation IBP\nHere we present the ME algorithm for nonnegative\nlinear-Gaussian IBP models and show that approxi-"},{"page":3,"text":"Scaling the Indian Buffet Process via Submodular Maximization\nmate MAP inference arises as a submodular maximiza-\ntion problem. Boldface variables are matrices with\n(row, column) subscripts; a dot indicates all elements\nof the dimension, and lowercase variables are scalars.\n3.1. Nonnegative Linear-Gaussian IBP Model\nWe consider the following probabilistic model:\np(X,Z,A|\u03b8) = p(X|Z,A,\u03c32\nX)p(A|\u03c32\nA)p(Z|\u03b1) (5)\np(X|Z,A,\u03c32\nA) =\nN\n?\nK\n?\nn=1\nN(Xn\u00b7;Zi\u00b7A,\u03c32\nAI) (6)\np(A|0,\u03c32\nA) =\nk=1\nD\n?\nd=1\nTN(akd;0,\u03c32\nA) (7)\nwith p([Z]|\u03b1) specified in Eq. 1. This is a nonnegative\nlinear-Gaussian IBP model, where the prior over the\nlatent factors, p(A|0,\u03c32\ncated Gaussian with nonnegative support, denoted\nTN. As we show below, this nonnegative prior yields\na submodular maximization problem when optimizing\nZ. We use a truncated Gaussian as it is conjugate to\nthe Gaussian likelihood, but other nonnegative priors\n(e.g. exponential) can be used. For brevity we assume\nthe hyperparameters, \u03b8 = {\u03b1,\u03c32\ndiscuss \u03b8 inference in the supplementary material.\nA), is a zero-mean i.i.d. trun-\nA,\u03c32\nX}, are known and\n3.2. MEIBP Evidence\nIn the ME framework, we approximate the true pos-\nterior distribution via a MFVB assumption:\np(Z,A|X,\u03b8) \u2248 q(A)\u03b4(Z \u2212 Z\u2217).\nThat is, we maintain a variational distribution over the\nlatent factors A and optimize the latent features Z.\nGiven the MFVB constraint, we determine the varia-\ntional distributions by minimizing the KL-divergence\nbetween the variational distributions and the true\nposterior, which is equivalent to maximizing a lower\nbound on the evidence (Attias, 2000):\n(8)\nlnp(X|\u03b8) = E\nq[lnp(X,A,Z|\u03b8)] + H[q] + D(q?p)\n\u2265 E\nwhere H[q] is the entropy of q and D(q?p) represents\nthe KL-divergence between the variational distribution\nand the true posterior. The evidence lower bound, F,\nfor the nonnegative linear-Gaussian IBP model is:\n?\nK+\n?\nq[lnp(X,A,Z|\u03b8)] + H[q] \u2261 F\n(9)\n1\n\u03c32\nX\nN\n?\nn=1\n\u22121\n2Zn\u00b7\u03a6\u03a6TZT\nn\u00b7+ Zn\u00b7\u03beT\nn\u00b7\n?\n\u2212 lnK+!\n+\nk=1\n?\nln(N \u2212 mk)!(mk\u2212 1)!\nN!\n+ \u03b7k\n?\n+ const(10)\nwith\n\u03benk= \u03a6k\u00b7XT\nn\u00b7+1\n2\nD\n?\nd=1\n?E[akd]2\u2212 E[a2\nkd]?\n(11)\nand\n\u03b7k=\nD\n?\nd=1\n?\n\u2212ln\n\u03c0\u03c32\n2\u03b12\/D\n2\nA\n\u2212E[a2\nkd]\n2\u03c32\nA\n+ H(q(akd))\n?\n(12)\nwhere \u03a6k\u00b7 = (E[ak1],...,E[akD]), and all expecta-\ntions are with respect to q(A), which is defined in the\nnext subsection. In \u00a73.5 we show that maximizing this\nlower bound with respect to Z can be formulated as a\nsubmodular maximization problem.\n3.3. Variational Factor Updates\nMaximizing Eq. 10 with respect to q(A) yields\nq(A) =\nK\n?\nk=1\nD\n?\nd=1\nTN(akd; \u02dc \u00b5kd, \u02dc \u03c32\nkd), (13)\nwith parameter updates\n\u02dc \u00b5kd= \u03c1k\nN\n?\nX,\nn=1\nznk\n?\nxnd\u2212\n?\nk??=k\nznk? E[ak?d]\n?\n(14)\n\u02dc \u03c32\nkd= \u03c1k\u03c32\n(15)\nwhere \u03c1k =\nO(NK2D), and the relevant moments are:\n?mk +\n\u03c32\n\u03c32\nX\nA\n?\u22121.\n?2\/\u03c0\nerfcx(\u2118kd)\nThese updates take\nE[akd] = \u02dc \u00b5kd+ \u02dc \u03c3kd\n(16)\nE?a2\nkd\n?= \u02dc \u00b52\nkd+ \u02dc \u03c32\nkd+ \u02dc \u03c3kd\u02dc \u00b5kd\n?2\/\u03c0\nerfcx(\u2118kd)\n(17)\nwith \u2118kd = \u2212\nrepresenting the scaled complementary error function.\n\u02dc \u00b5kd\n\u02dc \u03c3kd\n\u221a2and erfcx(y) = ey2(1 \u2212 erf(y))\n3.4. Evidence Lower Bound as K \u2192 \u221e\nHere we show that the evidence lower bound [Eq. 10] is\nwell-defined in the limit K \u2192 \u221e; in fact, all instances\nof K are simply replaced by K+. Therefore, similar to\nvariational IBP methods, a user must specify a maxi-\nmum model complexity K+. A benefit over variational\nIBP methods, however, is that the q(Z) updates are\nnot affected by inactive features\u2014see \u00a74.\nWe take this limit by breaking the evidence into com-\nponents 1,...,K+ and K++ 1,...,K and note that"},{"page":4,"text":"Scaling the Indian Buffet Process via Submodular Maximization\nwhen mk = 0: \u02dc \u00b5kd = 0, \u02dc \u03c32\n1\n2ln\u03c0e\u03c32\n2\n. After some algebra, the evidence becomes:\n?\nkd= \u03c32\nA, and H(akd) =\nA\n\u03c8K++1\n2\nK\n?\nk=K++1\nD\n?\nd=1\n\u2212ln\u03c0\u03c32\nA\n2\n\u2212E[a2\nkd]\n\u03c32\nA\n+ ln\u03c0e\u03c32\nA\n2\n?\n(18)\nwhere \u03c8K+is Eq. 10 but with K+ replacing all K.\nFrom Eq. 25, we see that E[a2\nwhich causes all terms to cancel in Eq. 18 except \u03c8K+.\nkd] = \u03c32\nAwhen mk= 0,\nThe evidence lower bound remains well-defined be-\ncause both the likelihood and IBP prior terms do not\ndepend on inactive features, so for inactive features\nthe KL-divergence between the posterior and varia-\ntional distributions is simply the KL-divergence be-\ntween p(A) and q(A). For inactive features, p(A) =\nq(A), and as a result, the KL-divergence is zero.\n3.5. Z Objective Function\nGiven q(A), we compute MAP estimates of Z by max-\nimizing the evidence [Eq. 10] for each n \u2208 {1,...,N}\nwhile holding constant all n?\u2208 {1,...,N}\\n. Decom-\nposing Eq. 10 into terms that depend on Zn\u00b7and those\nthat do not yields (see the supplementary material):\nF(Zn\u00b7) = \u2212\n1\n2\u03c32\nX\n?\nZn\u00b7\u03a6\u03a6TZT\nn\u00b7+ Zn\u00b7\u03c9T\nn\u00b7+ const\n\u2212 ln\n?\nK+\\n+\nK+\n?\nk=1\n?\n1{mk\\n=0}znk\n?\nD\n?\n??\n! (19)\n\u03a6k\u00b7=\nE[ak1],...,E[akD]\n\u03c9nk=1\n\u03c32\nX\n?\n\u03a6k\u00b7XT\nn\u00b7+1\n2\nd=1\n?E[akd]2\u2212 E[a2\nkd]??\n+ \u03bd(znk= 1) \u2212 \u03bd(znk= 0) + 1{mk\\n=0}\u03b7k,\nwhich is a quadratic pseudo-Boolean function plus a\nterm that penalizes K+, where 1{\u00b7}is the indicator\nfunction, a \u201c\\n\u201d subscript indicates the given variable\nis determined after removing the nthrow from Z, and\n\uf8f1\n\uf8f4\nWe can prove F(Zn\u00b7) is submodular given the follow-\ning two well-known propositions, see Fujishige (2005):\nProposition 1. Nonnegative linear combinations of\nsubmodular functions are submodular.\nProposition 2. A quadratic pseudo-Boolean function\nwith quadratic weight matrix W is submodular if and\nonly if Wij\u2264 0 for all i,j.\n\u03bd(znk) =\n\uf8f4\n\uf8f3\n\uf8f2\n0, if mk\\n= 0 and znk= 0\nln(N \u2212 mk\\n\u2212 znk)!\/N!\n+ ln(mk\\n+ znk\u2212 1)!, otherwise\nVia Proposition 2, we see that \u2212\nZn\u00b7\u03c9T\nProposition 1, Eq. 19 is submodular if and only if\n1\n2\u03c32\nXZn\u00b7\u03a6\u03a6TZT\nn\u00b7+\nn\u00b7is submodular when \u03a6 is nonnegative. From\nG(Zn\u00b7) = \u2212ln\n?\nK+\\n+\nK+\n?\nk=1\n?\n1{mk\\n=0}znk\n??\n!(20)\nis submodular. We prove this property by rephras-\ning G(Zn\u00b7) as a set function and using the defini-\ntion of submodularity given by Eq. 4.\nBn \u2286 V where V = {1,...,K+} and An,Bn \u2208 2V\nwith G(An) = \u2212ln(K+\\n+ KAn)!.\nKAn=?K+\nG(An\u222a {e}) \u2212 G(An) \u2265 G(Bn\u222a {e}) \u2212 G(Bn)\n?K+\\n+ KAn\nLet An \u2286\nHere we let\nk=11{mk\\n=0}1{k\u2208An}where k \u2208 An indi-\ncates znk= 1. G is submodular if for all e \u2208 V \\ Bn:\nln\n?!\n?K+\\n+ KAn\u222a{e}\nEq. 21 has two cases: (1) me\\n> 0 so KBn\u222a{e}= KBn\nand KAn\u222a{e}= KAn, yielding 0 \u2265 0 for Eq. 21, which\nis true for all e \u2208 V \\ Bnand An\u2286 Bn, (2) me\\n=\n0 so KBn\u222a{e}= KBn+ 1 and KAn\u222a{e}= KAn+ 1.\nAfter some algebra this yields KBn\u222a{e}\u2265 KAn\u222a{e}for\nEq. 21, which is again true for all e \u2208 V \\ Bn and\nAn\u2286 Bn. As a result, both components of Eq. 19 are\nsubmodular, and by Proposition 1, adding these terms\nyields a submodular function.\n?!\u2265 ln\n?K+\\n+ KBn\n?!\n?K+\\n+ KBn\u222a{e}\n?!\n(21)\n3.6. Z Optimization\nEq. 19 is an unconstrained nonmonotone submodular\nfunction. Feige et al. (2011) prove that an approx-\nimibility guarantee is NP-hard for this class of func-\ntions. However, Feige et al. (2011) also show that a\nlocal-search (ls) algorithm obtains a constant-factor\napproximation to the optimal solution, provided the\nsubmodular objective function is nonnegative. For a\nsubmodular function F : 2V\u2192 R with ground set\nV = {1,...,K+} and solution set A \u2286 V , the ls-\nalgorithm operates as follows:\n1. initialize: let A = {argmaxw\u2208VF({w})}\n2. grow: while there is an element w \u2208 V \\ A s.t.\nF(A \u222a {w}) > (1 +\n3. prune: if there is an element w \u2208 A s.t. F(A \\\n{w}) > (1+\n4. return: maximum of F(A) and F(V \\ A).\n?\n|V |2)F(A): let A := A \u222a {w}\n?\n|V |2)F(A): let A := A\\{w}, goto 2.\nThe ls-algorithm obtains a solution that is greater than\n1\n3(1 \u2212\n?\n|V |)OPT\u2014where ? is a parameter and OPT is"},{"page":5,"text":"Scaling the Indian Buffet Process via Submodular Maximization\nthe maximum value of F. The ls-algorithm performs\nO(1\nsteps.\n?|V |3log|V |) function calls in the grow\/prune\n2468 1012\n0\n0.2\n0.4\n0.6\n0.8\n1\nlatent features (K)\nfraction of opts\nLS \u2265 95% Optimum\nLS Finds Optimum\nRandom \u2265 95% Optimum\nRandom Finds Optimum\nFigure 1. Fraction of ls-algorithm and random solutions\nthat obtain [within 95% of] the true optimum using data\ngenerated from the nonnegative linear-Gaussian model\nwith N = 500,D = 50,\u03c3X = 1.0.\nSince Eq. 19 is not strictly nonnegative, we use\nits normalized cost function to interpret the ls-\napproximability guarantee: F(Zn\u00b7) \u2212 Fn0, where Fn0\nis the minimum value of F(Zn\u00b7). Using the normal-\nized cost function, we obtain the following optimality\nguarantee:\n?\nwhere the superscript \u201cls\u201d denotes the solution from\nthe greedy ls-algorithm and an asterisk denotes the\nset that obtains the true maximum. This inequality\nstates that the ls-algorithm solution is guaranteed to\nperform better than the minimum by an amount pro-\nportional to the difference between the optimum and\nthe minimum. However, we emphasize that this in-\nequality does not provide an optimality guarantee for\nthe global MAP solution.\nF(Zls\nn\u00b7) \u2265 Fn0+1\n3\n1 \u2212\n?\n|V |\n?\n(F(Z\u2217\nn\u00b7) \u2212 Fn0) (22)\nWe studied the empirical performance of the ls-\nalgorithm by generating high noise data (\u03c3X = 1)\nfrom the nonnegative linear-Gaussian model with N =\n500,D = 50 and compared the ls-algorithm with the\nbrute-force optimal solution as K varied from 2 to 12,\nperforming 1000K total optimizations for each of ten\nrandomly generated datasets. Furthermore, we com-\npared the ls-algorithm with randomly sampled Zn\u00b7so-\nlutions to demonstrate that the optimization space was\nnot skewed to favor solutions near the optimal value.\nFigure 1 shows the fraction of solutions that obtain the\ntrue optimum as well as the fraction of solutions that\nwere greater than 95% of F(Z\u2217\nerror bars indicate the combined standard deviation\nover the 10 \u00d7 1000K optimizations. The ls-algorithm\nfound the optimal solution roughly 70% of the time\nn\u00b7) \u2212 Fn0, where the\nfor K = 12 and obtained within 95% of the optimal\nsolution over 99.9% of the time for all K\u2014meaning we\ncould empirically replace the1\nrandom sampling comparison indicated that the opti-\nmization space did not favor nearly-optimal solutions:\nits convergence to 5% for within-95% optimal solutions\nwas characteristic of a uniform solution space.\n3in Eq. 22 with19\n20. The\nBy precomputing \u03a6\u03a6Tand maintaining an auxil-\niary vector of K+ weights, we can evaluate Eq.\n19 in constant time when adding\/removing elements\nto the solution set. In turn, the ls-algorithm op-\ntimizes F(Zn\u00b7) in K2\ntions. The O(1\n+logK+) component arises from the\nadd\/removal operations, but as we show in Figure 2,\nit is a loose upper bound that scales sub-quadratically\nin practice.\n+D + O(1\n?K3\n+logK+) opera-\n?K3\n0 100\nnumber of latent features (K)\n200300 400500\n0\n2\n4\n\u00b7104\nO(1) updates\nK*logK fit\ndata\nFigure 2. Number of O(1) updates per ls-optimization us-\ning data generated from the nonnegative linear-Gaussian\nmodel with N = 1000,D = 1000,\u03c3X = 1.0.\n4. Related Work\nSeveral proposals have been made for efficient infer-\nence with latent feature models. Table 1 summarizes\nthe per-iteration complexity of the methods discussed\nbelow. In the next section we compare these methods\non two synthetic and three real-world datasets.\nDoshi-Velez et al. (2009) formulated a coordinate as-\ncent variational inference technique for IBP models\n(VIBP). This method used the \u201cstick breaking\u201d for-\nmulation of the IBP, which maintained coupled beta-\ndistributed priors on the entries of Z\u2014marginalizing\nthese priors does not allow closed-form MFVB up-\ndates. Unlike MEIBP inference, maintaining the beta\npriors has the undesirable consequence that inactive\nfeatures contribute to the evidence lower bound and\nmust be ignored when updating the variational distri-\nbutions. This was not a problem for Doshi-Velez et al.\n(2009)\u2019s finite variational IBP, which computes vari-\national distributions for a linear-Gaussian likelihood\nwith a parametric beta-Bernoulli prior on the latent\nfeatures. The inference complexity for both methods"},{"page":6,"text":"Scaling the Indian Buffet Process via Submodular Maximization\nis O(NK2\n+D), which is dominated by updating q(Z).\nDing et al. (2010) used mixed expectation-propagation\nstyle updates with MFVB inference in order to per-\nform variational inference for a nonnegative linear-\nGaussian IBP model (INMF). The expectation-\npropagation style updates are more complicated than\nMFVB updates and have per-iteration complexity\nO(N(K3D+KD2)). Ding et al. (2010) motivated this\nframework by stating that the evidence lower bound\nof a linear-Gaussian likelihood with a truncated Gaus-\nsian prior on the latent factors is negative infinity. This\nis only true if the variational distribution is a Gaus-\nsian, however the free-form variational distribution for\ntheir model is a truncated Gaussian, which has a well-\ndefined evidence lower bound.\nDoshi-Velez & Ghahramani (2009) presented a linear-\ntime \u201caccelerated\u201d Gibbs sampler for conjugate IBP\nmodels that effectively marginalized over the latent\nfactors (AIBP). The per-iteration complexity was\nO(N(K2+ KD)). This is comparable to the uncol-\nlapsed IBP sampler (UGibbs) that has per-iteration\ncomplexity O(NDK2) but does not marginalize over\nthe latent factors, and as a result, takes longer to\nmix. In terms of both complexity and empirical per-\nformance, the accelerated Gibbs sampler is the most\nscalable sampling-based IBP inference technique cur-\nrently available. One constraint of the accelerated IBP\nis that the latent factor distribution must be conjugate\nto the likelihood, which for instance, does not allow\nnonnegative priors on the latent factors.\nRai & Daume III (2011) introduced a beam-search\nheuristic for locating approximate MAP solutions to\nlinear-Gaussian IBP models (BS-IBP). This heuris-\ntic sequentially adds a single data point to the model\nand determines the latent feature assignments by scor-\ning all 2K+latent feature combinations.\ning heuristic uses an estimate of the joint probability,\nP(X,Z) to score assignments, which evaluates the col-\nlapsed likelihood P(X|Z) for all 2K+possible assign-\nments: an expensive N3(K++ D) operation, yielding\na per-iteration complexity of O(N3(K++ D)2K+).\nThe scor-\n5. Experiments\nWe evaluated the inference quality and efficiency of\nMEIBP inference on two synthetic and three real-\nworld datasets. We used the runtime and predic-\ntive likelihood of held-out observations as our perfor-\nmance criteria and compared MEIBP inference with\nthe methods listed in Table 1 (the finite and infinite\nVIBP are differentiated with an \u201cf-\u201d and \u201ci-\u201d prefix).\nWe used a truncated Gaussian prior on the latent fac-\nTable 1. Worst-case\nlinear-Gaussian likelihood model for N D-dimensional ob-\nservations and K+ active latent features.\nper-iterationcomplexitygivena\nAlgorithm\nMEIBP\nVIBP (Doshi-Velez et al.,\n2009)\nAIBP (Doshi-Velez &\nGhahramani, 2009)\nUGibbs (Doshi-Velez &\nGhahramani, 2009)\nBS-IBP (Rai &\nDaume III, 2011)\nINMF (Ding et al., 2010)\nIteration Complexity\nO(N(K2\n+D+K3\n+lnK+))\nO(NK2\n+D)\nO(N(K2\n++ K+D))\nO(NK2\n+D)\nO(N3(K++ D)2K+)\nO(N(K3\n+D + K+D2))\ntors for UGibbs and INMF, and Gaussian priors for\nthe AIBP and variational methods.\ntions, we also included Schmidt et al. (2009)\u2019s iterated\nconditional modes algorithm, which computes a MAP\nestimate of a parametric nonnegative matrix factoriza-\ntion model: X = BA+E, where B and A have expo-\nnential priors and E is zero-mean Gaussian noise. We\nabbreviate this model \u201cBNMF\u201d; it has a per-iteration\ncomplexity of O(N(K2\n++ K+D)).\nIn our evalua-\nThe VIBP and MEIBP inference methods specify a\nmaximum K value, while the sampling methods are\nunbounded. Therefore, we also included truncated\nversions of the sampling methods (indicated by a \u201ct-\u201d\nprefix) for a fairer comparison. We centered all in-\nput data to have a 0-mean for the models with 0-\nmean Gaussian priors and a 0-minimum for nonneg-\native models, and all inferred matrices were initial-\nized randomly from their respective priors. Following\nDoshi-Velez & Ghahramani (2009), we fixed the hyper-\nparameters \u03c3X and \u03c3Ato3\ndard deviation across all dimensions of the data, and\nset \u03b1 = 3. We ran each algorithm until the multiplica-\ntive difference of the average training log-likelihood\ndiffered by less than 10\u22124between blocks of five it-\nerations with a maximum runtime of 36 hours. Our\nexperiments used MATLAB implementations of the\nalgorithms, as provided by the respective authors, on\n3.20 GHz processors.\n4\u03c3, where \u03c3 was the stan-\nSynthetic Data We created high-noise synthetic\ndatasets in the following way:\nBernoulli(p = 0.4), (2) generate A with K random,\npotentially overlapping binary factors, (3) let X =\nZA + E, where E \u223c N(0,1). We evaluated the pre-\ndictive likelihood on 20% of the dimensions from the\nlast half of the data (see supplementary information).\n(1) sample zn,k \u223c\nFigure 3 shows the evolution of the test log-likelihood\nover time for a small dataset with N = 500,D ="},{"page":7,"text":"Scaling the Indian Buffet Process via Submodular Maximization\n10\u22121\n100\n101\n102\n103\n\u22123.5\n\u22123\n\u22122.5\n\u00b7104\nbnmf\nmeibp\nt-ugibbs\nugibbs\naibp\nt-aibp\nf-vibp\ni-vibp\nseconds\ntest log-likelihood\nugibbs\nt-ugibbs\nt-aibp\naibp\nbnmf\nf-vibp\ni-vibp\nmeibp\n05 10 15\nhours\n20 2530 35\n\u22121.4\n\u22121.2\n\u22121\n\u22120.8\u00b7107\nbnmf\nmeibp\nt-ugibbs\nugibbs\naibp\nt-aibp\nf-vibp\ni-vibp\nFigure 3. Evolution of test log-liklihood over time for a synthetic dataset; Left: dataset with N = 500,D = 500,K = 20\n(NB: x-axis is log-scale) Right: dataset with N = 105,D = 103,K = 50 (NB: x-axis is linear-scale).\n500,K = 20 and a large dataset with N = 105,D =\n103,K = 50. All models were initialized randomly\nwith the true number of latent features, and the error\nregions display the standard deviation over five ran-\ndom restarts. The BS-IBP and INMF methods were\nremoved from our experiments following the synthetic\ndataset tests as both methods took at least an order\nof magnitude longer than the other methods: in 36\nhours, the BS-IBP did not complete a single iteration\non the small dataset, and the INMF did not complete\na single iteration on the large dataset.\nMEIBP converged quickest among the IBP models\nfor both the small and large dataset, while the para-\nmetric BNMF model converged much faster than all\nIBP models. However, the IBP models captured the\nsparsity of the latent features and the MEIBP and\nUGibbs eventually outperformed the BNMF on the\nsmall dataset, while only the MEIBP outperformed the\nBNMF on the large dataset. The VIBP methods con-\nverged quicker than the sampling counterparts but had\ntrouble escaping local optima. The uncollapsed sam-\nplers eventually performed as well as the MEIBP on\nthe small dataset but did not mix to a well-performing\ndistribution for the large dataset.\nReal\ndatasets used in our experiments.\nBC are dense real-valued datasets, whereas the Flickr\ndataset is a sparse binary dataset (0.81% filled). For\nthe Piano and Flickr datasets, we evaluated the pre-\ndictive likelihood on a held-out portion of 20% of the\ndimensions from the last half of the datasets.\nYale-BC dataset had roughly sixty-four facial images\nof thirty-eight subjects, and we removed the bottom\nhalf of five images from each subject for testing.\nData Table 2 summarizes the real-world\nPiano and Yale-\nThe\nFigure 4 shows the test log-likelihood and convergence\ntime for all inference methods applied to the real-world\ndatasets, averaged over five random restarts. All in-\nference methods were initialized with K = {10,25,50}\nTable 2. Summary of real-world datasets.\nDataset\nPiano (Poliner\n& Ellis, 2006)\nFlickr (Kollar &\nRoy, 2009)\nYale-BC (Lee\net al., 2005)\nSize (N \u00d7 D)\n16000 \u00d7 161\nDetails\nDFT of piano\nrecordings\nbinary image-tag\nindicators\nface images with\nvarious lightings\n25000 \u00d7 1500\n2414 \u00d7 32256\nas indicated by the size of the marker (the smallest\nmarker shows the K = 10 results).\nmethods (AIBP, UGibbs) also include a large faded\nmarker that shows the results for unbounded K.\nThe sampling\nThe Piano results were similar to the small synthetic\ndataset. The BNMF converged much faster than the\nIBP models, and the MEIBP performed best among\nthe IBP models in terms of runtime and test log-\nlikelihood\u2014it converged to a similar solution as the\nAIBP in one-third the time. Though UGibbs has the\nbest per-iteration complexity, it got stuck in poor local\noptima when randomly initialized. The VIBP meth-\nods and MEIBP expressed large uncertainty about the\nlatent factors early on and overcame these poor lo-\ncal optima. By using hard latent feature assignments,\nthe MEIBP took larger steps in the inference space\nthan the VIBP methods, which was beneficial for this\ndataset, and achieved similar results to the AIBP.\nMEIBP inference performed comparable to the best\nIBP sampling technique for the sparse binary Flickr\ndataset and converged over an order of magnitude\nfaster. Surprisingly, the dense BNMF inference per-\nformed very well on this dataset even though the\ndataset was sparse and binary.\nverged slower than the MEIBP because it inferred a\nsparse matrix from a dense prior, which took over four\ntimes as many iterations to converge compared to the\ndense datasets. While the t-AIBP converged to a bet-\nter solution than the MEIBP, it took over an order-\nThe BNMF con-"},{"page":8,"text":"Scaling the Indian Buffet Process via Submodular Maximization\n101\n102\n103\n104\n105\n\u22124\n\u22123.5\n\u22123\n\u22122.5\n\u00b7105\nconvergence time (s)\ntest log-likelihood\nPiano\nmeibp\nugibbs\naibp\nbnmf\nf-vibp\ni-vibp\n103\n104\n105\n\u22124.95\n\u22124.9\n\u22124.85\n\u00b7106\nmeibp+aibp\nconvergence time (s)\nFlickr\n103\n104\n105\n\u22122.8\n\u22122.7\n\u22122.6\n\u00b7106\nconvergence time (s)\nYale-BC\nFigure 4. Inference results on real-world datasets. The size of the marker indicates the K value for K = {10,25,50},\nwith larger markers indicating a larger K. AIBP and UGibbs also include a larger faded marker that shows the inference\nresults for unbounded K. The Flickr plot also shows the result of initializing the AIBP using the MEIBP result. The\nerror-bars indicate the standard deviation of convergence time and test log-likelihood over five random restarts.\nof-magnitude longer to surpass the MEIBP\u2019s perfor-\nmance. As we demonstrate with the Flickr results, ini-\ntializing the AIBP with the MEIBP outcome obtained\na similar solution in a fraction of the time (indicated\nas \u201cmeibp+aibp\u201d on the figure).\nThe MEIBP converged faster than the other IBP\nmethods for the Yale-BC dataset but to a lower test\nlikelihood. The UGibbs and BNMF also experienced\ndifficulty for this dataset, where BNMF converged to\na test log-likelihood around \u22123.6 \u00d7 106(not visible\nin the figure). These linear-Gaussian models with\nnonnegative priors performed worse than the models\nwith Gaussian priors because the dataset contained\nmany images with dark shadows covering part of the\nface.The nonnegative priors appeared to struggle\nwith reconstructing these shadows, because unlike the\nGaussian priors, they could not infer negative-valued\n\u201cshadow\u201d factors that obscured part of the image.\nIn the above experiments, the MEIBP consistently ex-\nhibited a sudden convergence whereby it obtained a lo-\ncal optima and the ls-algorithm did not change any Z\nassignments. This is a characteristic of using hard as-\nsignments with a greedy algorithm: at a certain point,\nchanging any latent feature assignments decreased the\nobjective function. This abrupt convergence, in combi-\nnation with the speed of the ls-algorithm, helped the\nMEIBP consistently converge faster than other IBP\nmethods. Furthermore, the submodular maximization\nalgorithm converged to local optima that were compa-\nrable or better than the sampling or variational results,\nthough at the cost of only obtaining a MAP solution.\nLike the variational methods, it maintained a distri-\nbution over A that prevented it from getting stuck in\nlocal optima early on, and like the sampling methods,\nthe MEIBP used hard Z assignments to take larger\nsteps in the inference space and obtain better optima.\n6. Summary and Future Work\nWe presented a new inference technique for IBP mod-\nels that used Kurihara & Welling (2008)\u2019s ME frame-\nwork to perform approximate MAP inference via\nsubmodular maximization.\nexploit the submodularity inherent in the evidence\nlower bound formulated in \u00a73, which arose from the\nquadratic pseudo-Boolean component of the linear-\nGaussian model. MEIBP inference converged faster\nthan competing IBP methods and obtained compara-\nble solutions on various datasets.\nOur key insight was to\nThere are many discrete Bayesian nonparametric pri-\nors, such as the Dirichlet process, and an interest-\ning area for future research will be to generalize our\nresults in order to phrase inference with these pri-\nors as submodular optimization problems. Further-\nmore, we used a simple local-search algorithm to\nobtain a\n3-approximation bound, but concurrently\nwith this work, Buchbinder et al. (2012) proposed a\nsimpler stochastic algorithm for unconstrained sub-\nmodular maximization that obtains an expected\napproximation bound. Using this algorithm, MEIBP\ninference has an improved worst case complexity of\nO(NK2\n+D). We will investigate this algorithm in an\nextended technical version of this paper.\n1\n1\n2-\nCode:\navailable at https:\/\/github.com\/cjrd\/MEIBP.\nA MATLAB implementation of MEIBP is\nAcknowledgements: CR was supported by the Win-\nston Churchill Foundation of the United States, and"},{"page":9,"text":"Scaling the Indian Buffet Process via Submodular Maximization\nZG was supported by EPSRC grant EP\/I036575\/1\nand grants from Google and Microsoft. We thank the\nanonymous reviewers for their helpful comments.\nReferences\nAttias, H. A variational bayesian framework for graph-\nical models. Advances in Neural Information Pro-\ncessing Systems, 12:209\u2013215, 2000.\nBroderick, T., Jordan, M. I., and Pitman, J. Clus-\nters and features from combinatorial stochastic pro-\ncesses. arXiv:1206.5862, 2012.\nBuchbinder, N., Feldman, M., Naor, J., and Schwartz,\nR. A tight linear time (1\/2)-approximation for un-\nconstrained submodular maximization. In 53rd An-\nnual Symposium on Foundations of Computer Sci-\nence, pp. 649\u2013658. IEEE, 2012.\nDing, N., Qi, Y.A., Xiang, R., Molloy, I., and Li,\nN. Nonparametric Bayesian matrix factorization by\nPower-EP. In 14th Int\u2019l Conf. on AISTATS, vol-\nume 9, pp. 169\u2013176, 2010.\nDoshi-Velez, F. and Ghahramani, Z. Accelerated sam-\npling for the Indian buffet process. In Proceedings of\nthe 26th Annual Int\u2019l Conference on Machine Learn-\ning, pp. 273\u2013280, 2009.\nDoshi-Velez, F., Miller, K. T., Van Gael, J., and Teh,\nY. W. Variational inference for the Indian buffet\nprocess. In 13th Int\u2019l Conf. on AISTATS, pp. 137\u2013\n144, 2009.\nFeige, U., Mirrokni, V.S., and Vondrak, J. Maximizing\nnon-monotone submodular functions. SIAM Jour-\nnal on Computing, 40(4):1133\u20131153, 2011.\nFujishige, S. Submodular functions and optimization,\nvolume 58. Elsevier Science Limited, 2005.\nGhahramani, Z. and Beal, M.J.\nrithms for variational bayesian learning.\nvances in Neural Information Processing Systems,\nvolume 13, 2001.\nPropagation algo-\nIn Ad-\nGriffiths, T. and Ghahramani, Z. Infinite latent fea-\nture models and the Indian buffet process. Technical\nreport, Gatsby Unit, UCL, London, UK, 2005.\nGriffiths, T. L. and Ghahramani, Z. Infinite latent fea-\nture models and the Indian buffet process. In Ad-\nvances in Neural Information Processing Systems,\nvolume 18, 2006.\nKnowles, D. and Ghahramani, Z. Infinite sparse factor\nanalysis and infinite independent components anal-\nysis. Independent Component Analysis and Signal\nSeparation, pp. 381\u2013388, 2007.\nKollar, T. and Roy, N. Utilizing object-object and\nobject-scene context when planning to find things.\nIn IEEE International Conference on Robotics and\nAutomation, pp. 2168 \u20132173, 2009.\nKurihara, K. and Welling, M. Bayesian k-means as a\nmaximization-expectation algorithm. Neural Com-\nputation, 21(4):1145\u20131172, 2008.\nLee, K.C., Ho, J., and Kriegman, D. Acquiring linear\nsubspaces for face recognition under variable light-\ning. IEEE Trans. Pattern Anal. Mach. Intelligence,\n27(5):684\u2013698, 2005.\nLov\u00b4 asz, L. Submodular functions and convexity. Math-\nematical programming: the state of the art, pp. 235\u2013\n257, 1983.\nPapaspiliopoulos, O. and Roberts, G. O. Retrospec-\ntive Markov chain Monte Carlo methods for Dirich-\nlet process hierarchical models. Biometrika, 95(1):\n169\u2013186, 2008.\nPoliner, G.E. and Ellis, D.P.W.\ntive model for polyphonic piano transcription.\nEURASIP Journal on Advances in Signal Process-\ning, 2006.\nA discrimina-\nRai, P. and Daume III, H. Beam search based map\nestimates for the Indian buffet process. In Proceed-\nings of the 28th Annual Int\u2019l Conference on Machine\nLearning, 2011.\nSchmidt, M. N., Winther, O., and Hansen, L. K.\nBayesian non-negative matrix factorization.\nInt\u2019l Conference on Independent Component Anal-\nysis and Signal Separation, volume 5441 of Lecture\nNotes in Computer Science (LNCS), pp. 540\u2013547.\nSpringer, 2009.\nIn\nTeh, Y.W., Gorur, D., and Ghahramani, Z. Stick-\nbreaking construction for the indian buffet process.\nIn Int\u2019l Conference on AISTATS, volume 11, 2007.\nWang, C. and Blei, D. Truncation-free online varia-\ntional inference for bayesian nonparametric models.\nIn Advances in Neural Information Processing Sys-\ntems, volume 25, 2012."},{"page":10,"text":"Scaling the Indian Buffet Process via Submodular Maximization\nSupplementary Material\nS.1. Truncated Gaussian Properties\nIn the main text we examined a truncated Gaussian\nof the form:\nTN(\u02dc \u00b5kd, \u02dc \u03c32\nkd) =\n2\nerfc\n?\n\u2212\n\u02dc \u00b5kd\n\u02dc \u03c3kd\n\u221a2\n?N(\u02dc \u00b5kd, \u02dc \u03c32\nkd) (23)\nwith N representing a Gaussian distribution. The first\ntwo moments of TN(\u02dc \u00b5kd, \u02dc \u03c32\nkd) are:\n?2\/\u03c0\nerfcx(\u2118kd)\nE[akd] = \u02dc \u00b5kd+ \u02dc \u03c3kd\n(24)\nE?a2\nkd\n?= \u02dc \u00b52\nkd+ \u02dc \u03c32\nkd+ \u02dc \u03c3kd\u02dc \u00b5kd\n?2\/\u03c0\nerfcx(\u2118kd)\n(25)\nwith \u2118kd = \u2212\nrepresenting the scaled complementary error function.\nThe entropy is\n\u02dc \u00b5kd\n\u02dc \u03c3kd\n\u221a2and erfcx(y) = ey2(1 \u2212 erf(y))\nH(q(akd)) =1\n2ln\u03c0e\u02dc \u03c32\nkd\n2\n?\n+ lnerfc\n?\n?\n\u2212\n\u02dc \u00b5kd\n\u02dc \u03c3kd\n\u02dc \u00b5kd\n\u02dc \u03c3kd\n\u221a2\n?\n??\u22121\n(26)\n+\u02dc \u00b5kd\n\u02dc \u03c3kd\n1\n2\u03c0\n?\nerfcx\n\u2212\n\u221a2\n.\n(27)\nS.2. Shifted Equivalence Classes\nHere we discuss the \u201cshifted\u201d equivalence class of bi-\nnary matrices first proposed by Ding et al. (2010). For\na given N \u00d7K binary matrix Z, the equivalence class\nfor this binary matrix [Z] is obtained by shifting all-\nzero columns to the right of the non-zero columns while\nmaintaining the non-zero column orderings, see Fig-\nure 5. Placing independent Beta(\u03b1\nBernoulli entries of Z and integrating over these pri-\nors yields the following probability for Z, see Eq. 27\nin Griffiths & Ghahramani (2005):\nK,1) priors on the\nP(Z) =\nK\n?\nk=1\n\u03b1\nK\u0393(mk+\u03b1\nK)\u0393(N \u2212 mk+ 1)\n\u0393(N + 1 +\u03b1\nK)\n(28)\nwhere mk =\nP(Z) = 0 for all Z. However, the probability of cer-\ntain equivalence classes of binary matrices, P([Z]), can\nremain non-zero as K \u2192 \u221e. Specifically, Griffiths &\nGhahramani (2005) show P([Z]) remains non-zero for\nthe \u201cleft-ordered form\u201d equivalence class of binary ma-\ntrices, whereby the columns of Z are ordered such that\nthe binary values of the columns are non-increasing,\nwhere the first row is the most significant bit. Here\n?N\nn=1znk. Letting K \u2192 \u221e yields\nwe outline a similar result for the shifted equivalence\nclass.1\nWe obtain the probability of the shifted equivalence\nclass by multiplying the multiplicity of the equivalence\nclass by the probability of a matrix within the class.\nFor a given matrix with K columns and K+non-zero\ncolumns, each shifted equivalence class has?K\n?K\nk=1\nK+\n?ma-\ntrices that map to it, yielding:\nP ([Z]) =\nK+\n? K\n?\n\u03b1\nK\u0393(mk+\u03b1\nK)\u0393(N \u2212 mk+ 1)\n\u0393(N + 1 +\u03b1\nK)\n.\n(29)\nFollowing a similar algebraic rearrangement as Grif-\nfiths & Ghahramani (2005) Eqs. 30-33, except replac-\ning the\n?2N\u22121\ncause of the different equivalence class multiplicities\u2014\nresults in:\nK!\nh=0\nKh!term with?K\nK+\n?\u2014which occurs be-\nP([Z]) =\u03b1K+\nK+!\u00b7\nK!\n(K \u2212 K+)!KK+\u00b7\n(N \u2212 mk)!?mk\u22121\n?\nN!\n?2N\u22121\nK)\nj=1(j +\u03b1\nK)\n?K\n\u00b7\nK+\n?\nk=1\nj=1(j +\u03b1\nN!\n. (30)\nWe then take the limit K \u2192 \u221e for each of the four\nterms. The first term has no K dependence and does\nnot change in the infinite limit. For the second term we\nlet K0= K \u2212 K+ and have\nin Griffiths & Ghahramani (2005) show that this term\nbecomes 1 as K \u2192 \u221e. The infinite limit of the third\nand fourth terms are determined in the Appendix of\nGriffiths & Ghahramani (2005). Combining all four\nterms together yields:\nK!\nK0!KK+. Equations 60-62\nP([Z]) =\u03b1K+\nK+!e\u2212\u03b1HN\nK+\n?\nk=1\n(N \u2212 mk)!(mk\u2212 1)!\nN!\n(31)\nwhere HN is the Nthharmonic number.\nThe probability of the shifted equivalence class is\nnearly identical to the probability of the left-ordered-\nform equivalence class:\nP([Z]lof) =\n\u03b1K+\n?2N\u22121\nh=1Kh\ne\u2212\u03b1HN\nK+\n?\nk=1\n(N \u2212 mk)!(mk\u2212 1)!\nN!\n,\n(32)\nwhere Khis the number of columns of Z with binary\nvalue h \u2208 {1,...,2N\u22121} when the first row is taken\n1Ding et al. (2010) proposed this equivalence class but\ndid not explicitly show that it remains well defined as K \u2192\n\u221e. Furthermore, they did not discuss the collapsed case\nwhere we first marginalize over the beta priors on Z."},{"page":11,"text":"Scaling the Indian Buffet Process via Submodular Maximization\nshifted\n\u2212 \u2212 \u2212 \u2212 \u2192\nFigure 5. Example of a binary matrix (left) and its shifted equivalence matrix (dark squares are 1, white squares are\n0)\u2014placing the two all-zero columns anywhere in the matrix will yield the same equivalence matrix.\nto be the most significant bit. The only difference be-\ntween Eq. 31 and Eq. 32 is the denominator of the first\nfraction. For the left-ordered-form, this term penalizes\nZ matrices with identical columns. In the feature as-\nsignment view, this term penalizes features that are\nassigned to the exact same set of observations. The\nK+! term in the shifted equivalence class prior does\nnot distinguish between identical and distinct columns\nof Z, and in turn, does not penalize repeated feature\nassignments. These two equivalence class probabilities\nare proportional in the limit of large N as the proba-\nbility of two columns being identical approaches 0.\nS.3. Hyperparameter Inference\nIn the main text we assumed the hyperparameters\n\u03b8 = {\u03c3X,\u03c3A,\u03b1} were known (i.e. estimated from the\ndata). Placing conjugate gamma hyperpriors on these\nparameters allows for a straightforward extension in\nwhich we infer their values. Formally, let\np(\u03c4X) = Gamma(\u03c4X;aX,bX)\np(\u03c4A) = Gamma(\u03c4A;aA,bA)\np(\u03b1) = Gamma(\u03b1;a\u03b1,b\u03b1)\n(33)\n(34)\n(35)\nwhere \u03c4 represents the precision, equivalent to the in-\nverse variance\n\u03c32, for the variance parameter indicated\nin the subscript. Update equations for the variational\ndistributions follow from standard update equations\nfor variational inference in exponential families, cf. At-\ntias (2000), and yield:\n1\nq(\u03c4X) = Gamma(? aX,?bX)\nq(\u03b1) = Gamma(? a\u03b1,?b\u03b1)\n? aA= aA+KD\n?bA= bA+1\n(36)\nq(\u03c4A) = Gamma(? aA,?bA) (37)\n(38)\nwith variance updates\n2\nK+\n?\n(39)\n2\nk=1\nD\n?\nd=1\nE?a2\nkd\n?\n(40)\nand\n? aX= aX+ND\n?bX= bX+1\n\u2212 2E[akd]znkxnd+ 2\n2\n?\n(41)\n2\nN\nn=1\nD\n?\nd=1\n?\nx2\nnd+\nK+\n?\nk=1\n?\nE?a2\nkd\n?znk\n(42)\nK+\n?\nk?=k+1\nznkznk?akdak?d\n??\n(43)\nand q(\u03b1) updates\n? a\u03b1= a\u03b1+ K+\n(44)\n?b\u03b1= b\u03b1+ HN.(45)\nMEIBP inference is carried out exactly as discussed\nin the main text except all instances of \u03c3X,\u03c3A, and \u03b1\nare replaced with the expectation from their respective\nvariational distribution. Furthermore the variational\nlower bound also has three additional entropy terms\nfor gamma distributions, one for each hyperparameter.\nS.4. Evidence as a function of Zn\u00b7\nAs shown in the main text, we obtain a submodular\nobjective function for each Zn\u00b7, n \u2208 {1,...,N} by ex-\namining the evidence as a function of Zn\u00b7while holding\nconstant all n?\u2208 {1,...,N} \\ n. The evidence is\n1\n\u03c32\nX\nn=1\n?\nN\n?\n?\n?\n\u22121\n2Zn\u00b7\u03a6\u03a6TZT\nn\u00b7+ Zn\u00b7\u03beT\nn\u00b7\n?\n\u2212 lnK+!\n+\nK+\nk=1\nln(N \u2212 mk)!(mk\u2212 1)!\nN!\n+ \u03b7k\n?\n+ const\n(46)\n\u03benk= \u03a6k\u00b7XT\nn\u00b7+1\n2\nD\n?\nA\nd=1\n?E[akd]2\u2212 E[a2\n\u2212E[a2\n2\u03c32\nA\nkd]?\n(47)\n\u03b7k=\nD\n?\nd=1\n?\n\u2212ln\n\u03c0\u03c32\n2\u03b12\/D\n2\nkd]\n+ H(q(akd))\n?\n, (48)"},{"page":12,"text":"Scaling the Indian Buffet Process via Submodular Maximization\nwhich nearly factorizes over the Zn\u00b7because the like-\nlihood component and parts of the prior components\nnaturally fit into a quadratic function of Zn\u00b7. The\nlnK+! and \u03b7k only couple the rows of Z when K+\nchanges, while the log-factorial term couples the rows\nof Z through the sums of the columns. Both of these\nterms only depend on statistics of Z (the mk values\nand K+), not the Z matrix itself, e.g. permuting the\nrows of Z would not affect these terms. Furthermore,\nlnK+and \u03b7khave no N dependence and become in-\nsignificant as N increases. These observations, in con-\njunction with the MEIBP performance in the exper-\nimental section of the main text, indicate that opti-\nmizing Eq. 46 for Zn\u00b7 is a reasonable surrogate for\noptimizing Z.\nHere we explicitly decompose Eq. 46 to show its\nZn\u00b7 dependency.\nstraightforward if we first define the function:\n?\n0, if mk\\n= 0 and znk= 0.\nDecomposing ln(N\u2212mk)!(mk\u22121)!\nN!\nis\n\u03bd(znk) =\nln(N \u2212 mk\\n\u2212 znk)!(mk\\n+ znk\u2212 1)!\/N!\n(49)\nwhere the \u201c\\n\u201d subscript indicates the variable with\nthe nthrow removed from Z. For a given n we have:\nK+\n?\nk=1\n\u03bd(znk) =\nK+\n?\nK+\n?\n+ \u03bd(znk= 0),\nk=1\nln(N \u2212 mk)!(mk\u2212 1)!\/N!\n=\nk=1\nznk(\u03bd(znk= 1) \u2212 \u03bd(znk= 0))\n(50)\nwhich makes the Zn\u00b7dependency explicit and lets us\nadd \u03bd(znk = 1) \u2212 \u03bd(znk = 0) into the inner-product\nterm, \u03ben\u00b7, and place \u03bd(znk= 0) into a constant term.\nWe can incorporate \u03b7kinto the inner-product term in\na similar manner for a given n \u2208 {1,...,N} :\nK+\n?\nk=1\n\u03b7k=\n?\nk:mk\\n>0\n\u03b7k+\nK+\n?\nk=1\n1{mk\\n=0}znk\u03b7k, (51)\nwhere the first term does not depend on Zn\u00b7 and is\nadded to the constant term, while the second term is\nadded to the inner-product term. Finally, for a given\nn \u2208 {1,...,N} the lnK! term becomes\n\uf8eb\nk=1\nlnK+! = ln\n\uf8edK+\\n+\nK+\n?\n?\n1{mk\\n=0}znk\n?\n\uf8f6\n\uf8f8!, (52)\nwhere 1{\u00b7} is the indicator function.\nthe main text, combining the above terms yields\nAs stated in\nthe following submodular objective function for n =\n1,...,N:\nF(Zn\u00b7) = \u2212\n1\n2\u03c32\nX\n\uf8eb\nZn\u00b7\u03a6\u03a6TZT\nn\u00b7+ Zn\u00b7\u03c9T\nn\u00b7+ const\n\u2212 ln\n\uf8edK+\\n+\n?\nK+\n?\nk=1\n?\n1{mk\\n=0}znk\n?\uf8f6\n\uf8f8! (53)\n(54)\u03a6k\u00b7=(E[ak1],...,E[akD])\n\u03c9nk=1\n\u03c32\nX\n\u03a6k\u00b7XT\nn\u00b7+1\n2\nD\n?\nd=1\n?E[akd]2\u2212 E[a2\nkd]?\n?\n+ \u03bd(znk= 1) \u2212 \u03bd(znk= 0) + 1{mk\\n=0}\u03b7k,\n(55)\n1{\u00b7} is the indicator function, and the subscript \u201c\\n\u201d\nis the value of the given variable after removing the\nnthrow from Z.\nS.5. Additional MEIBP Characterization\nIn this section, we will maintain a growing list of\nadditional MEIBP characterization experiments. See\nhttp:\/\/arxiv.org\/abs\/1304.3285 for the current\nversion.\nS.5.1. Learning K+\nAn ostensible advantage of using Bayesian nonpara-\nmetric priors is that a user does not need to specify\nthe multiplicity of the prior parameters. Clever sam-\npling techniques such as slice sampling and retrospec-\ntive sampling allow samples to be drawn from these\nnonparametric priors, c.f. Teh et al. (2007) and Pa-\npaspiliopoulos & Roberts (2008). However variational\nmethods are not directly amenable to Bayesian non-\nparametric priors as the variational optimization can-\nnot be performed over an unbounded prior space. In-\nstead, variational methods must specify a maximum\nmodel complexity (parameter multiplicity).\nheuristics have been proposed to address this limita-\ntion: Wang & Blei (2012) sampled from the variational\ndistribution for the local parameters\u2014which included\nsampling from the unbounded prior\u2014 and used the\nempirical distributions of the local samples to update\nthe global parameters, while Ding et al. (2010) simply\nstarted with K+= 1 and greedily added features. We\ndid not address these techniques in this work as the\nMEIBP performed competitively with the unbounded\nsampling techniques without employing these types of\nheuristics. Furthermore, here we demonstrate that the\nMEIBP can robustly infer the true number of latent\nfeatures when the K+bound is greater than the true\nnumber of latent features.\nSeveral"},{"page":13,"text":"Scaling the Indian Buffet Process via Submodular Maximization\nFor this experiment we generated the binary images\ndataset used in Griffiths & Ghahramani (2005), where\nthe dataset, X, consisted of 2000 6 \u00d7 6 images. Each\nrow of X was a 36 dimensional vector of pixel inten-\nsity values that was generated by using Z to linearly\ncombine a subset of the four binary factors shown in\nFigure 6. Gaussian white noise, N(0,\u03c3X), was then\nadded to each image, yielding X = ZA + E. The\nfeature vectors, Zn\u00b7were sampled from a distribution\nin which each factor was present with probability 0.5.\nFigure 7 shows four of these images with different \u03c3X\nvalues.\nFigure 6. The four binary latent factors used in the sensi-\ntivity analysis in this section. The white squares are ones\nand the dark squares are zeros.\nWe initialized the MEIBP with K = 20, \u03c3X=1.0,\n\u03c3A= 1.0, \u03b1 = 2, \u02dc \u00b5kd\u223c |N(0,0.05)| (variational fac-\ntor means), \u02dc \u03c3kd\u223c |N(0,0.1)| (variational factor stan-\ndard deviations), znk \u223c Bernoulli(1\ntialization, we tested the MEIBP robustness by per-\nforming MEIBP inference on X for \u03c3X= 0.1,...,1.0\nin 100 evenly spaced increments with all hyperparam-\neters and algorithm options unchanged during the ex-\nperiment. MEIBP convergence was determined in the\nsame way as the main experimental section. Figure 8\n(top) shows a histogram of the final number of MEIBP\nfeatures (Ktrue= 4) and Figure 8 (bottom) shows the\nfinal number of MEIBP features as a function of \u03c3X.\n3). With this ini-\nFigure 7. Example data used in the sensitivity analysis dis-\ncussed in \u00a7S.5.1. Each column contains the same combina-\ntion of latent factors, where the top row has a data noise\nterm of \u03c3X = 0.1, the middle row has \u03c3X = 0.5, and the\nbottom row has \u03c3X = 1.0. Top: histogram of final K+\nvalue. Bottom: final K+ value as a function of \u03c3X.\nThese results indicate that the regularizing nature of\nthe IBP prior tends to lead to the correct number of\nlatent features even when the K+bound is much larger\nthan the true K+. Furthermore this experiment indi-\ncates that MEIBP inference is robust to model noise,\nat least, for the simple data used in this experiment.\nAt a medium level of data noise, the inference occa-\nsionally finished with K+ = 3, which resulted from\ntwo true latent factors collapsing to the same inferred\nlatent feature. Once this occurred, MEIBP did not\nhave a mechanism for splitting the features. For \u03c3X\ncomparable to the latent factors, \u03c3X \u2265 0.9, MEIBP\noften inferred \u201cnoise features,\u201d which were essentially\nwhitenoise and were typically active for less than 4%\nof the data instances. In future experiments we will\nattempt to flesh out the practical differences between\nunbounded priors and priors that operate in a large\nbounded latent space.\n12345678\n0\n20\n40\n60\nK+\ncount\n00.20.40.60.81\n2\n3\n4\n5\n6\n7\n\u03c3X\nK+\nFigure 8. Final feature count (K+ value) for MEIBP infer-\nence where Ktrue = 4 for the binary image data with K+\ninitialized to 20 for \u03c3X = 0.1,...,1.0 in 100 evenly spaced\nincrements with all hyperparameters and algorithm options\nfixed during the experiment."}],"widgetId":"rgw25_56ab1cac69bfa"},"id":"rgw25_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=236661557&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw26_56ab1cac69bfa"},"id":"rgw26_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=236661557&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":236661557,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":236661557,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":70339848,"url":"researcher\/70339848_Xinghao_Pan","fullname":"Xinghao Pan","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278772864569363%401443476038875_m\/Xinghao_Pan.png"},{"id":70262224,"url":"researcher\/70262224_Stefanie_Jegelka","fullname":"Stefanie Jegelka","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2043680896,"url":"researcher\/2043680896_Joseph_E_Gonzalez","fullname":"Joseph E Gonzalez","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2053857884,"url":"researcher\/2053857884_Joseph_K_Bradley","fullname":"Joseph K Bradley","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Conference Paper","publicationDate":"Dec 2014","journal":null,"showEnrichedPublicationItem":false,"citationCount":2,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/265444271_Parallel_Double_Greedy_Submodular_Maximization","usePlainButton":true,"publicationUid":265444271,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/265444271_Parallel_Double_Greedy_Submodular_Maximization","title":"Parallel Double Greedy Submodular Maximization","displayTitleAsLink":true,"authors":[{"id":70339848,"url":"researcher\/70339848_Xinghao_Pan","fullname":"Xinghao Pan","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278772864569363%401443476038875_m\/Xinghao_Pan.png"},{"id":70262224,"url":"researcher\/70262224_Stefanie_Jegelka","fullname":"Stefanie Jegelka","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2043680896,"url":"researcher\/2043680896_Joseph_E_Gonzalez","fullname":"Joseph E Gonzalez","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2053857884,"url":"researcher\/2053857884_Joseph_K_Bradley","fullname":"Joseph K Bradley","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":65912024,"url":"researcher\/65912024_Michael_I_Jordan","fullname":"Michael I Jordan","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Advances in Neural Information Processing Systems, Montreal, Quebec, Canada; 12\/2014"],"abstract":"Many machine learning algorithms can be reduced to the maximization of submodular functions. Although well understood in the serial setting, the parallel maximization of submodular functions remains an open area of research with recent results only addressing monotone functions. The optimal algorithm for maximizing the more general class of non-monotone submodular functions was introduced by Buchbinder et al. (2012) and follows a strongly serial double-greedy logic and program analysis. In this work, we propose two methods to parallelize the double-greedy algorithm. The first, coordination-free approach emphasizes speed at the cost of a weaker approximation guarantee. The second, concurrency control approach guarantees a tight 1\/2-approximation, at the quantifiable cost of additional coordination and reduced parallelism. As a consequence we explore the tradeoff space between guaranteed performance and objective optimality. We implement and evaluate both algorithms on multi-core hardware and billion edge graphs, demonstrating both the scalability and tradeoffs of each approach.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/265444271_Parallel_Double_Greedy_Submodular_Maximization","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Xinghao_Pan\/publication\/265444271_Parallel_Double_Greedy_Submodular_Maximization\/links\/5457acde0cf26d5090ab4e45.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Xinghao_Pan","sourceName":"Xinghao Pan","hasSourceUrl":true},"publicationUid":265444271,"publicationUrl":"publication\/265444271_Parallel_Double_Greedy_Submodular_Maximization","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/265444271_Parallel_Double_Greedy_Submodular_Maximization\/links\/5457acde0cf26d5090ab4e45\/smallpreview.png","linkId":"5457acde0cf26d5090ab4e45","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=265444271&reference=5457acde0cf26d5090ab4e45&eventCode=&origin=publication_list","widgetId":"rgw30_56ab1cac69bfa"},"id":"rgw30_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=265444271&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"5457acde0cf26d5090ab4e45","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":236661557,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/265444271_Parallel_Double_Greedy_Submodular_Maximization\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["However, many important applications in machine learning lead to non-monotone submodular functions. For example, graphical model inference [5] [17], or trading off any submodular gain maximization with costs (functions of the form F (S) = G(S) \u2212 \u03bbM (S), where G(S) is monotone submodular and M (S) a linear (modular) cost function), such as for utility-privacy tradeoffs [18], require maximizing non-monotone submodular functions. For non-monotone functions, the simple greedy algorithm in [8] can perform arbitrarily poorly (see Appendix H.1 for an example). "],"widgetId":"rgw31_56ab1cac69bfa"},"id":"rgw31_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw29_56ab1cac69bfa"},"id":"rgw29_56ab1cac69bfa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=265444271&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":59370900,"url":"researcher\/59370900_Ashwinkumar_Badanidiyuru","fullname":"Ashwinkumar Badanidiyuru","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277611747004417%401443199206164_m"},{"id":58006559,"url":"researcher\/58006559_Baharan_Mirzasoleiman","fullname":"Baharan Mirzasoleiman","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":22792717,"url":"researcher\/22792717_Amin_Karbasi","fullname":"Amin Karbasi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":310456,"url":"researcher\/310456_Andreas_Krause","fullname":"Andreas Krause","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Conference Paper","publicationDate":"Jan 2014","journal":null,"showEnrichedPublicationItem":false,"citationCount":2,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/264654316_Streaming_Submodular_Maximization_Massive_Data_Summarization_on_the_Fly","usePlainButton":true,"publicationUid":264654316,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/264654316_Streaming_Submodular_Maximization_Massive_Data_Summarization_on_the_Fly","title":"Streaming Submodular Maximization: Massive Data Summarization on the Fly","displayTitleAsLink":true,"authors":[{"id":59370900,"url":"researcher\/59370900_Ashwinkumar_Badanidiyuru","fullname":"Ashwinkumar Badanidiyuru","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277611747004417%401443199206164_m"},{"id":58006559,"url":"researcher\/58006559_Baharan_Mirzasoleiman","fullname":"Baharan Mirzasoleiman","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":22792717,"url":"researcher\/22792717_Amin_Karbasi","fullname":"Amin Karbasi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":310456,"url":"researcher\/310456_Andreas_Krause","fullname":"Andreas Krause","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Proc. ACM Conference on Knowledge Discovery in Databases (KDD); 01\/2014"],"abstract":"How can one summarize a massive data set \"on the fly\", i.e., without even having seen it in its entirety? In this paper, we address the problem of extracting representative elements from a large stream of data. I.e., we would like to select a subset of say k data points from the stream that are most representative according to some objective function. Many natural notions of \"representativeness\" satisfy submodular-ity, an intuitive notion of diminishing returns. Thus, such problems can be reduced to maximizing a submodular set function subject to a cardinality constraint. Classical ap-proaches to submodular maximization require full access to the data set. We develop the first efficient streaming algo-rithm with constant factor 1\/2 \u2212 \u03b5 approximation guaran-tee to the optimum solution, requiring only a single pass through the data, and memory independent of data size. In our experiments, we extensively evaluate the effectiveness of our approach on several applications, including training large-scale kernel methods and exemplar-based clustering, on millions of data points. We observe that our streaming method, while achieving practically the same utility value, runs about 100 times faster than previous work.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/264654316_Streaming_Submodular_Maximization_Massive_Data_Summarization_on_the_Fly","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Baharan_Mirzasoleiman\/publication\/264654316_Streaming_Submodular_Maximization_Massive_Data_Summarization_on_the_Fly\/links\/53ea2f4c0cf28f342f41887e.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Baharan_Mirzasoleiman","sourceName":"Baharan Mirzasoleiman","hasSourceUrl":true},"publicationUid":264654316,"publicationUrl":"publication\/264654316_Streaming_Submodular_Maximization_Massive_Data_Summarization_on_the_Fly","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/264654316_Streaming_Submodular_Maximization_Massive_Data_Summarization_on_the_Fly\/links\/53ea2f4c0cf28f342f41887e\/smallpreview.png","linkId":"53ea2f4c0cf28f342f41887e","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=264654316&reference=53ea2f4c0cf28f342f41887e&eventCode=&origin=publication_list","widgetId":"rgw33_56ab1cac69bfa"},"id":"rgw33_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=264654316&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"53ea2f4c0cf28f342f41887e","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":236661557,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/264654316_Streaming_Submodular_Maximization_Massive_Data_Summarization_on_the_Fly\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Over the recent years, submodular optimization has been identified as a powerful tool for numerous data mining and machine learning applications including viral marketing [17], network monitoring [22], news article recommendation [10], nonparametric learning [14] [29], document and corpus summarization [23] [7] [33], network inference [30], and Determinantal Point Processes [13]. A problem of key importance in all these applications is to maximize a monotone submodular function subject to a cardinality constraint (i.e., a bound on the number k of elements that can be selected). "],"widgetId":"rgw34_56ab1cac69bfa"},"id":"rgw34_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw32_56ab1cac69bfa"},"id":"rgw32_56ab1cac69bfa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=264654316&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2045639709,"url":"researcher\/2045639709_Nabeel_Gillani","fullname":"Nabeel Gillani","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278772638076935%401443475984455_m"},{"id":49508295,"url":"researcher\/49508295_Rebecca_Eynon","fullname":"Rebecca Eynon","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2051939493,"url":"researcher\/2051939493_Michael_Osborne","fullname":"Michael Osborne","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2046440867,"url":"researcher\/2046440867_Isis_Hjorth","fullname":"Isis Hjorth","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":[[]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Mar 2014","journal":null,"showEnrichedPublicationItem":false,"citationCount":1,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/260946406_Communication_Communities_in_MOOCs","usePlainButton":true,"publicationUid":260946406,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/260946406_Communication_Communities_in_MOOCs","title":"Communication Communities in MOOCs","displayTitleAsLink":true,"authors":[{"id":2045639709,"url":"researcher\/2045639709_Nabeel_Gillani","fullname":"Nabeel Gillani","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278772638076935%401443475984455_m"},{"id":49508295,"url":"researcher\/49508295_Rebecca_Eynon","fullname":"Rebecca Eynon","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2051939493,"url":"researcher\/2051939493_Michael_Osborne","fullname":"Michael Osborne","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2046440867,"url":"researcher\/2046440867_Isis_Hjorth","fullname":"Isis Hjorth","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7326333,"url":"researcher\/7326333_Stephen_Roberts","fullname":"Stephen Roberts","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Massive Open Online Courses (MOOCs) bring together thousands of people from\ndifferent geographies and demographic backgrounds -- but to date, little is\nknown about how they learn or communicate. We introduce a new content-analysed\nMOOC dataset and use Bayesian Non-negative Matrix Factorization (BNMF) to\nextract communities of learners based on the nature of their online forum\nposts. We see that BNMF yields a superior probabilistic generative model for\nonline discussions when compared to other models, and that the communities it\nlearns are differentiated by their composite students' demographic and course\nperformance indicators. These findings suggest that computationally efficient\nprobabilistic generative modelling of MOOCs can reveal important insights for\neducational researchers and practitioners and help to develop more intelligent\nand responsive online learning environments.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/260946406_Communication_Communities_in_MOOCs","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Stephen_Roberts9\/publication\/260946406_Communication_Communities_in_MOOCs\/links\/00b495346a7e4970c3000000.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Stephen_Roberts9","sourceName":"Stephen J. Roberts","hasSourceUrl":true},"publicationUid":260946406,"publicationUrl":"publication\/260946406_Communication_Communities_in_MOOCs","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/260946406_Communication_Communities_in_MOOCs\/links\/00b495346a7e4970c3000000\/smallpreview.png","linkId":"00b495346a7e4970c3000000","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=260946406&reference=00b495346a7e4970c3000000&eventCode=&origin=publication_list","widgetId":"rgw36_56ab1cac69bfa"},"id":"rgw36_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=260946406&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"00b495346a7e4970c3000000","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":236661557,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/260946406_Communication_Communities_in_MOOCs\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw35_56ab1cac69bfa"},"id":"rgw35_56ab1cac69bfa","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=260946406&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":236661557,"publicationLink":"publication\/236661557_Scaling_the_Indian_Buffet_Process_via_Submodular_Maximization","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw28_56ab1cac69bfa"},"id":"rgw28_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=236661557&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=6","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":6,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw27_56ab1cac69bfa"},"id":"rgw27_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=236661557&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/236661557_Scaling_the_Indian_Buffet_Process_via_Submodular_Maximization","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1cac69bfa"},"id":"rgw2_56ab1cac69bfa","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":236661557},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=236661557&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1cac69bfa"},"id":"rgw1_56ab1cac69bfa","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"p3Y9dIQrhaW\/8HHJeTLUZI59DFpHTIcBIBc0\/utAiTLMvD8JOBM4mm7P8E4jNu9IQiNalfuX5MbtBJjSehl7zFGmYJ4ia7YKI0YHRPj5EqIySEklolizfehXoNqIFebzmUSajVSD8n+2ccd0KVtQzDLNBnITs+MIFKheAARZA\/Kf7cMn9y9Ei8Clac0jHnyOhppiq8dCDOXfDEiqwj5hD7baBNoPTD9ED6biM0N5H64gPUiQeF\/27iQR\/ZlgmiK68CftA+mWEEUIzkk4paQIMHh10R5A3U0IznveB7R0G9Y=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/236661557_Scaling_the_Indian_Buffet_Process_via_Submodular_Maximization\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Scaling the Indian Buffet Process via Submodular Maximization\" \/>\n<meta property=\"og:description\" content=\"Inference for latent feature models is inherently difficult as the inference\nspace grows exponentially with the size of the input data and number of latent\nfeatures. In this work, we use Kurihara...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/236661557_Scaling_the_Indian_Buffet_Process_via_Submodular_Maximization\/links\/034fa80a0cf2ac15472e918a\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/236661557_Scaling_the_Indian_Buffet_Process_via_Submodular_Maximization\" \/>\n<meta property=\"rg:id\" content=\"PB:236661557\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Scaling the Indian Buffet Process via Submodular Maximization\" \/>\n<meta name=\"citation_author\" content=\"Colorado Reed\" \/>\n<meta name=\"citation_author\" content=\"Zoubin Ghahramani\" \/>\n<meta name=\"citation_publication_date\" content=\"2013\/04\/11\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/236661557_Scaling_the_Indian_Buffet_Process_via_Submodular_Maximization\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/236661557_Scaling_the_Indian_Buffet_Process_via_Submodular_Maximization\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-3722ef69-f6e5-415b-a098-098486e6a1e3","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":629,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw37_56ab1cac69bfa"},"id":"rgw37_56ab1cac69bfa","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-3722ef69-f6e5-415b-a098-098486e6a1e3", "d85da18a8d2ea5544ed8741e8c1a431b6ef8ef55");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-3722ef69-f6e5-415b-a098-098486e6a1e3", "d85da18a8d2ea5544ed8741e8c1a431b6ef8ef55");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw38_56ab1cac69bfa"},"id":"rgw38_56ab1cac69bfa","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/236661557_Scaling_the_Indian_Buffet_Process_via_Submodular_Maximization","requestToken":"HESzZHwM7D+WaBMRwVMfUIilHrNAjjUzNO3mq+vGk7csCIdgBCaxmD\/VojBfyG\/qN1XyNfNJtHDn7J7pkRp2YdjyywP7NKqFSNEM4lk1qnUCCxe8NgBeYBJjf1uPE9xDRwSnj+pETAIDFA8eodim0OOy1Un3V6UgwuJvR3WiHJLvzR3Tbx3M9r9c2WFdHl13\/7DRrSmADdl09NDSYOROMVj1F3CRFipEFaXgohLWcTr+WHYSWXVJmHp5zDmRdM\/l5qZtcMJLAgjulPa7Xe1alc2yZND+QIzdKRiVKH8pd4Q=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=J2kO1uBnuC0nKfby9AkLSO1a2A1KtUr6s-qPRir3q2h4Qv42o3-8z3uBvcKX2_kA","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjM2NjYxNTU3X1NjYWxpbmdfdGhlX0luZGlhbl9CdWZmZXRfUHJvY2Vzc192aWFfU3VibW9kdWxhcl9NYXhpbWl6YXRpb24%3D","signupCallToAction":"Join for free","widgetId":"rgw40_56ab1cac69bfa"},"id":"rgw40_56ab1cac69bfa","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw39_56ab1cac69bfa"},"id":"rgw39_56ab1cac69bfa","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw41_56ab1cac69bfa"},"id":"rgw41_56ab1cac69bfa","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
