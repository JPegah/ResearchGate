<!DOCTYPE html> <html lang="en" class="" id="rgw39_56ab19254c6ed"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="kxu4YO9573zl4uHqCF27Gpspz94CVmhQi1DmdN+X/DNriX1aTnHpXrhM3SyHw8kjTeM5erndp3eZ/g2zt3OVEvmmjrx3gdOiUCBC5YydUJfKqSnlnuGNbpdVSYjRrlRC9NXpRhW7nwd31uabwMEOHJLXN/dmfACYwJL/tz1EnAj2WEyt+0h4ujkigaWoPg+XwCUn2aBPXeTp6utqXXFTqwjEXVC6j9kERkWLvuwRpVfJi7zRUpyUfMZ2wK0i0hH4o3pWuKQRDxnVyz+AAkZowXiICi1odOHNP72iZt+ST6I="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-04006536-4fcd-4627-a16e-331b72b90e3d",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process" />
<meta property="og:description" content="Stochastic variational inference (SVI) is emerging as the most promising
candidate for scaling inference in Bayesian probabilistic models to large
datasets. However, the performance of these..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process/links/55a53c7b08aef604aa042e0b/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process" />
<meta property="rg:id" content="PB:279309917" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process" />
<meta name="citation_author" content="Amar Shah" />
<meta name="citation_author" content="David A. Knowles" />
<meta name="citation_author" content="Zoubin Ghahramani" />
<meta name="citation_publication_date" content="2015/06/26" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/David_Knowles2/publication/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process/links/55a53c7b08aef604aa042e0b.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/215868066921738/styles/pow/publicliterature/FigureList.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab19254c6ed" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab19254c6ed" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab19254c6ed">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=An%20Empirical%20Study%20of%20Stochastic%20Variational%20Algorithms%20for%20the%20Beta%20Bernoulli%20Process&rft.date=2015&rft.au=Amar%20Shah%2CDavid%20A.%20Knowles%2CZoubin%20Ghahramani&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process</h1> <meta itemprop="headline" content="An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process/links/55a53c7b08aef604aa042e0b/smallpreview.png">  <div id="rgw7_56ab19254c6ed" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab19254c6ed"> <a href="researcher/2076919269_Amar_Shah" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Amar Shah" alt="Amar Shah" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Amar Shah</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw9_56ab19254c6ed">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2076919269_Amar_Shah"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Amar Shah" alt="Amar Shah" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2076919269_Amar_Shah" class="display-name">Amar Shah</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56ab19254c6ed" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/David_Knowles2" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="David A. Knowles" alt="David A. Knowles" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">David A. Knowles</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw11_56ab19254c6ed" data-account-key="David_Knowles2">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/David_Knowles2"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="David A. Knowles" alt="David A. Knowles" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/David_Knowles2" class="display-name">David A. Knowles</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Stanford_University" title="Stanford University">Stanford University</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56ab19254c6ed"> <a href="researcher/8159937_Zoubin_Ghahramani" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Zoubin Ghahramani</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw13_56ab19254c6ed">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/8159937_Zoubin_Ghahramani"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/8159937_Zoubin_Ghahramani" class="display-name">Zoubin Ghahramani</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">        <meta itemprop="datePublished" content="2015-06">  06/2015;               <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1506.08180" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw14_56ab19254c6ed" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>Stochastic variational inference (SVI) is emerging as the most promising<br />
candidate for scaling inference in Bayesian probabilistic models to large<br />
datasets. However, the performance of these methods has been assessed primarily<br />
in the context of Bayesian topic models, particularly latent Dirichlet<br />
allocation (LDA). Deriving several new algorithms, and using synthetic, image<br />
and genomic datasets, we investigate whether the understanding gleaned from LDA<br />
applies in the setting of sparse latent factor models, specifically beta<br />
process factor analysis (BPFA). We demonstrate that the big picture is<br />
consistent: using Gibbs sampling within SVI to maintain certain posterior<br />
dependencies is extremely effective. However, we find that different posterior<br />
dependencies are important in BPFA relative to LDA. Particularly,<br />
approximations able to model intra-local variable dependence perform best.</div> </p>  </div>   </div>     <div id="rgw15_56ab19254c6ed" class="figure-carousel"> <div class="carousel-hd"> Figures in this publication </div> <div class="carousel-bd"> <ul class="clearfix">  <li> <a href="/figure/279309917_fig1_Figure-4-Results-from-interpolation-and-denoising-of-the-512-512-pixel-&#39;Barbara&#39;" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 4. Results from interpolation and denoising of the 512 × 512..." data-key="279309917_fig1_Figure-4-Results-from-interpolation-and-denoising-of-the-512-512-pixel-&#39;Barbara&#39;"> <img class="fig" src="https://www.researchgate.net/profile/David_Knowles2/publication/279309917/figure/fig1/Figure-4-Results-from-interpolation-and-denoising-of-the-512-512-pixel-&#39;Barbara&#39;_small.png" alt="Figure 4. Results from interpolation and denoising of the 512 × 512..." title="Figure 4. Results from interpolation and denoising of the 512 × 512..."/> </a> </li>  </ul> </div> </div> <div class="action-container"> <div id="rgw16_56ab19254c6ed" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw30_56ab19254c6ed">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw31_56ab19254c6ed">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/David_Knowles2/publication/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process/links/55a53c7b08aef604aa042e0b.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/David_Knowles2">David A. Knowles</a>, <span class="js-publication-date"> Jul 14, 2015 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw33_56ab19254c6ed" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw34_56ab19254c6ed" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw35_56ab19254c6ed" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw36_56ab19254c6ed" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw37_56ab19254c6ed" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw38_56ab19254c6ed" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw32_56ab19254c6ed" src="https://www.researchgate.net/c/o1o9o3/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FDavid_Knowles2%2Fpublication%2F279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process%2Flinks%2F55a53c7b08aef604aa042e0b.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw29_56ab19254c6ed"  itemprop="articleBody">  <p>Page 1</p> <p>An Empirical Study of Stochastic Variational<br />Algorithms for the Beta Bernoulli Process<br />Amar Shah?<br />David A. Knowles†<br />Zoubin Ghahramani?<br />?University of Cambridge, Department of Engineering, Cambridge, UK<br />†Stanford University, Department of Computer Science, Stanford, CA, USA<br />AS793@CAM.AC.UK<br />DAVIDKNOWLES@CS.STANFORD.EDU<br />ZOUBIN@ENG.CAM.AC.UK<br />Abstract<br />Stochastic variational inference (SVI) is emerg-<br />ing as the most promising candidate for scal-<br />ing inference in Bayesian probabilistic models<br />to large datasets. However, the performance of<br />these methods has been assessed primarily in the<br />context of Bayesian topic models, particularly<br />latent Dirichlet allocation (LDA). Deriving sev-<br />eral new algorithms, and using synthetic, image<br />and genomic datasets, we investigate whether the<br />understanding gleaned from LDA applies in the<br />setting of sparse latent factor models, specifi-<br />cally beta process factor analysis (BPFA). We<br />demonstrate that the big picture is consistent: us-<br />ing Gibbs sampling within SVI to maintain cer-<br />tain posterior dependencies is extremely effec-<br />tive. However, we find that different posterior<br />dependencies are important in BPFA relative to<br />LDA. Particularly, approximations able to model<br />intra-local variable dependence perform best.<br />1. Introduction<br />The last two decades have seen an explosion in the de-<br />velopment of flexible statistical methods able to model di-<br />verse data sources. Bayesian nonparametric priors in par-<br />ticular provide a powerful framework to enable models to<br />adapt their complexity to the data at hand (Orbanz &amp; Teh,<br />2010). In the regression setting this might mean learn-<br />ing the smoothness of the output function (Rasmussen &amp;<br />Williams, 2006), for clustering adapting the number of<br />components (MacEachern &amp; M¨ uller, 1998), and in the case<br />of our interest, latent factor models, finding an appropri-<br />ate number of latent features (Knowles et al., 2011). While<br />such models are appealing for a range of applied data anal-<br />Proceedings of the 32ndInternational Conference on Machine<br />Learning, Lille, France, 2015. JMLR: W&amp;CP volume 37. Copy-<br />right 2015 by the author(s).<br />ysis applications, their scalability is often limited. The<br />posterior distribution over parameters and latent variables<br />is typically analytically intractable and highly multimodal,<br />making MCMC, particularly Gibbs sampling, the norm.<br />Along with concerns over performance and convergence,<br />MCMC methods are often impractical for the applied prac-<br />titioner: how should the multiple samples be summarized?<br />Variational methods work on the basis that simply finding<br />a good posterior mode, and giving some measure of the<br />associated uncertainty, is typically sufficient. In addition,<br />the predictive performance of variational methods is often<br />comparable to more computationally expensive sampling<br />based approaches (Ghahramani &amp; Beal, 1999).<br />Recently stochastic variational inference has begun to<br />emerge as the most promising avenue for scaling inference<br />in large latent variable models (Hoffman et al., 2013). Mar-<br />rying variational inference with stochastic gradient descent<br />allows principled updates using only minibatches of obser-<br />vations, greatly improving data scalability. While some<br />MCMC methods have been proposed to work with mini-<br />batches (Welling &amp; Teh, 2011; Ahn et al., 2012) they lack<br />theoretical guarantees and apply only to continuous, un-<br />bounded latent variables. While SVI has been influential<br />for Bayesian topic modeling, particularly latent Dirichlet<br />allocation (LDA, Mimno et al., 2012; Hoffman &amp; Blei,<br />2014; Wang &amp; Blei, 2012), the same cannot be said for<br />sparse factor analysis models for continuous data. While<br />the former has been driven by the ready availability of huge<br />text corpora, the scale of continuous data being generated<br />bynewgenomictechnologiesisstillgrowing. Forexample,<br />CyTOF, single cell time of flight mass spectrometry is able<br />tomeasuretheabundanceofdozensofproteinsinhundreds<br />of thousands of cells in a single run (Bendall et al., 2011).<br />Such complex, large scale, high dimensional datasets re-<br />quire sophisticated statistical models, but are typically ana-<br />lyzed using simple heuristic clustering methods or PCA,<br />which do not capture important structure, such as spar-<br />sity. As a result, scaling more advanced factor analysis type<br />models is of great interest.<br />arXiv:1506.08180v1  [stat.ML]  26 Jun 2015</p>  <p>Page 2</p> <p>Empirical Study of SVI for the Beta Bernoulli Process<br />When desigining a variational approximation to a poste-<br />rior distribution, one must trade off the accuracy of the<br />approximation with the complexity of optimizing the ev-<br />idence lower bound. Mean field approximations are sim-<br />ple to work with, but Hoffman &amp; Blei (2014) demonstrated<br />that in the context of LDA, maintaining posterior depen-<br />dence between “global” variables (topic vectors) and “lo-<br />cal” variables (document vectors) is crucial to finding good<br />solutions. Does this finding hold for sparse factor analy-<br />sis models? Our results suggest that contrary to the LDA<br />case, maintaining dependencies amongst local variables is<br />actually the most important ingredient for obtaining good<br />performance with beta Bernoulli process SVI.<br />2. Beta Process for Factor Analysis<br />The beta process (Hjort, 1990; Thibaux &amp; Jordan, 2007) is<br />an independent increments process defined as follows:<br />Definition 1. Let Ω be a measurable space and B its σ-<br />algebra. Let H0be a continuous probability measure on<br />(Ω,B) and α a positive scalar. Then for all disjoint, in-<br />finitesimal partitions, {B1,...,BK}, of Ω the beta process<br />is generated as follows,<br />H(Bk)<br />iid<br />∼ Beta(αH0(Bk),α(1 − H0(Bk)))<br />with K → ∞ and H0(Bk) → 0 for k = 1,...,K. We<br />denote the process H ∼ BP(αH0).<br />Hjort considers a generalization of this definition including<br />functions, α(Bk), which we set as constants for the sake<br />of simplicity. Analogous to the Dirichlet process, the beta<br />process may be written in set function form as<br />(1)<br />H(ω) =<br />∞<br />?<br />k=1<br />πkδωk(ω)<br />(2)<br />with H(ωi) = πi. Note that the beta process is not a nor-<br />malized random measure. Hence the π of a beta process<br />does not represent a probability mass function on Ω, but<br />instead can be used to parametrize the Bernoulli process, a<br />new measure on Ω defined as follows:<br />Definition 2. Let zi be an infinite row vector with kth<br />value, zik, generated by zik|πk<br />measure defined by Xi(ω) =?<br />If we were to stack samples of the infinite-dimensional vec-<br />tor, zi, to form a matrix, Z = [z1?,...,zN?]?, we may<br />view the beta-Bernoulli process as a prior over infinite bi-<br />nary matrices (Griffiths &amp; Ghahramani, 2011), where each<br />column in the matrix Z corresponds to a location, δω.<br />iid<br />∼ Bernoulli(πk). The<br />kzikδωk(ω) is then a draw<br />from a Bernoulli process, which we denote Xi∼ BeP(H).<br />Sampling H directly, as defined in (2), is difficult to do ex-<br />actly and efficiently. But, just as Aldous (1985) derived the<br />Chinese restaurant process, a marginalized approach used<br />for sampling from the Dirichlet process, there exists an<br />efficient marginalized scheme for sampling from the beta<br />process, called the Indian buffet process (IBP, Griffiths &amp;<br />Ghahramani, 2006; Thibaux &amp; Jordan, 2007).<br />The IBP sampling procedure introduces strong dependen-<br />cies between the rows of Z. Our goal is to derive a stochas-<br />tic variational inference scheme where we consider rows in<br />batches. It will hence be crucial to instantiate the global<br />parameters rather than marginalize over them.<br />For this reason, we shall consider a finite approximation<br />to the beta process which simply set K to a large, finite<br />number. The finite representation is written as<br />H(ω) =<br />K<br />?<br />k=1<br />πkδωk(ω)<br />πk∼ Beta(a/K,b(K − 1)/K),<br />and the K-dimensional vector, zi, is drawn from a finite<br />Bernoulli process parameterized by H.<br />ωk∼ H0<br />(3)<br />Consider modelling a data matrix Y ∈ RN×Dwhere rows<br />represent data points. Factor analysis models this data as<br />the product of two matrices L ∈ RN×Kand Φ ∈ RK×D,<br />plus an error matrix, E.<br />Y = LΦ + E<br />(4)<br />Prior belief about the structure of the data may be used<br />to induce the desired propeties of L and Φ, e.g. sparsity<br />(West, 2003; Rai &amp; Daum´ e, 2008; Knowles &amp; Ghahra-<br />mani, 2007). To encourage sparsity, we model L as the<br />Hadamard (element-wise) product between matrices Z and<br />W, L = Z ◦ W, where Z is binary and W is a Gaussian<br />weight matrix. This idea is described in Section 3 of (Grif-<br />fiths &amp; Ghahramani, 2011). We model the matrices Φ and<br />Z as N draws from a beta-Bernoulli process parameterized<br />by a beta process, H.<br />Using the truncated beta process of (3), we have the fol-<br />lowing generative process for observation i = 1,...,N and<br />features k = 1,...,K,<br />yi= (zi◦ wi)Φ + ?i<br />wi∼ N(0,γ−1<br />zik|πk∼ Bernoulli(πk)<br />?i∼ N(0,γ−1<br />πk∼ Beta(a/K,b(K − 1)/K)<br />φk∼ N(0,D−1I)<br />where all values are drawn independently. This is the gen-<br />erative model used for beta process factor analysis (Pais-<br />ley &amp; Carin, 2009). We place independent Gamma(c?,d?)<br />(5)<br />wI)<br />obsI)<br /><br /><br /><br /><br /><br />?<br />Local variables<br />Global variables</p>  <p>Page 3</p> <p>Empirical Study of SVI for the Beta Bernoulli Process<br />and Gamma(e?,f?) priors on γobsand γwrespectively. The<br />separation of local and global variables will be crucial for<br />the stochastic variational inference algorithm which we de-<br />rive in the next section. For the sake of brevity, we denote<br />the set of global variables β ≡ {π,Φ,γw,γobs} and sets<br />of local variables ψi≡ {wi,zi} for i = 1,...,N.<br />3. Variational Inference Schemes<br />The true posterior distribution p(β,ψ1:N|x1:N) involves<br />complicated dependencies between latent variables, which<br />makes inference complicated. The goal of variational in-<br />ference is to approximate the true posterior with a family<br />of distributions q(β,ψ1:N).<br />ber of the chosen family of distributions by minimizing<br />the KL-divergence between this variational distribution and<br />the true posterior. Equivalently, we maximize the evidence<br />lower bound (ELBO),<br />We choose the best mem-<br />L(q) = Eq[logp(β,ψ1:N,x1:N) − logq(β,ψ1:N)]. (6)<br />In this work, we compare the performance of a range of<br />variational approximations. Each of the approximations we<br />consider factorizes as follows<br />?<br />where q(γobs) = Gamma(c,d), q(γw) = Gamma(e,f),<br />q(φk) = N(τ−1<br />The global variational distributions are all of the same ex-<br />ponential family forms as their posterior conditional dis-<br />tributions. The full set of global variational parameters is<br />λ = {ak,bk,c,d,e,f,τk,µk}. Due to conjugacy of our<br />model, it is easy to show that the updates for the global<br />variational parameters during the variational M-step are as<br />follows<br />?<br />bk= b(K − 1)/K +<br />?<br />d = d?+<br />2Eq<br />?<br />f = f?+<br />2Eq<br />?<br />µk=<br />i<br />q(β,ψ1:N) =q(γobs)q(γw)<br />?<br />k<br />q(πk)q(φk)<br />?<br />q(ψ1:N|β),<br />kµk,τ−1<br />kI) and q(πk) = Beta(ak,bk).<br />ak= a/K +<br />i<br />Eq[zik]<br />(7)<br />?<br />i<br />?<br />1 − Eq[zik]<br />?<br />c = c?+<br />i<br />D/2<br />?<br />i<br />1<br />???yi− (zi◦ wi)Φ?2?<br />e = e?+<br />i<br />K/2<br />?<br />i<br />1<br />?wiwi??<br />?γobszikwik2?<br />?zikwiky−k<br />τk= D +<br />i<br />Eq<br />?<br />Eq<br />i<br />?<br />where Eqis an expectation over all latent variables with<br />respect to q (except when global parameters are being sam-<br />pled), and y−k<br />i<br />= yi− Eq<br />with the natural parameters of the global variational dis-<br />tributions. Natural gradients give the direction of steepest<br />ascent in Riemannian space, leading to faster convergence<br />for e.g. maximum likelihood estimation (Amari, 1998).<br />??<br />j?=kzijwijφj<br />?. We work<br />Our aim is to update the global variational parameters<br />stochastically, by considering subsets of the full dataset<br />and making sequential updates.<br />tor of global natural parameters of p(β), and by condi-<br />tional conjugacy, the vector of global natural parameters<br />of q(β) is η +?<br />each of the ithelements of the sums in Equation 7. We<br />have followed the notation of Hoffman &amp; Blei (2014). The<br />general framework of stochastic variational inference we<br />shall follow is summarized in Algorithm 1.<br />Let η denote the vec-<br />iηi(yi,ψi). In fact, η = [a/K,b(K −<br />1)/K,c?,d?,e?,f?,D,0], and ηiis a vector consisting of<br />The difficult step is in computing ˆ ηi, and it is entirely de-<br />pendent on the form of the local variable approximation, of<br />which we consider 2 types: ‘Unstructured’ methods where<br />q(ψ1:N|β) = q(ψ1:N) and ‘structured’ methods where<br />this equivalence does not hold. Our notion of ‘structure’<br />describes the dependence between local and global vari-<br />ables, as discussed by Hoffman &amp; Blei (2014).<br />3.1. Unstructured Variational Methods<br />The simplest, and most commonly used, approximation we<br />can make is the mean field approximation,<br />qMF(ψi) =<br />K<br />?<br />k=1<br />q(zik)q(wik)<br />(8)<br />where q(zik)<br />N(κ−1<br />parameters,<br />Eq(β)qMF(ψ1:N)[logp(y1:N,ψ1:N|β)<br />is optimized as a function of local variational parameters<br />{θik,νik,κik}. Up to irrelevant constants,<br />c<br />2d<br />i,k<br />κik<br />?νik2<br />?<br />+<br />=Bernoulli(θik) and q(wik)<br />Given the current set of global<br />thelocalELBO,<br />=<br />ikνik,κ−1<br />ik).<br />λ(t),<br />Llocal<br />logq(ψ1:N)]<br />=<br />−<br />LMF−SVI<br />local<br />=<br />?<br />?<br />2θikνik<br />µk<br />τky?<br />i<br />− θik<br />κik2+<br />1<br />κik<br />??µkµ?<br />νik<br />κik<br />τjτk<br />k<br />τk2<br />+1<br />τk<br />?<br />−<br />j?=k<br />?<br />?<br />θijθikνij<br />κij<br />µjµ?<br />k<br />?<br />−<br />e<br />2f<br />?<br />log(κik)<br />i,k<br />?νik2<br />κik2+<br />1<br />κik<br />?<br />i,k<br />θik(ψ(ak) − ψ(bk)) −1<br />?θiklogθik+ (1 − θik)log(1 − θik)?<br />2<br />?<br />i,k<br />−<br />i,k<br />(9)</p>  <p>Page 4</p> <p>Empirical Study of SVI for the Beta Bernoulli Process<br />where ψ is the digamma function. The mean field approx-<br />imation breaks dependencies between all local and global<br />variables, and will provide a baseline to compare against.<br />It is possible to compute EqMF(ψi)[ηi] analytically given<br />the optimized local variational parameters. We denote the<br />SVI algorithm which uses a mean field local variable ap-<br />proximation as MF-SVI. It is identical to the original SVI<br />algorithm introduced by Hoffman et al. (2013).<br />Mimnoetal.(2012)suggestedanonlineSVImethodwhich<br />maintains structure between local variables specifically for<br />the LDA. We generalize their idea by suggesting the fol-<br />lowing variational distribution over local parameters<br />qMimno(ψi) = exp(Eq(β)[logp(ψi|y1:N,β)])<br />where p(ψi|y1:N,β) is the true posterior conditional of<br />ψi. Whilst we are unable to compute EqMimno(ψi)[ηi] ana-<br />lytically, we are able to estimate it using MCMC. The SVI<br />algorithm using qMimnoas the local variational distribution<br />shall be called Mimno-SVI.<br />(10)<br />3.2. Structured Variational Methods<br />Instead of taking an expectation over q(β) to compute ˆ ηi<br />as in the previous section, we use the current set of global<br />parameters λ(t)to draw a sample β(t), and compute an<br />estimate of Eq(ψi|β(t))[ηi]. Under this framework, Algo-<br />rithm 1 becomes the SSVI-A algorithm of Hoffman &amp; Blei<br />(2014).<br />Once again, the simplest approximation that can be made is<br />the conditional mean-field approximation, where zik,wik<br />are independent given β, with q(zik) = Bernoulli(θik)<br />and q(wik) = N(κ−1<br />the local ELBO, EqMF(ψ1:N|β(t))[logp(y1:N,ψ1:N|β(t))−<br />logq(ψ1:N)] as a function of local variational parameters<br />{θik,νik,κik}, and compute EqMF(ψ1:N|β(t))[ηi] analyti-<br />cally given the optimized parameters. We shall call this<br />SVI method MF-SSVI.<br />ikνik,κ−1<br />ik). This time, we optimize<br />The Bernoulli-Gaussian products present in the generative<br />process in Equation 5 can be thought of as a spike-and-<br />slab model. Titsias &amp; L´ azaro-Gredilla (2011) developed a<br />variational method which maintains dependence between<br />zikand wikfor eack k, such that<br />?<br />× N?wik;zikκ−1<br />This approximation has the advantage that it maintains the<br />spike-slab beaviour of the product zikwik, and matches the<br />exact posterior when zik = 0. However, the dependen-<br />cies between local variables for which k ?= k?are lost.<br />Analogous to MF-SSVI, we optimize the local ELBO us-<br />ing qTitsiasas a function of {θik,νik,κik}, and compute<br />qTitsias(ψi|β(t)) =<br />k<br />Bernoulli(zik;θik)<br />(11)<br />ikνik,zikκ−1<br />ik+ (1 − zik)γ(t)<br />w<br />−1?.<br />Algorithm 1 General Stochastic Variational Inference<br />Initialize t = 1, λ(0).<br />repeat<br />Compute step size ρ(t)= (t + t0)−ζ.<br />Select subset of full data set, D.<br />Compute ˆ ηi, an (unbiased) estimator of Eq(ψi|β)[ηi]<br />for each i ∈ D<br />Set λ(t)= (1−ρ(t))λ(t−1)+ρ(t)?η+N<br />|D|<br />?<br />i∈Dˆ ηi<br />?<br />until convergence<br />EqTitsias(ψ1:N|β(t))[ηi] analytically given the optimized pa-<br />rameters. We denote the SVI algorithm which uses the<br />Titsias &amp; L´ azaro-Gredilla (2011) local approximation as<br />Titsias-SSVI.<br />Finally we consider using the exact local conditional dis-<br />tribution given by q(ψi|β(t)) = p(ψi|β(t),yi). We use<br />MCMC samples to compute ˆ ηiusing a Gibbs sampling<br />scheme. We therefore call this method Gibbs-SSVI.<br />MF-SVI (Hoffman et al., 2013), MF-SSVI, Gibbs-SSVI<br />(Hoffman &amp; Blei, 2014) and Mimno-SVI (Mimno et al.,<br />2012) have been considered in the context of LDA before,<br />but the latter 3 have not been applied to factor analysis to<br />the best of our knowledge. Titsias-SSVI is a new method as<br />Titsias &amp; L´ azaro-Gredilla (2011) applied their variational<br />approximation only to regression tasks. More details on the<br />variational approximations over local variables is provided<br />in the appendix.<br />4. Related Work<br />The idea of applying variational inference to the Indian<br />buffet process was first proposed in Doshi et al. (2008),<br />based on the stick breaking construction of the IBP (Teh<br />et al., 2007). Promising results were shown for the sim-<br />ple but somewhat limited “linear Gaussian” model, which<br />is the model presented here without the weight vector, wi.<br />Paisley &amp; Carin (2009) consider the simpler finite approx-<br />imation to the beta process described above, and extended<br />the model to include continuous weights wi. An exten-<br />sion using power-EP, able to handle non-negativity con-<br />straints, was developed in (Ding et al., 2010) but has not<br />been widely adopted. Alternative approaches to scale in-<br />ference in IBP based models have included parallelization<br />(Doshi-Velez et al., 2009) and submodular optimization<br />(Reed &amp; Ghahramani, 2013). The former only performed<br />approximate sampling, and the later is greedy and limited<br />to positive weights. Mean field based stochastic variational<br />inference schemes have been used for large scale dictio-<br />nary learning, with some success (Li et al., 2012; Polatkan<br />et al., 2014). However, we shall show that preserving de-<br />pendencies between local variables will greatly improves<br />performance on image interpolation and denoising tasks.</p>  <p>Page 5</p> <p>Empirical Study of SVI for the Beta Bernoulli Process<br />0204060<br />0<br />2<br />4<br />·105<br />Training Time (s)<br />4<br />Training Time (s)<br />Predictive Loglikelihood<br />1<br />026<br />10<br />15<br />20<br />25<br />30<br />35<br />PSNR<br />MF-SVI<br />MF-SSVI<br />Titsias-SVI<br />Mimno-SVI<br />Gibbs-SSVI<br />Gibbs<br />1<br />Figure 1. Predicitve loglikelihood versus training time on syn-<br />thetically generated data, comparing Gibbs-SSVI, MF-SSVI and<br />Gibbs sampling. The same legend is used throughout this paper.<br />Meanwhile the topic modelling community has taken great<br />strides developing stochastic variational inference methods<br />for latent Dirichlet allocation (Blei et al., 2003), encour-<br />aged by the availability of large corpora of text. The idea<br />was initially proposed in Hoffman et al. (2010), and refined<br />in Mimno et al. (2012) where the sparse updates of Gibbs<br />sampling were leveraged to scale inference on just a single<br />machine to 1.2 million books. The latter idea allows non-<br />truncated online learning (Wang &amp; Blei, 2012) of Bayesian<br />non-parametricmodels, thoughonlythehierachicalDirich-<br />let process (Teh et al., 2004) was demonstrated.<br />More recently, Hoffman &amp; Blei (2014); Liang &amp; Hoff-<br />man (2014) have shown that sampling from the global<br />variational distribution improves predictive performance<br />for the LDA and Bayesian non-negative matrix factoriza-<br />tion respectively. In fact, the idea of optimizing an in-<br />tractable variational inference algorithm by sampling from<br />global variational distributions has been proposed in var-<br />ious contexts to deal with non-conjugacy (Ji et al., 2010;<br />Nott et al., 2012; Gerrish, 2013; Paisley et al., 2012; Ran-<br />ganath et al., 2014). Kingma &amp; Welling (2014); Titsias<br />&amp; L´ azaro-Gredilla (2014); Salimans &amp; Knowles (2013)<br />propose change of variable methods to deal with non-<br />conjugacy or improve convergence speed. In this work we<br />focus more on the quality of the variational approximation<br />and attempt to exploit the conditional conjugacy.<br />5. Experiments<br />In this section we discuss our findings from a range of ex-<br />periments. Results from experiments carried out on syn-<br />thetically generated data are discussed first. We apply a<br />range of stochastic variational inference algorithms to carry<br />out image inpainting and denoising tasks next. Finally the<br />00.511.52<br />−1<br />0<br />1<br />2<br />·104<br />Training Time (s)<br />Predictive Loglikelihood<br />1<br />Figure 2. Predicitve loglikelihood versus training time on synthet-<br />ically generated data using Gibbs-SSVI with burn-in lengths of<br />0,1,3,5,10 and 25. Converged predictive loglikelihood is mono-<br />tonically increasing in burn-in length, so no legend is included.<br />same algorithms are applied to two large genomic datasets.<br />We choose to compare our models using predictive loglike-<br />lihood of held out data, which we compute as<br />p<br />?ˆ Y |Y<br />≈<br />?<br />≈<br />?<br />?<br />p(ˆ Y |β,ψ1:Ntest)q(β,ψ1:Ntest)d(β,ψ1:Ntest)<br />?<br />1<br />M<br />M<br />m=1<br />Ntest<br />i=1<br />N<br />?<br />ˆ yn|z(m)<br />i<br />◦ w(m)<br />i<br />A(m),I/γ(m)<br />obs<br />?<br />(12)<br />where?z(m)<br />being used, and ˆ yiis the ithdata point in the test set.<br />In each of our experiments, we transform the data to have<br />empirical mean 0 and variance 1. Hyperparameters are set<br />as follows: a = b = 10, c = 1, d = 10, e = f = 1, and a<br />learning rate schedule of ρt= t−0.75is employed.<br />i<br />,w(m)<br />i<br />,A(m),γ(m)<br />obs<br />?are independent samples<br />from q, for whichever type of variational approximation is<br />5.1. Synthetic Data<br />A key question that is important to consider when using<br />variational approximations of a particular form is, ‘how<br />close is the approximation to the true posterior?’. We at-<br />tempt to answer such a question with our first experiment.<br />Datawasgeneratedfromourpriorwithparametersγw= 1,<br />γobs= 100, K = 80, N = 1e5 and D = 40, with 7.5%<br />selected uniformly at random held out for testing on 100<br />independent experiments. We then applied Gibbs-SSVI<br />and MF-SSVI, as well as an uncollapsed Gibbs sampler to<br />the generated data using K = 150 potential features and<br />random initialization. The predictive mean squared errors<br />(MSE) of the 3 methods were 0.022±0.002, 0.027±0.004</p>  <p>Page 6</p> <p>Empirical Study of SVI for the Beta Bernoulli Process<br />46<br />g Time (s)<br />02<br />Training Time (s)<br />46<br />10<br />15<br />20<br />25<br />30<br />35<br />PSNR<br />1<br />(a) Gibbs initialization<br />02<br />Training Time (s)<br />46<br />10<br />15<br />20<br />25<br />30<br />35<br />PSNR<br />02<br />Training Time (s)<br />46<br />10<br />15<br />20<br />25<br />30<br />35<br />PSNR<br />1<br />(b) Random initialization<br />Figure 3. Results from interpolation of the 512 × 512 pixel ‘Boat’ image. PSNR vs training time shown using (a) Gibbs initialization<br />and (b) random initialization. The pictures shown are the original image (top left), the image to be reconstructed with 80% of pixels<br />unobserved (top right), the Gibbs-SSVI reconstruction (bottom left) and the MF-SSVI reconstruction (bottom right).<br />and 0.020 ± 0.002 and the average per iteration training<br />times were 1.5, 0.6 and 7.6 seconds respectively. Figure 1<br />illustrates our findings. The Gibbs sampler achieves a high<br />predictive likelihood, but the average training time per iter-<br />ation was very high versus the SVI methods. Warm starting<br />the Gibbs sampler at each iteration helped Gibbs-SSVI to<br />converge in few iterations, whereas stochastically choos-<br />ing subsets of data points in SSVI methods requires re-<br />initializing local variables at each epoch. Notice that the<br />predictive MSE of Gibbs-SSVI is close to the Gibbs sam-<br />pler, suggesting that the correct mean is being learnt. The<br />lower likelihood the SSVI methods are able to achieve is<br />therefore due to a poor calibration in posterior variance, a<br />known issue with variational methods (Consonni &amp; Marin,<br />2007).<br />Another question of interest to us was, ‘what is the em-<br />pirical trade-off between training time and unbiasedness in<br />the Gibbs-SSVI scheme?’. More specifically, if we allow<br />the Gibbs sampler over the local variables to converge, the<br />subsequent ELBO gradient estimates would be unbiased,<br />whilst using samples from a Gibbs chain which has not<br />converged would lead to biased gradient estimates. Con-<br />vergence of the Gibbs chain, however, may take a long<br />time. Therefore we experimented with a range of burn-in<br />lengths of the Gibbs chain on synthetically generated data.<br />Various burn-in and sample length combinations were dis-<br />cussed by Mimno et al. (2012). We tried burn-in lengths<br />of 0,1,3,5,10 and 25 whilst fixing the number of samples<br />used after burn-in to 3. The results can be seen in Figure 2.<br />When the burn-in length is below 3 we notice severe loss<br />in predictive power of the Gibbs-SSVI method. We notice<br />diminishing gains in predictive power as we increase the<br />length of burn-in. This experiment suggests that some bias<br />introduced by using samples from an unconverged Gibbs<br />chain may be worth the reduction in training time. For sub-<br />sequent experiments, we fix the burn-in length to 3.<br />5.2. Image Interpolation and Denoising<br />Zhou et al. (2009) first applied the beta process for sparse<br />image representation with good results and much follow<br />up research. The standard metric used for quantifying the<br />quality of a reconstructed image is the peak signal-to-noise<br />ratio (PSNR), defined as 20log10(maximage/rmse), where<br />maximageis the maximum possible pixel value and rmse is<br />the root mean squared error of the reconstruction.<br />We consider overlapping 8 × 8 pixel patches as individ-<br />ual 64 dimensional data points. The fact that the patches<br />are overlapping technically breaks the exchangeability as-<br />sumption of the prior distribution, however the extra model<br />averaging is beneficial to prediction. Five grayscale im-<br />ages originally from Portilla et al. (2003) were used for our<br />study: Boat, Barbara, Lena, House and Peppers. The first 3<br />are 512 × 512 in size whilst the last 2 are 256 × 256. The<br />datasets are therefore of size N = (512 − 7)2= 255,025<br />and N = (256−7)2= 62,001 for 512×512 and 256×256<br />images respectively. We use a batchsize of Nsubset= 250<br />and K = 250 features for our experiments.<br />For our first experiment, we consider the task of image in-<br />terpolation, where the task is to reconstruct an image where<br />only 20% of the pixels, chosen uniformly at random, are<br />observed. Li et al. (2012) consider a mean field based vari-<br />ational approximation for such a task, however, the learn-<br />ing rate schedule they used was ρt= (t + 1000)−0.5. This<br />implies ρt&lt; 0.032 for all t ≥ 1, and that their algorithm<br />relied heavily on the initialization of global parameters.<br />They ran an MCMC algorithm over a subset of the data<br />to initialize these global parameters, and we argue that this<br />was integral to the performance of their algorithm. We de-<br />cided to test how sensitive the variational algorithms were</p>  <p>Page 7</p> <p>Empirical Study of SVI for the Beta Bernoulli Process<br />Figure 4. Results from interpolation and denoising of the 512 ×<br />512 pixel ‘Barbara’ image. The pictures shown are the original<br />image (top left), the image to be reconstructed with 50 % of pixels<br />unobserved and remaining pixels corrupted with Gaussian noise<br />(top right), the Gibbs-SSVI reconstruction (bottom left) and the<br />MF-SSVI reconstruction (bottom right).<br />to different initialization methods and an example can be<br />seen in the performance graphs of Figure 3. We found that<br />initializing using MCMC improved the PSNR of MF-SVI,<br />MF-SSVI and Titsias-SSVI by 4.8 on average versus ran-<br />dom initialization. However, the analogous improvement<br />for Gibbs-SSVI and Mimno-SVI was 1.0. This suggests<br />that the methods which preserve intra-local variable struc-<br />ture are less sensitive to initialization.<br />Secondly, we considered the joint task of image interpo-<br />lation and denoising. Here, we observe 50 % of the pix-<br />els chosen uniformly at random, except they are now cor-<br />rupted with Gaussian noise with standard deviation 15 (the<br />original pixels take integer values in [0,255]). Results for<br />both image interpolation and denoising tasks are summa-<br />rized in Table 1. Gibbs-SSVI and Mimno-SVI consistently<br />outperform the other methods and an explanation, outlined<br />in Section 5.3, as to why this is the case can be deduced by<br />studying the images in Figures 3 and 4.<br />For the 512 × 512 images, the average training time per<br />epoch was 0.12,0.12,0.13,0.46 and 0.48 secs for the<br />MF-SVI, MF-SSVI, Titsias-SSVI, Mimno-SVI and Gibbs-<br />SSVI methods respectively, on a 2.4GHz dual core ma-<br />chine. Among the multiple experiments, the MF-SSVI re-<br />constructions were similar in appearance to the MF-SVI<br />and Titsias-SSVI methods, whilst the Gibbs-SSVI recon-<br />structions were similar to the Mimno-SVI ones. We be-<br />lieve the blurred appearance of the former 3 methods’ re-<br />constructions is a result of the independence between znk<br />and znk? for k ?= k?in their variational forms. In contrast,<br />the Gibbs-SSVI and Mimno-SVI methods maintain depen-<br />dencebetweenznkandznk?, andarethereforeabletoselect<br />a subset offeatures which collectively bestexplain the data.<br />The latter methods are consequently much more capable of<br />0510 1520<br />2<br />3<br />4<br />5<br />·105<br />Training Time (s)<br />Predictive Loglikelihood<br />1<br />Figure 5. Predicitve loglikelihood versus training time on cell line<br />data comparing five SVI algorithms.<br />capturing structure and detail in the images we tested on,<br />and there is a mild cost to pay in extra training time.<br />5.3. A Thought Experiment<br />To illustrate the problem with breaking dependencies be-<br />tween zikand zik? for k ?= k?, we can consider a simple<br />thoughtexperiment. Supposeyi= f+?i, yi∈ RD, where<br />f ∼ N(0,I) and ?i∼ N(0,0.05I) independently for i =<br />1,...,N. Now consider applying MF-SSVI and Gibbs-SVI<br />algorithms to this dataset, whilst fixing wik = 1 for each<br />i,k, and using K = 2 features. Let’s assume that at the cur-<br />rent iteration π(t)<br />1<br />≈ π(t)<br />ELBO for the MF-SSVI method will have local optima for<br />θi1= 1 − θi2, since exactly 1 feature is needed to explain<br />the data, so MF-SSVI would find a local optimum of the<br />form q(zi1) = Bernoulli(s), q(zi2) = Bernoulli(1 − s)<br />for some s ∈ (0,1). (We were able to verify this form of<br />local optimum empircally). The Gibbs-SSVI will generate<br />samples of the form zi= (0,1) and zi= (1,0).<br />2<br />and φ(t)<br />1<br />≈ φ(t)<br />2<br />≈ f. The local<br />We would like to predict yiwith each model. Since q(zi1)<br />and q(zi2) are independent under MF-SSVI, predictions<br />will be ˆ yi= 0 with probability s(1 − s), ˆ yi≈ yiwith<br />probabilitys2+(1−s)2andfinally ˆ yi≈ 2yiwithprobabil-<br />ity s(1 − s). Conversely, the Gibbs samper in Gibbs-SSVI<br />will place little to no probability on both, or neither of the 2<br />features being used for prediction. In summary, the Gibbs<br />based local variable estimates can handle the strong corre-<br />lation between the 2 features, whilst the MF based method<br />cannot and suffers dramatically because of it.<br />One possible solution to this problem would be to encour-<br />age all features to have limited correlation apriori, discour-<br />aging situations where multiple features are learned to be</p>  <p>Page 8</p> <p>Empirical Study of SVI for the Beta Bernoulli Process<br />Table 1. PSNR performance of image interpolation (left entries) and denoising (right entries) tasks using Gibbs initialization of global<br />parameters on a randomly chosen subset of data.<br />BOAT<br />BARBARA<br />LENA<br />HOUSE<br />PEPPERS<br />MF-SVI<br />MF-SSVI<br />TITSIAS-SSVI<br />MIMNO-SVI<br />GIBBS-SSVI<br />21.1<br />22.3<br />23.2<br />32.4<br />34.3<br />19.5<br />20.8<br />21.5<br />29.7<br />31.5<br />21.8<br />22.2<br />22.1<br />36.2<br />38.2<br />20.6<br />21.4<br />21.7<br />35.1<br />37.0<br />24.1<br />24.7<br />26.3<br />39.4<br />43.3<br />23.6<br />24.4<br />25.8<br />36.9<br />41.7<br />25.3<br />26.7<br />26.7<br />42.8<br />40.5<br />24.2<br />25.4<br />25.3<br />40.1<br />37.8<br />25.9<br />25.8<br />27.9<br />43.7<br />47.4<br />24.4<br />24.1<br />26.8<br />40.4<br />42.3<br />similar to each other. Encouraging dissimilarity in such<br />a way is challenging and would complicate the otherwise<br />clean updates that are possible in SVI methods.<br />5.4. Genomic data<br />Vast amounts of genomic data are currently being collected<br />as technology advances. It will be crucial to develop ma-<br />chine learning models and more importantly, inference al-<br />gorithms, which can cope with large data sets, whilst still<br />retaining flexible modelling ability. We consider 2 datasets<br />for which sparse latent feature modelling is appropriate.<br />We use K = 500 features for both experiments.<br />Cancer cell line data.<br />dia is a collection of around 450 cancer samples including<br />gene expression, copy number variation, and drug response<br />information. We focus on modeling the gene expression<br />data, which has measurements for around 15,000 genes. In<br />this setting we are more interested in finding overlapping<br />clusters (sparse features) of genes rather than samples, so<br />we effectively have N = 15000,D = 450. The latent fac-<br />tors found can then be interpreted as biological pathways,<br />or sets of genes regulated by the same transcription factor.<br />Understanding the structure in this data is valuable as a first<br />step towards associating the cellular characteristics of the<br />cancers to their drug response profiles. We randomly hold<br />out 10 % of the data for testing. Results for this experiment<br />are summarized in Figure 5.<br />The Cancer Cell Line Encyclope-<br />CyTOF data.<br />put technology capable of measuring up to 40 protein abun-<br />dance levels in thousands of individual cells per second.<br />The cells are controlled using flow cytometry and spe-<br />cific proteins are tagged using heavy metals which can be<br />measured using time-of-flight mass spectrometry. Exist-<br />ing analyses have attempted to group the observed cells<br />into non-overlapping subpopulations, but we here show<br />that the data can be effectively modeled as compromising<br />of a spectrum of cell types expressing different latent fac-<br />tors to differing extents. The sample we analyse consists<br />of human immune cells, so representing the heterogeneity<br />is relevant for understanding disease response. Our dataset<br />has N = 532,000,D = 40 and a random 5% is used as<br />CyTOF is a novel extremely high through-<br />test data. The results for the experiments on this data fol-<br />low a very similar pattern to that of the cell line gene ex-<br />pression data. The converged predictive log-likelihoods af-<br />ter training for 10 minutes are −1.1e6, −9.6e5, −9.4e5,<br />−3.8e5 and −3.2e5 for the MF-SVI, MF-SSVI, Titsias-<br />SSVI, Mimno-SVI and Gibbs-SSVI methods respectively.<br />6. Conclusions<br />In this work, we compare various stochastic variational in-<br />ference algorithms for beta process factor analysis. Whist<br />many methods in the literature have been proposed, we<br />have chosen to exploit the conditional conjugacy and the<br />exponential family nature of our model to create simple<br />natural parameter updates.<br />Hoffman &amp; Blei (2014) found that preserving structure be-<br />tween local and global variables significantly boosted per-<br />formance for the LDA, but based on our experiments, we<br />conclude that preserving intra-local variable dependence is<br />crucial to prediction in the beta-Bernoulli process. This is<br />evident from the fact that both Gibbs-SSVI and Mimno-<br />SVI consistently and significantly outperform MF-SVI,<br />MF-SSVI and Titsias-SSVI on a variety of image interpo-<br />lation and denoising tasks and on modelling genomic data.<br />The Titsias-SSVI method models dependence between zik<br />and wik, but does not appear to significantly outperform<br />MF-SSVI, suggesting that this dependence is not crucial<br />in prediction. Mimno-SVI does not maintain dependence<br />between local and global variables whilst MF-SSVI does,<br />and yet Mimno-SVI leads to better predictions. We discuss<br />why this is the case in a simple thought experiment, show-<br />ing the benefit of maintaining dependence between local<br />variables where k ?= k?. The multi-cluster, sparse nature<br />of the beta-Bernoulli process makes mean field type local<br />variable approximations highly sensitive to correlated fea-<br />tures. Gibbs-SSVI does also modestly outperform Mimno-<br />SVI through maintaining dependencies between global and<br />local variables.<br />In summary, care is needed to ensure that the dependen-<br />cies encoded by a particular variational approximation are<br />appropriate for the model being considered.</p>  <p>Page 9</p> <p>Empirical Study of SVI for the Beta Bernoulli Process<br />References<br />Ahn, Sungjin, Korattikara, Anoop, and Welling, Max.<br />Bayesian posterior sampling via stochastic gradient<br />fisher scoring. In Proceedings of the 29th International<br />Conference on Machine Learning (ICML-12), pp. 1591–<br />1598, 2012.<br />Aldous, D. J. Exchangeability and Related Topics. In´Ecole<br />d’´Et´ e de Probabiliti´ es de Saint-Flour XIII - 1983, pp. 1–<br />198. Springer Berlin Heidelberg, 1985.<br />Amari, S. Natural Gradient Works Efficiently in Learning.<br />Neural Computation, 10(2):251–276, 1998.<br />Bendall, S. C., Simonds, E. F., Qiu, P., El-ad, D. A.,<br />Krutzik, P. O., Finck, R., Bruggner, R. V., Melamed, R.,<br />Trejo, A., Ornatsky, O. I., et al. Single-cell mass cytom-<br />etry of differential immune and drug responses across a<br />human hematopoietic continuum. Science, 332(6030):<br />687–696, 2011.<br />Blei, D. M., Ng, A. Y., and Jordan, M. I. Latent Dirichlet<br />Allocation. Journal of Machine Learning Research, 3:<br />993–1022, 2003.<br />Consonni, G. and Marin, J. M. Mean-field Variational Ap-<br />proximate Bayesian Inference for Latent Variable Mod-<br />els. Computational Statistics and Data Analysis, 52:<br />790–798, 2007.<br />Ding, N., Xiang, R., Molloy, I., Li, N., et al. Nonpara-<br />metric Bayesian matrix factorization by Power-EP. In<br />International Conference on Artificial Intelligence and<br />Statistics, pp. 169–176, 2010.<br />Doshi, F., Miller, K. T., Gael, J. Van, and Teh, Y. W. Varia-<br />tional inference for the Indian buffet process. Advances<br />in Neural Information Processing Systems, 2008.<br />Doshi-Velez, F., Knowles, D. A., Mohamed, S., and<br />Ghahramani, Z. Large Scale Nonparametric Bayesian<br />Inference: Data Parallelisation in the Indian Buffet Pro-<br />cess. In Advances in Neural Information Processing Sys-<br />tems, volume 22, pp. 2–3, 2009.<br />Gerrish, S.<br />Modeling Influence and Decision Making. PhD thesis,<br />Princeton University, 2013.<br />Applications of Latent Variable Models in<br />Ghahramani, Z. and Beal, M. J. Variational Inference for<br />Bayesian Mixtures of Factor Analyzers. In Advances in<br />Neural Information Processing Systems, 1999.<br />Griffiths, T. and Ghahramani, Z. Infinite Latent Feature<br />Models and the Indian Buffet Process. Advances in Neu-<br />ral Information Processing Systems, 2006.<br />Griffiths, T. and Ghahramani, Z. The Indian Buffet Pro-<br />cess: An introduction and review. Journal of Machine<br />Learning Research, 12:1185–1224, 2011.<br />Hjort, N. L.<br />beta processes in models for life history data. Annals<br />of Statistics, 18(3):1259–1294, 1990.<br />Nonparametric Bayes estimators based on<br />Hoffman, M. D. and Blei, D. M. Structured Stochastic<br />Variational Inference. arXiv, 2014. http://arxiv.<br />org/abs/1404.4114.<br />Hoffman, M.D., Blei, D.M., andBach, F.R. OnlineLearn-<br />ing for Latent Dirichlet Allocation. In Advances in Neu-<br />ral Information Processing Systems, volume 2, pp. 5,<br />2010.<br />Hoffman, M. D., Blei, D.M., Wang, C., and Paisley, J.<br />Stochastic Variational Inference.<br />Learning Research, 14:1303–1347, 2013.<br />Journal of Machine<br />Ji, C., Shen, H., and West, M.<br />tions for Marginal Likelihoods. Technical Report, Duke<br />University, 2010. http://ftp.stat.duke.edu/<br />WorkingPapers/10-05.pdf.<br />Bounded Approxima-<br />Kingma, D. and Welling, M. Auto-Encoding Variational<br />Bayes. Intl. Conf. on Learning Representations, 2014.<br />Knowles, D. and Ghahramani, Z.<br />Analysis and Infinite Independent Components Analy-<br />sis. 7th International Conference on Independent Com-<br />ponent Analysis and Signal Separation, 2007.<br />Infinite Sparse Factor<br />Knowles, David, Ghahramani, Zoubin, et al.<br />metric bayesian sparse factor models with application to<br />gene expression modeling. The Annals of Applied Statis-<br />tics, 5(2B):1534–1552, 2011.<br />Nonpara-<br />Li, L., Silva, J., Zhou, M., and Carin, L. Online Bayesian<br />Dictionary Learning for Large Datasets. Intl. Conf. on<br />Acoustics, Speech and Signal Processing, 2012.<br />Liang, D. and Hoffman, M. D. Beta Process Non-negative<br />Matrix Factorization with Stochastic Structured Mean-<br />Field Variational Inference. arXiv, 2014. http://<br />arxiv.org/abs/1411.1804.<br />MacEachern, Steven N and M¨ uller, Peter. Estimating mix-<br />ture of dirichlet process models. Journal of Computa-<br />tional and Graphical Statistics, 7(2):223–238, 1998.<br />Mimno, D., Hoffman, M., and Blei, D. Sparse Stochastic<br />InferenceforLatentDirichletAllocation. Proceedingsof<br />the 29th International Conference on Machine Learning,<br />2012.</p>  <p>Page 10</p> <p>Empirical Study of SVI for the Beta Bernoulli Process<br />Nott, D., Tan, S., Villani, M., and Kohn, R.<br />sion Density Estimation with Variational Methods and<br />Stochastic Approximation. Journal of Computational<br />and Graphical Statistics, 21(3):797–820, 2012.<br />Regres-<br />Orbanz, Peter and Teh, Yee Whye. Bayesian nonparametric<br />models. In Encyclopedia of Machine Learning, pp. 81–<br />89. Springer, 2010.<br />Paisley, J. and Carin, L. Nonparametric Factor Analysis<br />with Beta Process Priors. Proceedings of the 26th Inter-<br />national Conference on Machine Learning, 2009.<br />Paisley, J., Blei, D., and Jordan, M. Variational Bayesian<br />Inference with Stochastic Search. Proceedings of the<br />29th International Conference on Machine Learning,<br />2012.<br />Polatkan, G., Zhou, M., Carin, L., Blei, D., and<br />Daubechies, I. A Bayesian Nonparametric Approach to<br />Image Super-resolution. IEEE Trans. on Pattern Analy-<br />sis and Machine Intelligence, 2014.<br />Portilla, J., Strela, V., Wainwright, M. J., and Simoncelli,<br />E. P. Image Denoising using Scale Mixtures of Gaus-<br />sians in the Wavelet Domain. IEEE Trans on Image Pro-<br />cessing, 2003.<br />Rai, P. and Daum´ e, H. The Infinite Hierarchical Factor Re-<br />gression Model . Advances in Neural Information Pro-<br />cessing Systems, 2008.<br />Ranganath, R., Gerrish, S., and Blei, D. Black Box Varia-<br />tional Inference. Proceedings of the 17th Conference on<br />Artificial Intelligence and Statistics, 2014.<br />Rasmussen, Carl and Williams, Chris. Gaussian processes<br />for machine learning. Gaussian Processes for Machine<br />Learning, 2006.<br />Reed, C. and Ghahramani, Z. Scaling the Indian Buffet<br />Process via Submodular Maximization. Proceedings of<br />the 30th International Conference on Machine Learning,<br />2013.<br />Salimans, T. and Knowles, D. Fixed-Form Variational Pos-<br />teriorApproximationThroughStochasticLinearRegres-<br />sion. Bayesian Analysis, 8:837–882, 2013.<br />Teh, Y. W., Jordan, M. I., Beal, M. J., and Blei, D. M. Shar-<br />ing clusters among related groups: Hierarchical dirichlet<br />processes. In Advances in Neural Information Process-<br />ing Systems, 2004.<br />Teh, Y. W., G¨ or¨ ur, D., and Ghahramani, Z. Stick break-<br />ing construction for the Indian buffet process. Proceed-<br />ings of the 11th Conference on Artificial Intelligence and<br />Statistics, 2007.<br />Thibaux, R. and Jordan, M.I.<br />cesses and the Indian Buffet Process. Proceedings of the<br />11th Conference on Artificial Intelligence and Statistics,<br />2007.<br />Hierarchical Beta Pro-<br />Titsias, M. K. and L´ azaro-Gredilla, M. Spike and Slab<br />Variational Inference for Multi-Task and Multiple Ker-<br />nel Learning. Advances in Neural Information Process-<br />ing Systems, 2011.<br />Titsias, M. K. and L´ azaro-Gredilla, M. Doubly Stochas-<br />tic Variational Bayes for Non-Conjugate Inference. Pro-<br />ceedings of the 31st International Conference on Ma-<br />chine Learning, 2014.<br />Wang, C. and Blei, D. Truncation-free stochastic varia-<br />tional inference for bayesian nonparametric models. Ad-<br />vances in Neural Information Processing Systems, 25:<br />422–430, 2012.<br />Welling, Max and Teh, Yee W.<br />stochastic gradient langevin dynamics. In Proceedings<br />of the 28th International Conference on Machine Learn-<br />ing (ICML-11), pp. 681–688, 2011.<br />Bayesian learning via<br />West, M. Bayesian Factor Regression Models in the “large<br />p, small n” Paradigm . Bayesian Statistics, 7:723–732,<br />2003.<br />Zhou, M., Chen, H., Paisley, J., Ren, L., Sapiro, G., and<br />Carin, L. Non-Parametric Bayesian Dictionary Learning<br />for Sparse Image Representations. Advances in Neural<br />Information Processing Systems, 2009.</p>  <p>Page 11</p> <p>Empirical Study of SVI for the Beta Bernoulli Process<br />Appendix<br />Here, we provide details about the local variable approximations introduced in the main text of the paper.<br />Mimno-SVI<br />The form of the local approximation in the Mimno-SVI method is<br />logqMimno(ψi) = Eq(β)[logp(ψi|y1:N,β)]<br />= Eq(β)<br />?<br />?<br />?<br />−γobs<br />2<br />??yi− (zi◦ wi)Φ??2+<br />zikwik<br />wik<br />?<br />k<br />ziklog<br />?<br />πk<br />1 − πk<br />µkµ?<br />τkτj<br />?<br />−γw<br />?<br />2wiw?<br />i<br />?<br />+ const<br />= −c<br />2d<br />k<br />??µkµ?<br />k<br />τk2<br />+1<br />τk<br />?<br />+<br />??<br />i+ const<br />j?=k<br />zijwij<br />j<br />− 2µky?<br />i<br />τk<br />?<br />+<br />k<br />zik(ψ(ak) − ψ(bk)) −<br />e<br />2fwiw?<br />It is clear that logqMimnois quadratic in each wikand linear in each zik, therefore a Gibbs based sampler can easily be<br />consructed to sample from qMimno, where wikis Gaussian given all other local variables, and zikis Bernoulli given all<br />other local variables.<br />MF-SSVI<br />The local ELBO in the MF-SSVI framework is very similar to that of the MF-SVI, the difference being that samples of the<br />global variables are used in MF-SSVI. The local ELBO has the following form<br />LMF−SSVI<br />local<br />=γobs<br />2<br />?<br />−γw<br />i,k<br />θikφk<br />?<br />2νik<br />κiky?<br />?νik2<br />i−<br />?νik2<br />?<br />?θiklogθik+ (1 − θik)log(1 − θik)?.<br />κik2+<br />1<br />κik<br />?<br />φ?<br />k−<br />?<br />?<br />j?=k<br />θijνij<br />κij<br />νik<br />κikφ?<br />j<br />?<br />2<br />?<br />?<br />i,k<br />κik2+<br />1<br />κik<br />?<br />+<br />?<br />i,k<br />θik<br />?<br />πk<br />1 − πk<br />−1<br />2<br />i,k<br />log(κik) −<br />i,k<br />This is optimized as a function of {θik,νik,κikusing gradient descent. Once a local optimum is found, EqMF(ψ1:N|β(t))[ηi]<br />can be computed analytically as a function of the optimized parameters and global variable samples.<br />Titsias-SSVI<br />Recall that the Titsias-SSVI method maintains dependence between zikand wikfor each k. The local ELBO for Titsias-<br />SSVI is<br />LTitsias−SSVI<br />local<br />=γobs<br />2<br />?<br />−γw<br />i,k<br />θikφk<br />?<br />2νik<br />κiky?<br />?νik2<br />?log(κik) − 1?+ (1 − θik)?log(γw) − 1??<br />?θiklogθik+ (1 − θik)log(1 − θik)?.<br />i−<br />?νik2<br />?<br />κik2+<br />1<br />κik<br />?<br />φ?<br />k−<br />?<br />?<br />j?=k<br />θijνij<br />κij<br />νik<br />κikφ?<br />j<br />?<br />2<br />?<br />?<br />?<br />i,k<br />κik2+<br />1<br />κik<br />+<br />?<br />i,k<br />θik<br />?<br />πk<br />1 − πk<br />−1<br />2<br />i,k<br />?<br />θik<br />−<br />i,k</p>  <p>Page 12</p> <p>Empirical Study of SVI for the Beta Bernoulli Process<br />Again, this function is maxized as a function of {θik,νik,κikusing gradient descent, and the optimized parameters along<br />with the global variable samples are used to compute EqTitsias(ψ1:N|β(t))[ηi] analytically.<br />Gibbs-SSVI<br />The Gibbs-SVI method uses the true posterior conditional distribution for local variables<br />logqGibbs(ψi) = logp(ψi|y1:N,β)<br />= −γobs<br />2<br />??yi− (zi◦ wi)Φ??2+<br />?<br />?<br />?<br />??<br />−γw<br />k<br />ziklog<br />?<br />πk<br />1 − πk<br />?<br />−γw<br />2wiw?<br />?<br />i+ const<br />= −γobs<br />2<br />k<br />zikwikφk<br />?<br />πk<br />wikφ?<br />k+<br />j?=k<br />zijwijφ?<br />j<br />?<br />− 2y?<br />i<br />+<br />k<br />ziklog<br />?<br />1 − πk<br />?<br />2wiw?<br />i+ const<br />Just as was the case with Mimno-SVI, we notice that logqGibbsis quadratic in each wikand linear in each zik, therefore a<br />Gibbs sampler can be designed to sample from qGibbs.</p>  <a href="https://www.researchgate.net/profile/David_Knowles2/publication/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process/links/55a53c7b08aef604aa042e0b.pdf">Download full-text</a> </div> <div id="rgw21_56ab19254c6ed" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw22_56ab19254c6ed">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw23_56ab19254c6ed"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/David_Knowles2/publication/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process/links/55a53c7b08aef604aa042e0b.pdf" class="publication-viewer" title="55a53c7b08aef604aa042e0b.pdf">55a53c7b08aef604aa042e0b.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/David_Knowles2">David A. Knowles</a> &middot; Jul 14, 2015 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw24_56ab19254c6ed"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://de.arxiv.org/pdf/1506.08180" target="_blank" rel="nofollow" class="publication-viewer" title="An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process">An Empirical Study of Stochastic Variational Algor...</a> </div>  <div class="details">   Available from <a href="http://de.arxiv.org/pdf/1506.08180" target="_blank" rel="nofollow">de.arxiv.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw26_56ab19254c6ed" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw27_56ab19254c6ed">  </ul> </div> </div>   <div id="rgw17_56ab19254c6ed" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw18_56ab19254c6ed"> <div> <h5> <a href="publication/284122815_Probabilistic_approach_to_assessing_and_monitoring_settlements_caused_by_tunneling" class="color-inherit ga-similar-publication-title"><span class="publication-title">Probabilistic approach to assessing and monitoring settlements caused by tunneling</span></a>  </h5>  <div class="authors"> <a href="researcher/2007870883_Carles_Camos" class="authors ga-similar-publication-author">Carles Camós</a>, <a href="researcher/2048026281_Olga_Spackova" class="authors ga-similar-publication-author">Olga Špačková</a>, <a href="researcher/71768049_Daniel_Straub" class="authors ga-similar-publication-author">Daniel Straub</a>, <a href="researcher/21819000_Climent_Molins" class="authors ga-similar-publication-author">Climent Molins</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56ab19254c6ed"> <div> <h5> <a href="publication/289694528_Probabilistic_model_of_software_approximate_correctness" class="color-inherit ga-similar-publication-title"><span class="publication-title">Probabilistic model of software approximate correctness</span></a>  </h5>  <div class="authors"> <a href="researcher/2093003360_Yanfang_Ma" class="authors ga-similar-publication-author">Yanfang Ma</a>, <a href="researcher/2093085218_Liang_Chen" class="authors ga-similar-publication-author">Liang Chen</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw20_56ab19254c6ed"> <div> <h5> <a href="publication/291390577_A_minimal_probabilistic_model_for_soil_moisture_in_seasonally_dry_climates" class="color-inherit ga-similar-publication-title"><span class="publication-title">A minimal probabilistic model for soil moisture in seasonally dry climates</span></a>  </h5>  <div class="authors"> <a href="researcher/2052326573_David_N_Dralle" class="authors ga-similar-publication-author">David N. Dralle</a>, <a href="researcher/16174332_Sally_E_Thompson" class="authors ga-similar-publication-author">Sally E. Thompson</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw40_56ab19254c6ed" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw41_56ab19254c6ed">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw42_56ab19254c6ed" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=9oZW7LJdQpt-M61xBMK5PjFmczgImmk3WgeIvABueZu6zQqegTr66TVcQuh4ZbaQ" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="KI7IdNc1WR/UaKTj6r0ACdL3qhfdDuwNL+q2xZordLKaNMnj+dJibE5saMBLRdVTiDJJJuz2l2sbgnc4I9psEZd9vJixwrYfkf/Kb6fr/X4MLJGjYmd6yXJCgg6fh/qeJhEj6mBUNeeV2ThkhtJtUe6trRQFsx13X4ag8EE7ZJb05mfDcb2n/H9Y+cONG8pgc7P3oX/d4L3sq+0zufoYS7DzBlfEDBC2OdedLTkuGFf0EzqGm/xROeEwsUI6c2Qyw6ZhGS7kLW8GRWKOPhoSF1g2CsA0aqJInuvAmKrR34Y="/> <input type="hidden" name="urlAfterLogin" value="publication/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process?ev=auth_pub"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjc5MzA5OTE3X0FuX0VtcGlyaWNhbF9TdHVkeV9vZl9TdG9jaGFzdGljX1ZhcmlhdGlvbmFsX0FsZ29yaXRobXNfZm9yX3RoZV9CZXRhX0Jlcm5vdWxsaV9Qcm9jZXNzP2V2PWF1dGhfcHVi"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjc5MzA5OTE3X0FuX0VtcGlyaWNhbF9TdHVkeV9vZl9TdG9jaGFzdGljX1ZhcmlhdGlvbmFsX0FsZ29yaXRobXNfZm9yX3RoZV9CZXRhX0Jlcm5vdWxsaV9Qcm9jZXNzP2V2PWF1dGhfcHVi"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjc5MzA5OTE3X0FuX0VtcGlyaWNhbF9TdHVkeV9vZl9TdG9jaGFzdGljX1ZhcmlhdGlvbmFsX0FsZ29yaXRobXNfZm9yX3RoZV9CZXRhX0Jlcm5vdWxsaV9Qcm9jZXNzP2V2PWF1dGhfcHVi"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw43_56ab19254c6ed"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 440;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FigureList","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"David A. Knowles","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/David_Knowles2","institution":"Stanford University","institutionUrl":false,"widgetId":"rgw4_56ab19254c6ed"},"id":"rgw4_56ab19254c6ed","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=7693199","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab19254c6ed"},"id":"rgw3_56ab19254c6ed","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=279309917","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":279309917,"title":"An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"06\/2015;","publicationDateRobot":"2015-06","article":""}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1506.08180","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process"},{"key":"rft.date","value":"2015"},{"key":"rft.au","value":"Amar Shah,David A. Knowles,Zoubin Ghahramani"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56ab19254c6ed"},"id":"rgw6_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=279309917","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":279309917,"peopleItems":[{"data":{"authorUrl":"researcher\/2076919269_Amar_Shah","authorNameOnPublication":"Amar Shah","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Amar Shah","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2076919269_Amar_Shah","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw9_56ab19254c6ed"},"id":"rgw9_56ab19254c6ed","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2076919269&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab19254c6ed"},"id":"rgw8_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2076919269&authorNameOnPublication=Amar%20Shah","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"David A. Knowles","accountUrl":"profile\/David_Knowles2","accountKey":"David_Knowles2","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"David A. Knowles","profile":{"professionalInstitution":{"professionalInstitutionName":"Stanford University","professionalInstitutionUrl":"institution\/Stanford_University"}},"professionalInstitutionName":"Stanford University","professionalInstitutionUrl":"institution\/Stanford_University","url":"profile\/David_Knowles2","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"David_Knowles2","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw11_56ab19254c6ed"},"id":"rgw11_56ab19254c6ed","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=7693199&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Stanford University","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":1,"publicationUid":279309917,"widgetId":"rgw10_56ab19254c6ed"},"id":"rgw10_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=7693199&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=1&publicationUid=279309917","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/8159937_Zoubin_Ghahramani","authorNameOnPublication":"Zoubin Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Zoubin Ghahramani","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/8159937_Zoubin_Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw13_56ab19254c6ed"},"id":"rgw13_56ab19254c6ed","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=8159937&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw12_56ab19254c6ed"},"id":"rgw12_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=8159937&authorNameOnPublication=Zoubin%20Ghahramani","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab19254c6ed"},"id":"rgw7_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=279309917&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":279309917,"abstract":"<noscript><\/noscript><div>Stochastic variational inference (SVI) is emerging as the most promising<br \/>\ncandidate for scaling inference in Bayesian probabilistic models to large<br \/>\ndatasets. However, the performance of these methods has been assessed primarily<br \/>\nin the context of Bayesian topic models, particularly latent Dirichlet<br \/>\nallocation (LDA). Deriving several new algorithms, and using synthetic, image<br \/>\nand genomic datasets, we investigate whether the understanding gleaned from LDA<br \/>\napplies in the setting of sparse latent factor models, specifically beta<br \/>\nprocess factor analysis (BPFA). We demonstrate that the big picture is<br \/>\nconsistent: using Gibbs sampling within SVI to maintain certain posterior<br \/>\ndependencies is extremely effective. However, we find that different posterior<br \/>\ndependencies are important in BPFA relative to LDA. Particularly,<br \/>\napproximations able to model intra-local variable dependence perform best.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw14_56ab19254c6ed"},"id":"rgw14_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=279309917","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":{"data":{"figures":[{"imageUrl":"https:\/\/www.researchgate.net\/profile\/David_Knowles2\/publication\/279309917\/figure\/fig1\/Figure-4-Results-from-interpolation-and-denoising-of-the-512-512-pixel-'Barbara'.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/David_Knowles2\/publication\/279309917\/figure\/fig1\/Figure-4-Results-from-interpolation-and-denoising-of-the-512-512-pixel-'Barbara'_small.png","figureUrl":"\/figure\/279309917_fig1_Figure-4-Results-from-interpolation-and-denoising-of-the-512-512-pixel-'Barbara'","selected":false,"title":"Figure 4. Results from interpolation and denoising of the 512 \u00d7 512...","key":"279309917_fig1_Figure-4-Results-from-interpolation-and-denoising-of-the-512-512-pixel-'Barbara'"}],"readerDocId":"4575162","linkBehaviour":"dialog","isDialog":true,"headerText":"Figures in this publication","isNewPublicationDesign":false,"widgetId":"rgw15_56ab19254c6ed"},"id":"rgw15_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/FigureList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FigureList.html?readerDocId=4575162&isDialog=1&linkBehaviour=dialog","viewClass":"views.publicliterature.FigureListView","yuiModules":["rg.views.publicliterature.FigureListView","css-pow-publicliterature-FigureList"],"stylesheets":["pow\/publicliterature\/FigureList.css"],"_isYUI":true},"previewImage":"https:\/\/i1.rgstatic.net\/publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process\/links\/55a53c7b08aef604aa042e0b\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw16_56ab19254c6ed"},"id":"rgw16_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab19254c6ed"},"id":"rgw5_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=279309917&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2007870883,"url":"researcher\/2007870883_Carles_Camos","fullname":"Carles Cam\u00f3s","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2048026281,"url":"researcher\/2048026281_Olga_Spackova","fullname":"Olga \u0160pa\u010dkov\u00e1","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":71768049,"url":"researcher\/71768049_Daniel_Straub","fullname":"Daniel Straub","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":21819000,"url":"researcher\/21819000_Climent_Molins","fullname":"Climent Molins","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Tunnelling and Underground Space Technology","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/284122815_Probabilistic_approach_to_assessing_and_monitoring_settlements_caused_by_tunneling","usePlainButton":true,"publicationUid":284122815,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.49","url":"publication\/284122815_Probabilistic_approach_to_assessing_and_monitoring_settlements_caused_by_tunneling","title":"Probabilistic approach to assessing and monitoring settlements caused by tunneling","displayTitleAsLink":true,"authors":[{"id":2007870883,"url":"researcher\/2007870883_Carles_Camos","fullname":"Carles Cam\u00f3s","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2048026281,"url":"researcher\/2048026281_Olga_Spackova","fullname":"Olga \u0160pa\u010dkov\u00e1","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":71768049,"url":"researcher\/71768049_Daniel_Straub","fullname":"Daniel Straub","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":21819000,"url":"researcher\/21819000_Climent_Molins","fullname":"Climent Molins","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Tunnelling and Underground Space Technology 01\/2016; 51:313-325. DOI:10.1016\/j.tust.2015.10.041"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/284122815_Probabilistic_approach_to_assessing_and_monitoring_settlements_caused_by_tunneling","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/284122815_Probabilistic_approach_to_assessing_and_monitoring_settlements_caused_by_tunneling\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab19254c6ed"},"id":"rgw18_56ab19254c6ed","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=284122815","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2093003360,"url":"researcher\/2093003360_Yanfang_Ma","fullname":"Yanfang Ma","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2093085218,"url":"researcher\/2093085218_Liang_Chen","fullname":"Liang Chen","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2016","journal":"Wuhan University Journal of Natural Sciences","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/289694528_Probabilistic_model_of_software_approximate_correctness","usePlainButton":true,"publicationUid":289694528,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/289694528_Probabilistic_model_of_software_approximate_correctness","title":"Probabilistic model of software approximate correctness","displayTitleAsLink":true,"authors":[{"id":2093003360,"url":"researcher\/2093003360_Yanfang_Ma","fullname":"Yanfang Ma","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2093085218,"url":"researcher\/2093085218_Liang_Chen","fullname":"Liang Chen","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Wuhan University Journal of Natural Sciences 02\/2016; 21(1):47-55. DOI:10.1007\/s11859-016-1137-x"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/289694528_Probabilistic_model_of_software_approximate_correctness","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/289694528_Probabilistic_model_of_software_approximate_correctness\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab19254c6ed"},"id":"rgw19_56ab19254c6ed","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=289694528","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2052326573,"url":"researcher\/2052326573_David_N_Dralle","fullname":"David N. Dralle","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":16174332,"url":"researcher\/16174332_Sally_E_Thompson","fullname":"Sally E. Thompson","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291390577_A_minimal_probabilistic_model_for_soil_moisture_in_seasonally_dry_climates","usePlainButton":true,"publicationUid":291390577,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/291390577_A_minimal_probabilistic_model_for_soil_moisture_in_seasonally_dry_climates","title":"A minimal probabilistic model for soil moisture in seasonally dry climates","displayTitleAsLink":true,"authors":[{"id":2052326573,"url":"researcher\/2052326573_David_N_Dralle","fullname":"David N. Dralle","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":16174332,"url":"researcher\/16174332_Sally_E_Thompson","fullname":"Sally E. Thompson","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291390577_A_minimal_probabilistic_model_for_soil_moisture_in_seasonally_dry_climates","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291390577_A_minimal_probabilistic_model_for_soil_moisture_in_seasonally_dry_climates\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56ab19254c6ed"},"id":"rgw20_56ab19254c6ed","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291390577","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw17_56ab19254c6ed"},"id":"rgw17_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=279309917&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":279309917,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":279309917,"publicationType":"article","linkId":"55a53c7b08aef604aa042e0b","fileName":"55a53c7b08aef604aa042e0b.pdf","fileUrl":"profile\/David_Knowles2\/publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process\/links\/55a53c7b08aef604aa042e0b.pdf","name":"David A. Knowles","nameUrl":"profile\/David_Knowles2","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Jul 14, 2015","fileSize":"1.48 MB","widgetId":"rgw23_56ab19254c6ed"},"id":"rgw23_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=279309917&linkId=55a53c7b08aef604aa042e0b&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":279309917,"publicationType":"article","linkId":"5595e44808ae21086d207ec4","fileName":"An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process","fileUrl":"http:\/\/de.arxiv.org\/pdf\/1506.08180","name":"de.arxiv.org","nameUrl":"http:\/\/de.arxiv.org\/pdf\/1506.08180","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw24_56ab19254c6ed"},"id":"rgw24_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=279309917&linkId=5595e44808ae21086d207ec4&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw22_56ab19254c6ed"},"id":"rgw22_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=279309917&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":17,"valueFormatted":"17","widgetId":"rgw25_56ab19254c6ed"},"id":"rgw25_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=279309917","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw21_56ab19254c6ed"},"id":"rgw21_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=279309917&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":279309917,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw27_56ab19254c6ed"},"id":"rgw27_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=279309917&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":17,"valueFormatted":"17","widgetId":"rgw28_56ab19254c6ed"},"id":"rgw28_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=279309917","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw26_56ab19254c6ed"},"id":"rgw26_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=279309917&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"An Empirical Study of Stochastic Variational\nAlgorithms for the Beta Bernoulli Process\nAmar Shah?\nDavid A. Knowles\u2020\nZoubin Ghahramani?\n?University of Cambridge, Department of Engineering, Cambridge, UK\n\u2020Stanford University, Department of Computer Science, Stanford, CA, USA\nAS793@CAM.AC.UK\nDAVIDKNOWLES@CS.STANFORD.EDU\nZOUBIN@ENG.CAM.AC.UK\nAbstract\nStochastic variational inference (SVI) is emerg-\ning as the most promising candidate for scal-\ning inference in Bayesian probabilistic models\nto large datasets. However, the performance of\nthese methods has been assessed primarily in the\ncontext of Bayesian topic models, particularly\nlatent Dirichlet allocation (LDA). Deriving sev-\neral new algorithms, and using synthetic, image\nand genomic datasets, we investigate whether the\nunderstanding gleaned from LDA applies in the\nsetting of sparse latent factor models, specifi-\ncally beta process factor analysis (BPFA). We\ndemonstrate that the big picture is consistent: us-\ning Gibbs sampling within SVI to maintain cer-\ntain posterior dependencies is extremely effec-\ntive. However, we find that different posterior\ndependencies are important in BPFA relative to\nLDA. Particularly, approximations able to model\nintra-local variable dependence perform best.\n1. Introduction\nThe last two decades have seen an explosion in the de-\nvelopment of flexible statistical methods able to model di-\nverse data sources. Bayesian nonparametric priors in par-\nticular provide a powerful framework to enable models to\nadapt their complexity to the data at hand (Orbanz & Teh,\n2010). In the regression setting this might mean learn-\ning the smoothness of the output function (Rasmussen &\nWilliams, 2006), for clustering adapting the number of\ncomponents (MacEachern & M\u00a8 uller, 1998), and in the case\nof our interest, latent factor models, finding an appropri-\nate number of latent features (Knowles et al., 2011). While\nsuch models are appealing for a range of applied data anal-\nProceedings of the 32ndInternational Conference on Machine\nLearning, Lille, France, 2015. JMLR: W&CP volume 37. Copy-\nright 2015 by the author(s).\nysis applications, their scalability is often limited. The\nposterior distribution over parameters and latent variables\nis typically analytically intractable and highly multimodal,\nmaking MCMC, particularly Gibbs sampling, the norm.\nAlong with concerns over performance and convergence,\nMCMC methods are often impractical for the applied prac-\ntitioner: how should the multiple samples be summarized?\nVariational methods work on the basis that simply finding\na good posterior mode, and giving some measure of the\nassociated uncertainty, is typically sufficient. In addition,\nthe predictive performance of variational methods is often\ncomparable to more computationally expensive sampling\nbased approaches (Ghahramani & Beal, 1999).\nRecently stochastic variational inference has begun to\nemerge as the most promising avenue for scaling inference\nin large latent variable models (Hoffman et al., 2013). Mar-\nrying variational inference with stochastic gradient descent\nallows principled updates using only minibatches of obser-\nvations, greatly improving data scalability. While some\nMCMC methods have been proposed to work with mini-\nbatches (Welling & Teh, 2011; Ahn et al., 2012) they lack\ntheoretical guarantees and apply only to continuous, un-\nbounded latent variables. While SVI has been influential\nfor Bayesian topic modeling, particularly latent Dirichlet\nallocation (LDA, Mimno et al., 2012; Hoffman & Blei,\n2014; Wang & Blei, 2012), the same cannot be said for\nsparse factor analysis models for continuous data. While\nthe former has been driven by the ready availability of huge\ntext corpora, the scale of continuous data being generated\nbynewgenomictechnologiesisstillgrowing. Forexample,\nCyTOF, single cell time of flight mass spectrometry is able\ntomeasuretheabundanceofdozensofproteinsinhundreds\nof thousands of cells in a single run (Bendall et al., 2011).\nSuch complex, large scale, high dimensional datasets re-\nquire sophisticated statistical models, but are typically ana-\nlyzed using simple heuristic clustering methods or PCA,\nwhich do not capture important structure, such as spar-\nsity. As a result, scaling more advanced factor analysis type\nmodels is of great interest.\narXiv:1506.08180v1  [stat.ML]  26 Jun 2015"},{"page":2,"text":"Empirical Study of SVI for the Beta Bernoulli Process\nWhen desigining a variational approximation to a poste-\nrior distribution, one must trade off the accuracy of the\napproximation with the complexity of optimizing the ev-\nidence lower bound. Mean field approximations are sim-\nple to work with, but Hoffman & Blei (2014) demonstrated\nthat in the context of LDA, maintaining posterior depen-\ndence between \u201cglobal\u201d variables (topic vectors) and \u201clo-\ncal\u201d variables (document vectors) is crucial to finding good\nsolutions. Does this finding hold for sparse factor analy-\nsis models? Our results suggest that contrary to the LDA\ncase, maintaining dependencies amongst local variables is\nactually the most important ingredient for obtaining good\nperformance with beta Bernoulli process SVI.\n2. Beta Process for Factor Analysis\nThe beta process (Hjort, 1990; Thibaux & Jordan, 2007) is\nan independent increments process defined as follows:\nDefinition 1. Let \u03a9 be a measurable space and B its \u03c3-\nalgebra. Let H0be a continuous probability measure on\n(\u03a9,B) and \u03b1 a positive scalar. Then for all disjoint, in-\nfinitesimal partitions, {B1,...,BK}, of \u03a9 the beta process\nis generated as follows,\nH(Bk)\niid\n\u223c Beta(\u03b1H0(Bk),\u03b1(1 \u2212 H0(Bk)))\nwith K \u2192 \u221e and H0(Bk) \u2192 0 for k = 1,...,K. We\ndenote the process H \u223c BP(\u03b1H0).\nHjort considers a generalization of this definition including\nfunctions, \u03b1(Bk), which we set as constants for the sake\nof simplicity. Analogous to the Dirichlet process, the beta\nprocess may be written in set function form as\n(1)\nH(\u03c9) =\n\u221e\n?\nk=1\n\u03c0k\u03b4\u03c9k(\u03c9)\n(2)\nwith H(\u03c9i) = \u03c0i. Note that the beta process is not a nor-\nmalized random measure. Hence the \u03c0 of a beta process\ndoes not represent a probability mass function on \u03a9, but\ninstead can be used to parametrize the Bernoulli process, a\nnew measure on \u03a9 defined as follows:\nDefinition 2. Let zi be an infinite row vector with kth\nvalue, zik, generated by zik|\u03c0k\nmeasure defined by Xi(\u03c9) =?\nIf we were to stack samples of the infinite-dimensional vec-\ntor, zi, to form a matrix, Z = [z1?,...,zN?]?, we may\nview the beta-Bernoulli process as a prior over infinite bi-\nnary matrices (Griffiths & Ghahramani, 2011), where each\ncolumn in the matrix Z corresponds to a location, \u03b4\u03c9.\niid\n\u223c Bernoulli(\u03c0k). The\nkzik\u03b4\u03c9k(\u03c9) is then a draw\nfrom a Bernoulli process, which we denote Xi\u223c BeP(H).\nSampling H directly, as defined in (2), is difficult to do ex-\nactly and efficiently. But, just as Aldous (1985) derived the\nChinese restaurant process, a marginalized approach used\nfor sampling from the Dirichlet process, there exists an\nefficient marginalized scheme for sampling from the beta\nprocess, called the Indian buffet process (IBP, Griffiths &\nGhahramani, 2006; Thibaux & Jordan, 2007).\nThe IBP sampling procedure introduces strong dependen-\ncies between the rows of Z. Our goal is to derive a stochas-\ntic variational inference scheme where we consider rows in\nbatches. It will hence be crucial to instantiate the global\nparameters rather than marginalize over them.\nFor this reason, we shall consider a finite approximation\nto the beta process which simply set K to a large, finite\nnumber. The finite representation is written as\nH(\u03c9) =\nK\n?\nk=1\n\u03c0k\u03b4\u03c9k(\u03c9)\n\u03c0k\u223c Beta(a\/K,b(K \u2212 1)\/K),\nand the K-dimensional vector, zi, is drawn from a finite\nBernoulli process parameterized by H.\n\u03c9k\u223c H0\n(3)\nConsider modelling a data matrix Y \u2208 RN\u00d7Dwhere rows\nrepresent data points. Factor analysis models this data as\nthe product of two matrices L \u2208 RN\u00d7Kand \u03a6 \u2208 RK\u00d7D,\nplus an error matrix, E.\nY = L\u03a6 + E\n(4)\nPrior belief about the structure of the data may be used\nto induce the desired propeties of L and \u03a6, e.g. sparsity\n(West, 2003; Rai & Daum\u00b4 e, 2008; Knowles & Ghahra-\nmani, 2007). To encourage sparsity, we model L as the\nHadamard (element-wise) product between matrices Z and\nW, L = Z \u25e6 W, where Z is binary and W is a Gaussian\nweight matrix. This idea is described in Section 3 of (Grif-\nfiths & Ghahramani, 2011). We model the matrices \u03a6 and\nZ as N draws from a beta-Bernoulli process parameterized\nby a beta process, H.\nUsing the truncated beta process of (3), we have the fol-\nlowing generative process for observation i = 1,...,N and\nfeatures k = 1,...,K,\nyi= (zi\u25e6 wi)\u03a6 + ?i\nwi\u223c N(0,\u03b3\u22121\nzik|\u03c0k\u223c Bernoulli(\u03c0k)\n?i\u223c N(0,\u03b3\u22121\n\u03c0k\u223c Beta(a\/K,b(K \u2212 1)\/K)\n\u03c6k\u223c N(0,D\u22121I)\nwhere all values are drawn independently. This is the gen-\nerative model used for beta process factor analysis (Pais-\nley & Carin, 2009). We place independent Gamma(c?,d?)\n(5)\nwI)\nobsI)\n\uf8fc\n\uf8f4\n\uf8f4\n\uf8fe\n\uf8fd\n?\nLocal variables\nGlobal variables"},{"page":3,"text":"Empirical Study of SVI for the Beta Bernoulli Process\nand Gamma(e?,f?) priors on \u03b3obsand \u03b3wrespectively. The\nseparation of local and global variables will be crucial for\nthe stochastic variational inference algorithm which we de-\nrive in the next section. For the sake of brevity, we denote\nthe set of global variables \u03b2 \u2261 {\u03c0,\u03a6,\u03b3w,\u03b3obs} and sets\nof local variables \u03c8i\u2261 {wi,zi} for i = 1,...,N.\n3. Variational Inference Schemes\nThe true posterior distribution p(\u03b2,\u03c81:N|x1:N) involves\ncomplicated dependencies between latent variables, which\nmakes inference complicated. The goal of variational in-\nference is to approximate the true posterior with a family\nof distributions q(\u03b2,\u03c81:N).\nber of the chosen family of distributions by minimizing\nthe KL-divergence between this variational distribution and\nthe true posterior. Equivalently, we maximize the evidence\nlower bound (ELBO),\nWe choose the best mem-\nL(q) = Eq[logp(\u03b2,\u03c81:N,x1:N) \u2212 logq(\u03b2,\u03c81:N)]. (6)\nIn this work, we compare the performance of a range of\nvariational approximations. Each of the approximations we\nconsider factorizes as follows\n?\nwhere q(\u03b3obs) = Gamma(c,d), q(\u03b3w) = Gamma(e,f),\nq(\u03c6k) = N(\u03c4\u22121\nThe global variational distributions are all of the same ex-\nponential family forms as their posterior conditional dis-\ntributions. The full set of global variational parameters is\n\u03bb = {ak,bk,c,d,e,f,\u03c4k,\u00b5k}. Due to conjugacy of our\nmodel, it is easy to show that the updates for the global\nvariational parameters during the variational M-step are as\nfollows\n?\nbk= b(K \u2212 1)\/K +\n?\nd = d?+\n2Eq\n?\nf = f?+\n2Eq\n?\n\u00b5k=\ni\nq(\u03b2,\u03c81:N) =q(\u03b3obs)q(\u03b3w)\n?\nk\nq(\u03c0k)q(\u03c6k)\n?\nq(\u03c81:N|\u03b2),\nk\u00b5k,\u03c4\u22121\nkI) and q(\u03c0k) = Beta(ak,bk).\nak= a\/K +\ni\nEq[zik]\n(7)\n?\ni\n?\n1 \u2212 Eq[zik]\n?\nc = c?+\ni\nD\/2\n?\ni\n1\n???yi\u2212 (zi\u25e6 wi)\u03a6?2?\ne = e?+\ni\nK\/2\n?\ni\n1\n?wiwi??\n?\u03b3obszikwik2?\n?zikwiky\u2212k\n\u03c4k= D +\ni\nEq\n?\nEq\ni\n?\nwhere Eqis an expectation over all latent variables with\nrespect to q (except when global parameters are being sam-\npled), and y\u2212k\ni\n= yi\u2212 Eq\nwith the natural parameters of the global variational dis-\ntributions. Natural gradients give the direction of steepest\nascent in Riemannian space, leading to faster convergence\nfor e.g. maximum likelihood estimation (Amari, 1998).\n??\nj?=kzijwij\u03c6j\n?. We work\nOur aim is to update the global variational parameters\nstochastically, by considering subsets of the full dataset\nand making sequential updates.\ntor of global natural parameters of p(\u03b2), and by condi-\ntional conjugacy, the vector of global natural parameters\nof q(\u03b2) is \u03b7 +?\neach of the ithelements of the sums in Equation 7. We\nhave followed the notation of Hoffman & Blei (2014). The\ngeneral framework of stochastic variational inference we\nshall follow is summarized in Algorithm 1.\nLet \u03b7 denote the vec-\ni\u03b7i(yi,\u03c8i). In fact, \u03b7 = [a\/K,b(K \u2212\n1)\/K,c?,d?,e?,f?,D,0], and \u03b7iis a vector consisting of\nThe difficult step is in computing \u02c6 \u03b7i, and it is entirely de-\npendent on the form of the local variable approximation, of\nwhich we consider 2 types: \u2018Unstructured\u2019 methods where\nq(\u03c81:N|\u03b2) = q(\u03c81:N) and \u2018structured\u2019 methods where\nthis equivalence does not hold. Our notion of \u2018structure\u2019\ndescribes the dependence between local and global vari-\nables, as discussed by Hoffman & Blei (2014).\n3.1. Unstructured Variational Methods\nThe simplest, and most commonly used, approximation we\ncan make is the mean field approximation,\nqMF(\u03c8i) =\nK\n?\nk=1\nq(zik)q(wik)\n(8)\nwhere q(zik)\nN(\u03ba\u22121\nparameters,\nEq(\u03b2)qMF(\u03c81:N)[logp(y1:N,\u03c81:N|\u03b2)\nis optimized as a function of local variational parameters\n{\u03b8ik,\u03bdik,\u03baik}. Up to irrelevant constants,\nc\n2d\ni,k\n\u03baik\n?\u03bdik2\n?\n+\n=Bernoulli(\u03b8ik) and q(wik)\nGiven the current set of global\nthelocalELBO,\n=\nik\u03bdik,\u03ba\u22121\nik).\n\u03bb(t),\nLlocal\nlogq(\u03c81:N)]\n=\n\u2212\nLMF\u2212SVI\nlocal\n=\n?\n?\n2\u03b8ik\u03bdik\n\u00b5k\n\u03c4ky?\ni\n\u2212 \u03b8ik\n\u03baik2+\n1\n\u03baik\n??\u00b5k\u00b5?\n\u03bdik\n\u03baik\n\u03c4j\u03c4k\nk\n\u03c4k2\n+1\n\u03c4k\n?\n\u2212\nj?=k\n?\n?\n\u03b8ij\u03b8ik\u03bdij\n\u03baij\n\u00b5j\u00b5?\nk\n?\n\u2212\ne\n2f\n?\nlog(\u03baik)\ni,k\n?\u03bdik2\n\u03baik2+\n1\n\u03baik\n?\ni,k\n\u03b8ik(\u03c8(ak) \u2212 \u03c8(bk)) \u22121\n?\u03b8iklog\u03b8ik+ (1 \u2212 \u03b8ik)log(1 \u2212 \u03b8ik)?\n2\n?\ni,k\n\u2212\ni,k\n(9)"},{"page":4,"text":"Empirical Study of SVI for the Beta Bernoulli Process\nwhere \u03c8 is the digamma function. The mean field approx-\nimation breaks dependencies between all local and global\nvariables, and will provide a baseline to compare against.\nIt is possible to compute EqMF(\u03c8i)[\u03b7i] analytically given\nthe optimized local variational parameters. We denote the\nSVI algorithm which uses a mean field local variable ap-\nproximation as MF-SVI. It is identical to the original SVI\nalgorithm introduced by Hoffman et al. (2013).\nMimnoetal.(2012)suggestedanonlineSVImethodwhich\nmaintains structure between local variables specifically for\nthe LDA. We generalize their idea by suggesting the fol-\nlowing variational distribution over local parameters\nqMimno(\u03c8i) = exp(Eq(\u03b2)[logp(\u03c8i|y1:N,\u03b2)])\nwhere p(\u03c8i|y1:N,\u03b2) is the true posterior conditional of\n\u03c8i. Whilst we are unable to compute EqMimno(\u03c8i)[\u03b7i] ana-\nlytically, we are able to estimate it using MCMC. The SVI\nalgorithm using qMimnoas the local variational distribution\nshall be called Mimno-SVI.\n(10)\n3.2. Structured Variational Methods\nInstead of taking an expectation over q(\u03b2) to compute \u02c6 \u03b7i\nas in the previous section, we use the current set of global\nparameters \u03bb(t)to draw a sample \u03b2(t), and compute an\nestimate of Eq(\u03c8i|\u03b2(t))[\u03b7i]. Under this framework, Algo-\nrithm 1 becomes the SSVI-A algorithm of Hoffman & Blei\n(2014).\nOnce again, the simplest approximation that can be made is\nthe conditional mean-field approximation, where zik,wik\nare independent given \u03b2, with q(zik) = Bernoulli(\u03b8ik)\nand q(wik) = N(\u03ba\u22121\nthe local ELBO, EqMF(\u03c81:N|\u03b2(t))[logp(y1:N,\u03c81:N|\u03b2(t))\u2212\nlogq(\u03c81:N)] as a function of local variational parameters\n{\u03b8ik,\u03bdik,\u03baik}, and compute EqMF(\u03c81:N|\u03b2(t))[\u03b7i] analyti-\ncally given the optimized parameters. We shall call this\nSVI method MF-SSVI.\nik\u03bdik,\u03ba\u22121\nik). This time, we optimize\nThe Bernoulli-Gaussian products present in the generative\nprocess in Equation 5 can be thought of as a spike-and-\nslab model. Titsias & L\u00b4 azaro-Gredilla (2011) developed a\nvariational method which maintains dependence between\nzikand wikfor eack k, such that\n?\n\u00d7 N?wik;zik\u03ba\u22121\nThis approximation has the advantage that it maintains the\nspike-slab beaviour of the product zikwik, and matches the\nexact posterior when zik = 0. However, the dependen-\ncies between local variables for which k ?= k?are lost.\nAnalogous to MF-SSVI, we optimize the local ELBO us-\ning qTitsiasas a function of {\u03b8ik,\u03bdik,\u03baik}, and compute\nqTitsias(\u03c8i|\u03b2(t)) =\nk\nBernoulli(zik;\u03b8ik)\n(11)\nik\u03bdik,zik\u03ba\u22121\nik+ (1 \u2212 zik)\u03b3(t)\nw\n\u22121?.\nAlgorithm 1 General Stochastic Variational Inference\nInitialize t = 1, \u03bb(0).\nrepeat\nCompute step size \u03c1(t)= (t + t0)\u2212\u03b6.\nSelect subset of full data set, D.\nCompute \u02c6 \u03b7i, an (unbiased) estimator of Eq(\u03c8i|\u03b2)[\u03b7i]\nfor each i \u2208 D\nSet \u03bb(t)= (1\u2212\u03c1(t))\u03bb(t\u22121)+\u03c1(t)?\u03b7+N\n|D|\n?\ni\u2208D\u02c6 \u03b7i\n?\nuntil convergence\nEqTitsias(\u03c81:N|\u03b2(t))[\u03b7i] analytically given the optimized pa-\nrameters. We denote the SVI algorithm which uses the\nTitsias & L\u00b4 azaro-Gredilla (2011) local approximation as\nTitsias-SSVI.\nFinally we consider using the exact local conditional dis-\ntribution given by q(\u03c8i|\u03b2(t)) = p(\u03c8i|\u03b2(t),yi). We use\nMCMC samples to compute \u02c6 \u03b7iusing a Gibbs sampling\nscheme. We therefore call this method Gibbs-SSVI.\nMF-SVI (Hoffman et al., 2013), MF-SSVI, Gibbs-SSVI\n(Hoffman & Blei, 2014) and Mimno-SVI (Mimno et al.,\n2012) have been considered in the context of LDA before,\nbut the latter 3 have not been applied to factor analysis to\nthe best of our knowledge. Titsias-SSVI is a new method as\nTitsias & L\u00b4 azaro-Gredilla (2011) applied their variational\napproximation only to regression tasks. More details on the\nvariational approximations over local variables is provided\nin the appendix.\n4. Related Work\nThe idea of applying variational inference to the Indian\nbuffet process was first proposed in Doshi et al. (2008),\nbased on the stick breaking construction of the IBP (Teh\net al., 2007). Promising results were shown for the sim-\nple but somewhat limited \u201clinear Gaussian\u201d model, which\nis the model presented here without the weight vector, wi.\nPaisley & Carin (2009) consider the simpler finite approx-\nimation to the beta process described above, and extended\nthe model to include continuous weights wi. An exten-\nsion using power-EP, able to handle non-negativity con-\nstraints, was developed in (Ding et al., 2010) but has not\nbeen widely adopted. Alternative approaches to scale in-\nference in IBP based models have included parallelization\n(Doshi-Velez et al., 2009) and submodular optimization\n(Reed & Ghahramani, 2013). The former only performed\napproximate sampling, and the later is greedy and limited\nto positive weights. Mean field based stochastic variational\ninference schemes have been used for large scale dictio-\nnary learning, with some success (Li et al., 2012; Polatkan\net al., 2014). However, we shall show that preserving de-\npendencies between local variables will greatly improves\nperformance on image interpolation and denoising tasks."},{"page":5,"text":"Empirical Study of SVI for the Beta Bernoulli Process\n0204060\n0\n2\n4\n\u00b7105\nTraining Time (s)\n4\nTraining Time (s)\nPredictive Loglikelihood\n1\n026\n10\n15\n20\n25\n30\n35\nPSNR\nMF-SVI\nMF-SSVI\nTitsias-SVI\nMimno-SVI\nGibbs-SSVI\nGibbs\n1\nFigure 1. Predicitve loglikelihood versus training time on syn-\nthetically generated data, comparing Gibbs-SSVI, MF-SSVI and\nGibbs sampling. The same legend is used throughout this paper.\nMeanwhile the topic modelling community has taken great\nstrides developing stochastic variational inference methods\nfor latent Dirichlet allocation (Blei et al., 2003), encour-\naged by the availability of large corpora of text. The idea\nwas initially proposed in Hoffman et al. (2010), and refined\nin Mimno et al. (2012) where the sparse updates of Gibbs\nsampling were leveraged to scale inference on just a single\nmachine to 1.2 million books. The latter idea allows non-\ntruncated online learning (Wang & Blei, 2012) of Bayesian\nnon-parametricmodels, thoughonlythehierachicalDirich-\nlet process (Teh et al., 2004) was demonstrated.\nMore recently, Hoffman & Blei (2014); Liang & Hoff-\nman (2014) have shown that sampling from the global\nvariational distribution improves predictive performance\nfor the LDA and Bayesian non-negative matrix factoriza-\ntion respectively. In fact, the idea of optimizing an in-\ntractable variational inference algorithm by sampling from\nglobal variational distributions has been proposed in var-\nious contexts to deal with non-conjugacy (Ji et al., 2010;\nNott et al., 2012; Gerrish, 2013; Paisley et al., 2012; Ran-\nganath et al., 2014). Kingma & Welling (2014); Titsias\n& L\u00b4 azaro-Gredilla (2014); Salimans & Knowles (2013)\npropose change of variable methods to deal with non-\nconjugacy or improve convergence speed. In this work we\nfocus more on the quality of the variational approximation\nand attempt to exploit the conditional conjugacy.\n5. Experiments\nIn this section we discuss our findings from a range of ex-\nperiments. Results from experiments carried out on syn-\nthetically generated data are discussed first. We apply a\nrange of stochastic variational inference algorithms to carry\nout image inpainting and denoising tasks next. Finally the\n00.511.52\n\u22121\n0\n1\n2\n\u00b7104\nTraining Time (s)\nPredictive Loglikelihood\n1\nFigure 2. Predicitve loglikelihood versus training time on synthet-\nically generated data using Gibbs-SSVI with burn-in lengths of\n0,1,3,5,10 and 25. Converged predictive loglikelihood is mono-\ntonically increasing in burn-in length, so no legend is included.\nsame algorithms are applied to two large genomic datasets.\nWe choose to compare our models using predictive loglike-\nlihood of held out data, which we compute as\np\n?\u02c6 Y |Y\n\u2248\n?\n\u2248\n?\n?\np(\u02c6 Y |\u03b2,\u03c81:Ntest)q(\u03b2,\u03c81:Ntest)d(\u03b2,\u03c81:Ntest)\n?\n1\nM\nM\nm=1\nNtest\ni=1\nN\n?\n\u02c6 yn|z(m)\ni\n\u25e6 w(m)\ni\nA(m),I\/\u03b3(m)\nobs\n?\n(12)\nwhere?z(m)\nbeing used, and \u02c6 yiis the ithdata point in the test set.\nIn each of our experiments, we transform the data to have\nempirical mean 0 and variance 1. Hyperparameters are set\nas follows: a = b = 10, c = 1, d = 10, e = f = 1, and a\nlearning rate schedule of \u03c1t= t\u22120.75is employed.\ni\n,w(m)\ni\n,A(m),\u03b3(m)\nobs\n?are independent samples\nfrom q, for whichever type of variational approximation is\n5.1. Synthetic Data\nA key question that is important to consider when using\nvariational approximations of a particular form is, \u2018how\nclose is the approximation to the true posterior?\u2019. We at-\ntempt to answer such a question with our first experiment.\nDatawasgeneratedfromourpriorwithparameters\u03b3w= 1,\n\u03b3obs= 100, K = 80, N = 1e5 and D = 40, with 7.5%\nselected uniformly at random held out for testing on 100\nindependent experiments. We then applied Gibbs-SSVI\nand MF-SSVI, as well as an uncollapsed Gibbs sampler to\nthe generated data using K = 150 potential features and\nrandom initialization. The predictive mean squared errors\n(MSE) of the 3 methods were 0.022\u00b10.002, 0.027\u00b10.004"},{"page":6,"text":"Empirical Study of SVI for the Beta Bernoulli Process\n46\ng Time (s)\n02\nTraining Time (s)\n46\n10\n15\n20\n25\n30\n35\nPSNR\n1\n(a) Gibbs initialization\n02\nTraining Time (s)\n46\n10\n15\n20\n25\n30\n35\nPSNR\n02\nTraining Time (s)\n46\n10\n15\n20\n25\n30\n35\nPSNR\n1\n(b) Random initialization\nFigure 3. Results from interpolation of the 512 \u00d7 512 pixel \u2018Boat\u2019 image. PSNR vs training time shown using (a) Gibbs initialization\nand (b) random initialization. The pictures shown are the original image (top left), the image to be reconstructed with 80% of pixels\nunobserved (top right), the Gibbs-SSVI reconstruction (bottom left) and the MF-SSVI reconstruction (bottom right).\nand 0.020 \u00b1 0.002 and the average per iteration training\ntimes were 1.5, 0.6 and 7.6 seconds respectively. Figure 1\nillustrates our findings. The Gibbs sampler achieves a high\npredictive likelihood, but the average training time per iter-\nation was very high versus the SVI methods. Warm starting\nthe Gibbs sampler at each iteration helped Gibbs-SSVI to\nconverge in few iterations, whereas stochastically choos-\ning subsets of data points in SSVI methods requires re-\ninitializing local variables at each epoch. Notice that the\npredictive MSE of Gibbs-SSVI is close to the Gibbs sam-\npler, suggesting that the correct mean is being learnt. The\nlower likelihood the SSVI methods are able to achieve is\ntherefore due to a poor calibration in posterior variance, a\nknown issue with variational methods (Consonni & Marin,\n2007).\nAnother question of interest to us was, \u2018what is the em-\npirical trade-off between training time and unbiasedness in\nthe Gibbs-SSVI scheme?\u2019. More specifically, if we allow\nthe Gibbs sampler over the local variables to converge, the\nsubsequent ELBO gradient estimates would be unbiased,\nwhilst using samples from a Gibbs chain which has not\nconverged would lead to biased gradient estimates. Con-\nvergence of the Gibbs chain, however, may take a long\ntime. Therefore we experimented with a range of burn-in\nlengths of the Gibbs chain on synthetically generated data.\nVarious burn-in and sample length combinations were dis-\ncussed by Mimno et al. (2012). We tried burn-in lengths\nof 0,1,3,5,10 and 25 whilst fixing the number of samples\nused after burn-in to 3. The results can be seen in Figure 2.\nWhen the burn-in length is below 3 we notice severe loss\nin predictive power of the Gibbs-SSVI method. We notice\ndiminishing gains in predictive power as we increase the\nlength of burn-in. This experiment suggests that some bias\nintroduced by using samples from an unconverged Gibbs\nchain may be worth the reduction in training time. For sub-\nsequent experiments, we fix the burn-in length to 3.\n5.2. Image Interpolation and Denoising\nZhou et al. (2009) first applied the beta process for sparse\nimage representation with good results and much follow\nup research. The standard metric used for quantifying the\nquality of a reconstructed image is the peak signal-to-noise\nratio (PSNR), defined as 20log10(maximage\/rmse), where\nmaximageis the maximum possible pixel value and rmse is\nthe root mean squared error of the reconstruction.\nWe consider overlapping 8 \u00d7 8 pixel patches as individ-\nual 64 dimensional data points. The fact that the patches\nare overlapping technically breaks the exchangeability as-\nsumption of the prior distribution, however the extra model\naveraging is beneficial to prediction. Five grayscale im-\nages originally from Portilla et al. (2003) were used for our\nstudy: Boat, Barbara, Lena, House and Peppers. The first 3\nare 512 \u00d7 512 in size whilst the last 2 are 256 \u00d7 256. The\ndatasets are therefore of size N = (512 \u2212 7)2= 255,025\nand N = (256\u22127)2= 62,001 for 512\u00d7512 and 256\u00d7256\nimages respectively. We use a batchsize of Nsubset= 250\nand K = 250 features for our experiments.\nFor our first experiment, we consider the task of image in-\nterpolation, where the task is to reconstruct an image where\nonly 20% of the pixels, chosen uniformly at random, are\nobserved. Li et al. (2012) consider a mean field based vari-\national approximation for such a task, however, the learn-\ning rate schedule they used was \u03c1t= (t + 1000)\u22120.5. This\nimplies \u03c1t< 0.032 for all t \u2265 1, and that their algorithm\nrelied heavily on the initialization of global parameters.\nThey ran an MCMC algorithm over a subset of the data\nto initialize these global parameters, and we argue that this\nwas integral to the performance of their algorithm. We de-\ncided to test how sensitive the variational algorithms were"},{"page":7,"text":"Empirical Study of SVI for the Beta Bernoulli Process\nFigure 4. Results from interpolation and denoising of the 512 \u00d7\n512 pixel \u2018Barbara\u2019 image. The pictures shown are the original\nimage (top left), the image to be reconstructed with 50 % of pixels\nunobserved and remaining pixels corrupted with Gaussian noise\n(top right), the Gibbs-SSVI reconstruction (bottom left) and the\nMF-SSVI reconstruction (bottom right).\nto different initialization methods and an example can be\nseen in the performance graphs of Figure 3. We found that\ninitializing using MCMC improved the PSNR of MF-SVI,\nMF-SSVI and Titsias-SSVI by 4.8 on average versus ran-\ndom initialization. However, the analogous improvement\nfor Gibbs-SSVI and Mimno-SVI was 1.0. This suggests\nthat the methods which preserve intra-local variable struc-\nture are less sensitive to initialization.\nSecondly, we considered the joint task of image interpo-\nlation and denoising. Here, we observe 50 % of the pix-\nels chosen uniformly at random, except they are now cor-\nrupted with Gaussian noise with standard deviation 15 (the\noriginal pixels take integer values in [0,255]). Results for\nboth image interpolation and denoising tasks are summa-\nrized in Table 1. Gibbs-SSVI and Mimno-SVI consistently\noutperform the other methods and an explanation, outlined\nin Section 5.3, as to why this is the case can be deduced by\nstudying the images in Figures 3 and 4.\nFor the 512 \u00d7 512 images, the average training time per\nepoch was 0.12,0.12,0.13,0.46 and 0.48 secs for the\nMF-SVI, MF-SSVI, Titsias-SSVI, Mimno-SVI and Gibbs-\nSSVI methods respectively, on a 2.4GHz dual core ma-\nchine. Among the multiple experiments, the MF-SSVI re-\nconstructions were similar in appearance to the MF-SVI\nand Titsias-SSVI methods, whilst the Gibbs-SSVI recon-\nstructions were similar to the Mimno-SVI ones. We be-\nlieve the blurred appearance of the former 3 methods\u2019 re-\nconstructions is a result of the independence between znk\nand znk? for k ?= k?in their variational forms. In contrast,\nthe Gibbs-SSVI and Mimno-SVI methods maintain depen-\ndencebetweenznkandznk?, andarethereforeabletoselect\na subset offeatures which collectively bestexplain the data.\nThe latter methods are consequently much more capable of\n0510 1520\n2\n3\n4\n5\n\u00b7105\nTraining Time (s)\nPredictive Loglikelihood\n1\nFigure 5. Predicitve loglikelihood versus training time on cell line\ndata comparing five SVI algorithms.\ncapturing structure and detail in the images we tested on,\nand there is a mild cost to pay in extra training time.\n5.3. A Thought Experiment\nTo illustrate the problem with breaking dependencies be-\ntween zikand zik? for k ?= k?, we can consider a simple\nthoughtexperiment. Supposeyi= f+?i, yi\u2208 RD, where\nf \u223c N(0,I) and ?i\u223c N(0,0.05I) independently for i =\n1,...,N. Now consider applying MF-SSVI and Gibbs-SVI\nalgorithms to this dataset, whilst fixing wik = 1 for each\ni,k, and using K = 2 features. Let\u2019s assume that at the cur-\nrent iteration \u03c0(t)\n1\n\u2248 \u03c0(t)\nELBO for the MF-SSVI method will have local optima for\n\u03b8i1= 1 \u2212 \u03b8i2, since exactly 1 feature is needed to explain\nthe data, so MF-SSVI would find a local optimum of the\nform q(zi1) = Bernoulli(s), q(zi2) = Bernoulli(1 \u2212 s)\nfor some s \u2208 (0,1). (We were able to verify this form of\nlocal optimum empircally). The Gibbs-SSVI will generate\nsamples of the form zi= (0,1) and zi= (1,0).\n2\nand \u03c6(t)\n1\n\u2248 \u03c6(t)\n2\n\u2248 f. The local\nWe would like to predict yiwith each model. Since q(zi1)\nand q(zi2) are independent under MF-SSVI, predictions\nwill be \u02c6 yi= 0 with probability s(1 \u2212 s), \u02c6 yi\u2248 yiwith\nprobabilitys2+(1\u2212s)2andfinally \u02c6 yi\u2248 2yiwithprobabil-\nity s(1 \u2212 s). Conversely, the Gibbs samper in Gibbs-SSVI\nwill place little to no probability on both, or neither of the 2\nfeatures being used for prediction. In summary, the Gibbs\nbased local variable estimates can handle the strong corre-\nlation between the 2 features, whilst the MF based method\ncannot and suffers dramatically because of it.\nOne possible solution to this problem would be to encour-\nage all features to have limited correlation apriori, discour-\naging situations where multiple features are learned to be"},{"page":8,"text":"Empirical Study of SVI for the Beta Bernoulli Process\nTable 1. PSNR performance of image interpolation (left entries) and denoising (right entries) tasks using Gibbs initialization of global\nparameters on a randomly chosen subset of data.\nBOAT\nBARBARA\nLENA\nHOUSE\nPEPPERS\nMF-SVI\nMF-SSVI\nTITSIAS-SSVI\nMIMNO-SVI\nGIBBS-SSVI\n21.1\n22.3\n23.2\n32.4\n34.3\n19.5\n20.8\n21.5\n29.7\n31.5\n21.8\n22.2\n22.1\n36.2\n38.2\n20.6\n21.4\n21.7\n35.1\n37.0\n24.1\n24.7\n26.3\n39.4\n43.3\n23.6\n24.4\n25.8\n36.9\n41.7\n25.3\n26.7\n26.7\n42.8\n40.5\n24.2\n25.4\n25.3\n40.1\n37.8\n25.9\n25.8\n27.9\n43.7\n47.4\n24.4\n24.1\n26.8\n40.4\n42.3\nsimilar to each other. Encouraging dissimilarity in such\na way is challenging and would complicate the otherwise\nclean updates that are possible in SVI methods.\n5.4. Genomic data\nVast amounts of genomic data are currently being collected\nas technology advances. It will be crucial to develop ma-\nchine learning models and more importantly, inference al-\ngorithms, which can cope with large data sets, whilst still\nretaining flexible modelling ability. We consider 2 datasets\nfor which sparse latent feature modelling is appropriate.\nWe use K = 500 features for both experiments.\nCancer cell line data.\ndia is a collection of around 450 cancer samples including\ngene expression, copy number variation, and drug response\ninformation. We focus on modeling the gene expression\ndata, which has measurements for around 15,000 genes. In\nthis setting we are more interested in finding overlapping\nclusters (sparse features) of genes rather than samples, so\nwe effectively have N = 15000,D = 450. The latent fac-\ntors found can then be interpreted as biological pathways,\nor sets of genes regulated by the same transcription factor.\nUnderstanding the structure in this data is valuable as a first\nstep towards associating the cellular characteristics of the\ncancers to their drug response profiles. We randomly hold\nout 10 % of the data for testing. Results for this experiment\nare summarized in Figure 5.\nThe Cancer Cell Line Encyclope-\nCyTOF data.\nput technology capable of measuring up to 40 protein abun-\ndance levels in thousands of individual cells per second.\nThe cells are controlled using flow cytometry and spe-\ncific proteins are tagged using heavy metals which can be\nmeasured using time-of-flight mass spectrometry. Exist-\ning analyses have attempted to group the observed cells\ninto non-overlapping subpopulations, but we here show\nthat the data can be effectively modeled as compromising\nof a spectrum of cell types expressing different latent fac-\ntors to differing extents. The sample we analyse consists\nof human immune cells, so representing the heterogeneity\nis relevant for understanding disease response. Our dataset\nhas N = 532,000,D = 40 and a random 5% is used as\nCyTOF is a novel extremely high through-\ntest data. The results for the experiments on this data fol-\nlow a very similar pattern to that of the cell line gene ex-\npression data. The converged predictive log-likelihoods af-\nter training for 10 minutes are \u22121.1e6, \u22129.6e5, \u22129.4e5,\n\u22123.8e5 and \u22123.2e5 for the MF-SVI, MF-SSVI, Titsias-\nSSVI, Mimno-SVI and Gibbs-SSVI methods respectively.\n6. Conclusions\nIn this work, we compare various stochastic variational in-\nference algorithms for beta process factor analysis. Whist\nmany methods in the literature have been proposed, we\nhave chosen to exploit the conditional conjugacy and the\nexponential family nature of our model to create simple\nnatural parameter updates.\nHoffman & Blei (2014) found that preserving structure be-\ntween local and global variables significantly boosted per-\nformance for the LDA, but based on our experiments, we\nconclude that preserving intra-local variable dependence is\ncrucial to prediction in the beta-Bernoulli process. This is\nevident from the fact that both Gibbs-SSVI and Mimno-\nSVI consistently and significantly outperform MF-SVI,\nMF-SSVI and Titsias-SSVI on a variety of image interpo-\nlation and denoising tasks and on modelling genomic data.\nThe Titsias-SSVI method models dependence between zik\nand wik, but does not appear to significantly outperform\nMF-SSVI, suggesting that this dependence is not crucial\nin prediction. Mimno-SVI does not maintain dependence\nbetween local and global variables whilst MF-SSVI does,\nand yet Mimno-SVI leads to better predictions. We discuss\nwhy this is the case in a simple thought experiment, show-\ning the benefit of maintaining dependence between local\nvariables where k ?= k?. The multi-cluster, sparse nature\nof the beta-Bernoulli process makes mean field type local\nvariable approximations highly sensitive to correlated fea-\ntures. Gibbs-SSVI does also modestly outperform Mimno-\nSVI through maintaining dependencies between global and\nlocal variables.\nIn summary, care is needed to ensure that the dependen-\ncies encoded by a particular variational approximation are\nappropriate for the model being considered."},{"page":9,"text":"Empirical Study of SVI for the Beta Bernoulli Process\nReferences\nAhn, Sungjin, Korattikara, Anoop, and Welling, Max.\nBayesian posterior sampling via stochastic gradient\nfisher scoring. In Proceedings of the 29th International\nConference on Machine Learning (ICML-12), pp. 1591\u2013\n1598, 2012.\nAldous, D. J. Exchangeability and Related Topics. In\u00b4Ecole\nd\u2019\u00b4Et\u00b4 e de Probabiliti\u00b4 es de Saint-Flour XIII - 1983, pp. 1\u2013\n198. Springer Berlin Heidelberg, 1985.\nAmari, S. Natural Gradient Works Efficiently in Learning.\nNeural Computation, 10(2):251\u2013276, 1998.\nBendall, S. C., Simonds, E. F., Qiu, P., El-ad, D. A.,\nKrutzik, P. O., Finck, R., Bruggner, R. V., Melamed, R.,\nTrejo, A., Ornatsky, O. I., et al. Single-cell mass cytom-\netry of differential immune and drug responses across a\nhuman hematopoietic continuum. Science, 332(6030):\n687\u2013696, 2011.\nBlei, D. M., Ng, A. Y., and Jordan, M. I. Latent Dirichlet\nAllocation. Journal of Machine Learning Research, 3:\n993\u20131022, 2003.\nConsonni, G. and Marin, J. M. Mean-field Variational Ap-\nproximate Bayesian Inference for Latent Variable Mod-\nels. Computational Statistics and Data Analysis, 52:\n790\u2013798, 2007.\nDing, N., Xiang, R., Molloy, I., Li, N., et al. Nonpara-\nmetric Bayesian matrix factorization by Power-EP. In\nInternational Conference on Artificial Intelligence and\nStatistics, pp. 169\u2013176, 2010.\nDoshi, F., Miller, K. T., Gael, J. Van, and Teh, Y. W. Varia-\ntional inference for the Indian buffet process. Advances\nin Neural Information Processing Systems, 2008.\nDoshi-Velez, F., Knowles, D. A., Mohamed, S., and\nGhahramani, Z. Large Scale Nonparametric Bayesian\nInference: Data Parallelisation in the Indian Buffet Pro-\ncess. In Advances in Neural Information Processing Sys-\ntems, volume 22, pp. 2\u20133, 2009.\nGerrish, S.\nModeling Influence and Decision Making. PhD thesis,\nPrinceton University, 2013.\nApplications of Latent Variable Models in\nGhahramani, Z. and Beal, M. J. Variational Inference for\nBayesian Mixtures of Factor Analyzers. In Advances in\nNeural Information Processing Systems, 1999.\nGriffiths, T. and Ghahramani, Z. Infinite Latent Feature\nModels and the Indian Buffet Process. Advances in Neu-\nral Information Processing Systems, 2006.\nGriffiths, T. and Ghahramani, Z. The Indian Buffet Pro-\ncess: An introduction and review. Journal of Machine\nLearning Research, 12:1185\u20131224, 2011.\nHjort, N. L.\nbeta processes in models for life history data. Annals\nof Statistics, 18(3):1259\u20131294, 1990.\nNonparametric Bayes estimators based on\nHoffman, M. D. and Blei, D. M. Structured Stochastic\nVariational Inference. arXiv, 2014. http:\/\/arxiv.\norg\/abs\/1404.4114.\nHoffman, M.D., Blei, D.M., andBach, F.R. OnlineLearn-\ning for Latent Dirichlet Allocation. In Advances in Neu-\nral Information Processing Systems, volume 2, pp. 5,\n2010.\nHoffman, M. D., Blei, D.M., Wang, C., and Paisley, J.\nStochastic Variational Inference.\nLearning Research, 14:1303\u20131347, 2013.\nJournal of Machine\nJi, C., Shen, H., and West, M.\ntions for Marginal Likelihoods. Technical Report, Duke\nUniversity, 2010. http:\/\/ftp.stat.duke.edu\/\nWorkingPapers\/10-05.pdf.\nBounded Approxima-\nKingma, D. and Welling, M. Auto-Encoding Variational\nBayes. Intl. Conf. on Learning Representations, 2014.\nKnowles, D. and Ghahramani, Z.\nAnalysis and Infinite Independent Components Analy-\nsis. 7th International Conference on Independent Com-\nponent Analysis and Signal Separation, 2007.\nInfinite Sparse Factor\nKnowles, David, Ghahramani, Zoubin, et al.\nmetric bayesian sparse factor models with application to\ngene expression modeling. The Annals of Applied Statis-\ntics, 5(2B):1534\u20131552, 2011.\nNonpara-\nLi, L., Silva, J., Zhou, M., and Carin, L. Online Bayesian\nDictionary Learning for Large Datasets. Intl. Conf. on\nAcoustics, Speech and Signal Processing, 2012.\nLiang, D. and Hoffman, M. D. Beta Process Non-negative\nMatrix Factorization with Stochastic Structured Mean-\nField Variational Inference. arXiv, 2014. http:\/\/\narxiv.org\/abs\/1411.1804.\nMacEachern, Steven N and M\u00a8 uller, Peter. Estimating mix-\nture of dirichlet process models. Journal of Computa-\ntional and Graphical Statistics, 7(2):223\u2013238, 1998.\nMimno, D., Hoffman, M., and Blei, D. Sparse Stochastic\nInferenceforLatentDirichletAllocation. Proceedingsof\nthe 29th International Conference on Machine Learning,\n2012."},{"page":10,"text":"Empirical Study of SVI for the Beta Bernoulli Process\nNott, D., Tan, S., Villani, M., and Kohn, R.\nsion Density Estimation with Variational Methods and\nStochastic Approximation. Journal of Computational\nand Graphical Statistics, 21(3):797\u2013820, 2012.\nRegres-\nOrbanz, Peter and Teh, Yee Whye. Bayesian nonparametric\nmodels. In Encyclopedia of Machine Learning, pp. 81\u2013\n89. Springer, 2010.\nPaisley, J. and Carin, L. Nonparametric Factor Analysis\nwith Beta Process Priors. Proceedings of the 26th Inter-\nnational Conference on Machine Learning, 2009.\nPaisley, J., Blei, D., and Jordan, M. Variational Bayesian\nInference with Stochastic Search. Proceedings of the\n29th International Conference on Machine Learning,\n2012.\nPolatkan, G., Zhou, M., Carin, L., Blei, D., and\nDaubechies, I. A Bayesian Nonparametric Approach to\nImage Super-resolution. IEEE Trans. on Pattern Analy-\nsis and Machine Intelligence, 2014.\nPortilla, J., Strela, V., Wainwright, M. J., and Simoncelli,\nE. P. Image Denoising using Scale Mixtures of Gaus-\nsians in the Wavelet Domain. IEEE Trans on Image Pro-\ncessing, 2003.\nRai, P. and Daum\u00b4 e, H. The Infinite Hierarchical Factor Re-\ngression Model . Advances in Neural Information Pro-\ncessing Systems, 2008.\nRanganath, R., Gerrish, S., and Blei, D. Black Box Varia-\ntional Inference. Proceedings of the 17th Conference on\nArtificial Intelligence and Statistics, 2014.\nRasmussen, Carl and Williams, Chris. Gaussian processes\nfor machine learning. Gaussian Processes for Machine\nLearning, 2006.\nReed, C. and Ghahramani, Z. Scaling the Indian Buffet\nProcess via Submodular Maximization. Proceedings of\nthe 30th International Conference on Machine Learning,\n2013.\nSalimans, T. and Knowles, D. Fixed-Form Variational Pos-\nteriorApproximationThroughStochasticLinearRegres-\nsion. Bayesian Analysis, 8:837\u2013882, 2013.\nTeh, Y. W., Jordan, M. I., Beal, M. J., and Blei, D. M. Shar-\ning clusters among related groups: Hierarchical dirichlet\nprocesses. In Advances in Neural Information Process-\ning Systems, 2004.\nTeh, Y. W., G\u00a8 or\u00a8 ur, D., and Ghahramani, Z. Stick break-\ning construction for the Indian buffet process. Proceed-\nings of the 11th Conference on Artificial Intelligence and\nStatistics, 2007.\nThibaux, R. and Jordan, M.I.\ncesses and the Indian Buffet Process. Proceedings of the\n11th Conference on Artificial Intelligence and Statistics,\n2007.\nHierarchical Beta Pro-\nTitsias, M. K. and L\u00b4 azaro-Gredilla, M. Spike and Slab\nVariational Inference for Multi-Task and Multiple Ker-\nnel Learning. Advances in Neural Information Process-\ning Systems, 2011.\nTitsias, M. K. and L\u00b4 azaro-Gredilla, M. Doubly Stochas-\ntic Variational Bayes for Non-Conjugate Inference. Pro-\nceedings of the 31st International Conference on Ma-\nchine Learning, 2014.\nWang, C. and Blei, D. Truncation-free stochastic varia-\ntional inference for bayesian nonparametric models. Ad-\nvances in Neural Information Processing Systems, 25:\n422\u2013430, 2012.\nWelling, Max and Teh, Yee W.\nstochastic gradient langevin dynamics. In Proceedings\nof the 28th International Conference on Machine Learn-\ning (ICML-11), pp. 681\u2013688, 2011.\nBayesian learning via\nWest, M. Bayesian Factor Regression Models in the \u201clarge\np, small n\u201d Paradigm . Bayesian Statistics, 7:723\u2013732,\n2003.\nZhou, M., Chen, H., Paisley, J., Ren, L., Sapiro, G., and\nCarin, L. Non-Parametric Bayesian Dictionary Learning\nfor Sparse Image Representations. Advances in Neural\nInformation Processing Systems, 2009."},{"page":11,"text":"Empirical Study of SVI for the Beta Bernoulli Process\nAppendix\nHere, we provide details about the local variable approximations introduced in the main text of the paper.\nMimno-SVI\nThe form of the local approximation in the Mimno-SVI method is\nlogqMimno(\u03c8i) = Eq(\u03b2)[logp(\u03c8i|y1:N,\u03b2)]\n= Eq(\u03b2)\n?\n?\n?\n\u2212\u03b3obs\n2\n??yi\u2212 (zi\u25e6 wi)\u03a6??2+\nzikwik\nwik\n?\nk\nziklog\n?\n\u03c0k\n1 \u2212 \u03c0k\n\u00b5k\u00b5?\n\u03c4k\u03c4j\n?\n\u2212\u03b3w\n?\n2wiw?\ni\n?\n+ const\n= \u2212c\n2d\nk\n??\u00b5k\u00b5?\nk\n\u03c4k2\n+1\n\u03c4k\n?\n+\n??\ni+ const\nj?=k\nzijwij\nj\n\u2212 2\u00b5ky?\ni\n\u03c4k\n?\n+\nk\nzik(\u03c8(ak) \u2212 \u03c8(bk)) \u2212\ne\n2fwiw?\nIt is clear that logqMimnois quadratic in each wikand linear in each zik, therefore a Gibbs based sampler can easily be\nconsructed to sample from qMimno, where wikis Gaussian given all other local variables, and zikis Bernoulli given all\nother local variables.\nMF-SSVI\nThe local ELBO in the MF-SSVI framework is very similar to that of the MF-SVI, the difference being that samples of the\nglobal variables are used in MF-SSVI. The local ELBO has the following form\nLMF\u2212SSVI\nlocal\n=\u03b3obs\n2\n?\n\u2212\u03b3w\ni,k\n\u03b8ik\u03c6k\n?\n2\u03bdik\n\u03baiky?\n?\u03bdik2\ni\u2212\n?\u03bdik2\n?\n?\u03b8iklog\u03b8ik+ (1 \u2212 \u03b8ik)log(1 \u2212 \u03b8ik)?.\n\u03baik2+\n1\n\u03baik\n?\n\u03c6?\nk\u2212\n?\n?\nj?=k\n\u03b8ij\u03bdij\n\u03baij\n\u03bdik\n\u03baik\u03c6?\nj\n?\n2\n?\n?\ni,k\n\u03baik2+\n1\n\u03baik\n?\n+\n?\ni,k\n\u03b8ik\n?\n\u03c0k\n1 \u2212 \u03c0k\n\u22121\n2\ni,k\nlog(\u03baik) \u2212\ni,k\nThis is optimized as a function of {\u03b8ik,\u03bdik,\u03baikusing gradient descent. Once a local optimum is found, EqMF(\u03c81:N|\u03b2(t))[\u03b7i]\ncan be computed analytically as a function of the optimized parameters and global variable samples.\nTitsias-SSVI\nRecall that the Titsias-SSVI method maintains dependence between zikand wikfor each k. The local ELBO for Titsias-\nSSVI is\nLTitsias\u2212SSVI\nlocal\n=\u03b3obs\n2\n?\n\u2212\u03b3w\ni,k\n\u03b8ik\u03c6k\n?\n2\u03bdik\n\u03baiky?\n?\u03bdik2\n?log(\u03baik) \u2212 1?+ (1 \u2212 \u03b8ik)?log(\u03b3w) \u2212 1??\n?\u03b8iklog\u03b8ik+ (1 \u2212 \u03b8ik)log(1 \u2212 \u03b8ik)?.\ni\u2212\n?\u03bdik2\n?\n\u03baik2+\n1\n\u03baik\n?\n\u03c6?\nk\u2212\n?\n?\nj?=k\n\u03b8ij\u03bdij\n\u03baij\n\u03bdik\n\u03baik\u03c6?\nj\n?\n2\n?\n?\n?\ni,k\n\u03baik2+\n1\n\u03baik\n+\n?\ni,k\n\u03b8ik\n?\n\u03c0k\n1 \u2212 \u03c0k\n\u22121\n2\ni,k\n?\n\u03b8ik\n\u2212\ni,k"},{"page":12,"text":"Empirical Study of SVI for the Beta Bernoulli Process\nAgain, this function is maxized as a function of {\u03b8ik,\u03bdik,\u03baikusing gradient descent, and the optimized parameters along\nwith the global variable samples are used to compute EqTitsias(\u03c81:N|\u03b2(t))[\u03b7i] analytically.\nGibbs-SSVI\nThe Gibbs-SVI method uses the true posterior conditional distribution for local variables\nlogqGibbs(\u03c8i) = logp(\u03c8i|y1:N,\u03b2)\n= \u2212\u03b3obs\n2\n??yi\u2212 (zi\u25e6 wi)\u03a6??2+\n?\n?\n?\n??\n\u2212\u03b3w\nk\nziklog\n?\n\u03c0k\n1 \u2212 \u03c0k\n?\n\u2212\u03b3w\n2wiw?\n?\ni+ const\n= \u2212\u03b3obs\n2\nk\nzikwik\u03c6k\n?\n\u03c0k\nwik\u03c6?\nk+\nj?=k\nzijwij\u03c6?\nj\n?\n\u2212 2y?\ni\n+\nk\nziklog\n?\n1 \u2212 \u03c0k\n?\n2wiw?\ni+ const\nJust as was the case with Mimno-SVI, we notice that logqGibbsis quadratic in each wikand linear in each zik, therefore a\nGibbs sampler can be designed to sample from qGibbs."}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/David_Knowles2\/publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process\/links\/55a53c7b08aef604aa042e0b.pdf","widgetId":"rgw29_56ab19254c6ed"},"id":"rgw29_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=279309917&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw30_56ab19254c6ed"},"id":"rgw30_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=279309917&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":279309917,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"55a53c7b08aef604aa042e0b","name":"David A. Knowles","date":"Jul 14, 2015 ","nameLink":"profile\/David_Knowles2","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/David_Knowles2\/publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process\/links\/55a53c7b08aef604aa042e0b.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/David_Knowles2\/publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process\/links\/55a53c7b08aef604aa042e0b.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"a6ba8368747c892702032e815493af06","showFileSizeNote":false,"fileSize":"1.48 MB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"55a53c7b08aef604aa042e0b","name":"David A. Knowles","date":"Jul 14, 2015 ","nameLink":"profile\/David_Knowles2","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/David_Knowles2\/publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process\/links\/55a53c7b08aef604aa042e0b.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/David_Knowles2\/publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process\/links\/55a53c7b08aef604aa042e0b.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"a6ba8368747c892702032e815493af06","showFileSizeNote":false,"fileSize":"1.48 MB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[{"props":{"position":"float","orientation":"portrait","coords":"pag:7:rect:55.44,229.39,234.01,63.46","ordinal":"4"},"assetId":"AS:294396223344656@1447200937844"}],"figureAssetIds":["AS:294396223344656@1447200937844"],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=tC0S4zjo2iuih9cBLhpG7V7C8lLwJozE4ymco1ABQWIefQlaEx8re30u9BErBxvdiSLx1qkFWKrlhjce1b-UUw.McLLQ1EvSb4x0731H-gs-3So-FLp94wzTUAOEtwHCXQ4NMYxF9GptNO5cUt-uEA0hOVokaQn9UMo8XP35Jsq3g","clickOnPill":"publication.PublicationFigures.html?_sg=9CgJDEVSU6BYvbpAqKnz3Us9ZacK9ccsWiftFuuXU7Opc_YgCMeTt0-sMBtAfG2ymfrO2mHr7MdOGpVpUkFQ0w.cJv4kv11ATqz08gjM_3pz5fEi5o0XC-DF4DcYcmsupXQXnUyESi4N_p-FcV6vgfmxF6lRk9lICs3MCvmVMwv-Q"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1o9o3\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FDavid_Knowles2%2Fpublication%2F279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process%2Flinks%2F55a53c7b08aef604aa042e0b.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1o9o3\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=r11Isqcan8Lgtue1FGDcEqDcpPku6UrXrxoYfiC4rSZqDd9btc9fOoN6G5rRNFxnbSMI6cLkg0g1aXlwyes5Tg","urlHash":"bbff32ae10f29e715da981a1d16e8637","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=qmLYlzEVy-zCnKMEas9GdKUgxAMlhZshh0IlyQN1RqTsTvwUE-q6RmDHcIqlaG0w2vRbBF33tGs5VOCO4aOoB0ZkE88LZbkHN3JqbfopEcU.Jnl1sckYaRdvTnarbL7DttYxd7nWDwBvhZ0B8otqGaikMRNLf02Ll_Ocz3FivtaM-rzlt1X_EuJpSu2aegxoNg.Kj5vc5MMmfrahC7swJFJvCZ6eWw-hmXsMyRlrTGgWTJ7pYMBJxnQbuDIZ4Su4oivQ4coOC4EeAGZahUv82UO7A","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"55a53c7b08aef604aa042e0b","trackedDownloads":{"55a53c7b08aef604aa042e0b":{"v":false,"d":false}},"assetId":"AS:251158589603853@1436892282412","readerDocId":"4575162","assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":279309917,"commentCursorPromo":null,"widgetId":"rgw32_56ab19254c6ed"},"id":"rgw32_56ab19254c6ed","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FDavid_Knowles2%2Fpublication%2F279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process%2Flinks%2F55a53c7b08aef604aa042e0b.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A251158589603853%401436892282412&publicationUid=279309917&linkId=55a53c7b08aef604aa042e0b&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=wGVRMYqlVz3Adpsg3RAvPoW5a1HvLqFc-FEIRxgxmO1h2QhhEt8GVsQjVX7b0Gu6V61gHkmsvyaaFaKkWgFEHWWlH8bpKM2NtF1QfIdx0Sg.4rW4pPiV5BgUEUvJDHFL2jwjw8bY6AW0FOa33TlHPLPT04y_1XXJ7-28MX0FsgmHoZd5JR3XkffE4nnFsPZ1kA.nYKRwsEscY-gyjhiCcDG5w-sOem_DO0Iz_9zeulMv2caJRbkqRitOQLVeexSwbX_GI6G0KYigcTE5W6a4XHbFg","publicationUid":279309917,"trackedDownloads":{"55a53c7b08aef604aa042e0b":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw34_56ab19254c6ed"},"id":"rgw34_56ab19254c6ed","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw35_56ab19254c6ed"},"id":"rgw35_56ab19254c6ed","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw36_56ab19254c6ed"},"id":"rgw36_56ab19254c6ed","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw37_56ab19254c6ed"},"id":"rgw37_56ab19254c6ed","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw38_56ab19254c6ed"},"id":"rgw38_56ab19254c6ed","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw33_56ab19254c6ed"},"id":"rgw33_56ab19254c6ed","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw31_56ab19254c6ed"},"id":"rgw31_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab19254c6ed"},"id":"rgw2_56ab19254c6ed","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":279309917},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=279309917&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab19254c6ed"},"id":"rgw1_56ab19254c6ed","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"kxu4YO9573zl4uHqCF27Gpspz94CVmhQi1DmdN+X\/DNriX1aTnHpXrhM3SyHw8kjTeM5erndp3eZ\/g2zt3OVEvmmjrx3gdOiUCBC5YydUJfKqSnlnuGNbpdVSYjRrlRC9NXpRhW7nwd31uabwMEOHJLXN\/dmfACYwJL\/tz1EnAj2WEyt+0h4ujkigaWoPg+XwCUn2aBPXeTp6utqXXFTqwjEXVC6j9kERkWLvuwRpVfJi7zRUpyUfMZ2wK0i0hH4o3pWuKQRDxnVyz+AAkZowXiICi1odOHNP72iZt+ST6I=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process\" \/>\n<meta property=\"og:description\" content=\"Stochastic variational inference (SVI) is emerging as the most promising\ncandidate for scaling inference in Bayesian probabilistic models to large\ndatasets. However, the performance of these...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process\/links\/55a53c7b08aef604aa042e0b\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process\" \/>\n<meta property=\"rg:id\" content=\"PB:279309917\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"An Empirical Study of Stochastic Variational Algorithms for the Beta Bernoulli Process\" \/>\n<meta name=\"citation_author\" content=\"Amar Shah\" \/>\n<meta name=\"citation_author\" content=\"David A. Knowles\" \/>\n<meta name=\"citation_author\" content=\"Zoubin Ghahramani\" \/>\n<meta name=\"citation_publication_date\" content=\"2015\/06\/26\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/David_Knowles2\/publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process\/links\/55a53c7b08aef604aa042e0b.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/215868066921738\/styles\/pow\/publicliterature\/FigureList.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-04006536-4fcd-4627-a16e-331b72b90e3d","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":425,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw39_56ab19254c6ed"},"id":"rgw39_56ab19254c6ed","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-04006536-4fcd-4627-a16e-331b72b90e3d", "bbbb035955b120c593d8c2bc33978fc90fe2c801");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-04006536-4fcd-4627-a16e-331b72b90e3d", "bbbb035955b120c593d8c2bc33978fc90fe2c801");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw40_56ab19254c6ed"},"id":"rgw40_56ab19254c6ed","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/279309917_An_Empirical_Study_of_Stochastic_Variational_Algorithms_for_the_Beta_Bernoulli_Process?ev=auth_pub","requestToken":"KI7IdNc1WR\/UaKTj6r0ACdL3qhfdDuwNL+q2xZordLKaNMnj+dJibE5saMBLRdVTiDJJJuz2l2sbgnc4I9psEZd9vJixwrYfkf\/Kb6fr\/X4MLJGjYmd6yXJCgg6fh\/qeJhEj6mBUNeeV2ThkhtJtUe6trRQFsx13X4ag8EE7ZJb05mfDcb2n\/H9Y+cONG8pgc7P3oX\/d4L3sq+0zufoYS7DzBlfEDBC2OdedLTkuGFf0EzqGm\/xROeEwsUI6c2Qyw6ZhGS7kLW8GRWKOPhoSF1g2CsA0aqJInuvAmKrR34Y=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=9oZW7LJdQpt-M61xBMK5PjFmczgImmk3WgeIvABueZu6zQqegTr66TVcQuh4ZbaQ","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjc5MzA5OTE3X0FuX0VtcGlyaWNhbF9TdHVkeV9vZl9TdG9jaGFzdGljX1ZhcmlhdGlvbmFsX0FsZ29yaXRobXNfZm9yX3RoZV9CZXRhX0Jlcm5vdWxsaV9Qcm9jZXNzP2V2PWF1dGhfcHVi","signupCallToAction":"Join for free","widgetId":"rgw42_56ab19254c6ed"},"id":"rgw42_56ab19254c6ed","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw41_56ab19254c6ed"},"id":"rgw41_56ab19254c6ed","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw43_56ab19254c6ed"},"id":"rgw43_56ab19254c6ed","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
