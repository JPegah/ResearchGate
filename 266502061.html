<!DOCTYPE html> <html lang="en" class="" id="rgw33_56ab1ddd160c2"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="PvSiIvx9NvH6SXZ7IkIh8PCp1+CiM+ND7Es2ffFGRxzPjY8r9FF6uJ69vCe6CXT8Y4WaXQiCFfKkZl6iHtvyA/3hWZGhh74zWPasaSWdbZr3NXW+p5OtKb+icFDc4NvliFKiWwKrDanq8RTp3aL9FyfrVX0XzSYzkD3dKVzeXSYM2uZj/iG09eUPn54yGcfmLnDnImIPanyhvujwghi6EG4A+7g7Ama9G8NEUmL4H0ymEf9goR4Le7OxmWM49aFu0Np5RTLnGYHeHnNOBPHDF7wy72cp1mkW+lAxyBBVGgk="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-795edb08-f5ec-49d4-9de1-ac17fce3d714",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/266502061_Fast_Bayesian_Inference_for_Non-Conjugate_Gaussian_Process_Regression" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression" />
<meta property="og:description" content="We present a new variational inference algorithm for Gaussian process regres-sion with non-conjugate likelihood functions, with application to a wide array of problems including binary and..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/266502061_Fast_Bayesian_Inference_for_Non-Conjugate_Gaussian_Process_Regression/links/54365a070cf2bf1f1f2b8032/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/266502061_Fast_Bayesian_Inference_for_Non-Conjugate_Gaussian_Process_Regression" />
<meta property="rg:id" content="PB:266502061" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression" />
<meta name="citation_author" content="Mohammad Emtiyaz Khan" />
<meta name="citation_author" content="Shakir Mohamed" />
<meta name="citation_author" content="Kevin P Murphy" />
<meta name="citation_publication_date" content="2012/01/01" />
<meta name="citation_journal_title" content="Advances in neural information processing systems" />
<meta name="citation_issn" content="1049-5258" />
<meta name="citation_volume" content="4" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/266502061_Fast_Bayesian_Inference_for_Non-Conjugate_Gaussian_Process_Regression" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/266502061_Fast_Bayesian_Inference_for_Non-Conjugate_Gaussian_Process_Regression" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression</title>
<meta name="description" content="Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1ddd160c2" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1ddd160c2" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab1ddd160c2">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Fast%20Bayesian%20Inference%20for%20Non-Conjugate%20Gaussian%20Process%20Regression&rft.title=Advances%20in%20Neural%20Information%20Processing%20Systems&rft.jtitle=Advances%20in%20Neural%20Information%20Processing%20Systems&rft.volume=4&rft.date=2012&rft.issn=1049-5258&rft.au=Mohammad%20Emtiyaz%20Khan%2CShakir%20Mohamed%2CKevin%20P%20Murphy&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression</h1> <meta itemprop="headline" content="Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/266502061_Fast_Bayesian_Inference_for_Non-Conjugate_Gaussian_Process_Regression/links/54365a070cf2bf1f1f2b8032/smallpreview.png">  <div id="rgw8_56ab1ddd160c2" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56ab1ddd160c2" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Mohammad_Khan106" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Mohammad Emtiyaz Khan" alt="Mohammad Emtiyaz Khan" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Mohammad Emtiyaz Khan</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw10_56ab1ddd160c2" data-account-key="Mohammad_Khan106">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Mohammad_Khan106"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Mohammad Emtiyaz Khan" alt="Mohammad Emtiyaz Khan" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Mohammad_Khan106" class="display-name">Mohammad Emtiyaz Khan</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Ecole_Polytechnique_Federale_de_Lausanne" title="École Polytechnique Fédérale de Lausanne">École Polytechnique Fédérale de Lausanne</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56ab1ddd160c2"> <a href="researcher/82243933_Shakir_Mohamed" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Shakir Mohamed" alt="Shakir Mohamed" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Shakir Mohamed</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56ab1ddd160c2">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/82243933_Shakir_Mohamed"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Shakir Mohamed" alt="Shakir Mohamed" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/82243933_Shakir_Mohamed" class="display-name">Shakir Mohamed</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw13_56ab1ddd160c2"> <a href="researcher/2055470381_Kevin_P_Murphy" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Kevin P Murphy" alt="Kevin P Murphy" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Kevin P Murphy</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw14_56ab1ddd160c2">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2055470381_Kevin_P_Murphy"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Kevin P Murphy" alt="Kevin P Murphy" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2055470381_Kevin_P_Murphy" class="display-name">Kevin P Murphy</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/1049-5258_Advances_in_neural_information_processing_systems"><span itemprop="name">Advances in neural information processing systems</span></a> </span>        <meta itemprop="datePublished" content="2012-01">  01/2012;  4.             </div> <div id="rgw15_56ab1ddd160c2" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>We present a new variational inference algorithm for Gaussian process regres-sion with non-conjugate likelihood functions, with application to a wide array of problems including binary and multi-class classification, and ordinal regression. Our method constructs a concave lower bound that is optimized using an efficient fixed-point updating algorithm. We show that the new algorithm has highly com-petitive computational complexity, matching that of alternative approximate infer-ence methods. We also prove that the use of concave variational bounds provides stable and guaranteed convergence &ndash; a property not available to other approaches. We show empirically for both binary and multi-class classification that our new algorithm converges much faster than existing variational methods, and without any degradation in performance.</div> </p>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw28_56ab1ddd160c2">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw27_56ab1ddd160c2"  itemprop="articleBody">  <p>Page 1</p> <p>Fast Bayesian Inference for Non-Conjugate<br />Gaussian Process Regression<br />Mohammad Emtiyaz Khan, Shakir Mohamed, and Kevin P. Murphy<br />Department of Computer Science, University of British Columbia<br />Abstract<br />We present a new variational inference algorithm for Gaussian process regres-<br />sion with non-conjugate likelihood functions, with application to a wide array of<br />problems including binary and multi-class classification, and ordinal regression.<br />Our method constructs a concave lower bound that is optimized using an efficient<br />fixed-point updating algorithm. We show that the new algorithm has highly com-<br />petitive computational complexity, matching that of alternative approximate infer-<br />ence methods. We also prove that the use of concave variational bounds provides<br />stable and guaranteed convergence – a property not available to other approaches.<br />We show empirically for both binary and multi-class classification that our new<br />algorithm converges much faster than existing variational methods, and without<br />any degradation in performance.<br />1Introduction<br />Gaussian processes (GP) are a popular non-parametric prior for function estimation. For real-valued<br />outputs, we can combine the GP prior with a Gaussian likelihood and perform exact posterior in-<br />ference in closed form. However, in other cases, such as classification, the likelihood is no longer<br />conjugate to the GP prior, and exact inference is no longer tractable.<br />Various approaches are available to deal with this intractability. One approach is Markov Chain<br />Monte Carlo (MCMC) techniques [1, 11, 22, 9]. Although this can be accurate, it is often quite<br />slow, and assessing convergence is challenging. There is therefore great interest in deterministic ap-<br />proximate inference methods. One recent approach is the Integrated Nested Laplace Approximation<br />(INLA) [21], which uses numerical integration to approximate the marginal likelihood. Unfortu-<br />nately, this method is limited to six or fewer hyperparameters, and is thus not suitable for models<br />with a large number of hyperparameters. Expectation propagation (EP) [17] is a popular alterna-<br />tive, and is a method that approximates the posterior distribution by maintaining expectations and<br />iterating until these expectations are consistent for all variables. Although this is fast and accurate<br />for the case of binary classification [15, 18], there are difficulties extending EP to many other cases,<br />such as multi-class classification and parameter learning [24, 13]. In addition, EP is known to have<br />convergence issues and can be numerically unstable.<br />In this paper, we use a variational approach, where we compute a lower bound to the log marginal<br />likelihood using Jensen’s inequality. Unlike EP, this approach does not suffer from numerical issues<br />and convergence problems, and can easily handle multi-class and other likelihoods. This is an active<br />area of research and many solutions have been proposed, see for example, [23, 6, 5, 19, 14]. Un-<br />fortunately, most of these methods are slow, since they attempt to solve for the posterior covariance<br />matrix, which has size O(N2), where N is the number of data points. In [19], a reparameteriza-<br />tion was proposed that only requires computing O(N) variational parameters. Unfortunately, this<br />method relies on a non-concave lower bound. In this paper, we propose a new lower bound that is<br />concave, and derive an efficient iterative algorithm for its maximization. Since the original objective<br />is unimodal, we reach the same global optimum as the other methods, but we do so much faster.<br />1</p>  <p>Page 2</p> <p>p(z|X,θ) = N(z|µ,Σ)<br />(1)<br />p(y|z) =<br />N<br />?<br />n=1<br />p(yn|zn)<br />(2)<br />Type<br />Binary<br />Categorical<br />Ordinal<br />Count<br />Distribution<br />Bernoulli logit<br />Multinomial logit<br />Cumulative logit<br />Poisson<br />p(y|z)<br />p(y = 1|z) = σ(z)<br />p(y = k|z) = ezk−lse(z)<br />p(y ≤ k|z) = σ(φk− z)<br />p(y = k|z) =e−ezekz<br />k!<br />z2 <br />y2 <br />X <br />Σ <br />µ <br />θ <br />z1 <br />y1 <br />zN <br />yN <br />Table 1: Gaussian process regression (top left) and its graphical model (right), along with the exam-<br />ple likelihoods for outputs (bottom left). Here, σ(z) = 1/(1 + e−z), lse(·) is the log-sum-exp func-<br />tion, k indexes over discrete output values, and φkare real numbers such that φ1&lt; φ2&lt; ... &lt; φK<br />for K ordered categories.<br />2 Gaussian Process Regression<br />Gaussianprocess(GP)regressionisapowerfulmethodfornon-parametricregressionthathasgained<br />a great deal of attention as a flexible and accurate modeling approach. Consider N data points with<br />the n’th observation denoted by yn, with corresponding features xn. A Gaussian process model uses<br />a non-linear latent function z(x) to obtain the distribution of the observation y using an appropriate<br />likelihood [15, 18]. For example, when y is binary, a Bernoulli logit/probit likelihood is appropriate.<br />Similarly, for count observations, a Poisson distribution can be used.<br />A Gaussian process [20] specifies a distribution over z(x), and is a stochastic process that is char-<br />acterized by a mean function µ(x) and a covariance function Σ(x,x?), which are specified using a<br />kernel function that depends on the observed features x. Assuming a GP prior over z(x) implies that<br />a random vector is associated with every input x, such that given all inputs X = [x1,x2,...,xN],<br />the joint distribution over z = [z(x1),z(x2),...,z(xN)] is Gaussian.<br />The GP prior is shown in Eq. 1. Here, µ is a vector with µ(xi) as its i’th element, Σ is a matrix with<br />Σ(xi,xj) as the (i,j)’th entry, and θ are the hyperparameters of the mean and covariance functions.<br />We assume throughout a zero mean-function and a squared-exponential covariance function (also<br />known as radial-basis function or Gaussian) defined as: Σ(xi,xj) = σ2exp[−(xi− xj)T(xi−<br />xj)/(2s)]. The set of hyperparameters is θ = (s,σ). We also define Ω = Σ−1.<br />Given the GP prior, the observations are modeled using the likelihood shown in Eq. 2. The exact<br />form of the distribution p(yn|zn) depends on the type of observations and different choices instan-<br />tiates many existing models for GP regression [15, 18, 10, 14]. We consider frequently encountered<br />data such as binary, ordinal, categorical and count observations, and describe their likelihoods in Ta-<br />ble 1. For the case of categorical observations, the latent function z is a vector whose k’th element<br />is the latent function for k’th category. A graphical model for Gaussian process regression is also<br />shown.<br />Given these models, there are three tasks that are to be performed: posterior inference, prediction<br />at test inputs, and model selection. In all cases, the likelihoods we consider are not conjugate to<br />the Gaussian prior distribution and as a result, the posterior distribution is intractable. Similarly,<br />the integrations required in computing the predictive distribution and the marginal likelihood are<br />intractable. To deal with this intractability we make use of variational methods.<br />3Variational Lower Bound to the Log Marginal Likelihood<br />Inference and model selection are always problematic in any Gaussian process regression using non-<br />conjugate likelihoods due to the fact that the marginal likelihood contains an intractable integral. In<br />this section, we derive a tractable variational lower bound to the marginal likelihood. We show<br />2</p>  <p>Page 3</p> <p>that the lower bound takes a well known form and can be maximized using concave optimization.<br />Throughout the section, we assume scalar zn, with extension to the vector case being straightfor-<br />ward.<br />We begin with the intractable log marginal likelihood L(θ) in Eq. 3 and introduce a variational<br />posterior distribution q(z|γ). We use a Gaussian posterior with mean m and covariance V. The<br />full set of variational parameters is thus γ = {m,V}. As log is a concave function, we obtain a<br />lower bound LJ(θ,γ) using Jensen’s inequality, given in Eq. 4. The first integral is simply the<br />Kullback−Leibler (KL) divergence from the variational Gaussian posterior q(z|m,V) to the GP<br />prior p(z|µ,Σ) as shown in Eq. 5, and has a closed-form expression that we substitute to get the<br />first term in Eq. 6 (inside square brackets), with Ω = Σ−1.<br />The second integral can be expressed in terms of the expectation with respect to the marginal<br />q(zn|mn,Vnn) as shown in the second term of Eq. 5. Here mnis the n’th element of m and<br />Vnnis the n’th diagonal element of V, the two variables collectively denoted by γn. The lower<br />bound LJis still intractable since the expectation of logp(yn|zn) is not available in closed form for<br />the distributions listed in Table 1. To derive a tractable lower bound, we make use of local variational<br />bounds (LVB) fb, defined such that E[logp(yn|zn)] ≥ fb(yn,mn,Vnn), giving us Eq. 6.<br />?<br />z<br />≥ LJ(θ,γ) := −<br />z<br />N<br />?<br />?log|VΩ|−tr(VΩ) −(m−µ)TΩ(m−µ)+N?+<br />We discuss the choice of LVBs in the next section, but first discuss the well-known form that the<br />lower bound of Eq. 6 takes. Given V, the optimization function with respect to m is a nonlinear<br />least-squares function. Similarly, the function with respect to V is similar to the graphical lasso<br />[8] or covariance selection problem [7], but is different in that the argument is a covariance matrix<br />instead of a precision matrix [8]. These two objective functions are coupled through the non-linear<br />term fb(·). Usually this term arises due to the prior distribution and may be non-smooth, for exam-<br />ple, in graphical lasso. In our case, this term arises from the likelihood, and is smooth and concave<br />as we discuss in next section.<br />It is straightforward to show that the variational lower bound is strictly concave with respect to<br />γ if fbis jointly concave with respect to mnand Vnn. Strict concavity of terms other than fbis<br />well-known since both the least squares and covariance selection problems are concave. Similar<br />concavity results have been discussed by Braun and McAuliffe [5] for the discrete choice model,<br />and more recently by Challis and Barber [6] for the Bayesian linear model, who consider concavity<br />with respect to the Cholesky factor of V. We consider concavity with respect to V instead of its<br />Cholesky factor, which allows us to exploit the special structure of V, as explained in Section 5.<br />L(θ) = logp(z|θ)p(y|z)dz = log<br />q(z|γ)logq(z|γ)<br />?<br />?<br />z<br />q(z|γ)p(z|θ)p(y|z)<br />q(z|γ)<br />dz<br />(3)<br />?<br />p(z|θ)dz +<br />z<br />q(z|γ)logp(y|z)dz<br />(4)<br />=−DKL[q(z|γ)||p(z|θ)]+<br />n=1<br />Eq(zn|γn)[logp(yn|zn)]<br />(5)<br />≥ LJ(θ,γ):=1<br />2<br />N<br />?<br />n=1<br />fb(yn,mn,Vnn).<br />(6)<br />4Concave Local Variational Bounds<br />In this section, we describe concave LVBs for various likelihoods. For simplicity, we suppress<br />the dependence on n and consider the log-likelihood of a scalar observation y given a predictor z<br />distributed according to q(z|γ) = N(z|m,v) with γ = {m,v}. We describe the LVBs for the<br />likelihoods given in Table 1 with z being a scalar for count, binary, and ordinal data, but a vector of<br />length K for categorical data, K being the number of classes. When V is a matrix, we denote its<br />diagonal by v.<br />For the Poison distribution, the expectation is available in closed form and we do not need any<br />bounding: E[logp(y|η)] = ym − exp(m + v/2) − logy!. This function is jointly concave with<br />respect to m and v since the exponential is a convex function.<br />3</p>  <p>Page 4</p> <p>For binary data, we use the piecewise linear/quadratic bounds proposed by [16], which is a bound<br />on the logistic-log-partition (LLP) function log(1+exp(x)) and can be used to obtain a bound over<br />the sigmoid function σ(x). The final bound can be expressed as sum of R pieces: E(logp(y|η)) =<br />fb(y,m,v) = ym −?R<br />An important property of the piecewise bound is that its maximum error is bounded and can be<br />driven to zero by increasing the number of pieces. This means that the lower bound in Eq. 6 can<br />be made arbitrarily tight by increasing the number of pieces. For this reason, this bound always<br />performs better than other existing bounds, such as Jaakola’s bound [12], given that the number<br />of pieces is chosen appropriately. Finally, the cumulative logit likeilhood for ordinal observations<br />depends on σ(x) and its expectation can be bounded using piecewise bounds in a similar way.<br />For the multinomial logit distribution, we can use the bounds proposed by [3] and [4], both leading<br />to concave LVBs. The first bound takes the form fb(y,m,V) = yTm − lse(m + v/2) with y<br />represented using a 1-of-K encoding. This function is jointly concave with respect to m and v,<br />which can be shown by noting the fact that the log-sum-exp function is convex. The second bound<br />is the product of sigmoids bound proposed by [4] which bounds the likelihood with product of<br />sigmoids (see Eq. 3 in [4]), with each sigmoid bounded using Jaakkola’s bound [12]. We can also<br />use piecewise linear/quadratic bound to bound each sigmoid. Alternatively, we can use the recently<br />proposed stick-breaking likelihood of [14] which uses piecewise bounds as well.<br />Finally, note that the original log-likelihood may not be concave itself, but if it is such that LJhas<br />a unique solution, then designing a concave variational lower bound will allow us to use concave<br />optimization to efficiently maximize the lower bound.<br />r=1fbr(m,v) where fbris the expectation of r’th quadratic piece. The<br />function fbris jointly concave with respect to m,v and their gradients are available in closed-form.<br />5Existing Algorithms for Variational Inference<br />In this section, we assume that for each output ynthere is a corresponding scalar latent function zn.<br />All our results can be easily extended to the case of multi-class outputs where the latent function is a<br />vector. In variational inference, we find the approximate Gaussian posterior distribution with mean<br />m and covariance V that maximizes Eq. 6. The simplest approach is to use gradient-based methods<br />for optimization, but this can be problematic since the number of variational parameters is quadratic<br />in N due to the covariance matrix V. The authors of [19] speculate that this may perhaps be the<br />reason behind limited use of Gaussian variational approximations.<br />We now show that the problem is simpler than it appears to be, and in fact the number of parameters<br />can be reduced to O(N) from O(N2). First, we write the gradients with respect to m and v in Eq.<br />7 and 8 and equate to zero, using gm<br />Also, gmandgvarethevectorsofthesegradients, anddiag(gv) isthematrixwithgvasitsdiagonal.<br />−Ω(m − µ) + gm= 0<br />1<br />2<br />At the solution, we see that V is completely specified if gvis known. This property can be exploited<br />to reduce the number of variational parameters.<br />Opper and Archambeau [19] (and [18]) propose a reparameterization to reduce the number of pa-<br />rameters to O(N). From the fixed-point equation, we note that at the solution m and V will have<br />the following form,<br />V = (Σ−1+ diag(λ))−1<br />m = µ + Σα,<br />where α and λ are real vectors with λd&gt; 0,∀d. At the maximum (but not everywhere), α and λ<br />will be equal to gmand gvrespectively. Therefore, instead of solving the fixed-point equations to<br />obtain m and V, we can reparameterize the lower bound with respect to α and λ. Substituting Eq.<br />9 and 10 in Eq. 6 and after simplification using the matrix inversion and determinant lemmas, we<br />get the following new objective function (for a detailed derivation, see [18]),<br />n:= ∂fb(yn,mn,vn)/∂mnand gv<br />n:= ∂fb(yn,mn,vn)/∂vn.<br />(7)<br />(8)<br />?V−1− Ω?+ diag(gv) = 0<br />(9)<br />(10)<br />1<br />2<br />?−log(|Bλ||diag(λ)|) + Tr(B−1<br />λΣ) − αTΣα?+<br />N<br />?<br />n=1<br />fb(yn,mn,Vnn),<br />(11)<br />4</p>  <p>Page 5</p> <p>with Bλ= diag(λ)−1+ Σ. Since the mapping between {α,λ} and {m,V} is one-to-one, we can<br />recover the latter given the former. The one-to-one relationship also implies that the new objective<br />function has a unique maximum. The new lower bound involves vectors of size N, reducing the<br />number of variational parameters to O(N).<br />The problem with this reparameterization is that the new lower bound is no longer concave, even<br />though it has a unique maximum. To see this, consider the 1-D case. We collect all the terms<br />involving V from Eq. 6, except the LVB term, to define the function f(V ) = [log(V Σ−1) −<br />V Σ−1]/2. We substitute the reparameterization V = (Σ−1+ λ)−1to get a new function f(λ) =<br />[−log(1 + Σλ) − (1 + Σλ)−1]/2. The second derivative of this function is f??(λ) =<br />Σλ)]2(Σλ−1). Clearly, this derivative is negative for λ &lt; 1/Σ and non-negative otherwise, making<br />the function neither concave nor convex.<br />The objective function is still unimodal and the maximum of (11) is equal to the maximum of<br />(6). With the reparameterization, we loose concavity and therefore the algorithm may have slow<br />convergence. Our experimental results (Section 7) confirm the slow convergence.<br />1<br />2[Σ/(1 +<br />6 Fast Convergent Variational Inference using Coordinate Ascent<br />Wenowderiveanalgorithmthatreducesthenumberofvariationalparametersto2N whilemaintain-<br />ing concavity. Our algorithm uses simple scalar fixed-point updates to obtain the diagonal elements<br />of V. The complete algorithm is shown in Algorithm 1.<br />To derive the algorithm, we first note that the fixed-point equation Eq. 8 has an attractive property:<br />at the solution, the off-diagonal elements of V−1are the same as the off-diagonal elements of Ω,<br />i.e. if we denote K := V−1, then Kij= Ωij. We need only find the diagonal elements of K to get<br />the full V. This is difficult, however, since the gradient gvdepends on v.<br />We take the approach of optimizing each diagonal element Kiifixing all others (and fixing m as<br />well). We partition V as shown on the left side of Eq. 12, indexing the last row by 2 and rest of the<br />rows by 1. We consider a similar partitioning of K and Ω. Our goal is to compute v22and k22given<br />all other elements of K. Matrices K and V are related through the blockwise inversion, as shown<br />below.<br /><br /><br />From the right bottom corner, we have the first relation below, which we simplify further.<br />?<br />V11<br />vT<br />12<br />v12<br />v22<br />?<br />=<br /><br />K−1<br />11+K−1<br />11k12kT<br />k22−kT<br />kT<br />k22−kT<br />12K−1<br />11k12<br />11<br />12K−1<br />−<br />K−1<br />12K−1<br />1<br />12K−1<br />11k12<br />k22−kT<br />11k12<br />−<br />12K−1<br />12K−1<br />11<br />11k12<br />k22−kT<br />11k12<br /><br /><br /><br />(12)<br />v22= 1/(k22− kT<br />12K−1<br />12K−1<br />11k12)<br />⇒<br />k22=?k22+ 1/v22<br />(13)<br />where we define?k22:= kT<br />the value of k22from Eq. 13 in Eq. 14 to get Eq. 15. It is easy to check (by taking derivative) that<br />the value v22that satisfies this fixed-point can be found by maximizing the function defined in Eq.<br />16.<br />11k12. We also know from the fixed point Eq. 8 that the optimal v22<br />and k22satisfy Eq. 14 at the solution, where gv<br />22is the gradient of fbwith respect to v22. Substitute<br />0 = k22− Ω22+ 2gv<br />0 =?k22+ 1/v22− Ω22+ 2gv<br />22<br />(14)<br />22<br />(15)<br />f(v) = log(v) − (Ω22−?k22)v + 2fb(y2,m22,v)<br />update: v22← 1/(Ω22−?k22− 2gv<br />evaluated at every fixed-point iteration. In fact, we do not need to compute it explicitly, since we<br />can obtain its value using Eq. 13:?k22= k22− 1/v22, and we do this before starting a fixed-point<br />which is usually constant and very low.<br />(16)<br />The function f(v) is a strictly concave function and can be optimized by iterating the following<br />22). We will refer to this as a “fixed-point iteration”.<br />Since all elements of K, except k22, are fixed,?k22can be computed beforehand and need not be<br />iteration. The complexity of these iterations depends on the number of gradient evaluations gv<br />22,<br />5</p>  <p>Page 6</p> <p>After convergence of the fixed-point iterations, we update V using Eq. 12. It turns out that this is a<br />rank-one update, the complexity of which is O(N2). To show these updates, let us denote the new<br />values obtained after the fixed-point iterations by knew<br />values by kold<br />Eq. 13, we get the second equality. Similarly, we use the top left corner of Eq. 12 to get the first<br />equality in Eq. 18, and use Eq. 13 and 17 to get the second equality.<br />22<br />and vnew<br />22<br />respectively. and denote the old<br />22and vold<br />22. We use the right top corner of Eq. 12 to get first equality in Eq. 17. Using<br />K−1<br />11k12= −(kold<br />K−1<br />22−?k22)vold<br />12= −vold<br />11k12kT<br />kold<br />12/vold<br />22<br />(17)<br />11= Vold<br />11−K−1<br />12K−1<br />11<br />22−?k22<br />= Vold<br />11− vold<br />12(vold<br />12)T/vold<br />22<br />(18)<br />Note that both K−1<br />Vnew. We use Eq. 12 to write updates for Vnewand use 17, 18, and 13 to simplify.<br />11and k12do not change after the fixed point iteration. We use this fact to obtain<br />vnew<br />12<br />=<br />K−1<br />11k12<br />knew<br />22<br />11+K−1<br />−?k22<br />= −vnew<br />22<br />vold<br />22<br />12K−1<br />−?k22<br />vold<br />12<br />(19)<br />Vnew<br />11<br />= K−1<br />11k12kT<br />knew<br />22<br />11<br />= Vold<br />11+vnew<br />22<br />− vold<br />(vold<br />22<br />22)2<br />vold<br />12(vold<br />12)T<br />(20)<br />After updating V, we update m by optimizing the following non-linear least squares problem,<br />max<br />m−1<br />2(m − µ)TΩ(m − µ) +<br />N<br />?<br />n=1<br />fb(yn,mn,Vnn)<br />(21)<br />We use Newton’s method, the cost of which is O(N3).<br />6.1Computational complexity<br />The final procedure is shown in Algorithm 1. The main advantage of our algorithm is its fast<br />convergence as we show this in the results section.<br />O(N3+?<br />due to the summation. In all our experiments, Ifp<br />The overall computational complexity is<br />nIfp<br />n). First term is due to O(N2) update of V for all n and also due to the opti-<br />mization of m. Second term is for Ifp<br />n is usually 3 to 5, adding very little cost.<br />n fixed-point iterations, the total cost of which is linear in N<br />6.2Proof of convergence<br />Proposition 2.7.1 in [2] states that the coordinate ascent algorithm converges if the maximization<br />with respect to each coordinate is uniquely attained. This is indeed the case for us since each fixed<br />point iteration solves a concave problem of the form given by Eq. 16. Similarly, optimization with<br />respect to m is also strictly concave. Hence, convergence of our algorithm is assured.<br />6.3Proof that V will always be positive definite<br />Let us assume that we start with a positive definite K, for example, we can initialize it with Ω. Now<br />consider the update of v22and k22. Note that vnew<br />22<br />16 which involves the log term. Using this and Eq. 13, we get knew<br />Schur complement knew<br />22<br />− kT<br />follows that Knewwill also be positive definite, and hence Vnewwill be positive definite.<br />will be positive since it is the maximum of Eq.<br />&gt; kT<br />11k12&gt; 0. Using this and the fact that K11is positive definite, it<br />22<br />12K−1<br />11k12. Hence, the<br />12K−1<br />7 Results<br />We now show that the proposed algorithm leads to a significant gain in the speed of Gaussian process<br />regression. The software to reproduce the results of this section are available online1. We evaluate<br />the performance of our fast variational inference algorithm against existing inference methods for<br />1http://www.cs.ubc.ca/emtiyaz/software/codeNIPS2012.html<br />6</p>  <p>Page 7</p> <p>Algorithm 1 Fast convergent coordinate-ascent algorithm<br />1. Initialize K ← Ω,V ← Ω−1,m ← µ, where Ω := Σ−1.<br />2. Alternate between updating the diagonal of V and then m until convergence, as follows:<br />(a) Update the i’th diagonal of V for all i = 1,...,N:<br />i. Rearrange V and Ω so that the i’th column is the last one.<br />ii.?k22← k22− 1/v22.<br />iv. Run fixed-point iterations for a few steps: v22← 1/(Ω22−?k22− 2gv<br />A. V11← V11+ (v22− vold<br />B. v12← −v22v12/vold<br />vi. Update k22←?k22+ 1/v22.<br />iii. Store old value vold<br />22← v22.<br />22).<br />v. Update V.<br />22)v12vT<br />12/(vold<br />22)2.<br />22.<br />(b) Update m by maximizing the least-squares problem of Eq. 21.<br />binary and multi-class classification. For binary classification, we use the UCI ionosphere data (with<br />351 data examples containing 34 features). For multi-class classification, we use the UCI forensic<br />glass data set with 214 data examples each with 6 category output and features of length 8. In both<br />cases, we use 80% of the dataset for training and the rest for testing.<br />We consider GP classification using the Bernoulli logit likelihood, for which we use the piecewise<br />bound of [16] with 20 pieces. We compare our algorithm with the approach of Opper and Archam-<br />beau [19] (Eq. 11). For the latter, we use L-BFGS method for optimization. We also compared to<br />the naive method of optimizing with respect to full m and V, e.g. method of [5], but do not present<br />these results since these algorithms have very slow convergence.<br />We examine the computational cost for each method in terms of the number of floating point oper-<br />ations (flops) for four hyperparameter settings θ = {log(s),log(σ)}. This comparison is shown in<br />Figure 1(a). The y-axis shows (negative of) the value of the lower bound, and the x-axis shows the<br />number of flops. We draw markers at iteration 1,2,4,50 and in steps of 50 from then on. In all cases,<br />due to non-concavity, the optimization of the Opper and Archambeau reparameterization (black<br />curve with squares) convergence slowly, passing through flat regions of the objective and requiring<br />a large number of computations to reach convergence. The proposed algorithm (blue curve with<br />circles) has consistently faster convergence than the existing method. For this dataset, our algorithm<br />always converged in 5 iterations.<br />We also compare the total cost to convergence, where we count the total number of flops until<br />successive increase in the objective function is below 10−3. Each entry is a different setting of<br />{log(s),log(σ)}. Rows correspond to values of log(s) while columns correspond to log(σ), with<br />units M,G,T denoting Mega-, Giga-, and Terra-flops. We can see that the proposed algorithm takes<br />a much smaller number of operations compared to the existing algorithm.<br />Proposed Algorithm<br />-1<br />-16M<br />1 26M<br />3 47M<br />13<br />7M<br />20M<br />81M<br />7M<br />22M<br />75M<br />Opper and Archambeau<br />-1<br />-120G<br />1 101G<br />3 38G<br />13<br />212G<br />24T<br />1T<br />6T<br />24T<br />24T<br />We also applied our method to two more datasets of [18], namely ’sonar’ and ’usps-3vs5’ dataset<br />and observed similar behavior.<br />Next, we apply our algorithm to the problem of multi-class classification, following [14], using the<br />stick-breaking likelihood, and compare to inference using the approach of Opper and Archambeau<br />[19] (Eq. 11). We show results comparing the lower bound vs the number of flops taken in Figure<br />1(b), for four hyperparameter settings {log(s),log(σ)}. We show markers at iterations 1, 2, 10,<br />100 and every 100th iteration thereafter. The results follow those discussed for binary classification,<br />7</p>  <p>Page 8</p> <p>0 300<br />Mega−Flops<br />600 900<br />134<br />138<br />142<br />(−1.0,−1.0)<br />neg−LogLik<br />01000<br />Mega−Flops<br />2000 3000<br />300<br />600<br />900<br />(−1.0,2.5)<br />neg−LogLik<br />0 5K 10K 15K20K<br />80<br />110<br />140<br />170<br />200<br />(3.5,3.5)<br />Mega−Flops<br />neg−LogLik<br />0 20004000 6000 8000<br />100<br />200<br />300<br />Mega−Flops<br />neg−LogLik<br />(1.0,1.0)<br /> <br /> <br />Opper−Arch<br />proposed<br />(a) Ionosphere data<br />0 1000 20003000 4000<br />260<br />270<br />280<br />290<br />300<br />310<br />320<br />(−1.0, −1.0)<br />Neg−LogLik<br />Mega−flops<br />0 10K20K30K 40K50K<br />500<br />1000<br />1500<br />2000<br />(−1.0, 2.5)<br />Neg−LogLik<br />Mega−flops<br />0 20K40K60K80K 100K<br />200<br />250<br />300<br />350<br />400<br />(2.5, 2.5)<br />Neg−LogLik<br />Mega−flops<br />010K20K 30K 40K 50K<br />200<br />300<br />400<br />500<br />600<br />(1.0, 1.0)<br />Neg−LogLik<br />Mega−flops<br /> <br /> <br />proposed<br />Opper−Arch<br />(b) Forensic glass data<br />Figure 1: Convergence results for (a) the binary classification on the ionosphere data set and (b) the<br />multi-class classification on the glass dataset. We plot the negative of the lower bound vs the number<br />of flops. Each plot shows the progress of algorithms for a hyperparameter setting {log(s),log(σ)}<br />shown at the top of the plot. The proposed algorithm always converges faster than the other method,<br />in fact, in less than 5 iterations.<br />where both methods reach the same lower bound value, but the existing approach converging much<br />slower, with our algorithm always converged within 20 iterations.<br />8 Discussion<br />In this paper we have presented a new variational inference algorithm for non-conjugate GP re-<br />gression. We derived a concave variational lower bound to the log marginal likelihood, and used<br />concavity to develop an efficient optimization algorithm. We demonstrated the efficacy of our new<br />algorithm on both binary and multiclass GP classification, demonstrating significant improvement<br />in convergence.<br />Our proposed algorithm is related to many existing methods for GP regression. For example, the<br />objective function that we consider is exactly the KL minimization method discussed in [18], for<br />which a gradient based optimization was used. Our algorithm uses an efficient approach where we<br />update the marginals of the posterior and then do a rank one update of the covariance matrix. Our<br />results show that this leads to fast convergence.<br />Our algorithm also takes a similar form to the popular EP algorithm [17], e.g. see Algorithm 3.5 in<br />[20]. Both EP and our algorithm update posterior marginals, followed by a rank-one update of<br />the covariance. Therefore, the computational complexity of our approach is similar to that of EP.<br />The advantage of our approach is that, unlike EP, it does not suffer from any numerical issues (for<br />example, no negative variances) and is guaranteed to converge.<br />The derivation of our algorithm is based on the observation that the posterior covariance has a special<br />structure, and does not directly use the concavity of the lower bound. An alternate derivation based<br />on the Fenchel duality exists and shows that the fixed-point iterations compute dual variables which<br />are related to the gradients of fb. We skip this derivation since it is tedious, and present the more<br />intuitive derivation instead. The alternative derivation will be made available in an online appendix.<br />Acknowledgements<br />We thank the reviewers for their valuable suggestions. SM is supported by the Canadian Institute<br />for Advanced Research (CIFAR).<br />8</p>  <p>Page 9</p> <p>References<br />[1] J. Albert and S. Chib. Bayesian analysis of binary and polychotomous response data. J. of the<br />Am. Stat. Assoc., 88(422):669–679, 1993.<br />[2] Dimitri P. Bertsekas. Nonlinear Programming. Athena Scientific, second edition, 1999.<br />[3] D. Blei and J. Lafferty. Correlated topic models. In Advances in Neural Information Proceed-<br />ings Systems, 2006.<br />[4] G. Bouchard. Efficient bounds for the softmax and applications to approximate inference in<br />hybrid models. In NIPS 2007 Workshop on Approximate Inference in Hybrid Models, 2007.<br />[5] M. Braun and J. McAuliffe. Variational inference for large-scale models of discrete choice.<br />Journal of the American Statistical Association, 105(489):324–335, 2010.<br />[6] E. Challis and D. Barber. Concave Gaussian variational approximations for inference in large-<br />scale Bayesian linear models. In Proceedings of the International Conference on Artificial<br />Intelligence and Statistics, volume 6, page 7, 2011.<br />[7] A. Dempster. Covariance selection. Biometrics, 28(1), 1972.<br />[8] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graph-<br />ical lasso. Biostatistics, 9(3):432, 2008.<br />[9] S. Fr¨ uhwirth-Schnatter and R. Fr¨ uhwirth. Data augmentation and MCMC for binary and multi-<br />nomial logit models. Statistical Modelling and Regression Structures, pages 111–132, 2010.<br />[10] M. Girolami and S. Rogers. Variational Bayesian multinomial probit regression with Gaussian<br />process priors. Neural Comptuation, 18(8):1790 – 1817, 2006.<br />[11] C. Holmes and L. Held. Bayesian auxiliary variable models for binary and multinomial regres-<br />sion. Bayesian Analysis, 1(1):145–168, 2006.<br />[12] T. Jaakkola and M. Jordan. A variational approach to Bayesian logistic regression problems<br />and their extensions. In AI + Statistics, 1996.<br />[13] P. Jyl¨ anki, J. Vanhatalo, and A. Vehtari. Robust Gaussian process regression with a student-t<br />likelihood. The Journal of Machine Learning Research, 999888:3227–3257, 2011.<br />[14] M. Khan, S. Mohamed, B. Marlin, and K. Murphy. A stick-breaking likelihood for categorical<br />data analysis with latent Gaussian models. In Proceedings of the International Conference on<br />Artificial Intelligence and Statistics, 2012.<br />[15] M. Kuss and C. E. Rasmussen. Assessing approximate inference for binary Gaussian process<br />classification. J. of Machine Learning Research, 6:1679–1704, 2005.<br />[16] B. Marlin, M. Khan, and K. Murphy. Piecewise bounds for estimating Bernoulli-logistic latent<br />Gaussian models. In Intl. Conf. on Machine Learning, 2011.<br />[17] T. Minka. Expectation propagation for approximate Bayesian inference. In UAI, 2001.<br />[18] H. Nickisch and C.E. Rasmussen. Approximations for binary Gaussian process classification.<br />Journal of Machine Learning Research, 9(10), 2008.<br />[19] M. Opper and C. Archambeau. The variational Gaussian approximation revisited. Neural<br />computation, 21(3):786–792, 2009.<br />[20] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press,<br />2006.<br />[21] H. Rue, S. Martino, and N. Chopin. Approximate Bayesian inference for latent Gaussian<br />models using integrated nested Laplace approximations. J. of Royal Stat. Soc. Series B, 71:<br />319–392, 2009.<br />[22] S. L. Scott. Data augmentation, frequentist estimation, and the Bayesian analysis of multino-<br />mial logit models. Statistical Papers, 52(1):87–109, 2011.<br />[23] M. Seeger. Bayesian Inference and Optimal Design in the Sparse Linear Model. J. of Machine<br />Learning Research, 9:759–813, 2008.<br />[24] M. Seeger and H. Nickisch. Fast Convergent Algorithms for Expectation Propagation Ap-<br />proximate Bayesian Inference. In Proceedings of the International Conference on Artificial<br />Intelligence and Statistics, 2011.<br />9</p>   </div> <div id="rgw20_56ab1ddd160c2" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw21_56ab1ddd160c2">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw22_56ab1ddd160c2"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://books.nips.cc/papers/files/nips25/NIPS2012_1448.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression">Fast Bayesian Inference for Non-Conjugate Gaussian...</a> </div>  <div class="details">   Available from <a href="http://books.nips.cc/papers/files/nips25/NIPS2012_1448.pdf" target="_blank" rel="nofollow">books.nips.cc</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw29_56ab1ddd160c2" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (1) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw30_56ab1ddd160c2" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   "  id="rgw31_56ab1ddd160c2" >  <div class="indent-left">  <div id="rgw32_56ab1ddd160c2" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Maurizio_Filippone" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Maurizio Filippone </div> </div>   </div>  </div>  <div class="indent-right">      </div>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes"> <span class="publication-title js-publication-title">MCMC for Variationally Sparse Gaussian Processes</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/71410443_James_Hensman" class="authors js-author-name ga-publications-authors">James Hensman</a> &middot;     <a href="researcher/2048369253_Alexander_G_de_G_Matthews" class="authors js-author-name ga-publications-authors">Alexander G. de G. Matthews</a> &middot;     <a href="researcher/70871340_Maurizio_Filippone" class="authors js-author-name ga-publications-authors">Maurizio Filippone</a> &middot;     <a href="researcher/8159937_Zoubin_Ghahramani" class="authors js-author-name ga-publications-authors">Zoubin Ghahramani</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Gaussian process (GP) models form a core part of probabilistic machine
learning. Considerable research effort has been made into attacking three
issues with GP models: how to compute efficiently when the number of data is
large; how to approximate the posterior when the likelihood is not Gaussian and
how to estimate covariance function parameter posteriors. This paper
simultaneously addresses these, using a variational approximation to the
posterior which is sparse in support of the function but otherwise free-form.
The result is a Hybrid Monte-Carlo sampling scheme which allows for a
non-Gaussian approximation over the function values and covariance parameters
simultaneously, with efficient computations based on inducing-point sparse GPs.
Code to replicate each experiment in this paper will be available shortly. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Jun 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Maurizio_Filippone/publication/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes/links/558a857008aee1fc9174ea84.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw24_56ab1ddd160c2" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw25_56ab1ddd160c2">  </ul> </div> </div>   <div id="rgw16_56ab1ddd160c2" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw17_56ab1ddd160c2"> <div> <h5> <a href="publication/284136871_Markov_chain_Monte_Carlo_simulation_using_the_DREAM_software_package_Theory_concepts_and_MATLAB_implementation" class="color-inherit ga-similar-publication-title"><span class="publication-title">Markov chain Monte Carlo simulation using the DREAM software package: Theory, concepts, and MATLAB implementation</span></a>  </h5>  <div class="authors"> <a href="researcher/10295979_Jasper_A_Vrugt" class="authors ga-similar-publication-author">Jasper A. Vrugt</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56ab1ddd160c2"> <div> <h5> <a href="publication/282374013_Bayesian_Inference_in_Cumulative_Distribution_Fields" class="color-inherit ga-similar-publication-title"><span class="publication-title">Bayesian Inference in Cumulative Distribution Fields</span></a>  </h5>  <div class="authors"> <a href="researcher/2082018640_Ricardo_Silva" class="authors ga-similar-publication-author">Ricardo Silva</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56ab1ddd160c2"> <div> <h5> <a href="publication/283873425_On_the_use_of_kernel_approximate_Bayesian_computation_to_infer_population_history" class="color-inherit ga-similar-publication-title"><span class="publication-title">On the use of kernel approximate Bayesian computation to infer population history</span></a>  </h5>  <div class="authors"> <a href="researcher/36738803_Shigeki_Nakagome" class="authors ga-similar-publication-author">Shigeki Nakagome</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw34_56ab1ddd160c2" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw35_56ab1ddd160c2">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw36_56ab1ddd160c2" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=Wbmm4ZRTrZDwaTdnB4ERiUeMaK8LkeqDBrLNqYcAqkkbEVBfIIZSIJxiJihrapiS" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="D6dX0rwY3Br135u/7Z92dyRTNANiQ1mJYrVWfXJoMTyGZDM0A3J+9BvQ8RsRmP2psuJLJwglxQH8j8j68lhrlFBmfDivyBuZ5XvktID9AppmypP9STRYtWWI/dO3RsgxPFSMBS15s2kzZvZxZMUIL+2dpaxI5vxWcE7+tYD8iT7vlmy6giOzBXOIQDLCAIfusGC1vKQz1APdoC7sUPiW6FDgDi1p+ar0Zd+XUwGiuH6v7KzuiVKD6h5f9XVbJCaDQwr7VS2lj2kbR2a9/+pq/CusChoYSLGMrdCFE4fsGUE="/> <input type="hidden" name="urlAfterLogin" value="publication/266502061_Fast_Bayesian_Inference_for_Non-Conjugate_Gaussian_Process_Regression"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjY2NTAyMDYxX0Zhc3RfQmF5ZXNpYW5fSW5mZXJlbmNlX2Zvcl9Ob24tQ29uanVnYXRlX0dhdXNzaWFuX1Byb2Nlc3NfUmVncmVzc2lvbg%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjY2NTAyMDYxX0Zhc3RfQmF5ZXNpYW5fSW5mZXJlbmNlX2Zvcl9Ob24tQ29uanVnYXRlX0dhdXNzaWFuX1Byb2Nlc3NfUmVncmVzc2lvbg%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjY2NTAyMDYxX0Zhc3RfQmF5ZXNpYW5fSW5mZXJlbmNlX2Zvcl9Ob24tQ29uanVnYXRlX0dhdXNzaWFuX1Byb2Nlc3NfUmVncmVzc2lvbg%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw37_56ab1ddd160c2"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 1108;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Mohammad Emtiyaz Khan","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Mohammad_Khan106","institution":"\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne","institutionUrl":false,"widgetId":"rgw4_56ab1ddd160c2"},"id":"rgw4_56ab1ddd160c2","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=5885512","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab1ddd160c2"},"id":"rgw3_56ab1ddd160c2","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=266502061","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":266502061,"title":"Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression","journalTitle":"Advances in neural information processing systems","journalDetailsTooltip":{"data":{"journalTitle":"Advances in neural information processing systems","journalAbbrev":"Adv Neural Inform Process Syst","publisher":"IEEE Conference on Neural Information Processing Systems--Natural and Synthetic, Massachusetts Institute of Technology Press","issn":"1049-5258","impactFactor":"0.00","fiveYearImpactFactor":"0.00","citedHalfLife":"0.00","immediacyIndex":"0.00","eigenFactor":"0.00","articleInfluence":"0.00","widgetId":"rgw6_56ab1ddd160c2"},"id":"rgw6_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=1049-5258","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"01\/2012;","publicationDateRobot":"2012-01","article":"4.","journalTitle":"Advances in neural information processing systems","journalUrl":"journal\/1049-5258_Advances_in_neural_information_processing_systems"}},"source":false,"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression"},{"key":"rft.title","value":"Advances in Neural Information Processing Systems"},{"key":"rft.jtitle","value":"Advances in Neural Information Processing Systems"},{"key":"rft.volume","value":"4"},{"key":"rft.date","value":"2012"},{"key":"rft.issn","value":"1049-5258"},{"key":"rft.au","value":"Mohammad Emtiyaz Khan,Shakir Mohamed,Kevin P Murphy"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56ab1ddd160c2"},"id":"rgw7_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=266502061","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":266502061,"peopleItems":[{"data":{"authorNameOnPublication":"Mohammad Emtiyaz Khan","accountUrl":"profile\/Mohammad_Khan106","accountKey":"Mohammad_Khan106","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Mohammad Emtiyaz Khan","profile":{"professionalInstitution":{"professionalInstitutionName":"\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne","professionalInstitutionUrl":"institution\/Ecole_Polytechnique_Federale_de_Lausanne"}},"professionalInstitutionName":"\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne","professionalInstitutionUrl":"institution\/Ecole_Polytechnique_Federale_de_Lausanne","url":"profile\/Mohammad_Khan106","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Mohammad_Khan106","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw10_56ab1ddd160c2"},"id":"rgw10_56ab1ddd160c2","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=5885512&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":1,"publicationUid":266502061,"widgetId":"rgw9_56ab1ddd160c2"},"id":"rgw9_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=5885512&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=1&publicationUid=266502061","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/82243933_Shakir_Mohamed","authorNameOnPublication":"Shakir Mohamed","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Shakir Mohamed","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/82243933_Shakir_Mohamed","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56ab1ddd160c2"},"id":"rgw12_56ab1ddd160c2","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=82243933&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56ab1ddd160c2"},"id":"rgw11_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=82243933&authorNameOnPublication=Shakir%20Mohamed","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/2055470381_Kevin_P_Murphy","authorNameOnPublication":"Kevin P Murphy","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Kevin P Murphy","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2055470381_Kevin_P_Murphy","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw14_56ab1ddd160c2"},"id":"rgw14_56ab1ddd160c2","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2055470381&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw13_56ab1ddd160c2"},"id":"rgw13_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2055470381&authorNameOnPublication=Kevin%20P%20Murphy","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab1ddd160c2"},"id":"rgw8_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=266502061&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":266502061,"abstract":"<noscript><\/noscript><div>We present a new variational inference algorithm for Gaussian process regres-sion with non-conjugate likelihood functions, with application to a wide array of problems including binary and multi-class classification, and ordinal regression. Our method constructs a concave lower bound that is optimized using an efficient fixed-point updating algorithm. We show that the new algorithm has highly com-petitive computational complexity, matching that of alternative approximate infer-ence methods. We also prove that the use of concave variational bounds provides stable and guaranteed convergence &ndash; a property not available to other approaches. We show empirically for both binary and multi-class classification that our new algorithm converges much faster than existing variational methods, and without any degradation in performance.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw15_56ab1ddd160c2"},"id":"rgw15_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=266502061","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/266502061_Fast_Bayesian_Inference_for_Non-Conjugate_Gaussian_Process_Regression\/links\/54365a070cf2bf1f1f2b8032\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw5_56ab1ddd160c2"},"id":"rgw5_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=266502061&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":10295979,"url":"researcher\/10295979_Jasper_A_Vrugt","fullname":"Jasper A. Vrugt","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Environmental Modelling and Software","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/284136871_Markov_chain_Monte_Carlo_simulation_using_the_DREAM_software_package_Theory_concepts_and_MATLAB_implementation","usePlainButton":true,"publicationUid":284136871,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"4.42","url":"publication\/284136871_Markov_chain_Monte_Carlo_simulation_using_the_DREAM_software_package_Theory_concepts_and_MATLAB_implementation","title":"Markov chain Monte Carlo simulation using the DREAM software package: Theory, concepts, and MATLAB implementation","displayTitleAsLink":true,"authors":[{"id":10295979,"url":"researcher\/10295979_Jasper_A_Vrugt","fullname":"Jasper A. Vrugt","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Environmental Modelling and Software 01\/2016; 75:273-316. DOI:10.1016\/j.envsoft.2015.08.013"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/284136871_Markov_chain_Monte_Carlo_simulation_using_the_DREAM_software_package_Theory_concepts_and_MATLAB_implementation","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/284136871_Markov_chain_Monte_Carlo_simulation_using_the_DREAM_software_package_Theory_concepts_and_MATLAB_implementation\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab1ddd160c2"},"id":"rgw17_56ab1ddd160c2","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=284136871","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2082018640,"url":"researcher\/2082018640_Ricardo_Silva","fullname":"Ricardo Silva","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Nov 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/282374013_Bayesian_Inference_in_Cumulative_Distribution_Fields","usePlainButton":true,"publicationUid":282374013,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/282374013_Bayesian_Inference_in_Cumulative_Distribution_Fields","title":"Bayesian Inference in Cumulative Distribution Fields","displayTitleAsLink":true,"authors":[{"id":2082018640,"url":"researcher\/2082018640_Ricardo_Silva","fullname":"Ricardo Silva","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/282374013_Bayesian_Inference_in_Cumulative_Distribution_Fields","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/282374013_Bayesian_Inference_in_Cumulative_Distribution_Fields\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab1ddd160c2"},"id":"rgw18_56ab1ddd160c2","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=282374013","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":36738803,"url":"researcher\/36738803_Shigeki_Nakagome","fullname":"Shigeki Nakagome","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Oct 2015","journal":"Genes & Genetic Systems","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283873425_On_the_use_of_kernel_approximate_Bayesian_computation_to_infer_population_history","usePlainButton":true,"publicationUid":283873425,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"0.93","url":"publication\/283873425_On_the_use_of_kernel_approximate_Bayesian_computation_to_infer_population_history","title":"On the use of kernel approximate Bayesian computation to infer population history","displayTitleAsLink":true,"authors":[{"id":36738803,"url":"researcher\/36738803_Shigeki_Nakagome","fullname":"Shigeki Nakagome","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Genes & Genetic Systems 10\/2015; 90(3):153-162. DOI:10.1266\/ggs.90.153"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283873425_On_the_use_of_kernel_approximate_Bayesian_computation_to_infer_population_history","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283873425_On_the_use_of_kernel_approximate_Bayesian_computation_to_infer_population_history\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab1ddd160c2"},"id":"rgw19_56ab1ddd160c2","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=283873425","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw16_56ab1ddd160c2"},"id":"rgw16_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=266502061&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":266502061,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":266502061,"publicationType":"article","linkId":"54365a070cf2bf1f1f2b8032","fileName":"Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression","fileUrl":"http:\/\/books.nips.cc\/papers\/files\/nips25\/NIPS2012_1448.pdf","name":"books.nips.cc","nameUrl":"http:\/\/books.nips.cc\/papers\/files\/nips25\/NIPS2012_1448.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw22_56ab1ddd160c2"},"id":"rgw22_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=266502061&linkId=54365a070cf2bf1f1f2b8032&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw21_56ab1ddd160c2"},"id":"rgw21_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=266502061&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":0,"valueFormatted":"0","widgetId":"rgw23_56ab1ddd160c2"},"id":"rgw23_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=266502061","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw20_56ab1ddd160c2"},"id":"rgw20_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=266502061&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":266502061,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw25_56ab1ddd160c2"},"id":"rgw25_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=266502061&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":0,"valueFormatted":"0","widgetId":"rgw26_56ab1ddd160c2"},"id":"rgw26_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=266502061","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw24_56ab1ddd160c2"},"id":"rgw24_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=266502061&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Fast Bayesian Inference for Non-Conjugate\nGaussian Process Regression\nMohammad Emtiyaz Khan, Shakir Mohamed, and Kevin P. Murphy\nDepartment of Computer Science, University of British Columbia\nAbstract\nWe present a new variational inference algorithm for Gaussian process regres-\nsion with non-conjugate likelihood functions, with application to a wide array of\nproblems including binary and multi-class classification, and ordinal regression.\nOur method constructs a concave lower bound that is optimized using an efficient\nfixed-point updating algorithm. We show that the new algorithm has highly com-\npetitive computational complexity, matching that of alternative approximate infer-\nence methods. We also prove that the use of concave variational bounds provides\nstable and guaranteed convergence \u2013 a property not available to other approaches.\nWe show empirically for both binary and multi-class classification that our new\nalgorithm converges much faster than existing variational methods, and without\nany degradation in performance.\n1Introduction\nGaussian processes (GP) are a popular non-parametric prior for function estimation. For real-valued\noutputs, we can combine the GP prior with a Gaussian likelihood and perform exact posterior in-\nference in closed form. However, in other cases, such as classification, the likelihood is no longer\nconjugate to the GP prior, and exact inference is no longer tractable.\nVarious approaches are available to deal with this intractability. One approach is Markov Chain\nMonte Carlo (MCMC) techniques [1, 11, 22, 9]. Although this can be accurate, it is often quite\nslow, and assessing convergence is challenging. There is therefore great interest in deterministic ap-\nproximate inference methods. One recent approach is the Integrated Nested Laplace Approximation\n(INLA) [21], which uses numerical integration to approximate the marginal likelihood. Unfortu-\nnately, this method is limited to six or fewer hyperparameters, and is thus not suitable for models\nwith a large number of hyperparameters. Expectation propagation (EP) [17] is a popular alterna-\ntive, and is a method that approximates the posterior distribution by maintaining expectations and\niterating until these expectations are consistent for all variables. Although this is fast and accurate\nfor the case of binary classification [15, 18], there are difficulties extending EP to many other cases,\nsuch as multi-class classification and parameter learning [24, 13]. In addition, EP is known to have\nconvergence issues and can be numerically unstable.\nIn this paper, we use a variational approach, where we compute a lower bound to the log marginal\nlikelihood using Jensen\u2019s inequality. Unlike EP, this approach does not suffer from numerical issues\nand convergence problems, and can easily handle multi-class and other likelihoods. This is an active\narea of research and many solutions have been proposed, see for example, [23, 6, 5, 19, 14]. Un-\nfortunately, most of these methods are slow, since they attempt to solve for the posterior covariance\nmatrix, which has size O(N2), where N is the number of data points. In [19], a reparameteriza-\ntion was proposed that only requires computing O(N) variational parameters. Unfortunately, this\nmethod relies on a non-concave lower bound. In this paper, we propose a new lower bound that is\nconcave, and derive an efficient iterative algorithm for its maximization. Since the original objective\nis unimodal, we reach the same global optimum as the other methods, but we do so much faster.\n1"},{"page":2,"text":"p(z|X,\u03b8) = N(z|\u00b5,\u03a3)\n(1)\np(y|z) =\nN\n?\nn=1\np(yn|zn)\n(2)\nType\nBinary\nCategorical\nOrdinal\nCount\nDistribution\nBernoulli logit\nMultinomial logit\nCumulative logit\nPoisson\np(y|z)\np(y = 1|z) = \u03c3(z)\np(y = k|z) = ezk\u2212lse(z)\np(y \u2264 k|z) = \u03c3(\u03c6k\u2212 z)\np(y = k|z) =e\u2212ezekz\nk!\nz2 \ny2 \nX \n\u03a3 \n\u00b5 \n\u03b8 \nz1 \ny1 \nzN \nyN \nTable 1: Gaussian process regression (top left) and its graphical model (right), along with the exam-\nple likelihoods for outputs (bottom left). Here, \u03c3(z) = 1\/(1 + e\u2212z), lse(\u00b7) is the log-sum-exp func-\ntion, k indexes over discrete output values, and \u03c6kare real numbers such that \u03c61< \u03c62< ... < \u03c6K\nfor K ordered categories.\n2 Gaussian Process Regression\nGaussianprocess(GP)regressionisapowerfulmethodfornon-parametricregressionthathasgained\na great deal of attention as a flexible and accurate modeling approach. Consider N data points with\nthe n\u2019th observation denoted by yn, with corresponding features xn. A Gaussian process model uses\na non-linear latent function z(x) to obtain the distribution of the observation y using an appropriate\nlikelihood [15, 18]. For example, when y is binary, a Bernoulli logit\/probit likelihood is appropriate.\nSimilarly, for count observations, a Poisson distribution can be used.\nA Gaussian process [20] specifies a distribution over z(x), and is a stochastic process that is char-\nacterized by a mean function \u00b5(x) and a covariance function \u03a3(x,x?), which are specified using a\nkernel function that depends on the observed features x. Assuming a GP prior over z(x) implies that\na random vector is associated with every input x, such that given all inputs X = [x1,x2,...,xN],\nthe joint distribution over z = [z(x1),z(x2),...,z(xN)] is Gaussian.\nThe GP prior is shown in Eq. 1. Here, \u00b5 is a vector with \u00b5(xi) as its i\u2019th element, \u03a3 is a matrix with\n\u03a3(xi,xj) as the (i,j)\u2019th entry, and \u03b8 are the hyperparameters of the mean and covariance functions.\nWe assume throughout a zero mean-function and a squared-exponential covariance function (also\nknown as radial-basis function or Gaussian) defined as: \u03a3(xi,xj) = \u03c32exp[\u2212(xi\u2212 xj)T(xi\u2212\nxj)\/(2s)]. The set of hyperparameters is \u03b8 = (s,\u03c3). We also define \u03a9 = \u03a3\u22121.\nGiven the GP prior, the observations are modeled using the likelihood shown in Eq. 2. The exact\nform of the distribution p(yn|zn) depends on the type of observations and different choices instan-\ntiates many existing models for GP regression [15, 18, 10, 14]. We consider frequently encountered\ndata such as binary, ordinal, categorical and count observations, and describe their likelihoods in Ta-\nble 1. For the case of categorical observations, the latent function z is a vector whose k\u2019th element\nis the latent function for k\u2019th category. A graphical model for Gaussian process regression is also\nshown.\nGiven these models, there are three tasks that are to be performed: posterior inference, prediction\nat test inputs, and model selection. In all cases, the likelihoods we consider are not conjugate to\nthe Gaussian prior distribution and as a result, the posterior distribution is intractable. Similarly,\nthe integrations required in computing the predictive distribution and the marginal likelihood are\nintractable. To deal with this intractability we make use of variational methods.\n3Variational Lower Bound to the Log Marginal Likelihood\nInference and model selection are always problematic in any Gaussian process regression using non-\nconjugate likelihoods due to the fact that the marginal likelihood contains an intractable integral. In\nthis section, we derive a tractable variational lower bound to the marginal likelihood. We show\n2"},{"page":3,"text":"that the lower bound takes a well known form and can be maximized using concave optimization.\nThroughout the section, we assume scalar zn, with extension to the vector case being straightfor-\nward.\nWe begin with the intractable log marginal likelihood L(\u03b8) in Eq. 3 and introduce a variational\nposterior distribution q(z|\u03b3). We use a Gaussian posterior with mean m and covariance V. The\nfull set of variational parameters is thus \u03b3 = {m,V}. As log is a concave function, we obtain a\nlower bound LJ(\u03b8,\u03b3) using Jensen\u2019s inequality, given in Eq. 4. The first integral is simply the\nKullback\u2212Leibler (KL) divergence from the variational Gaussian posterior q(z|m,V) to the GP\nprior p(z|\u00b5,\u03a3) as shown in Eq. 5, and has a closed-form expression that we substitute to get the\nfirst term in Eq. 6 (inside square brackets), with \u03a9 = \u03a3\u22121.\nThe second integral can be expressed in terms of the expectation with respect to the marginal\nq(zn|mn,Vnn) as shown in the second term of Eq. 5. Here mnis the n\u2019th element of m and\nVnnis the n\u2019th diagonal element of V, the two variables collectively denoted by \u03b3n. The lower\nbound LJis still intractable since the expectation of logp(yn|zn) is not available in closed form for\nthe distributions listed in Table 1. To derive a tractable lower bound, we make use of local variational\nbounds (LVB) fb, defined such that E[logp(yn|zn)] \u2265 fb(yn,mn,Vnn), giving us Eq. 6.\n?\nz\n\u2265 LJ(\u03b8,\u03b3) := \u2212\nz\nN\n?\n?log|V\u03a9|\u2212tr(V\u03a9) \u2212(m\u2212\u00b5)T\u03a9(m\u2212\u00b5)+N?+\nWe discuss the choice of LVBs in the next section, but first discuss the well-known form that the\nlower bound of Eq. 6 takes. Given V, the optimization function with respect to m is a nonlinear\nleast-squares function. Similarly, the function with respect to V is similar to the graphical lasso\n[8] or covariance selection problem [7], but is different in that the argument is a covariance matrix\ninstead of a precision matrix [8]. These two objective functions are coupled through the non-linear\nterm fb(\u00b7). Usually this term arises due to the prior distribution and may be non-smooth, for exam-\nple, in graphical lasso. In our case, this term arises from the likelihood, and is smooth and concave\nas we discuss in next section.\nIt is straightforward to show that the variational lower bound is strictly concave with respect to\n\u03b3 if fbis jointly concave with respect to mnand Vnn. Strict concavity of terms other than fbis\nwell-known since both the least squares and covariance selection problems are concave. Similar\nconcavity results have been discussed by Braun and McAuliffe [5] for the discrete choice model,\nand more recently by Challis and Barber [6] for the Bayesian linear model, who consider concavity\nwith respect to the Cholesky factor of V. We consider concavity with respect to V instead of its\nCholesky factor, which allows us to exploit the special structure of V, as explained in Section 5.\nL(\u03b8) = logp(z|\u03b8)p(y|z)dz = log\nq(z|\u03b3)logq(z|\u03b3)\n?\n?\nz\nq(z|\u03b3)p(z|\u03b8)p(y|z)\nq(z|\u03b3)\ndz\n(3)\n?\np(z|\u03b8)dz +\nz\nq(z|\u03b3)logp(y|z)dz\n(4)\n=\u2212DKL[q(z|\u03b3)||p(z|\u03b8)]+\nn=1\nEq(zn|\u03b3n)[logp(yn|zn)]\n(5)\n\u2265 LJ(\u03b8,\u03b3):=1\n2\nN\n?\nn=1\nfb(yn,mn,Vnn).\n(6)\n4Concave Local Variational Bounds\nIn this section, we describe concave LVBs for various likelihoods. For simplicity, we suppress\nthe dependence on n and consider the log-likelihood of a scalar observation y given a predictor z\ndistributed according to q(z|\u03b3) = N(z|m,v) with \u03b3 = {m,v}. We describe the LVBs for the\nlikelihoods given in Table 1 with z being a scalar for count, binary, and ordinal data, but a vector of\nlength K for categorical data, K being the number of classes. When V is a matrix, we denote its\ndiagonal by v.\nFor the Poison distribution, the expectation is available in closed form and we do not need any\nbounding: E[logp(y|\u03b7)] = ym \u2212 exp(m + v\/2) \u2212 logy!. This function is jointly concave with\nrespect to m and v since the exponential is a convex function.\n3"},{"page":4,"text":"For binary data, we use the piecewise linear\/quadratic bounds proposed by [16], which is a bound\non the logistic-log-partition (LLP) function log(1+exp(x)) and can be used to obtain a bound over\nthe sigmoid function \u03c3(x). The final bound can be expressed as sum of R pieces: E(logp(y|\u03b7)) =\nfb(y,m,v) = ym \u2212?R\nAn important property of the piecewise bound is that its maximum error is bounded and can be\ndriven to zero by increasing the number of pieces. This means that the lower bound in Eq. 6 can\nbe made arbitrarily tight by increasing the number of pieces. For this reason, this bound always\nperforms better than other existing bounds, such as Jaakola\u2019s bound [12], given that the number\nof pieces is chosen appropriately. Finally, the cumulative logit likeilhood for ordinal observations\ndepends on \u03c3(x) and its expectation can be bounded using piecewise bounds in a similar way.\nFor the multinomial logit distribution, we can use the bounds proposed by [3] and [4], both leading\nto concave LVBs. The first bound takes the form fb(y,m,V) = yTm \u2212 lse(m + v\/2) with y\nrepresented using a 1-of-K encoding. This function is jointly concave with respect to m and v,\nwhich can be shown by noting the fact that the log-sum-exp function is convex. The second bound\nis the product of sigmoids bound proposed by [4] which bounds the likelihood with product of\nsigmoids (see Eq. 3 in [4]), with each sigmoid bounded using Jaakkola\u2019s bound [12]. We can also\nuse piecewise linear\/quadratic bound to bound each sigmoid. Alternatively, we can use the recently\nproposed stick-breaking likelihood of [14] which uses piecewise bounds as well.\nFinally, note that the original log-likelihood may not be concave itself, but if it is such that LJhas\na unique solution, then designing a concave variational lower bound will allow us to use concave\noptimization to efficiently maximize the lower bound.\nr=1fbr(m,v) where fbris the expectation of r\u2019th quadratic piece. The\nfunction fbris jointly concave with respect to m,v and their gradients are available in closed-form.\n5Existing Algorithms for Variational Inference\nIn this section, we assume that for each output ynthere is a corresponding scalar latent function zn.\nAll our results can be easily extended to the case of multi-class outputs where the latent function is a\nvector. In variational inference, we find the approximate Gaussian posterior distribution with mean\nm and covariance V that maximizes Eq. 6. The simplest approach is to use gradient-based methods\nfor optimization, but this can be problematic since the number of variational parameters is quadratic\nin N due to the covariance matrix V. The authors of [19] speculate that this may perhaps be the\nreason behind limited use of Gaussian variational approximations.\nWe now show that the problem is simpler than it appears to be, and in fact the number of parameters\ncan be reduced to O(N) from O(N2). First, we write the gradients with respect to m and v in Eq.\n7 and 8 and equate to zero, using gm\nAlso, gmandgvarethevectorsofthesegradients, anddiag(gv) isthematrixwithgvasitsdiagonal.\n\u2212\u03a9(m \u2212 \u00b5) + gm= 0\n1\n2\nAt the solution, we see that V is completely specified if gvis known. This property can be exploited\nto reduce the number of variational parameters.\nOpper and Archambeau [19] (and [18]) propose a reparameterization to reduce the number of pa-\nrameters to O(N). From the fixed-point equation, we note that at the solution m and V will have\nthe following form,\nV = (\u03a3\u22121+ diag(\u03bb))\u22121\nm = \u00b5 + \u03a3\u03b1,\nwhere \u03b1 and \u03bb are real vectors with \u03bbd> 0,\u2200d. At the maximum (but not everywhere), \u03b1 and \u03bb\nwill be equal to gmand gvrespectively. Therefore, instead of solving the fixed-point equations to\nobtain m and V, we can reparameterize the lower bound with respect to \u03b1 and \u03bb. Substituting Eq.\n9 and 10 in Eq. 6 and after simplification using the matrix inversion and determinant lemmas, we\nget the following new objective function (for a detailed derivation, see [18]),\nn:= \u2202fb(yn,mn,vn)\/\u2202mnand gv\nn:= \u2202fb(yn,mn,vn)\/\u2202vn.\n(7)\n(8)\n?V\u22121\u2212 \u03a9?+ diag(gv) = 0\n(9)\n(10)\n1\n2\n?\u2212log(|B\u03bb||diag(\u03bb)|) + Tr(B\u22121\n\u03bb\u03a3) \u2212 \u03b1T\u03a3\u03b1?+\nN\n?\nn=1\nfb(yn,mn,Vnn),\n(11)\n4"},{"page":5,"text":"with B\u03bb= diag(\u03bb)\u22121+ \u03a3. Since the mapping between {\u03b1,\u03bb} and {m,V} is one-to-one, we can\nrecover the latter given the former. The one-to-one relationship also implies that the new objective\nfunction has a unique maximum. The new lower bound involves vectors of size N, reducing the\nnumber of variational parameters to O(N).\nThe problem with this reparameterization is that the new lower bound is no longer concave, even\nthough it has a unique maximum. To see this, consider the 1-D case. We collect all the terms\ninvolving V from Eq. 6, except the LVB term, to define the function f(V ) = [log(V \u03a3\u22121) \u2212\nV \u03a3\u22121]\/2. We substitute the reparameterization V = (\u03a3\u22121+ \u03bb)\u22121to get a new function f(\u03bb) =\n[\u2212log(1 + \u03a3\u03bb) \u2212 (1 + \u03a3\u03bb)\u22121]\/2. The second derivative of this function is f??(\u03bb) =\n\u03a3\u03bb)]2(\u03a3\u03bb\u22121). Clearly, this derivative is negative for \u03bb < 1\/\u03a3 and non-negative otherwise, making\nthe function neither concave nor convex.\nThe objective function is still unimodal and the maximum of (11) is equal to the maximum of\n(6). With the reparameterization, we loose concavity and therefore the algorithm may have slow\nconvergence. Our experimental results (Section 7) confirm the slow convergence.\n1\n2[\u03a3\/(1 +\n6 Fast Convergent Variational Inference using Coordinate Ascent\nWenowderiveanalgorithmthatreducesthenumberofvariationalparametersto2N whilemaintain-\ning concavity. Our algorithm uses simple scalar fixed-point updates to obtain the diagonal elements\nof V. The complete algorithm is shown in Algorithm 1.\nTo derive the algorithm, we first note that the fixed-point equation Eq. 8 has an attractive property:\nat the solution, the off-diagonal elements of V\u22121are the same as the off-diagonal elements of \u03a9,\ni.e. if we denote K := V\u22121, then Kij= \u03a9ij. We need only find the diagonal elements of K to get\nthe full V. This is difficult, however, since the gradient gvdepends on v.\nWe take the approach of optimizing each diagonal element Kiifixing all others (and fixing m as\nwell). We partition V as shown on the left side of Eq. 12, indexing the last row by 2 and rest of the\nrows by 1. We consider a similar partitioning of K and \u03a9. Our goal is to compute v22and k22given\nall other elements of K. Matrices K and V are related through the blockwise inversion, as shown\nbelow.\n\uf8ee\n\uf8f0\nFrom the right bottom corner, we have the first relation below, which we simplify further.\n?\nV11\nvT\n12\nv12\nv22\n?\n=\n\uf8ef\nK\u22121\n11+K\u22121\n11k12kT\nk22\u2212kT\nkT\nk22\u2212kT\n12K\u22121\n11k12\n11\n12K\u22121\n\u2212\nK\u22121\n12K\u22121\n1\n12K\u22121\n11k12\nk22\u2212kT\n11k12\n\u2212\n12K\u22121\n12K\u22121\n11\n11k12\nk22\u2212kT\n11k12\n\uf8f9\n\uf8fb\n\uf8fa\n(12)\nv22= 1\/(k22\u2212 kT\n12K\u22121\n12K\u22121\n11k12)\n\u21d2\nk22=?k22+ 1\/v22\n(13)\nwhere we define?k22:= kT\nthe value of k22from Eq. 13 in Eq. 14 to get Eq. 15. It is easy to check (by taking derivative) that\nthe value v22that satisfies this fixed-point can be found by maximizing the function defined in Eq.\n16.\n11k12. We also know from the fixed point Eq. 8 that the optimal v22\nand k22satisfy Eq. 14 at the solution, where gv\n22is the gradient of fbwith respect to v22. Substitute\n0 = k22\u2212 \u03a922+ 2gv\n0 =?k22+ 1\/v22\u2212 \u03a922+ 2gv\n22\n(14)\n22\n(15)\nf(v) = log(v) \u2212 (\u03a922\u2212?k22)v + 2fb(y2,m22,v)\nupdate: v22\u2190 1\/(\u03a922\u2212?k22\u2212 2gv\nevaluated at every fixed-point iteration. In fact, we do not need to compute it explicitly, since we\ncan obtain its value using Eq. 13:?k22= k22\u2212 1\/v22, and we do this before starting a fixed-point\nwhich is usually constant and very low.\n(16)\nThe function f(v) is a strictly concave function and can be optimized by iterating the following\n22). We will refer to this as a \u201cfixed-point iteration\u201d.\nSince all elements of K, except k22, are fixed,?k22can be computed beforehand and need not be\niteration. The complexity of these iterations depends on the number of gradient evaluations gv\n22,\n5"},{"page":6,"text":"After convergence of the fixed-point iterations, we update V using Eq. 12. It turns out that this is a\nrank-one update, the complexity of which is O(N2). To show these updates, let us denote the new\nvalues obtained after the fixed-point iterations by knew\nvalues by kold\nEq. 13, we get the second equality. Similarly, we use the top left corner of Eq. 12 to get the first\nequality in Eq. 18, and use Eq. 13 and 17 to get the second equality.\n22\nand vnew\n22\nrespectively. and denote the old\n22and vold\n22. We use the right top corner of Eq. 12 to get first equality in Eq. 17. Using\nK\u22121\n11k12= \u2212(kold\nK\u22121\n22\u2212?k22)vold\n12= \u2212vold\n11k12kT\nkold\n12\/vold\n22\n(17)\n11= Vold\n11\u2212K\u22121\n12K\u22121\n11\n22\u2212?k22\n= Vold\n11\u2212 vold\n12(vold\n12)T\/vold\n22\n(18)\nNote that both K\u22121\nVnew. We use Eq. 12 to write updates for Vnewand use 17, 18, and 13 to simplify.\n11and k12do not change after the fixed point iteration. We use this fact to obtain\nvnew\n12\n=\nK\u22121\n11k12\nknew\n22\n11+K\u22121\n\u2212?k22\n= \u2212vnew\n22\nvold\n22\n12K\u22121\n\u2212?k22\nvold\n12\n(19)\nVnew\n11\n= K\u22121\n11k12kT\nknew\n22\n11\n= Vold\n11+vnew\n22\n\u2212 vold\n(vold\n22\n22)2\nvold\n12(vold\n12)T\n(20)\nAfter updating V, we update m by optimizing the following non-linear least squares problem,\nmax\nm\u22121\n2(m \u2212 \u00b5)T\u03a9(m \u2212 \u00b5) +\nN\n?\nn=1\nfb(yn,mn,Vnn)\n(21)\nWe use Newton\u2019s method, the cost of which is O(N3).\n6.1Computational complexity\nThe final procedure is shown in Algorithm 1. The main advantage of our algorithm is its fast\nconvergence as we show this in the results section.\nO(N3+?\ndue to the summation. In all our experiments, Ifp\nThe overall computational complexity is\nnIfp\nn). First term is due to O(N2) update of V for all n and also due to the opti-\nmization of m. Second term is for Ifp\nn is usually 3 to 5, adding very little cost.\nn fixed-point iterations, the total cost of which is linear in N\n6.2Proof of convergence\nProposition 2.7.1 in [2] states that the coordinate ascent algorithm converges if the maximization\nwith respect to each coordinate is uniquely attained. This is indeed the case for us since each fixed\npoint iteration solves a concave problem of the form given by Eq. 16. Similarly, optimization with\nrespect to m is also strictly concave. Hence, convergence of our algorithm is assured.\n6.3Proof that V will always be positive definite\nLet us assume that we start with a positive definite K, for example, we can initialize it with \u03a9. Now\nconsider the update of v22and k22. Note that vnew\n22\n16 which involves the log term. Using this and Eq. 13, we get knew\nSchur complement knew\n22\n\u2212 kT\nfollows that Knewwill also be positive definite, and hence Vnewwill be positive definite.\nwill be positive since it is the maximum of Eq.\n> kT\n11k12> 0. Using this and the fact that K11is positive definite, it\n22\n12K\u22121\n11k12. Hence, the\n12K\u22121\n7 Results\nWe now show that the proposed algorithm leads to a significant gain in the speed of Gaussian process\nregression. The software to reproduce the results of this section are available online1. We evaluate\nthe performance of our fast variational inference algorithm against existing inference methods for\n1http:\/\/www.cs.ubc.ca\/emtiyaz\/software\/codeNIPS2012.html\n6"},{"page":7,"text":"Algorithm 1 Fast convergent coordinate-ascent algorithm\n1. Initialize K \u2190 \u03a9,V \u2190 \u03a9\u22121,m \u2190 \u00b5, where \u03a9 := \u03a3\u22121.\n2. Alternate between updating the diagonal of V and then m until convergence, as follows:\n(a) Update the i\u2019th diagonal of V for all i = 1,...,N:\ni. Rearrange V and \u03a9 so that the i\u2019th column is the last one.\nii.?k22\u2190 k22\u2212 1\/v22.\niv. Run fixed-point iterations for a few steps: v22\u2190 1\/(\u03a922\u2212?k22\u2212 2gv\nA. V11\u2190 V11+ (v22\u2212 vold\nB. v12\u2190 \u2212v22v12\/vold\nvi. Update k22\u2190?k22+ 1\/v22.\niii. Store old value vold\n22\u2190 v22.\n22).\nv. Update V.\n22)v12vT\n12\/(vold\n22)2.\n22.\n(b) Update m by maximizing the least-squares problem of Eq. 21.\nbinary and multi-class classification. For binary classification, we use the UCI ionosphere data (with\n351 data examples containing 34 features). For multi-class classification, we use the UCI forensic\nglass data set with 214 data examples each with 6 category output and features of length 8. In both\ncases, we use 80% of the dataset for training and the rest for testing.\nWe consider GP classification using the Bernoulli logit likelihood, for which we use the piecewise\nbound of [16] with 20 pieces. We compare our algorithm with the approach of Opper and Archam-\nbeau [19] (Eq. 11). For the latter, we use L-BFGS method for optimization. We also compared to\nthe naive method of optimizing with respect to full m and V, e.g. method of [5], but do not present\nthese results since these algorithms have very slow convergence.\nWe examine the computational cost for each method in terms of the number of floating point oper-\nations (flops) for four hyperparameter settings \u03b8 = {log(s),log(\u03c3)}. This comparison is shown in\nFigure 1(a). The y-axis shows (negative of) the value of the lower bound, and the x-axis shows the\nnumber of flops. We draw markers at iteration 1,2,4,50 and in steps of 50 from then on. In all cases,\ndue to non-concavity, the optimization of the Opper and Archambeau reparameterization (black\ncurve with squares) convergence slowly, passing through flat regions of the objective and requiring\na large number of computations to reach convergence. The proposed algorithm (blue curve with\ncircles) has consistently faster convergence than the existing method. For this dataset, our algorithm\nalways converged in 5 iterations.\nWe also compare the total cost to convergence, where we count the total number of flops until\nsuccessive increase in the objective function is below 10\u22123. Each entry is a different setting of\n{log(s),log(\u03c3)}. Rows correspond to values of log(s) while columns correspond to log(\u03c3), with\nunits M,G,T denoting Mega-, Giga-, and Terra-flops. We can see that the proposed algorithm takes\na much smaller number of operations compared to the existing algorithm.\nProposed Algorithm\n-1\n-16M\n1 26M\n3 47M\n13\n7M\n20M\n81M\n7M\n22M\n75M\nOpper and Archambeau\n-1\n-120G\n1 101G\n3 38G\n13\n212G\n24T\n1T\n6T\n24T\n24T\nWe also applied our method to two more datasets of [18], namely \u2019sonar\u2019 and \u2019usps-3vs5\u2019 dataset\nand observed similar behavior.\nNext, we apply our algorithm to the problem of multi-class classification, following [14], using the\nstick-breaking likelihood, and compare to inference using the approach of Opper and Archambeau\n[19] (Eq. 11). We show results comparing the lower bound vs the number of flops taken in Figure\n1(b), for four hyperparameter settings {log(s),log(\u03c3)}. We show markers at iterations 1, 2, 10,\n100 and every 100th iteration thereafter. The results follow those discussed for binary classification,\n7"},{"page":8,"text":"0 300\nMega\u2212Flops\n600 900\n134\n138\n142\n(\u22121.0,\u22121.0)\nneg\u2212LogLik\n01000\nMega\u2212Flops\n2000 3000\n300\n600\n900\n(\u22121.0,2.5)\nneg\u2212LogLik\n0 5K 10K 15K20K\n80\n110\n140\n170\n200\n(3.5,3.5)\nMega\u2212Flops\nneg\u2212LogLik\n0 20004000 6000 8000\n100\n200\n300\nMega\u2212Flops\nneg\u2212LogLik\n(1.0,1.0)\n \n \nOpper\u2212Arch\nproposed\n(a) Ionosphere data\n0 1000 20003000 4000\n260\n270\n280\n290\n300\n310\n320\n(\u22121.0, \u22121.0)\nNeg\u2212LogLik\nMega\u2212flops\n0 10K20K30K 40K50K\n500\n1000\n1500\n2000\n(\u22121.0, 2.5)\nNeg\u2212LogLik\nMega\u2212flops\n0 20K40K60K80K 100K\n200\n250\n300\n350\n400\n(2.5, 2.5)\nNeg\u2212LogLik\nMega\u2212flops\n010K20K 30K 40K 50K\n200\n300\n400\n500\n600\n(1.0, 1.0)\nNeg\u2212LogLik\nMega\u2212flops\n \n \nproposed\nOpper\u2212Arch\n(b) Forensic glass data\nFigure 1: Convergence results for (a) the binary classification on the ionosphere data set and (b) the\nmulti-class classification on the glass dataset. We plot the negative of the lower bound vs the number\nof flops. Each plot shows the progress of algorithms for a hyperparameter setting {log(s),log(\u03c3)}\nshown at the top of the plot. The proposed algorithm always converges faster than the other method,\nin fact, in less than 5 iterations.\nwhere both methods reach the same lower bound value, but the existing approach converging much\nslower, with our algorithm always converged within 20 iterations.\n8 Discussion\nIn this paper we have presented a new variational inference algorithm for non-conjugate GP re-\ngression. We derived a concave variational lower bound to the log marginal likelihood, and used\nconcavity to develop an efficient optimization algorithm. We demonstrated the efficacy of our new\nalgorithm on both binary and multiclass GP classification, demonstrating significant improvement\nin convergence.\nOur proposed algorithm is related to many existing methods for GP regression. For example, the\nobjective function that we consider is exactly the KL minimization method discussed in [18], for\nwhich a gradient based optimization was used. Our algorithm uses an efficient approach where we\nupdate the marginals of the posterior and then do a rank one update of the covariance matrix. Our\nresults show that this leads to fast convergence.\nOur algorithm also takes a similar form to the popular EP algorithm [17], e.g. see Algorithm 3.5 in\n[20]. Both EP and our algorithm update posterior marginals, followed by a rank-one update of\nthe covariance. Therefore, the computational complexity of our approach is similar to that of EP.\nThe advantage of our approach is that, unlike EP, it does not suffer from any numerical issues (for\nexample, no negative variances) and is guaranteed to converge.\nThe derivation of our algorithm is based on the observation that the posterior covariance has a special\nstructure, and does not directly use the concavity of the lower bound. An alternate derivation based\non the Fenchel duality exists and shows that the fixed-point iterations compute dual variables which\nare related to the gradients of fb. We skip this derivation since it is tedious, and present the more\nintuitive derivation instead. The alternative derivation will be made available in an online appendix.\nAcknowledgements\nWe thank the reviewers for their valuable suggestions. SM is supported by the Canadian Institute\nfor Advanced Research (CIFAR).\n8"},{"page":9,"text":"References\n[1] J. Albert and S. Chib. Bayesian analysis of binary and polychotomous response data. J. of the\nAm. Stat. Assoc., 88(422):669\u2013679, 1993.\n[2] Dimitri P. Bertsekas. Nonlinear Programming. Athena Scientific, second edition, 1999.\n[3] D. Blei and J. Lafferty. Correlated topic models. In Advances in Neural Information Proceed-\nings Systems, 2006.\n[4] G. Bouchard. Efficient bounds for the softmax and applications to approximate inference in\nhybrid models. In NIPS 2007 Workshop on Approximate Inference in Hybrid Models, 2007.\n[5] M. Braun and J. McAuliffe. Variational inference for large-scale models of discrete choice.\nJournal of the American Statistical Association, 105(489):324\u2013335, 2010.\n[6] E. Challis and D. Barber. Concave Gaussian variational approximations for inference in large-\nscale Bayesian linear models. In Proceedings of the International Conference on Artificial\nIntelligence and Statistics, volume 6, page 7, 2011.\n[7] A. Dempster. Covariance selection. Biometrics, 28(1), 1972.\n[8] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graph-\nical lasso. Biostatistics, 9(3):432, 2008.\n[9] S. Fr\u00a8 uhwirth-Schnatter and R. Fr\u00a8 uhwirth. Data augmentation and MCMC for binary and multi-\nnomial logit models. Statistical Modelling and Regression Structures, pages 111\u2013132, 2010.\n[10] M. Girolami and S. Rogers. Variational Bayesian multinomial probit regression with Gaussian\nprocess priors. Neural Comptuation, 18(8):1790 \u2013 1817, 2006.\n[11] C. Holmes and L. Held. Bayesian auxiliary variable models for binary and multinomial regres-\nsion. Bayesian Analysis, 1(1):145\u2013168, 2006.\n[12] T. Jaakkola and M. Jordan. A variational approach to Bayesian logistic regression problems\nand their extensions. In AI + Statistics, 1996.\n[13] P. Jyl\u00a8 anki, J. Vanhatalo, and A. Vehtari. Robust Gaussian process regression with a student-t\nlikelihood. The Journal of Machine Learning Research, 999888:3227\u20133257, 2011.\n[14] M. Khan, S. Mohamed, B. Marlin, and K. Murphy. A stick-breaking likelihood for categorical\ndata analysis with latent Gaussian models. In Proceedings of the International Conference on\nArtificial Intelligence and Statistics, 2012.\n[15] M. Kuss and C. E. Rasmussen. Assessing approximate inference for binary Gaussian process\nclassification. J. of Machine Learning Research, 6:1679\u20131704, 2005.\n[16] B. Marlin, M. Khan, and K. Murphy. Piecewise bounds for estimating Bernoulli-logistic latent\nGaussian models. In Intl. Conf. on Machine Learning, 2011.\n[17] T. Minka. Expectation propagation for approximate Bayesian inference. In UAI, 2001.\n[18] H. Nickisch and C.E. Rasmussen. Approximations for binary Gaussian process classification.\nJournal of Machine Learning Research, 9(10), 2008.\n[19] M. Opper and C. Archambeau. The variational Gaussian approximation revisited. Neural\ncomputation, 21(3):786\u2013792, 2009.\n[20] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press,\n2006.\n[21] H. Rue, S. Martino, and N. Chopin. Approximate Bayesian inference for latent Gaussian\nmodels using integrated nested Laplace approximations. J. of Royal Stat. Soc. Series B, 71:\n319\u2013392, 2009.\n[22] S. L. Scott. Data augmentation, frequentist estimation, and the Bayesian analysis of multino-\nmial logit models. Statistical Papers, 52(1):87\u2013109, 2011.\n[23] M. Seeger. Bayesian Inference and Optimal Design in the Sparse Linear Model. J. of Machine\nLearning Research, 9:759\u2013813, 2008.\n[24] M. Seeger and H. Nickisch. Fast Convergent Algorithms for Expectation Propagation Ap-\nproximate Bayesian Inference. In Proceedings of the International Conference on Artificial\nIntelligence and Statistics, 2011.\n9"}],"widgetId":"rgw27_56ab1ddd160c2"},"id":"rgw27_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=266502061&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw28_56ab1ddd160c2"},"id":"rgw28_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=266502061&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":266502061,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":266502061,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":71410443,"url":"researcher\/71410443_James_Hensman","fullname":"James Hensman","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277099177889798%401443077000994_m"},{"id":2048369253,"url":"researcher\/2048369253_Alexander_G_de_G_Matthews","fullname":"Alexander G. de G. Matthews","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70871340,"url":"researcher\/70871340_Maurizio_Filippone","fullname":"Maurizio Filippone","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[[]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Jun 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":1,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes","usePlainButton":true,"publicationUid":278332447,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes","title":"MCMC for Variationally Sparse Gaussian Processes","displayTitleAsLink":true,"authors":[{"id":71410443,"url":"researcher\/71410443_James_Hensman","fullname":"James Hensman","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277099177889798%401443077000994_m"},{"id":2048369253,"url":"researcher\/2048369253_Alexander_G_de_G_Matthews","fullname":"Alexander G. de G. Matthews","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70871340,"url":"researcher\/70871340_Maurizio_Filippone","fullname":"Maurizio Filippone","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Gaussian process (GP) models form a core part of probabilistic machine\nlearning. Considerable research effort has been made into attacking three\nissues with GP models: how to compute efficiently when the number of data is\nlarge; how to approximate the posterior when the likelihood is not Gaussian and\nhow to estimate covariance function parameter posteriors. This paper\nsimultaneously addresses these, using a variational approximation to the\nposterior which is sparse in support of the function but otherwise free-form.\nThe result is a Hybrid Monte-Carlo sampling scheme which allows for a\nnon-Gaussian approximation over the function values and covariance parameters\nsimultaneously, with efficient computations based on inducing-point sparse GPs.\nCode to replicate each experiment in this paper will be available shortly.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Maurizio_Filippone\/publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes\/links\/558a857008aee1fc9174ea84.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Maurizio_Filippone","sourceName":"Maurizio Filippone","hasSourceUrl":true},"publicationUid":278332447,"publicationUrl":"publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes\/links\/558a857008aee1fc9174ea84\/smallpreview.png","linkId":"558a857008aee1fc9174ea84","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=278332447&reference=558a857008aee1fc9174ea84&eventCode=&origin=publication_list","widgetId":"rgw32_56ab1ddd160c2"},"id":"rgw32_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=278332447&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"558a857008aee1fc9174ea84","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":266502061,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw31_56ab1ddd160c2"},"id":"rgw31_56ab1ddd160c2","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=278332447&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":266502061,"publicationLink":"publication\/266502061_Fast_Bayesian_Inference_for_Non-Conjugate_Gaussian_Process_Regression","hasShowMore":false,"newOffset":3,"pageSize":10,"widgetId":"rgw30_56ab1ddd160c2"},"id":"rgw30_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=266502061&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=1","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":1,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw29_56ab1ddd160c2"},"id":"rgw29_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=266502061&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/266502061_Fast_Bayesian_Inference_for_Non-Conjugate_Gaussian_Process_Regression","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1ddd160c2"},"id":"rgw2_56ab1ddd160c2","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":266502061},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=266502061&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1ddd160c2"},"id":"rgw1_56ab1ddd160c2","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"PvSiIvx9NvH6SXZ7IkIh8PCp1+CiM+ND7Es2ffFGRxzPjY8r9FF6uJ69vCe6CXT8Y4WaXQiCFfKkZl6iHtvyA\/3hWZGhh74zWPasaSWdbZr3NXW+p5OtKb+icFDc4NvliFKiWwKrDanq8RTp3aL9FyfrVX0XzSYzkD3dKVzeXSYM2uZj\/iG09eUPn54yGcfmLnDnImIPanyhvujwghi6EG4A+7g7Ama9G8NEUmL4H0ymEf9goR4Le7OxmWM49aFu0Np5RTLnGYHeHnNOBPHDF7wy72cp1mkW+lAxyBBVGgk=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/266502061_Fast_Bayesian_Inference_for_Non-Conjugate_Gaussian_Process_Regression\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression\" \/>\n<meta property=\"og:description\" content=\"We present a new variational inference algorithm for Gaussian process regres-sion with non-conjugate likelihood functions, with application to a wide array of problems including binary and...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/266502061_Fast_Bayesian_Inference_for_Non-Conjugate_Gaussian_Process_Regression\/links\/54365a070cf2bf1f1f2b8032\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/266502061_Fast_Bayesian_Inference_for_Non-Conjugate_Gaussian_Process_Regression\" \/>\n<meta property=\"rg:id\" content=\"PB:266502061\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Fast Bayesian Inference for Non-Conjugate Gaussian Process Regression\" \/>\n<meta name=\"citation_author\" content=\"Mohammad Emtiyaz Khan\" \/>\n<meta name=\"citation_author\" content=\"Shakir Mohamed\" \/>\n<meta name=\"citation_author\" content=\"Kevin P Murphy\" \/>\n<meta name=\"citation_publication_date\" content=\"2012\/01\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"Advances in neural information processing systems\" \/>\n<meta name=\"citation_issn\" content=\"1049-5258\" \/>\n<meta name=\"citation_volume\" content=\"4\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/266502061_Fast_Bayesian_Inference_for_Non-Conjugate_Gaussian_Process_Regression\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/266502061_Fast_Bayesian_Inference_for_Non-Conjugate_Gaussian_Process_Regression\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-795edb08-f5ec-49d4-9de1-ac17fce3d714","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":1085,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw33_56ab1ddd160c2"},"id":"rgw33_56ab1ddd160c2","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-795edb08-f5ec-49d4-9de1-ac17fce3d714", "78d73baefa212fbd6a437d2153e1cdacaf6c515a");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-795edb08-f5ec-49d4-9de1-ac17fce3d714", "78d73baefa212fbd6a437d2153e1cdacaf6c515a");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw34_56ab1ddd160c2"},"id":"rgw34_56ab1ddd160c2","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/266502061_Fast_Bayesian_Inference_for_Non-Conjugate_Gaussian_Process_Regression","requestToken":"D6dX0rwY3Br135u\/7Z92dyRTNANiQ1mJYrVWfXJoMTyGZDM0A3J+9BvQ8RsRmP2psuJLJwglxQH8j8j68lhrlFBmfDivyBuZ5XvktID9AppmypP9STRYtWWI\/dO3RsgxPFSMBS15s2kzZvZxZMUIL+2dpaxI5vxWcE7+tYD8iT7vlmy6giOzBXOIQDLCAIfusGC1vKQz1APdoC7sUPiW6FDgDi1p+ar0Zd+XUwGiuH6v7KzuiVKD6h5f9XVbJCaDQwr7VS2lj2kbR2a9\/+pq\/CusChoYSLGMrdCFE4fsGUE=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=Wbmm4ZRTrZDwaTdnB4ERiUeMaK8LkeqDBrLNqYcAqkkbEVBfIIZSIJxiJihrapiS","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjY2NTAyMDYxX0Zhc3RfQmF5ZXNpYW5fSW5mZXJlbmNlX2Zvcl9Ob24tQ29uanVnYXRlX0dhdXNzaWFuX1Byb2Nlc3NfUmVncmVzc2lvbg%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw36_56ab1ddd160c2"},"id":"rgw36_56ab1ddd160c2","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw35_56ab1ddd160c2"},"id":"rgw35_56ab1ddd160c2","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw37_56ab1ddd160c2"},"id":"rgw37_56ab1ddd160c2","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
