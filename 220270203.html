<!DOCTYPE html> <html lang="en" class="" id="rgw39_56ab1b92e5dae"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="b7y7GFo75Q08AlaLGZ96DcgG4PG2LlFxrW3m0u9dalbSXvvnUPdYdY8mlSV0M7h71e3ORg7YLaFkkf6sdCbNmWsVaiO7s3sniekbn2oNWcQJtpVhKVqlkfXuZr3pDm4kmFTs1rNlEdzelWQ60PQevYzeofQCNiWTlFyIiJbTk1fmMB5FkGtwPnaH8Plj1gdumMbfsoJgHaFoYAvECdNI3mKAztnkwHEo4bMX96nKzysXdnn0W/4UMOD7ST0mmusGuZvvgg+mqBRJHa05F0Vj/xOHqgtB1yG86SGfOk/1qqM="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-5fabe608-c453-4c50-bd5f-c8b86154e680",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/220270203_Infinite_Latent_Feature_Models_and_the_Indian_Buffet_Process" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Infinite Latent Feature Models and the Indian Buffet Process" />
<meta property="og:description" content="We define a probability distribution over equivalence classes of binary ma- trices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/220270203_Infinite_Latent_Feature_Models_and_the_Indian_Buffet_Process/links/0ff74a100cf25dfdcf514e8e/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/220270203_Infinite_Latent_Feature_Models_and_the_Indian_Buffet_Process" />
<meta property="rg:id" content="PB:220270203" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Infinite Latent Feature Models and the Indian Buffet Process" />
<meta name="citation_author" content="Tom Griffiths" />
<meta name="citation_author" content="Zoubin Ghahramani" />
<meta name="citation_conference_title" content="Advances in Neural Information Processing Systems 18 [Neural Information Processing Systems, NIPS 2005, December 5-8, 2005, Vancouver, British Columbia, Canada]" />
<meta name="citation_publication_date" content="2005/01/01" />
<meta name="citation_journal_title" content="Advances in neural information processing systems" />
<meta name="citation_issn" content="1049-5258" />
<meta name="citation_volume" content="18" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/220270203_Infinite_Latent_Feature_Models_and_the_Indian_Buffet_Process" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/220270203_Infinite_Latent_Feature_Models_and_the_Indian_Buffet_Process" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Conference Paper');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Infinite Latent Feature Models and the Indian Buffet Process</title>
<meta name="description" content="Infinite Latent Feature Models and the Indian Buffet Process on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1b92e5dae" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1b92e5dae" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw6_56ab1b92e5dae">  <div class="type-label"> Conference Paper   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Infinite%20Latent%20Feature%20Models%20and%20the%20Indian%20Buffet%20Process&rft.title=Advances%20in%20Neural%20Information%20Processing%20Systems&rft.jtitle=Advances%20in%20Neural%20Information%20Processing%20Systems&rft.volume=18&rft.date=2005&rft.issn=1049-5258&rft.au=Tom%20Griffiths%2CZoubin%20Ghahramani&rft.genre=inProceedings"></span> <h1 class="pub-title" itemprop="name">Infinite Latent Feature Models and the Indian Buffet Process</h1> <meta itemprop="headline" content="Infinite Latent Feature Models and the Indian Buffet Process">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/220270203_Infinite_Latent_Feature_Models_and_the_Indian_Buffet_Process/links/0ff74a100cf25dfdcf514e8e/smallpreview.png">  <div id="rgw9_56ab1b92e5dae" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw10_56ab1b92e5dae"> <a href="researcher/70358432_Tom_Griffiths" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Tom Griffiths" alt="Tom Griffiths" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Tom Griffiths</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw11_56ab1b92e5dae">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/70358432_Tom_Griffiths"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Tom Griffiths" alt="Tom Griffiths" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/70358432_Tom_Griffiths" class="display-name">Tom Griffiths</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56ab1b92e5dae"> <a href="researcher/8159937_Zoubin_Ghahramani" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Zoubin Ghahramani</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw13_56ab1b92e5dae">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/8159937_Zoubin_Ghahramani"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/8159937_Zoubin_Ghahramani" class="display-name">Zoubin Ghahramani</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">         Conference: Advances in Neural Information Processing Systems 18 [Neural Information Processing Systems, NIPS 2005, December 5-8, 2005, Vancouver, British Columbia, Canada]      <div class="pub-source"> Source: <a href="http://dblp.uni-trier.de/db/conf/nips/nips2005.html#GriffithsG05" rel="nofollow">DBLP</a> </div>  </div> <div id="rgw14_56ab1b92e5dae" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>We define a probability distribution over equivalence classes of binary ma- trices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features. We derive the distribution by taking the limit of a distribution over N &times; K binary matrices as K ! 1, a strategy inspired by the derivation of the Chinese restaurant process (Aldous, 1985; Pitman, 2002) as the limit of a Dirichlet-multinomial model. This strategy preserves the exchangeability of the rows of matrices. We define several simple generative processes that result in the same distri- bution over equivalence classes of binary matrices, one of which we call the Indian buffet process. We illustrate the use of this distribution as a prior in an infinite latent feature model, deriving a Markov chain Monte Carlo algo- rithm for inference in this model and applying this algorithm to an artificial dataset.</div> </p>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw27_56ab1b92e5dae">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw26_56ab1b92e5dae"  itemprop="articleBody">  <p>Page 1</p> <p>Infinite Latent Feature Models<br />and the Indian Buffet Process<br />Thomas L. Griffiths<br />Cognitive and Linguistic Sciences<br />Brown University, Providence RI<br />tom griffiths@brown.edu<br />Zoubin Ghahramani<br />Gatsby Computational Neuroscience Unit<br />University College London, London<br />zoubin@gatsby.ucl.ac.uk<br />Abstract<br />We define a probability distribution over equivalence classes of binary<br />matrices with a finite number of rows and an unbounded number of<br />columns. This distribution is suitable for use as a prior in probabilistic<br />models that represent objects using a potentially infinite array of features.<br />We identify a simple generative process that results in the same distribu-<br />tion over equivalence classes, which we call the Indian buffet process.<br />We illustrate the use of this distribution as a prior in an infinite latent fea-<br />turemodel, derivingaMarkovchainMonteCarloalgorithmforinference<br />in this model and applying the algorithm to an image dataset.<br />1 Introduction<br />The statistical models typically used in unsupervised learning draw upon a relatively small<br />repertoire of representations. The simplest representation, used in mixture models, asso-<br />ciates each object with a single latent class. This approach is appropriate when objects<br />can be partitioned into relatively homogeneous subsets. However, the properties of many<br />objects are better captured by representing each object using multiple latent features. For<br />instance, we could choose to represent each object as a binary vector, with entries indicat-<br />ing the presence or absence of each feature [1], allow each feature to take on a continuous<br />value, representing objects with points in a latent space [2], or define a factorial model, in<br />which each feature takes on one of a discrete set of values [3, 4].<br />A critical question in all of these approaches is the dimensionality of the representation:<br />how many classes or features are needed to express the latent structure expressed by a<br />set of objects. Often, determining the dimensionality of the representation is treated as a<br />model selection problem, with a particular dimensionality being chosen based upon some<br />measure of simplicity or generalization performance. This assumes that there is a single,<br />finite-dimensional representation that correctly characterizes the properties of the observed<br />objects. An alternative is to assume that the true dimensionality is unbounded, and that the<br />observed objects manifest only a finite subset of classes or features [5]. This alternative<br />is pursued in nonparametric Bayesian models, such as Dirichlet process mixture models<br />[6, 7, 8, 9]. In a Dirichlet process mixture model, each object is assigned to a latent class,<br />and each class is associated with a distribution over observable properties. The prior dis-<br />tribution over assignments of objects to classes is defined in such a way that the number<br />of classes used by the model is bounded only by the number of objects, making Dirichlet<br />process mixture models “infinite” mixture models [10].<br />The prior distribution assumed in a Dirichlet process mixture model can be specified in</p>  <p>Page 2</p> <p>terms of a sequential process called the Chinese restaurant process (CRP) [11, 12]. In the<br />CRP, N customers enter a restaurant with infinitely many tables, each with infinite seating<br />capacity. The ith customer chooses an already-occupied table k with probability<br />where mkis the number of current occupants, and chooses a new table with probability<br />α<br />i−1+α. Customers are exchangeable under this process: the probability of a particular<br />seating arrangement depends only on the number of people at each table, and not the order<br />in which they enter the restaurant.<br />If we replace customers with objects and tables with classes, the CRP specifies a distribu-<br />tion over partitions of objects into classes. A partition is a division of the set of N objects<br />into subsets, where each object belongs to a single subset and the ordering of the subsets<br />does not matter. Two assignments of objects to classes that result in the same division of<br />objects correspond to the same partition. For example, if we had three objects, the class<br />assignments {c1,c2,c3} = {1,1,2} would correspond to the same partition as {2,2,1},<br />since all that differs between these two cases is the labels of the classes. A partition thus<br />defines an equivalence class of assignment vectors.<br />The distribution over partitions implied by the CRP can be derived by taking the limit of<br />the probability of the corresponding equivalence class of assignment vectors in a model<br />where class assignments are generated from a multinomial distribution with a Dirichlet<br />prior [9, 10]. In this paper, we derive an infinitely exchangeable distribution over infinite<br />binary matrices by pursuing this strategy of taking the limit of a finite model. We also de-<br />scribe a stochastic process (the Indian buffet process, akin to the CRP) which generates this<br />distribution. Finally, we demonstrate how this distribution can be used as a prior in statisti-<br />cal models in which each object is represented by a sparse subset of an unbounded number<br />of features. Further discussion of the properties of this distribution, some generalizations,<br />and additional experiments, are available in the longer version of this paper [13].<br />mk<br />i−1+α,<br />2 A distribution on infinite binary matrices<br />In a latent feature model, each object is represented by a vector of latent feature values fi,<br />and the observable properties of that object xiare generated from a distribution determined<br />by its latent features. Latent feature values can be continuous, as in principal component<br />analysis (PCA) [2], or discrete, as in cooperative vector quantization (CVQ) [3, 4]. In the<br />remainder of this section, we will assume that feature values are continuous. Using the ma-<br />trix F =?fT<br />trices conditioned on those features, p(X|F), where p(·) is a probability density function.<br />These distributions can be dealt with separately: p(F) specifies the number of features and<br />the distribution over values associated with each feature, while p(X|F) determines how<br />these features relate to the properties of objects. Our focus will be on p(F), showing how<br />such a prior can be defined without limiting the number of features.<br />We can break F into two components: a binary matrix Z indicating which features are pos-<br />sessed by each object, with zik= 1 if object i has feature k and 0 otherwise, and a matrix<br />V indicating the value of each feature for each object. F is the elementwise product of Z<br />and V, F = Z ⊗ V, as illustrated in Figure 1. In many latent feature models (e.g., PCA)<br />objects have non-zero values on every feature, and every entry of Z is 1. In sparse latent<br />feature models (e.g., sparse PCA [14, 15]) only a subset of features take on non-zero values<br />for each object, and Z picks out these subsets. A prior on F can be defined by specifying<br />priors for Z and V, with p(F) = P(Z)p(V), where P(·) is a probability mass function.<br />We will focus on defining a prior on Z, since the effective dimensionality of a latent feature<br />model is determined by Z. Assuming that Z is sparse, we can define a prior for infinite la-<br />tent feature models by defining a distribution over infinite binary matrices. Our discussion<br />of the Chinese restaurant process provides two desiderata for such a distribution: objects<br />1fT<br />2··· fT<br />N<br />?Tto indicate the latent feature values for all N objects, the model<br />is specified by a prior over features, p(F), and a distribution over observed property ma-</p>  <p>Page 3</p> <p>(c)<br />objects<br />N<br />K features<br />objects<br />N<br />K features<br />0<br />0<br />0<br />00<br />0<br />−0.1<br />1.8<br />−3.2<br />0.9<br />0.9<br />−0.3<br />0.2 −2.8<br />1.4<br />objects<br />N<br />K features<br />5<br />0<br />0<br />0<br />00<br />0<br />2<br />5<br />1<br />1<br />4<br />4<br />3<br />3<br />(a) (b)<br />Figure 1: A binary matrix Z, as shown in (a), indicates which features take non-zero values.<br />Elementwise multiplication of Z by a matrix V of continuous values produces a represen-<br />tation like (b). If V contains discrete values, we obtain a representation like (c).<br />should be exchangeable, and posterior inference should be tractable. It also suggests a<br />method by which these desiderata can be satisfied: start with a model that assumes a finite<br />number of features, and consider the limit as the number of features approaches infinity.<br />2.1 A finite feature model<br />We have N objects and K features, and the possession of feature k by object i is indicated<br />by a binary variable zik. The zikform a binary N × K feature matrix, Z. Assume that<br />each object possesses feature k with probability πk, and that the features are generated<br />independently. Under this model, the probability of Z given π = {π1,π2,...,πK}, is<br />P(Z|π) =<br />K<br />?<br />k=1<br />N<br />?<br />i=1<br />P(zik|πk) =<br />K<br />?<br />k=1<br />πmk<br />k(1 − πk)N−mk,<br />(1)<br />where mk=?N<br />i=1zikis the number of objects possessing feature k. We can define a prior<br />on π by assuming that each πkfollows a beta distribution, to give<br />πk|α ∼ Beta(α<br />zik|πk∼ Bernoulli(πk)<br />Each zikis independent of all other assignments, conditioned on πk, and the πkare gener-<br />ated independently. We can integrate out π to obtain the probability of Z, which is<br />K,1)<br />P(Z)=<br />K<br />?<br />k=1<br />α<br />KΓ(mk+α<br />K)Γ(N − mk+ 1)<br />Γ(N + 1 +α<br />K)<br />.<br />(2)<br />This distribution is exchangeable, since mkis not affected by the ordering of the objects.<br />2.2 Equivalence classes<br />In order to find the limit of the distribution specified by Equation 2 as K → ∞, we need to<br />define equivalence classes of binary matrices – the analogue of partitions for class assign-<br />ments. Our equivalence classes will be defined with respect to a function on binary matri-<br />ces, lof(·). This function maps binary matrices to left-ordered binary matrices. lof(Z) is<br />obtained by ordering the columns of the binary matrix Z from left to right by the magnitude<br />of the binary number expressed by that column, taking the first row as the most significant<br />bit. The left-ordering of a binary matrix is shown in Figure 2. In the first row of the left-<br />ordered matrix, the columns for which z1k= 1 are grouped at the left. In the second row,<br />the columns for which z2k= 1 are grouped at the left of the sets for which z1k= 1. This<br />grouping structure persists throughout the matrix.<br />The history of feature k at object i is defined to be (z1k,...,z(i−1)k). Where no object is<br />specified, we will use history to refer to the full history of feature k, (z1k,...,zNk). We</p>  <p>Page 4</p> <p>lof<br />Figure 2: Left-ordered form. A binary matrix is transformed into a left-ordered binary<br />matrix by the function lof(·). The entries in the left-ordered matrix were generated from<br />the Indian buffet process with α = 10. Empty columns are omitted from both matrices.<br />will individuate the histories of features using the decimal equivalent of the binary numbers<br />corresponding to the column entries. For example, at object 3, features can have one of four<br />histories: 0, corresponding to a feature with no previous assignments, 1, being a feature for<br />which z2k= 1 but z1k= 0, 2, being a feature for which z1k= 1 but z2k= 0, and 3, being<br />a feature possessed by both previous objects were assigned. Khwill denote the number of<br />features possessing the history h, with K0being the number of features for which mk= 0<br />and K+=?2N−1<br />equivalence class of a binary matrix Z, denoted [Z], is the set of binary matrices that are<br />lof-equivalent to Z. lof-equivalence classes play the role for binary matrices that parti-<br />tions play for assignment vectors: they collapse together all binary matrices (assignment<br />vectors) that differ only in column ordering (class labels). lof-equivalence classes are pre-<br />served through permutation of the rows or the columns of a matrix, provided the same<br />permutations are applied to the other members of the equivalence class. Performing infer-<br />ence at the level of lof-equivalence classes is appropriate in models where feature order<br />is not identifiable, with p(X|F) being unaffected by the order of the columns of F. Any<br />model in which the probability of X is specified in terms of a linear function of F, such<br />as PCA or CVQ, has this property. The cardinality of the lof-equivalence class [Z] is<br />?<br />2.3 Taking the infinite limit<br />h=1Khbeing the number of features for which mk&gt; 0, so K = K0+K+.<br />Two binary matrices Y and Z are lof-equivalent if lof(Y) = lof(Z). The lof-<br />K<br />K0...K2N−1<br />?<br />=<br />K!<br />Q2N−1<br />h=0<br />Kh!, where Khis the number of columns with full history h.<br />UnderthedistributiondefinedbyEquation2, theprobabilityofaparticularlof-equivalence<br />class of binary matrices, [Z], is<br />P([Z]) =<br />?<br />Z∈[Z]<br />P(Z) =<br />K!<br />?2N−1<br />h=0Kh!<br />K<br />?<br />k=1<br />α<br />KΓ(mk+α<br />K)Γ(N − mk+ 1)<br />Γ(N + 1 +α<br />K)<br />.<br />(3)<br />Rearranging terms, and using the fact that Γ(x) = (x − 1)Γ(x − 1) for x &gt; 1, we can<br />compute the limit of P([Z]) as K approaches infinity<br />lim<br />K→∞<br />αK+<br />?2N−1<br />αK+<br />?2N−1<br />h=1Kh!<br />·<br />K!<br />K0!KK+·<br />?<br />N!<br />?N<br />exp{−αHN}<br />j=1(j +α<br />K)<br />?K<br />·<br />K+<br />?<br />K+<br />?<br />1<br />j. This distribution is infinitely<br />k=1<br />(N − mk)!?mk−1<br />j=1(j +α<br />N!<br />K)<br />=<br />h=1Kh!<br />·1··<br />k=1<br />(N − mk)!(mk− 1)!<br />N!<br />,<br />(4)<br />where HN is the Nth harmonic number, HN =?N<br />details of this limit are provided in [13].<br />j=1<br />exchangeable, since neither Khnor mkare affected by the ordering on objects. Technical</p>  <p>Page 5</p> <p>2.4 The Indian buffet process<br />The probability distribution defined in Equation 4 can be derived from a simple stochastic<br />process. Due to the similarity to the Chinese restaurant process, we will also use a culinary<br />metaphor, appropriately adjusted for geography. Indian restaurants in London offer buffets<br />with an apparently infinite number of dishes. We will define a distribution over infinite<br />binary matrices by specifying how customers (objects) choose dishes (features).<br />In our Indian buffet process (IBP), N customers enter a restaurant one after another. Each<br />customer encounters a buffet consisting of infinitely many dishes arranged in a line. The<br />first customer starts at the left of the buffet and takes a serving from each dish, stopping<br />after a Poisson(α) number of dishes. The ith customer moves along the buffet, sampling<br />dishes in proportion to their popularity, taking dish k with probabilitymk<br />number of previous customers who have sampled that dish. Having reached the end of all<br />previous sampled dishes, the ith customer then tries a Poisson(α<br />We can indicate which customers chose which dishes using a binary matrix Z with N rows<br />and infinitely many columns, where zik= 1 if the ith customer sampled the kth dish.<br />Using K(i)<br />1<br />to indicate the number of new dishes sampled by the ith customer, the proba-<br />bility of any particular matrix being produced by the IBP is<br />i, where mkis the<br />i) number of new dishes.<br />P(Z) =<br />αK+<br />?N<br />i=1K(i)<br />1!<br />exp{−αHN}<br />K+<br />?<br />k=1<br />(N − mk)!(mk− 1)!<br />N!<br />.<br />(5)<br />The matrices produced by this process are generally not in left-ordered form. These ma-<br />trices are also not ordered arbitrarily, because the Poisson draws always result in choices<br />of new dishes that are to the right of the previously sampled dishes. Customers are not<br />exchangeable under this distribution, as the number of dishes counted as K(i)<br />upon the order in which the customers make their choices. However, if we only pay at-<br />tention to the lof-equivalence classes of the matrices generated by this process, we obtain<br />the infinitely exchangeable distribution P([Z]) given by Equation 4:<br />1<br />depends<br />QN<br />Q2N−1<br />h=1<br />i=1K(i)<br />1!<br />Kh!matrices<br />generated via this process map to the same left-ordered form, and P([Z]) is obtained by<br />multiplying P(Z) from Equation 5 by this quantity. A similar but slightly more compli-<br />cated process can be defined to produce left-ordered matrices directly [13].<br />2.5Conditional distributions<br />To define a Gibbs sampler for models using the IBP, we need to know the conditional<br />distribution on feature assignments, P(zik= 1|Z−(ik)). In the finite model, where P(Z)<br />is given by Equation 2, it is straightforward to compute this conditional distribution for any<br />zik. Integrating over πkgives<br />P(zik= 1|z−i,k) =m−i,k+α<br />K<br />N +α<br />K<br />,<br />(6)<br />where z−i,kis the set of assignments of other objects, not including i, for feature k, and<br />m−i,kis the number of objects possessing feature k, not including i. We need only condi-<br />tion on z−i,krather than Z−(ik)because the columns of the matrix are independent.<br />In the infinite case, we can derive the conditional distribution from the (exchangeable) IBP.<br />Choosing an ordering on objects such that the ith object corresponds to the last customer<br />to visit the buffet, we obtain<br />P(zik= 1|z−i,k) =m−i,k<br />N<br />,<br />(7)<br />for any k such that m−i,k &gt; 0. The same result can be obtained by taking the limit of<br />Equation 6 as K → ∞. The number of new features associated with object i should be</p>  <p>Page 6</p> <p>drawn from a Poisson(α<br />same kind of limiting argument as that presented above.<br />N) distribution. This can also be derived from Equation 6, using the<br />3 A linear-Gaussian binary latent feature model<br />To illustrate how the IBP can be used as a prior in models for unsupervised learning, we<br />derived and tested a linear-Gaussian latent feature model in which the features are binary.<br />In this case the feature matrix F reduces to the binary matrix Z. As above, we will start<br />with a finite model and then consider the infinite limit.<br />In our finite model, the D-dimensional vector of properties of an object i, xiis generated<br />from a Gaussian distribution with mean ziA and covariance matrix ΣX = σ2<br />ziis a K-dimensional binary vector, and A is a K × D matrix of weights. In matrix<br />notation, E [X] = ZA. If Z is a feature matrix, this is a form of binary factor analysis. The<br />distribution of X given Z, A, and σXis matrix Gaussian with mean ZA and covariance<br />matrix σ2<br />mean 0 and covariance matrix σ2<br />XI, where<br />XI, where I is the identity matrix. The prior on A is also matrix Gaussian, with<br />AI. Integrating out A, we have<br />1<br />(2π)ND/2σ(N−K)D<br />X<br />σKD<br />tr(XT(I − Z(ZTZ +σ2<br />p(X|Z,σX,σA)=<br />A<br />|ZTZ +<br />σ2<br />σ2<br />X<br />AI|D/2<br />exp{−<br />1<br />2σ2<br />X<br />X<br />σ2<br />A<br />I)−1ZT)X)}. (8)<br />This result is intuitive: the exponentiated term is the difference between the inner product<br />of X and its projection onto the space spanned by Z, regularized to an extent determined<br />by the ratio of the variance of the noise in X to the variance of the prior on A. It follows<br />that p(X|Z,σX,σA) depends only on the non-zero columns of Z, and thus remains well-<br />defined when we take the limit as K → ∞ (for more details see [13]).<br />We can define a Gibbs sampler for this model by computing the full conditional distribution<br />P(zik|X,Z−(i,k),σX,σA) ∝ p(X|Z,σX,σA)P(zik|z−i,k).<br />The two terms on the right hand side can be evaluated using Equations 8 and 7 respectively.<br />The Gibbs sampler is then straightforward. Assignments for features for which m−i,k&gt; 0<br />are drawn from the distribution specified by Equation 9. The distribution over the number<br />of new features for each object can be approximated by truncation, computing probabilities<br />for a range of values of K(i)<br />1<br />up to an upper bound. For each value, p(X|Z,σX,σA) can<br />be computed from Equation 8, and the prior on the number of new features is Poisson(α<br />We will demonstrate this Gibbs sampler for the infinite binary linear-Gaussian model on a<br />dataset consisting of 100 240 × 320 pixel images. We represented each image, xi, using<br />a 100-dimensional vector corresponding to the weights of the mean image and the first 99<br />principal components. Each image contained up to four everyday objects – a $20 bill, a<br />Klein bottle, a prehistoric handaxe, and a cellular phone. Each object constituted a single<br />latent feature responsible for the observed pixel values. The images were generated by<br />sampling a feature vector, zi, from a distribution under which each feature was present<br />with probability 0.5, and then taking a photograph containing the appropriate objects using<br />a LogiTech digital webcam. Sample images are shown in Figure 3 (a).<br />The Gibbs sampler was initialized with K+ = 1, choosing the feature assignments for<br />the first column by setting zi1 = 1 with probability 0.5. σA, σX, and α were initially<br />set to 0.5, 1.7, and 1 respectively, and then sampled by adding Metropolis steps to the<br />MCMC algorithm. Figure 3 shows trace plots for the first 1000 iterations of MCMC for the<br />number of features used by at least one object, K+, and the model parameters σA, σX, and<br />α. All of these quantities stabilized after approximately 100 iterations, with the algorithm<br />(9)<br />N).</p>  <p>Page 7</p> <p>(a)<br />(Positive)<br />(b)<br />(Negative) (Negative)(Negative)<br />0  0  0  0<br />(c)<br />0  1  0  0 1  1  1  0 1  0  1  1<br />0 100 200300 400500 600700 800900 1000<br />0<br />5<br />10<br /> K+<br />0 100 200300 400500 6007008009001000<br />0<br />2<br />4<br />α<br />0 100 200 300400 500600700 8009001000<br />0<br />1<br />2<br />σX<br />0 100200 300400 500600 700800 9001000<br />0<br />1<br />2<br />σA<br />Iteration<br />Figure 3: Data and results for the demonstration of the infinite linear-Gaussian binary<br />latent feature model. (a) Four sample images from the 100 in the dataset. Each image<br />had 320 × 240 pixels, and contained from zero to four everyday objects. (b) The posterior<br />mean of the weights (A) for the four most frequent binary features from the 1000th sample.<br />Each image corresponds to a single feature. These features perfectly indicate the presence<br />or absence of the four objects. The first feature indicates the presence of the $20 bill,<br />the other three indicate the absence of the Klein bottle, the handaxe, and the cellphone.<br />(c) Reconstructions of the images in (a) using the binary codes inferred for those images.<br />These reconstructions are based upon the posterior mean of A for the 1000th sample. For<br />example, the code for the first image indicates that the $20 bill is absent, while the other<br />three objects are not. The lower panels show trace plots for the dimensionality of the<br />representation (K+) and the parameters α, σX, and σAover 1000 iterations of sampling.<br />The values of all parameters stabilize after approximately 100 iterations.</p>  <p>Page 8</p> <p>finding solutions with approximately seven latent features. The four most common features<br />perfectly indicated the presence and absence of the four objects (shown in Figure 3 (b)), and<br />three less common features coded for slight differences in the locations of those objects.<br />4 Conclusion<br />We have shown that the methods that have been used to define infinite latent class models<br />[6, 7, 8, 9, 10, 11, 12] can be extended to models in which objects are represented in<br />terms of a set of latent features, deriving a distribution on infinite binary matrices that can<br />be used as a prior for such models. While we derived this prior as the infinite limit of<br />a simple distribution on finite binary matrices, we have shown that the same distribution<br />can be specified in terms of a simple stochastic process – the Indian buffet process. This<br />distribution satisfies our two desiderata for a prior for infinite latent feature models: objects<br />are exchangeable, and inference remains tractable. Our success in transferring the strategy<br />of taking the limit of a finite model from latent classes to latent features suggests that a<br />similar approach could be applied with other representations, expanding the forms of latent<br />structure that can be recovered through unsupervised learning.<br />References<br />[1] N. Ueda and K. Saito. Parametric mixture models for multi-labeled text. In Advances in Neural<br />Information Processing Systems 15, Cambridge, 2003. MIT Press.<br />[2] I. T. Jolliffe. Principal component analysis. Springer, New York, 1986.<br />[3] R. S. Zemel and G. E. Hinton. Developing population codes by minimizing description length.<br />In Advances in Neural Information Processing Systems 6. Morgan Kaufmann, San Francisco,<br />CA, 1994.<br />[4] Z. Ghahramani. Factorial learning and the EM algorithm. In Advances in Neural Information<br />Processing Systems 7. Morgan Kaufmann, San Francisco, CA, 1995.<br />[5] C. E. Rasmussen and Z. Ghahramani. Occam’s razor. In Advances in Neural Information<br />Processing Systems 13. MIT Press, Cambridge, MA, 2001.<br />[6] C. Antoniak. Mixtures of Dirichlet processes with applications to Bayesian nonparametric<br />problems. The Annals of Statistics, 2:1152–1174, 1974.<br />[7] M. D. Escobar and M. West. Bayesian density estimation and inference using mixtures. Journal<br />of the American Statistical Association, 90:577–588, 1995.<br />[8] T. S. Ferguson. Bayesian density estimation by mixtures of normal distributions. In M. Rizvi,<br />J. Rustagi, and D. Siegmund, editors, Recent advances in statistics, pages 287–302. Academic<br />Press, New York, 1983.<br />[9] R. M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of<br />Computational and Graphical Statistics, 9:249–265, 2000.<br />[10] C. Rasmussen. The infinite Gaussian mixture model. In Advances in Neural Information Pro-<br />cessing Systems 12. MIT Press, Cambridge, MA, 2000.<br />[11] D. Aldous. Exchangeability and related topics. In´Ecole d’´ et´ e de probabilit´ es de Saint-Flour,<br />XIII—1983, pages 1–198. Springer, Berlin, 1985.<br />[12] J. Pitman. Combinatorial stochastic processes, 2002. Notes for Saint Flour Summer School.<br />[13] T. L. Griffiths and Z. Ghahramani. Infinite latent feature models and the Indian buffet process.<br />Technical Report 2005-001, Gatsby Computational Neuroscience Unit, 2005.<br />[14] A. d’Aspremont, L. El Ghaoui, I. Jordan, and G. R. G. Lanckriet. A direct formulation for<br />sparse PCA using semidefinite programming. In Advances in Neural Information Processing<br />Systems 17. MIT Press, Cambridge, MA, 2005.<br />[15] H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis. Journal of Compu-<br />tational and Graphical Statistics, in press.</p>   </div> <div id="rgw19_56ab1b92e5dae" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw20_56ab1b92e5dae">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw21_56ab1b92e5dae"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.60.3951&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Infinite Latent Feature Models and the Indian Buffet Process">Infinite Latent Feature Models and the Indian Buff...</a> </div>  <div class="details">   Available from <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.60.3951&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow">psu.edu</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw28_56ab1b92e5dae" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (276) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw29_56ab1b92e5dae" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw30_56ab1b92e5dae" >  <div class="indent-left">  <div id="rgw31_56ab1b92e5dae" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/288889800_Strategies_and_Principles_of_Distributed_Machine_Learning_on_Big_Data">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1512.09295" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: de.arxiv.org </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw32_56ab1b92e5dae">  <li class="citation-context-item"> "The majority of ML applications are supported by a moderate number of families of well-developed ML approaches, each of which embodies a continuum of technical elements from model design, to algorithmic innovation, and even to perfection of the software implementation, and which attracts ever-growing novel contributions from the research and development community. Modern examples of such approaches include Graphical Models [54] [28] [58], Regularized Bayesian models [72] [70] [71], Nonparametric Bayesian models [18] [49], Sparse Structured models [63] [27], Large-margin methods [8] [46], Deep learning [21] [29], Matrix Factorization [31] [41], Sparse Coding [44] [32], and Latent Space Modeling [4] [68]. A common ML practice that ensures mathematical soundness and outcome reproducibility is for practitioners and researchers to write an ML program (using any generic high-level programming language) for an application-specific instance of a particular ML approach (e.g. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/288889800_Strategies_and_Principles_of_Distributed_Machine_Learning_on_Big_Data"> <span class="publication-title js-publication-title">Strategies and Principles of Distributed Machine Learning on Big Data</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/9100047_Eric_P_Xing" class="authors js-author-name ga-publications-authors">Eric P. Xing</a> &middot;     <a href="researcher/49041600_Qirong_Ho" class="authors js-author-name ga-publications-authors">Qirong Ho</a> &middot;     <a href="researcher/2031310989_Pengtao_Xie" class="authors js-author-name ga-publications-authors">Pengtao Xie</a> &middot;     <a href="researcher/2041017717_Wei_Dai" class="authors js-author-name ga-publications-authors">Wei Dai</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> The rise of Big Data has led to new demands for Machine Learning (ML) systems
to learn complex models with millions to billions of parameters, that promise
adequate capacity to digest massive datasets and offer powerful predictive
analytics thereupon. In order to run ML algorithms at such scales, on a
distributed cluster with 10s to 1000s of machines, it is often the case that
significant engineering efforts are required --- and one might fairly ask if
such engineering truly falls within the domain of ML research or not. Taking
the view that Big ML systems can benefit greatly from ML-rooted statistical and
algorithmic insights --- and that ML researchers should therefore not shy away
from such systems design --- we discuss a series of principles and strategies
distilled from our recent efforts on industrial-scale ML solutions. These
principles and strategies span a continuum from application, to engineering,
and to theoretical research and development of Big ML systems and
architectures, with the goal of understanding how to make them efficient,
generally-applicable, and supported with convergence and scaling guarantees.
They concern four key questions which traditionally receive little attention in
ML research: How to distribute an ML program over a cluster? How to bridge ML
computation with inter-machine communication? How to perform such
communication? What should be communicated between machines? By exposing
underlying statistical and algorithmic characteristics unique to ML programs
but not typically seen in traditional computer programs, and by dissecting
successful cases to reveal how we have harnessed these principles to design and
develop both high-performance distributed ML software as well as
general-purpose ML frameworks, we present opportunities for ML researchers and
practitioners to further shape and grow the area that lies between ML and
systems. </span> </div>    <div class="publication-meta publication-meta">  <span class="ico-publication-preview reset-background"></span> Preview    &middot; Article &middot; Dec 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw33_56ab1b92e5dae" >  <div class="indent-left">  <div id="rgw34_56ab1b92e5dae" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/283433373_Streaming_Distributed_Variational_Inference_for_Bayesian_Nonparametrics">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Julian_Straub" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Julian Straub </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw35_56ab1b92e5dae">  <li class="citation-context-item"> "Bayesian nonparametrics are also naturally suited to parallelization of data processing, due to the exchangeability, and thus conditional independence, they often exhibit via de Finetti&#39;s theorem. For example, labels from the Chinese Restaurant process [3] are rendered i.i.d. by conditioning on the underlying Dirichlet process (DP) random measure, and feature assignments from the Indian Buffet process [4] are rendered i.i.d. by conditioning on the underlying beta process (BP) random measure. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/283433373_Streaming_Distributed_Variational_Inference_for_Bayesian_Nonparametrics"> <span class="publication-title js-publication-title">Streaming, Distributed Variational Inference for Bayesian Nonparametrics</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2008913272_Trevor_Campbell" class="authors js-author-name ga-publications-authors">Trevor Campbell</a> &middot;     <a href="researcher/2073344635_Julian_Straub" class="authors js-author-name ga-publications-authors">Julian Straub</a> &middot;     <a href="researcher/11213370_John_W_Fisher_III" class="authors js-author-name ga-publications-authors">John W. Fisher III</a> &middot;     <a href="researcher/46470644_Jonathan_P_How" class="authors js-author-name ga-publications-authors">Jonathan P. How</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> This paper presents a methodology for creating streaming, distributed
inference algorithms for Bayesian nonparametric (BNP) models. In the proposed
framework, processing nodes receive a sequence of data minibatches, compute a
variational posterior for each, and make asynchronous streaming updates to a
central model. In contrast to previous algorithms, the proposed framework is
truly streaming, distributed, asynchronous, learning-rate-free, and
truncation-free. The key challenge in developing the framework, arising from
the fact that BNP models do not impose an inherent ordering on their
components, is finding the correspondence between minibatch and central BNP
posterior components before performing each update. To address this, the paper
develops a combinatorial optimization problem over component correspondences,
and provides an efficient solution technique. The paper concludes with an
application of the methodology to the DP mixture model, with experimental
results demonstrating its practical scalability and performance. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Oct 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Julian_Straub/publication/283433373_Streaming_Distributed_Variational_Inference_for_Bayesian_Nonparametrics/links/565332a508aeafc2aabb1013.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw36_56ab1b92e5dae" >  <div class="indent-left">  <div id="rgw37_56ab1b92e5dae" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/283279776_Scaled_subordinators_and_generalizations_of_the_Indian_buffet_process">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Lancelot_James" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Lancelot F. James </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw38_56ab1b92e5dae">  <li class="citation-context-item"> "i.e. in terms of the object/feature interpretation, n k of the first n observed object possess feature k, and K n is the total number of distinct feature exhibited by the first n objects. The following version is due to Ghahramani, Griffiths, and Sollich [14]: Choose c, θ &gt; 0. 1. The first row of Z contains K 1 ∼ Poisson(c) consecutive non-zero entries. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/283279776_Scaled_subordinators_and_generalizations_of_the_Indian_buffet_process"> <span class="publication-title js-publication-title">Scaled subordinators and generalizations of the Indian buffet process</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/7196613_Lancelot_F_James" class="authors js-author-name ga-publications-authors">Lancelot F. James</a> &middot;     <a href="researcher/69976956_Peter_Orbanz" class="authors js-author-name ga-publications-authors">Peter Orbanz</a> &middot;     <a href="researcher/9164246_Yee_Whye_Teh" class="authors js-author-name ga-publications-authors">Yee Whye Teh</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> We study random families of subsets of $\mathbb{N}$ that are similar to
exchangeable random partitions, but do not require constituent sets to be
disjoint: Each element of ${\mathbb{N}}$ may be contained in multiple subsets.
One class of such objects, known as Indian buffet processes, has become a
popular tool in machine learning. Based on an equivalence between Indian buffet
and scale-invariant Poisson processes, we identify a random scaling variable
whose role is similar to that played in exchangeable partition models by the
total mass of a random measure. Analogous to the construction of exchangeable
partitions from normalized subordinators, random families of sets can be
constructed from randomly scaled subordinators. Coupling to a heavy-tailed
scaling variable induces a power law on the number of sets containing the first
$n$ elements. Several examples, with properties desirable in applications, are
derived explicitly. A relationship to exchangeable partitions is made precise
as a correspondence between scaled subordinators and Poisson-Kingman measures,
generalizing a result of Arratia, Barbour and Tavare on scale-invariant
processes. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Oct 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Lancelot_James/publication/283279776_Scaled_subordinators_and_generalizations_of_the_Indian_buffet_process/links/5646af7d08aef646e6cdd493.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw23_56ab1b92e5dae" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw24_56ab1b92e5dae">  </ul> </div> </div>   <div id="rgw15_56ab1b92e5dae" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw16_56ab1b92e5dae"> <div> <h5> <a href="publication/291690019_Building_probabilistic_models_of_fire_occurrence_and_fire_risk_zoning_using_logistic_regression_in_Shanxi_Province_China" class="color-inherit ga-similar-publication-title"><span class="publication-title">Building probabilistic models of fire occurrence and fire risk zoning using logistic regression in Shanxi Province, China</span></a>  </h5>  <div class="authors"> <a href="researcher/2095468381_Jinghu_Pan" class="authors ga-similar-publication-author">Jinghu Pan</a>, <a href="researcher/2095490091_Weiguo_Wang" class="authors ga-similar-publication-author">Weiguo Wang</a>, <a href="researcher/2095526081_Junfeng_Li" class="authors ga-similar-publication-author">Junfeng Li</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56ab1b92e5dae"> <div> <h5> <a href="publication/291340346_Temperature-Dependent_Emission_Kinetics_of_Colloidal_Semiconductor_Nanoplatelets_Strongly_Modified_by_Stacking" class="color-inherit ga-similar-publication-title"><span class="publication-title">Temperature-Dependent Emission Kinetics of Colloidal Semiconductor Nanoplatelets Strongly Modified by Stacking</span></a>  </h5>  <div class="authors"> <a href="researcher/2059777116_Onur_Erdem" class="authors ga-similar-publication-author">Onur Erdem</a>, <a href="researcher/31320601_Murat_Olutas" class="authors ga-similar-publication-author">Murat Olutas</a>, <a href="researcher/60001925_Burak_Guzelturk" class="authors ga-similar-publication-author">Burak Guzelturk</a>, <a href="researcher/2008924001_Yusuf_Kelestemur" class="authors ga-similar-publication-author">Yusuf Kelestemur</a>, <a href="researcher/38633698_Hilmi_Volkan_Demir" class="authors ga-similar-publication-author">Hilmi Volkan Demir</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56ab1b92e5dae"> <div> <h5> <a href="publication/290527271_Simulated_Quantum_Annealing_Can_Be_Exponentially_Faster_than_Classical_Simulated_Annealing" class="color-inherit ga-similar-publication-title"><span class="publication-title">Simulated Quantum Annealing Can Be Exponentially Faster than Classical Simulated Annealing</span></a>  </h5>  <div class="authors"> <a href="researcher/2093958263_Elizabeth_Crosson" class="authors ga-similar-publication-author">Elizabeth Crosson</a>, <a href="researcher/2094036446_Aram_W_Harrow" class="authors ga-similar-publication-author">Aram W. Harrow</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw40_56ab1b92e5dae" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw41_56ab1b92e5dae">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw42_56ab1b92e5dae" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=EJqOkuvHtbgQoow5gRiQshXl8upjj8UNNIqqTAGd1lpjjkfulqJB3zHTUMRbpmn2" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="FZVS+W7hPLHI4Hbi0znffK3F2zK3EyzWIHSnzQP5YBttmwJdYQhGQ/e+IuQ89DCYQXEP8gxuSbKTpWZeOUiHGBMvPUGrFITHKA1CWTOwENx89TbgpcZZAfNAiJzUBwoUrK01v95x+j7/hME54mz6I0pAiEFuYzfoKul+gp/riXH2Wm+oNv5rzz2v8/abTK0gb9efwx3UUflYKoL9RATQD8KcF8v6ye/NHcvkp2OpwTdlUh52IvTqx8vAz621YnCgfC91ADN7P1bFyY+1KIcrpZ25GBFdQRz3H3+c7Vp7MJw="/> <input type="hidden" name="urlAfterLogin" value="publication/220270203_Infinite_Latent_Feature_Models_and_the_Indian_Buffet_Process"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMjcwMjAzX0luZmluaXRlX0xhdGVudF9GZWF0dXJlX01vZGVsc19hbmRfdGhlX0luZGlhbl9CdWZmZXRfUHJvY2Vzcw%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMjcwMjAzX0luZmluaXRlX0xhdGVudF9GZWF0dXJlX01vZGVsc19hbmRfdGhlX0luZGlhbl9CdWZmZXRfUHJvY2Vzcw%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMjcwMjAzX0luZmluaXRlX0xhdGVudF9GZWF0dXJlX01vZGVsc19hbmRfdGhlX0luZGlhbl9CdWZmZXRfUHJvY2Vzcw%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw43_56ab1b92e5dae"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 522;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"profileUrl":"researcher\/70358432_Tom_Griffiths","fullname":"Tom Griffiths","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2549355721578\/images\/template\/default\/profile\/profile_default_m.png","profileStats":[null,{"data":{"publicationCount":3,"widgetId":"rgw5_56ab1b92e5dae"},"id":"rgw5_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorPublicationCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorPublicationCount.html?authorUid=70358432","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},null],"widgetId":"rgw4_56ab1b92e5dae"},"id":"rgw4_56ab1b92e5dae","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorBadge.html?authorUid=70358432","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw3_56ab1b92e5dae"},"id":"rgw3_56ab1b92e5dae","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=220270203","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":220270203,"title":"Infinite Latent Feature Models and the Indian Buffet Process","journalTitle":"Advances in neural information processing systems","journalDetailsTooltip":{"data":{"journalTitle":"Advances in neural information processing systems","journalAbbrev":"Adv Neural Inform Process Syst","publisher":"IEEE Conference on Neural Information Processing Systems--Natural and Synthetic, Massachusetts Institute of Technology Press","issn":"1049-5258","impactFactor":"0.00","fiveYearImpactFactor":"0.00","citedHalfLife":"0.00","immediacyIndex":"0.00","eigenFactor":"0.00","articleInfluence":"0.00","widgetId":"rgw7_56ab1b92e5dae"},"id":"rgw7_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=1049-5258","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Conference Paper","details":{"conferenceInfos":"Conference: Advances in Neural Information Processing Systems 18 [Neural Information Processing Systems, NIPS 2005, December 5-8, 2005, Vancouver, British Columbia, Canada]"},"source":{"sourceUrl":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2005.html#GriffithsG05","sourceName":"DBLP"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Infinite Latent Feature Models and the Indian Buffet Process"},{"key":"rft.title","value":"Advances in Neural Information Processing Systems"},{"key":"rft.jtitle","value":"Advances in Neural Information Processing Systems"},{"key":"rft.volume","value":"18"},{"key":"rft.date","value":"2005"},{"key":"rft.issn","value":"1049-5258"},{"key":"rft.au","value":"Tom Griffiths,Zoubin Ghahramani"},{"key":"rft.genre","value":"inProceedings"}],"widgetId":"rgw8_56ab1b92e5dae"},"id":"rgw8_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=220270203","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":220270203,"peopleItems":[{"data":{"authorUrl":"researcher\/70358432_Tom_Griffiths","authorNameOnPublication":"Tom Griffiths","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Tom Griffiths","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/70358432_Tom_Griffiths","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw11_56ab1b92e5dae"},"id":"rgw11_56ab1b92e5dae","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=70358432&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw10_56ab1b92e5dae"},"id":"rgw10_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=70358432&authorNameOnPublication=Tom%20Griffiths","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/8159937_Zoubin_Ghahramani","authorNameOnPublication":"Zoubin Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Zoubin Ghahramani","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/8159937_Zoubin_Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw13_56ab1b92e5dae"},"id":"rgw13_56ab1b92e5dae","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=8159937&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw12_56ab1b92e5dae"},"id":"rgw12_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=8159937&authorNameOnPublication=Zoubin%20Ghahramani","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw9_56ab1b92e5dae"},"id":"rgw9_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=220270203&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":220270203,"abstract":"<noscript><\/noscript><div>We define a probability distribution over equivalence classes of binary ma- trices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features. We derive the distribution by taking the limit of a distribution over N &times; K binary matrices as K ! 1, a strategy inspired by the derivation of the Chinese restaurant process (Aldous, 1985; Pitman, 2002) as the limit of a Dirichlet-multinomial model. This strategy preserves the exchangeability of the rows of matrices. We define several simple generative processes that result in the same distri- bution over equivalence classes of binary matrices, one of which we call the Indian buffet process. We illustrate the use of this distribution as a prior in an infinite latent feature model, deriving a Markov chain Monte Carlo algo- rithm for inference in this model and applying this algorithm to an artificial dataset.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw14_56ab1b92e5dae"},"id":"rgw14_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=220270203","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/220270203_Infinite_Latent_Feature_Models_and_the_Indian_Buffet_Process\/links\/0ff74a100cf25dfdcf514e8e\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw6_56ab1b92e5dae"},"id":"rgw6_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=220270203&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2095468381,"url":"researcher\/2095468381_Jinghu_Pan","fullname":"Jinghu Pan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095490091,"url":"researcher\/2095490091_Weiguo_Wang","fullname":"Weiguo Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095526081,"url":"researcher\/2095526081_Junfeng_Li","fullname":"Junfeng Li","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Natural Hazards","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291690019_Building_probabilistic_models_of_fire_occurrence_and_fire_risk_zoning_using_logistic_regression_in_Shanxi_Province_China","usePlainButton":true,"publicationUid":291690019,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.72","url":"publication\/291690019_Building_probabilistic_models_of_fire_occurrence_and_fire_risk_zoning_using_logistic_regression_in_Shanxi_Province_China","title":"Building probabilistic models of fire occurrence and fire risk zoning using logistic regression in Shanxi Province, China","displayTitleAsLink":true,"authors":[{"id":2095468381,"url":"researcher\/2095468381_Jinghu_Pan","fullname":"Jinghu Pan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095490091,"url":"researcher\/2095490091_Weiguo_Wang","fullname":"Weiguo Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095526081,"url":"researcher\/2095526081_Junfeng_Li","fullname":"Junfeng Li","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Natural Hazards 01\/2016;  DOI:10.1007\/s11069-016-2160-0"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291690019_Building_probabilistic_models_of_fire_occurrence_and_fire_risk_zoning_using_logistic_regression_in_Shanxi_Province_China","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291690019_Building_probabilistic_models_of_fire_occurrence_and_fire_risk_zoning_using_logistic_regression_in_Shanxi_Province_China\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56ab1b92e5dae"},"id":"rgw16_56ab1b92e5dae","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291690019","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2059777116,"url":"researcher\/2059777116_Onur_Erdem","fullname":"Onur Erdem","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":31320601,"url":"researcher\/31320601_Murat_Olutas","fullname":"Murat Olutas","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":60001925,"url":"researcher\/60001925_Burak_Guzelturk","fullname":"Burak Guzelturk","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2008924001,"url":"researcher\/2008924001_Yusuf_Kelestemur","fullname":"Yusuf Kelestemur","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Journal of Physical Chemistry Letters","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291340346_Temperature-Dependent_Emission_Kinetics_of_Colloidal_Semiconductor_Nanoplatelets_Strongly_Modified_by_Stacking","usePlainButton":true,"publicationUid":291340346,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"7.46","url":"publication\/291340346_Temperature-Dependent_Emission_Kinetics_of_Colloidal_Semiconductor_Nanoplatelets_Strongly_Modified_by_Stacking","title":"Temperature-Dependent Emission Kinetics of Colloidal Semiconductor Nanoplatelets Strongly Modified by Stacking","displayTitleAsLink":true,"authors":[{"id":2059777116,"url":"researcher\/2059777116_Onur_Erdem","fullname":"Onur Erdem","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":31320601,"url":"researcher\/31320601_Murat_Olutas","fullname":"Murat Olutas","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":60001925,"url":"researcher\/60001925_Burak_Guzelturk","fullname":"Burak Guzelturk","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2008924001,"url":"researcher\/2008924001_Yusuf_Kelestemur","fullname":"Yusuf Kelestemur","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38633698,"url":"researcher\/38633698_Hilmi_Volkan_Demir","fullname":"Hilmi Volkan Demir","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of Physical Chemistry Letters 01\/2016;  DOI:10.1021\/acs.jpclett.5b02763"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291340346_Temperature-Dependent_Emission_Kinetics_of_Colloidal_Semiconductor_Nanoplatelets_Strongly_Modified_by_Stacking","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291340346_Temperature-Dependent_Emission_Kinetics_of_Colloidal_Semiconductor_Nanoplatelets_Strongly_Modified_by_Stacking\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab1b92e5dae"},"id":"rgw17_56ab1b92e5dae","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291340346","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2093958263,"url":"researcher\/2093958263_Elizabeth_Crosson","fullname":"Elizabeth Crosson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2094036446,"url":"researcher\/2094036446_Aram_W_Harrow","fullname":"Aram W. Harrow","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/290527271_Simulated_Quantum_Annealing_Can_Be_Exponentially_Faster_than_Classical_Simulated_Annealing","usePlainButton":true,"publicationUid":290527271,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/290527271_Simulated_Quantum_Annealing_Can_Be_Exponentially_Faster_than_Classical_Simulated_Annealing","title":"Simulated Quantum Annealing Can Be Exponentially Faster than Classical Simulated Annealing","displayTitleAsLink":true,"authors":[{"id":2093958263,"url":"researcher\/2093958263_Elizabeth_Crosson","fullname":"Elizabeth Crosson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2094036446,"url":"researcher\/2094036446_Aram_W_Harrow","fullname":"Aram W. Harrow","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/290527271_Simulated_Quantum_Annealing_Can_Be_Exponentially_Faster_than_Classical_Simulated_Annealing","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/290527271_Simulated_Quantum_Annealing_Can_Be_Exponentially_Faster_than_Classical_Simulated_Annealing\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab1b92e5dae"},"id":"rgw18_56ab1b92e5dae","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=290527271","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw15_56ab1b92e5dae"},"id":"rgw15_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=220270203&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":220270203,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":220270203,"publicationType":"inProceedings","linkId":"0ff74a100cf25dfdcf514e8e","fileName":"Infinite Latent Feature Models and the Indian Buffet Process","fileUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.60.3951&amp;rep=rep1&amp;type=pdf","name":"psu.edu","nameUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.60.3951&amp;rep=rep1&amp;type=pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw21_56ab1b92e5dae"},"id":"rgw21_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=220270203&linkId=0ff74a100cf25dfdcf514e8e&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw20_56ab1b92e5dae"},"id":"rgw20_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=220270203&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":2,"valueFormatted":"2","widgetId":"rgw22_56ab1b92e5dae"},"id":"rgw22_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=220270203","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw19_56ab1b92e5dae"},"id":"rgw19_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=220270203&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":220270203,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw24_56ab1b92e5dae"},"id":"rgw24_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=220270203&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":2,"valueFormatted":"2","widgetId":"rgw25_56ab1b92e5dae"},"id":"rgw25_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=220270203","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw23_56ab1b92e5dae"},"id":"rgw23_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=220270203&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Infinite Latent Feature Models\nand the Indian Buffet Process\nThomas L. Griffiths\nCognitive and Linguistic Sciences\nBrown University, Providence RI\ntom griffiths@brown.edu\nZoubin Ghahramani\nGatsby Computational Neuroscience Unit\nUniversity College London, London\nzoubin@gatsby.ucl.ac.uk\nAbstract\nWe define a probability distribution over equivalence classes of binary\nmatrices with a finite number of rows and an unbounded number of\ncolumns. This distribution is suitable for use as a prior in probabilistic\nmodels that represent objects using a potentially infinite array of features.\nWe identify a simple generative process that results in the same distribu-\ntion over equivalence classes, which we call the Indian buffet process.\nWe illustrate the use of this distribution as a prior in an infinite latent fea-\nturemodel, derivingaMarkovchainMonteCarloalgorithmforinference\nin this model and applying the algorithm to an image dataset.\n1 Introduction\nThe statistical models typically used in unsupervised learning draw upon a relatively small\nrepertoire of representations. The simplest representation, used in mixture models, asso-\nciates each object with a single latent class. This approach is appropriate when objects\ncan be partitioned into relatively homogeneous subsets. However, the properties of many\nobjects are better captured by representing each object using multiple latent features. For\ninstance, we could choose to represent each object as a binary vector, with entries indicat-\ning the presence or absence of each feature [1], allow each feature to take on a continuous\nvalue, representing objects with points in a latent space [2], or define a factorial model, in\nwhich each feature takes on one of a discrete set of values [3, 4].\nA critical question in all of these approaches is the dimensionality of the representation:\nhow many classes or features are needed to express the latent structure expressed by a\nset of objects. Often, determining the dimensionality of the representation is treated as a\nmodel selection problem, with a particular dimensionality being chosen based upon some\nmeasure of simplicity or generalization performance. This assumes that there is a single,\nfinite-dimensional representation that correctly characterizes the properties of the observed\nobjects. An alternative is to assume that the true dimensionality is unbounded, and that the\nobserved objects manifest only a finite subset of classes or features [5]. This alternative\nis pursued in nonparametric Bayesian models, such as Dirichlet process mixture models\n[6, 7, 8, 9]. In a Dirichlet process mixture model, each object is assigned to a latent class,\nand each class is associated with a distribution over observable properties. The prior dis-\ntribution over assignments of objects to classes is defined in such a way that the number\nof classes used by the model is bounded only by the number of objects, making Dirichlet\nprocess mixture models \u201cinfinite\u201d mixture models [10].\nThe prior distribution assumed in a Dirichlet process mixture model can be specified in"},{"page":2,"text":"terms of a sequential process called the Chinese restaurant process (CRP) [11, 12]. In the\nCRP, N customers enter a restaurant with infinitely many tables, each with infinite seating\ncapacity. The ith customer chooses an already-occupied table k with probability\nwhere mkis the number of current occupants, and chooses a new table with probability\n\u03b1\ni\u22121+\u03b1. Customers are exchangeable under this process: the probability of a particular\nseating arrangement depends only on the number of people at each table, and not the order\nin which they enter the restaurant.\nIf we replace customers with objects and tables with classes, the CRP specifies a distribu-\ntion over partitions of objects into classes. A partition is a division of the set of N objects\ninto subsets, where each object belongs to a single subset and the ordering of the subsets\ndoes not matter. Two assignments of objects to classes that result in the same division of\nobjects correspond to the same partition. For example, if we had three objects, the class\nassignments {c1,c2,c3} = {1,1,2} would correspond to the same partition as {2,2,1},\nsince all that differs between these two cases is the labels of the classes. A partition thus\ndefines an equivalence class of assignment vectors.\nThe distribution over partitions implied by the CRP can be derived by taking the limit of\nthe probability of the corresponding equivalence class of assignment vectors in a model\nwhere class assignments are generated from a multinomial distribution with a Dirichlet\nprior [9, 10]. In this paper, we derive an infinitely exchangeable distribution over infinite\nbinary matrices by pursuing this strategy of taking the limit of a finite model. We also de-\nscribe a stochastic process (the Indian buffet process, akin to the CRP) which generates this\ndistribution. Finally, we demonstrate how this distribution can be used as a prior in statisti-\ncal models in which each object is represented by a sparse subset of an unbounded number\nof features. Further discussion of the properties of this distribution, some generalizations,\nand additional experiments, are available in the longer version of this paper [13].\nmk\ni\u22121+\u03b1,\n2 A distribution on infinite binary matrices\nIn a latent feature model, each object is represented by a vector of latent feature values fi,\nand the observable properties of that object xiare generated from a distribution determined\nby its latent features. Latent feature values can be continuous, as in principal component\nanalysis (PCA) [2], or discrete, as in cooperative vector quantization (CVQ) [3, 4]. In the\nremainder of this section, we will assume that feature values are continuous. Using the ma-\ntrix F =?fT\ntrices conditioned on those features, p(X|F), where p(\u00b7) is a probability density function.\nThese distributions can be dealt with separately: p(F) specifies the number of features and\nthe distribution over values associated with each feature, while p(X|F) determines how\nthese features relate to the properties of objects. Our focus will be on p(F), showing how\nsuch a prior can be defined without limiting the number of features.\nWe can break F into two components: a binary matrix Z indicating which features are pos-\nsessed by each object, with zik= 1 if object i has feature k and 0 otherwise, and a matrix\nV indicating the value of each feature for each object. F is the elementwise product of Z\nand V, F = Z \u2297 V, as illustrated in Figure 1. In many latent feature models (e.g., PCA)\nobjects have non-zero values on every feature, and every entry of Z is 1. In sparse latent\nfeature models (e.g., sparse PCA [14, 15]) only a subset of features take on non-zero values\nfor each object, and Z picks out these subsets. A prior on F can be defined by specifying\npriors for Z and V, with p(F) = P(Z)p(V), where P(\u00b7) is a probability mass function.\nWe will focus on defining a prior on Z, since the effective dimensionality of a latent feature\nmodel is determined by Z. Assuming that Z is sparse, we can define a prior for infinite la-\ntent feature models by defining a distribution over infinite binary matrices. Our discussion\nof the Chinese restaurant process provides two desiderata for such a distribution: objects\n1fT\n2\u00b7\u00b7\u00b7 fT\nN\n?Tto indicate the latent feature values for all N objects, the model\nis specified by a prior over features, p(F), and a distribution over observed property ma-"},{"page":3,"text":"(c)\nobjects\nN\nK features\nobjects\nN\nK features\n0\n0\n0\n00\n0\n\u22120.1\n1.8\n\u22123.2\n0.9\n0.9\n\u22120.3\n0.2 \u22122.8\n1.4\nobjects\nN\nK features\n5\n0\n0\n0\n00\n0\n2\n5\n1\n1\n4\n4\n3\n3\n(a) (b)\nFigure 1: A binary matrix Z, as shown in (a), indicates which features take non-zero values.\nElementwise multiplication of Z by a matrix V of continuous values produces a represen-\ntation like (b). If V contains discrete values, we obtain a representation like (c).\nshould be exchangeable, and posterior inference should be tractable. It also suggests a\nmethod by which these desiderata can be satisfied: start with a model that assumes a finite\nnumber of features, and consider the limit as the number of features approaches infinity.\n2.1 A finite feature model\nWe have N objects and K features, and the possession of feature k by object i is indicated\nby a binary variable zik. The zikform a binary N \u00d7 K feature matrix, Z. Assume that\neach object possesses feature k with probability \u03c0k, and that the features are generated\nindependently. Under this model, the probability of Z given \u03c0 = {\u03c01,\u03c02,...,\u03c0K}, is\nP(Z|\u03c0) =\nK\n?\nk=1\nN\n?\ni=1\nP(zik|\u03c0k) =\nK\n?\nk=1\n\u03c0mk\nk(1 \u2212 \u03c0k)N\u2212mk,\n(1)\nwhere mk=?N\ni=1zikis the number of objects possessing feature k. We can define a prior\non \u03c0 by assuming that each \u03c0kfollows a beta distribution, to give\n\u03c0k|\u03b1 \u223c Beta(\u03b1\nzik|\u03c0k\u223c Bernoulli(\u03c0k)\nEach zikis independent of all other assignments, conditioned on \u03c0k, and the \u03c0kare gener-\nated independently. We can integrate out \u03c0 to obtain the probability of Z, which is\nK,1)\nP(Z)=\nK\n?\nk=1\n\u03b1\nK\u0393(mk+\u03b1\nK)\u0393(N \u2212 mk+ 1)\n\u0393(N + 1 +\u03b1\nK)\n.\n(2)\nThis distribution is exchangeable, since mkis not affected by the ordering of the objects.\n2.2 Equivalence classes\nIn order to find the limit of the distribution specified by Equation 2 as K \u2192 \u221e, we need to\ndefine equivalence classes of binary matrices \u2013 the analogue of partitions for class assign-\nments. Our equivalence classes will be defined with respect to a function on binary matri-\nces, lof(\u00b7). This function maps binary matrices to left-ordered binary matrices. lof(Z) is\nobtained by ordering the columns of the binary matrix Z from left to right by the magnitude\nof the binary number expressed by that column, taking the first row as the most significant\nbit. The left-ordering of a binary matrix is shown in Figure 2. In the first row of the left-\nordered matrix, the columns for which z1k= 1 are grouped at the left. In the second row,\nthe columns for which z2k= 1 are grouped at the left of the sets for which z1k= 1. This\ngrouping structure persists throughout the matrix.\nThe history of feature k at object i is defined to be (z1k,...,z(i\u22121)k). Where no object is\nspecified, we will use history to refer to the full history of feature k, (z1k,...,zNk). We"},{"page":4,"text":"lof\nFigure 2: Left-ordered form. A binary matrix is transformed into a left-ordered binary\nmatrix by the function lof(\u00b7). The entries in the left-ordered matrix were generated from\nthe Indian buffet process with \u03b1 = 10. Empty columns are omitted from both matrices.\nwill individuate the histories of features using the decimal equivalent of the binary numbers\ncorresponding to the column entries. For example, at object 3, features can have one of four\nhistories: 0, corresponding to a feature with no previous assignments, 1, being a feature for\nwhich z2k= 1 but z1k= 0, 2, being a feature for which z1k= 1 but z2k= 0, and 3, being\na feature possessed by both previous objects were assigned. Khwill denote the number of\nfeatures possessing the history h, with K0being the number of features for which mk= 0\nand K+=?2N\u22121\nequivalence class of a binary matrix Z, denoted [Z], is the set of binary matrices that are\nlof-equivalent to Z. lof-equivalence classes play the role for binary matrices that parti-\ntions play for assignment vectors: they collapse together all binary matrices (assignment\nvectors) that differ only in column ordering (class labels). lof-equivalence classes are pre-\nserved through permutation of the rows or the columns of a matrix, provided the same\npermutations are applied to the other members of the equivalence class. Performing infer-\nence at the level of lof-equivalence classes is appropriate in models where feature order\nis not identifiable, with p(X|F) being unaffected by the order of the columns of F. Any\nmodel in which the probability of X is specified in terms of a linear function of F, such\nas PCA or CVQ, has this property. The cardinality of the lof-equivalence class [Z] is\n?\n2.3 Taking the infinite limit\nh=1Khbeing the number of features for which mk> 0, so K = K0+K+.\nTwo binary matrices Y and Z are lof-equivalent if lof(Y) = lof(Z). The lof-\nK\nK0...K2N\u22121\n?\n=\nK!\nQ2N\u22121\nh=0\nKh!, where Khis the number of columns with full history h.\nUnderthedistributiondefinedbyEquation2, theprobabilityofaparticularlof-equivalence\nclass of binary matrices, [Z], is\nP([Z]) =\n?\nZ\u2208[Z]\nP(Z) =\nK!\n?2N\u22121\nh=0Kh!\nK\n?\nk=1\n\u03b1\nK\u0393(mk+\u03b1\nK)\u0393(N \u2212 mk+ 1)\n\u0393(N + 1 +\u03b1\nK)\n.\n(3)\nRearranging terms, and using the fact that \u0393(x) = (x \u2212 1)\u0393(x \u2212 1) for x > 1, we can\ncompute the limit of P([Z]) as K approaches infinity\nlim\nK\u2192\u221e\n\u03b1K+\n?2N\u22121\n\u03b1K+\n?2N\u22121\nh=1Kh!\n\u00b7\nK!\nK0!KK+\u00b7\n?\nN!\n?N\nexp{\u2212\u03b1HN}\nj=1(j +\u03b1\nK)\n?K\n\u00b7\nK+\n?\nK+\n?\n1\nj. This distribution is infinitely\nk=1\n(N \u2212 mk)!?mk\u22121\nj=1(j +\u03b1\nN!\nK)\n=\nh=1Kh!\n\u00b71\u00b7\u00b7\nk=1\n(N \u2212 mk)!(mk\u2212 1)!\nN!\n,\n(4)\nwhere HN is the Nth harmonic number, HN =?N\ndetails of this limit are provided in [13].\nj=1\nexchangeable, since neither Khnor mkare affected by the ordering on objects. Technical"},{"page":5,"text":"2.4 The Indian buffet process\nThe probability distribution defined in Equation 4 can be derived from a simple stochastic\nprocess. Due to the similarity to the Chinese restaurant process, we will also use a culinary\nmetaphor, appropriately adjusted for geography. Indian restaurants in London offer buffets\nwith an apparently infinite number of dishes. We will define a distribution over infinite\nbinary matrices by specifying how customers (objects) choose dishes (features).\nIn our Indian buffet process (IBP), N customers enter a restaurant one after another. Each\ncustomer encounters a buffet consisting of infinitely many dishes arranged in a line. The\nfirst customer starts at the left of the buffet and takes a serving from each dish, stopping\nafter a Poisson(\u03b1) number of dishes. The ith customer moves along the buffet, sampling\ndishes in proportion to their popularity, taking dish k with probabilitymk\nnumber of previous customers who have sampled that dish. Having reached the end of all\nprevious sampled dishes, the ith customer then tries a Poisson(\u03b1\nWe can indicate which customers chose which dishes using a binary matrix Z with N rows\nand infinitely many columns, where zik= 1 if the ith customer sampled the kth dish.\nUsing K(i)\n1\nto indicate the number of new dishes sampled by the ith customer, the proba-\nbility of any particular matrix being produced by the IBP is\ni, where mkis the\ni) number of new dishes.\nP(Z) =\n\u03b1K+\n?N\ni=1K(i)\n1!\nexp{\u2212\u03b1HN}\nK+\n?\nk=1\n(N \u2212 mk)!(mk\u2212 1)!\nN!\n.\n(5)\nThe matrices produced by this process are generally not in left-ordered form. These ma-\ntrices are also not ordered arbitrarily, because the Poisson draws always result in choices\nof new dishes that are to the right of the previously sampled dishes. Customers are not\nexchangeable under this distribution, as the number of dishes counted as K(i)\nupon the order in which the customers make their choices. However, if we only pay at-\ntention to the lof-equivalence classes of the matrices generated by this process, we obtain\nthe infinitely exchangeable distribution P([Z]) given by Equation 4:\n1\ndepends\nQN\nQ2N\u22121\nh=1\ni=1K(i)\n1!\nKh!matrices\ngenerated via this process map to the same left-ordered form, and P([Z]) is obtained by\nmultiplying P(Z) from Equation 5 by this quantity. A similar but slightly more compli-\ncated process can be defined to produce left-ordered matrices directly [13].\n2.5Conditional distributions\nTo define a Gibbs sampler for models using the IBP, we need to know the conditional\ndistribution on feature assignments, P(zik= 1|Z\u2212(ik)). In the finite model, where P(Z)\nis given by Equation 2, it is straightforward to compute this conditional distribution for any\nzik. Integrating over \u03c0kgives\nP(zik= 1|z\u2212i,k) =m\u2212i,k+\u03b1\nK\nN +\u03b1\nK\n,\n(6)\nwhere z\u2212i,kis the set of assignments of other objects, not including i, for feature k, and\nm\u2212i,kis the number of objects possessing feature k, not including i. We need only condi-\ntion on z\u2212i,krather than Z\u2212(ik)because the columns of the matrix are independent.\nIn the infinite case, we can derive the conditional distribution from the (exchangeable) IBP.\nChoosing an ordering on objects such that the ith object corresponds to the last customer\nto visit the buffet, we obtain\nP(zik= 1|z\u2212i,k) =m\u2212i,k\nN\n,\n(7)\nfor any k such that m\u2212i,k > 0. The same result can be obtained by taking the limit of\nEquation 6 as K \u2192 \u221e. The number of new features associated with object i should be"},{"page":6,"text":"drawn from a Poisson(\u03b1\nsame kind of limiting argument as that presented above.\nN) distribution. This can also be derived from Equation 6, using the\n3 A linear-Gaussian binary latent feature model\nTo illustrate how the IBP can be used as a prior in models for unsupervised learning, we\nderived and tested a linear-Gaussian latent feature model in which the features are binary.\nIn this case the feature matrix F reduces to the binary matrix Z. As above, we will start\nwith a finite model and then consider the infinite limit.\nIn our finite model, the D-dimensional vector of properties of an object i, xiis generated\nfrom a Gaussian distribution with mean ziA and covariance matrix \u03a3X = \u03c32\nziis a K-dimensional binary vector, and A is a K \u00d7 D matrix of weights. In matrix\nnotation, E [X] = ZA. If Z is a feature matrix, this is a form of binary factor analysis. The\ndistribution of X given Z, A, and \u03c3Xis matrix Gaussian with mean ZA and covariance\nmatrix \u03c32\nmean 0 and covariance matrix \u03c32\nXI, where\nXI, where I is the identity matrix. The prior on A is also matrix Gaussian, with\nAI. Integrating out A, we have\n1\n(2\u03c0)ND\/2\u03c3(N\u2212K)D\nX\n\u03c3KD\ntr(XT(I \u2212 Z(ZTZ +\u03c32\np(X|Z,\u03c3X,\u03c3A)=\nA\n|ZTZ +\n\u03c32\n\u03c32\nX\nAI|D\/2\nexp{\u2212\n1\n2\u03c32\nX\nX\n\u03c32\nA\nI)\u22121ZT)X)}. (8)\nThis result is intuitive: the exponentiated term is the difference between the inner product\nof X and its projection onto the space spanned by Z, regularized to an extent determined\nby the ratio of the variance of the noise in X to the variance of the prior on A. It follows\nthat p(X|Z,\u03c3X,\u03c3A) depends only on the non-zero columns of Z, and thus remains well-\ndefined when we take the limit as K \u2192 \u221e (for more details see [13]).\nWe can define a Gibbs sampler for this model by computing the full conditional distribution\nP(zik|X,Z\u2212(i,k),\u03c3X,\u03c3A) \u221d p(X|Z,\u03c3X,\u03c3A)P(zik|z\u2212i,k).\nThe two terms on the right hand side can be evaluated using Equations 8 and 7 respectively.\nThe Gibbs sampler is then straightforward. Assignments for features for which m\u2212i,k> 0\nare drawn from the distribution specified by Equation 9. The distribution over the number\nof new features for each object can be approximated by truncation, computing probabilities\nfor a range of values of K(i)\n1\nup to an upper bound. For each value, p(X|Z,\u03c3X,\u03c3A) can\nbe computed from Equation 8, and the prior on the number of new features is Poisson(\u03b1\nWe will demonstrate this Gibbs sampler for the infinite binary linear-Gaussian model on a\ndataset consisting of 100 240 \u00d7 320 pixel images. We represented each image, xi, using\na 100-dimensional vector corresponding to the weights of the mean image and the first 99\nprincipal components. Each image contained up to four everyday objects \u2013 a $20 bill, a\nKlein bottle, a prehistoric handaxe, and a cellular phone. Each object constituted a single\nlatent feature responsible for the observed pixel values. The images were generated by\nsampling a feature vector, zi, from a distribution under which each feature was present\nwith probability 0.5, and then taking a photograph containing the appropriate objects using\na LogiTech digital webcam. Sample images are shown in Figure 3 (a).\nThe Gibbs sampler was initialized with K+ = 1, choosing the feature assignments for\nthe first column by setting zi1 = 1 with probability 0.5. \u03c3A, \u03c3X, and \u03b1 were initially\nset to 0.5, 1.7, and 1 respectively, and then sampled by adding Metropolis steps to the\nMCMC algorithm. Figure 3 shows trace plots for the first 1000 iterations of MCMC for the\nnumber of features used by at least one object, K+, and the model parameters \u03c3A, \u03c3X, and\n\u03b1. All of these quantities stabilized after approximately 100 iterations, with the algorithm\n(9)\nN)."},{"page":7,"text":"(a)\n(Positive)\n(b)\n(Negative) (Negative)(Negative)\n0  0  0  0\n(c)\n0  1  0  0 1  1  1  0 1  0  1  1\n0 100 200300 400500 600700 800900 1000\n0\n5\n10\n K+\n0 100 200300 400500 6007008009001000\n0\n2\n4\n\u03b1\n0 100 200 300400 500600700 8009001000\n0\n1\n2\n\u03c3X\n0 100200 300400 500600 700800 9001000\n0\n1\n2\n\u03c3A\nIteration\nFigure 3: Data and results for the demonstration of the infinite linear-Gaussian binary\nlatent feature model. (a) Four sample images from the 100 in the dataset. Each image\nhad 320 \u00d7 240 pixels, and contained from zero to four everyday objects. (b) The posterior\nmean of the weights (A) for the four most frequent binary features from the 1000th sample.\nEach image corresponds to a single feature. These features perfectly indicate the presence\nor absence of the four objects. The first feature indicates the presence of the $20 bill,\nthe other three indicate the absence of the Klein bottle, the handaxe, and the cellphone.\n(c) Reconstructions of the images in (a) using the binary codes inferred for those images.\nThese reconstructions are based upon the posterior mean of A for the 1000th sample. For\nexample, the code for the first image indicates that the $20 bill is absent, while the other\nthree objects are not. The lower panels show trace plots for the dimensionality of the\nrepresentation (K+) and the parameters \u03b1, \u03c3X, and \u03c3Aover 1000 iterations of sampling.\nThe values of all parameters stabilize after approximately 100 iterations."},{"page":8,"text":"finding solutions with approximately seven latent features. The four most common features\nperfectly indicated the presence and absence of the four objects (shown in Figure 3 (b)), and\nthree less common features coded for slight differences in the locations of those objects.\n4 Conclusion\nWe have shown that the methods that have been used to define infinite latent class models\n[6, 7, 8, 9, 10, 11, 12] can be extended to models in which objects are represented in\nterms of a set of latent features, deriving a distribution on infinite binary matrices that can\nbe used as a prior for such models. While we derived this prior as the infinite limit of\na simple distribution on finite binary matrices, we have shown that the same distribution\ncan be specified in terms of a simple stochastic process \u2013 the Indian buffet process. This\ndistribution satisfies our two desiderata for a prior for infinite latent feature models: objects\nare exchangeable, and inference remains tractable. Our success in transferring the strategy\nof taking the limit of a finite model from latent classes to latent features suggests that a\nsimilar approach could be applied with other representations, expanding the forms of latent\nstructure that can be recovered through unsupervised learning.\nReferences\n[1] N. Ueda and K. Saito. Parametric mixture models for multi-labeled text. In Advances in Neural\nInformation Processing Systems 15, Cambridge, 2003. MIT Press.\n[2] I. T. Jolliffe. Principal component analysis. Springer, New York, 1986.\n[3] R. S. Zemel and G. E. Hinton. Developing population codes by minimizing description length.\nIn Advances in Neural Information Processing Systems 6. Morgan Kaufmann, San Francisco,\nCA, 1994.\n[4] Z. Ghahramani. Factorial learning and the EM algorithm. In Advances in Neural Information\nProcessing Systems 7. Morgan Kaufmann, San Francisco, CA, 1995.\n[5] C. E. Rasmussen and Z. Ghahramani. Occam\u2019s razor. In Advances in Neural Information\nProcessing Systems 13. MIT Press, Cambridge, MA, 2001.\n[6] C. Antoniak. Mixtures of Dirichlet processes with applications to Bayesian nonparametric\nproblems. The Annals of Statistics, 2:1152\u20131174, 1974.\n[7] M. D. Escobar and M. West. Bayesian density estimation and inference using mixtures. Journal\nof the American Statistical Association, 90:577\u2013588, 1995.\n[8] T. S. Ferguson. Bayesian density estimation by mixtures of normal distributions. In M. Rizvi,\nJ. Rustagi, and D. Siegmund, editors, Recent advances in statistics, pages 287\u2013302. Academic\nPress, New York, 1983.\n[9] R. M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of\nComputational and Graphical Statistics, 9:249\u2013265, 2000.\n[10] C. Rasmussen. The infinite Gaussian mixture model. In Advances in Neural Information Pro-\ncessing Systems 12. MIT Press, Cambridge, MA, 2000.\n[11] D. Aldous. Exchangeability and related topics. In\u00b4Ecole d\u2019\u00b4 et\u00b4 e de probabilit\u00b4 es de Saint-Flour,\nXIII\u20141983, pages 1\u2013198. Springer, Berlin, 1985.\n[12] J. Pitman. Combinatorial stochastic processes, 2002. Notes for Saint Flour Summer School.\n[13] T. L. Griffiths and Z. Ghahramani. Infinite latent feature models and the Indian buffet process.\nTechnical Report 2005-001, Gatsby Computational Neuroscience Unit, 2005.\n[14] A. d\u2019Aspremont, L. El Ghaoui, I. Jordan, and G. R. G. Lanckriet. A direct formulation for\nsparse PCA using semidefinite programming. In Advances in Neural Information Processing\nSystems 17. MIT Press, Cambridge, MA, 2005.\n[15] H. Zou, T. Hastie, and R. Tibshirani. Sparse principal component analysis. Journal of Compu-\ntational and Graphical Statistics, in press."}],"widgetId":"rgw26_56ab1b92e5dae"},"id":"rgw26_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=220270203&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw27_56ab1b92e5dae"},"id":"rgw27_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=220270203&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":220270203,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":220270203,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithSlurp","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":9100047,"url":"researcher\/9100047_Eric_P_Xing","fullname":"Eric P. Xing","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":49041600,"url":"researcher\/49041600_Qirong_Ho","fullname":"Qirong Ho","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2031310989,"url":"researcher\/2031310989_Pengtao_Xie","fullname":"Pengtao Xie","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2041017717,"url":"researcher\/2041017717_Wei_Dai","fullname":"Wei Dai","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":false,"isSlurp":true,"isNoText":false,"publicationType":"Article","publicationDate":"Dec 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/288889800_Strategies_and_Principles_of_Distributed_Machine_Learning_on_Big_Data","usePlainButton":true,"publicationUid":288889800,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/288889800_Strategies_and_Principles_of_Distributed_Machine_Learning_on_Big_Data","title":"Strategies and Principles of Distributed Machine Learning on Big Data","displayTitleAsLink":true,"authors":[{"id":9100047,"url":"researcher\/9100047_Eric_P_Xing","fullname":"Eric P. Xing","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":49041600,"url":"researcher\/49041600_Qirong_Ho","fullname":"Qirong Ho","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2031310989,"url":"researcher\/2031310989_Pengtao_Xie","fullname":"Pengtao Xie","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2041017717,"url":"researcher\/2041017717_Wei_Dai","fullname":"Wei Dai","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"The rise of Big Data has led to new demands for Machine Learning (ML) systems\nto learn complex models with millions to billions of parameters, that promise\nadequate capacity to digest massive datasets and offer powerful predictive\nanalytics thereupon. In order to run ML algorithms at such scales, on a\ndistributed cluster with 10s to 1000s of machines, it is often the case that\nsignificant engineering efforts are required --- and one might fairly ask if\nsuch engineering truly falls within the domain of ML research or not. Taking\nthe view that Big ML systems can benefit greatly from ML-rooted statistical and\nalgorithmic insights --- and that ML researchers should therefore not shy away\nfrom such systems design --- we discuss a series of principles and strategies\ndistilled from our recent efforts on industrial-scale ML solutions. These\nprinciples and strategies span a continuum from application, to engineering,\nand to theoretical research and development of Big ML systems and\narchitectures, with the goal of understanding how to make them efficient,\ngenerally-applicable, and supported with convergence and scaling guarantees.\nThey concern four key questions which traditionally receive little attention in\nML research: How to distribute an ML program over a cluster? How to bridge ML\ncomputation with inter-machine communication? How to perform such\ncommunication? What should be communicated between machines? By exposing\nunderlying statistical and algorithmic characteristics unique to ML programs\nbut not typically seen in traditional computer programs, and by dissecting\nsuccessful cases to reveal how we have harnessed these principles to design and\ndevelop both high-performance distributed ML software as well as\ngeneral-purpose ML frameworks, we present opportunities for ML researchers and\npractitioners to further shape and grow the area that lies between ML and\nsystems.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/288889800_Strategies_and_Principles_of_Distributed_Machine_Learning_on_Big_Data","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1512.09295","sourceName":"de.arxiv.org","hasSourceUrl":true},"publicationUid":288889800,"publicationUrl":"publication\/288889800_Strategies_and_Principles_of_Distributed_Machine_Learning_on_Big_Data","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/288889800_Strategies_and_Principles_of_Distributed_Machine_Learning_on_Big_Data\/links\/568b097708ae051f9afa8af7\/smallpreview.png","linkId":"568b097708ae051f9afa8af7","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=288889800&reference=568b097708ae051f9afa8af7&eventCode=&origin=publication_list","widgetId":"rgw31_56ab1b92e5dae"},"id":"rgw31_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=288889800&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":220270203,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/288889800_Strategies_and_Principles_of_Distributed_Machine_Learning_on_Big_Data\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["The majority of ML applications are supported by a moderate number of families of well-developed ML approaches, each of which embodies a continuum of technical elements from model design, to algorithmic innovation, and even to perfection of the software implementation, and which attracts ever-growing novel contributions from the research and development community. Modern examples of such approaches include Graphical Models [54] [28] [58], Regularized Bayesian models [72] [70] [71], Nonparametric Bayesian models [18] [49], Sparse Structured models [63] [27], Large-margin methods [8] [46], Deep learning [21] [29], Matrix Factorization [31] [41], Sparse Coding [44] [32], and Latent Space Modeling [4] [68]. A common ML practice that ensures mathematical soundness and outcome reproducibility is for practitioners and researchers to write an ML program (using any generic high-level programming language) for an application-specific instance of a particular ML approach (e.g. "],"widgetId":"rgw32_56ab1b92e5dae"},"id":"rgw32_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw30_56ab1b92e5dae"},"id":"rgw30_56ab1b92e5dae","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=288889800&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2008913272,"url":"researcher\/2008913272_Trevor_Campbell","fullname":"Trevor Campbell","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2073344635,"url":"researcher\/2073344635_Julian_Straub","fullname":"Julian Straub","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A279795402330117%401443719830319_m"},{"id":11213370,"url":"researcher\/11213370_John_W_Fisher_III","fullname":"John W. Fisher III","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":46470644,"url":"researcher\/46470644_Jonathan_P_How","fullname":"Jonathan P. How","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Oct 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283433373_Streaming_Distributed_Variational_Inference_for_Bayesian_Nonparametrics","usePlainButton":true,"publicationUid":283433373,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/283433373_Streaming_Distributed_Variational_Inference_for_Bayesian_Nonparametrics","title":"Streaming, Distributed Variational Inference for Bayesian Nonparametrics","displayTitleAsLink":true,"authors":[{"id":2008913272,"url":"researcher\/2008913272_Trevor_Campbell","fullname":"Trevor Campbell","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2073344635,"url":"researcher\/2073344635_Julian_Straub","fullname":"Julian Straub","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A279795402330117%401443719830319_m"},{"id":11213370,"url":"researcher\/11213370_John_W_Fisher_III","fullname":"John W. Fisher III","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":46470644,"url":"researcher\/46470644_Jonathan_P_How","fullname":"Jonathan P. How","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"This paper presents a methodology for creating streaming, distributed\ninference algorithms for Bayesian nonparametric (BNP) models. In the proposed\nframework, processing nodes receive a sequence of data minibatches, compute a\nvariational posterior for each, and make asynchronous streaming updates to a\ncentral model. In contrast to previous algorithms, the proposed framework is\ntruly streaming, distributed, asynchronous, learning-rate-free, and\ntruncation-free. The key challenge in developing the framework, arising from\nthe fact that BNP models do not impose an inherent ordering on their\ncomponents, is finding the correspondence between minibatch and central BNP\nposterior components before performing each update. To address this, the paper\ndevelops a combinatorial optimization problem over component correspondences,\nand provides an efficient solution technique. The paper concludes with an\napplication of the methodology to the DP mixture model, with experimental\nresults demonstrating its practical scalability and performance.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283433373_Streaming_Distributed_Variational_Inference_for_Bayesian_Nonparametrics","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Julian_Straub\/publication\/283433373_Streaming_Distributed_Variational_Inference_for_Bayesian_Nonparametrics\/links\/565332a508aeafc2aabb1013.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Julian_Straub","sourceName":"Julian Straub","hasSourceUrl":true},"publicationUid":283433373,"publicationUrl":"publication\/283433373_Streaming_Distributed_Variational_Inference_for_Bayesian_Nonparametrics","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/283433373_Streaming_Distributed_Variational_Inference_for_Bayesian_Nonparametrics\/links\/565332a508aeafc2aabb1013\/smallpreview.png","linkId":"565332a508aeafc2aabb1013","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=283433373&reference=565332a508aeafc2aabb1013&eventCode=&origin=publication_list","widgetId":"rgw34_56ab1b92e5dae"},"id":"rgw34_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=283433373&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"565332a508aeafc2aabb1013","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":220270203,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283433373_Streaming_Distributed_Variational_Inference_for_Bayesian_Nonparametrics\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Bayesian nonparametrics are also naturally suited to parallelization of data processing, due to the exchangeability, and thus conditional independence, they often exhibit via de Finetti's theorem. For example, labels from the Chinese Restaurant process [3] are rendered i.i.d. by conditioning on the underlying Dirichlet process (DP) random measure, and feature assignments from the Indian Buffet process [4] are rendered i.i.d. by conditioning on the underlying beta process (BP) random measure. "],"widgetId":"rgw35_56ab1b92e5dae"},"id":"rgw35_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw33_56ab1b92e5dae"},"id":"rgw33_56ab1b92e5dae","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=283433373&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":7196613,"url":"researcher\/7196613_Lancelot_F_James","fullname":"Lancelot F. James","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69976956,"url":"researcher\/69976956_Peter_Orbanz","fullname":"Peter Orbanz","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9164246,"url":"researcher\/9164246_Yee_Whye_Teh","fullname":"Yee Whye Teh","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Oct 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283279776_Scaled_subordinators_and_generalizations_of_the_Indian_buffet_process","usePlainButton":true,"publicationUid":283279776,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/283279776_Scaled_subordinators_and_generalizations_of_the_Indian_buffet_process","title":"Scaled subordinators and generalizations of the Indian buffet process","displayTitleAsLink":true,"authors":[{"id":7196613,"url":"researcher\/7196613_Lancelot_F_James","fullname":"Lancelot F. James","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69976956,"url":"researcher\/69976956_Peter_Orbanz","fullname":"Peter Orbanz","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9164246,"url":"researcher\/9164246_Yee_Whye_Teh","fullname":"Yee Whye Teh","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"We study random families of subsets of $\\mathbb{N}$ that are similar to\nexchangeable random partitions, but do not require constituent sets to be\ndisjoint: Each element of ${\\mathbb{N}}$ may be contained in multiple subsets.\nOne class of such objects, known as Indian buffet processes, has become a\npopular tool in machine learning. Based on an equivalence between Indian buffet\nand scale-invariant Poisson processes, we identify a random scaling variable\nwhose role is similar to that played in exchangeable partition models by the\ntotal mass of a random measure. Analogous to the construction of exchangeable\npartitions from normalized subordinators, random families of sets can be\nconstructed from randomly scaled subordinators. Coupling to a heavy-tailed\nscaling variable induces a power law on the number of sets containing the first\n$n$ elements. Several examples, with properties desirable in applications, are\nderived explicitly. A relationship to exchangeable partitions is made precise\nas a correspondence between scaled subordinators and Poisson-Kingman measures,\ngeneralizing a result of Arratia, Barbour and Tavare on scale-invariant\nprocesses.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283279776_Scaled_subordinators_and_generalizations_of_the_Indian_buffet_process","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Lancelot_James\/publication\/283279776_Scaled_subordinators_and_generalizations_of_the_Indian_buffet_process\/links\/5646af7d08aef646e6cdd493.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Lancelot_James","sourceName":"Lancelot F. James","hasSourceUrl":true},"publicationUid":283279776,"publicationUrl":"publication\/283279776_Scaled_subordinators_and_generalizations_of_the_Indian_buffet_process","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/283279776_Scaled_subordinators_and_generalizations_of_the_Indian_buffet_process\/links\/5646af7d08aef646e6cdd493\/smallpreview.png","linkId":"5646af7d08aef646e6cdd493","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=283279776&reference=5646af7d08aef646e6cdd493&eventCode=&origin=publication_list","widgetId":"rgw37_56ab1b92e5dae"},"id":"rgw37_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=283279776&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"5646af7d08aef646e6cdd493","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":220270203,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283279776_Scaled_subordinators_and_generalizations_of_the_Indian_buffet_process\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["i.e. in terms of the object\/feature interpretation, n k of the first n observed object possess feature k, and K n is the total number of distinct feature exhibited by the first n objects. The following version is due to Ghahramani, Griffiths, and Sollich [14]: Choose c, \u03b8 > 0. 1. The first row of Z contains K 1 \u223c Poisson(c) consecutive non-zero entries. "],"widgetId":"rgw38_56ab1b92e5dae"},"id":"rgw38_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw36_56ab1b92e5dae"},"id":"rgw36_56ab1b92e5dae","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=283279776&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":220270203,"publicationLink":"publication\/220270203_Infinite_Latent_Feature_Models_and_the_Indian_Buffet_Process","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw29_56ab1b92e5dae"},"id":"rgw29_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=220270203&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=276","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":276,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw28_56ab1b92e5dae"},"id":"rgw28_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=220270203&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/220270203_Infinite_Latent_Feature_Models_and_the_Indian_Buffet_Process","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1b92e5dae"},"id":"rgw2_56ab1b92e5dae","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":220270203},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=220270203&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1b92e5dae"},"id":"rgw1_56ab1b92e5dae","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"b7y7GFo75Q08AlaLGZ96DcgG4PG2LlFxrW3m0u9dalbSXvvnUPdYdY8mlSV0M7h71e3ORg7YLaFkkf6sdCbNmWsVaiO7s3sniekbn2oNWcQJtpVhKVqlkfXuZr3pDm4kmFTs1rNlEdzelWQ60PQevYzeofQCNiWTlFyIiJbTk1fmMB5FkGtwPnaH8Plj1gdumMbfsoJgHaFoYAvECdNI3mKAztnkwHEo4bMX96nKzysXdnn0W\/4UMOD7ST0mmusGuZvvgg+mqBRJHa05F0Vj\/xOHqgtB1yG86SGfOk\/1qqM=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/220270203_Infinite_Latent_Feature_Models_and_the_Indian_Buffet_Process\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Infinite Latent Feature Models and the Indian Buffet Process\" \/>\n<meta property=\"og:description\" content=\"We define a probability distribution over equivalence classes of binary ma- trices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/220270203_Infinite_Latent_Feature_Models_and_the_Indian_Buffet_Process\/links\/0ff74a100cf25dfdcf514e8e\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/220270203_Infinite_Latent_Feature_Models_and_the_Indian_Buffet_Process\" \/>\n<meta property=\"rg:id\" content=\"PB:220270203\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Infinite Latent Feature Models and the Indian Buffet Process\" \/>\n<meta name=\"citation_author\" content=\"Tom Griffiths\" \/>\n<meta name=\"citation_author\" content=\"Zoubin Ghahramani\" \/>\n<meta name=\"citation_conference_title\" content=\"Advances in Neural Information Processing Systems 18 [Neural Information Processing Systems, NIPS 2005, December 5-8, 2005, Vancouver, British Columbia, Canada]\" \/>\n<meta name=\"citation_publication_date\" content=\"2005\/01\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"Advances in neural information processing systems\" \/>\n<meta name=\"citation_issn\" content=\"1049-5258\" \/>\n<meta name=\"citation_volume\" content=\"18\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/220270203_Infinite_Latent_Feature_Models_and_the_Indian_Buffet_Process\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/220270203_Infinite_Latent_Feature_Models_and_the_Indian_Buffet_Process\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Conference Paper');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-5fabe608-c453-4c50-bd5f-c8b86154e680","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":496,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw39_56ab1b92e5dae"},"id":"rgw39_56ab1b92e5dae","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-5fabe608-c453-4c50-bd5f-c8b86154e680", "cdff595216caee0d47019c1f3a2fa2ecd5b3d864");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-5fabe608-c453-4c50-bd5f-c8b86154e680", "cdff595216caee0d47019c1f3a2fa2ecd5b3d864");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw40_56ab1b92e5dae"},"id":"rgw40_56ab1b92e5dae","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/220270203_Infinite_Latent_Feature_Models_and_the_Indian_Buffet_Process","requestToken":"FZVS+W7hPLHI4Hbi0znffK3F2zK3EyzWIHSnzQP5YBttmwJdYQhGQ\/e+IuQ89DCYQXEP8gxuSbKTpWZeOUiHGBMvPUGrFITHKA1CWTOwENx89TbgpcZZAfNAiJzUBwoUrK01v95x+j7\/hME54mz6I0pAiEFuYzfoKul+gp\/riXH2Wm+oNv5rzz2v8\/abTK0gb9efwx3UUflYKoL9RATQD8KcF8v6ye\/NHcvkp2OpwTdlUh52IvTqx8vAz621YnCgfC91ADN7P1bFyY+1KIcrpZ25GBFdQRz3H3+c7Vp7MJw=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=EJqOkuvHtbgQoow5gRiQshXl8upjj8UNNIqqTAGd1lpjjkfulqJB3zHTUMRbpmn2","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjIwMjcwMjAzX0luZmluaXRlX0xhdGVudF9GZWF0dXJlX01vZGVsc19hbmRfdGhlX0luZGlhbl9CdWZmZXRfUHJvY2Vzcw%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw42_56ab1b92e5dae"},"id":"rgw42_56ab1b92e5dae","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw41_56ab1b92e5dae"},"id":"rgw41_56ab1b92e5dae","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw43_56ab1b92e5dae"},"id":"rgw43_56ab1b92e5dae","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Conference Paper","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
