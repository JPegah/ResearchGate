<!DOCTYPE html> <html lang="en" class="" id="rgw44_56aba18d812a4"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="7GVGBncgsLmP3cclCdQlyzw+E2Ph0tJ7NLR5NzvEKuZ1FC0np4DB1ZY/EuTQtABcB1dloC51qXSJ8FfSDKhVlO8J7HKemO597S5JAYhclnX1fioe2kPdVoUNykb5TLpz6B3AjJAznET4HjiI7Q5tEJgi4etWasIjLLwgRPkIxdP3HIowtP8n9x3XFbrA9aqZBx1pOko/OkWO47eidwYG4otHXWCRp7IHhjxb9VUkHDACruQCxPmJm1x7nn/fF/yKZpjWOawEJ7AXagl51srf34WkIBOQXvKzwto6YX4Zr5M="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-b2cb5b31-265c-46fd-a781-2244c731fd68",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Sequential design of computer experiments for the estimation of a
probability of failure" />
<meta property="og:description" content="This paper deals with the problem of estimating the volume of the excursion set of a function f:ℝd
→ℝ above a given threshold, under a probability measure on ℝd
that is assumed to be known. In the..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure/links/00b4951ad79fb75b24000000/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure" />
<meta property="rg:id" content="PB:46587486" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1007/s11222-011-9241-4" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Sequential design of computer experiments for the estimation of a
probability of failure" />
<meta name="citation_author" content="Julien Bect" />
<meta name="citation_author" content="David Ginsbourger" />
<meta name="citation_author" content="Ling Li" />
<meta name="citation_author" content="Victor Picheny" />
<meta name="citation_author" content="Emmanuel Vazquez" />
<meta name="citation_publication_date" content="2010/09/27" />
<meta name="citation_journal_title" content="Statistics and Computing" />
<meta name="citation_issn" content="0960-3174" />
<meta name="citation_volume" content="22" />
<meta name="citation_issue" content="3" />
<meta name="citation_doi" content="10.1007/s11222-011-9241-4" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/David_Ginsbourger/publication/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure/links/00b4951ad79fb75b24000000.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/215868066921738/styles/pow/publicliterature/FigureList.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Sequential design of computer experiments for the estimation of a
probability of failure (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Sequential design of computer experiments for the estimation of a
probability of failure on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56aba18d812a4" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56aba18d812a4" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56aba18d812a4">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1007%2Fs11222-011-9241-4&rft.atitle=Sequential%20design%20of%20computer%20experiments%20for%20the%20estimation%20of%20a%0Aprobability%20of%20failure&rft.title=Statistics%20and%20Computing&rft.jtitle=Statistics%20and%20Computing&rft.volume=22&rft.issue=3&rft.date=2010&rft.issn=0960-3174&rft.au=Julien%20Bect%2CDavid%20Ginsbourger%2CLing%20Li%2CVictor%20Picheny%2CEmmanuel%20Vazquez&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Sequential design of computer experiments for the estimation of a
probability of failure</h1> <meta itemprop="headline" content="Sequential design of computer experiments for the estimation of a
probability of failure">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure/links/00b4951ad79fb75b24000000/smallpreview.png">  <div id="rgw8_56aba18d812a4" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56aba18d812a4"> <a href="researcher/14088780_Julien_Bect" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Julien Bect" alt="Julien Bect" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Julien Bect</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw10_56aba18d812a4">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/14088780_Julien_Bect"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Julien Bect" alt="Julien Bect" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/14088780_Julien_Bect" class="display-name">Julien Bect</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56aba18d812a4" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/David_Ginsbourger" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272766523932678%401442044015317_m/David_Ginsbourger.png" title="David Ginsbourger" alt="David Ginsbourger" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">David Ginsbourger</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw12_56aba18d812a4" data-account-key="David_Ginsbourger">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/David_Ginsbourger"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272766523932678%401442044015317_l/David_Ginsbourger.png" title="David Ginsbourger" alt="David Ginsbourger" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/David_Ginsbourger" class="display-name">David Ginsbourger</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Universitaet_Bern" title="Universität Bern">Universität Bern</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw13_56aba18d812a4"> <a href="researcher/47265550_Ling_Li" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Ling Li" alt="Ling Li" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Ling Li</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw14_56aba18d812a4">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/47265550_Ling_Li"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Ling Li" alt="Ling Li" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/47265550_Ling_Li" class="display-name">Ling Li</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw15_56aba18d812a4"> <a href="researcher/19512962_Victor_Picheny" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Victor Picheny" alt="Victor Picheny" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Victor Picheny</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw16_56aba18d812a4">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/19512962_Victor_Picheny"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Victor Picheny" alt="Victor Picheny" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/19512962_Victor_Picheny" class="display-name">Victor Picheny</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw17_56aba18d812a4"> <a href="researcher/26760014_Emmanuel_Vazquez" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Emmanuel Vazquez" alt="Emmanuel Vazquez" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Emmanuel Vazquez</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw18_56aba18d812a4">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/26760014_Emmanuel_Vazquez"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Emmanuel Vazquez" alt="Emmanuel Vazquez" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/26760014_Emmanuel_Vazquez" class="display-name">Emmanuel Vazquez</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/0960-3174_Statistics_and_Computing"><span itemprop="name">Statistics and Computing</span></a> </span>    (Impact Factor: 1.62).     <meta itemprop="datePublished" content="2010-09">  09/2010;  22(3).    DOI:&nbsp;10.1007/s11222-011-9241-4           <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1009.5177" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw19_56aba18d812a4" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>This paper deals with the problem of estimating the volume of the excursion set of a function f:ℝd<br />
&rarr;ℝ above a given threshold, under a probability measure on ℝd<br />
that is assumed to be known. In the industrial world, this corresponds to the problem of estimating a probability of failure of a system. When only an expensive-to-simulate model of the system is available, the budget for simulations is usually severely limited and therefore classical Monte Carlo methods ought to be avoided. One of the main contributions of this article is to derive SUR (stepwise uncertainty reduction) strategies from a Bayesian formulation of the problem of estimating a probability of failure. These sequential strategies use a Gaussian process model of f and aim at performing evaluations of f as efficiently as possible to infer the value of the probability of failure. We compare these strategies to other strategies also based on a Gaussian process model for estimating a probability of failure.</div> </p>  </div>  </div>   </div>     <div id="rgw20_56aba18d812a4" class="figure-carousel"> <div class="carousel-hd"> Figures in this publication </div> <div class="carousel-bd"> <ul class="clearfix">  <li> <a href="/figure/46587486_fig1_Fig-5-Left-mesh-plot-of-the-performance-function-f-corresponding-to-the-four-branch" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Fig. 5 Left: mesh plot of the performance function f corresponding to..." data-key="46587486_fig1_Fig-5-Left-mesh-plot-of-the-performance-function-f-corresponding-to-the-four-branch"> <img class="fig" src="https://www.researchgate.net/profile/David_Ginsbourger/publication/46587486/figure/fig1/Fig-5-Left-mesh-plot-of-the-performance-function-f-corresponding-to-the-four-branch_small.png" alt="Fig. 5 Left: mesh plot of the performance function f corresponding to..." title="Fig. 5 Left: mesh plot of the performance function f corresponding to..."/> </a> </li>  </ul> </div> </div> <div class="action-container"> <div id="rgw21_56aba18d812a4" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw35_56aba18d812a4">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw36_56aba18d812a4">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/David_Ginsbourger/publication/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure/links/00b4951ad79fb75b24000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/David_Ginsbourger">David Ginsbourger</a>   </span>  </div>  <div class="social-share-container"><div id="rgw38_56aba18d812a4" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw39_56aba18d812a4" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw40_56aba18d812a4" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw41_56aba18d812a4" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw42_56aba18d812a4" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw43_56aba18d812a4" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw37_56aba18d812a4" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FDavid_Ginsbourger%2Fpublication%2F46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure%2Flinks%2F00b4951ad79fb75b24000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw34_56aba18d812a4"  itemprop="articleBody">  <p>Page 1</p> <p>arXiv:1009.5177v1  [stat.CO]  27 Sep 2010<br />Sequential design of computer experiments for the<br />estimation of a probability of failure<br />Julien Bect⋆· David Ginsbourger · Ling Li ·<br />Victor Picheny · Emmanuel Vazquez⋆<br />Abstract This paper deals with the problem of estimating the volume of the excursion<br />set of a function f : Rd→ R above a given threshold, under a probability measure<br />on Rdthat is assumed to be known. In the industrial world, this corresponds to the<br />problem of estimating a probability of failure of a system. When only an expensive-to-<br />simulate model of the system is available, the budget for simulations is usually severely<br />limited and therefore classical Monte Carlo methods ought to be avoided. One of the<br />main contributions of this article is to derive SUR (stepwise uncertainty reduction)<br />strategies from a Bayesian-theoretic formulation of the problem of estimating a prob-<br />ability of failure. These sequential strategies use a Gaussian process model of f and<br />aim at performing evaluations of f as efficiently as possible to infer the value of the<br />probability of failure. We compare these strategies to other strategies also based on a<br />Gaussian process model for estimating a probability of failure.<br />Keywords Computer experiments · Sequential design · Gaussian processes ·<br />Probability of failure · Stepwise uncertainty reduction<br />⋆corresponding authors<br />J. Bect, L. Li, and E. Vazquez<br />SUPELEC, Gif-sur-Yvette, France.<br />E-mail: {firstname}.{lastname}@supelec.fr<br />V. Picheny<br />Ecole Centrale Paris, Chatenay-Malabry, France.<br />E-mail: victor.picheny@ecp.fr<br />D. Ginsbourger<br />Institute of Mathematical Statistics and Actuarial Science, University of Bern, Switzerland.<br />E-mail: david.ginsbourger@stat.unibe.ch</p>  <p>Page 2</p> <p>2<br />1 Introduction<br />The design of a system or a technological product has to take into account the fact<br />that some design parameters are subject to unknown variations that may affect the<br />reliability of the system. In particular, it is important to estimate the probability of<br />the system working under abnormal or dangerous operating conditions due to random<br />dispersions of its characteristic parameters. The probability of failure of a system is<br />usually expressed as the probability of the excursion set of a function above a fixed<br />threshold. More precisely, let f be a measurable real function defined over a probability<br />space (X,B(X),PX), with X ⊆ Rd, and let u ∈ R be a threshold. The problem to be<br />considered in this paper is the estimation of the volume, under PX, of the excursion<br />set<br />Γ := {x ∈ X : f(x) &gt; u}<br />(1)<br />of the function f above the level u. In the context of robust design, the volume α :=<br />PX(Γ) can be viewed as the probability of failure of a system. Then, X plays the<br />role of an input or factor space, the probability PX models the uncertainty on the<br />inputs or the factors, and f is some performance function derived from the outputs<br />of the system, whose evaluation may involve complex and time-consuming computer<br />simulations. When f is expensive to evaluate, the estimation of α must be carried out<br />with a restricted number of evaluations of f, generally excluding the estimation of the<br />probability of excursion by a Monte Carlo approach. Indeed, consider the empirical<br />estimator<br />αm :=1<br />m<br />m<br />?<br />i=1<br />1{f(Xi)&gt;u},(2)<br />where the Xis are independent random variables with distribution PX. According to<br />the strong law of large numbers, the estimator αm converges to α almost surely when<br />m increases. Moreover, it is an unbiased estimator of α, i.e. E(αm) = α. Its mean<br />square error is<br />E?(αm− α)2?=1<br />If the probability of failure α is small, then the standard deviation of αm is approxi-<br />mately (α/m)1/2. To achieve a given standard deviation δα thus requires approximately<br />1/(δ2α) evaluations, which can be prohibitively high if α is small. By way of illustra-<br />tion, if α = 2 × 10−3and δ = 0.1, we obtain m = 50000. If one evaluation of f takes,<br />say, one minute, then the entire estimation procedure will take about 35 days to com-<br />mα?1 − α?.<br />plete. Of course, a host of refined random sampling methods have been proposed to<br />improve over the basic Monte Carlo convergence rate; for instance, methods based on<br />importance sampling with cross-entropy (Rubinstein and Kroese, 2004), subset sam-<br />pling (Au and Beck, 2001) or line sampling (Pradlwarter et al., 2007). They will not<br />be considered here for the sake of brevity and because the required number of function<br />evaluations is still very high.</p>  <p>Page 3</p> <p>3<br />Until recently, all the methods that do not require a large number of evaluations<br />of f were based on the use of parametric approximations for either the function f<br />itself or the boundary ∂Γ of Γ. The so-called response surface method falls in the first<br />category (see, e.g., Bucher and Bourgund, 1990; Rajashekhar and Ellingwood, 1993,<br />and references therein). The most popular approaches in the second category are the<br />first- and second-order reliability method (FORM and SORM), which are based on a<br />linear or quadratic approximation of ∂Γ around the most probable failure point (see,<br />e.g., Bjerager, 1990). In all these methods, the accuracy of the estimator depends on<br />the actual shape of either f or ∂Γ and its resemblance to the approximant: they do<br />not provide statistically consistent estimators of the probability of failure.<br />This paper focuses on sequential sampling strategies based on Gaussian processes<br />and kriging, which can been seen as a non-parametric approximation method. Several<br />strategies of this kind have been proposed recently in the literature by Ranjan et al.<br />(2008), Bichon et al. (2008), Picheny et al. (2010) and Echard et al. (2010). The idea<br />is that the Gaussian process model, which captures prior knowledge about the un-<br />known function f, makes it possible to assess the uncertainty about the position<br />of Γ given a set of evaluation results. This line of research has its roots in the<br />field of design and analysis of computer experiments (see, e.g., Sacks et al., 1989;<br />Currin et al., 1991; Welch et al., 1992; Oakley and O’Hagan, 2002, 2004; Oakley, 2004;<br />Bayarri et al., 2007). More specifically, kriging-based sequential strategies for the esti-<br />mation of a probability of failure are closely related to the field of Bayesian global op-<br />timization (Mockus et al., 1978; Mockus, 1989; Jones et al., 1998; Villemonteix, 2008;<br />Villemonteix et al., 2009; Ginsbourger, 2009).<br />The contribution of this paper is twofold. First, we introduce a Bayesian decision-<br />theoretic framework from which the theoretical form of an optimal strategy for the<br />estimation of a probability of failure can be derived. One-step lookahead sub-optimal<br />strategies are then proposed1, which are suitable for numerical evaluation and imple-<br />mentation on computers. These strategies will be called SUR (stepwise uncertainty<br />reduction) strategies in reference to the work of D. Geman and its collaborators (see,<br />e.g. Fleuret and Geman, 1999). Second, we provide a review in a unified framework<br />of all the kriging-based strategies proposed so far in the literature and compare them<br />numerically with the SUR strategies proposed in this paper.<br />The outline of the paper is as follows. Section 2 introduces the Bayesian framework<br />and recalls the basics of dynamic programming and Gaussian processes. Section 3 in-<br />troduces SUR strategies, from the decision-theoretic underpinnings, down to the imple-<br />mentation level. Section 4 provides a review of other kriging-based strategies proposed<br />in the literature. Section 5 provides some illustrations and reports an empirical com-<br />1Preliminary accounts of this work have been presented in Vazquez and Piera-Martinez<br />(2007) and Vazquez and Bect (2009).</p>  <p>Page 4</p> <p>4<br />parison of these sampling criteria. Finally, Section 6 presents conclusions and offers<br />perspectives for future work.<br />2 Bayesian decision-theoretic framework<br />2.1 Bayes risk and sequential strategies<br />Let f be a continuous function. We shall assume that f corresponds to a computer<br />program whose output is not a closed-form expression of the inputs. Our objective is<br />to obtain a numerical approximation of the probability of failure<br />α(f) = PX{x ∈ X : f(x) &gt; u} =<br />?<br />X<br />1f&gt;udPX,(3)<br />where 1f&gt;ustands for the characteristic function of the excursion set Γ, such that<br />for any x ∈ X, 1f&gt;u(x) equals one if x ∈ Γ and zero otherwise. The approximation<br />of α(f) has to be built from a set of computer experiments, where an experiment<br />simply consists in choosing an x ∈ X and computing the value of f at x. The result<br />of a pointwise evaluation of f carries information about f and quantities depending<br />on f and, in particular, about 1f&gt;uand α(f). In the context of expensive computer<br />experiments, we shall also suppose that the number of evaluations is limited. Thus,<br />the estimation of α(f) must be carried out using a fixed number, say N, of evaluations<br />of f.<br />A sequential non-randomized algorithm to estimate α(f) with a budget of N eval-<br />uations is a pair (XN, ? αN),<br />XN: f ?→ XN(f) = (X1(f),X2(f),...,XN(f)) ∈ XN,<br />? αN: f ?→ ? αN(f) ∈ R+,<br />with the following properties:<br />a) There exists x1∈ X such that X1(f) = x1, i.e. X1does not depend on f.<br />b) Let Zn(f) = f(Xn(f)), 1 ≤ n ≤ N. For all 1 ≤ n &lt; N, Xn+1(f) depends measur-<br />ably2on In(f), where In = ((X1,Z1),...,(Xn,Zn)).<br />c) ? αN(f) depends measurably on IN(f).<br />and ? αN will be called an estimator. The algorithm (XN, ? αN) describes a sequence<br />prior to any evaluation; for each n = 1,...,N − 1, the algorithm uses information<br />In(f) to choose the next evaluation point Xn+1(f); the estimation ? αN(f) of α(f) is<br />further restricted: for instance, when K computer simulations can be run in parallel,<br />The mapping XNwill be referred to as a strategy, or policy, or design of experiments,<br />of decisions, made from an increasing amount of information: X1(f) = x1is chosen<br />the terminal decision. In some applications, the class of sequential algorithms must be<br />2i.e., there is a measurable map ϕn: (X × R)n→ X such that Xn= ϕn◦ In</p>  <p>Page 5</p> <p>5<br />algorithms that query batches of K evaluations at a time may be preferred (see, e.g.<br />Ginsbourger et al., 2010). In this paper no such restriction is imposed.<br />The choice of the estimator ? αNwill be addressed in Section 2.4: for now, we simply<br />strategy XN; that is, one that will produce a good final approximation ? αN(f) of α(f).<br />Given a loss function L : R×R → R, we define the error of approximation of a strategy<br />XN∈ ANon f as ǫ(XN,f) = L(? αN(f),α(f)). In this paper, we shall consider the<br />We adopt a Bayesian approach to this decision problem: the unknown function f<br />assume that an estimator has been chosen, and focus on the problem of finding a good<br />Let ANbe the class of all strategies XNthat query sequentially N evaluations of f.<br />quadratic loss function, so that ǫ(XN,f) = (? αN(f) − α(f))2.<br />is considered as a sample path of a real-valued random process ξ defined on some<br />probability space (Ω,B,P0) with parameter in x ∈ X, and a good strategy is a strat-<br />egy that achieves, or gets close to, the Bayes risk rB := infXN∈ANE0(ǫ(XN,ξ)),<br />where E0 denotes the expectation with respect to P0. From a subjective Bayesian<br />point of view, the stochastic model ξ is a representation of our uncertain initial knowl-<br />edge about f. From a more pragmatic perspective, the prior distribution can be seen<br />as a tool to define a notion of a good strategy in an average sense. Another interest-<br />ing route, not followed in this paper, would have been to consider the minimax risk<br />infXN∈ANmaxfE0(ǫ(XN,ξ)) over some class of functions.<br />Notations. From now on, we shall consider the stochastic model ξ instead of<br />the deterministic function f and, for abbreviation, the explicit dependence on ξ will be<br />dropped when no there is no risk of confusion; e.g., ? αNwill denote the random variable<br />and En to denote respectively the σ-algebra generated by In, the conditional distribu-<br />tion P0(· | Fn) and the conditional expectation E0(· | Fn). Note that the dependence<br />of Xn+1on In can be rephrased by saying that Xn+1is Fn-measurable. Recall that<br />En(Z) is Fn-measurable, and thus can be seen as a measurable function of In, for any<br />random variable Z.<br />? αN(ξ), Xnwill denote the random variable Xn(ξ), etc. We will use the notations Fn, Pn<br />2.2 Optimal and k-step lookahead strategies<br />It is well-known (see, e.g., Berry and Fristedt, 1985; Mockus, 1989; Bertsekas, 1995)<br />that an optimal strategy for such a finite horizon problem, i.e. a strategy X⋆<br />such that E0(ǫ(X⋆<br />N,ξ)) = rB, can be formally obtained by dynamic programming:<br />let RN = EN(ǫ(XN,ξ)) = EN<br />backward induction<br />N∈ AN<br />?(? αN− α)2?<br />?Rn+1| Xn+1= x?,<br />denote the terminal risk and define by<br />Rn = min<br />x∈XEn<br />n = N − 1,...,0. (4)<br />To get an insight into (4), notice that Rn+1, n = 0,...,N − 1, depends measurably<br />on In+1= (In,Xn+1,Zn+1), so that En<br />?Rn+1| Xn+1= x?<br />is in fact an expectation</p>  <p>Page 6</p> <p>6<br />with respect to Zn+1, and Rn is an Fn-measurable random variable. Then, we have<br />R0= rB, and the strategy X⋆<br />Ndefined by<br />?Rn+1| Xn+1= x?,<br />is optimal3. It is crucial to observe here that, for this dynamic programming problem,<br />X⋆<br />n+1= argmin<br />x∈X<br />En<br />n = 1,...,N − 1, (5)<br />both the space of possible actions and the space of possible outcomes at each step are<br />continuous, and the state space (X × R)nat step n is of dimension n(d + 1). Any<br />direct attempt at solving (4)–(5) numerically, over an horizon N of more than a few<br />steps, will suffer from the curse of dimensionality.<br />Using (4), the optimal strategy can be expanded as<br />?<br />A very general approach to construct sub-optimal—but hopefully good—strategies is<br />X⋆<br />n+1= argmin<br />x∈X<br />En<br />min<br />Xn+2En+1... min<br />XN<br />EN−1RN<br />??? Xn+1= x<br />?<br />.<br />to truncate this expansion after k terms, replacing the exact risk Rn+kby any available<br />surrogate? Rn+k. Examples of such surrogates will be given in Sections 3 and 4. The<br />?<br />resulting strategy,<br />Xn+1= argmin<br />x∈X<br />En<br />min<br />Xn+2<br />En+1... min<br />Xn+k<br />En+k−1? Rn+k<br />??? Xn+1= x<br />?<br />. (6)<br />is called a k-step lookahead strategy (see, e.g., Bertsekas, 1995, Section 6.3). Note that<br />both the optimal strategy (5) and the k-step lookahead strategy implicitly define a<br />sampling criterion Jn(x), Fn-measurable, the minimum of which indicates the next<br />evaluation to be performed. For instance, in the case of the k-step lookahead strategy,<br />the sampling criterion is<br />Jn(x) = En<br />?<br />min<br />Xn+2<br />En+1... min<br />Xn+k<br />En+k−1? Rn+k<br />??? Xn+1= x<br />?<br />.<br />In the rest of the paper, we restrict our attention to the class of one-step lookahead<br />strategies, which is, as we shall see in Section 3, large enough to provide very efficient<br />algorithms. We leave aside the interesting question of whether more complex k-step<br />lookahead strategies (with k ≥ 2) could provide a significant improvement over the<br />strategies examined in this paper.<br />Remark 1 In practice, the analysis of a computer code usually begins with an ex-<br />ploratory phase, during which the output of the code is computed on a space-filling<br />design of size n0&lt; N (see, e.g., Santner et al., 2003). Such an exploratory phase will<br />3Proving rigorously that, for a given P0 and ? αN, equations (4) and (5) actually define<br />this paper. This can be done for instance, in the case of a Gaussian process with continuous<br />covariance function (as considered later), by proving that x ?→ En(Rn+1| Xn+1(ξ) = x) is a<br />continuous function on X and then using a measurable selection theorem.<br />a (measurable!) strategy X⋆<br />N∈ AN is technical problem that is not of primary interest in</p>  <p>Page 7</p> <p>7<br />be colloquially referred to as the initial design. Sequential strategies such as (5) and (6)<br />are meant to be used after this initial design, at steps n0+ 1, ..., N. An important<br />(and largely open) question is the choice of the size n0of the initial design, for a given<br />global budget N.<br />2.3 Gaussian process priors<br />Restricting ξ to be a Gaussian process makes it possible to deal with the conditional<br />distributions Pn and conditional expectations En that appear in the strategies above.<br />The idea of modeling an unknown function f by a Gaussian process has originally<br />been introduced circa 1960 in time series analysis (Parzen, 1962), optimization theory<br />(Kushner, 1964) and geostatistics (see, e.g., Chil` es and Delfiner, 1999, and the refer-<br />ences therein). Today, the Gaussian process model plays a central role in the design<br />and analysis of computer experiments (see, e.g., Sacks et al., 1989; Currin et al., 1991;<br />Welch et al., 1992; Santner et al., 2003). Recall that the distribution of a Gaussian pro-<br />cess ξ is uniquely determined by its mean function m(x) := E0(ξ(x)), x ∈ X, and its<br />covariance function k(x,y) := E0((ξ(x) − m(x))(ξ(y) − m(y))), x,y ∈ X. Hereafter,<br />we shall use the notation ξ ∼ GP(m, k) to say that ξ is a Gaussian process with mean<br />function m and covariance function k.<br />Let ξ ∼ GP(0, k) be a zero-mean Gaussian process. The best linear unbiased<br />predictor (BLUP) of ξ(x) from observations ξ(xi), i = 1,...,n, also called the kriging<br />predictor of ξ(x), is the orthogonal projection<br />?ξ(x;xn) :=<br />n<br />?<br />i=1<br />λi(x;xn)ξ(xi) (7)<br />of ξ(x) onto span{ξ(xi),i = 1,...,n}. Here, the notation xnstands for the set of<br />points xn= {x1,...,xn}. The weights λi(x;xn) are the solutions of a system of linear<br />equations<br />k(xn,xn)λ(x;xn) = k(x,xn) (8)<br />where k(xn,xn) stands for the n × n covariance matrix of the observation vector,<br />λ(x;xn) = (λ1(x;xn),...,λn(x;xn))T, and k(x,xn) is a vector with entries k(x,xi).<br />The function x ?→?ξ(x;xn) conditioned on ξ(x1) = f(x1),...,ξ(xn) = f(xn), is de-<br />Santner et al., 2003). The covariance function of the error of prediction, also called<br />terministic, and provides a cheap surrogate model for the true function f (see, e.g.,<br />kriging covariance is given by<br />k(x,y;xn) := cov<br />?<br />ξ(x) −?ξ(x;xn),ξ(y) −?ξ(y;xn)<br />= k(x,y) −<br />i<br />?<br />?<br />λi(x;xn)k(y,xi). (9)</p>  <p>Page 8</p> <p>8<br />The variance of the prediction error, also called the kriging variance, is defined as<br />σ2(x;xn) = kn(x,x;xn). One fundamental property of a zero-mean Gaussian process<br />is the following (see, e.g., Chil` es and Delfiner, 1999, Chapter 3) :<br />Proposition 1 If ξ ∼ GP(0, k), then the random process ξ conditioned on the σ-<br />algebra Fngenerated by ξ(x1),...,ξ(xn), which we shall denote by ξ | Fn, is a Gaussian<br />process with mean?ξ(·; xn) given by (7)-(8) and covariance k ( ·, ·; xn) given by (9).<br />all x ∈ X.<br />In the domain of computer experiments, the mean of a Gaussian process is generally<br />In particular,?ξ(x;xn) = E0<br />?ξ(x) | Fn<br />?<br />is the best Fn-measurable predictor of ξ(x), for<br />written as a linear parametric function<br />m(·) = βTp(·), (10)<br />where β is a vector of unknown parameters, and p = (p1,...,pl)Tis an l-dimensional<br />vector of functions (in practice, polynomials). The simplest case is when the mean<br />function is assumed to be an unknown constant m, in which case we can take β = m<br />and p : x ∈ X ?→ 1. The covariance function is generally written as a translation-<br />invariant function:<br />k : (x,y) ∈ X2?→ σ2ρθ(x − y),<br />where σ2is the variance of the (stationary) Gaussian process and ρθis the correlation<br />function, which generally depends on a parameter vector θ. When the mean is written<br />under the form (10), the kriging predictor is again a linear combination of the obser-<br />vations, as in (7), and the weights λi(x;xn) are again solutions of a system of linear<br />equations (see, e.g., Chil` es and Delfiner, 1999), which can be written under a matrix<br />form as<br />?<br />k(xn,xn) p(xn)T<br />p(xn)0<br />??<br />λ(x;xn)<br />µ(x)<br />?<br />=<br />?<br />k(x,xn)<br />p(x)<br />?<br />,(11)<br />where p(xn) is an l × n matrix with entries pi(xj), i = 1,...,l, j = 1,...,n, µ is a<br />vector of Lagrange coefficients (k(xn,xn), λ(x;xn), k(x,xn) as above). The kriging<br />covariance function is given in this case by<br />?<br />= k(x,y) − λ(x;xn)Tk(y,xn) − µ(x)Tp(y).<br />k(x,y;xn) := covξ(x) −?ξ(x;xn),ξ(y) −?ξ(y;xn)<br />?<br />(12)<br />The following result holds (Kimeldorf and Wahba, 1970; O’Hagan, 1978):<br />Proposition 2 Let k be a covariance function.<br />?<br />m : x ?→ βTp(x), β ∼ URl<br />where URl stands for the (improper) uniform distribution over Rl, and where?ξ(·;xn)<br />If<br />ξ | m ∼ GP(m, k)<br />then ξ | Fn ∼ GP<br />??ξ(·;xn), k(·, ·; xn)<br />?<br />,<br />and k(·, ·; xn) are given by (7), (11) and (12).</p>  <p>Page 9</p> <p>9<br />Proposition 2 justifies the use of kriging in a Bayesian framework provided that the<br />covariance function of ξ is known. However, the covariance function is rarely assumed<br />to be known in applications. Instead, the covariance function is generally taken in some<br />parametric class (in this paper, we use the so-called Mat´ ern covariance function, see<br />Appendix A). A fully Bayesian approach also requires to choose a prior distribution<br />for the unknown parameters of the covariance (see, e.g., Handcock and Stein, 1993;<br />Kennedy and O’Hagan, 2001; Paulo, 2005). Sampling techniques (Monte Carlo Markov<br />Chains, Sequential Monte Carlo...) are then generally used to approximate the posterior<br />distribution of the unknown covariance parameters. Very often, the popular empirical<br />Bayes approach is used instead, which consists in plugging-in the maximum likelihood<br />(ML) estimate to approximate the posterior distribution of ξ. This approach has been<br />used in previous papers about contour estimation or probability of failure estimation<br />(Picheny et al., 2010; Ranjan et al., 2008; Bichon et al., 2008). In Section 5.2 we will<br />adopt a plug-in approach as well.<br />Simplified notations. In the rest of the paper, we shall use the following simplified<br />notations when there is no risk of confusion:?ξn(x) :=?ξn(x;Xn), σ2<br />n(x) := σ2<br />n(x;Xn).<br />2.4 Estimators of the probability of failure<br />Given a random process ξ and a strategy XN, the optimal estimator that minimizes<br />E0<br />among all Fn-measurable estimators ? αn, 1 ≤ n ≤ N, is<br />? αn = En(α) = En<br />where<br />?(α − ? αn)2?<br />??<br />X<br />1ξ&gt;udPX<br />?<br />=<br />?<br />X<br />pndPX, (13)<br />pn : x ∈ X ?→ Pn{ξ(x) &gt; u} . (14)<br />When ξ is a Gaussian process, the probability pn(x) of exceeding u at x ∈ X given In<br />has a simple closed-form expression:<br />pn(x) = 1 − Φ<br />??ξn(x) − u<br />σn(x)<br />?<br />, (15)<br />where Φ is the cumulative distribution function of the normal distribution. Thus, in<br />the Gaussian case, the estimator (13) is amenable to a numerical approximation, by<br />integrating the excess probability pn over X (for instance using Monte Carlo sampling,<br />see Section 3.3).<br />Another natural way to obtain an estimator of α given In is to look for a Fn-<br />measurable hard approximation ηn : X → {0,1} of the excess indicator 1ξ&gt;u. If ηn is<br />close in some sense to 1ξ&gt;u, the estimator<br />?<br />? αn =<br />X<br />ηndPX<br />(16)</p>  <p>Page 10</p> <p>10<br />should be close to α. More precisely,<br />En<br />?<br />(? αn− α)2?<br />Let τn(x) = En<br />misclassification; that is, the probability to predict a point above (resp. under) the<br />= En<br />???<br />(ηn− 1ξ&gt;u)dPX<br />?2?<br />≤<br />?<br />En<br />?<br />(ηn− 1ξ&gt;u)2?<br />dPX. (17)<br />?(ηn(x) − 1ξ(x)&gt;u)2?<br />= Pn{ηn(x) ?= 1ξ(x)&gt;u} be the probability of<br />threshold when the true value is under (resp. above) the threshold. Thus, (17) shows<br />that it is desirable to use a classifier ηnsuch that τnis small for all x ∈ X. For instance,<br />the method called smart (Deheeger and Lemaire, 2007) uses a support vector machine<br />to build ηn. Note that<br />τn(x)= pn(x) + (1 − 2pn(x))ηn(x).<br />Therefore, the right-hand side of (17) is minimized if we set<br />ηn(x) = 1pn(x)&gt;1/2= 1¯ξn(x)&gt;u,(18)<br />where¯ξn(x) denotes the posterior median of ξ(x). Then, we have<br />τn(x) = min(pn(x),1 − pn(x)).<br />In the case of a Gaussian process, the posterior median and the posterior mean are<br />equal. Then, the classifier that minimizes τn(x) for each x ∈ X is ηn = 1?ξn&gt;u, in<br />which case<br />τn(x) = Pn<br />?<br />(ξ(x) − u)(?ξn(x) − u) &lt; 0<br />?<br />= 1 − Φ<br />????ξn(x) − u??<br />σn(x)<br />?<br />.(19)<br />Notice that for ηn = 1?ξn&gt;u, we have ? αn = α(?ξn). Therefore, this approach to obtain<br />an estimator of α can be seen as a type of plug-in estimation.<br />3 Stepwise uncertainty reduction<br />3.1 Principle<br />A very natural and straightforward way of building a one-step lookahead strategy is to<br />select greedily each evaluation as if it were the last one. This kind of strategy, sometimes<br />called myopic, has been successfully applied in the field of Bayesian global optimiza-<br />tion (Mockus et al., 1978; Mockus, 1989), yielding the famous expected improvement<br />criterion later popularized in the EGO algorithm of Jones et al. (1998).<br />When the Bayesian risk provides a measure of the estimation error or uncertainty<br />(as in the present case), we call such a strategy a stepwise uncertainty reduction (SUR)<br />strategy. In the field of global optimization, the IAGO algorithm is an example of a SUR<br />strategy, where the Shannon entropy of the minimizer is used instead of the quadratic</p>  <p>Page 11</p> <p>11<br />cost (Villemonteix et al., 2009). When considered in terms of utility rather than cost,<br />such strategies have also been called knowledge gradient policies by Frazier et al. (2008).<br />Given a sequence of estimators (? αn)n≥1, a direct application of the above principle<br />Jn(x) = En<br />(α − ? αn+1)2| Xn+1= x<br />Having found no closed-form expression for this criterion, and no efficient numerical<br />procedure for its approximation, we will proceed by upper-bounding and discretiz-<br />using the quadratic loss function yields the sampling criterion<br />??<br />.(20)<br />ing (20) in order to get an expression that will lend itself to a numerically tractable<br />approximation. By doing so, several SUR strategies will be derived, depending on the<br />choice of estimator (the posterior mean (13) or the plug-in estimator (16) with (18))<br />and bounding technique.<br />3.2 Upper bounds of the SUR sampling criterion<br />Recall that τn(x) = min(pn(x),1−pn(x)) is the probability of misclassification at x us-<br />ing the optimal classifier 1?ξn(x)&gt;u. Let us further denote by νn(x) := pn(x) (1 − pn(x))<br />the variance of the excess indicator 1ξ(x)≥u.<br />Proposition 3 Assume that either ? αn = En(α) or ? αn =?1?ξn≥udPX. Define Gn :=<br /><br /><br />Then, for all x ∈ X and all n ∈ {0,...,N − 1},<br />?<br />X<br />?γn(y)dPXfor all n ∈ {0,...,N − 1}, with<br /><br />γn :=<br />νn = pn(1 − pn) = τn(1 − τn),<br />τn = min(pn,1 − pn),<br />if ? αn = En(α),<br />if ? αn =?1?ξn≥udPX.<br />?<br />Jn(x) ≤ ? Jn(x) := En<br />?<br />G2<br />n+1| Xn+1= x.<br />Note that γn(x) is a function of pn(x) that vanishes at 0 and 1, and reaches its maximum<br />at 1/2; that is, when the uncertainty on 1?ξn(x)&gt;uis maximal (see Figure 1).<br />Proof First, observe that, for all n ≥ 0, α − ? αn =?<br />Un : x ∈ X ?→ Un(x) =<br /><br />Moreover, note that γn = ?Un?2<br />L2(Ω,Fn,P), W ?→ En<br />(see, e.g., Vestrup, 2003, section 10.7) we get that<br />?<br />UndPX, with<br /><br /><br />1ξ(x)&gt;u− pn(x)<br />1ξ(x)&gt;u− 1?ξn(x)&gt;u<br />nin both cases, where ?· ?n : L2(Ω,B,P) →<br />?W2?1/2. Then, using the generalized Minkowski inequality<br />???∫ UndPX<br />if ? αn = En(α),<br />if ? αn =?1?ξn≥udPX.<br />(21)<br />???n<br />≤?Un?ndPX =<br />?<br />√γndPX = Gn.(22)</p>  <p>Page 12</p> <p>12<br />Finally, it follows from the tower property of conditional expectations and (22) that,<br />for all n ≥ 0,<br />Jn(x) = En<br />?<br />???∫ Un+1dPX<br />?α − ? αn+1?2<br />?<br />n+1| Xn+1= x<br />??2<br />?<br />= En<br />n+1<br />??? Xn+1= x<br />?<br />≤ En<br />G2<br />n+1| Xn+1= x<br />?<br />.<br />⊓ ⊔<br />0 0.20.4 0.60.81<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br /> <br /> <br />pn<br />γn<br />γn = pn(1 − pn)<br />γn = min(pn,1 − pn)<br />Fig. 1 γn as a function of pn (see Proposition 3). In both cases, γn is maximum at pn= 1/2.<br />Note that two other upper-bounding sampling criteria readily follow from those of<br />Proposition 3, by using the Cauchy-Schwarz inequality in L2(X,B(X),PX):<br />??<br />? Jn(x) ≤ En<br />γn+1dPX<br />??? Xn+1= x<br />?<br />. (23)<br />As a result, we can write four SUR criteria, whose expressions are summarized in Ta-<br />ble 1. Criterion JSUR<br />1,nhas been proposed in the PhD thesis of Piera-Martinez (2008) and<br />in conference papers (Vazquez and Piera-Martinez, 2007; Vazquez and Bect, 2009); the<br />other ones, to the best of our knowledge, are new. Each criterion is expressed as the<br />conditional expectation of some (possibly squared) Fn+1-measurable integral criterion,<br />with an integrand that can be expressed as a function of the probability of misclassifi-<br />cation τn+1. It is interesting to note that the integral in JSUR<br />square error (IMSE)4for the process 1ξ&gt;u.<br />4<br />is the integrated mean<br />4The IMSE criterion is usually applied to the response surface ξ itself (see, e.g.,<br />Box and Draper, 1987; Sacks et al., 1989). The originality here is to consider the IMSE of<br />the process 1ξ&gt;uinstead. Another way of adapting the IMSE criterion for the estimation of a<br />probability of failure, proposed by Picheny et al. (2010), is recalled in Section 4.1.</p>  <p>Page 13</p> <p>13<br />Table 1 Expressions of four SUR-type criteria.<br />SUR-type sampling criterion<br />??? ?τn+1dPX<br />??? ?νn+1dPX<br />??<br />??<br />How it is obtained<br />JSUR<br />1,n(x) = En<br />?2??? Xn+1= x<br />?2??? Xn+1= x<br />??? Xn+1= x<br />??? Xn+1= x<br />?<br />?<br />?<br />?<br />Prop. 3 with ? αn=?<br />Prop. 3 with ? αn= En(α)<br />Eq. (23) with ? αn=?<br />Eq. (23) with ? αn= En(α)<br />1? ξn&gt;udPX<br />JSUR<br />2,n(x) = En<br />JSUR<br />3,n(x) = En<br />τn+1dPX<br />1? ξn&gt;udPX<br />JSUR<br />4,n(x) = En<br />νn+1dPX<br />3.3 Discretizations<br />In this section, we proceed with the necessary integral discretizations of the SUR crite-<br />ria to make them suitable for numerical evaluation and implementation on computers.<br />Assume that n steps of the algorithm have already been performed and consider, for<br />instance, the criterion<br />JSUR<br />3,n(x) = En<br />?<br />∫ τn+1(y)PX(dy)<br />??? Xn+1= x<br />?<br />.(24)<br />Remember that, for each y ∈ X, the probability of misclassification τn+1(y) is Fn+1-<br />measurable and, therefore, is a function of In+1= (In,Xn+1,Zn+1). Since Inis known<br />at this point, we introduce the notation vn+1(y;Xn+1,Zn+1) = τn+1(y) to emphasize<br />the fact that, when a new evaluation point must be chosen at step (n + 1), τn+1(y)<br />depends on the choice of Xn+1and the random outcome Zn+1. Let us further denote<br />by Qn,x the probability distribution of ξ(x) under Pn. Then, (24) can be rewritten as<br />??<br />and the corresponding strategy is:<br />??<br />Given In and a triple (x,y,z), vn+1(y;x,z) can be computed efficiently using the<br />equations provided in Sections 2.3 and 2.4.<br />JSUR<br />3,n(x) =<br />R×X<br />vn+1(y;x,z) Qn,x(dz)PX(dy),<br />Xn+1 = argmin<br />x∈X<br />R×X<br />vn+1(y;x,z) Qn,x(dz)PX(dy).(25)<br />At this point, we need to address: 1) the computation of the integral on X with<br />respect to PX; 2) the computation of the integral on R with respect to Qn,x; 3) the<br />minimization of the resulting criterion with respect to x ∈ X.<br />To solve the first problem, we draw an i.i.d. sequence Y1,...,Ym ∼ PXand use the<br />Monte Carlo approximation:<br />?<br />X<br />vn+1(y;x,z) PX(dy) ≈<br />1<br />m<br />m<br />?<br />j=1<br />vn+1(Yj;x,z).<br />An increasing sample size n ?→ mn should be used to build a convergent algo-<br />rithm for the estimation of α (possibly with a different sequence Yn,1,...,Yn,mn</p>  <p>Page 14</p> <p>14<br />at each step). In this paper we adopt a different approach instead, which is to<br />take a fixed sample size m &gt; 0 and keep the same sample Y1,...,Ym through-<br />out the iterations. Equivalently, it means that we choose to work from the start<br />on a discretized version of the problem: we replace PX by the empirical distribu-<br />tion?PX,n =<br />En(αm) =<br />m<br />approach has be coined meta-estimation by Arnaud et al. (2010): the objective is to<br />1<br />m<br />?1ξ&gt;ud?PX,n =<br />?m<br />?<br />j=1δYj, and our goal is now to estimate the Monte Carlo esti-<br />1<br />m<br />jpn(Yj) or the plug-in estimate<br />mator αm =<br />?m<br />j=11ξ(Yj)&gt;u, using either the posterior mean<br />1<br />m<br />1<br />?<br />j1?ξ(Yj;Xn)&gt;u. This kind of<br />estimate the value of a precise Monte Carlo estimator of α(f) (m being large), using<br />prior information on f to alleviate the computational burden of running m times the<br />computer code f. This point of view also underlies the work in structural reliability<br />of Hurtado (2004, 2007), Deheeger and Lemaire (2007), Deheeger (2008), and more<br />recently Echard et al. (2010).<br />The new point of view also suggests a natural solution for the third problem,<br />which is to replace the continuous search for a minimizer x ∈ X by a discrete search<br />over the set Xm := {Y1,...,Ym}. This is obviously sub-optimal, even in the meta-<br />estimation framework introduced above, since picking x ∈ X \ Xm can sometimes<br />bring more information about ξ(Y1),...,ξ(Ym) than the best possible choice in Xm.<br />Global optimization algorithm may of course be used to tackle directly the continuous<br />search problem: for instance, Ranjan et al. (2008) use a combination of a genetic algo-<br />rithm and local search technique, Bichon et al. (2008) use the DIRECT algorithm and<br />Picheny et al. (2010) use a covariance-matrix-adaptation evolution strategy. In this pa-<br />per we will stick to the discrete search approach, since it is much simpler to implement<br />(we shall present in Section 3.4 a method to handle the case of large m) and provides<br />satisfactory results (see Section 5).<br />Finally, remark that the second problem boils down to the computation of a one-<br />dimensional integral with respect to Lebesgue’s measure. Indeed, since ξ is a Gaus-<br />sian process, Qn,x is a Gaussian probability distribution with mean?ξn(x) and vari-<br />Gauss-Hermite quadrature with Q points (see, e.g., Press et al., 1992, Chapter 4) :<br />?<br />q=1<br />where u1,...,uQ denote the quadrature points and w1,...,wQ the corresponding<br />weights. Note that this is equivalent to replacing under Pn the random variable ξ(x) by<br />a quantized random variable with probability distribution?Q<br />Taking all three discretizations into account, the proposed strategy is:<br />ance σ2<br />n(x) as explained in Section 2.3. The integral can be computed using a standard<br />vn+1(y;x,z)Qn,x(dz) ≈<br />1<br />√π<br />Q<br />?<br />wqvn+1(y;x,?ξn(x) + σn(x)uq<br />√2),<br />q=1w′qδzn+1,q(x), where<br />w′q= wq/√π and zn+1,q(x) =?ξn(x) + σn(x)uq<br />√2.<br />Xn+1 = argmin<br />1≤k≤m<br />m<br />?<br />j=1<br />Q<br />?<br />q=1<br />w′<br />qvn+1<br />?Yj; Yk,zn+1,q(Yk)?.(26)</p>  <p>Page 15</p> <p>15<br />3.4 Implementation<br />This section gives implementation guidelines for the SUR strategies described in Sec-<br />tion 3. As said in Section 3.3, the strategy (26) can, in principle, be translated directly<br />into a computer program. In practice however, we feel that there is still room for<br />different implementations. In particular, it is important to keep the computational<br />complexity of the strategies at a reasonable level. We shall explain in this section some<br />simplifications we have made to achieve this goal.<br />A straight implementation of (26) for the choice of an additional evaluation point<br />is described in Table 2. This procedure is meant to be called iteratively in a sequential<br />algorithm, such as that described for instance in Table 3. Note that the only parameter<br />to be specified in the SUR strategy (26) is Q, which will be taken equal to 12 in the<br />rest of the paper.<br />Table 2 Procedure to select a new evaluation point Xn+1∈ X using a SUR strategy<br />Require computer representations of<br />a) a set In= {(X1,f(X1)),...,(Xn,f(Xn))} of evaluation results;<br />b) a Gaussian process prior ξ with a (possibly unknown linear parametric) mean function and<br />a covariance function kθ, with parameter θ;<br />c) a (pseudo-)random sample Xm= {Y1,...,Ym} of size m drawn from the distribution PX;<br />d) quadrature points u1,...,uQand corresponding weights w′<br />e) a threshold u.<br />1,...,w′<br />Q;<br />1. compute the kriging approximation? fn and kriging variance σ2<br />2.1 for each point Yk, k ∈ {1,...,m}, compute the kriging weights λi(Yk;{Xn,Yj}),<br />i ∈ {1,...,(n + 1)}, and the kriging variances σ2<br />2.2 compute zn+1,q(Yj) =? fn(Yj) + σn(Yj)uq<br />2.3.1 computethekriging approximation<br />(Yj,f(Yj) = zn+1,q(Yj)), using the weights λi(Yk;{Xn,Yj}), i = 1,...,(n + 1),<br />k = 1,...,m, obtained at Step 2.1.<br />2.3.2 for each k ∈ {1,...,m}, compute vn+1(Yk; Yj,zn+1,q(Yj)), using u,˜fn+1,j,q<br />obtained in 2.3.1, and σ2<br />n+1(Yk;{Xn,Yj}) obtained in 2.1<br />2.4 compute Jn(Yj) =?m<br />non Xm from In<br />2. for each candidate point Yj, j ∈ {1,...,m},<br />n+1(Yk;{Xn,Yj})<br />√2, for q = 1,...,Q<br />2.3 for each zn+1,q(Yj), q ∈ {1,...,Q},<br />˜fn+1,j,q<br />on<br />Xm<br />fromIn ∪<br />k=1<br />?Q<br />q=1w′qvn+1(Yk; Yj,zn+1,q(Yj)).<br />3. find j⋆= argminjJn(Yj) and set Xn+1= Yj⋆<br />To assess the complexity of a SUR sampling strategy, recall that kriging takes<br />O(mn2) operations to predict the value of f at m locations from n evaluation results<br />of f (we suppose that m &gt; n and no approximation is carried out). In the procedure<br />to select an evaluation, a first kriging prediction is performed at Step 1 and then, m</p>  <p>Page 16</p> <p>16<br />Table 3 Sequential estimation of a probability of failure<br />1. Construct an initial design of size n0&lt; N and evaluate f at the points of the initial design.<br />2. Choose the parametric form of the mean of the Gaussian process ξ by analyzing the results<br />of the evaluations of f on the initial design.<br />2. Choose the covariance function kθof ξ<br />3. Generate a Monte Carlo sample of size m from PX<br />3. While the evaluation budget N is not exhausted,<br />3.1 optional step: estimate the parameters of the covariance function (case of a plug-in<br />approach);<br />3.2 call a procedure to select a new evaluation point;<br />3.3 perform the new evaluation.<br />4. Estimate the probability of failure obtained from the N evaluations of f.<br />different predictions have to performed at step 2.1. This cost becomes rapidly burden-<br />some for large values of n and m, and we must further simplify (26) to be able to work<br />on applications where m must be large. A natural idea to alleviate the computational<br />cost of the strategy is to avoid dealing with candidate points that have a very low prob-<br />ability of misclassification, since they are probably far from the frontier of the domain<br />of failure. It is also likely that those points with a low probability of misclassification<br />will have a very small contribution in the variance of the error of estimation ? αn−αm.<br />a way that the first summation (over m) and the search set for the minimizer is re-<br />Therefore, the idea is to rewrite the sampling strategy described by (26), in such<br />stricted to a subset of points Yjcorresponding to the m0largest values of τn(Yj). The<br />corresponding algorithm is not described here for the sake of brevity but can easily be<br />adapted from that of Table 2. Section 5.3 will show that this pruning scheme has almost<br />no consequence on the performances of the SUR strategies, even when one considers<br />small values for m0(for instance m0= 200).<br />4 Other strategies proposed in the literature<br />4.1 The targeted IMSE criterion<br />The targeted IMSE proposed in Picheny et al. (2010) is a modification of the IMSE (In-<br />tegrated Mean Square Error) sampling criterion (Sacks et al., 1989). While the IMSE<br />sampling criterion computes the average of the kriging variance (over a compact do-<br />main X) in order to achieve a space-filling design, the targeted IMSE computes a<br />weighted average of the kriging variance for a better exploration of the regions near<br />the frontier of the domain of failure. The idea is to put a large weight in regions where<br />the kriging prediction is close to the threshold u, and a small one otherwise. Given In,<br />the targeted IMSE sampling criterion, hereafter abbreviated as tIMSE, can be written</p>  <p>Page 17</p> <p>17<br />as<br />JtIMSE<br />n<br />(x) = En<br />??<br />σ2<br />n+1(y;X1,...,Xn,x) Wn(y)PX(dy),<br />X<br />?ξ −?ξn+1<br />?2WndPX<br />??? Xn+1= x<br />?<br />(27)<br />=<br />?<br />X<br />(28)<br />where Wn is a weight function based on In. The weight function suggested by<br />Picheny et al. (2010) is<br />Wn(x) =<br />1<br />sn(x)√2π<br />exp<br />?<br />−1<br />2<br />??ξn(x) − u<br />sn(x)<br />?2?<br />,(29)<br />where s2<br />n(x) = σ2<br />ε+ σ2<br />n(x). Note that Wn(x) is large when?ξn(x) ≈ u and σ2<br />The tIMSE criterion operates a trade-off between global uncertainty reduction (high<br />kriging variance σ2<br />n) and exploration of target regions (high weight function Wn). The<br />weight function depends on a parameter σε &gt; 0, which allows to tune the width of<br />the “window of interest” around the threshold. For large values of σε, JtIMSEbehaves<br />approximately like the IMSE sampling criterion. The choice of an appropriate value<br />n(x) ≈ 0,<br />i.e., when the function is known to be close to u.<br />for σε, when the goal is to estimate a probability of failure, will be discussed on the<br />basis of numerical experiments in Section 5.3.<br />The tIMSE strategy requires a computation of the expectation with respect to ξ(x)<br />in (27), which can be done analytically, yielding (28). The computation of the integral<br />with respect to PXon X can be carried out with a Monte Carlo approach, as explained<br />in Section 3.3. Finally, the optimization of the criterion is replaced by a discrete search<br />in our implementation.<br />4.2 Criteria based on the marginal distributions<br />Other sampling criteria proposed by Ranjan et al. (2008), Bichon et al. (2008) and<br />Echard et al. (2010) are briefly reviewed in this section. A common feature of these<br />three criteria is that, unlike the SUR and tIMSE criteria discussed so far, they only<br />depend on the marginal posterior distribution at the considered candidate point x ∈ X,<br />which is a Gaussian N??ξn(x),σ2<br />A natural idea, in order to sequentially improve the estimation of the probability<br />n(x)?distribution. As a consequence, they are of course<br />much cheaper to compute than integral criteria like SUR and tIMSE.<br />of failure, is to visit the point x ∈ X where the event {ξ(x) ≥ u} is the most un-<br />certain. This idea, which has been explored by Echard, Gayton, and Lemaire (2010),<br />corresponds formally to the sampling criterion<br />JEGL<br />n<br />(x) = τn(x) = 1 − Φ<br />???u −?ξn(x)??<br />σn(x)<br />?<br />.(30)</p>  <p>Page 18</p> <p>18<br />As in the case of the tIMSE criterion and also, less explicitely, in SUR criteria, a trade-<br />off is realized between global uncertainty reduction (choosing points with a high σ2<br />n(x))<br />and exploration of the neighboorhood of the estimated contour (where<br />??u −?ξn(x)??is<br />small).<br />The same leading principle motivates the criteria proposed by Ranjan et al. (2008)<br />and Bichon et al. (2008), which can be seen as special cases of the following sampling<br />criterion:<br />JRB<br />n (x) := En<br />?<br />max<br />?<br />0,ǫ(x)δ− |u − ξ(x)|δ??<br />, (31)<br />where ǫ(x) = ασn(x), α,δ &gt; 0. The following proposition provides some insights into<br />this sampling criterion:<br />Proposition 4 Define Gα,δ: ]0,1[ → R+ by<br />Gα,δ(p) := E<br />?<br />max<br />?<br />0,αδ−??Φ−1(p) + U????<br />,<br />where U is a Gaussian N(0,1) random variable. Let ϕ and Φ denote respectively the<br />probability density function and the cumulative distribution function of U.<br />a)Gα,δ(p) = Gα,δ(1 − p) for all p ∈]0,1[.<br />Gα,δis strictly increasing on ]0,1/2] and vanishes at 0. Therefore, Gα,δis also<br />strictly decreasing on [1/2,1[, vanishes at 1, and has a unique maximum at p = 1/2.<br />b)<br />c) Criterion (31) can be rewritten as<br />JRB<br />n (x) = σn(x)δGα,δ<br />?pn(x)?.(32)<br />d)Gα,1has the following closed-form expression:<br />Gα,1(p) = α?Φ(t+) − Φ(t−)?<br />− t?2Φ(t) − Φ(t+) − Φ(t−)?<br />−?2ϕ(t) − ϕ(t+) − ϕ(t−)?,<br />(33)<br />where t = Φ−1(1 − p), t+= t + α and t−= t − α.<br />Gα,2has the following closed-form expression: e)<br />Gα,2(p) =<br />?α2− 1 − t2??Φ(t+) − Φ(t−)?<br />+ t+ϕ(t+) − t−ϕ(t−),<br />− 2t?ϕ(t+) − ϕ(t−)?<br />(34)<br />with the same notations.<br />It follows from a) and c) that JRB<br />ance σ2<br />n(x) and the probability of misclassification τn(x) = min(pn(x),1 − pn(x)).<br />Note that, in the computation of Gα,δ<br />and (34) is equal to?u −?ξn(x)?/σn(x), i.e., equal to the normalized distance between<br />n (x) can also be seen as a function of the kriging vari-<br />?pn(x)?, the quantity denoted by t in (33)<br />the predicted value and the threshold.</p>  <p>Page 19</p> <p>19<br />Bichon et al.’s expected feasibility function corresponds to (32) with δ = 1, and can<br />be computed efficiently using (33). Similarly, Ranjan et al.’s expected improvement5<br />function corresponds to (32) with δ = 2, and can be computed efficiently using (34).<br />Proof (Proposition 4)<br />a) Using the identity Φ−1(1 − p) = −Φ−1(p), we get<br />??U + Φ−1(1 − p)??=<br />where<br />b) Let Sp = max?0,αδ−??Φ−1(p) + U???. Straightforward computations show that<br />sequence, p ?→ P?Sp &lt; s?<br />other assertions then follow from a).<br />c) Recall that ξ(x) ∼ N??ξn(x),σ2<br />A proof of d) and e) is given in Appendix B.<br />???U − Φ−1(p)<br />???<br />d=<br />???U + Φ−1(p)<br />???,<br />d= denotes an equality in distribution. Therefore Gα,δ(1 − p) = Gα,δ(p).<br />t ?→ P(|t + U| ≤ v) is strictly decreasing to 0 on [0,+∞[, for all v &gt; 0. As a con-<br />is strictly increasing to 1 on [1/2,1[, for all s ∈<br />Therefore, Gα,δis strictly decreasing on [1/2,1[ and tends to zeros when p → 1. The<br />?0,αδ?.<br />n(x)?<br />under Pn. Therefore U :=<br />?ξ(x) −<br />?ξn(x)?/σn(x) ∼ N(0,1) under Pn, and the result follows by substitution in (31).<br />Remark 2 In the case δ = 1, our result coincides with the expression given by<br />⊓ ⊔<br />Bichon et al. (2008, Eq. (17)). In the case δ = 2, we have found and corrected a<br />mistake in the computations of Ranjan et al. (2008, Eq. (8) and Appendix B).<br />5 Numerical experiments<br />5.1 A one-dimensional illustration of a SUR strategy<br />The objective of this section is to show the progress of a SUR strategy in a simple<br />one-dimensional case. We wish to estimate α = PX{f &gt; 1}, where f is a given function<br />defined over X = R, endowed with the probability distribution PX= N(0,σ2), σ =<br />0.4, as depicted in Figure 2. We know in advance that α ≈ 0.2. Thus, a Monte Carlo<br />sample of size m = 1500 will give a good estimate of α.<br />In this illustration, ξ is a Gaussian process with constant but unknown mean and a<br />Mat´ ern covariance function, whose parameters are kept fixed, for the sake of simplicity.<br />Figure 2 shows an initial design of four points and the sampling criterion JSUR<br />1,n=4. Notice<br />that the sampling criterion is only computed at the points of the Monte Carlo sample.<br />Figures 3 and 4 show the progress of the SUR strategy after a few iterations. Observe<br />that the unknown function f is sampled so that the probability of excursion pn almost<br />equals zero or one in the region where the density of PXis high.<br />5Despite its name and some similarity between the formulas, this criterion should not be<br />confused with the well-known EI criterion in the field of optimization (Mockus et al., 1978;<br />Jones et al., 1998).</p>  <p>Page 20</p> <p>20<br />−2−1.5−1−0.500.511.52<br />0<br />0.5<br />1<br />−2 −1.5−1−0.500.511.52<br />0<br />0.5<br />1<br />−2−1.5−1−0.5 00.511.52<br />0.02<br />0.04<br />0.06<br />Fig. 2 Illustration of a SUR strategy. This figure shows the initial design. Top: threshold<br />u = 1 (horizontal dashed line); function f (thin line); n = 4 initial evaluations (squares);<br />kriging approximation fn (thick line); 95% confidence intervals computed from the kriging<br />variance (shaded area). Middle: probability of excursion (solid line); probability density of<br />PX(dotted line). Bottom: graph of JSUR<br />1,n=4(Yi), i = 1,...,m = 1500, the minimum of which<br />indicates where the next evaluation of f should be done (i.e., near the origin).<br />−2−1.5−1−0.500.511.52<br />0<br />0.5<br />1<br />−2−1.5−1 −0.500.51 1.52<br />0<br />0.5<br />1<br />−2−1.5−1 −0.500.511.52<br />4<br />6<br />8<br />10x 10<br />−3<br />Fig. 3 Illustration of a SUR strategy (see also Figures 2 and 4). This figure shows the progress<br />of the SUR strategy after two iterations—a total of n = 6 evaluations (squares) have been<br />performed. The next evaluation point will be approximately at x = −0.5<br />.</p>  <p>Page 21</p> <p>21<br />−2−1.5−1−0.500.511.52<br />0<br />0.5<br />1<br />−2−1.5−1 −0.500.511.52<br />0<br />0.5<br />1<br />−2−1.5−1−0.500.511.52<br />0<br />1<br />2<br />3x 10<br />−5<br />Fig. 4 Illustration of a SUR strategy (see also Figures 2 and 3). This figure shows the progress<br />of the SUR strategy after eight iterations—a total of n = 12 evaluations (squares) have been<br />performed. At this stage, the probability of excursion pn almost equals 0 or 1 in the region<br />where the density of PXis high.<br />5.2 An example in structural reliability<br />In this section, we evaluate all criteria discussed in Section 3 and Section 4 through a<br />classical benchmark example in structural reliability (see, e.g., Borri and Speranzini,<br />1997; Waarts, 2000; Schueremans, 2001; Deheeger, 2008). Echard et al. (2010)<br />used this benchmark to make a comparison among several methods proposed in<br />Schueremans and Gemert (2005), some of which are based on the construction of a<br />response surface. The objective of the benchmark is to estimate the probability of<br />failure of a so-called four-branch series system. A failure happens when the system is<br />working under the threshold u = 0. The performance function f for this system is<br />defined as<br />f : (x1,x2) ∈ R2?→ f(x1,x2) = min<br /><br /><br /><br /><br /><br /><br /><br /><br /><br />3 + 0.1(x1− x2)2− (x1+ x2)/√2;<br />3 + 0.1(x1− x2)2+ (x1+ x2)/√2;<br />(x1− x2) + 6/√2;<br />(x2− x1) + 6/√2<br /><br /><br /><br /><br /><br /><br /><br /><br /><br />.<br />The uncertain input factors are supposed to be independent and have standard normal<br />distribution. Figure 5 shows the performance function, the failure domain and the input<br />distribution. Observe that f has a first-derivative discontinuity along four straight lines<br />originating from the point (0,0).</p>  <p>Page 22</p> <p>22<br /> <br /> <br />−6−4−20246<br />−6<br />−4<br />−2<br />0<br />2<br />4<br />6<br />f = 0<br />Fig. 5 Left: mesh plot of the performance function f corresponding to the four-branch series<br />system; a failure happens when f is below the transparent plane; Right: contour plot of f;<br />limit state f = 0 (thick line); sample of size m = 3 × 103from PX(dots).<br />For each sequential method, we will follow the procedure described in Table 3. We<br />generate an initial design of n0 = 10 points (five times the dimension of the factor<br />space) using a maximin LHS (Latin Hypercube Sampling). The same initial design is<br />used for all methods. We generate a Monte Carlo sample of size m = 30000. Since<br />f can be evaluated very quickly, it is possible to compute the value of f for all the<br />points of this sample. Then, an empirical estimation of the probability of failure gives<br />αm = 4.16 × 10−3. A Gaussian process with constant unknown mean and a Mat´ ern<br />covariance function is used as our prior information about f. The parameters of the<br />Mat´ ern covariance functions are estimated on the initial design by REML (see, e.g.<br />Stein, 1999). We obtain σ2≈ 30, ν ≈ 3 and ρ1≈ ρ2≈ 12, which corresponds to a<br />fairly smooth prior. In this experiment, we choose to re-estimate the parameters of the<br />covariance after each new evaluation.<br />The probability of failure is estimated by (13). To evaluate the rate of convergence,<br />we compute the number nγ of iterations that must be performed using a given strat-<br />egy to observe a stabilization of the relative error of estimation within an interval of<br />length 2γ:<br />nγ = min<br />?<br />n ≥ 0;∀k ≥ n,|? αn0+k− αm|<br />αm<br />&lt; γ<br />?<br />.<br />We could observe the convergence of the estimators to αm for all sampling criteria,<br />except JtIMSEwith σ2<br />ε= 1. In fact, JtIMSEwith σ2<br />ε= 1 is a space-filling criterion and<br />it is interesting to observe that the convergence is slow with this criterion. The results<br />are presented in Table 4. Figure 6 shows the points that have been evaluated in three<br />cases. It seems that the best results are obtained when the points are chosen close to<br />the frontier of the domain of failure.</p>  <p>Page 23</p> <p>23<br />1<br />2<br />3<br />4<br />5<br />6<br />7<br />8<br />9<br />10<br />11<br />12<br />13<br />14<br />15<br />16<br />−6 −4−20246<br />−6<br />−4<br />−2<br />0<br />2<br />4<br />6<br />1<br />2<br />3<br />4<br />5<br />6<br />7<br />8<br />9<br />10<br />11<br />12<br />13<br />14<br />15<br />16<br />−6−4−20246<br />−6<br />−4<br />−2<br />0<br />2<br />4<br />6<br />1<br />2<br />3<br />4<br />5<br />6<br />7<br />8<br />9<br />10<br />11<br />12<br />13<br />14<br />15<br />16<br />−6−4−20246<br />−6<br />−4<br />−2<br />0<br />2<br />4<br />6<br />Fig. 6 The first 16 points (squares) evaluated using sampling criterion JSUR<br />with σ2<br />ε= 1 (right). Numbers near squares indicate the order<br />of evaluation. The location of the n0= 10 points of the initial design are indicated by circles.<br />1<br />(left), JtIMSE<br />ε= 0.1 (middle), JtIMSEwith σ2<br />5.3 Average performance on sample paths of a Gaussian process<br />This section provides a comparison of all the criteria introduced or recalled in this<br />paper, on the basis of their average performance on the sample paths of a zero-mean<br />Gaussian process defined on X = [0,1]d, for d ∈ {1,2,3}. In all experiments, the<br />same covariance function is used for the generation of the sample paths and for the<br />computation of the sampling criteria. We have considered isotropic Mat´ ern covariance<br />functions, whose parameters are given in Table 5. An initial maximin LHS design of<br />size n0(also given in the table) is used: note that the value of n reported on the x-axis<br />of Figures 7–11 is the total number of evaluations, including the initial design.<br />The d input variables are assumed to be independent and uniformly distributed<br />on [0,1], i.e., PXis the uniform distribution on X. An m-sample Y1, ..., Ym from PX<br />Table 4 Comparison of the convergence to αm for different sampling strategies.<br />experiment<br />criterion<br />parameters<br />n0.1<br />n0.01<br />1<br />JSUR<br />1<br />–<br />16<br />30<br />2<br />JSUR<br />2<br />–<br />15<br />28<br />3<br />JSUR<br />3<br />–<br />11<br />24<br />4<br />JSUR<br />4<br />–<br />11<br />27<br />experiment<br />criterion<br />parameters<br />n0.1<br />n0.01<br />5<br />JEGL<br />–<br />15<br />23<br />6<br />JRB<br />7<br />JRB<br />8<br />JRB<br />9<br />JRB<br />δ = 1, α = 0.5<br />21<br />36<br />δ = 1, α = 2<br />21<br />36<br />δ = 2, α = 0.5<br />21<br />34<br />δ = 2, α = 2<br />19<br />32<br />experiment<br />criterion<br />parameters<br />n0.1<br />n0.01<br />10<br />JtIMSE<br />σ2<br />17<br />33<br />11<br />JtIMSE<br />σ2<br />ε= 0.1<br />19<br />54<br />12<br />JtIMSE<br />σ2<br />ε= 1<br />8<br />&gt; 120<br />ε= 10−6</p>  <p>Page 24</p> <p>24<br />Table 5 Size of the initial design and covariance parameters for the experiments of Section 5.3.<br />The parametrization of the Mat´ ern covariance function used here is defined in Appendix A.<br />d<br />1<br />2<br />3<br />n0<br />3<br />10<br />15<br />σ2<br />1.0<br />1.0<br />1.0<br />νρ<br />2.0<br />2.0<br />2.0<br />0.100<br />0.252<br />0.363<br />is drawn one and for all, and used both for the approximation of integrals (in SUR and<br />tIMSE criteria) and for the discrete search of the next sampling point (for all criteria).<br />We take m = 500 and use the same MC sample for all criteria in a given dimension d.<br />We adopt the meta-estimation framework as described in Section 3.3; in other<br />words, our goal is to estimate the MC estimator αm. We choose to adjust the thresh-<br />old u in order to have αm = 0.02 for all sample paths (note that, as a consequence,<br />there are exactly mαm = 10 points in the failure region) and we measure the per-<br />formance of a strategy after n evaluations by its relative mean-square error (MSE)<br />expressed in decibels (dB):<br />rMSE := 10 log10<br /><br /><br /><br />1<br />L<br />L<br />?<br />l=1<br />?<br />? α(l)<br />m,n− αm<br />α2m<br />?2<br /><br /> ,<br /><br />where ? α(l)<br />We use a sequential maximin strategy as a reference in all of our experiments. This<br />m,n=<br />1<br />m<br />?m<br />j=1p(l)<br />n(Yj) is the posterior mean of the MC estimator αm after n<br />evaluations on the lthsimulated sample path (L = 4000).<br />simple space-filling strategy is defined by Xn+1= argmaxjmin1≤i≤n<br />the argmax runs over all indices j such that Yj?∈ {X1,...,Xn}. Note that this strategy<br />does not depend on the choice of a Gaussian process model.<br />??Yj− Xi<br />??, where<br />Our first experiment (Figure 7 ) provides a comparison of the four SUR strategies<br />proposed in Section 3.2. It appears that all of them perform roughly the same when<br />compared to the reference strategy. A closer look, however, reveals that the strate-<br />gies JSUR<br />1<br />and JSUR<br />2<br />provided by Proposition 3 perform slightly better than the other<br />two (noticeably so in the case d = 3).<br />The performance of the tIMSE strategy is shown on Figure 8 for several value of<br />its tuning parameter σ2<br />ε(other values, not shown here, have been tried as well). It is<br />clear that the performance of this strategy improves when σ2<br />εgoes to zero, whatever<br />the dimension.<br />The performance of the strategy based on JRB<br />α,δis shown on Figure 9 for several<br />values of its parameters. It appears that the criterion proposed by Bichon et al. (2008),<br />which corresponds to δ = 1, performs better than the one proposed by Ranjan et al.<br />(2008), which corresponds to δ = 2, for the same value of α. Moreover, the value α = 0.5<br />has been found in our experiments to produce the best results.</p>  <p>Page 25</p> <p>25<br />Figure 10 illustrates that the loss of performance associated to the “pruning trick”<br />introduced in Section 3.4 can be negligible if the size m0of the pruned MC sample is<br />large enough (here, m0has been taken equal to 50). In practice, the value of m0should<br />be chosen small enough to keep the overhead of the sequential strategy reasonable—in<br />other words, large values of m0should only be used for very complex computer codes.<br />Finally, a comparison involving the best strategy obtained in each category is pre-<br />sented on Figure 11. The best result is consistently obtained with the SUR strategy<br />based on JSUR<br />1,n. The tIMSE strategy with σ2<br />as good. Note that both strategies are one-step lookahead strategies based on the ap-<br />ε≈ 0 provides results which are almost<br />proximation of the risk by an integral criterion, which makes them rather expensive<br />to compute. Simpler strategies based on the marginal distribution (criteria JRB<br />JEGL<br />n<br />) provide interesting alternatives for moderately expensive computer codes: their<br />performances, although not as good as those of one-step lookahead criterions, are still<br />n<br />and<br />much better than that of the reference space-filling strategy.<br />6 Concluding remarks<br />One of the main objectives of this paper was to present a synthetic viewpoint on se-<br />quential strategies based on a Gaussian process model and kriging for the estimation of<br />a probability of failure. The starting point of this presentation is a Bayesian decision-<br />theoretic framework from which the theoretical form of an optimal strategy for the<br />estimation of a probability of failure can be derived. Unfortunately, the dynamic pro-<br />gramming problem corresponding to this strategy is not numerically tractable. It is<br />nonetheless possible to derive from there the ingredients of a sub-optimal strategy: the<br />idea is to focus on one-step lookahead suboptimal strategies, where the exact risk is<br />replaced by a substitute risk that accounts for the information gain about α expected<br />from a new evaluation. We call such a strategy a stepwise uncertainty reduction (SUR)<br />strategy. Our numerical experiments show that SUR strategies perform better, on av-<br />erage, than the other strategies proposed in the literature. However, this comes at a<br />higher computational cost than strategies based only on marginal distributions. The<br />tIMSE sampling criterion, which seems to have a convergence rate comparable to that<br />of the SUR criterions when σ2<br />ε≈ 0, also has a high computational complexity.<br />Can we say that the sequential strategies presented in this paper are interesting<br />alternatives to classical importance sampling methods for estimating a probability of<br />failure, for instance the subset sampling method of Au and Beck (2001)? In our opinion,<br />the answer to this questions depends on our capacity to choose an appropriate prior.<br />In the example of Section 5.2, as well as in many other examples of the literature<br />using Gaussian processes in the domain of computer experiments, the prior is easy to<br />choose because X is a low-dimensional space and f tends to be smooth. Then, the<br />plug-in approach which consists in using ML or REML to estimate the parameters of</p>  <p>Page 26</p> <p>26<br />rMSE (dB)<br />n<br />rMSE (dB)<br />n<br />rMSE (dB)<br />n<br />JSUR<br />1<br />JSUR<br />2<br />JSUR<br />3<br />JSUR<br />4<br />ref.<br />black solid line<br />gray solid line<br />gray mixed line<br />black mixed line<br />black dashed line<br />Upper-left:<br />Upper-right:<br />Lower-left:<br />d = 1<br />d = 2<br />d = 3<br />203040<br />50<br />60<br />20 304050<br />60<br />51015202530<br />-15<br />-10<br />-5<br />0<br />-25<br />-20<br />-15<br />-10<br />-5<br />0<br />-30<br />-20<br />-10<br />0<br />10<br />Fig. 7 Relative MSE performance of several SUR strategies.<br />rMSE (dB)<br />n<br />rMSE (dB)<br />n<br />rMSE (dB)<br />n<br />σ2<br />σ2<br />σ2<br />ref.<br />ε= 10−6<br />ε= 0.1<br />ε= 1<br />black solid line<br />gray solid line<br />black mixed line<br />black dashed line<br />Upper-left:<br />Upper-right:<br />Lower-left:<br />d = 1<br />d = 2<br />d = 3<br />203040<br />5060<br />203040 50 60<br />5<br />10 1520 2530<br />-15<br />-10<br />-5<br />0<br />-25<br />-20<br />-15<br />-10<br />-5<br />0<br />-30<br />-20<br />-10<br />0<br />10<br />Fig. 8 Relative MSE performance of the tIMSE strategy for several values of its parameter.</p>  <p>Page 27</p> <p>27<br />rMSE (dB)<br />n<br />rMSE (dB)<br />n<br />rMSE (dB)<br />n<br />α = 0.5<br />α = 0.5<br />α = 2<br />α = 2<br />ref.<br />δ = 1<br />δ = 2<br />δ = 1<br />δ = 2<br />black solid line<br />gray solid line<br />black mixed line<br />gray mixed line<br />black dashed line<br />Upper-left:<br />Upper-right:<br />Lower-left:<br />d = 1<br />d = 2<br />d = 3<br />2030<br />40<br />5060<br />203040<br />50<br />605 10 152025<br />30<br />-15<br />-10<br />-5<br />0<br />-20<br />-15<br />-10<br />-5<br />0<br />-30<br />-20<br />-10<br />0<br />10<br />Fig. 9 Relative MSE performance of the JRBcriterion, for several values of its parameters.<br />rMSE (dB)<br />n<br />rMSE (dB)<br />n<br />rMSE (dB)<br />n<br />JSUR<br />1<br />JSUR<br />3<br />without pruning<br />pruning m0= 50<br />ref.<br />black<br />gray<br />solid line<br />mixed line<br />black dashed line<br />Upper-left:<br />Upper-right:<br />Lower-left:<br />d = 1<br />d = 2<br />d = 3<br />20<br />30405060<br />20<br />30 40<br />50605 1015<br />20<br />2530<br />-15<br />-10<br />-5<br />0<br />-25<br />-20<br />-15<br />-10<br />-5<br />0<br />-40<br />-30<br />-20<br />-10<br />0<br />10<br />Fig. 10 Relative MSE performance of two SUR criteria, with and without the “pruning trick”<br />described in Section 3.4.</p>  <p>Page 28</p> <p>28<br />rMSE (dB)<br />n<br />rMSE (dB)<br />n<br />rMSE (dB)<br />n<br />JSUR<br />1<br />tIMSE (σ2<br />JRB(α = 0.5, δ = 1)<br />JEGL<br />ref.<br />black solid line<br />gray solid line<br />black mixed line<br />gray mixed line<br />black dashed line<br />ε= 10−6)<br />Upper-left:<br />Upper-right:<br />Lower-left:<br />d = 1<br />d = 2<br />d = 3<br />203040<br />50<br />60<br />20304050<br />60<br />5 1015202530<br />-15<br />-10<br />-5<br />0<br />-25<br />-20<br />-15<br />-10<br />-5<br />0<br />-30<br />-20<br />-10<br />0<br />10<br />Fig. 11 Relative MSE performance the best strategy in each category.<br />the covariance function of the Gaussian process after each new evaluation is likely to<br />succeed. If X is high-dimensional and f is expensive to evaluate, difficulties arise. In<br />particular, our sampling strategies do not take into account our uncertain knowledge<br />of the covariance parameters, and there is no guarantee that ML estimation will do<br />well when the points are chosen by a sampling strategy that favors some localized<br />target region (the neighboorhood the frontier of the domain of failure in this paper,<br />but the question is equally relevant in the field optimization, for instance). The difficult<br />problem of deciding the size n0of the initial design is crucial in this connection. Fully<br />Bayes procedures constitute a possible direction for future research, as long as they<br />don’t introduce an unacceptable computational overhead. Whatever the route, we feel<br />that the robustness of Gaussian process-based sampling strategies with respect to the<br />procedure of estimation of the covariance parameters should be addressed carefully in<br />order to make these methods usable in the industrial world.<br />Acknowledgements This work was partially supported by the French Agence Nationale de<br />la Recherche in the context of the project opus (ref. anr-07-cis7-010) and by the French pˆ ole<br />de comp´ etitivit´ e systematic in the context of the project csdl.</p>  <p>Page 29</p> <p>29<br />Appendix<br />A The Mat´ ern covariance<br />The exponential covariance and the Mat´ ern covariance are among the most conventionally<br />used stationary covariances in the literature of design and analysis of computer experiments.<br />The Mat´ ern covariance class (Yaglom, 1986) offers the possibility to adjust the regularity of ξ<br />with a single parameter. Stein (1999) advocates the use of the following parametrization of the<br />Mat´ ern function:<br />?<br />where Γ is the Gamma function and Kν is the modified Bessel function of the second kind.<br />The parameter ν &gt; 0 controls regularity at the origin of the function. To model a real-valued<br />function f defined over X ⊂ Rd, with d ≥ 1, we use the following anisotropic form of the<br />Mat´ ern covariance:<br /><br />i=1<br />κν(h) =<br />1<br />2ν−1Γ(ν)<br />2ν1/2h<br />?νKν<br />?<br />2ν1/2h<br />?<br />,h ∈ R<br />(35)<br />kθ(x,y) = σ2κν<br /><br />?<br />?<br />?<br />?<br />d<br />?<br />(x[i]− y[i])2<br />ρ2<br />i<br /><br />,x,y ∈ Rd<br />(36)<br />where x[i],y[i]denote the ithcoordinate of x and y, the positive scalar σ2is a variance pa-<br />rameter (we have kθ(x,x) = σ2), and the positive scalars ρirepresent scale or range param-<br />eters of the covariance, i.e., characteristic correlation lengths. Since σ2&gt; 0,ν &gt; 0,ρi &gt; 0,<br />i = 1,...,d, we can take the logarithm of these scalars, and consider the vector of parameters<br />θ = {log σ2,logν,−logρ1,...,−logρd} ∈ Rd+2, which is a practical parameterization when<br />σ2,ν,ρi, i = 1,...,d, need to be estimated from data.<br />B Calculation of Bichon et al.’s and Ranjan et al.’s criteria<br />B.1 A preliminary decomposition common to both criteria<br />Recall that t = Φ−1(1 − p), t+= t + α and t−= t − α. Then,<br />Gα,δ(p) = Gα,δ(1 − p) = E<br />?<br />?t+<br />= αδ?Φ(t+) − Φ(t−)?<br />?<br />max<br />?<br />0,αδ−??t − U??δ??<br />ϕ(u)du<br />=<br />αδ−|t−u|δ≥0<br />?<br />?<br />αδ− |t − u|δ?<br />=<br />t−<br />αδ− |t − u|δ?<br />ϕ(u)du<br />−<br />?t+<br />?<br />t−<br />|t − u|δϕ(u)du<br />???<br />Term A<br />.(37)<br />The computation of the integral A will be carried separately in the next two sections for δ = 1<br />and δ = 2. For this purpose, we shall need the following elementary results:<br />?b<br />?b<br />a<br />uϕ(u)du = ϕ(a) − ϕ(b),(38)<br />a<br />u2ϕ(u)du = aϕ(a) − bϕ(b) + Φ(b) − Φ(a).(39)</p>  <p>Page 30</p> <p>30<br />B.2 Case δ = 1<br />Let us compute the value A1 of the integral A for δ = 1:<br />A1 =<br />?t+<br />??t<br />= t?2Φ(t) − Φ(t−) − Φ(t+)?<br />t−<br />|t − u|ϕ(u)du =<br />?t<br />t−(t − u)ϕ(u)du +<br />?<br />+ 2ϕ(t) − ϕ(t−) − ϕ(t+),<br />?t+<br />t<br />(u − t)ϕ(u)du<br />?t+<br />= t<br />t−ϕ(u)du −<br />?t+<br />t<br />ϕ(u)du−<br />?t<br />t−uϕ(u)du +<br />t<br />uϕ(u)du<br />(40)<br />where (38) has been used to get the final result. Pluging (40) into (37) yields (33).<br />B.3 Case δ = 2<br />Let us compute the value A2 of the integral A for δ = 2:<br />A2 =<br />?t+<br />?t+<br />= t2?Φ(t+) − Φ(t−)?<br />+ t−ϕ(t−) − t+ϕ(t+) + Φ(t+) − Φ(t−),<br />t−<br />(t − u)2ϕ(u)du<br />= t2<br />t−<br />ϕ(u)du − 2t<br />?t+<br />− 2t?ϕ(t−) − ϕ(t+)?<br />t−<br />uϕ(u)du +<br />?t+<br />t−<br />u2ϕ(u)du<br />(41)<br />where (38) and (39) have been used to get the final result. Pluging (40) into (37) yields (34).<br />References<br />Arnaud, A., Bect, J., Couplet, M., Pasanisi, A., Vazquez, E.: ´Evaluation d’un risque<br />d’inondation fluviale par planification s´ equentielle d’exp´ eriences. In: 42` emes Journ´ ees de<br />Statistique (2010)<br />Au, S.K., Beck, J.: Estimation of small failure probabilities in high dimensions by subset<br />simulation. Probab. Engrg. Mechan. 16(4), 263–277 (2001)<br />Bayarri, M.J., Berger, J.O., Paulo, R., Sacks, J., Cafeo, J.A., Cavendish, J., Lin, C.H., Tu, J.:<br />A framework for validation of computer models. Technometrics 49(2), 138–154 (2007)<br />Berry, D.A., Fristedt, B.: Bandit problems: sequential allocation of experiments. Chapman &amp;<br />Hall (1985)<br />Bertsekas, D.P.: Dynamic programming and optimal control vol. 1. Athena Scientific (1995)<br />Bichon, B.J., Eldred, M.S., Swiler, L.P., Mahadevan, S., McFarland, J.M.: Efficient global<br />reliability analysis for nonlinear implicit performance functions.<br />2459–2468 (2008)<br />Bjerager, P.: On computational methods for structural reliability analysis. Structural Safety<br />9, 76–96 (1990)<br />Borri, A., Speranzini, E.: Structural reliability analysis using a standard deterministic finite<br />element code. Structural Safety 19(4), 361–382 (1997)<br />Box, G.E.P., Draper, N.R.: Empirical Model-Building and Response Surfaces. Wiley (1987)<br />Bucher, C.G., Bourgund, U.: A fast and efficient response surface approach for structural<br />reliability problems. Structural Safety 7(1), 57–66 (1990)<br />Chil` es, J.P., Delfiner, P.: Geostatistics: Modeling Spatial Uncertainty. Wiley, New York (1999)<br />AIAA Journal 46(10),</p>  <p>Page 31</p> <p>31<br />Currin, C., Mitchell, T., Morris, M., Ylvisaker, D.: Bayesian prediction of deterministic func-<br />tions, with applications to the design and analysis of computer experiments.<br />Statist. Assoc. 86(416), 953–963 (1991)<br />Deheeger, F.: Couplage m´ ecano-fiabiliste :2SMART – m´ ethodologie d’apprentissage stochas-<br />tique en fiabilit´ e. Ph.D. thesis, Universit´ e Blaise Pascal – Clermont II (2008)<br />Deheeger, F., Lemaire, M.: Support vector machine for efficient subset simulations:2SMART<br />method. In: Kanda, J., Takada, T., Furuta, H. (eds.) 10th International Conference on<br />Application of Statistics and Probability in Civil Engineering, Proceedings and Monographs<br />in Engineering, Water and Earth Sciences, pp. 259–260. Taylor &amp; Francis (2007)<br />Echard, B., Gayton, N., Lemaire, M.: Kriging-based monte carlo simulation to compute the<br />probability of failure efficiently: AK-MCS method. In: 6` emes Journ´ ees Nationales de Fia-<br />bilit´ e, 24–26 mars, Toulouse, France (2010)<br />Fleuret, F., Geman, D.: Graded learning for object detection. In: Proceedings of the workshop<br />on Statistical and Computational Theories of Vision of the IEEE International Conference<br />on Computer Vision and Pattern Recognition (CVPR/SCTV) (1999)<br />Frazier, P.I., Powell, W.B., Dayanik, S.: A knowledge-gradient policy for sequential information<br />collection. SIAM Journal on Control and Optimization 47(5), 2410–2439 (2008)<br />Ginsbourger, D.: M´ etamod` eles multiples pour l’approximation et l’optimisation de fonctions<br />num´ eriques multivariables. Ph.D. thesis, Ecole nationale sup´ erieure des Mines de Saint-<br />Etienne (2009)<br />Ginsbourger, D., Le Riche, R., L., C.: Kriging is well-suited to parallelize optimization. In:<br />Hiot, L.M., Ong, Y.S., Tenne, Y., Goh, C.K. (eds.) Computational Intelligence in Expen-<br />sive Optimization Problems, Adaptation Learning and Optimization, vol. 2, pp. 131–162.<br />Springer (2010)<br />Handcock, M.S., Stein, M.L.: A bayesian analysis of kriging. Technometrics 35(4), 403–410<br />(1993)<br />Hurtado, J.E.: An examination of methods for approximating implicit limit state functions<br />from the viewpoint of statistical learning theory. Structural Safety 26(3), 271–293 (2004)<br />Hurtado, J.E.: Filtered importance sampling with support vector margin: A powerful method<br />for structural reliability analysis. Structural Safety 29(1), 2–15 (2007)<br />Jones, D.R., Schonlau, M., William, J.: Efficient global optimization of expensive black-box<br />functions. Journal of Global Optimization 13(4), 455–492 (1998)<br />Kennedy, M., O’Hagan, A.: Bayesian calibration of computer models. Journal of the Royal<br />Statistical Society. Series B (Statistical Methodology) 63(3), 425–464 (2001)<br />Kimeldorf, G.S., Wahba, G.: A correspondence between Bayesian estimation on stochastic<br />processes and smoothing by splines. Ann. Math. Statist. 41(2), 495–502 (1970)<br />Kushner, H.J.: A new method of locating the maximum point of an arbitrary multipeak curve<br />in the presence of noise. J. Basic Engineering 86, 97–106 (1964)<br />Mockus, J.: Bayesian Approach to Global Optimization. Theory and Applications. Kluwer<br />Academic Publisher, Dordrecht (1989)<br />Mockus, J., Tiesis, V., Zilinskas, A.: The application of Bayesian methods for seeking the<br />extremum. In: Dixon, L., Szego, E.G. (eds.) Towards Global Optimization, vol. 2, pp. 117–<br />129. Elsevier (1978)<br />Oakley, J.: Estimating percentiles of uncertain computer code outputs. J. Roy. Statist. Soc.<br />Ser. C 53(1), 83–93 (2004)<br />Oakley, J., O’Hagan, A.: Bayesian inference for the uncertainty distribution of computer model<br />outputs. Biometrika 89(4) (2002)<br />Oakley, J., O’Hagan, A.: Probabilistic sensitivity analysis of complex models: a Bayesian ap-<br />proach. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 66(3),<br />751–769 (2004)<br />J. Amer.</p>  <p>Page 32</p> <p>32<br />O’Hagan, A.: Curve fitting and optimal design for prediction. Journal of the Royal Statistical<br />Society. Series B (Methodological) 40(1), 1–42 (1978)<br />Parzen, E.: An approach to time series analysis. Ann. Math. Stat. 32, 951–989 (1962)<br />Paulo, R.: Default priors for gaussian processes. Annals of Statistics 33(2), 556–582 (2005)<br />Picheny, V., Ginsbourger, D., Roustant, O., Haftka, R.T., Kim, N.H.: Adaptive designs of<br />experiments for accurate approximation of target regions (2010)<br />Piera-Martinez, M.: Mod´ elisation des comportements extrˆ emes en ing´ enierie. Ph.D. thesis,<br />Universit´ e Paris Sud - Paris XI (2008)<br />Pradlwarter, H., Schu¨ eller, G., Koutsourelakis, P., Charmpis, D.: Application of line sampling<br />simulation method to reliability benchmark problems. Structural Safety 29(3), 208 – 221<br />(2007). A Benchmark Study on Reliability in High Dimensions<br />Press, W.H., Teukolsky, S.A., Vetterling, W.T., Flannery, B.P.: Numerical Recipes in C. The<br />Art of Scientific Computing (Second Edition). Cambridge University Press (1992)<br />Rajashekhar, M.R., Ellingwood, B.R.: A new look at the response surface approach for relia-<br />bility analysis. Structural Safety 12(3), 205–220 (1993)<br />Ranjan, P., Bingham, D., Michailidis, G.: Sequential experiment design for contour estimation<br />from complex computer codes. Technometrics 50(4), 527–541 (2008)<br />Rubinstein, R., Kroese, D.: The Cross-Entropy Method. Springer (2004)<br />Sacks, J., Welch, W.J., Mitchell, T.J., Wynn, H.P.: Design and analysis of computer experi-<br />ments. Statistical Science 4(4), 409–435 (1989)<br />Santner, T.J., Williams, B.J., Notz, W.: The Design and Analysis of Computer Experiments.<br />Springer Verlag (2003)<br />Schueremans, L.: Probabilistic evaluation of structural unreinforced masonry. Ph.D. thesis,<br />Catholic University of Leuven (2001)<br />Schueremans, L., Gemert, D.V.: Benefit of splines and neural networks in simulation based<br />structural reliability analysis. Structural safety 27(3), 246–261 (2005)<br />Stein, M.L.: Interpolation of Spatial Data: Some Theory for Kriging. Springer, New York<br />(1999)<br />Vazquez, E., Bect, J.: A sequential Bayesian algorithm to estimate a probability of failure. In:<br />Proceedings of the 15th IFAC Symposium on System Identification, SYSID 2009 15th IFAC<br />Symposium on System Identification, SYSID 2009. Saint-Malo France (2009)<br />Vazquez, E., Piera-Martinez, M.: Estimation du volume des ensembles d’excursion d’un pro-<br />cessus gaussien par krigeage intrins` eque. In: 39` eme Journ´ ees de Statistiques Conf´ erence<br />Journ´ ee de Statistiques. Angers France (2007)<br />Vestrup, E.M.: The Theory of Measures and Integration. Wiley (2003)<br />Villemonteix, J.: Optimisation de fonctions coˆ uteuses. Ph.D. thesis, Universit´ e Paris-Sud XI,<br />Facult´ e des Sciences d’Orsay (2008)<br />Villemonteix, J., Vazquez, E., Walter, E.: An informational approach to the global optimization<br />of expensive-to-evaluate functions. Journal of Global Optimization 44(4), 509–534 (2009)<br />Waarts, P.H.: Structural reliability using finite element methods: an appraisal of DARS. Ph.D.<br />thesis, Delft University of Technology (2000)<br />Welch, W.J., Buck, R.J., Sacks, J., Wynn, H.P., Mitchell, T.J., Morris, M.D.: Screening, pre-<br />dicting and computer experiments. Technometrics 34, 15–25 (1992)<br />Yaglom, A.M.: Correlation Theory of Stationary and Related Random Functions I: Basic<br />Results. Springer Series in Statistics. Springer-Verlag, New york (1986)</p>  <a href="https://www.researchgate.net/profile/David_Ginsbourger/publication/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure/links/00b4951ad79fb75b24000000.pdf">Download full-text</a> </div> <div id="rgw26_56aba18d812a4" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw27_56aba18d812a4">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw28_56aba18d812a4"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/David_Ginsbourger/publication/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure/links/00b4951ad79fb75b24000000.pdf" class="publication-viewer" title="1009.5177.pdf">1009.5177.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/David_Ginsbourger">David Ginsbourger</a> &middot; Jan 21, 2016 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw29_56aba18d812a4"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://arxiv.org/pdf/1009.5177.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Sequential design of computer experiments for the estimation of a
probability of failure">Sequential design of computer experiments for the ...</a> </div>  <div class="details">   Available from <a href="http://arxiv.org/pdf/1009.5177.pdf" target="_blank" rel="nofollow">ArXiv</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw31_56aba18d812a4" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw32_56aba18d812a4">  </ul> </div> </div>   <div id="rgw22_56aba18d812a4" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw23_56aba18d812a4"> <div> <h5> <a href="publication/292074525_Estimating_Shape_Constrained_Functions_Using_Gaussian_Processes" class="color-inherit ga-similar-publication-title"><span class="publication-title">Estimating Shape Constrained Functions Using Gaussian Processes</span></a>  </h5>  <div class="authors"> <a href="researcher/2007637025_Xiaojing_Wang" class="authors ga-similar-publication-author">Xiaojing Wang</a>, <a href="researcher/14502191_James_O_Berger" class="authors ga-similar-publication-author">James O. Berger</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw24_56aba18d812a4"> <div> <h5> <a href="publication/291816598_A_novel_strategy_of_data_characteristic_test_for_selecting_process_monitoring_method_automatically" class="color-inherit ga-similar-publication-title"><span class="publication-title">A novel strategy of data characteristic test for selecting process monitoring method automatically</span></a>  </h5>  <div class="authors"> <a href="researcher/2084308549_Shumei_Zhang" class="authors ga-similar-publication-author">Shumei Zhang</a>, <a href="researcher/9578343_Fuli_Wang" class="authors ga-similar-publication-author">Fuli Wang</a>, <a href="researcher/2095674429_Luping_Zhao" class="authors ga-similar-publication-author">Luping Zhao</a>, <a href="researcher/13741264_Shu_Wang" class="authors ga-similar-publication-author">Shu Wang</a>, <a href="researcher/2059974219_Yuqing_Chang" class="authors ga-similar-publication-author">Yuqing Chang</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw25_56aba18d812a4"> <div> <h5> <a href="publication/291557235_Bivariate_Analysis_of_Incomplete_Degradation_Observations_Based_on_Inverse_Gaussian_Processes_and_Copulas" class="color-inherit ga-similar-publication-title"><span class="publication-title">Bivariate Analysis of Incomplete Degradation Observations Based on Inverse Gaussian Processes and Copulas</span></a>  </h5>  <div class="authors"> <a href="researcher/2024916213_Weiwen_Peng" class="authors ga-similar-publication-author">Weiwen Peng</a>, <a href="researcher/2024538341_Yan-Feng_Li" class="authors ga-similar-publication-author">Yan-Feng Li</a>, <a href="researcher/2045787984_Yuan-Jian_Yang" class="authors ga-similar-publication-author">Yuan-Jian Yang</a>, <a href="researcher/2024213535_Shun-Peng_Zhu" class="authors ga-similar-publication-author">Shun-Peng Zhu</a>, <a href="researcher/11363817_Hong-Zhong_Huang" class="authors ga-similar-publication-author">Hong-Zhong Huang</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw45_56aba18d812a4" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw46_56aba18d812a4">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw47_56aba18d812a4" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=ZyqCP8Hx7H8t13x7JVqeRgppc85DHPY6OltfDhc_d12LGJB7yeQkKhTCmjikOqAT" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="tPUdiIQwIj0qqh1574yaJ2eQUUHEFQOwsSHc3qWoKSuNECx375odJF1bFAwTcZoEXAWqVZFD/9uZsQMjnFfCyCw5/lK3VZDOHhZRQRFi2FgSN4E6MbTi8B/KJZSWIgii6CDDKjlOGLVjk8U5UuXsH17HFG1BTCk84u5EfDvSBPZF9dMFU5AGPmqQmZazLP6rDqa3WUdQ9jJgnBAUnLolmt1WkMynpEtBKp3PiJdtbXBHKUKDCJjtsBThE18BLIhF85ixExucCyn9M5ejxwC9DNawgZjbP7LI5gK+O6hFG5U="/> <input type="hidden" name="urlAfterLogin" value="publication/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vNDY1ODc0ODZfU2VxdWVudGlhbF9kZXNpZ25fb2ZfY29tcHV0ZXJfZXhwZXJpbWVudHNfZm9yX3RoZV9lc3RpbWF0aW9uX29mX2Fwcm9iYWJpbGl0eV9vZl9mYWlsdXJl"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vNDY1ODc0ODZfU2VxdWVudGlhbF9kZXNpZ25fb2ZfY29tcHV0ZXJfZXhwZXJpbWVudHNfZm9yX3RoZV9lc3RpbWF0aW9uX29mX2Fwcm9iYWJpbGl0eV9vZl9mYWlsdXJl"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vNDY1ODc0ODZfU2VxdWVudGlhbF9kZXNpZ25fb2ZfY29tcHV0ZXJfZXhwZXJpbWVudHNfZm9yX3RoZV9lc3RpbWF0aW9uX29mX2Fwcm9iYWJpbGl0eV9vZl9mYWlsdXJl"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw48_56aba18d812a4"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 469;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FigureList","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"David Ginsbourger","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272766523932678%401442044015317_m\/David_Ginsbourger.png","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/David_Ginsbourger","institution":"Universit\u00e4t Bern","institutionUrl":false,"widgetId":"rgw4_56aba18d812a4"},"id":"rgw4_56aba18d812a4","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=3114215","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56aba18d812a4"},"id":"rgw3_56aba18d812a4","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=46587486","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":46587486,"title":"Sequential design of computer experiments for the estimation of a\nprobability of failure","journalTitle":"Statistics and Computing","journalDetailsTooltip":{"data":{"journalTitle":"Statistics and Computing","journalAbbrev":"STAT COMPUT","publisher":"Springer Verlag","issn":"0960-3174","impactFactor":"1.62","fiveYearImpactFactor":"1.92","citedHalfLife":">10.0","immediacyIndex":"0.27","eigenFactor":"0.01","articleInfluence":"1.66","widgetId":"rgw6_56aba18d812a4"},"id":"rgw6_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=0960-3174","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Article","details":{"doi":"10.1007\/s11222-011-9241-4","journalInfos":{"journal":"","publicationDate":"09\/2010;","publicationDateRobot":"2010-09","article":"22(3).","journalTitle":"Statistics and Computing","journalUrl":"journal\/0960-3174_Statistics_and_Computing","impactFactor":1.62}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1009.5177","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1007\/s11222-011-9241-4"},{"key":"rft.atitle","value":"Sequential design of computer experiments for the estimation of a\nprobability of failure"},{"key":"rft.title","value":"Statistics and Computing"},{"key":"rft.jtitle","value":"Statistics and Computing"},{"key":"rft.volume","value":"22"},{"key":"rft.issue","value":"3"},{"key":"rft.date","value":"2010"},{"key":"rft.issn","value":"0960-3174"},{"key":"rft.au","value":"Julien Bect,David Ginsbourger,Ling Li,Victor Picheny,Emmanuel Vazquez"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56aba18d812a4"},"id":"rgw7_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=46587486","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":46587486,"peopleItems":[{"data":{"authorUrl":"researcher\/14088780_Julien_Bect","authorNameOnPublication":"Julien Bect","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Julien Bect","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/14088780_Julien_Bect","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw10_56aba18d812a4"},"id":"rgw10_56aba18d812a4","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=14088780&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw9_56aba18d812a4"},"id":"rgw9_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=14088780&authorNameOnPublication=Julien%20Bect","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"David Ginsbourger","accountUrl":"profile\/David_Ginsbourger","accountKey":"David_Ginsbourger","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272766523932678%401442044015317_m\/David_Ginsbourger.png","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"David Ginsbourger","profile":{"professionalInstitution":{"professionalInstitutionName":"Universit\u00e4t Bern","professionalInstitutionUrl":"institution\/Universitaet_Bern"}},"professionalInstitutionName":"Universit\u00e4t Bern","professionalInstitutionUrl":"institution\/Universitaet_Bern","url":"profile\/David_Ginsbourger","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272766523932678%401442044015317_l\/David_Ginsbourger.png","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"David_Ginsbourger","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw12_56aba18d812a4"},"id":"rgw12_56aba18d812a4","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=3114215&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Universit\u00e4t Bern","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":5,"accountCount":1,"publicationUid":46587486,"widgetId":"rgw11_56aba18d812a4"},"id":"rgw11_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=3114215&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=5&accountCount=1&publicationUid=46587486","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/47265550_Ling_Li","authorNameOnPublication":"Ling Li","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Ling Li","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/47265550_Ling_Li","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw14_56aba18d812a4"},"id":"rgw14_56aba18d812a4","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=47265550&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw13_56aba18d812a4"},"id":"rgw13_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=47265550&authorNameOnPublication=Ling%20Li","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/19512962_Victor_Picheny","authorNameOnPublication":"Victor Picheny","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Victor Picheny","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/19512962_Victor_Picheny","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw16_56aba18d812a4"},"id":"rgw16_56aba18d812a4","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=19512962&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw15_56aba18d812a4"},"id":"rgw15_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=19512962&authorNameOnPublication=Victor%20Picheny","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/26760014_Emmanuel_Vazquez","authorNameOnPublication":"Emmanuel Vazquez","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Emmanuel Vazquez","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/26760014_Emmanuel_Vazquez","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw18_56aba18d812a4"},"id":"rgw18_56aba18d812a4","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=26760014&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw17_56aba18d812a4"},"id":"rgw17_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=26760014&authorNameOnPublication=Emmanuel%20Vazquez","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56aba18d812a4"},"id":"rgw8_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=46587486&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":46587486,"abstract":"<noscript><\/noscript><div>This paper deals with the problem of estimating the volume of the excursion set of a function f:\u211dd<br \/>\n&rarr;\u211d above a given threshold, under a probability measure on \u211dd<br \/>\nthat is assumed to be known. In the industrial world, this corresponds to the problem of estimating a probability of failure of a system. When only an expensive-to-simulate model of the system is available, the budget for simulations is usually severely limited and therefore classical Monte Carlo methods ought to be avoided. One of the main contributions of this article is to derive SUR (stepwise uncertainty reduction) strategies from a Bayesian formulation of the problem of estimating a probability of failure. These sequential strategies use a Gaussian process model of f and aim at performing evaluations of f as efficiently as possible to infer the value of the probability of failure. We compare these strategies to other strategies also based on a Gaussian process model for estimating a probability of failure.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw19_56aba18d812a4"},"id":"rgw19_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=46587486","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":{"data":{"figures":[{"imageUrl":"https:\/\/www.researchgate.net\/profile\/David_Ginsbourger\/publication\/46587486\/figure\/fig1\/Fig-5-Left-mesh-plot-of-the-performance-function-f-corresponding-to-the-four-branch.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/David_Ginsbourger\/publication\/46587486\/figure\/fig1\/Fig-5-Left-mesh-plot-of-the-performance-function-f-corresponding-to-the-four-branch_small.png","figureUrl":"\/figure\/46587486_fig1_Fig-5-Left-mesh-plot-of-the-performance-function-f-corresponding-to-the-four-branch","selected":false,"title":"Fig. 5 Left: mesh plot of the performance function f corresponding to...","key":"46587486_fig1_Fig-5-Left-mesh-plot-of-the-performance-function-f-corresponding-to-the-four-branch"}],"readerDocId":"2758294","linkBehaviour":"dialog","isDialog":true,"headerText":"Figures in this publication","isNewPublicationDesign":false,"widgetId":"rgw20_56aba18d812a4"},"id":"rgw20_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/FigureList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FigureList.html?readerDocId=2758294&isDialog=1&linkBehaviour=dialog","viewClass":"views.publicliterature.FigureListView","yuiModules":["rg.views.publicliterature.FigureListView","css-pow-publicliterature-FigureList"],"stylesheets":["pow\/publicliterature\/FigureList.css"],"_isYUI":true},"previewImage":"https:\/\/i1.rgstatic.net\/publication\/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure\/links\/00b4951ad79fb75b24000000\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw21_56aba18d812a4"},"id":"rgw21_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56aba18d812a4"},"id":"rgw5_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=46587486&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2007637025,"url":"researcher\/2007637025_Xiaojing_Wang","fullname":"Xiaojing Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":14502191,"url":"researcher\/14502191_James_O_Berger","fullname":"James O. Berger","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/292074525_Estimating_Shape_Constrained_Functions_Using_Gaussian_Processes","usePlainButton":true,"publicationUid":292074525,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/292074525_Estimating_Shape_Constrained_Functions_Using_Gaussian_Processes","title":"Estimating Shape Constrained Functions Using Gaussian Processes","displayTitleAsLink":true,"authors":[{"id":2007637025,"url":"researcher\/2007637025_Xiaojing_Wang","fullname":"Xiaojing Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":14502191,"url":"researcher\/14502191_James_O_Berger","fullname":"James O. Berger","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["01\/2016; 4(1):1-25. DOI:10.1137\/140955033"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/292074525_Estimating_Shape_Constrained_Functions_Using_Gaussian_Processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/292074525_Estimating_Shape_Constrained_Functions_Using_Gaussian_Processes\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw23_56aba18d812a4"},"id":"rgw23_56aba18d812a4","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=292074525","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2084308549,"url":"researcher\/2084308549_Shumei_Zhang","fullname":"Shumei Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9578343,"url":"researcher\/9578343_Fuli_Wang","fullname":"Fuli Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095674429,"url":"researcher\/2095674429_Luping_Zhao","fullname":"Luping Zhao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":13741264,"url":"researcher\/13741264_Shu_Wang","fullname":"Shu Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Industrial & Engineering Chemistry Research","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291816598_A_novel_strategy_of_data_characteristic_test_for_selecting_process_monitoring_method_automatically","usePlainButton":true,"publicationUid":291816598,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.59","url":"publication\/291816598_A_novel_strategy_of_data_characteristic_test_for_selecting_process_monitoring_method_automatically","title":"A novel strategy of data characteristic test for selecting process monitoring method automatically","displayTitleAsLink":true,"authors":[{"id":2084308549,"url":"researcher\/2084308549_Shumei_Zhang","fullname":"Shumei Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9578343,"url":"researcher\/9578343_Fuli_Wang","fullname":"Fuli Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095674429,"url":"researcher\/2095674429_Luping_Zhao","fullname":"Luping Zhao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":13741264,"url":"researcher\/13741264_Shu_Wang","fullname":"Shu Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2059974219,"url":"researcher\/2059974219_Yuqing_Chang","fullname":"Yuqing Chang","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Industrial & Engineering Chemistry Research 01\/2016;  DOI:10.1021\/acs.iecr.5b03525"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291816598_A_novel_strategy_of_data_characteristic_test_for_selecting_process_monitoring_method_automatically","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291816598_A_novel_strategy_of_data_characteristic_test_for_selecting_process_monitoring_method_automatically\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw24_56aba18d812a4"},"id":"rgw24_56aba18d812a4","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291816598","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2024916213,"url":"researcher\/2024916213_Weiwen_Peng","fullname":"Weiwen Peng","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2024538341,"url":"researcher\/2024538341_Yan-Feng_Li","fullname":"Yan-Feng Li","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2045787984,"url":"researcher\/2045787984_Yuan-Jian_Yang","fullname":"Yuan-Jian Yang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2024213535,"url":"researcher\/2024213535_Shun-Peng_Zhu","fullname":"Shun-Peng Zhu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"IEEE Transactions on Reliability","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291557235_Bivariate_Analysis_of_Incomplete_Degradation_Observations_Based_on_Inverse_Gaussian_Processes_and_Copulas","usePlainButton":true,"publicationUid":291557235,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.93","url":"publication\/291557235_Bivariate_Analysis_of_Incomplete_Degradation_Observations_Based_on_Inverse_Gaussian_Processes_and_Copulas","title":"Bivariate Analysis of Incomplete Degradation Observations Based on Inverse Gaussian Processes and Copulas","displayTitleAsLink":true,"authors":[{"id":2024916213,"url":"researcher\/2024916213_Weiwen_Peng","fullname":"Weiwen Peng","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2024538341,"url":"researcher\/2024538341_Yan-Feng_Li","fullname":"Yan-Feng Li","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2045787984,"url":"researcher\/2045787984_Yuan-Jian_Yang","fullname":"Yuan-Jian Yang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2024213535,"url":"researcher\/2024213535_Shun-Peng_Zhu","fullname":"Shun-Peng Zhu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":11363817,"url":"researcher\/11363817_Hong-Zhong_Huang","fullname":"Hong-Zhong Huang","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["IEEE Transactions on Reliability 01\/2016;  DOI:10.1109\/TR.2015.2513038"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291557235_Bivariate_Analysis_of_Incomplete_Degradation_Observations_Based_on_Inverse_Gaussian_Processes_and_Copulas","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291557235_Bivariate_Analysis_of_Incomplete_Degradation_Observations_Based_on_Inverse_Gaussian_Processes_and_Copulas\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw25_56aba18d812a4"},"id":"rgw25_56aba18d812a4","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291557235","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw22_56aba18d812a4"},"id":"rgw22_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=46587486&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":46587486,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":46587486,"publicationType":"article","linkId":"00b4951ad79fb75b24000000","fileName":"1009.5177.pdf","fileUrl":"profile\/David_Ginsbourger\/publication\/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure\/links\/00b4951ad79fb75b24000000.pdf","name":"David Ginsbourger","nameUrl":"profile\/David_Ginsbourger","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Jan 21, 2016","fileSize":"1.04 MB","widgetId":"rgw28_56aba18d812a4"},"id":"rgw28_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=46587486&linkId=00b4951ad79fb75b24000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":46587486,"publicationType":"article","linkId":"0f64a39e38294e886aa24c19","fileName":"Sequential design of computer experiments for the estimation of a\nprobability of failure","fileUrl":"http:\/\/arxiv.org\/pdf\/1009.5177.pdf","name":"ArXiv","nameUrl":"http:\/\/arxiv.org\/pdf\/1009.5177.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw29_56aba18d812a4"},"id":"rgw29_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=46587486&linkId=0f64a39e38294e886aa24c19&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw27_56aba18d812a4"},"id":"rgw27_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=46587486&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":47,"valueFormatted":"47","widgetId":"rgw30_56aba18d812a4"},"id":"rgw30_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=46587486","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw26_56aba18d812a4"},"id":"rgw26_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=46587486&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":46587486,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw32_56aba18d812a4"},"id":"rgw32_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=46587486&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":47,"valueFormatted":"47","widgetId":"rgw33_56aba18d812a4"},"id":"rgw33_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=46587486","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw31_56aba18d812a4"},"id":"rgw31_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=46587486&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"arXiv:1009.5177v1  [stat.CO]  27 Sep 2010\nSequential design of computer experiments for the\nestimation of a probability of failure\nJulien Bect\u22c6\u00b7 David Ginsbourger \u00b7 Ling Li \u00b7\nVictor Picheny \u00b7 Emmanuel Vazquez\u22c6\nAbstract This paper deals with the problem of estimating the volume of the excursion\nset of a function f : Rd\u2192 R above a given threshold, under a probability measure\non Rdthat is assumed to be known. In the industrial world, this corresponds to the\nproblem of estimating a probability of failure of a system. When only an expensive-to-\nsimulate model of the system is available, the budget for simulations is usually severely\nlimited and therefore classical Monte Carlo methods ought to be avoided. One of the\nmain contributions of this article is to derive SUR (stepwise uncertainty reduction)\nstrategies from a Bayesian-theoretic formulation of the problem of estimating a prob-\nability of failure. These sequential strategies use a Gaussian process model of f and\naim at performing evaluations of f as efficiently as possible to infer the value of the\nprobability of failure. We compare these strategies to other strategies also based on a\nGaussian process model for estimating a probability of failure.\nKeywords Computer experiments \u00b7 Sequential design \u00b7 Gaussian processes \u00b7\nProbability of failure \u00b7 Stepwise uncertainty reduction\n\u22c6corresponding authors\nJ. Bect, L. Li, and E. Vazquez\nSUPELEC, Gif-sur-Yvette, France.\nE-mail: {firstname}.{lastname}@supelec.fr\nV. Picheny\nEcole Centrale Paris, Chatenay-Malabry, France.\nE-mail: victor.picheny@ecp.fr\nD. Ginsbourger\nInstitute of Mathematical Statistics and Actuarial Science, University of Bern, Switzerland.\nE-mail: david.ginsbourger@stat.unibe.ch"},{"page":2,"text":"2\n1 Introduction\nThe design of a system or a technological product has to take into account the fact\nthat some design parameters are subject to unknown variations that may affect the\nreliability of the system. In particular, it is important to estimate the probability of\nthe system working under abnormal or dangerous operating conditions due to random\ndispersions of its characteristic parameters. The probability of failure of a system is\nusually expressed as the probability of the excursion set of a function above a fixed\nthreshold. More precisely, let f be a measurable real function defined over a probability\nspace (X,B(X),PX), with X \u2286 Rd, and let u \u2208 R be a threshold. The problem to be\nconsidered in this paper is the estimation of the volume, under PX, of the excursion\nset\n\u0393 := {x \u2208 X : f(x) > u}\n(1)\nof the function f above the level u. In the context of robust design, the volume \u03b1 :=\nPX(\u0393) can be viewed as the probability of failure of a system. Then, X plays the\nrole of an input or factor space, the probability PX models the uncertainty on the\ninputs or the factors, and f is some performance function derived from the outputs\nof the system, whose evaluation may involve complex and time-consuming computer\nsimulations. When f is expensive to evaluate, the estimation of \u03b1 must be carried out\nwith a restricted number of evaluations of f, generally excluding the estimation of the\nprobability of excursion by a Monte Carlo approach. Indeed, consider the empirical\nestimator\n\u03b1m :=1\nm\nm\n?\ni=1\n1{f(Xi)>u},(2)\nwhere the Xis are independent random variables with distribution PX. According to\nthe strong law of large numbers, the estimator \u03b1m converges to \u03b1 almost surely when\nm increases. Moreover, it is an unbiased estimator of \u03b1, i.e. E(\u03b1m) = \u03b1. Its mean\nsquare error is\nE?(\u03b1m\u2212 \u03b1)2?=1\nIf the probability of failure \u03b1 is small, then the standard deviation of \u03b1m is approxi-\nmately (\u03b1\/m)1\/2. To achieve a given standard deviation \u03b4\u03b1 thus requires approximately\n1\/(\u03b42\u03b1) evaluations, which can be prohibitively high if \u03b1 is small. By way of illustra-\ntion, if \u03b1 = 2 \u00d7 10\u22123and \u03b4 = 0.1, we obtain m = 50000. If one evaluation of f takes,\nsay, one minute, then the entire estimation procedure will take about 35 days to com-\nm\u03b1?1 \u2212 \u03b1?.\nplete. Of course, a host of refined random sampling methods have been proposed to\nimprove over the basic Monte Carlo convergence rate; for instance, methods based on\nimportance sampling with cross-entropy (Rubinstein and Kroese, 2004), subset sam-\npling (Au and Beck, 2001) or line sampling (Pradlwarter et al., 2007). They will not\nbe considered here for the sake of brevity and because the required number of function\nevaluations is still very high."},{"page":3,"text":"3\nUntil recently, all the methods that do not require a large number of evaluations\nof f were based on the use of parametric approximations for either the function f\nitself or the boundary \u2202\u0393 of \u0393. The so-called response surface method falls in the first\ncategory (see, e.g., Bucher and Bourgund, 1990; Rajashekhar and Ellingwood, 1993,\nand references therein). The most popular approaches in the second category are the\nfirst- and second-order reliability method (FORM and SORM), which are based on a\nlinear or quadratic approximation of \u2202\u0393 around the most probable failure point (see,\ne.g., Bjerager, 1990). In all these methods, the accuracy of the estimator depends on\nthe actual shape of either f or \u2202\u0393 and its resemblance to the approximant: they do\nnot provide statistically consistent estimators of the probability of failure.\nThis paper focuses on sequential sampling strategies based on Gaussian processes\nand kriging, which can been seen as a non-parametric approximation method. Several\nstrategies of this kind have been proposed recently in the literature by Ranjan et al.\n(2008), Bichon et al. (2008), Picheny et al. (2010) and Echard et al. (2010). The idea\nis that the Gaussian process model, which captures prior knowledge about the un-\nknown function f, makes it possible to assess the uncertainty about the position\nof \u0393 given a set of evaluation results. This line of research has its roots in the\nfield of design and analysis of computer experiments (see, e.g., Sacks et al., 1989;\nCurrin et al., 1991; Welch et al., 1992; Oakley and O\u2019Hagan, 2002, 2004; Oakley, 2004;\nBayarri et al., 2007). More specifically, kriging-based sequential strategies for the esti-\nmation of a probability of failure are closely related to the field of Bayesian global op-\ntimization (Mockus et al., 1978; Mockus, 1989; Jones et al., 1998; Villemonteix, 2008;\nVillemonteix et al., 2009; Ginsbourger, 2009).\nThe contribution of this paper is twofold. First, we introduce a Bayesian decision-\ntheoretic framework from which the theoretical form of an optimal strategy for the\nestimation of a probability of failure can be derived. One-step lookahead sub-optimal\nstrategies are then proposed1, which are suitable for numerical evaluation and imple-\nmentation on computers. These strategies will be called SUR (stepwise uncertainty\nreduction) strategies in reference to the work of D. Geman and its collaborators (see,\ne.g. Fleuret and Geman, 1999). Second, we provide a review in a unified framework\nof all the kriging-based strategies proposed so far in the literature and compare them\nnumerically with the SUR strategies proposed in this paper.\nThe outline of the paper is as follows. Section 2 introduces the Bayesian framework\nand recalls the basics of dynamic programming and Gaussian processes. Section 3 in-\ntroduces SUR strategies, from the decision-theoretic underpinnings, down to the imple-\nmentation level. Section 4 provides a review of other kriging-based strategies proposed\nin the literature. Section 5 provides some illustrations and reports an empirical com-\n1Preliminary accounts of this work have been presented in Vazquez and Piera-Martinez\n(2007) and Vazquez and Bect (2009)."},{"page":4,"text":"4\nparison of these sampling criteria. Finally, Section 6 presents conclusions and offers\nperspectives for future work.\n2 Bayesian decision-theoretic framework\n2.1 Bayes risk and sequential strategies\nLet f be a continuous function. We shall assume that f corresponds to a computer\nprogram whose output is not a closed-form expression of the inputs. Our objective is\nto obtain a numerical approximation of the probability of failure\n\u03b1(f) = PX{x \u2208 X : f(x) > u} =\n?\nX\n1f>udPX,(3)\nwhere 1f>ustands for the characteristic function of the excursion set \u0393, such that\nfor any x \u2208 X, 1f>u(x) equals one if x \u2208 \u0393 and zero otherwise. The approximation\nof \u03b1(f) has to be built from a set of computer experiments, where an experiment\nsimply consists in choosing an x \u2208 X and computing the value of f at x. The result\nof a pointwise evaluation of f carries information about f and quantities depending\non f and, in particular, about 1f>uand \u03b1(f). In the context of expensive computer\nexperiments, we shall also suppose that the number of evaluations is limited. Thus,\nthe estimation of \u03b1(f) must be carried out using a fixed number, say N, of evaluations\nof f.\nA sequential non-randomized algorithm to estimate \u03b1(f) with a budget of N eval-\nuations is a pair (XN, ? \u03b1N),\nXN: f ?\u2192 XN(f) = (X1(f),X2(f),...,XN(f)) \u2208 XN,\n? \u03b1N: f ?\u2192 ? \u03b1N(f) \u2208 R+,\nwith the following properties:\na) There exists x1\u2208 X such that X1(f) = x1, i.e. X1does not depend on f.\nb) Let Zn(f) = f(Xn(f)), 1 \u2264 n \u2264 N. For all 1 \u2264 n < N, Xn+1(f) depends measur-\nably2on In(f), where In = ((X1,Z1),...,(Xn,Zn)).\nc) ? \u03b1N(f) depends measurably on IN(f).\nand ? \u03b1N will be called an estimator. The algorithm (XN, ? \u03b1N) describes a sequence\nprior to any evaluation; for each n = 1,...,N \u2212 1, the algorithm uses information\nIn(f) to choose the next evaluation point Xn+1(f); the estimation ? \u03b1N(f) of \u03b1(f) is\nfurther restricted: for instance, when K computer simulations can be run in parallel,\nThe mapping XNwill be referred to as a strategy, or policy, or design of experiments,\nof decisions, made from an increasing amount of information: X1(f) = x1is chosen\nthe terminal decision. In some applications, the class of sequential algorithms must be\n2i.e., there is a measurable map \u03d5n: (X \u00d7 R)n\u2192 X such that Xn= \u03d5n\u25e6 In"},{"page":5,"text":"5\nalgorithms that query batches of K evaluations at a time may be preferred (see, e.g.\nGinsbourger et al., 2010). In this paper no such restriction is imposed.\nThe choice of the estimator ? \u03b1Nwill be addressed in Section 2.4: for now, we simply\nstrategy XN; that is, one that will produce a good final approximation ? \u03b1N(f) of \u03b1(f).\nGiven a loss function L : R\u00d7R \u2192 R, we define the error of approximation of a strategy\nXN\u2208 ANon f as \u01eb(XN,f) = L(? \u03b1N(f),\u03b1(f)). In this paper, we shall consider the\nWe adopt a Bayesian approach to this decision problem: the unknown function f\nassume that an estimator has been chosen, and focus on the problem of finding a good\nLet ANbe the class of all strategies XNthat query sequentially N evaluations of f.\nquadratic loss function, so that \u01eb(XN,f) = (? \u03b1N(f) \u2212 \u03b1(f))2.\nis considered as a sample path of a real-valued random process \u03be defined on some\nprobability space (\u03a9,B,P0) with parameter in x \u2208 X, and a good strategy is a strat-\negy that achieves, or gets close to, the Bayes risk rB := infXN\u2208ANE0(\u01eb(XN,\u03be)),\nwhere E0 denotes the expectation with respect to P0. From a subjective Bayesian\npoint of view, the stochastic model \u03be is a representation of our uncertain initial knowl-\nedge about f. From a more pragmatic perspective, the prior distribution can be seen\nas a tool to define a notion of a good strategy in an average sense. Another interest-\ning route, not followed in this paper, would have been to consider the minimax risk\ninfXN\u2208ANmaxfE0(\u01eb(XN,\u03be)) over some class of functions.\nNotations. From now on, we shall consider the stochastic model \u03be instead of\nthe deterministic function f and, for abbreviation, the explicit dependence on \u03be will be\ndropped when no there is no risk of confusion; e.g., ? \u03b1Nwill denote the random variable\nand En to denote respectively the \u03c3-algebra generated by In, the conditional distribu-\ntion P0(\u00b7 | Fn) and the conditional expectation E0(\u00b7 | Fn). Note that the dependence\nof Xn+1on In can be rephrased by saying that Xn+1is Fn-measurable. Recall that\nEn(Z) is Fn-measurable, and thus can be seen as a measurable function of In, for any\nrandom variable Z.\n? \u03b1N(\u03be), Xnwill denote the random variable Xn(\u03be), etc. We will use the notations Fn, Pn\n2.2 Optimal and k-step lookahead strategies\nIt is well-known (see, e.g., Berry and Fristedt, 1985; Mockus, 1989; Bertsekas, 1995)\nthat an optimal strategy for such a finite horizon problem, i.e. a strategy X\u22c6\nsuch that E0(\u01eb(X\u22c6\nN,\u03be)) = rB, can be formally obtained by dynamic programming:\nlet RN = EN(\u01eb(XN,\u03be)) = EN\nbackward induction\nN\u2208 AN\n?(? \u03b1N\u2212 \u03b1)2?\n?Rn+1| Xn+1= x?,\ndenote the terminal risk and define by\nRn = min\nx\u2208XEn\nn = N \u2212 1,...,0. (4)\nTo get an insight into (4), notice that Rn+1, n = 0,...,N \u2212 1, depends measurably\non In+1= (In,Xn+1,Zn+1), so that En\n?Rn+1| Xn+1= x?\nis in fact an expectation"},{"page":6,"text":"6\nwith respect to Zn+1, and Rn is an Fn-measurable random variable. Then, we have\nR0= rB, and the strategy X\u22c6\nNdefined by\n?Rn+1| Xn+1= x?,\nis optimal3. It is crucial to observe here that, for this dynamic programming problem,\nX\u22c6\nn+1= argmin\nx\u2208X\nEn\nn = 1,...,N \u2212 1, (5)\nboth the space of possible actions and the space of possible outcomes at each step are\ncontinuous, and the state space (X \u00d7 R)nat step n is of dimension n(d + 1). Any\ndirect attempt at solving (4)\u2013(5) numerically, over an horizon N of more than a few\nsteps, will suffer from the curse of dimensionality.\nUsing (4), the optimal strategy can be expanded as\n?\nA very general approach to construct sub-optimal\u2014but hopefully good\u2014strategies is\nX\u22c6\nn+1= argmin\nx\u2208X\nEn\nmin\nXn+2En+1... min\nXN\nEN\u22121RN\n??? Xn+1= x\n?\n.\nto truncate this expansion after k terms, replacing the exact risk Rn+kby any available\nsurrogate? Rn+k. Examples of such surrogates will be given in Sections 3 and 4. The\n?\nresulting strategy,\nXn+1= argmin\nx\u2208X\nEn\nmin\nXn+2\nEn+1... min\nXn+k\nEn+k\u22121? Rn+k\n??? Xn+1= x\n?\n. (6)\nis called a k-step lookahead strategy (see, e.g., Bertsekas, 1995, Section 6.3). Note that\nboth the optimal strategy (5) and the k-step lookahead strategy implicitly define a\nsampling criterion Jn(x), Fn-measurable, the minimum of which indicates the next\nevaluation to be performed. For instance, in the case of the k-step lookahead strategy,\nthe sampling criterion is\nJn(x) = En\n?\nmin\nXn+2\nEn+1... min\nXn+k\nEn+k\u22121? Rn+k\n??? Xn+1= x\n?\n.\nIn the rest of the paper, we restrict our attention to the class of one-step lookahead\nstrategies, which is, as we shall see in Section 3, large enough to provide very efficient\nalgorithms. We leave aside the interesting question of whether more complex k-step\nlookahead strategies (with k \u2265 2) could provide a significant improvement over the\nstrategies examined in this paper.\nRemark 1 In practice, the analysis of a computer code usually begins with an ex-\nploratory phase, during which the output of the code is computed on a space-filling\ndesign of size n0< N (see, e.g., Santner et al., 2003). Such an exploratory phase will\n3Proving rigorously that, for a given P0 and ? \u03b1N, equations (4) and (5) actually define\nthis paper. This can be done for instance, in the case of a Gaussian process with continuous\ncovariance function (as considered later), by proving that x ?\u2192 En(Rn+1| Xn+1(\u03be) = x) is a\ncontinuous function on X and then using a measurable selection theorem.\na (measurable!) strategy X\u22c6\nN\u2208 AN is technical problem that is not of primary interest in"},{"page":7,"text":"7\nbe colloquially referred to as the initial design. Sequential strategies such as (5) and (6)\nare meant to be used after this initial design, at steps n0+ 1, ..., N. An important\n(and largely open) question is the choice of the size n0of the initial design, for a given\nglobal budget N.\n2.3 Gaussian process priors\nRestricting \u03be to be a Gaussian process makes it possible to deal with the conditional\ndistributions Pn and conditional expectations En that appear in the strategies above.\nThe idea of modeling an unknown function f by a Gaussian process has originally\nbeen introduced circa 1960 in time series analysis (Parzen, 1962), optimization theory\n(Kushner, 1964) and geostatistics (see, e.g., Chil` es and Delfiner, 1999, and the refer-\nences therein). Today, the Gaussian process model plays a central role in the design\nand analysis of computer experiments (see, e.g., Sacks et al., 1989; Currin et al., 1991;\nWelch et al., 1992; Santner et al., 2003). Recall that the distribution of a Gaussian pro-\ncess \u03be is uniquely determined by its mean function m(x) := E0(\u03be(x)), x \u2208 X, and its\ncovariance function k(x,y) := E0((\u03be(x) \u2212 m(x))(\u03be(y) \u2212 m(y))), x,y \u2208 X. Hereafter,\nwe shall use the notation \u03be \u223c GP(m, k) to say that \u03be is a Gaussian process with mean\nfunction m and covariance function k.\nLet \u03be \u223c GP(0, k) be a zero-mean Gaussian process. The best linear unbiased\npredictor (BLUP) of \u03be(x) from observations \u03be(xi), i = 1,...,n, also called the kriging\npredictor of \u03be(x), is the orthogonal projection\n?\u03be(x;xn) :=\nn\n?\ni=1\n\u03bbi(x;xn)\u03be(xi) (7)\nof \u03be(x) onto span{\u03be(xi),i = 1,...,n}. Here, the notation xnstands for the set of\npoints xn= {x1,...,xn}. The weights \u03bbi(x;xn) are the solutions of a system of linear\nequations\nk(xn,xn)\u03bb(x;xn) = k(x,xn) (8)\nwhere k(xn,xn) stands for the n \u00d7 n covariance matrix of the observation vector,\n\u03bb(x;xn) = (\u03bb1(x;xn),...,\u03bbn(x;xn))T, and k(x,xn) is a vector with entries k(x,xi).\nThe function x ?\u2192?\u03be(x;xn) conditioned on \u03be(x1) = f(x1),...,\u03be(xn) = f(xn), is de-\nSantner et al., 2003). The covariance function of the error of prediction, also called\nterministic, and provides a cheap surrogate model for the true function f (see, e.g.,\nkriging covariance is given by\nk(x,y;xn) := cov\n?\n\u03be(x) \u2212?\u03be(x;xn),\u03be(y) \u2212?\u03be(y;xn)\n= k(x,y) \u2212\ni\n?\n?\n\u03bbi(x;xn)k(y,xi). (9)"},{"page":8,"text":"8\nThe variance of the prediction error, also called the kriging variance, is defined as\n\u03c32(x;xn) = kn(x,x;xn). One fundamental property of a zero-mean Gaussian process\nis the following (see, e.g., Chil` es and Delfiner, 1999, Chapter 3) :\nProposition 1 If \u03be \u223c GP(0, k), then the random process \u03be conditioned on the \u03c3-\nalgebra Fngenerated by \u03be(x1),...,\u03be(xn), which we shall denote by \u03be | Fn, is a Gaussian\nprocess with mean?\u03be(\u00b7; xn) given by (7)-(8) and covariance k ( \u00b7, \u00b7; xn) given by (9).\nall x \u2208 X.\nIn the domain of computer experiments, the mean of a Gaussian process is generally\nIn particular,?\u03be(x;xn) = E0\n?\u03be(x) | Fn\n?\nis the best Fn-measurable predictor of \u03be(x), for\nwritten as a linear parametric function\nm(\u00b7) = \u03b2Tp(\u00b7), (10)\nwhere \u03b2 is a vector of unknown parameters, and p = (p1,...,pl)Tis an l-dimensional\nvector of functions (in practice, polynomials). The simplest case is when the mean\nfunction is assumed to be an unknown constant m, in which case we can take \u03b2 = m\nand p : x \u2208 X ?\u2192 1. The covariance function is generally written as a translation-\ninvariant function:\nk : (x,y) \u2208 X2?\u2192 \u03c32\u03c1\u03b8(x \u2212 y),\nwhere \u03c32is the variance of the (stationary) Gaussian process and \u03c1\u03b8is the correlation\nfunction, which generally depends on a parameter vector \u03b8. When the mean is written\nunder the form (10), the kriging predictor is again a linear combination of the obser-\nvations, as in (7), and the weights \u03bbi(x;xn) are again solutions of a system of linear\nequations (see, e.g., Chil` es and Delfiner, 1999), which can be written under a matrix\nform as\n?\nk(xn,xn) p(xn)T\np(xn)0\n??\n\u03bb(x;xn)\n\u00b5(x)\n?\n=\n?\nk(x,xn)\np(x)\n?\n,(11)\nwhere p(xn) is an l \u00d7 n matrix with entries pi(xj), i = 1,...,l, j = 1,...,n, \u00b5 is a\nvector of Lagrange coefficients (k(xn,xn), \u03bb(x;xn), k(x,xn) as above). The kriging\ncovariance function is given in this case by\n?\n= k(x,y) \u2212 \u03bb(x;xn)Tk(y,xn) \u2212 \u00b5(x)Tp(y).\nk(x,y;xn) := cov\u03be(x) \u2212?\u03be(x;xn),\u03be(y) \u2212?\u03be(y;xn)\n?\n(12)\nThe following result holds (Kimeldorf and Wahba, 1970; O\u2019Hagan, 1978):\nProposition 2 Let k be a covariance function.\n?\nm : x ?\u2192 \u03b2Tp(x), \u03b2 \u223c URl\nwhere URl stands for the (improper) uniform distribution over Rl, and where?\u03be(\u00b7;xn)\nIf\n\u03be | m \u223c GP(m, k)\nthen \u03be | Fn \u223c GP\n??\u03be(\u00b7;xn), k(\u00b7, \u00b7; xn)\n?\n,\nand k(\u00b7, \u00b7; xn) are given by (7), (11) and (12)."},{"page":9,"text":"9\nProposition 2 justifies the use of kriging in a Bayesian framework provided that the\ncovariance function of \u03be is known. However, the covariance function is rarely assumed\nto be known in applications. Instead, the covariance function is generally taken in some\nparametric class (in this paper, we use the so-called Mat\u00b4 ern covariance function, see\nAppendix A). A fully Bayesian approach also requires to choose a prior distribution\nfor the unknown parameters of the covariance (see, e.g., Handcock and Stein, 1993;\nKennedy and O\u2019Hagan, 2001; Paulo, 2005). Sampling techniques (Monte Carlo Markov\nChains, Sequential Monte Carlo...) are then generally used to approximate the posterior\ndistribution of the unknown covariance parameters. Very often, the popular empirical\nBayes approach is used instead, which consists in plugging-in the maximum likelihood\n(ML) estimate to approximate the posterior distribution of \u03be. This approach has been\nused in previous papers about contour estimation or probability of failure estimation\n(Picheny et al., 2010; Ranjan et al., 2008; Bichon et al., 2008). In Section 5.2 we will\nadopt a plug-in approach as well.\nSimplified notations. In the rest of the paper, we shall use the following simplified\nnotations when there is no risk of confusion:?\u03ben(x) :=?\u03ben(x;Xn), \u03c32\nn(x) := \u03c32\nn(x;Xn).\n2.4 Estimators of the probability of failure\nGiven a random process \u03be and a strategy XN, the optimal estimator that minimizes\nE0\namong all Fn-measurable estimators ? \u03b1n, 1 \u2264 n \u2264 N, is\n? \u03b1n = En(\u03b1) = En\nwhere\n?(\u03b1 \u2212 ? \u03b1n)2?\n??\nX\n1\u03be>udPX\n?\n=\n?\nX\npndPX, (13)\npn : x \u2208 X ?\u2192 Pn{\u03be(x) > u} . (14)\nWhen \u03be is a Gaussian process, the probability pn(x) of exceeding u at x \u2208 X given In\nhas a simple closed-form expression:\npn(x) = 1 \u2212 \u03a6\n??\u03ben(x) \u2212 u\n\u03c3n(x)\n?\n, (15)\nwhere \u03a6 is the cumulative distribution function of the normal distribution. Thus, in\nthe Gaussian case, the estimator (13) is amenable to a numerical approximation, by\nintegrating the excess probability pn over X (for instance using Monte Carlo sampling,\nsee Section 3.3).\nAnother natural way to obtain an estimator of \u03b1 given In is to look for a Fn-\nmeasurable hard approximation \u03b7n : X \u2192 {0,1} of the excess indicator 1\u03be>u. If \u03b7n is\nclose in some sense to 1\u03be>u, the estimator\n?\n? \u03b1n =\nX\n\u03b7ndPX\n(16)"},{"page":10,"text":"10\nshould be close to \u03b1. More precisely,\nEn\n?\n(? \u03b1n\u2212 \u03b1)2?\nLet \u03c4n(x) = En\nmisclassification; that is, the probability to predict a point above (resp. under) the\n= En\n???\n(\u03b7n\u2212 1\u03be>u)dPX\n?2?\n\u2264\n?\nEn\n?\n(\u03b7n\u2212 1\u03be>u)2?\ndPX. (17)\n?(\u03b7n(x) \u2212 1\u03be(x)>u)2?\n= Pn{\u03b7n(x) ?= 1\u03be(x)>u} be the probability of\nthreshold when the true value is under (resp. above) the threshold. Thus, (17) shows\nthat it is desirable to use a classifier \u03b7nsuch that \u03c4nis small for all x \u2208 X. For instance,\nthe method called smart (Deheeger and Lemaire, 2007) uses a support vector machine\nto build \u03b7n. Note that\n\u03c4n(x)= pn(x) + (1 \u2212 2pn(x))\u03b7n(x).\nTherefore, the right-hand side of (17) is minimized if we set\n\u03b7n(x) = 1pn(x)>1\/2= 1\u00af\u03ben(x)>u,(18)\nwhere\u00af\u03ben(x) denotes the posterior median of \u03be(x). Then, we have\n\u03c4n(x) = min(pn(x),1 \u2212 pn(x)).\nIn the case of a Gaussian process, the posterior median and the posterior mean are\nequal. Then, the classifier that minimizes \u03c4n(x) for each x \u2208 X is \u03b7n = 1?\u03ben>u, in\nwhich case\n\u03c4n(x) = Pn\n?\n(\u03be(x) \u2212 u)(?\u03ben(x) \u2212 u) < 0\n?\n= 1 \u2212 \u03a6\n????\u03ben(x) \u2212 u??\n\u03c3n(x)\n?\n.(19)\nNotice that for \u03b7n = 1?\u03ben>u, we have ? \u03b1n = \u03b1(?\u03ben). Therefore, this approach to obtain\nan estimator of \u03b1 can be seen as a type of plug-in estimation.\n3 Stepwise uncertainty reduction\n3.1 Principle\nA very natural and straightforward way of building a one-step lookahead strategy is to\nselect greedily each evaluation as if it were the last one. This kind of strategy, sometimes\ncalled myopic, has been successfully applied in the field of Bayesian global optimiza-\ntion (Mockus et al., 1978; Mockus, 1989), yielding the famous expected improvement\ncriterion later popularized in the EGO algorithm of Jones et al. (1998).\nWhen the Bayesian risk provides a measure of the estimation error or uncertainty\n(as in the present case), we call such a strategy a stepwise uncertainty reduction (SUR)\nstrategy. In the field of global optimization, the IAGO algorithm is an example of a SUR\nstrategy, where the Shannon entropy of the minimizer is used instead of the quadratic"},{"page":11,"text":"11\ncost (Villemonteix et al., 2009). When considered in terms of utility rather than cost,\nsuch strategies have also been called knowledge gradient policies by Frazier et al. (2008).\nGiven a sequence of estimators (? \u03b1n)n\u22651, a direct application of the above principle\nJn(x) = En\n(\u03b1 \u2212 ? \u03b1n+1)2| Xn+1= x\nHaving found no closed-form expression for this criterion, and no efficient numerical\nprocedure for its approximation, we will proceed by upper-bounding and discretiz-\nusing the quadratic loss function yields the sampling criterion\n??\n.(20)\ning (20) in order to get an expression that will lend itself to a numerically tractable\napproximation. By doing so, several SUR strategies will be derived, depending on the\nchoice of estimator (the posterior mean (13) or the plug-in estimator (16) with (18))\nand bounding technique.\n3.2 Upper bounds of the SUR sampling criterion\nRecall that \u03c4n(x) = min(pn(x),1\u2212pn(x)) is the probability of misclassification at x us-\ning the optimal classifier 1?\u03ben(x)>u. Let us further denote by \u03bdn(x) := pn(x) (1 \u2212 pn(x))\nthe variance of the excess indicator 1\u03be(x)\u2265u.\nProposition 3 Assume that either ? \u03b1n = En(\u03b1) or ? \u03b1n =?1?\u03ben\u2265udPX. Define Gn :=\n\uf8f1\n\uf8f3\nThen, for all x \u2208 X and all n \u2208 {0,...,N \u2212 1},\n?\nX\n?\u03b3n(y)dPXfor all n \u2208 {0,...,N \u2212 1}, with\n\uf8f2\n\u03b3n :=\n\u03bdn = pn(1 \u2212 pn) = \u03c4n(1 \u2212 \u03c4n),\n\u03c4n = min(pn,1 \u2212 pn),\nif ? \u03b1n = En(\u03b1),\nif ? \u03b1n =?1?\u03ben\u2265udPX.\n?\nJn(x) \u2264 ? Jn(x) := En\n?\nG2\nn+1| Xn+1= x.\nNote that \u03b3n(x) is a function of pn(x) that vanishes at 0 and 1, and reaches its maximum\nat 1\/2; that is, when the uncertainty on 1?\u03ben(x)>uis maximal (see Figure 1).\nProof First, observe that, for all n \u2265 0, \u03b1 \u2212 ? \u03b1n =?\nUn : x \u2208 X ?\u2192 Un(x) =\n\uf8f3\nMoreover, note that \u03b3n = ?Un?2\nL2(\u03a9,Fn,P), W ?\u2192 En\n(see, e.g., Vestrup, 2003, section 10.7) we get that\n?\nUndPX, with\n\uf8f1\n\uf8f2\n1\u03be(x)>u\u2212 pn(x)\n1\u03be(x)>u\u2212 1?\u03ben(x)>u\nnin both cases, where ?\u00b7 ?n : L2(\u03a9,B,P) \u2192\n?W2?1\/2. Then, using the generalized Minkowski inequality\n???\u222b UndPX\nif ? \u03b1n = En(\u03b1),\nif ? \u03b1n =?1?\u03ben\u2265udPX.\n(21)\n???n\n\u2264?Un?ndPX =\n?\n\u221a\u03b3ndPX = Gn.(22)"},{"page":12,"text":"12\nFinally, it follows from the tower property of conditional expectations and (22) that,\nfor all n \u2265 0,\nJn(x) = En\n?\n???\u222b Un+1dPX\n?\u03b1 \u2212 ? \u03b1n+1?2\n?\nn+1| Xn+1= x\n??2\n?\n= En\nn+1\n??? Xn+1= x\n?\n\u2264 En\nG2\nn+1| Xn+1= x\n?\n.\n\u2293 \u2294\n0 0.20.4 0.60.81\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n \n \npn\n\u03b3n\n\u03b3n = pn(1 \u2212 pn)\n\u03b3n = min(pn,1 \u2212 pn)\nFig. 1 \u03b3n as a function of pn (see Proposition 3). In both cases, \u03b3n is maximum at pn= 1\/2.\nNote that two other upper-bounding sampling criteria readily follow from those of\nProposition 3, by using the Cauchy-Schwarz inequality in L2(X,B(X),PX):\n??\n? Jn(x) \u2264 En\n\u03b3n+1dPX\n??? Xn+1= x\n?\n. (23)\nAs a result, we can write four SUR criteria, whose expressions are summarized in Ta-\nble 1. Criterion JSUR\n1,nhas been proposed in the PhD thesis of Piera-Martinez (2008) and\nin conference papers (Vazquez and Piera-Martinez, 2007; Vazquez and Bect, 2009); the\nother ones, to the best of our knowledge, are new. Each criterion is expressed as the\nconditional expectation of some (possibly squared) Fn+1-measurable integral criterion,\nwith an integrand that can be expressed as a function of the probability of misclassifi-\ncation \u03c4n+1. It is interesting to note that the integral in JSUR\nsquare error (IMSE)4for the process 1\u03be>u.\n4\nis the integrated mean\n4The IMSE criterion is usually applied to the response surface \u03be itself (see, e.g.,\nBox and Draper, 1987; Sacks et al., 1989). The originality here is to consider the IMSE of\nthe process 1\u03be>uinstead. Another way of adapting the IMSE criterion for the estimation of a\nprobability of failure, proposed by Picheny et al. (2010), is recalled in Section 4.1."},{"page":13,"text":"13\nTable 1 Expressions of four SUR-type criteria.\nSUR-type sampling criterion\n??? ?\u03c4n+1dPX\n??? ?\u03bdn+1dPX\n??\n??\nHow it is obtained\nJSUR\n1,n(x) = En\n?2??? Xn+1= x\n?2??? Xn+1= x\n??? Xn+1= x\n??? Xn+1= x\n?\n?\n?\n?\nProp. 3 with ? \u03b1n=?\nProp. 3 with ? \u03b1n= En(\u03b1)\nEq. (23) with ? \u03b1n=?\nEq. (23) with ? \u03b1n= En(\u03b1)\n1? \u03ben>udPX\nJSUR\n2,n(x) = En\nJSUR\n3,n(x) = En\n\u03c4n+1dPX\n1? \u03ben>udPX\nJSUR\n4,n(x) = En\n\u03bdn+1dPX\n3.3 Discretizations\nIn this section, we proceed with the necessary integral discretizations of the SUR crite-\nria to make them suitable for numerical evaluation and implementation on computers.\nAssume that n steps of the algorithm have already been performed and consider, for\ninstance, the criterion\nJSUR\n3,n(x) = En\n?\n\u222b \u03c4n+1(y)PX(dy)\n??? Xn+1= x\n?\n.(24)\nRemember that, for each y \u2208 X, the probability of misclassification \u03c4n+1(y) is Fn+1-\nmeasurable and, therefore, is a function of In+1= (In,Xn+1,Zn+1). Since Inis known\nat this point, we introduce the notation vn+1(y;Xn+1,Zn+1) = \u03c4n+1(y) to emphasize\nthe fact that, when a new evaluation point must be chosen at step (n + 1), \u03c4n+1(y)\ndepends on the choice of Xn+1and the random outcome Zn+1. Let us further denote\nby Qn,x the probability distribution of \u03be(x) under Pn. Then, (24) can be rewritten as\n??\nand the corresponding strategy is:\n??\nGiven In and a triple (x,y,z), vn+1(y;x,z) can be computed efficiently using the\nequations provided in Sections 2.3 and 2.4.\nJSUR\n3,n(x) =\nR\u00d7X\nvn+1(y;x,z) Qn,x(dz)PX(dy),\nXn+1 = argmin\nx\u2208X\nR\u00d7X\nvn+1(y;x,z) Qn,x(dz)PX(dy).(25)\nAt this point, we need to address: 1) the computation of the integral on X with\nrespect to PX; 2) the computation of the integral on R with respect to Qn,x; 3) the\nminimization of the resulting criterion with respect to x \u2208 X.\nTo solve the first problem, we draw an i.i.d. sequence Y1,...,Ym \u223c PXand use the\nMonte Carlo approximation:\n?\nX\nvn+1(y;x,z) PX(dy) \u2248\n1\nm\nm\n?\nj=1\nvn+1(Yj;x,z).\nAn increasing sample size n ?\u2192 mn should be used to build a convergent algo-\nrithm for the estimation of \u03b1 (possibly with a different sequence Yn,1,...,Yn,mn"},{"page":14,"text":"14\nat each step). In this paper we adopt a different approach instead, which is to\ntake a fixed sample size m > 0 and keep the same sample Y1,...,Ym through-\nout the iterations. Equivalently, it means that we choose to work from the start\non a discretized version of the problem: we replace PX by the empirical distribu-\ntion?PX,n =\nEn(\u03b1m) =\nm\napproach has be coined meta-estimation by Arnaud et al. (2010): the objective is to\n1\nm\n?1\u03be>ud?PX,n =\n?m\n?\nj=1\u03b4Yj, and our goal is now to estimate the Monte Carlo esti-\n1\nm\njpn(Yj) or the plug-in estimate\nmator \u03b1m =\n?m\nj=11\u03be(Yj)>u, using either the posterior mean\n1\nm\n1\n?\nj1?\u03be(Yj;Xn)>u. This kind of\nestimate the value of a precise Monte Carlo estimator of \u03b1(f) (m being large), using\nprior information on f to alleviate the computational burden of running m times the\ncomputer code f. This point of view also underlies the work in structural reliability\nof Hurtado (2004, 2007), Deheeger and Lemaire (2007), Deheeger (2008), and more\nrecently Echard et al. (2010).\nThe new point of view also suggests a natural solution for the third problem,\nwhich is to replace the continuous search for a minimizer x \u2208 X by a discrete search\nover the set Xm := {Y1,...,Ym}. This is obviously sub-optimal, even in the meta-\nestimation framework introduced above, since picking x \u2208 X \\ Xm can sometimes\nbring more information about \u03be(Y1),...,\u03be(Ym) than the best possible choice in Xm.\nGlobal optimization algorithm may of course be used to tackle directly the continuous\nsearch problem: for instance, Ranjan et al. (2008) use a combination of a genetic algo-\nrithm and local search technique, Bichon et al. (2008) use the DIRECT algorithm and\nPicheny et al. (2010) use a covariance-matrix-adaptation evolution strategy. In this pa-\nper we will stick to the discrete search approach, since it is much simpler to implement\n(we shall present in Section 3.4 a method to handle the case of large m) and provides\nsatisfactory results (see Section 5).\nFinally, remark that the second problem boils down to the computation of a one-\ndimensional integral with respect to Lebesgue\u2019s measure. Indeed, since \u03be is a Gaus-\nsian process, Qn,x is a Gaussian probability distribution with mean?\u03ben(x) and vari-\nGauss-Hermite quadrature with Q points (see, e.g., Press et al., 1992, Chapter 4) :\n?\nq=1\nwhere u1,...,uQ denote the quadrature points and w1,...,wQ the corresponding\nweights. Note that this is equivalent to replacing under Pn the random variable \u03be(x) by\na quantized random variable with probability distribution?Q\nTaking all three discretizations into account, the proposed strategy is:\nance \u03c32\nn(x) as explained in Section 2.3. The integral can be computed using a standard\nvn+1(y;x,z)Qn,x(dz) \u2248\n1\n\u221a\u03c0\nQ\n?\nwqvn+1(y;x,?\u03ben(x) + \u03c3n(x)uq\n\u221a2),\nq=1w\u2032q\u03b4zn+1,q(x), where\nw\u2032q= wq\/\u221a\u03c0 and zn+1,q(x) =?\u03ben(x) + \u03c3n(x)uq\n\u221a2.\nXn+1 = argmin\n1\u2264k\u2264m\nm\n?\nj=1\nQ\n?\nq=1\nw\u2032\nqvn+1\n?Yj; Yk,zn+1,q(Yk)?.(26)"},{"page":15,"text":"15\n3.4 Implementation\nThis section gives implementation guidelines for the SUR strategies described in Sec-\ntion 3. As said in Section 3.3, the strategy (26) can, in principle, be translated directly\ninto a computer program. In practice however, we feel that there is still room for\ndifferent implementations. In particular, it is important to keep the computational\ncomplexity of the strategies at a reasonable level. We shall explain in this section some\nsimplifications we have made to achieve this goal.\nA straight implementation of (26) for the choice of an additional evaluation point\nis described in Table 2. This procedure is meant to be called iteratively in a sequential\nalgorithm, such as that described for instance in Table 3. Note that the only parameter\nto be specified in the SUR strategy (26) is Q, which will be taken equal to 12 in the\nrest of the paper.\nTable 2 Procedure to select a new evaluation point Xn+1\u2208 X using a SUR strategy\nRequire computer representations of\na) a set In= {(X1,f(X1)),...,(Xn,f(Xn))} of evaluation results;\nb) a Gaussian process prior \u03be with a (possibly unknown linear parametric) mean function and\na covariance function k\u03b8, with parameter \u03b8;\nc) a (pseudo-)random sample Xm= {Y1,...,Ym} of size m drawn from the distribution PX;\nd) quadrature points u1,...,uQand corresponding weights w\u2032\ne) a threshold u.\n1,...,w\u2032\nQ;\n1. compute the kriging approximation? fn and kriging variance \u03c32\n2.1 for each point Yk, k \u2208 {1,...,m}, compute the kriging weights \u03bbi(Yk;{Xn,Yj}),\ni \u2208 {1,...,(n + 1)}, and the kriging variances \u03c32\n2.2 compute zn+1,q(Yj) =? fn(Yj) + \u03c3n(Yj)uq\n2.3.1 computethekriging approximation\n(Yj,f(Yj) = zn+1,q(Yj)), using the weights \u03bbi(Yk;{Xn,Yj}), i = 1,...,(n + 1),\nk = 1,...,m, obtained at Step 2.1.\n2.3.2 for each k \u2208 {1,...,m}, compute vn+1(Yk; Yj,zn+1,q(Yj)), using u,\u02dcfn+1,j,q\nobtained in 2.3.1, and \u03c32\nn+1(Yk;{Xn,Yj}) obtained in 2.1\n2.4 compute Jn(Yj) =?m\nnon Xm from In\n2. for each candidate point Yj, j \u2208 {1,...,m},\nn+1(Yk;{Xn,Yj})\n\u221a2, for q = 1,...,Q\n2.3 for each zn+1,q(Yj), q \u2208 {1,...,Q},\n\u02dcfn+1,j,q\non\nXm\nfromIn \u222a\nk=1\n?Q\nq=1w\u2032qvn+1(Yk; Yj,zn+1,q(Yj)).\n3. find j\u22c6= argminjJn(Yj) and set Xn+1= Yj\u22c6\nTo assess the complexity of a SUR sampling strategy, recall that kriging takes\nO(mn2) operations to predict the value of f at m locations from n evaluation results\nof f (we suppose that m > n and no approximation is carried out). In the procedure\nto select an evaluation, a first kriging prediction is performed at Step 1 and then, m"},{"page":16,"text":"16\nTable 3 Sequential estimation of a probability of failure\n1. Construct an initial design of size n0< N and evaluate f at the points of the initial design.\n2. Choose the parametric form of the mean of the Gaussian process \u03be by analyzing the results\nof the evaluations of f on the initial design.\n2. Choose the covariance function k\u03b8of \u03be\n3. Generate a Monte Carlo sample of size m from PX\n3. While the evaluation budget N is not exhausted,\n3.1 optional step: estimate the parameters of the covariance function (case of a plug-in\napproach);\n3.2 call a procedure to select a new evaluation point;\n3.3 perform the new evaluation.\n4. Estimate the probability of failure obtained from the N evaluations of f.\ndifferent predictions have to performed at step 2.1. This cost becomes rapidly burden-\nsome for large values of n and m, and we must further simplify (26) to be able to work\non applications where m must be large. A natural idea to alleviate the computational\ncost of the strategy is to avoid dealing with candidate points that have a very low prob-\nability of misclassification, since they are probably far from the frontier of the domain\nof failure. It is also likely that those points with a low probability of misclassification\nwill have a very small contribution in the variance of the error of estimation ? \u03b1n\u2212\u03b1m.\na way that the first summation (over m) and the search set for the minimizer is re-\nTherefore, the idea is to rewrite the sampling strategy described by (26), in such\nstricted to a subset of points Yjcorresponding to the m0largest values of \u03c4n(Yj). The\ncorresponding algorithm is not described here for the sake of brevity but can easily be\nadapted from that of Table 2. Section 5.3 will show that this pruning scheme has almost\nno consequence on the performances of the SUR strategies, even when one considers\nsmall values for m0(for instance m0= 200).\n4 Other strategies proposed in the literature\n4.1 The targeted IMSE criterion\nThe targeted IMSE proposed in Picheny et al. (2010) is a modification of the IMSE (In-\ntegrated Mean Square Error) sampling criterion (Sacks et al., 1989). While the IMSE\nsampling criterion computes the average of the kriging variance (over a compact do-\nmain X) in order to achieve a space-filling design, the targeted IMSE computes a\nweighted average of the kriging variance for a better exploration of the regions near\nthe frontier of the domain of failure. The idea is to put a large weight in regions where\nthe kriging prediction is close to the threshold u, and a small one otherwise. Given In,\nthe targeted IMSE sampling criterion, hereafter abbreviated as tIMSE, can be written"},{"page":17,"text":"17\nas\nJtIMSE\nn\n(x) = En\n??\n\u03c32\nn+1(y;X1,...,Xn,x) Wn(y)PX(dy),\nX\n?\u03be \u2212?\u03ben+1\n?2WndPX\n??? Xn+1= x\n?\n(27)\n=\n?\nX\n(28)\nwhere Wn is a weight function based on In. The weight function suggested by\nPicheny et al. (2010) is\nWn(x) =\n1\nsn(x)\u221a2\u03c0\nexp\n?\n\u22121\n2\n??\u03ben(x) \u2212 u\nsn(x)\n?2?\n,(29)\nwhere s2\nn(x) = \u03c32\n\u03b5+ \u03c32\nn(x). Note that Wn(x) is large when?\u03ben(x) \u2248 u and \u03c32\nThe tIMSE criterion operates a trade-off between global uncertainty reduction (high\nkriging variance \u03c32\nn) and exploration of target regions (high weight function Wn). The\nweight function depends on a parameter \u03c3\u03b5 > 0, which allows to tune the width of\nthe \u201cwindow of interest\u201d around the threshold. For large values of \u03c3\u03b5, JtIMSEbehaves\napproximately like the IMSE sampling criterion. The choice of an appropriate value\nn(x) \u2248 0,\ni.e., when the function is known to be close to u.\nfor \u03c3\u03b5, when the goal is to estimate a probability of failure, will be discussed on the\nbasis of numerical experiments in Section 5.3.\nThe tIMSE strategy requires a computation of the expectation with respect to \u03be(x)\nin (27), which can be done analytically, yielding (28). The computation of the integral\nwith respect to PXon X can be carried out with a Monte Carlo approach, as explained\nin Section 3.3. Finally, the optimization of the criterion is replaced by a discrete search\nin our implementation.\n4.2 Criteria based on the marginal distributions\nOther sampling criteria proposed by Ranjan et al. (2008), Bichon et al. (2008) and\nEchard et al. (2010) are briefly reviewed in this section. A common feature of these\nthree criteria is that, unlike the SUR and tIMSE criteria discussed so far, they only\ndepend on the marginal posterior distribution at the considered candidate point x \u2208 X,\nwhich is a Gaussian N??\u03ben(x),\u03c32\nA natural idea, in order to sequentially improve the estimation of the probability\nn(x)?distribution. As a consequence, they are of course\nmuch cheaper to compute than integral criteria like SUR and tIMSE.\nof failure, is to visit the point x \u2208 X where the event {\u03be(x) \u2265 u} is the most un-\ncertain. This idea, which has been explored by Echard, Gayton, and Lemaire (2010),\ncorresponds formally to the sampling criterion\nJEGL\nn\n(x) = \u03c4n(x) = 1 \u2212 \u03a6\n???u \u2212?\u03ben(x)??\n\u03c3n(x)\n?\n.(30)"},{"page":18,"text":"18\nAs in the case of the tIMSE criterion and also, less explicitely, in SUR criteria, a trade-\noff is realized between global uncertainty reduction (choosing points with a high \u03c32\nn(x))\nand exploration of the neighboorhood of the estimated contour (where\n??u \u2212?\u03ben(x)??is\nsmall).\nThe same leading principle motivates the criteria proposed by Ranjan et al. (2008)\nand Bichon et al. (2008), which can be seen as special cases of the following sampling\ncriterion:\nJRB\nn (x) := En\n?\nmax\n?\n0,\u01eb(x)\u03b4\u2212 |u \u2212 \u03be(x)|\u03b4??\n, (31)\nwhere \u01eb(x) = \u03b1\u03c3n(x), \u03b1,\u03b4 > 0. The following proposition provides some insights into\nthis sampling criterion:\nProposition 4 Define G\u03b1,\u03b4: ]0,1[ \u2192 R+ by\nG\u03b1,\u03b4(p) := E\n?\nmax\n?\n0,\u03b1\u03b4\u2212??\u03a6\u22121(p) + U????\n,\nwhere U is a Gaussian N(0,1) random variable. Let \u03d5 and \u03a6 denote respectively the\nprobability density function and the cumulative distribution function of U.\na)G\u03b1,\u03b4(p) = G\u03b1,\u03b4(1 \u2212 p) for all p \u2208]0,1[.\nG\u03b1,\u03b4is strictly increasing on ]0,1\/2] and vanishes at 0. Therefore, G\u03b1,\u03b4is also\nstrictly decreasing on [1\/2,1[, vanishes at 1, and has a unique maximum at p = 1\/2.\nb)\nc) Criterion (31) can be rewritten as\nJRB\nn (x) = \u03c3n(x)\u03b4G\u03b1,\u03b4\n?pn(x)?.(32)\nd)G\u03b1,1has the following closed-form expression:\nG\u03b1,1(p) = \u03b1?\u03a6(t+) \u2212 \u03a6(t\u2212)?\n\u2212 t?2\u03a6(t) \u2212 \u03a6(t+) \u2212 \u03a6(t\u2212)?\n\u2212?2\u03d5(t) \u2212 \u03d5(t+) \u2212 \u03d5(t\u2212)?,\n(33)\nwhere t = \u03a6\u22121(1 \u2212 p), t+= t + \u03b1 and t\u2212= t \u2212 \u03b1.\nG\u03b1,2has the following closed-form expression: e)\nG\u03b1,2(p) =\n?\u03b12\u2212 1 \u2212 t2??\u03a6(t+) \u2212 \u03a6(t\u2212)?\n+ t+\u03d5(t+) \u2212 t\u2212\u03d5(t\u2212),\n\u2212 2t?\u03d5(t+) \u2212 \u03d5(t\u2212)?\n(34)\nwith the same notations.\nIt follows from a) and c) that JRB\nance \u03c32\nn(x) and the probability of misclassification \u03c4n(x) = min(pn(x),1 \u2212 pn(x)).\nNote that, in the computation of G\u03b1,\u03b4\nand (34) is equal to?u \u2212?\u03ben(x)?\/\u03c3n(x), i.e., equal to the normalized distance between\nn (x) can also be seen as a function of the kriging vari-\n?pn(x)?, the quantity denoted by t in (33)\nthe predicted value and the threshold."},{"page":19,"text":"19\nBichon et al.\u2019s expected feasibility function corresponds to (32) with \u03b4 = 1, and can\nbe computed efficiently using (33). Similarly, Ranjan et al.\u2019s expected improvement5\nfunction corresponds to (32) with \u03b4 = 2, and can be computed efficiently using (34).\nProof (Proposition 4)\na) Using the identity \u03a6\u22121(1 \u2212 p) = \u2212\u03a6\u22121(p), we get\n??U + \u03a6\u22121(1 \u2212 p)??=\nwhere\nb) Let Sp = max?0,\u03b1\u03b4\u2212??\u03a6\u22121(p) + U???. Straightforward computations show that\nsequence, p ?\u2192 P?Sp < s?\nother assertions then follow from a).\nc) Recall that \u03be(x) \u223c N??\u03ben(x),\u03c32\nA proof of d) and e) is given in Appendix B.\n???U \u2212 \u03a6\u22121(p)\n???\nd=\n???U + \u03a6\u22121(p)\n???,\nd= denotes an equality in distribution. Therefore G\u03b1,\u03b4(1 \u2212 p) = G\u03b1,\u03b4(p).\nt ?\u2192 P(|t + U| \u2264 v) is strictly decreasing to 0 on [0,+\u221e[, for all v > 0. As a con-\nis strictly increasing to 1 on [1\/2,1[, for all s \u2208\nTherefore, G\u03b1,\u03b4is strictly decreasing on [1\/2,1[ and tends to zeros when p \u2192 1. The\n?0,\u03b1\u03b4?.\nn(x)?\nunder Pn. Therefore U :=\n?\u03be(x) \u2212\n?\u03ben(x)?\/\u03c3n(x) \u223c N(0,1) under Pn, and the result follows by substitution in (31).\nRemark 2 In the case \u03b4 = 1, our result coincides with the expression given by\n\u2293 \u2294\nBichon et al. (2008, Eq. (17)). In the case \u03b4 = 2, we have found and corrected a\nmistake in the computations of Ranjan et al. (2008, Eq. (8) and Appendix B).\n5 Numerical experiments\n5.1 A one-dimensional illustration of a SUR strategy\nThe objective of this section is to show the progress of a SUR strategy in a simple\none-dimensional case. We wish to estimate \u03b1 = PX{f > 1}, where f is a given function\ndefined over X = R, endowed with the probability distribution PX= N(0,\u03c32), \u03c3 =\n0.4, as depicted in Figure 2. We know in advance that \u03b1 \u2248 0.2. Thus, a Monte Carlo\nsample of size m = 1500 will give a good estimate of \u03b1.\nIn this illustration, \u03be is a Gaussian process with constant but unknown mean and a\nMat\u00b4 ern covariance function, whose parameters are kept fixed, for the sake of simplicity.\nFigure 2 shows an initial design of four points and the sampling criterion JSUR\n1,n=4. Notice\nthat the sampling criterion is only computed at the points of the Monte Carlo sample.\nFigures 3 and 4 show the progress of the SUR strategy after a few iterations. Observe\nthat the unknown function f is sampled so that the probability of excursion pn almost\nequals zero or one in the region where the density of PXis high.\n5Despite its name and some similarity between the formulas, this criterion should not be\nconfused with the well-known EI criterion in the field of optimization (Mockus et al., 1978;\nJones et al., 1998)."},{"page":20,"text":"20\n\u22122\u22121.5\u22121\u22120.500.511.52\n0\n0.5\n1\n\u22122 \u22121.5\u22121\u22120.500.511.52\n0\n0.5\n1\n\u22122\u22121.5\u22121\u22120.5 00.511.52\n0.02\n0.04\n0.06\nFig. 2 Illustration of a SUR strategy. This figure shows the initial design. Top: threshold\nu = 1 (horizontal dashed line); function f (thin line); n = 4 initial evaluations (squares);\nkriging approximation fn (thick line); 95% confidence intervals computed from the kriging\nvariance (shaded area). Middle: probability of excursion (solid line); probability density of\nPX(dotted line). Bottom: graph of JSUR\n1,n=4(Yi), i = 1,...,m = 1500, the minimum of which\nindicates where the next evaluation of f should be done (i.e., near the origin).\n\u22122\u22121.5\u22121\u22120.500.511.52\n0\n0.5\n1\n\u22122\u22121.5\u22121 \u22120.500.51 1.52\n0\n0.5\n1\n\u22122\u22121.5\u22121 \u22120.500.511.52\n4\n6\n8\n10x 10\n\u22123\nFig. 3 Illustration of a SUR strategy (see also Figures 2 and 4). This figure shows the progress\nof the SUR strategy after two iterations\u2014a total of n = 6 evaluations (squares) have been\nperformed. The next evaluation point will be approximately at x = \u22120.5\n."},{"page":21,"text":"21\n\u22122\u22121.5\u22121\u22120.500.511.52\n0\n0.5\n1\n\u22122\u22121.5\u22121 \u22120.500.511.52\n0\n0.5\n1\n\u22122\u22121.5\u22121\u22120.500.511.52\n0\n1\n2\n3x 10\n\u22125\nFig. 4 Illustration of a SUR strategy (see also Figures 2 and 3). This figure shows the progress\nof the SUR strategy after eight iterations\u2014a total of n = 12 evaluations (squares) have been\nperformed. At this stage, the probability of excursion pn almost equals 0 or 1 in the region\nwhere the density of PXis high.\n5.2 An example in structural reliability\nIn this section, we evaluate all criteria discussed in Section 3 and Section 4 through a\nclassical benchmark example in structural reliability (see, e.g., Borri and Speranzini,\n1997; Waarts, 2000; Schueremans, 2001; Deheeger, 2008). Echard et al. (2010)\nused this benchmark to make a comparison among several methods proposed in\nSchueremans and Gemert (2005), some of which are based on the construction of a\nresponse surface. The objective of the benchmark is to estimate the probability of\nfailure of a so-called four-branch series system. A failure happens when the system is\nworking under the threshold u = 0. The performance function f for this system is\ndefined as\nf : (x1,x2) \u2208 R2?\u2192 f(x1,x2) = min\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\uf8f2\n3 + 0.1(x1\u2212 x2)2\u2212 (x1+ x2)\/\u221a2;\n3 + 0.1(x1\u2212 x2)2+ (x1+ x2)\/\u221a2;\n(x1\u2212 x2) + 6\/\u221a2;\n(x2\u2212 x1) + 6\/\u221a2\n\uf8fc\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8fe\n\uf8fd\n.\nThe uncertain input factors are supposed to be independent and have standard normal\ndistribution. Figure 5 shows the performance function, the failure domain and the input\ndistribution. Observe that f has a first-derivative discontinuity along four straight lines\noriginating from the point (0,0)."},{"page":22,"text":"22\n \n \n\u22126\u22124\u221220246\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\nf = 0\nFig. 5 Left: mesh plot of the performance function f corresponding to the four-branch series\nsystem; a failure happens when f is below the transparent plane; Right: contour plot of f;\nlimit state f = 0 (thick line); sample of size m = 3 \u00d7 103from PX(dots).\nFor each sequential method, we will follow the procedure described in Table 3. We\ngenerate an initial design of n0 = 10 points (five times the dimension of the factor\nspace) using a maximin LHS (Latin Hypercube Sampling). The same initial design is\nused for all methods. We generate a Monte Carlo sample of size m = 30000. Since\nf can be evaluated very quickly, it is possible to compute the value of f for all the\npoints of this sample. Then, an empirical estimation of the probability of failure gives\n\u03b1m = 4.16 \u00d7 10\u22123. A Gaussian process with constant unknown mean and a Mat\u00b4 ern\ncovariance function is used as our prior information about f. The parameters of the\nMat\u00b4 ern covariance functions are estimated on the initial design by REML (see, e.g.\nStein, 1999). We obtain \u03c32\u2248 30, \u03bd \u2248 3 and \u03c11\u2248 \u03c12\u2248 12, which corresponds to a\nfairly smooth prior. In this experiment, we choose to re-estimate the parameters of the\ncovariance after each new evaluation.\nThe probability of failure is estimated by (13). To evaluate the rate of convergence,\nwe compute the number n\u03b3 of iterations that must be performed using a given strat-\negy to observe a stabilization of the relative error of estimation within an interval of\nlength 2\u03b3:\nn\u03b3 = min\n?\nn \u2265 0;\u2200k \u2265 n,|? \u03b1n0+k\u2212 \u03b1m|\n\u03b1m\n< \u03b3\n?\n.\nWe could observe the convergence of the estimators to \u03b1m for all sampling criteria,\nexcept JtIMSEwith \u03c32\n\u03b5= 1. In fact, JtIMSEwith \u03c32\n\u03b5= 1 is a space-filling criterion and\nit is interesting to observe that the convergence is slow with this criterion. The results\nare presented in Table 4. Figure 6 shows the points that have been evaluated in three\ncases. It seems that the best results are obtained when the points are chosen close to\nthe frontier of the domain of failure."},{"page":23,"text":"23\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\u22126 \u22124\u221220246\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\u22126\u22124\u221220246\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\u22126\u22124\u221220246\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\nFig. 6 The first 16 points (squares) evaluated using sampling criterion JSUR\nwith \u03c32\n\u03b5= 1 (right). Numbers near squares indicate the order\nof evaluation. The location of the n0= 10 points of the initial design are indicated by circles.\n1\n(left), JtIMSE\n\u03b5= 0.1 (middle), JtIMSEwith \u03c32\n5.3 Average performance on sample paths of a Gaussian process\nThis section provides a comparison of all the criteria introduced or recalled in this\npaper, on the basis of their average performance on the sample paths of a zero-mean\nGaussian process defined on X = [0,1]d, for d \u2208 {1,2,3}. In all experiments, the\nsame covariance function is used for the generation of the sample paths and for the\ncomputation of the sampling criteria. We have considered isotropic Mat\u00b4 ern covariance\nfunctions, whose parameters are given in Table 5. An initial maximin LHS design of\nsize n0(also given in the table) is used: note that the value of n reported on the x-axis\nof Figures 7\u201311 is the total number of evaluations, including the initial design.\nThe d input variables are assumed to be independent and uniformly distributed\non [0,1], i.e., PXis the uniform distribution on X. An m-sample Y1, ..., Ym from PX\nTable 4 Comparison of the convergence to \u03b1m for different sampling strategies.\nexperiment\ncriterion\nparameters\nn0.1\nn0.01\n1\nJSUR\n1\n\u2013\n16\n30\n2\nJSUR\n2\n\u2013\n15\n28\n3\nJSUR\n3\n\u2013\n11\n24\n4\nJSUR\n4\n\u2013\n11\n27\nexperiment\ncriterion\nparameters\nn0.1\nn0.01\n5\nJEGL\n\u2013\n15\n23\n6\nJRB\n7\nJRB\n8\nJRB\n9\nJRB\n\u03b4 = 1, \u03b1 = 0.5\n21\n36\n\u03b4 = 1, \u03b1 = 2\n21\n36\n\u03b4 = 2, \u03b1 = 0.5\n21\n34\n\u03b4 = 2, \u03b1 = 2\n19\n32\nexperiment\ncriterion\nparameters\nn0.1\nn0.01\n10\nJtIMSE\n\u03c32\n17\n33\n11\nJtIMSE\n\u03c32\n\u03b5= 0.1\n19\n54\n12\nJtIMSE\n\u03c32\n\u03b5= 1\n8\n> 120\n\u03b5= 10\u22126"},{"page":24,"text":"24\nTable 5 Size of the initial design and covariance parameters for the experiments of Section 5.3.\nThe parametrization of the Mat\u00b4 ern covariance function used here is defined in Appendix A.\nd\n1\n2\n3\nn0\n3\n10\n15\n\u03c32\n1.0\n1.0\n1.0\n\u03bd\u03c1\n2.0\n2.0\n2.0\n0.100\n0.252\n0.363\nis drawn one and for all, and used both for the approximation of integrals (in SUR and\ntIMSE criteria) and for the discrete search of the next sampling point (for all criteria).\nWe take m = 500 and use the same MC sample for all criteria in a given dimension d.\nWe adopt the meta-estimation framework as described in Section 3.3; in other\nwords, our goal is to estimate the MC estimator \u03b1m. We choose to adjust the thresh-\nold u in order to have \u03b1m = 0.02 for all sample paths (note that, as a consequence,\nthere are exactly m\u03b1m = 10 points in the failure region) and we measure the per-\nformance of a strategy after n evaluations by its relative mean-square error (MSE)\nexpressed in decibels (dB):\nrMSE := 10 log10\n\uf8eb\n\uf8ed\n\uf8ec\n1\nL\nL\n?\nl=1\n?\n? \u03b1(l)\nm,n\u2212 \u03b1m\n\u03b12m\n?2\n\uf8f6\n\uf8f8 ,\n\uf8f7\nwhere ? \u03b1(l)\nWe use a sequential maximin strategy as a reference in all of our experiments. This\nm,n=\n1\nm\n?m\nj=1p(l)\nn(Yj) is the posterior mean of the MC estimator \u03b1m after n\nevaluations on the lthsimulated sample path (L = 4000).\nsimple space-filling strategy is defined by Xn+1= argmaxjmin1\u2264i\u2264n\nthe argmax runs over all indices j such that Yj?\u2208 {X1,...,Xn}. Note that this strategy\ndoes not depend on the choice of a Gaussian process model.\n??Yj\u2212 Xi\n??, where\nOur first experiment (Figure 7 ) provides a comparison of the four SUR strategies\nproposed in Section 3.2. It appears that all of them perform roughly the same when\ncompared to the reference strategy. A closer look, however, reveals that the strate-\ngies JSUR\n1\nand JSUR\n2\nprovided by Proposition 3 perform slightly better than the other\ntwo (noticeably so in the case d = 3).\nThe performance of the tIMSE strategy is shown on Figure 8 for several value of\nits tuning parameter \u03c32\n\u03b5(other values, not shown here, have been tried as well). It is\nclear that the performance of this strategy improves when \u03c32\n\u03b5goes to zero, whatever\nthe dimension.\nThe performance of the strategy based on JRB\n\u03b1,\u03b4is shown on Figure 9 for several\nvalues of its parameters. It appears that the criterion proposed by Bichon et al. (2008),\nwhich corresponds to \u03b4 = 1, performs better than the one proposed by Ranjan et al.\n(2008), which corresponds to \u03b4 = 2, for the same value of \u03b1. Moreover, the value \u03b1 = 0.5\nhas been found in our experiments to produce the best results."},{"page":25,"text":"25\nFigure 10 illustrates that the loss of performance associated to the \u201cpruning trick\u201d\nintroduced in Section 3.4 can be negligible if the size m0of the pruned MC sample is\nlarge enough (here, m0has been taken equal to 50). In practice, the value of m0should\nbe chosen small enough to keep the overhead of the sequential strategy reasonable\u2014in\nother words, large values of m0should only be used for very complex computer codes.\nFinally, a comparison involving the best strategy obtained in each category is pre-\nsented on Figure 11. The best result is consistently obtained with the SUR strategy\nbased on JSUR\n1,n. The tIMSE strategy with \u03c32\nas good. Note that both strategies are one-step lookahead strategies based on the ap-\n\u03b5\u2248 0 provides results which are almost\nproximation of the risk by an integral criterion, which makes them rather expensive\nto compute. Simpler strategies based on the marginal distribution (criteria JRB\nJEGL\nn\n) provide interesting alternatives for moderately expensive computer codes: their\nperformances, although not as good as those of one-step lookahead criterions, are still\nn\nand\nmuch better than that of the reference space-filling strategy.\n6 Concluding remarks\nOne of the main objectives of this paper was to present a synthetic viewpoint on se-\nquential strategies based on a Gaussian process model and kriging for the estimation of\na probability of failure. The starting point of this presentation is a Bayesian decision-\ntheoretic framework from which the theoretical form of an optimal strategy for the\nestimation of a probability of failure can be derived. Unfortunately, the dynamic pro-\ngramming problem corresponding to this strategy is not numerically tractable. It is\nnonetheless possible to derive from there the ingredients of a sub-optimal strategy: the\nidea is to focus on one-step lookahead suboptimal strategies, where the exact risk is\nreplaced by a substitute risk that accounts for the information gain about \u03b1 expected\nfrom a new evaluation. We call such a strategy a stepwise uncertainty reduction (SUR)\nstrategy. Our numerical experiments show that SUR strategies perform better, on av-\nerage, than the other strategies proposed in the literature. However, this comes at a\nhigher computational cost than strategies based only on marginal distributions. The\ntIMSE sampling criterion, which seems to have a convergence rate comparable to that\nof the SUR criterions when \u03c32\n\u03b5\u2248 0, also has a high computational complexity.\nCan we say that the sequential strategies presented in this paper are interesting\nalternatives to classical importance sampling methods for estimating a probability of\nfailure, for instance the subset sampling method of Au and Beck (2001)? In our opinion,\nthe answer to this questions depends on our capacity to choose an appropriate prior.\nIn the example of Section 5.2, as well as in many other examples of the literature\nusing Gaussian processes in the domain of computer experiments, the prior is easy to\nchoose because X is a low-dimensional space and f tends to be smooth. Then, the\nplug-in approach which consists in using ML or REML to estimate the parameters of"},{"page":26,"text":"26\nrMSE (dB)\nn\nrMSE (dB)\nn\nrMSE (dB)\nn\nJSUR\n1\nJSUR\n2\nJSUR\n3\nJSUR\n4\nref.\nblack solid line\ngray solid line\ngray mixed line\nblack mixed line\nblack dashed line\nUpper-left:\nUpper-right:\nLower-left:\nd = 1\nd = 2\nd = 3\n203040\n50\n60\n20 304050\n60\n51015202530\n-15\n-10\n-5\n0\n-25\n-20\n-15\n-10\n-5\n0\n-30\n-20\n-10\n0\n10\nFig. 7 Relative MSE performance of several SUR strategies.\nrMSE (dB)\nn\nrMSE (dB)\nn\nrMSE (dB)\nn\n\u03c32\n\u03c32\n\u03c32\nref.\n\u03b5= 10\u22126\n\u03b5= 0.1\n\u03b5= 1\nblack solid line\ngray solid line\nblack mixed line\nblack dashed line\nUpper-left:\nUpper-right:\nLower-left:\nd = 1\nd = 2\nd = 3\n203040\n5060\n203040 50 60\n5\n10 1520 2530\n-15\n-10\n-5\n0\n-25\n-20\n-15\n-10\n-5\n0\n-30\n-20\n-10\n0\n10\nFig. 8 Relative MSE performance of the tIMSE strategy for several values of its parameter."},{"page":27,"text":"27\nrMSE (dB)\nn\nrMSE (dB)\nn\nrMSE (dB)\nn\n\u03b1 = 0.5\n\u03b1 = 0.5\n\u03b1 = 2\n\u03b1 = 2\nref.\n\u03b4 = 1\n\u03b4 = 2\n\u03b4 = 1\n\u03b4 = 2\nblack solid line\ngray solid line\nblack mixed line\ngray mixed line\nblack dashed line\nUpper-left:\nUpper-right:\nLower-left:\nd = 1\nd = 2\nd = 3\n2030\n40\n5060\n203040\n50\n605 10 152025\n30\n-15\n-10\n-5\n0\n-20\n-15\n-10\n-5\n0\n-30\n-20\n-10\n0\n10\nFig. 9 Relative MSE performance of the JRBcriterion, for several values of its parameters.\nrMSE (dB)\nn\nrMSE (dB)\nn\nrMSE (dB)\nn\nJSUR\n1\nJSUR\n3\nwithout pruning\npruning m0= 50\nref.\nblack\ngray\nsolid line\nmixed line\nblack dashed line\nUpper-left:\nUpper-right:\nLower-left:\nd = 1\nd = 2\nd = 3\n20\n30405060\n20\n30 40\n50605 1015\n20\n2530\n-15\n-10\n-5\n0\n-25\n-20\n-15\n-10\n-5\n0\n-40\n-30\n-20\n-10\n0\n10\nFig. 10 Relative MSE performance of two SUR criteria, with and without the \u201cpruning trick\u201d\ndescribed in Section 3.4."},{"page":28,"text":"28\nrMSE (dB)\nn\nrMSE (dB)\nn\nrMSE (dB)\nn\nJSUR\n1\ntIMSE (\u03c32\nJRB(\u03b1 = 0.5, \u03b4 = 1)\nJEGL\nref.\nblack solid line\ngray solid line\nblack mixed line\ngray mixed line\nblack dashed line\n\u03b5= 10\u22126)\nUpper-left:\nUpper-right:\nLower-left:\nd = 1\nd = 2\nd = 3\n203040\n50\n60\n20304050\n60\n5 1015202530\n-15\n-10\n-5\n0\n-25\n-20\n-15\n-10\n-5\n0\n-30\n-20\n-10\n0\n10\nFig. 11 Relative MSE performance the best strategy in each category.\nthe covariance function of the Gaussian process after each new evaluation is likely to\nsucceed. If X is high-dimensional and f is expensive to evaluate, difficulties arise. In\nparticular, our sampling strategies do not take into account our uncertain knowledge\nof the covariance parameters, and there is no guarantee that ML estimation will do\nwell when the points are chosen by a sampling strategy that favors some localized\ntarget region (the neighboorhood the frontier of the domain of failure in this paper,\nbut the question is equally relevant in the field optimization, for instance). The difficult\nproblem of deciding the size n0of the initial design is crucial in this connection. Fully\nBayes procedures constitute a possible direction for future research, as long as they\ndon\u2019t introduce an unacceptable computational overhead. Whatever the route, we feel\nthat the robustness of Gaussian process-based sampling strategies with respect to the\nprocedure of estimation of the covariance parameters should be addressed carefully in\norder to make these methods usable in the industrial world.\nAcknowledgements This work was partially supported by the French Agence Nationale de\nla Recherche in the context of the project opus (ref. anr-07-cis7-010) and by the French p\u02c6 ole\nde comp\u00b4 etitivit\u00b4 e systematic in the context of the project csdl."},{"page":29,"text":"29\nAppendix\nA The Mat\u00b4 ern covariance\nThe exponential covariance and the Mat\u00b4 ern covariance are among the most conventionally\nused stationary covariances in the literature of design and analysis of computer experiments.\nThe Mat\u00b4 ern covariance class (Yaglom, 1986) offers the possibility to adjust the regularity of \u03be\nwith a single parameter. Stein (1999) advocates the use of the following parametrization of the\nMat\u00b4 ern function:\n?\nwhere \u0393 is the Gamma function and K\u03bd is the modified Bessel function of the second kind.\nThe parameter \u03bd > 0 controls regularity at the origin of the function. To model a real-valued\nfunction f defined over X \u2282 Rd, with d \u2265 1, we use the following anisotropic form of the\nMat\u00b4 ern covariance:\n\uf8eb\ni=1\n\u03ba\u03bd(h) =\n1\n2\u03bd\u22121\u0393(\u03bd)\n2\u03bd1\/2h\n?\u03bdK\u03bd\n?\n2\u03bd1\/2h\n?\n,h \u2208 R\n(35)\nk\u03b8(x,y) = \u03c32\u03ba\u03bd\n\uf8ed\n?\n?\n?\n?\nd\n?\n(x[i]\u2212 y[i])2\n\u03c12\ni\n\uf8f6\n\uf8f8,x,y \u2208 Rd\n(36)\nwhere x[i],y[i]denote the ithcoordinate of x and y, the positive scalar \u03c32is a variance pa-\nrameter (we have k\u03b8(x,x) = \u03c32), and the positive scalars \u03c1irepresent scale or range param-\neters of the covariance, i.e., characteristic correlation lengths. Since \u03c32> 0,\u03bd > 0,\u03c1i > 0,\ni = 1,...,d, we can take the logarithm of these scalars, and consider the vector of parameters\n\u03b8 = {log \u03c32,log\u03bd,\u2212log\u03c11,...,\u2212log\u03c1d} \u2208 Rd+2, which is a practical parameterization when\n\u03c32,\u03bd,\u03c1i, i = 1,...,d, need to be estimated from data.\nB Calculation of Bichon et al.\u2019s and Ranjan et al.\u2019s criteria\nB.1 A preliminary decomposition common to both criteria\nRecall that t = \u03a6\u22121(1 \u2212 p), t+= t + \u03b1 and t\u2212= t \u2212 \u03b1. Then,\nG\u03b1,\u03b4(p) = G\u03b1,\u03b4(1 \u2212 p) = E\n?\n?t+\n= \u03b1\u03b4?\u03a6(t+) \u2212 \u03a6(t\u2212)?\n?\nmax\n?\n0,\u03b1\u03b4\u2212??t \u2212 U??\u03b4??\n\u03d5(u)du\n=\n\u03b1\u03b4\u2212|t\u2212u|\u03b4\u22650\n?\n?\n\u03b1\u03b4\u2212 |t \u2212 u|\u03b4?\n=\nt\u2212\n\u03b1\u03b4\u2212 |t \u2212 u|\u03b4?\n\u03d5(u)du\n\u2212\n?t+\n?\nt\u2212\n|t \u2212 u|\u03b4\u03d5(u)du\n???\nTerm A\n.(37)\nThe computation of the integral A will be carried separately in the next two sections for \u03b4 = 1\nand \u03b4 = 2. For this purpose, we shall need the following elementary results:\n?b\n?b\na\nu\u03d5(u)du = \u03d5(a) \u2212 \u03d5(b),(38)\na\nu2\u03d5(u)du = a\u03d5(a) \u2212 b\u03d5(b) + \u03a6(b) \u2212 \u03a6(a).(39)"},{"page":30,"text":"30\nB.2 Case \u03b4 = 1\nLet us compute the value A1 of the integral A for \u03b4 = 1:\nA1 =\n?t+\n??t\n= t?2\u03a6(t) \u2212 \u03a6(t\u2212) \u2212 \u03a6(t+)?\nt\u2212\n|t \u2212 u|\u03d5(u)du =\n?t\nt\u2212(t \u2212 u)\u03d5(u)du +\n?\n+ 2\u03d5(t) \u2212 \u03d5(t\u2212) \u2212 \u03d5(t+),\n?t+\nt\n(u \u2212 t)\u03d5(u)du\n?t+\n= t\nt\u2212\u03d5(u)du \u2212\n?t+\nt\n\u03d5(u)du\u2212\n?t\nt\u2212u\u03d5(u)du +\nt\nu\u03d5(u)du\n(40)\nwhere (38) has been used to get the final result. Pluging (40) into (37) yields (33).\nB.3 Case \u03b4 = 2\nLet us compute the value A2 of the integral A for \u03b4 = 2:\nA2 =\n?t+\n?t+\n= t2?\u03a6(t+) \u2212 \u03a6(t\u2212)?\n+ t\u2212\u03d5(t\u2212) \u2212 t+\u03d5(t+) + \u03a6(t+) \u2212 \u03a6(t\u2212),\nt\u2212\n(t \u2212 u)2\u03d5(u)du\n= t2\nt\u2212\n\u03d5(u)du \u2212 2t\n?t+\n\u2212 2t?\u03d5(t\u2212) \u2212 \u03d5(t+)?\nt\u2212\nu\u03d5(u)du +\n?t+\nt\u2212\nu2\u03d5(u)du\n(41)\nwhere (38) and (39) have been used to get the final result. Pluging (40) into (37) yields (34).\nReferences\nArnaud, A., Bect, J., Couplet, M., Pasanisi, A., Vazquez, E.: \u00b4Evaluation d\u2019un risque\nd\u2019inondation fluviale par planification s\u00b4 equentielle d\u2019exp\u00b4 eriences. In: 42` emes Journ\u00b4 ees de\nStatistique (2010)\nAu, S.K., Beck, J.: Estimation of small failure probabilities in high dimensions by subset\nsimulation. Probab. Engrg. Mechan. 16(4), 263\u2013277 (2001)\nBayarri, M.J., Berger, J.O., Paulo, R., Sacks, J., Cafeo, J.A., Cavendish, J., Lin, C.H., Tu, J.:\nA framework for validation of computer models. Technometrics 49(2), 138\u2013154 (2007)\nBerry, D.A., Fristedt, B.: Bandit problems: sequential allocation of experiments. Chapman &\nHall (1985)\nBertsekas, D.P.: Dynamic programming and optimal control vol. 1. Athena Scientific (1995)\nBichon, B.J., Eldred, M.S., Swiler, L.P., Mahadevan, S., McFarland, J.M.: Efficient global\nreliability analysis for nonlinear implicit performance functions.\n2459\u20132468 (2008)\nBjerager, P.: On computational methods for structural reliability analysis. Structural Safety\n9, 76\u201396 (1990)\nBorri, A., Speranzini, E.: Structural reliability analysis using a standard deterministic finite\nelement code. Structural Safety 19(4), 361\u2013382 (1997)\nBox, G.E.P., Draper, N.R.: Empirical Model-Building and Response Surfaces. Wiley (1987)\nBucher, C.G., Bourgund, U.: A fast and efficient response surface approach for structural\nreliability problems. Structural Safety 7(1), 57\u201366 (1990)\nChil` es, J.P., Delfiner, P.: Geostatistics: Modeling Spatial Uncertainty. Wiley, New York (1999)\nAIAA Journal 46(10),"},{"page":31,"text":"31\nCurrin, C., Mitchell, T., Morris, M., Ylvisaker, D.: Bayesian prediction of deterministic func-\ntions, with applications to the design and analysis of computer experiments.\nStatist. Assoc. 86(416), 953\u2013963 (1991)\nDeheeger, F.: Couplage m\u00b4 ecano-fiabiliste :2SMART \u2013 m\u00b4 ethodologie d\u2019apprentissage stochas-\ntique en fiabilit\u00b4 e. Ph.D. thesis, Universit\u00b4 e Blaise Pascal \u2013 Clermont II (2008)\nDeheeger, F., Lemaire, M.: Support vector machine for efficient subset simulations:2SMART\nmethod. In: Kanda, J., Takada, T., Furuta, H. (eds.) 10th International Conference on\nApplication of Statistics and Probability in Civil Engineering, Proceedings and Monographs\nin Engineering, Water and Earth Sciences, pp. 259\u2013260. Taylor & Francis (2007)\nEchard, B., Gayton, N., Lemaire, M.: Kriging-based monte carlo simulation to compute the\nprobability of failure efficiently: AK-MCS method. In: 6` emes Journ\u00b4 ees Nationales de Fia-\nbilit\u00b4 e, 24\u201326 mars, Toulouse, France (2010)\nFleuret, F., Geman, D.: Graded learning for object detection. In: Proceedings of the workshop\non Statistical and Computational Theories of Vision of the IEEE International Conference\non Computer Vision and Pattern Recognition (CVPR\/SCTV) (1999)\nFrazier, P.I., Powell, W.B., Dayanik, S.: A knowledge-gradient policy for sequential information\ncollection. SIAM Journal on Control and Optimization 47(5), 2410\u20132439 (2008)\nGinsbourger, D.: M\u00b4 etamod` eles multiples pour l\u2019approximation et l\u2019optimisation de fonctions\nnum\u00b4 eriques multivariables. Ph.D. thesis, Ecole nationale sup\u00b4 erieure des Mines de Saint-\nEtienne (2009)\nGinsbourger, D., Le Riche, R., L., C.: Kriging is well-suited to parallelize optimization. In:\nHiot, L.M., Ong, Y.S., Tenne, Y., Goh, C.K. (eds.) Computational Intelligence in Expen-\nsive Optimization Problems, Adaptation Learning and Optimization, vol. 2, pp. 131\u2013162.\nSpringer (2010)\nHandcock, M.S., Stein, M.L.: A bayesian analysis of kriging. Technometrics 35(4), 403\u2013410\n(1993)\nHurtado, J.E.: An examination of methods for approximating implicit limit state functions\nfrom the viewpoint of statistical learning theory. Structural Safety 26(3), 271\u2013293 (2004)\nHurtado, J.E.: Filtered importance sampling with support vector margin: A powerful method\nfor structural reliability analysis. Structural Safety 29(1), 2\u201315 (2007)\nJones, D.R., Schonlau, M., William, J.: Efficient global optimization of expensive black-box\nfunctions. Journal of Global Optimization 13(4), 455\u2013492 (1998)\nKennedy, M., O\u2019Hagan, A.: Bayesian calibration of computer models. Journal of the Royal\nStatistical Society. Series B (Statistical Methodology) 63(3), 425\u2013464 (2001)\nKimeldorf, G.S., Wahba, G.: A correspondence between Bayesian estimation on stochastic\nprocesses and smoothing by splines. Ann. Math. Statist. 41(2), 495\u2013502 (1970)\nKushner, H.J.: A new method of locating the maximum point of an arbitrary multipeak curve\nin the presence of noise. J. Basic Engineering 86, 97\u2013106 (1964)\nMockus, J.: Bayesian Approach to Global Optimization. Theory and Applications. Kluwer\nAcademic Publisher, Dordrecht (1989)\nMockus, J., Tiesis, V., Zilinskas, A.: The application of Bayesian methods for seeking the\nextremum. In: Dixon, L., Szego, E.G. (eds.) Towards Global Optimization, vol. 2, pp. 117\u2013\n129. Elsevier (1978)\nOakley, J.: Estimating percentiles of uncertain computer code outputs. J. Roy. Statist. Soc.\nSer. C 53(1), 83\u201393 (2004)\nOakley, J., O\u2019Hagan, A.: Bayesian inference for the uncertainty distribution of computer model\noutputs. Biometrika 89(4) (2002)\nOakley, J., O\u2019Hagan, A.: Probabilistic sensitivity analysis of complex models: a Bayesian ap-\nproach. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 66(3),\n751\u2013769 (2004)\nJ. Amer."},{"page":32,"text":"32\nO\u2019Hagan, A.: Curve fitting and optimal design for prediction. Journal of the Royal Statistical\nSociety. Series B (Methodological) 40(1), 1\u201342 (1978)\nParzen, E.: An approach to time series analysis. Ann. Math. Stat. 32, 951\u2013989 (1962)\nPaulo, R.: Default priors for gaussian processes. Annals of Statistics 33(2), 556\u2013582 (2005)\nPicheny, V., Ginsbourger, D., Roustant, O., Haftka, R.T., Kim, N.H.: Adaptive designs of\nexperiments for accurate approximation of target regions (2010)\nPiera-Martinez, M.: Mod\u00b4 elisation des comportements extr\u02c6 emes en ing\u00b4 enierie. Ph.D. thesis,\nUniversit\u00b4 e Paris Sud - Paris XI (2008)\nPradlwarter, H., Schu\u00a8 eller, G., Koutsourelakis, P., Charmpis, D.: Application of line sampling\nsimulation method to reliability benchmark problems. Structural Safety 29(3), 208 \u2013 221\n(2007). A Benchmark Study on Reliability in High Dimensions\nPress, W.H., Teukolsky, S.A., Vetterling, W.T., Flannery, B.P.: Numerical Recipes in C. The\nArt of Scientific Computing (Second Edition). Cambridge University Press (1992)\nRajashekhar, M.R., Ellingwood, B.R.: A new look at the response surface approach for relia-\nbility analysis. Structural Safety 12(3), 205\u2013220 (1993)\nRanjan, P., Bingham, D., Michailidis, G.: Sequential experiment design for contour estimation\nfrom complex computer codes. Technometrics 50(4), 527\u2013541 (2008)\nRubinstein, R., Kroese, D.: The Cross-Entropy Method. Springer (2004)\nSacks, J., Welch, W.J., Mitchell, T.J., Wynn, H.P.: Design and analysis of computer experi-\nments. Statistical Science 4(4), 409\u2013435 (1989)\nSantner, T.J., Williams, B.J., Notz, W.: The Design and Analysis of Computer Experiments.\nSpringer Verlag (2003)\nSchueremans, L.: Probabilistic evaluation of structural unreinforced masonry. Ph.D. thesis,\nCatholic University of Leuven (2001)\nSchueremans, L., Gemert, D.V.: Benefit of splines and neural networks in simulation based\nstructural reliability analysis. Structural safety 27(3), 246\u2013261 (2005)\nStein, M.L.: Interpolation of Spatial Data: Some Theory for Kriging. Springer, New York\n(1999)\nVazquez, E., Bect, J.: A sequential Bayesian algorithm to estimate a probability of failure. In:\nProceedings of the 15th IFAC Symposium on System Identification, SYSID 2009 15th IFAC\nSymposium on System Identification, SYSID 2009. Saint-Malo France (2009)\nVazquez, E., Piera-Martinez, M.: Estimation du volume des ensembles d\u2019excursion d\u2019un pro-\ncessus gaussien par krigeage intrins` eque. In: 39` eme Journ\u00b4 ees de Statistiques Conf\u00b4 erence\nJourn\u00b4 ee de Statistiques. Angers France (2007)\nVestrup, E.M.: The Theory of Measures and Integration. Wiley (2003)\nVillemonteix, J.: Optimisation de fonctions co\u02c6 uteuses. Ph.D. thesis, Universit\u00b4 e Paris-Sud XI,\nFacult\u00b4 e des Sciences d\u2019Orsay (2008)\nVillemonteix, J., Vazquez, E., Walter, E.: An informational approach to the global optimization\nof expensive-to-evaluate functions. Journal of Global Optimization 44(4), 509\u2013534 (2009)\nWaarts, P.H.: Structural reliability using finite element methods: an appraisal of DARS. Ph.D.\nthesis, Delft University of Technology (2000)\nWelch, W.J., Buck, R.J., Sacks, J., Wynn, H.P., Mitchell, T.J., Morris, M.D.: Screening, pre-\ndicting and computer experiments. Technometrics 34, 15\u201325 (1992)\nYaglom, A.M.: Correlation Theory of Stationary and Related Random Functions I: Basic\nResults. Springer Series in Statistics. Springer-Verlag, New york (1986)"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/David_Ginsbourger\/publication\/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure\/links\/00b4951ad79fb75b24000000.pdf","widgetId":"rgw34_56aba18d812a4"},"id":"rgw34_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=46587486&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw35_56aba18d812a4"},"id":"rgw35_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=46587486&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":46587486,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"00b4951ad79fb75b24000000","name":"David Ginsbourger","date":null,"nameLink":"profile\/David_Ginsbourger","filename":"1009.5177.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/David_Ginsbourger\/publication\/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure\/links\/00b4951ad79fb75b24000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/David_Ginsbourger\/publication\/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure\/links\/00b4951ad79fb75b24000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"a3399266a37cdad7193ab3799bff9a8b","showFileSizeNote":false,"fileSize":"1.04 MB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"00b4951ad79fb75b24000000","name":"David Ginsbourger","date":null,"nameLink":"profile\/David_Ginsbourger","filename":"1009.5177.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/David_Ginsbourger\/publication\/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure\/links\/00b4951ad79fb75b24000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/David_Ginsbourger\/publication\/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure\/links\/00b4951ad79fb75b24000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"a3399266a37cdad7193ab3799bff9a8b","showFileSizeNote":false,"fileSize":"1.04 MB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[{"props":{"position":"float","orientation":"portrait","coords":"pag:22:rect:123.00,253.56,345.82,35.67","ordinal":"5"},"assetId":"AS:277061462708224@1443068008327"}],"figureAssetIds":["AS:277061462708224@1443068008327"],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=zRawfm7mYTabs0P7iJS1Hn7hAAJP03_OM4wM0IQ4xYBf3sl5-ch9PIvGk8PpiZuCmh12FdeBo7zm5nQmxak_DQ.hM25ZqNSEZmMDXaT1dWS_mRIxeMrmr64JvsWRiaK6IPbGyFzhHBQaP_e8fbm_8a_bupGxeb0Dq8Bh70_klt0DQ","clickOnPill":"publication.PublicationFigures.html?_sg=pskcN0qt8p8qrkVqak7IH0_XhATiAgLvazucYfKIrPLwLU5Be4Wga1hQkbQe74C6GaebvuymMmJ2P2K4fBfdLA.KWdVCsQDIvg3oADVB3xOT8HkyQAQ-8SJE4imnbwm1ye4kuRnq9QVfKBnoPsH7uWxVNpZryO0CJd1kSb8_N_D2Q"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FDavid_Ginsbourger%2Fpublication%2F46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure%2Flinks%2F00b4951ad79fb75b24000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=oHJjoKNM7yTdCcGmbA5icFojYTCoJTubhj5XMMV6e0syKp-x4AwwtK9-gh1ec2ZueKiT1iTxIZ6CxoO6dTYYbw","urlHash":"a834d0efe6a295f09ad09206989e59e3","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=lYLMJfz9apZZGIW_indI9WdYV1gnlYCdO9J8PFhG9Dvl2YYigndpA1nIiCgylUOl8Kh2ImxapUVki9behDRQKN0tgjufV3Wqm6uPfbl5Deo.XECPZZ4dAvx5tAxU1gORL0ZVrXGAZ98ywhsWW5bpez1etZi92_0DuNXaf7NmR5md8Fy_AgEbxFuijnH83FLJfg.7SZUvY_R8GrT8v49mOa32J29BIUU32SC8Jrdn5S8DQDaqd6F9AdRmLYBGT1hDE18dnFT_dxsVTjYzcRahg3R1g","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"00b4951ad79fb75b24000000","trackedDownloads":{"00b4951ad79fb75b24000000":{"v":false,"d":false}},"assetId":"AS:101703621808135@1401259441678","readerDocId":"2758294","assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":46587486,"commentCursorPromo":null,"widgetId":"rgw37_56aba18d812a4"},"id":"rgw37_56aba18d812a4","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FDavid_Ginsbourger%2Fpublication%2F46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure%2Flinks%2F00b4951ad79fb75b24000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A101703621808135%401401259441678&publicationUid=46587486&linkId=00b4951ad79fb75b24000000&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Sequential design of computer experiments for the estimation of a\nprobability of failure","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=hWXTauZz2OGtAoRlBsMq0ikjZMDBmPI8sCpVGSDY84NGQEaQprIdcLjHry3pdzcP03JNjpcZ2jjDIAH6mAzFO3xYye4JvCQ8aaX6FXKK-lc.fm5EqoNEWb9D9qBLq3Y31eAQO76tUvXYS3PDcBkjHJOr3hctbs6-QBot6nTN4-Lt_qFPPiiD2t6XCo9mIGn1wQ.oVHmy8uB92CZg4a21IQybpTV0xiPVhBNSqLiNXODzPsB7Hm3gVjlgpMXfZhG5jcwyIvKsr77lrMoDmPRb6xFiQ","publicationUid":46587486,"trackedDownloads":{"00b4951ad79fb75b24000000":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw39_56aba18d812a4"},"id":"rgw39_56aba18d812a4","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw40_56aba18d812a4"},"id":"rgw40_56aba18d812a4","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw41_56aba18d812a4"},"id":"rgw41_56aba18d812a4","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw42_56aba18d812a4"},"id":"rgw42_56aba18d812a4","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw43_56aba18d812a4"},"id":"rgw43_56aba18d812a4","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw38_56aba18d812a4"},"id":"rgw38_56aba18d812a4","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw36_56aba18d812a4"},"id":"rgw36_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56aba18d812a4"},"id":"rgw2_56aba18d812a4","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":46587486},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=46587486&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56aba18d812a4"},"id":"rgw1_56aba18d812a4","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"7GVGBncgsLmP3cclCdQlyzw+E2Ph0tJ7NLR5NzvEKuZ1FC0np4DB1ZY\/EuTQtABcB1dloC51qXSJ8FfSDKhVlO8J7HKemO597S5JAYhclnX1fioe2kPdVoUNykb5TLpz6B3AjJAznET4HjiI7Q5tEJgi4etWasIjLLwgRPkIxdP3HIowtP8n9x3XFbrA9aqZBx1pOko\/OkWO47eidwYG4otHXWCRp7IHhjxb9VUkHDACruQCxPmJm1x7nn\/fF\/yKZpjWOawEJ7AXagl51srf34WkIBOQXvKzwto6YX4Zr5M=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Sequential design of computer experiments for the estimation of a\nprobability of failure\" \/>\n<meta property=\"og:description\" content=\"This paper deals with the problem of estimating the volume of the excursion set of a function f:\u211dd\n\u2192\u211d above a given threshold, under a probability measure on \u211dd\nthat is assumed to be known. In the...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure\/links\/00b4951ad79fb75b24000000\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure\" \/>\n<meta property=\"rg:id\" content=\"PB:46587486\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1007\/s11222-011-9241-4\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Sequential design of computer experiments for the estimation of a\nprobability of failure\" \/>\n<meta name=\"citation_author\" content=\"Julien Bect\" \/>\n<meta name=\"citation_author\" content=\"David Ginsbourger\" \/>\n<meta name=\"citation_author\" content=\"Ling Li\" \/>\n<meta name=\"citation_author\" content=\"Victor Picheny\" \/>\n<meta name=\"citation_author\" content=\"Emmanuel Vazquez\" \/>\n<meta name=\"citation_publication_date\" content=\"2010\/09\/27\" \/>\n<meta name=\"citation_journal_title\" content=\"Statistics and Computing\" \/>\n<meta name=\"citation_issn\" content=\"0960-3174\" \/>\n<meta name=\"citation_volume\" content=\"22\" \/>\n<meta name=\"citation_issue\" content=\"3\" \/>\n<meta name=\"citation_doi\" content=\"10.1007\/s11222-011-9241-4\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/David_Ginsbourger\/publication\/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure\/links\/00b4951ad79fb75b24000000.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/215868066921738\/styles\/pow\/publicliterature\/FigureList.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-b2cb5b31-265c-46fd-a781-2244c731fd68","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":453,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw44_56aba18d812a4"},"id":"rgw44_56aba18d812a4","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-b2cb5b31-265c-46fd-a781-2244c731fd68", "52cf2e4f334b3084aeefb74859c2ed0969d0587c");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-b2cb5b31-265c-46fd-a781-2244c731fd68", "52cf2e4f334b3084aeefb74859c2ed0969d0587c");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw45_56aba18d812a4"},"id":"rgw45_56aba18d812a4","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/46587486_Sequential_design_of_computer_experiments_for_the_estimation_of_aprobability_of_failure","requestToken":"tPUdiIQwIj0qqh1574yaJ2eQUUHEFQOwsSHc3qWoKSuNECx375odJF1bFAwTcZoEXAWqVZFD\/9uZsQMjnFfCyCw5\/lK3VZDOHhZRQRFi2FgSN4E6MbTi8B\/KJZSWIgii6CDDKjlOGLVjk8U5UuXsH17HFG1BTCk84u5EfDvSBPZF9dMFU5AGPmqQmZazLP6rDqa3WUdQ9jJgnBAUnLolmt1WkMynpEtBKp3PiJdtbXBHKUKDCJjtsBThE18BLIhF85ixExucCyn9M5ejxwC9DNawgZjbP7LI5gK+O6hFG5U=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=ZyqCP8Hx7H8t13x7JVqeRgppc85DHPY6OltfDhc_d12LGJB7yeQkKhTCmjikOqAT","encodedUrlAfterLogin":"cHVibGljYXRpb24vNDY1ODc0ODZfU2VxdWVudGlhbF9kZXNpZ25fb2ZfY29tcHV0ZXJfZXhwZXJpbWVudHNfZm9yX3RoZV9lc3RpbWF0aW9uX29mX2Fwcm9iYWJpbGl0eV9vZl9mYWlsdXJl","signupCallToAction":"Join for free","widgetId":"rgw47_56aba18d812a4"},"id":"rgw47_56aba18d812a4","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw46_56aba18d812a4"},"id":"rgw46_56aba18d812a4","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw48_56aba18d812a4"},"id":"rgw48_56aba18d812a4","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
