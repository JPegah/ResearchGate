<!DOCTYPE html> <html lang="en" class="" id="rgw42_56ab1ecceaa39"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="t7SXIoIFY8qSCKAmIKLKhMIZADDhe1dEcgYezsNLgZQsLAvT0V8Z1bWt2SLxd5twCyZslG7WRx5TFsC7CZDI2xlSGUClC2nlrYHw0T7Jz77dCxv13bo2LvKX0UHSrR4AWGASyG4Ez1CV932Zq4X1065DmmkZFZt5oLkw0wQhxPEWm8LSJ2KGlMigm+ljzMxEf3SM4HGWW9YpEZyt5rV36eNsUwtREUP6zgIwB2bJcndKp9XUA21g5ZmsQH4cPvjvsB30K3WrZ/q6zLRC4Kclj3/RTj3JR7hntUtvSNfO01Y="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-9aecc9a4-5de8-4133-b114-a19f9e8a02fa",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/260089482_Distributed_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable_Models" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models" />
<meta property="og:description" content="The recently developed Bayesian Gaussian process latent variable model
(GPLVM) is a powerful generative model for discovering low dimensional
embeddings in linear time complexity. However, modern..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/260089482_Distributed_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable_Models/links/02f9df6c0cf28b84cae11e25/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/260089482_Distributed_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable_Models" />
<meta property="rg:id" content="PB:260089482" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models" />
<meta name="citation_author" content="Yarin Gal" />
<meta name="citation_author" content="Mark van der Wilk" />
<meta name="citation_author" content="Carl E. Rasmussen" />
<meta name="citation_publication_date" content="2014/02/06" />
<meta name="citation_journal_title" content="Advances in neural information processing systems" />
<meta name="citation_issn" content="1049-5258" />
<meta name="citation_volume" content="4" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/260089482_Distributed_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable_Models" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/260089482_Distributed_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable_Models" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models</title>
<meta name="description" content="Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1ecceaa39" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1ecceaa39" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw7_56ab1ecceaa39">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Distributed%20Variational%20Inference%20in%20Sparse%20Gaussian%20Process%20Regression%20and%20Latent%20Variable%20Models&rft.title=Advances%20in%20Neural%20Information%20Processing%20Systems&rft.jtitle=Advances%20in%20Neural%20Information%20Processing%20Systems&rft.volume=4&rft.date=2014&rft.issn=1049-5258&rft.au=Yarin%20Gal%2CMark%20van%20der%20Wilk%2CCarl%20E.%20Rasmussen&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models</h1> <meta itemprop="headline" content="Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/260089482_Distributed_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable_Models/links/02f9df6c0cf28b84cae11e25/smallpreview.png">  <div id="rgw10_56ab1ecceaa39" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw11_56ab1ecceaa39"> <a href="researcher/2069013556_Yarin_Gal" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Yarin Gal" alt="Yarin Gal" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Yarin Gal</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56ab1ecceaa39">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2069013556_Yarin_Gal"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Yarin Gal" alt="Yarin Gal" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2069013556_Yarin_Gal" class="display-name">Yarin Gal</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw13_56ab1ecceaa39"> <a href="researcher/2043504277_Mark_van_der_Wilk" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Mark van der Wilk" alt="Mark van der Wilk" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Mark van der Wilk</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw14_56ab1ecceaa39">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2043504277_Mark_van_der_Wilk"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Mark van der Wilk" alt="Mark van der Wilk" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2043504277_Mark_van_der_Wilk" class="display-name">Mark van der Wilk</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw15_56ab1ecceaa39"> <a href="researcher/43277170_Carl_E_Rasmussen" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Carl E. Rasmussen" alt="Carl E. Rasmussen" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Carl E. Rasmussen</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw16_56ab1ecceaa39">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/43277170_Carl_E_Rasmussen"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Carl E. Rasmussen" alt="Carl E. Rasmussen" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/43277170_Carl_E_Rasmussen" class="display-name">Carl E. Rasmussen</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/1049-5258_Advances_in_neural_information_processing_systems"><span itemprop="name">Advances in neural information processing systems</span></a> </span>        <meta itemprop="datePublished" content="2014-02">  02/2014;  4.             <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1402.1389" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw17_56ab1ecceaa39" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>The recently developed Bayesian Gaussian process latent variable model<br />
(GPLVM) is a powerful generative model for discovering low dimensional<br />
embeddings in linear time complexity. However, modern datasets are so large<br />
that even linear-time models find them difficult to cope with. We introduce a<br />
novel re-parametrisation of variational inference for the GPLVM and sparse GP<br />
model that allows for an efficient distributed inference algorithm.<br />
We present a unifying derivation for both models, analytically deriving the<br />
optimal variational distribution over the inducing points. We then assess the<br />
suggested inference on datasets of different sizes, showing that it scales well<br />
with both data and computational resources. We furthermore demonstrate its<br />
practicality in real-world settings using datasets with up to 100 thousand<br />
points, comparing the inference to sequential implementations, assessing the<br />
distribution of the load among the different nodes, and testing its robustness<br />
to network failures.</div> </p>  </div>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw30_56ab1ecceaa39">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw29_56ab1ecceaa39"  itemprop="articleBody">  <p>Page 1</p> <p>Distributed Variational Inference in Sparse Gaussian Process Regression and<br />Latent Variable Models<br />Yarin Gal∗<br />Mark van der Wilk∗<br />Carl E. Rasmussen<br />University of Cambridge<br />YG279@CAM.AC.UK<br />MV310@CAM.AC.UK<br />CER54@CAM.AC.UK<br />Abstract<br />The recently developed Bayesian Gaussian pro-<br />cess latent variable model (GPLVM) is a pow-<br />erful generative model for discovering low di-<br />mensional embeddings in linear time complexity.<br />However, modern datasets are so large that even<br />linear-time models find them difficult to cope<br />with. We introduce a novel re-parametrisation of<br />variational inference for the GPLVM and sparse<br />GP model that allows for an efficient distributed<br />inference algorithm.<br />We present a unifying derivation for both mod-<br />els, analytically deriving the optimal variational<br />distribution over the inducing points. We then<br />assess the suggested inference on datasets of dif-<br />ferent sizes, showing that it scales well with both<br />data and computational resources. We further-<br />more demonstrate its practicality in real-world<br />settings using datasets with up to 100 thousand<br />points, comparing the inference to sequential im-<br />plementations, assessing the distribution of the<br />load among the different nodes, and testing its<br />robustness to network failures.<br />1. Introduction<br />The Bayesian Gaussian process latent variable model<br />(GPLVM, Titsias &amp; Lawrence (2010)) forms an important<br />component in the Bayesian non-parametric arsenal. Orig-<br />inating as an extension of sparse Gaussian process regres-<br />sion (Titsias, 2009), it can be used to perform non-linear<br />dimensionality reductions in linear time complexity. How-<br />ever, the use of the model with big datasets such as the ones<br />used in continuous-space natural language disambiguation<br />is quite cumbersome and challenging, and thus the model<br />has largely been ignored in such communities. Many other<br />Bayesian non-parametric tools share this limitation: the<br />Dirichlet process, for example, is slow to perform infer-<br />ence in, and often weeks pass between development cycles<br />∗Joint first author.<br />when working on large datasets (Gal &amp; Blunsom, 2013).<br />It is desirable to scale the model up to be able to han-<br />dle large amounts of data. One approach is to distribute<br />computation across many nodes in a parallel implemen-<br />tation. Many have reasoned about the requirements such<br />distributed inference procedures should satisfy (Brockwell,<br />2006; Wilkinson, 2005; Asuncion et al., 2008). The infer-<br />ence procedure should:<br />1. distribute the computational load evenly across cores,<br />2. scale favourably with the number of nodes,<br />3. and have low overhead in the global steps.<br />In this paper we introduce a novel distributed inference<br />algorithm for sparse GPs and the GPLVM that satisfies<br />the requirements above. We derive an exact unifying re-<br />parametrisation of the bounds derived by Titsias (2009) and<br />Titsias &amp; Lawrence (2010) which allows us to perform in-<br />ference using the original guarantees without the need for<br />weakerlowerbounds, andusingtheoptimalvariationaldis-<br />tribution over the inducing points. This is achieved by the<br />fact that conditioned on the inducing points, the data de-<br />couples and the variational parameters can be updated in-<br />dependently on different nodes, with the only communi-<br />cation between nodes requiring constant time. This also<br />allows the optimisation of the embeddings in the GPLVM<br />to be done by parallel scaled conjugate gradient (SCG).<br />We present an extensive set of experiments showing that<br />inference running time scales inversely with computa-<br />tional power. We also compare the results of our re-<br />parameterisation to those obtained from GPy (Titsias &amp;<br />Lawrence, 2010). We further demonstrate the practicality<br />of the inference, inspecting the distribution of the load over<br />the different nodes and comparing run-times to sequential<br />implementations. We test the robustness of the inference<br />by dropping out nodes at random, and measuring the re-<br />sulting log-marginal likelihood. Finally, we test the perfor-<br />mance of the GPLVM on datasets of sizes not commonly<br />handled in the GP community. We perform dimensionality<br />reduction on datasets with up to 100 thousand points and<br />demonstrate results on the USPS dataset.<br />arXiv:1402.1389v1  [stat.ML]  6 Feb 2014</p>  <p>Page 2</p> <p>Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models<br />The main contributions of this paper can be summarised<br />as follows. We scale the GPLVM and sparse GPs, pre-<br />senting the first unifying parallel inference algorithm for<br />them able to process datasets with hundreds of thousands<br />of points. An extensive set of experiments demonstrates the<br />properties of this suggested inference. The proposed infer-<br />ence was implemented in Python using the Map-Reduce<br />framework (Dean &amp; Ghemawat, 2008) to work on multi-<br />core architectures, and is available as an open-source pack-<br />age2. The full derivation of the re-parametrisation of the<br />inference is given in the supplementary material.<br />open source software package contains an extensively doc-<br />umentedimplementationofthederivations, withreferences<br />to the equations presented in the supplementary material<br />for explanation.<br />The<br />This paper is structured as follows. Is §2 we quickly review<br />the Gaussian process latent variable model and sparse GP<br />regression. In §3 we develop parallel inference in a unify-<br />ing setting, and in§4 wepresent anexperimental evaluation<br />of the inference. We expand on this in §5 where we demon-<br />strate the practicality of the inference in real-world settings<br />comparing it to sequential implementations and assessing<br />its distribution of the load among the different nodes, and<br />review related work in §6. Finally, we present the conclu-<br />sions in §7.<br />2. The Gaussian Process Latent Variable<br />Model and Sparse GP Regression<br />Next we quickly review the sparse Gaussian process re-<br />gression model and the Gaussian process latent variable<br />model (GPLVM). We will review the model structure and<br />the approximations developed (Titsias &amp; Lawrence, 2010;<br />Titsias, 2009) to make the inference efficient.<br />2.1. Sparse Gaussian Process Regression<br />Given<br />{X1,...,Xn}<br />{F1,...,Fn} we would like to find the posterior dis-<br />tribution over the functions mapping the inputs to the<br />outputs. The functions are assumed to be d dimensional<br />while the inputs are q dimensional.<br />written in matrix form for convenience:<br />a trainingdataset<br />their<br />consisting<br />corresponding<br />of<br />n<br />inputs<br />outputsand<br />This data is often<br />X ∈ Rn×q<br />F ∈ Rn×d<br />Fi= g(Xi)<br />Here we will adopt the convention that capitals denote ma-<br />trices of data, while subscripted vectors of the same letter<br />will denote a row, i.e. a single data point. For example, Fi<br />2see github.com/markvdw/GParML<br />denotes the i’th function value, while F denotes the matrix<br />of all given function values.<br />In the regression setting we place a Gaussian process prior<br />over the space of functions. This implies a joint Gaussian<br />distribution over all the function values3. For multivariate<br />functions, each dimension is be modelled by a separate GP.<br />gi∼ GP(µ(x),k(x,x?))<br />Kab= k(xa,xb)<br />p(F|X) = N(F;µ(X),K)<br />=exp?−1<br />We are often given a noisy evaluations of the function. For<br />this we introduce a new dataset Y containing the noisy ob-<br />servations, making the function values F latent. We as-<br />sume that the noise on each observation is i.i.d, with noise<br />precision β,<br />?<br />(2πβ−1)nd/2<br />2Tr?(F − µ(X))TK−1(F − µ(X))??<br />(2π)nd/2|K|d/2<br />p(Y |F) =<br />exp<br />−β<br />2Tr?(Y − F)T(Y − F)??<br />.<br />Evaluating p(Y |X) directly is an expensive operation that<br />involves the inversion of the n by n matrix K – thus requir-<br />ing O(n3) time complexity. Instead, Snelson &amp; Ghahra-<br />mani (2006) suggested the use of a collection of m “induc-<br />ing points” – a set of points lying in the same input space<br />with corresponding values in the output space. These in-<br />ducing points aim to summarise the characteristics of the<br />function using less points than the training data.<br />Given the locations of the inducing points Z, an m by q<br />matrix for m inducing points, and the inferred values of the<br />points u, an m by d matrix, prediction corresponds to tak-<br />ing the GP posterior using only the inducing points instead<br />of the whole training set, which requires only O(m3) time<br />complexity,<br />p(F∗|X∗,Y ) ≈<br />?<br />where Kmm is the covariance between the m inducing<br />points, and likewise for the other subscripts.<br />N?F∗;k∗mK−1<br />mmu,k∗∗− k∗mK−1<br />mmkm∗<br />?p(u|Y,X)du<br />Learning the distribution over the values of the inducing<br />points requires a simplifying approximation to be made on<br />p(F|X,u,Z), i.e. how the training data relates to the in-<br />ducing points. One example is assuming the determinis-<br />tic relationship F = KnmK−1<br />complexity of O(nm2). Qui˜ nonero-Candela &amp; Rasmussen<br />3We follow the definition of matrix normal distribution<br />(Arnold, 1981). For a full treatment of Gaussian Processes, see<br />Rasmussen &amp; Williams (2006).<br />mmu, giving a computational</p>  <p>Page 3</p> <p>Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models<br />(2005) view this procedure as changing the prior to make<br />inference more tractable, with Z as hyperparameters which<br />can be tuned using optimisation. On the other hand, Titsias<br />(2009) relates this approximation to a variational approx-<br />imation, with Z as variational parameters. This gives the<br />marginal likelihood above an alternative interpretation as a<br />lower bound on the exact marginal likelihood. Again, the<br />Z values can be optimised over to tighten the lower bound.<br />A detailed derivation is given in section 3 of the supple-<br />mentary material.<br />2.2. Gaussian Process Latent Variable Models<br />The GPLVM model is the unsupervised equivalent of the<br />regression problem above. This model can be viewed as a<br />non-linear generalisation of PCA (Lawrence, 2005). The<br />model set-up is identical to the regression case, only we<br />assume a prior over the now latent variable X and attempt<br />to infer both the mapping from X to Y and the distribution<br />over X at the same time.<br />Xi∼ N(Xi;0,I)<br />F(Xi) ∼ GP(0,k(X,X))<br />Yi∼ N(Fi,β−1I)<br />A Variational Bayes approximation for this model has been<br />developed by Titsias &amp; Lawrence (2010) using similar<br />techniques as for variational sparse GPs. In fact, the sparse<br />GP can be seen as a special case of the GPLVM where the<br />inputs are given zero variance.<br />The main task in deriving approximate inference revolves<br />around finding a variational lower bound to:<br />?<br />Which leads to a Gaussian approximation to the posterior<br />q(X) ≈ p(X|Y ), explained in detail in section 4 of the<br />supplementary material. In the next section we derive a<br />parallel inference scheme for both models following a re-<br />parametrisation of the derivations of Titsias (2009) which<br />allows us to decouple the distribution over data points.<br />p(Y ) =p(Y |F)p(F|X)p(X)d(F,X)<br />3. Parallel inference<br />We now exploit the conditional independence of the data<br />given the inducing points to derive a parallel inference<br />scheme for both the sparse GP model and the GPLVM,<br />which will allow us to easily scale these models to large<br />datasets. The key equations are given below, with an in-<br />depth explanation given in sections sections 3 and 4 of the<br />supplementary material. We present a unifying derivation<br />of the inference procedures for both the regression case and<br />the latent variable modelling (LVM) case, by identifying<br />that the explicit inputs in the regression case are identical<br />to the latent inputs in the LVM case when their mean is set<br />to the observed inputs and used with variance 0 (i.e. the<br />latent inputs are fixed and not optimised).<br />We start with the general expression for the log marginal<br />likelihood of the sparse GP regression model, after intro-<br />ducing the inducing points,<br />?<br />The LVM derivation encapsulates this expression by mul-<br />tiplying with the prior over X and then marginalising over<br />X:<br />?<br />We then introduce a free-form variational distribution q(u)<br />over the inducing points, and another over X (where in the<br />regression case, p(X)’s and q(X)’s variance is set to 0 and<br />their mean set to X). Using Jensen’s inequality we get the<br />following lower bound:<br />logp(Y |X) = logp(Y |F)p(F|X,u)p(u)d(u,F).<br />logp(Y ) = logp(Y |F)p(F|X,u)p(u)p(X)d(u,F,X).<br />log p(Y |X)<br />≥<br />?<br />?<br />p(F|X,u)q(u)logp(Y |F)p(u)<br />??<br />q(u)<br />d(u,F)<br />=q(u)p(F|X,u)logp(Y |F)d(F) + logp(u)<br />q(u)<br />(3.1)<br />?<br />d(u)<br />all distributions that involve u also depend on Z which we<br />have omitted for brevity. Next we integrate p(Y ) over X to<br />be able to use 3.1,<br />?<br />≥<br />logp(Y ) = logq(X)p(Y |X)p(X)<br />?<br />q(X)<br />d(X)<br />?<br />q(X) logp(Y |X) + logp(X)<br />q(X)<br />?<br />d(X)<br />(3.2)<br />and obtain a bound which can be used for both models.<br />Up to here the derivation is identical to the two derivations<br />given in (Titsias &amp; Lawrence, 2010; Titsias, 2009). How-<br />ever, now we exploit the conditional independence when<br />conditioned on u to break the inference into small inde-<br />pendent components.<br />3.1. Decoupling the data conditioned on the inducing<br />points<br />The introduction of the inducing points decouples the func-<br />tion values from each other in the following sense. If we<br />represent Y as the individual data points (Y1;Y2;...;Yn)<br />with Yi∈ R1×dand similarly for F, we can write the lower</p>  <p>Page 4</p> <p>Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models<br />bound as a sum over the data points, since Yiare indepen-<br />dent of Fjfor j ?= i:<br />?<br />?<br />i=1<br />n<br />?<br />Simplifying this expression and integrating over X we get<br />that each term is given by<br />?YiYT<br />− 2?Fi?p(Fi|Xi,u)q(Xi)YT<br />where we use triangular brackets ?F?p(F)to denote the ex-<br />pectation of F with respect to the distribution p(F).<br />p(F|X,u)logp(Y |F)d(F)<br />=p(F|X,u)<br />?<br />n<br />?<br />logp(Yi|Fi)d(F)<br />=<br />i=1<br />p(Fi|Xi,u)logp(Yi|Fi)d(Fi)<br />−d<br />2log(2πβ−1) −β<br />2<br />i<br />i +?FiFT<br />i<br />?<br />p(Fi|Xi,u)q(Xi))?<br />Now, using calculus of variations we can find optimal q(u)<br />analytically. Plugging the optimal distribution into eq. 3.1<br />and using further algebraic manipulations we obtain the<br />following lower bound:<br />logp(Y ) ≥<br />−nd<br />−d<br />+β2<br />2<br />log2π +dn<br />2<br />logβ +d<br />2log|Kmm|<br />2A −βd<br />2log|Kmm+ βD| −β<br />2B +βd<br />2Tr(K−1<br />mmD)<br />2Tr(CT· (Kmm+ βD)−1· C) − KL<br />(3.3)<br />where<br />A =<br />n<br />?<br />n<br />?<br />i=1<br />YiYT<br />i<br />C =<br />i=1<br />?Kmi?q(Xi)Yi<br />B =<br />n<br />?<br />n<br />?<br />i=1<br />?Kii?q(Xi)<br />D =<br />i=1<br />?KmiKim?q(Xi)<br />and<br />KL =<br />n<br />?<br />i=1<br />KL(q(Xi)||p(Xi))<br />when the inputs are latent or set to 0 when they are ob-<br />served.<br />Notice that the obtained unifying bound is identical to<br />the ones derived in (Titsias, 2009) for the regression case<br />and (Titsias &amp; Lawrence, 2010) for the LVM case since<br />?Kmi?q(Xi)= Kmifor q(Xi) with variance 0 and mean<br />Xi. However, the terms are re-parametrised as independent<br />sums over the input points – sums that can be computed on<br />different nodes in a network without inter-communication.<br />An in-depth explanation of the different transitions is given<br />in the supplementary material sections 3 and 4.<br />3.2. The parallel inference<br />A parallel inference algorithm can be easily derived based<br />on this factorisation. Using the Map-Reduce framework<br />(Dean &amp; Ghemawat, 2008) we can maintain different sub-<br />sets of the inputs and their corresponding outputs on each<br />node in a parallel implementation and distribute the global<br />parameters (such as the kernel hyper-parameters and the lo-<br />cations of the inducing points) to the nodes, collecting only<br />the partial terms calculated on each node.<br />We denote byG the set of global parameters over which we<br />need to perform optimisation. These include Z (the loca-<br />tions of the inducing points), β (the observation noise), and<br />k (the set of kernel hyper-parameters). Additionally we de-<br />note by Lkthe set of local parameters on each node k that<br />need to be optimised. These include the mean and variance<br />for each output point for the LVM model. First, we send<br />to all end-point nodes the global parameters G for them to<br />calculate the partial terms ?Kmi?q(Xi)Yi, ?KmiKim?q(Xi),<br />?Kii?q(Xi), YiYT<br />tion of these terms is explained in more detail in the sup-<br />plementary material section 4. The end-point nodes return<br />these partial terms to the central node (these are m×m×q<br />matrices – constant space complexity for fixed m). The<br />central node then sends the accumulated terms and partial<br />derivatives back to the nodes and performs global optimi-<br />sation over G. For the LVM task the nodes then perform<br />at the same time local optimisation on Lk, the embedding<br />posterior parameters. In total, we have two Map-Reduce<br />steps between the central node and the end-point nodes to<br />follow:<br />i, and KL(q(Xi)||p(Xi)). The calcula-<br />1. The central node distributes G,<br />2. Each end-point node k returns a partial sum of the<br />terms A,B,C,D and KL based on Lk,<br />3. The central node calculates F, ∂F (m×m×q matri-<br />ces) and distributes to the end-point nodes,<br />4. The central node optimises G; at the same time the<br />end-point nodes optimise Lk.<br />for the regression task the third step is not required as well<br />as the second part of the forth step. The appendices of the<br />supplementary material contain the derivations of the par-<br />tial derivatives with respect to the global variables as well<br />as the local ones.<br />Optimisation of the global parameters can be done using<br />any procedure that utilises the calculated partial derivative<br />(such as scaled conjugate gradient (Møller, 1993)), and the<br />optimisation of the local variables can be carried out by<br />parallelising SCG or using local gradient descent. We now<br />explore the developed inference empirically and evaluate<br />its properties on a range of tasks.</p>  <p>Page 5</p> <p>Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models<br />easydata<br />Parallel implementation latent space<br />4<br />3<br />2<br />1<br />0<br />1<br />2<br />3<br />1.5<br />1.0<br />0.5<br />0.0<br />0.5<br />1.0<br />1.5<br />2.0<br />2.5<br />1.0<br />0.5<br />0.0<br />0.5<br />1.0<br />1.5<br />3210123<br />Dim 1<br />1.5<br />1.0<br />0.5<br />0.0<br />0.5<br />1.0<br />1.5<br />2.0<br />2.5<br />Dim 2<br />3210123<br />1.5<br />1.0<br />0.5<br />0.0<br />0.5<br />1.0<br />1.5<br />2.0<br />2.5<br />PCA latent space<br />Figure 1. A sample from the synthetic data with 100 points (left) and its lower dimensional embedding using GPLVM (centre) and PCA<br />(right).<br />Time scaling with cores<br />100<br />101<br />cores<br />100<br />101<br />102<br />time / iter (s)<br />Parallel (no overhead)<br />Parallel (total)<br />Figure 2. Running time per iteration for 100K points synthetic<br />dataset, asafunctionofavailablecoresonlog-scale. Alsoplotted<br />is the total amount of time spent in the computations alone to give<br />an indication of the thread-communication overhead.<br />0 2040<br />dataset size (103)<br />6080100<br />0<br />5<br />10<br />15<br />20<br />25<br />30<br />35<br />40<br />time / iter (s)<br />Time scaling with data<br />available cores<br />Parallel (total)<br />Parallel (no overhead)<br />GPy<br />010 2030 405060<br />Figure 3. Time per iteration when scaling the computational<br />resources proportionally to dataset size up to 100K points.<br />Time per iteration with and without threading overhead are<br />shown, as well as GPy running time (a sequential implementa-<br />tion of the inference), for comparison.<br />4. Experimental Evaluation<br />We assessed the inference on a wide set of experiments<br />evaluating its scalability with computation power as well as<br />with data, and explored the numerical stability of the infer-<br />ence. We further explored the distribution of the load over<br />the different nodes and compared the inference to sequen-<br />tial implementations such as GPy (Titsias &amp; Lawrence,<br />2010). Finally, we tested the robustness of our parallel in-<br />ference procedure by dropping-out nodes at random and<br />measure the resulting log-marginal likelihood. We tested<br />the performance of the GPLVM on large datasets not com-<br />monly handled in the GP community performing dimen-<br />sionality reduction and density estimation on the full USPS<br />dataset(4Kdatapoints)anddatasetswithhundredsofthou-<br />sands of points.<br />4.1. Implementation &amp; Setup<br />In the following experiments we used an SE ARD kernel<br />over the latent space in order to automatically determine<br />the intrinsic dimensionality of the latent space, as in (Tit-<br />sias &amp; Lawrence, 2010). We initialise our latent points<br />using PCA and our inducing points using k-means with<br />added noise. We optimise using scaled conjugate gradi-<br />ent (Møller, 1993) following the original implementation<br />by (Titsias &amp; Lawrence, 2010).<br />Our experiments were run on a 4 processor, 64-core<br />Opteron 6276 machine. One caveat concerning this pro-<br />cessor is that there is only one floating point unit (FPU) per<br />two cores. Often a single thread does not utilise the FPU<br />fully, so clever instruction scheduling allows two threads<br />to use one FPU without much performance degradation.<br />Throughout these experiments we assume a 64-core ma-<br />chine, but the FPU sharing could account for some of the<br />diminishing returns observed when using a large number of<br />threads.</p>  <p>Page 6</p> <p>Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models<br />321012<br />3<br />3<br />First principal latent dimension<br />2<br />1<br />0<br />1<br />2<br />3<br />4<br />Second principal latent dimension<br />Parallel GPLVM<br />32<br />4<br />2<br />0<br />2<br />4<br />Second principal latent dimension<br />Figure 4. Latent space produced by the parallel inference (left) and GPy (right) using the oilflow dataset (Titsias &amp; Lawrence, 2010).<br />4.2. Scaling with Computation Power<br />We investigate how much inference on a given dataset can<br />be sped up using our parallel implementation, given more<br />computational resources. In the ideal case we would see<br />a halving of time given double the resources, but due to<br />overheads in distributing the computation it is usually only<br />possible to get close.<br />We assess the improvement of the running time of the algo-<br />rithm on a simple synthetic dataset of which large amounts<br />of data could easily be generated. The dataset was obtained<br />by simulating a 1D latent space and transforming this into<br />3D observations through linear functions with sines super-<br />imposed (see figure 1). 100k points were generated and the<br />algorithm was run using an increasing number of cores and<br />a 2 dimensional latent space. We measured both the total<br />running time of the algorithm, including initialisation and<br />threading overheads, and the amount of time spent only in<br />the two Map-Reduce functions. This allows us to assess<br />the impact of our new inference on the running time as a<br />whole, and the effectiveness on just the parts which were<br />parallelised.<br />Figure 2 shows the improvement of run-times as a function<br />of available cores. When initialisation and threading over-<br />heads are not considered, we obtain a relation very close<br />to the ideal t ∝ c · (cores)−1. When doubling the num-<br />ber of cores from 5 to 10 we achieve a factor 1.99 decrease<br />in computation time – very close to ideal. There is some<br />hint of diminishing returns when we scale up from 30 to 60<br />cores, where we decrease running time by a factor of 1.644.<br />Wheninitialisationandoverheadsareconsidered, wespeed<br />up inference by a factor of 1.96 for a change from 5 to 10<br />cores, and by a factor of 1.54 for a change from 30 to 60<br />cores.<br />Figure 2 also indicates that there is a substantial overhead<br />in our implementation. This is not due to fixed costs such<br />as the PCA initialisation, but more due to the large costs<br />associated with creating the worker threads in Python that<br />run the map functions in parallel. There is still much per-<br />formance to be gained from optimising this.<br />4.3. Scaling with Data<br />Using the same setup, we assessed the scaling of the run-<br />ning time as we increased both the dataset size and com-<br />putational resources equally. This answers the question<br />of how large a dataset we can handle given an increasing<br />amount of resources.<br />For a doubling of data, we doubled the number of avail-<br />able CPUs. In the ideal case of no constant costs and no<br />threading overheads, computation time should be constant.<br />Again, we measure the total running time of the algorithm,<br />and the time spent only in the Map-Reduce functions.<br />Figure 3 shows that we are able to effectively utilise the<br />extra computational resources. Our total running time, in-<br />cluding overheads, takes 67% longer for a dataset scaled<br />by 60 times. The Map-Reduce calculations only take 35%<br />longer.<br />4.4. Comparison to GPy<br />We also compare the computation time of our inference<br />procedure to the single threaded, but highly optimised<br />GPy implementation (see figure 3).<br />significantly outperform GPy given more computational<br />resources. Our parallel inference allows us to run the<br />GPLVM on datasets which would simply take too long to<br />run with a single threaded implementation. However, for<br />small datasets GPy is significantly faster. This is partly due<br />to the large overheads discussed in the previous experiment<br />and partly due to optimisations in GPy. GPy performs cer-<br />tain calculations in native C++, while our implementation<br />is written fully in Python.<br />We show that we<br />In addition to scaling experiments, we compare our latent<br />space to GPy, which we use as a reference implementation.<br />Just like in the original paper (Titsias &amp; Lawrence, 2010),<br />we used the oil-flow dataset. Both algorithms were run un-</p>  <p>Page 7</p> <p>Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models<br />Load balancing - 5 cores<br />0 100200300 400500<br />iter<br />20.2<br />20.4<br />20.6<br />20.8<br />21.0<br />Thread execution time (s)<br />mean<br />min<br />max<br />0100200300 400500<br />iter<br />0.0<br />0.5<br />1.0<br />1.5<br />2.0<br />2.5<br />3.0<br />Thread execution time (s)<br />Load balancing - 60 cores<br />mean<br />min<br />max<br />Figure 5. Load distribution for each iteration. The maximum time spent in a thread is the rate limiting step. Shown are the minimum,<br />mean and maximum execution times of all threads when using 5 (left) and 60 (right) cores.<br />til no significant improvement in the marginal likelihood<br />was found.<br />The two latent spaces are shown in figure 4. The latent<br />spaces are qualitatively similar, but differ due to a slightly<br />different implementation of the optimiser. Like the results<br />in (Titsias &amp; Lawrence, 2010) all but one of the ARD pa-<br />rameters decrease to zero, giving an effectively 1D latent<br />space.<br />4.5. USPS Data<br />As a practical test, we ran our inference scheme on the<br />USPS digits dataset, as used in (Rasmussen &amp; Williams,<br />2006). We trained a GPLVM over the complete dataset of<br />4649 examples containing all digits from 0-9, using 150<br />inducing points. The training was run overnight and was<br />complete the following day. We managed to reconstruct<br />digits with 34% of their pixels missing (see figure 6). We<br />also ran the same experiment using only 1000 digits to as-<br />sess the utility of using more data in the GPLVM. We com-<br />pared the average mean reconstruction error and found that<br />the larger dataset gives a 5.9% improvement.<br />As this dataset can now be run overnight, a greater number<br />of development cycles can be used for fine tuning of the<br />model.<br />Figure 6. Digit reconstruction of the USPS data. Left panel<br />shows the input to the model, with dropped pixels, the middle<br />pannel shows the reconstructed digit, and the right panel shows<br />the ground truth.<br />5. Practicality of the Inference<br />5.1. Distribution of the Load<br />One of our stated requirements for a practical parallel in-<br />ference algorithm is an approximately equal distribution of<br />the load on the nodes. This is especially relevant in a Map-<br />Reduce framework, where the reduce step can only happen<br />after all map computations have finished, so the maximum<br />execution time of one of the threads is the rate limiting step.<br />Figure 5 shows the minimum, maximum and average exe-<br />cution time of all threads for a range of iterations of a run.<br />On average there is a 3.7% difference between the mean<br />and maximum run-time of a thread, suggesting an even dis-<br />tribution of the load.<br />5.2. Robustness to Node Failure<br />One other desirable characteristic of a parallel inference<br />scheme is robustness to failure of nodes. One way of deal-<br />ing with this would be to load the data to a different node<br />and restart the calculation. However, since the speed of one<br />iteration is limited by the slowest calculation on one of the<br />nodes, this could slow down the algorithm by the time it<br />takes to load the intermediate data onto the new node. An<br />alternative strategy would be to drop the partial term from<br />the calculation and use a slightly noisy gradient calculation<br />in the optimisation for one iteration. Here we investigate<br />the robustness of our inference to this procedure.<br />We ran our parallel inference on the oil-flow dataset using<br />the same setting as above for 500 iterations accumulating<br />the log marginal likelihood as a function of the iteration.<br />We used 10 nodes and simulated failure frequencies of 0%,<br />1% and 2% per iteration. The experiment was repeated<br />10 times and the log marginal likelihood averaged. Even a<br />failure rate of 1% per iteration for 500 iterations translates<br />to a high number of 1 out the 10 nodes failing on average<br />every 10 iterations.</p>  <p>Page 8</p> <p>Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models<br />100<br />101<br />102<br />103<br />iteration<br />25000<br />20000<br />15000<br />10000<br />5000<br />0<br />log marginal likelihood<br />0<br />0.01<br />0.02<br />Figure 7. Node failure test, for node failure frequencies of 0%,<br />1% and 2% per iteration. Shown is the average log marginal like-<br />lihood as a function of the iteration for 500 iterations.<br />As we observe in figure 7 a node failure frequency of 1%<br />hurtstotalperformancebydecreasingthelogmarginallike-<br />lihood from -1500 to -5000 on average. It seems that a<br />higher failure frequency leads to convergence to worse lo-<br />cal optima or a failure of the optimiser, possibly because of<br />the finite differences approximation to the function curva-<br />ture used by SCG, which might suffer from noisy gradient<br />estimations. It is also interesting to note that the embed-<br />dings discovered are less pronounced than the ones shown<br />in figure 4 but still have only one major latent dimension.<br />For 0% failure rate the ARD parameters are 0.02 for all but<br />one dimension (0.15), for 1% failure rate the ARD param-<br />eters are 0.10 for all but one dimension (0.17), and for 2%<br />failure rate the ARD parameters are 0.29 for all but one<br />dimension (0.34).<br />6. Related Work<br />Recent research carried out by Hensman et al. (2013) pro-<br />posed stochastic variational inference (SVI, Hoffman et al.<br />(2013)) for the problem of scaling up sparse Gaussian pro-<br />cess regression. In their research, the variational distri-<br />bution over the inducing points was explicitly represented<br />and optimisation was performed over the variational distri-<br />bution itself instead of using the optimal analytical solu-<br />tion, a necessity of the SVI setting that cannot be averted.<br />Hensman et al. (2013) proposed future directions for re-<br />search which include the derivation of SVI for GPLVMs,<br />and suggested that the proposed SVI for sparse GP regres-<br />sion could be carried out in parallel.<br />However, in order to perform SVI a weaker lower bound<br />on the log marginal likelihood than the one proposed by<br />Titsias (2009) has to be used, and many parameters have to<br />be optimised in addition to the kernel hyper-parameters. In<br />addition to that, since the model uses mini-batches, the lo-<br />cations of the inducing points cannot be inferred easily and<br />have to be fixed in advance. This is due to the strong cor-<br />relation between the locations and values of the inducing<br />points. Figure 8 demonstrates that a negative log marginal<br />likelihood local minimum for the location of an inducing<br />point when fixing u is not necessarily a minimum as a func-<br />tion of u. Furthermore, in the SVI setting many additional<br />optimiser parameters have to be introduced and fine-tuned<br />by hand (Hensman et al., 2013) to control the step-length<br />of different quantities such as the gradient. Additionally,<br />many heuristics are used to decide what quantities to up-<br />date when. For example, the step-lengths do not change for<br />the first epoch, and then change differently for different pa-<br />rameters. These difficulties make SVI rather hard to work<br />with.<br />7. Conclusions<br />We have scaled the GPLVM and sparse GPs model present-<br />ing the first unifying parallel inference algorithm which is<br />able to process datasets with hundreds of thousands of data<br />points. An extensive set of experiments studying the prop-<br />erties of the suggested inference was presented. The infer-<br />ence was implemented for a multi-core architecture and is<br />available as an open-source package, containing an exten-<br />sively documented implementation of the derivations, with<br />references to the equations presented in the supplementary<br />material for explanation.<br />−1.6−1.55−1.5−1.45−1.4−1.35−1.3−1.25−1.2<br />0<br />500<br />1000<br />1500<br />Fixed u<br />nlml<br />−1.6−1.55−1.5−1.45 −1.4<br />z<br />−1.35−1.3−1.25−1.2<br />50.6<br />50.7<br />50.8<br />u(z)<br />nlml<br />Figure 8. Negative log-likelihood as a function of the location of<br />a single inducing point z. In the top panel q(u) is fixed and the in<br />bottom q(u) is a function z. A negative log marginal likelihood<br />local minimum for the location of a single inducing point when<br />fixing u is not necessarily a minimum as a function of u</p>  <p>Page 9</p> <p>Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models<br />References<br />Arnold, S.F. The theory of linear models and multivariate<br />analysis. Wiley series in probability and mathematical<br />statistics: Probability and mathematical statistics. Wiley,<br />1981. ISBN 9780471050650.<br />Asuncion, Arthur U, Smyth, Padhraic, and Welling, Max.<br />Asynchronous distributed learning of topic models. In<br />Advances in Neural Information Processing Systems, pp.<br />81–88, 2008.<br />Brockwell, A. E. Parallel Markov Chain Monte Carlo sim-<br />ulation by Pre-Fetching. Journal of Computational and<br />Graphical Statistics, 15(1):pp. 246–261, 2006.<br />10618600.<br />ISSN<br />Dean, Jeffrey and Ghemawat, Sanjay. MapReduce: Sim-<br />plified data processing on large clusters. Commun. ACM,<br />51(1):107–113, January 2008. ISSN 0001-0782.<br />Gal, Yarin and Blunsom, Phil. A systematic Bayesian treat-<br />ment of the IBM alignment models. In Proceedings of<br />NAACL-HLT, pp. 969–977, 2013.<br />Hensman, James, Fusi, Nicolo, and Lawrence, Neil D.<br />Gaussian processes for big data. 2013.<br />Hoffman, Matthew D., Blei, David M., Wang, Chong, and<br />Paisley, John. Stochastic Variational Inference. JOUR-<br />NAL OF MACHINE LEARNING RESEARCH, 14:1303–<br />1347, MAY 2013. ISSN 1532-4435.<br />Lawrence, Neil. Probabilistic non-linear principal compo-<br />nent analysis with gaussian process latent variable mod-<br />els. TheJournalofMachineLearningResearch, 6:1783–<br />1816, 2005.<br />Møller, Martin Fodslette. A scaled conjugate gradient al-<br />gorithm for fast supervised learning. Neural networks, 6<br />(4):525–533, 1993.<br />Qui˜ nonero-Candela, Joaquin and Rasmussen, Carl Ed-<br />ward. A unifying view of sparse approximate gaussian<br />process regression. Journal of Machine Learning Re-<br />search, 6:2005, 2005.<br />Rasmussen, Carl Edward and Williams, Christopher K. I.<br />Gaussian Processes for Machine Learning (Adaptive<br />Computation and Machine Learning). The MIT Press,<br />2006. ISBN 026218253X.<br />Snelson, Edward and Ghahramani, Zoubin. Sparse gaus-<br />sian processes using pseudo-inputs. In Advances in Neu-<br />ral Information Processing Systems 18, pp. 1257–1264.<br />MIT press, 2006.<br />Titsias, M. K. Variational learning of inducing variables in<br />sparse Gaussian processes. Technical report, Technical<br />Report, 2009.<br />Titsias, Michalis and Lawrence, Neil. Bayesian gaussian<br />process latent variable model. 2010.<br />Wilkinson, Darren J. Parallel Bayesian computation. In<br />Kontoghiorghes, Erricos John (ed.), Handbook of Paral-<br />lel Computing and Statistics, volume 184, pp. 477–508.<br />Chapman and Hall/CRC, Boca Raton, FL, USA, 2005.</p>   </div> <div id="rgw22_56ab1ecceaa39" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw23_56ab1ecceaa39">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw24_56ab1ecceaa39"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://de.arxiv.org/pdf/1402.1389" target="_blank" rel="nofollow" class="publication-viewer" title="Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models">Distributed Variational Inference in Sparse Gaussi...</a> </div>  <div class="details">   Available from <a href="http://de.arxiv.org/pdf/1402.1389" target="_blank" rel="nofollow">de.arxiv.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw31_56ab1ecceaa39" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (15) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw32_56ab1ecceaa39" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw33_56ab1ecceaa39" >  <div class="indent-left">  <div id="rgw34_56ab1ecceaa39" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/281486855_Semi-described_and_semi-supervised_learning_with_Gaussian_processes">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Andreas_Damianou" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Andreas Damianou </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw35_56ab1ecceaa39">  <li class="citation-context-item"> "For this, we develop a variational constraint mechanism which constrains the distribution of the input space given the observed noisy values. This approach is fast, and the whole framework can be incorporated into a parallel inference algorithm [Gal et al., 2014; Dai et al., 2014]. In contrast, Damianou et al. [2011] consider a separate process for modelling the input distribution. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/281486855_Semi-described_and_semi-supervised_learning_with_Gaussian_processes"> <span class="publication-title js-publication-title">Semi-described and semi-supervised learning with Gaussian processes</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/59288229_Andreas_Damianou" class="authors js-author-name ga-publications-authors">Andreas Damianou</a> &middot;     <a href="researcher/39663468_Neil_D_Lawrence" class="authors js-author-name ga-publications-authors">Neil D. Lawrence</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Propagating input uncertainty through non-linear Gaussian process (GP)
mappings is intractable. This hinders the task of training GPs using uncertain
and partially observed inputs. In this paper we refer to this task as
&quot;semi-described learning&quot;. We then introduce a GP framework that solves both,
the semi-described and the semi-supervised learning problems (where missing
values occur in the outputs). Auto-regressive state space simulation is also
recognised as a special case of semi-described learning. To achieve our goal we
develop variational methods for handling semi-described inputs in GPs, and
couple them with algorithms that allow for imputing the missing values while
treating the uncertainty in a principled, Bayesian manner. Extensive
experiments on simulated and real-world data study the problems of iterative
forecasting and regression/classification with missing values. The results
suggest that the principled propagation of uncertainty stemming from our
framework can significantly improve performance in these tasks. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Sep 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Andreas_Damianou/publication/281486855_Semi-described_and_semi-supervised_learning_with_Gaussian_processes/links/5631381008ae13bc6c3565ac.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw36_56ab1ecceaa39" >  <div class="indent-left">  <div id="rgw37_56ab1ecceaa39" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/283296484_Semi-described_and_semi-supervised_learning_with_Gaussian_processes">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Andreas_Damianou" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Andreas Damianou </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw38_56ab1ecceaa39">  <li class="citation-context-item"> "For this, we develop a variational constraint mechanism which constrains the distribution of the input space given the observed noisy values. This approach is fast, and the whole framework can be incorporated into a parallel inference algorithm [Gal et al., 2014; Dai et al., 2014]. In contrast, Damianou et al. [2011] consider a separate process for modelling the input distribution. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Conference Paper:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/283296484_Semi-described_and_semi-supervised_learning_with_Gaussian_processes"> <span class="publication-title js-publication-title">Semi-described and semi-supervised learning with Gaussian processes</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/59288229_Andreas_Damianou" class="authors js-author-name ga-publications-authors">Andreas Damianou</a> &middot;     <a href="researcher/39663468_Neil_D_Lawrence" class="authors js-author-name ga-publications-authors">Neil D Lawrence</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Propagating input uncertainty through non-linear Gaussian process (GP) mappings is intractable. This hinders the task of training GPs using uncertain and partially observed inputs. In this paper we refer to this task as &quot; semi-described learning &quot;. We then introduce a GP framework that solves both, the semi-described and the semi-supervised learning problems (where missing values occur in the outputs). Auto-regressive state space simulation is also recognised as a special case of semi-described learning. To achieve our goal we develop variational methods for handling semi-described inputs in GPs, and couple them with algorithms that allow for imputing the missing values while treating the uncertainty in a principled, Bayesian manner. Extensive experiments on simulated and real-world data study the problems of iterative forecasting and regres-sion/classification with missing values. The results suggest that the principled propagation of uncertainty stemming from our framework can significantly improve performance in these tasks. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Conference Paper &middot; Jul 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Andreas_Damianou/publication/283296484_Semi-described_and_semi-supervised_learning_with_Gaussian_processes/links/56313a2308ae13bc6c356667.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw39_56ab1ecceaa39" >  <div class="indent-left">  <div id="rgw40_56ab1ecceaa39" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/278047923_Mondrian_Forests_for_Large-Scale_Regression_when_Uncertainty_Matters">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1506.03805" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: de.arxiv.org </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw41_56ab1ecceaa39">  <li class="citation-context-item"> "We refer to [5] for a recent summary of large scale Gaussian processes. Hensman et al. [10] and Gal et al. [8] use stochastic variational inference to speed up GPs, building on the variational bound developed by Titsias [24]. Deisenroth and Ng [5] present the robust Bayesian committee machine (rBCM), which combines predictions from experts that operate on subsets of data. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/278047923_Mondrian_Forests_for_Large-Scale_Regression_when_Uncertainty_Matters"> <span class="publication-title js-publication-title">Mondrian Forests for Large-Scale Regression when Uncertainty Matters</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/74747270_Balaji_Lakshminarayanan" class="authors js-author-name ga-publications-authors">Balaji Lakshminarayanan</a> &middot;     <a href="researcher/43837085_Daniel_M_Roy" class="authors js-author-name ga-publications-authors">Daniel M. Roy</a> &middot;     <a href="researcher/9164246_Yee_Whye_Teh" class="authors js-author-name ga-publications-authors">Yee Whye Teh</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Decision forests and their variants deliver efficient state-of-the-art
prediction performance, but many applications, such as probabilistic numerics,
Bayesian optimization, uncertainty quantification, and planning, also demand a
measure of the uncertainty associated with each prediction. Existing approaches
to measuring the uncertainty of decision forest predictions are known to
perform poorly, and so Gaussian processes and approximate variants are the
standard tools in these application areas. With a goal of providing efficient
state-of-the-art predictions together with estimates of uncertainty, we
describe a regression framework using Mondrian forests, an approach to decision
forests where the underlying random decision trees are modeled as i.i.d.
Mondrian processes and efficient algorithms perform nonparametric Bayesian
inference within each tree and ordinary model combination across trees. In our
framework, the underlying nonparametric inference is a Gaussian diffusion over
the tree, which results in a multivariate Gaussian calculation for inference in
light of finite data that can be carried out efficiently using belief
propagation. On a synthetic task designed to mimic a typical probabilistic
numerical task, we demonstrate that Mondrian forest regression delivers far
superior uncertainty quantification than existing decision forest methods with
little-to-no cost in predictive performance. We then turn to a real-world
probabilistic numerics benchmark modeling flight delay, where we compare
Mondrian forests also to large-scale GP approximate methods. Our experiments
demonstrate that Mondrian forests can deliver superior uncertainty assessments
to GPs, as measured by negative predictive log density, with little-to-no loss
in RMSE performance. </span> </div>    <div class="publication-meta publication-meta">  <span class="ico-publication-preview reset-background"></span> Preview    &middot; Article &middot; Jun 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw26_56ab1ecceaa39" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw27_56ab1ecceaa39">  </ul> </div> </div>   <div id="rgw18_56ab1ecceaa39" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw19_56ab1ecceaa39"> <div> <h5> <a href="publication/260089617_Variational_Inference_in_the_Gaussian_Process_Latent_Variable_Model_and_Sparse_GP_Regression_--_a_Gentle_Tutorial" class="color-inherit ga-similar-publication-title"><span class="publication-title">Variational Inference in the Gaussian Process Latent Variable Model and Sparse GP Regression -- a Gentle Tutorial</span></a>  </h5>  <div class="authors"> <a href="researcher/2069013556_Yarin_Gal" class="authors ga-similar-publication-author">Yarin Gal</a>, <a href="researcher/2043504277_Mark_van_der_Wilk" class="authors ga-similar-publication-author">Mark van der Wilk</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw20_56ab1ecceaa39"> <div> <h5> <a href="publication/222448041_Reverse_regression_for_latent-variable_models" class="color-inherit ga-similar-publication-title"><span class="publication-title">Reverse regression for latent-variable models</span></a>  </h5>  <div class="authors"> <a href="researcher/61611585_David_K_Levine" class="authors ga-similar-publication-author">David K. Levine</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw21_56ab1ecceaa39"> <div> <h5> <a href="publication/221602983_Incorporating_Boosted_Regression_Trees_into_Ecological_Latent_Variable_Models" class="color-inherit ga-similar-publication-title"><span class="publication-title">Incorporating Boosted Regression Trees into Ecological Latent Variable Models</span></a>  </h5>  <div class="authors"> <a href="researcher/70520515_Rebecca_A_Hutchinson" class="authors ga-similar-publication-author">Rebecca A. Hutchinson</a>, <a href="researcher/70633086_Li-Ping_Liu" class="authors ga-similar-publication-author">Li-Ping Liu</a>, <a href="researcher/3230926_Thomas_G_Dietterich" class="authors ga-similar-publication-author">Thomas G. Dietterich</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw43_56ab1ecceaa39" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw44_56ab1ecceaa39">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw45_56ab1ecceaa39" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=6W7weEOTqeG6npvhwzeGQQ1e94CJPKZSXIg3H5BRvojpF5YtrpG23bm__KBXnmqB" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="FpzFFYINX+rq3OSHElJ+cStdzWDXC0bX2WRbgSDoewEDFYqthRUSi3eR3tHMAuGX5a3/ye+KXj81tlmg9Az6Qz6U4qPD2y7aUkZFDuu+sOsSi3huVNt2TOkFbJKrdJKSVciYo2uVGB4bdtVv9DaY5in4sSa7h/rsz3vRxWPUgpXm8MlPpH1mZL9rgoxbZVM/Zgb+nV4ECGScRM6yLJ7k4sm+y9b7r8XQsrRZ3ixeRGpNqa+AFKQZiu7EQZ9pO4rJg2RyQRV5NtdloGfkHvLDxdONlBjL8uZ6utAhxo6dHkQ="/> <input type="hidden" name="urlAfterLogin" value="publication/260089482_Distributed_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable_Models"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjYwMDg5NDgyX0Rpc3RyaWJ1dGVkX1ZhcmlhdGlvbmFsX0luZmVyZW5jZV9pbl9TcGFyc2VfR2F1c3NpYW5fUHJvY2Vzc19SZWdyZXNzaW9uX2FuZF9MYXRlbnRfVmFyaWFibGVfTW9kZWxz"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjYwMDg5NDgyX0Rpc3RyaWJ1dGVkX1ZhcmlhdGlvbmFsX0luZmVyZW5jZV9pbl9TcGFyc2VfR2F1c3NpYW5fUHJvY2Vzc19SZWdyZXNzaW9uX2FuZF9MYXRlbnRfVmFyaWFibGVfTW9kZWxz"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjYwMDg5NDgyX0Rpc3RyaWJ1dGVkX1ZhcmlhdGlvbmFsX0luZmVyZW5jZV9pbl9TcGFyc2VfR2F1c3NpYW5fUHJvY2Vzc19SZWdyZXNzaW9uX2FuZF9MYXRlbnRfVmFyaWFibGVfTW9kZWxz"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw46_56ab1ecceaa39"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 1477;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"profileUrl":"researcher\/2069013556_Yarin_Gal","fullname":"Yarin Gal","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2549355721578\/images\/template\/default\/profile\/profile_default_m.png","profileStats":[null,{"data":{"publicationCount":9,"widgetId":"rgw5_56ab1ecceaa39"},"id":"rgw5_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorPublicationCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorPublicationCount.html?authorUid=2069013556","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"followerCount":1,"widgetId":"rgw6_56ab1ecceaa39"},"id":"rgw6_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorFollowerCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorFollowerCount.html?authorUid=2069013556","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw4_56ab1ecceaa39"},"id":"rgw4_56ab1ecceaa39","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorBadge.html?authorUid=2069013556","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw3_56ab1ecceaa39"},"id":"rgw3_56ab1ecceaa39","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=260089482","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":260089482,"title":"Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models","journalTitle":"Advances in neural information processing systems","journalDetailsTooltip":{"data":{"journalTitle":"Advances in neural information processing systems","journalAbbrev":"Adv Neural Inform Process Syst","publisher":"IEEE Conference on Neural Information Processing Systems--Natural and Synthetic, Massachusetts Institute of Technology Press","issn":"1049-5258","impactFactor":"0.00","fiveYearImpactFactor":"0.00","citedHalfLife":"0.00","immediacyIndex":"0.00","eigenFactor":"0.00","articleInfluence":"0.00","widgetId":"rgw8_56ab1ecceaa39"},"id":"rgw8_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=1049-5258","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"02\/2014;","publicationDateRobot":"2014-02","article":"4.","journalTitle":"Advances in neural information processing systems","journalUrl":"journal\/1049-5258_Advances_in_neural_information_processing_systems"}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1402.1389","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models"},{"key":"rft.title","value":"Advances in Neural Information Processing Systems"},{"key":"rft.jtitle","value":"Advances in Neural Information Processing Systems"},{"key":"rft.volume","value":"4"},{"key":"rft.date","value":"2014"},{"key":"rft.issn","value":"1049-5258"},{"key":"rft.au","value":"Yarin Gal,Mark van der Wilk,Carl E. Rasmussen"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw9_56ab1ecceaa39"},"id":"rgw9_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=260089482","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":260089482,"peopleItems":[{"data":{"authorUrl":"researcher\/2069013556_Yarin_Gal","authorNameOnPublication":"Yarin Gal","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Yarin Gal","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2069013556_Yarin_Gal","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56ab1ecceaa39"},"id":"rgw12_56ab1ecceaa39","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2069013556&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56ab1ecceaa39"},"id":"rgw11_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2069013556&authorNameOnPublication=Yarin%20Gal","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/2043504277_Mark_van_der_Wilk","authorNameOnPublication":"Mark van der Wilk","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Mark van der Wilk","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2043504277_Mark_van_der_Wilk","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw14_56ab1ecceaa39"},"id":"rgw14_56ab1ecceaa39","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2043504277&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw13_56ab1ecceaa39"},"id":"rgw13_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2043504277&authorNameOnPublication=Mark%20van%20der%20Wilk","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/43277170_Carl_E_Rasmussen","authorNameOnPublication":"Carl E. Rasmussen","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Carl E. Rasmussen","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/43277170_Carl_E_Rasmussen","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw16_56ab1ecceaa39"},"id":"rgw16_56ab1ecceaa39","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=43277170&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw15_56ab1ecceaa39"},"id":"rgw15_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=43277170&authorNameOnPublication=Carl%20E.%20Rasmussen","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw10_56ab1ecceaa39"},"id":"rgw10_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=260089482&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":260089482,"abstract":"<noscript><\/noscript><div>The recently developed Bayesian Gaussian process latent variable model<br \/>\n(GPLVM) is a powerful generative model for discovering low dimensional<br \/>\nembeddings in linear time complexity. However, modern datasets are so large<br \/>\nthat even linear-time models find them difficult to cope with. We introduce a<br \/>\nnovel re-parametrisation of variational inference for the GPLVM and sparse GP<br \/>\nmodel that allows for an efficient distributed inference algorithm.<br \/>\nWe present a unifying derivation for both models, analytically deriving the<br \/>\noptimal variational distribution over the inducing points. We then assess the<br \/>\nsuggested inference on datasets of different sizes, showing that it scales well<br \/>\nwith both data and computational resources. We furthermore demonstrate its<br \/>\npracticality in real-world settings using datasets with up to 100 thousand<br \/>\npoints, comparing the inference to sequential implementations, assessing the<br \/>\ndistribution of the load among the different nodes, and testing its robustness<br \/>\nto network failures.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw17_56ab1ecceaa39"},"id":"rgw17_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=260089482","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/260089482_Distributed_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable_Models\/links\/02f9df6c0cf28b84cae11e25\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw7_56ab1ecceaa39"},"id":"rgw7_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=260089482&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2069013556,"url":"researcher\/2069013556_Yarin_Gal","fullname":"Yarin Gal","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2043504277,"url":"researcher\/2043504277_Mark_van_der_Wilk","fullname":"Mark van der Wilk","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2014","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/260089617_Variational_Inference_in_the_Gaussian_Process_Latent_Variable_Model_and_Sparse_GP_Regression_--_a_Gentle_Tutorial","usePlainButton":true,"publicationUid":260089617,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/260089617_Variational_Inference_in_the_Gaussian_Process_Latent_Variable_Model_and_Sparse_GP_Regression_--_a_Gentle_Tutorial","title":"Variational Inference in the Gaussian Process Latent Variable Model and Sparse GP Regression -- a Gentle Tutorial","displayTitleAsLink":true,"authors":[{"id":2069013556,"url":"researcher\/2069013556_Yarin_Gal","fullname":"Yarin Gal","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2043504277,"url":"researcher\/2043504277_Mark_van_der_Wilk","fullname":"Mark van der Wilk","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/260089617_Variational_Inference_in_the_Gaussian_Process_Latent_Variable_Model_and_Sparse_GP_Regression_--_a_Gentle_Tutorial","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/260089617_Variational_Inference_in_the_Gaussian_Process_Latent_Variable_Model_and_Sparse_GP_Regression_--_a_Gentle_Tutorial\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab1ecceaa39"},"id":"rgw19_56ab1ecceaa39","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=260089617","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":61611585,"url":"researcher\/61611585_David_K_Levine","fullname":"David K. Levine","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 1984","journal":"Journal of Econometrics","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/222448041_Reverse_regression_for_latent-variable_models","usePlainButton":true,"publicationUid":222448041,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.60","url":"publication\/222448041_Reverse_regression_for_latent-variable_models","title":"Reverse regression for latent-variable models","displayTitleAsLink":true,"authors":[{"id":61611585,"url":"researcher\/61611585_David_K_Levine","fullname":"David K. Levine","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of Econometrics 02\/1984; 32(2-32):291-292. DOI:10.1016\/0304-4076(86)90042-4"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/222448041_Reverse_regression_for_latent-variable_models","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/222448041_Reverse_regression_for_latent-variable_models\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56ab1ecceaa39"},"id":"rgw20_56ab1ecceaa39","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=222448041","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":70520515,"url":"researcher\/70520515_Rebecca_A_Hutchinson","fullname":"Rebecca A. Hutchinson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70633086,"url":"researcher\/70633086_Li-Ping_Liu","fullname":"Li-Ping Liu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":3230926,"url":"researcher\/3230926_Thomas_G_Dietterich","fullname":"Thomas G. Dietterich","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Conference Paper","publicationDate":"Jan 2011","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/221602983_Incorporating_Boosted_Regression_Trees_into_Ecological_Latent_Variable_Models","usePlainButton":true,"publicationUid":221602983,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/221602983_Incorporating_Boosted_Regression_Trees_into_Ecological_Latent_Variable_Models","title":"Incorporating Boosted Regression Trees into Ecological Latent Variable Models","displayTitleAsLink":true,"authors":[{"id":70520515,"url":"researcher\/70520515_Rebecca_A_Hutchinson","fullname":"Rebecca A. Hutchinson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70633086,"url":"researcher\/70633086_Li-Ping_Liu","fullname":"Li-Ping Liu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":3230926,"url":"researcher\/3230926_Thomas_G_Dietterich","fullname":"Thomas G. Dietterich","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011, San Francisco, California, USA, August 7-11, 2011; 01\/2011"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/221602983_Incorporating_Boosted_Regression_Trees_into_Ecological_Latent_Variable_Models","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/221602983_Incorporating_Boosted_Regression_Trees_into_Ecological_Latent_Variable_Models\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw21_56ab1ecceaa39"},"id":"rgw21_56ab1ecceaa39","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=221602983","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw18_56ab1ecceaa39"},"id":"rgw18_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=260089482&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":260089482,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":260089482,"publicationType":"article","linkId":"02f9df6c0cf28b84cae11e25","fileName":"Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models","fileUrl":"http:\/\/de.arxiv.org\/pdf\/1402.1389","name":"de.arxiv.org","nameUrl":"http:\/\/de.arxiv.org\/pdf\/1402.1389","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw24_56ab1ecceaa39"},"id":"rgw24_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=260089482&linkId=02f9df6c0cf28b84cae11e25&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw23_56ab1ecceaa39"},"id":"rgw23_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=260089482&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":8,"valueFormatted":"8","widgetId":"rgw25_56ab1ecceaa39"},"id":"rgw25_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=260089482","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw22_56ab1ecceaa39"},"id":"rgw22_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=260089482&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":260089482,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw27_56ab1ecceaa39"},"id":"rgw27_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=260089482&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":8,"valueFormatted":"8","widgetId":"rgw28_56ab1ecceaa39"},"id":"rgw28_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=260089482","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw26_56ab1ecceaa39"},"id":"rgw26_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=260089482&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Distributed Variational Inference in Sparse Gaussian Process Regression and\nLatent Variable Models\nYarin Gal\u2217\nMark van der Wilk\u2217\nCarl E. Rasmussen\nUniversity of Cambridge\nYG279@CAM.AC.UK\nMV310@CAM.AC.UK\nCER54@CAM.AC.UK\nAbstract\nThe recently developed Bayesian Gaussian pro-\ncess latent variable model (GPLVM) is a pow-\nerful generative model for discovering low di-\nmensional embeddings in linear time complexity.\nHowever, modern datasets are so large that even\nlinear-time models find them difficult to cope\nwith. We introduce a novel re-parametrisation of\nvariational inference for the GPLVM and sparse\nGP model that allows for an efficient distributed\ninference algorithm.\nWe present a unifying derivation for both mod-\nels, analytically deriving the optimal variational\ndistribution over the inducing points. We then\nassess the suggested inference on datasets of dif-\nferent sizes, showing that it scales well with both\ndata and computational resources. We further-\nmore demonstrate its practicality in real-world\nsettings using datasets with up to 100 thousand\npoints, comparing the inference to sequential im-\nplementations, assessing the distribution of the\nload among the different nodes, and testing its\nrobustness to network failures.\n1. Introduction\nThe Bayesian Gaussian process latent variable model\n(GPLVM, Titsias & Lawrence (2010)) forms an important\ncomponent in the Bayesian non-parametric arsenal. Orig-\ninating as an extension of sparse Gaussian process regres-\nsion (Titsias, 2009), it can be used to perform non-linear\ndimensionality reductions in linear time complexity. How-\never, the use of the model with big datasets such as the ones\nused in continuous-space natural language disambiguation\nis quite cumbersome and challenging, and thus the model\nhas largely been ignored in such communities. Many other\nBayesian non-parametric tools share this limitation: the\nDirichlet process, for example, is slow to perform infer-\nence in, and often weeks pass between development cycles\n\u2217Joint first author.\nwhen working on large datasets (Gal & Blunsom, 2013).\nIt is desirable to scale the model up to be able to han-\ndle large amounts of data. One approach is to distribute\ncomputation across many nodes in a parallel implemen-\ntation. Many have reasoned about the requirements such\ndistributed inference procedures should satisfy (Brockwell,\n2006; Wilkinson, 2005; Asuncion et al., 2008). The infer-\nence procedure should:\n1. distribute the computational load evenly across cores,\n2. scale favourably with the number of nodes,\n3. and have low overhead in the global steps.\nIn this paper we introduce a novel distributed inference\nalgorithm for sparse GPs and the GPLVM that satisfies\nthe requirements above. We derive an exact unifying re-\nparametrisation of the bounds derived by Titsias (2009) and\nTitsias & Lawrence (2010) which allows us to perform in-\nference using the original guarantees without the need for\nweakerlowerbounds, andusingtheoptimalvariationaldis-\ntribution over the inducing points. This is achieved by the\nfact that conditioned on the inducing points, the data de-\ncouples and the variational parameters can be updated in-\ndependently on different nodes, with the only communi-\ncation between nodes requiring constant time. This also\nallows the optimisation of the embeddings in the GPLVM\nto be done by parallel scaled conjugate gradient (SCG).\nWe present an extensive set of experiments showing that\ninference running time scales inversely with computa-\ntional power. We also compare the results of our re-\nparameterisation to those obtained from GPy (Titsias &\nLawrence, 2010). We further demonstrate the practicality\nof the inference, inspecting the distribution of the load over\nthe different nodes and comparing run-times to sequential\nimplementations. We test the robustness of the inference\nby dropping out nodes at random, and measuring the re-\nsulting log-marginal likelihood. Finally, we test the perfor-\nmance of the GPLVM on datasets of sizes not commonly\nhandled in the GP community. We perform dimensionality\nreduction on datasets with up to 100 thousand points and\ndemonstrate results on the USPS dataset.\narXiv:1402.1389v1  [stat.ML]  6 Feb 2014"},{"page":2,"text":"Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models\nThe main contributions of this paper can be summarised\nas follows. We scale the GPLVM and sparse GPs, pre-\nsenting the first unifying parallel inference algorithm for\nthem able to process datasets with hundreds of thousands\nof points. An extensive set of experiments demonstrates the\nproperties of this suggested inference. The proposed infer-\nence was implemented in Python using the Map-Reduce\nframework (Dean & Ghemawat, 2008) to work on multi-\ncore architectures, and is available as an open-source pack-\nage2. The full derivation of the re-parametrisation of the\ninference is given in the supplementary material.\nopen source software package contains an extensively doc-\numentedimplementationofthederivations, withreferences\nto the equations presented in the supplementary material\nfor explanation.\nThe\nThis paper is structured as follows. Is \u00a72 we quickly review\nthe Gaussian process latent variable model and sparse GP\nregression. In \u00a73 we develop parallel inference in a unify-\ning setting, and in\u00a74 wepresent anexperimental evaluation\nof the inference. We expand on this in \u00a75 where we demon-\nstrate the practicality of the inference in real-world settings\ncomparing it to sequential implementations and assessing\nits distribution of the load among the different nodes, and\nreview related work in \u00a76. Finally, we present the conclu-\nsions in \u00a77.\n2. The Gaussian Process Latent Variable\nModel and Sparse GP Regression\nNext we quickly review the sparse Gaussian process re-\ngression model and the Gaussian process latent variable\nmodel (GPLVM). We will review the model structure and\nthe approximations developed (Titsias & Lawrence, 2010;\nTitsias, 2009) to make the inference efficient.\n2.1. Sparse Gaussian Process Regression\nGiven\n{X1,...,Xn}\n{F1,...,Fn} we would like to find the posterior dis-\ntribution over the functions mapping the inputs to the\noutputs. The functions are assumed to be d dimensional\nwhile the inputs are q dimensional.\nwritten in matrix form for convenience:\na trainingdataset\ntheir\nconsisting\ncorresponding\nof\nn\ninputs\noutputsand\nThis data is often\nX \u2208 Rn\u00d7q\nF \u2208 Rn\u00d7d\nFi= g(Xi)\nHere we will adopt the convention that capitals denote ma-\ntrices of data, while subscripted vectors of the same letter\nwill denote a row, i.e. a single data point. For example, Fi\n2see github.com\/markvdw\/GParML\ndenotes the i\u2019th function value, while F denotes the matrix\nof all given function values.\nIn the regression setting we place a Gaussian process prior\nover the space of functions. This implies a joint Gaussian\ndistribution over all the function values3. For multivariate\nfunctions, each dimension is be modelled by a separate GP.\ngi\u223c GP(\u00b5(x),k(x,x?))\nKab= k(xa,xb)\np(F|X) = N(F;\u00b5(X),K)\n=exp?\u22121\nWe are often given a noisy evaluations of the function. For\nthis we introduce a new dataset Y containing the noisy ob-\nservations, making the function values F latent. We as-\nsume that the noise on each observation is i.i.d, with noise\nprecision \u03b2,\n?\n(2\u03c0\u03b2\u22121)nd\/2\n2Tr?(F \u2212 \u00b5(X))TK\u22121(F \u2212 \u00b5(X))??\n(2\u03c0)nd\/2|K|d\/2\np(Y |F) =\nexp\n\u2212\u03b2\n2Tr?(Y \u2212 F)T(Y \u2212 F)??\n.\nEvaluating p(Y |X) directly is an expensive operation that\ninvolves the inversion of the n by n matrix K \u2013 thus requir-\ning O(n3) time complexity. Instead, Snelson & Ghahra-\nmani (2006) suggested the use of a collection of m \u201cinduc-\ning points\u201d \u2013 a set of points lying in the same input space\nwith corresponding values in the output space. These in-\nducing points aim to summarise the characteristics of the\nfunction using less points than the training data.\nGiven the locations of the inducing points Z, an m by q\nmatrix for m inducing points, and the inferred values of the\npoints u, an m by d matrix, prediction corresponds to tak-\ning the GP posterior using only the inducing points instead\nof the whole training set, which requires only O(m3) time\ncomplexity,\np(F\u2217|X\u2217,Y ) \u2248\n?\nwhere Kmm is the covariance between the m inducing\npoints, and likewise for the other subscripts.\nN?F\u2217;k\u2217mK\u22121\nmmu,k\u2217\u2217\u2212 k\u2217mK\u22121\nmmkm\u2217\n?p(u|Y,X)du\nLearning the distribution over the values of the inducing\npoints requires a simplifying approximation to be made on\np(F|X,u,Z), i.e. how the training data relates to the in-\nducing points. One example is assuming the determinis-\ntic relationship F = KnmK\u22121\ncomplexity of O(nm2). Qui\u02dc nonero-Candela & Rasmussen\n3We follow the definition of matrix normal distribution\n(Arnold, 1981). For a full treatment of Gaussian Processes, see\nRasmussen & Williams (2006).\nmmu, giving a computational"},{"page":3,"text":"Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models\n(2005) view this procedure as changing the prior to make\ninference more tractable, with Z as hyperparameters which\ncan be tuned using optimisation. On the other hand, Titsias\n(2009) relates this approximation to a variational approx-\nimation, with Z as variational parameters. This gives the\nmarginal likelihood above an alternative interpretation as a\nlower bound on the exact marginal likelihood. Again, the\nZ values can be optimised over to tighten the lower bound.\nA detailed derivation is given in section 3 of the supple-\nmentary material.\n2.2. Gaussian Process Latent Variable Models\nThe GPLVM model is the unsupervised equivalent of the\nregression problem above. This model can be viewed as a\nnon-linear generalisation of PCA (Lawrence, 2005). The\nmodel set-up is identical to the regression case, only we\nassume a prior over the now latent variable X and attempt\nto infer both the mapping from X to Y and the distribution\nover X at the same time.\nXi\u223c N(Xi;0,I)\nF(Xi) \u223c GP(0,k(X,X))\nYi\u223c N(Fi,\u03b2\u22121I)\nA Variational Bayes approximation for this model has been\ndeveloped by Titsias & Lawrence (2010) using similar\ntechniques as for variational sparse GPs. In fact, the sparse\nGP can be seen as a special case of the GPLVM where the\ninputs are given zero variance.\nThe main task in deriving approximate inference revolves\naround finding a variational lower bound to:\n?\nWhich leads to a Gaussian approximation to the posterior\nq(X) \u2248 p(X|Y ), explained in detail in section 4 of the\nsupplementary material. In the next section we derive a\nparallel inference scheme for both models following a re-\nparametrisation of the derivations of Titsias (2009) which\nallows us to decouple the distribution over data points.\np(Y ) =p(Y |F)p(F|X)p(X)d(F,X)\n3. Parallel inference\nWe now exploit the conditional independence of the data\ngiven the inducing points to derive a parallel inference\nscheme for both the sparse GP model and the GPLVM,\nwhich will allow us to easily scale these models to large\ndatasets. The key equations are given below, with an in-\ndepth explanation given in sections sections 3 and 4 of the\nsupplementary material. We present a unifying derivation\nof the inference procedures for both the regression case and\nthe latent variable modelling (LVM) case, by identifying\nthat the explicit inputs in the regression case are identical\nto the latent inputs in the LVM case when their mean is set\nto the observed inputs and used with variance 0 (i.e. the\nlatent inputs are fixed and not optimised).\nWe start with the general expression for the log marginal\nlikelihood of the sparse GP regression model, after intro-\nducing the inducing points,\n?\nThe LVM derivation encapsulates this expression by mul-\ntiplying with the prior over X and then marginalising over\nX:\n?\nWe then introduce a free-form variational distribution q(u)\nover the inducing points, and another over X (where in the\nregression case, p(X)\u2019s and q(X)\u2019s variance is set to 0 and\ntheir mean set to X). Using Jensen\u2019s inequality we get the\nfollowing lower bound:\nlogp(Y |X) = logp(Y |F)p(F|X,u)p(u)d(u,F).\nlogp(Y ) = logp(Y |F)p(F|X,u)p(u)p(X)d(u,F,X).\nlog p(Y |X)\n\u2265\n?\n?\np(F|X,u)q(u)logp(Y |F)p(u)\n??\nq(u)\nd(u,F)\n=q(u)p(F|X,u)logp(Y |F)d(F) + logp(u)\nq(u)\n(3.1)\n?\nd(u)\nall distributions that involve u also depend on Z which we\nhave omitted for brevity. Next we integrate p(Y ) over X to\nbe able to use 3.1,\n?\n\u2265\nlogp(Y ) = logq(X)p(Y |X)p(X)\n?\nq(X)\nd(X)\n?\nq(X) logp(Y |X) + logp(X)\nq(X)\n?\nd(X)\n(3.2)\nand obtain a bound which can be used for both models.\nUp to here the derivation is identical to the two derivations\ngiven in (Titsias & Lawrence, 2010; Titsias, 2009). How-\never, now we exploit the conditional independence when\nconditioned on u to break the inference into small inde-\npendent components.\n3.1. Decoupling the data conditioned on the inducing\npoints\nThe introduction of the inducing points decouples the func-\ntion values from each other in the following sense. If we\nrepresent Y as the individual data points (Y1;Y2;...;Yn)\nwith Yi\u2208 R1\u00d7dand similarly for F, we can write the lower"},{"page":4,"text":"Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models\nbound as a sum over the data points, since Yiare indepen-\ndent of Fjfor j ?= i:\n?\n?\ni=1\nn\n?\nSimplifying this expression and integrating over X we get\nthat each term is given by\n?YiYT\n\u2212 2?Fi?p(Fi|Xi,u)q(Xi)YT\nwhere we use triangular brackets ?F?p(F)to denote the ex-\npectation of F with respect to the distribution p(F).\np(F|X,u)logp(Y |F)d(F)\n=p(F|X,u)\n?\nn\n?\nlogp(Yi|Fi)d(F)\n=\ni=1\np(Fi|Xi,u)logp(Yi|Fi)d(Fi)\n\u2212d\n2log(2\u03c0\u03b2\u22121) \u2212\u03b2\n2\ni\ni +?FiFT\ni\n?\np(Fi|Xi,u)q(Xi))?\nNow, using calculus of variations we can find optimal q(u)\nanalytically. Plugging the optimal distribution into eq. 3.1\nand using further algebraic manipulations we obtain the\nfollowing lower bound:\nlogp(Y ) \u2265\n\u2212nd\n\u2212d\n+\u03b22\n2\nlog2\u03c0 +dn\n2\nlog\u03b2 +d\n2log|Kmm|\n2A \u2212\u03b2d\n2log|Kmm+ \u03b2D| \u2212\u03b2\n2B +\u03b2d\n2Tr(K\u22121\nmmD)\n2Tr(CT\u00b7 (Kmm+ \u03b2D)\u22121\u00b7 C) \u2212 KL\n(3.3)\nwhere\nA =\nn\n?\nn\n?\ni=1\nYiYT\ni\nC =\ni=1\n?Kmi?q(Xi)Yi\nB =\nn\n?\nn\n?\ni=1\n?Kii?q(Xi)\nD =\ni=1\n?KmiKim?q(Xi)\nand\nKL =\nn\n?\ni=1\nKL(q(Xi)||p(Xi))\nwhen the inputs are latent or set to 0 when they are ob-\nserved.\nNotice that the obtained unifying bound is identical to\nthe ones derived in (Titsias, 2009) for the regression case\nand (Titsias & Lawrence, 2010) for the LVM case since\n?Kmi?q(Xi)= Kmifor q(Xi) with variance 0 and mean\nXi. However, the terms are re-parametrised as independent\nsums over the input points \u2013 sums that can be computed on\ndifferent nodes in a network without inter-communication.\nAn in-depth explanation of the different transitions is given\nin the supplementary material sections 3 and 4.\n3.2. The parallel inference\nA parallel inference algorithm can be easily derived based\non this factorisation. Using the Map-Reduce framework\n(Dean & Ghemawat, 2008) we can maintain different sub-\nsets of the inputs and their corresponding outputs on each\nnode in a parallel implementation and distribute the global\nparameters (such as the kernel hyper-parameters and the lo-\ncations of the inducing points) to the nodes, collecting only\nthe partial terms calculated on each node.\nWe denote byG the set of global parameters over which we\nneed to perform optimisation. These include Z (the loca-\ntions of the inducing points), \u03b2 (the observation noise), and\nk (the set of kernel hyper-parameters). Additionally we de-\nnote by Lkthe set of local parameters on each node k that\nneed to be optimised. These include the mean and variance\nfor each output point for the LVM model. First, we send\nto all end-point nodes the global parameters G for them to\ncalculate the partial terms ?Kmi?q(Xi)Yi, ?KmiKim?q(Xi),\n?Kii?q(Xi), YiYT\ntion of these terms is explained in more detail in the sup-\nplementary material section 4. The end-point nodes return\nthese partial terms to the central node (these are m\u00d7m\u00d7q\nmatrices \u2013 constant space complexity for fixed m). The\ncentral node then sends the accumulated terms and partial\nderivatives back to the nodes and performs global optimi-\nsation over G. For the LVM task the nodes then perform\nat the same time local optimisation on Lk, the embedding\nposterior parameters. In total, we have two Map-Reduce\nsteps between the central node and the end-point nodes to\nfollow:\ni, and KL(q(Xi)||p(Xi)). The calcula-\n1. The central node distributes G,\n2. Each end-point node k returns a partial sum of the\nterms A,B,C,D and KL based on Lk,\n3. The central node calculates F, \u2202F (m\u00d7m\u00d7q matri-\nces) and distributes to the end-point nodes,\n4. The central node optimises G; at the same time the\nend-point nodes optimise Lk.\nfor the regression task the third step is not required as well\nas the second part of the forth step. The appendices of the\nsupplementary material contain the derivations of the par-\ntial derivatives with respect to the global variables as well\nas the local ones.\nOptimisation of the global parameters can be done using\nany procedure that utilises the calculated partial derivative\n(such as scaled conjugate gradient (M\u00f8ller, 1993)), and the\noptimisation of the local variables can be carried out by\nparallelising SCG or using local gradient descent. We now\nexplore the developed inference empirically and evaluate\nits properties on a range of tasks."},{"page":5,"text":"Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models\neasydata\nParallel implementation latent space\n4\n3\n2\n1\n0\n1\n2\n3\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n3210123\nDim 1\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nDim 2\n3210123\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nPCA latent space\nFigure 1. A sample from the synthetic data with 100 points (left) and its lower dimensional embedding using GPLVM (centre) and PCA\n(right).\nTime scaling with cores\n100\n101\ncores\n100\n101\n102\ntime \/ iter (s)\nParallel (no overhead)\nParallel (total)\nFigure 2. Running time per iteration for 100K points synthetic\ndataset, asafunctionofavailablecoresonlog-scale. Alsoplotted\nis the total amount of time spent in the computations alone to give\nan indication of the thread-communication overhead.\n0 2040\ndataset size (103)\n6080100\n0\n5\n10\n15\n20\n25\n30\n35\n40\ntime \/ iter (s)\nTime scaling with data\navailable cores\nParallel (total)\nParallel (no overhead)\nGPy\n010 2030 405060\nFigure 3. Time per iteration when scaling the computational\nresources proportionally to dataset size up to 100K points.\nTime per iteration with and without threading overhead are\nshown, as well as GPy running time (a sequential implementa-\ntion of the inference), for comparison.\n4. Experimental Evaluation\nWe assessed the inference on a wide set of experiments\nevaluating its scalability with computation power as well as\nwith data, and explored the numerical stability of the infer-\nence. We further explored the distribution of the load over\nthe different nodes and compared the inference to sequen-\ntial implementations such as GPy (Titsias & Lawrence,\n2010). Finally, we tested the robustness of our parallel in-\nference procedure by dropping-out nodes at random and\nmeasure the resulting log-marginal likelihood. We tested\nthe performance of the GPLVM on large datasets not com-\nmonly handled in the GP community performing dimen-\nsionality reduction and density estimation on the full USPS\ndataset(4Kdatapoints)anddatasetswithhundredsofthou-\nsands of points.\n4.1. Implementation & Setup\nIn the following experiments we used an SE ARD kernel\nover the latent space in order to automatically determine\nthe intrinsic dimensionality of the latent space, as in (Tit-\nsias & Lawrence, 2010). We initialise our latent points\nusing PCA and our inducing points using k-means with\nadded noise. We optimise using scaled conjugate gradi-\nent (M\u00f8ller, 1993) following the original implementation\nby (Titsias & Lawrence, 2010).\nOur experiments were run on a 4 processor, 64-core\nOpteron 6276 machine. One caveat concerning this pro-\ncessor is that there is only one floating point unit (FPU) per\ntwo cores. Often a single thread does not utilise the FPU\nfully, so clever instruction scheduling allows two threads\nto use one FPU without much performance degradation.\nThroughout these experiments we assume a 64-core ma-\nchine, but the FPU sharing could account for some of the\ndiminishing returns observed when using a large number of\nthreads."},{"page":6,"text":"Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models\n321012\n3\n3\nFirst principal latent dimension\n2\n1\n0\n1\n2\n3\n4\nSecond principal latent dimension\nParallel GPLVM\n32\n4\n2\n0\n2\n4\nSecond principal latent dimension\nFigure 4. Latent space produced by the parallel inference (left) and GPy (right) using the oilflow dataset (Titsias & Lawrence, 2010).\n4.2. Scaling with Computation Power\nWe investigate how much inference on a given dataset can\nbe sped up using our parallel implementation, given more\ncomputational resources. In the ideal case we would see\na halving of time given double the resources, but due to\noverheads in distributing the computation it is usually only\npossible to get close.\nWe assess the improvement of the running time of the algo-\nrithm on a simple synthetic dataset of which large amounts\nof data could easily be generated. The dataset was obtained\nby simulating a 1D latent space and transforming this into\n3D observations through linear functions with sines super-\nimposed (see figure 1). 100k points were generated and the\nalgorithm was run using an increasing number of cores and\na 2 dimensional latent space. We measured both the total\nrunning time of the algorithm, including initialisation and\nthreading overheads, and the amount of time spent only in\nthe two Map-Reduce functions. This allows us to assess\nthe impact of our new inference on the running time as a\nwhole, and the effectiveness on just the parts which were\nparallelised.\nFigure 2 shows the improvement of run-times as a function\nof available cores. When initialisation and threading over-\nheads are not considered, we obtain a relation very close\nto the ideal t \u221d c \u00b7 (cores)\u22121. When doubling the num-\nber of cores from 5 to 10 we achieve a factor 1.99 decrease\nin computation time \u2013 very close to ideal. There is some\nhint of diminishing returns when we scale up from 30 to 60\ncores, where we decrease running time by a factor of 1.644.\nWheninitialisationandoverheadsareconsidered, wespeed\nup inference by a factor of 1.96 for a change from 5 to 10\ncores, and by a factor of 1.54 for a change from 30 to 60\ncores.\nFigure 2 also indicates that there is a substantial overhead\nin our implementation. This is not due to fixed costs such\nas the PCA initialisation, but more due to the large costs\nassociated with creating the worker threads in Python that\nrun the map functions in parallel. There is still much per-\nformance to be gained from optimising this.\n4.3. Scaling with Data\nUsing the same setup, we assessed the scaling of the run-\nning time as we increased both the dataset size and com-\nputational resources equally. This answers the question\nof how large a dataset we can handle given an increasing\namount of resources.\nFor a doubling of data, we doubled the number of avail-\nable CPUs. In the ideal case of no constant costs and no\nthreading overheads, computation time should be constant.\nAgain, we measure the total running time of the algorithm,\nand the time spent only in the Map-Reduce functions.\nFigure 3 shows that we are able to effectively utilise the\nextra computational resources. Our total running time, in-\ncluding overheads, takes 67% longer for a dataset scaled\nby 60 times. The Map-Reduce calculations only take 35%\nlonger.\n4.4. Comparison to GPy\nWe also compare the computation time of our inference\nprocedure to the single threaded, but highly optimised\nGPy implementation (see figure 3).\nsignificantly outperform GPy given more computational\nresources. Our parallel inference allows us to run the\nGPLVM on datasets which would simply take too long to\nrun with a single threaded implementation. However, for\nsmall datasets GPy is significantly faster. This is partly due\nto the large overheads discussed in the previous experiment\nand partly due to optimisations in GPy. GPy performs cer-\ntain calculations in native C++, while our implementation\nis written fully in Python.\nWe show that we\nIn addition to scaling experiments, we compare our latent\nspace to GPy, which we use as a reference implementation.\nJust like in the original paper (Titsias & Lawrence, 2010),\nwe used the oil-flow dataset. Both algorithms were run un-"},{"page":7,"text":"Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models\nLoad balancing - 5 cores\n0 100200300 400500\niter\n20.2\n20.4\n20.6\n20.8\n21.0\nThread execution time (s)\nmean\nmin\nmax\n0100200300 400500\niter\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nThread execution time (s)\nLoad balancing - 60 cores\nmean\nmin\nmax\nFigure 5. Load distribution for each iteration. The maximum time spent in a thread is the rate limiting step. Shown are the minimum,\nmean and maximum execution times of all threads when using 5 (left) and 60 (right) cores.\ntil no significant improvement in the marginal likelihood\nwas found.\nThe two latent spaces are shown in figure 4. The latent\nspaces are qualitatively similar, but differ due to a slightly\ndifferent implementation of the optimiser. Like the results\nin (Titsias & Lawrence, 2010) all but one of the ARD pa-\nrameters decrease to zero, giving an effectively 1D latent\nspace.\n4.5. USPS Data\nAs a practical test, we ran our inference scheme on the\nUSPS digits dataset, as used in (Rasmussen & Williams,\n2006). We trained a GPLVM over the complete dataset of\n4649 examples containing all digits from 0-9, using 150\ninducing points. The training was run overnight and was\ncomplete the following day. We managed to reconstruct\ndigits with 34% of their pixels missing (see figure 6). We\nalso ran the same experiment using only 1000 digits to as-\nsess the utility of using more data in the GPLVM. We com-\npared the average mean reconstruction error and found that\nthe larger dataset gives a 5.9% improvement.\nAs this dataset can now be run overnight, a greater number\nof development cycles can be used for fine tuning of the\nmodel.\nFigure 6. Digit reconstruction of the USPS data. Left panel\nshows the input to the model, with dropped pixels, the middle\npannel shows the reconstructed digit, and the right panel shows\nthe ground truth.\n5. Practicality of the Inference\n5.1. Distribution of the Load\nOne of our stated requirements for a practical parallel in-\nference algorithm is an approximately equal distribution of\nthe load on the nodes. This is especially relevant in a Map-\nReduce framework, where the reduce step can only happen\nafter all map computations have finished, so the maximum\nexecution time of one of the threads is the rate limiting step.\nFigure 5 shows the minimum, maximum and average exe-\ncution time of all threads for a range of iterations of a run.\nOn average there is a 3.7% difference between the mean\nand maximum run-time of a thread, suggesting an even dis-\ntribution of the load.\n5.2. Robustness to Node Failure\nOne other desirable characteristic of a parallel inference\nscheme is robustness to failure of nodes. One way of deal-\ning with this would be to load the data to a different node\nand restart the calculation. However, since the speed of one\niteration is limited by the slowest calculation on one of the\nnodes, this could slow down the algorithm by the time it\ntakes to load the intermediate data onto the new node. An\nalternative strategy would be to drop the partial term from\nthe calculation and use a slightly noisy gradient calculation\nin the optimisation for one iteration. Here we investigate\nthe robustness of our inference to this procedure.\nWe ran our parallel inference on the oil-flow dataset using\nthe same setting as above for 500 iterations accumulating\nthe log marginal likelihood as a function of the iteration.\nWe used 10 nodes and simulated failure frequencies of 0%,\n1% and 2% per iteration. The experiment was repeated\n10 times and the log marginal likelihood averaged. Even a\nfailure rate of 1% per iteration for 500 iterations translates\nto a high number of 1 out the 10 nodes failing on average\nevery 10 iterations."},{"page":8,"text":"Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models\n100\n101\n102\n103\niteration\n25000\n20000\n15000\n10000\n5000\n0\nlog marginal likelihood\n0\n0.01\n0.02\nFigure 7. Node failure test, for node failure frequencies of 0%,\n1% and 2% per iteration. Shown is the average log marginal like-\nlihood as a function of the iteration for 500 iterations.\nAs we observe in figure 7 a node failure frequency of 1%\nhurtstotalperformancebydecreasingthelogmarginallike-\nlihood from -1500 to -5000 on average. It seems that a\nhigher failure frequency leads to convergence to worse lo-\ncal optima or a failure of the optimiser, possibly because of\nthe finite differences approximation to the function curva-\nture used by SCG, which might suffer from noisy gradient\nestimations. It is also interesting to note that the embed-\ndings discovered are less pronounced than the ones shown\nin figure 4 but still have only one major latent dimension.\nFor 0% failure rate the ARD parameters are 0.02 for all but\none dimension (0.15), for 1% failure rate the ARD param-\neters are 0.10 for all but one dimension (0.17), and for 2%\nfailure rate the ARD parameters are 0.29 for all but one\ndimension (0.34).\n6. Related Work\nRecent research carried out by Hensman et al. (2013) pro-\nposed stochastic variational inference (SVI, Hoffman et al.\n(2013)) for the problem of scaling up sparse Gaussian pro-\ncess regression. In their research, the variational distri-\nbution over the inducing points was explicitly represented\nand optimisation was performed over the variational distri-\nbution itself instead of using the optimal analytical solu-\ntion, a necessity of the SVI setting that cannot be averted.\nHensman et al. (2013) proposed future directions for re-\nsearch which include the derivation of SVI for GPLVMs,\nand suggested that the proposed SVI for sparse GP regres-\nsion could be carried out in parallel.\nHowever, in order to perform SVI a weaker lower bound\non the log marginal likelihood than the one proposed by\nTitsias (2009) has to be used, and many parameters have to\nbe optimised in addition to the kernel hyper-parameters. In\naddition to that, since the model uses mini-batches, the lo-\ncations of the inducing points cannot be inferred easily and\nhave to be fixed in advance. This is due to the strong cor-\nrelation between the locations and values of the inducing\npoints. Figure 8 demonstrates that a negative log marginal\nlikelihood local minimum for the location of an inducing\npoint when fixing u is not necessarily a minimum as a func-\ntion of u. Furthermore, in the SVI setting many additional\noptimiser parameters have to be introduced and fine-tuned\nby hand (Hensman et al., 2013) to control the step-length\nof different quantities such as the gradient. Additionally,\nmany heuristics are used to decide what quantities to up-\ndate when. For example, the step-lengths do not change for\nthe first epoch, and then change differently for different pa-\nrameters. These difficulties make SVI rather hard to work\nwith.\n7. Conclusions\nWe have scaled the GPLVM and sparse GPs model present-\ning the first unifying parallel inference algorithm which is\nable to process datasets with hundreds of thousands of data\npoints. An extensive set of experiments studying the prop-\nerties of the suggested inference was presented. The infer-\nence was implemented for a multi-core architecture and is\navailable as an open-source package, containing an exten-\nsively documented implementation of the derivations, with\nreferences to the equations presented in the supplementary\nmaterial for explanation.\n\u22121.6\u22121.55\u22121.5\u22121.45\u22121.4\u22121.35\u22121.3\u22121.25\u22121.2\n0\n500\n1000\n1500\nFixed u\nnlml\n\u22121.6\u22121.55\u22121.5\u22121.45 \u22121.4\nz\n\u22121.35\u22121.3\u22121.25\u22121.2\n50.6\n50.7\n50.8\nu(z)\nnlml\nFigure 8. Negative log-likelihood as a function of the location of\na single inducing point z. In the top panel q(u) is fixed and the in\nbottom q(u) is a function z. A negative log marginal likelihood\nlocal minimum for the location of a single inducing point when\nfixing u is not necessarily a minimum as a function of u"},{"page":9,"text":"Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models\nReferences\nArnold, S.F. The theory of linear models and multivariate\nanalysis. Wiley series in probability and mathematical\nstatistics: Probability and mathematical statistics. Wiley,\n1981. ISBN 9780471050650.\nAsuncion, Arthur U, Smyth, Padhraic, and Welling, Max.\nAsynchronous distributed learning of topic models. In\nAdvances in Neural Information Processing Systems, pp.\n81\u201388, 2008.\nBrockwell, A. E. Parallel Markov Chain Monte Carlo sim-\nulation by Pre-Fetching. Journal of Computational and\nGraphical Statistics, 15(1):pp. 246\u2013261, 2006.\n10618600.\nISSN\nDean, Jeffrey and Ghemawat, Sanjay. MapReduce: Sim-\nplified data processing on large clusters. Commun. ACM,\n51(1):107\u2013113, January 2008. ISSN 0001-0782.\nGal, Yarin and Blunsom, Phil. A systematic Bayesian treat-\nment of the IBM alignment models. In Proceedings of\nNAACL-HLT, pp. 969\u2013977, 2013.\nHensman, James, Fusi, Nicolo, and Lawrence, Neil D.\nGaussian processes for big data. 2013.\nHoffman, Matthew D., Blei, David M., Wang, Chong, and\nPaisley, John. Stochastic Variational Inference. JOUR-\nNAL OF MACHINE LEARNING RESEARCH, 14:1303\u2013\n1347, MAY 2013. ISSN 1532-4435.\nLawrence, Neil. Probabilistic non-linear principal compo-\nnent analysis with gaussian process latent variable mod-\nels. TheJournalofMachineLearningResearch, 6:1783\u2013\n1816, 2005.\nM\u00f8ller, Martin Fodslette. A scaled conjugate gradient al-\ngorithm for fast supervised learning. Neural networks, 6\n(4):525\u2013533, 1993.\nQui\u02dc nonero-Candela, Joaquin and Rasmussen, Carl Ed-\nward. A unifying view of sparse approximate gaussian\nprocess regression. Journal of Machine Learning Re-\nsearch, 6:2005, 2005.\nRasmussen, Carl Edward and Williams, Christopher K. I.\nGaussian Processes for Machine Learning (Adaptive\nComputation and Machine Learning). The MIT Press,\n2006. ISBN 026218253X.\nSnelson, Edward and Ghahramani, Zoubin. Sparse gaus-\nsian processes using pseudo-inputs. In Advances in Neu-\nral Information Processing Systems 18, pp. 1257\u20131264.\nMIT press, 2006.\nTitsias, M. K. Variational learning of inducing variables in\nsparse Gaussian processes. Technical report, Technical\nReport, 2009.\nTitsias, Michalis and Lawrence, Neil. Bayesian gaussian\nprocess latent variable model. 2010.\nWilkinson, Darren J. Parallel Bayesian computation. In\nKontoghiorghes, Erricos John (ed.), Handbook of Paral-\nlel Computing and Statistics, volume 184, pp. 477\u2013508.\nChapman and Hall\/CRC, Boca Raton, FL, USA, 2005."}],"widgetId":"rgw29_56ab1ecceaa39"},"id":"rgw29_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=260089482&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw30_56ab1ecceaa39"},"id":"rgw30_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=260089482&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":260089482,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":260089482,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":59288229,"url":"researcher\/59288229_Andreas_Damianou","fullname":"Andreas Damianou","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272454912311306%401441969721269_m\/Andreas_Damianou.png"},{"id":39663468,"url":"researcher\/39663468_Neil_D_Lawrence","fullname":"Neil D. Lawrence","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Sep 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":2,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/281486855_Semi-described_and_semi-supervised_learning_with_Gaussian_processes","usePlainButton":true,"publicationUid":281486855,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/281486855_Semi-described_and_semi-supervised_learning_with_Gaussian_processes","title":"Semi-described and semi-supervised learning with Gaussian processes","displayTitleAsLink":true,"authors":[{"id":59288229,"url":"researcher\/59288229_Andreas_Damianou","fullname":"Andreas Damianou","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272454912311306%401441969721269_m\/Andreas_Damianou.png"},{"id":39663468,"url":"researcher\/39663468_Neil_D_Lawrence","fullname":"Neil D. Lawrence","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Propagating input uncertainty through non-linear Gaussian process (GP)\nmappings is intractable. This hinders the task of training GPs using uncertain\nand partially observed inputs. In this paper we refer to this task as\n\"semi-described learning\". We then introduce a GP framework that solves both,\nthe semi-described and the semi-supervised learning problems (where missing\nvalues occur in the outputs). Auto-regressive state space simulation is also\nrecognised as a special case of semi-described learning. To achieve our goal we\ndevelop variational methods for handling semi-described inputs in GPs, and\ncouple them with algorithms that allow for imputing the missing values while\ntreating the uncertainty in a principled, Bayesian manner. Extensive\nexperiments on simulated and real-world data study the problems of iterative\nforecasting and regression\/classification with missing values. The results\nsuggest that the principled propagation of uncertainty stemming from our\nframework can significantly improve performance in these tasks.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/281486855_Semi-described_and_semi-supervised_learning_with_Gaussian_processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Andreas_Damianou\/publication\/281486855_Semi-described_and_semi-supervised_learning_with_Gaussian_processes\/links\/5631381008ae13bc6c3565ac.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Andreas_Damianou","sourceName":"Andreas Damianou","hasSourceUrl":true},"publicationUid":281486855,"publicationUrl":"publication\/281486855_Semi-described_and_semi-supervised_learning_with_Gaussian_processes","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/281486855_Semi-described_and_semi-supervised_learning_with_Gaussian_processes\/links\/5631381008ae13bc6c3565ac\/smallpreview.png","linkId":"5631381008ae13bc6c3565ac","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=281486855&reference=5631381008ae13bc6c3565ac&eventCode=&origin=publication_list","widgetId":"rgw34_56ab1ecceaa39"},"id":"rgw34_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=281486855&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"5631381008ae13bc6c3565ac","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":260089482,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/281486855_Semi-described_and_semi-supervised_learning_with_Gaussian_processes\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["For this, we develop a variational constraint mechanism which constrains the distribution of the input space given the observed noisy values. This approach is fast, and the whole framework can be incorporated into a parallel inference algorithm [Gal et al., 2014; Dai et al., 2014]. In contrast, Damianou et al. [2011] consider a separate process for modelling the input distribution. "],"widgetId":"rgw35_56ab1ecceaa39"},"id":"rgw35_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw33_56ab1ecceaa39"},"id":"rgw33_56ab1ecceaa39","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=281486855&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":59288229,"url":"researcher\/59288229_Andreas_Damianou","fullname":"Andreas Damianou","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272454912311306%401441969721269_m\/Andreas_Damianou.png"},{"id":39663468,"url":"researcher\/39663468_Neil_D_Lawrence","fullname":"Neil D Lawrence","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Conference Paper","publicationDate":"Jul 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283296484_Semi-described_and_semi-supervised_learning_with_Gaussian_processes","usePlainButton":true,"publicationUid":283296484,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/283296484_Semi-described_and_semi-supervised_learning_with_Gaussian_processes","title":"Semi-described and semi-supervised learning with Gaussian processes","displayTitleAsLink":true,"authors":[{"id":59288229,"url":"researcher\/59288229_Andreas_Damianou","fullname":"Andreas Damianou","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272454912311306%401441969721269_m\/Andreas_Damianou.png"},{"id":39663468,"url":"researcher\/39663468_Neil_D_Lawrence","fullname":"Neil D Lawrence","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["31st Conference on Uncertainty in Artificial Intelligence (UAI), Amsterdam; 07\/2015"],"abstract":"Propagating input uncertainty through non-linear Gaussian process (GP) mappings is intractable. This hinders the task of training GPs using uncertain and partially observed inputs. In this paper we refer to this task as \" semi-described learning \". We then introduce a GP framework that solves both, the semi-described and the semi-supervised learning problems (where missing values occur in the outputs). Auto-regressive state space simulation is also recognised as a special case of semi-described learning. To achieve our goal we develop variational methods for handling semi-described inputs in GPs, and couple them with algorithms that allow for imputing the missing values while treating the uncertainty in a principled, Bayesian manner. Extensive experiments on simulated and real-world data study the problems of iterative forecasting and regres-sion\/classification with missing values. The results suggest that the principled propagation of uncertainty stemming from our framework can significantly improve performance in these tasks.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283296484_Semi-described_and_semi-supervised_learning_with_Gaussian_processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Andreas_Damianou\/publication\/283296484_Semi-described_and_semi-supervised_learning_with_Gaussian_processes\/links\/56313a2308ae13bc6c356667.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Andreas_Damianou","sourceName":"Andreas Damianou","hasSourceUrl":true},"publicationUid":283296484,"publicationUrl":"publication\/283296484_Semi-described_and_semi-supervised_learning_with_Gaussian_processes","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/283296484_Semi-described_and_semi-supervised_learning_with_Gaussian_processes\/links\/56313a2308ae13bc6c356667\/smallpreview.png","linkId":"56313a2308ae13bc6c356667","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=283296484&reference=56313a2308ae13bc6c356667&eventCode=&origin=publication_list","widgetId":"rgw37_56ab1ecceaa39"},"id":"rgw37_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=283296484&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"56313a2308ae13bc6c356667","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":260089482,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283296484_Semi-described_and_semi-supervised_learning_with_Gaussian_processes\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["For this, we develop a variational constraint mechanism which constrains the distribution of the input space given the observed noisy values. This approach is fast, and the whole framework can be incorporated into a parallel inference algorithm [Gal et al., 2014; Dai et al., 2014]. In contrast, Damianou et al. [2011] consider a separate process for modelling the input distribution. "],"widgetId":"rgw38_56ab1ecceaa39"},"id":"rgw38_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw36_56ab1ecceaa39"},"id":"rgw36_56ab1ecceaa39","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=283296484&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithSlurp","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":74747270,"url":"researcher\/74747270_Balaji_Lakshminarayanan","fullname":"Balaji Lakshminarayanan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":43837085,"url":"researcher\/43837085_Daniel_M_Roy","fullname":"Daniel M. Roy","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9164246,"url":"researcher\/9164246_Yee_Whye_Teh","fullname":"Yee Whye Teh","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":false,"isSlurp":true,"isNoText":false,"publicationType":"Article","publicationDate":"Jun 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/278047923_Mondrian_Forests_for_Large-Scale_Regression_when_Uncertainty_Matters","usePlainButton":true,"publicationUid":278047923,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/278047923_Mondrian_Forests_for_Large-Scale_Regression_when_Uncertainty_Matters","title":"Mondrian Forests for Large-Scale Regression when Uncertainty Matters","displayTitleAsLink":true,"authors":[{"id":74747270,"url":"researcher\/74747270_Balaji_Lakshminarayanan","fullname":"Balaji Lakshminarayanan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":43837085,"url":"researcher\/43837085_Daniel_M_Roy","fullname":"Daniel M. Roy","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9164246,"url":"researcher\/9164246_Yee_Whye_Teh","fullname":"Yee Whye Teh","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Decision forests and their variants deliver efficient state-of-the-art\nprediction performance, but many applications, such as probabilistic numerics,\nBayesian optimization, uncertainty quantification, and planning, also demand a\nmeasure of the uncertainty associated with each prediction. Existing approaches\nto measuring the uncertainty of decision forest predictions are known to\nperform poorly, and so Gaussian processes and approximate variants are the\nstandard tools in these application areas. With a goal of providing efficient\nstate-of-the-art predictions together with estimates of uncertainty, we\ndescribe a regression framework using Mondrian forests, an approach to decision\nforests where the underlying random decision trees are modeled as i.i.d.\nMondrian processes and efficient algorithms perform nonparametric Bayesian\ninference within each tree and ordinary model combination across trees. In our\nframework, the underlying nonparametric inference is a Gaussian diffusion over\nthe tree, which results in a multivariate Gaussian calculation for inference in\nlight of finite data that can be carried out efficiently using belief\npropagation. On a synthetic task designed to mimic a typical probabilistic\nnumerical task, we demonstrate that Mondrian forest regression delivers far\nsuperior uncertainty quantification than existing decision forest methods with\nlittle-to-no cost in predictive performance. We then turn to a real-world\nprobabilistic numerics benchmark modeling flight delay, where we compare\nMondrian forests also to large-scale GP approximate methods. Our experiments\ndemonstrate that Mondrian forests can deliver superior uncertainty assessments\nto GPs, as measured by negative predictive log density, with little-to-no loss\nin RMSE performance.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/278047923_Mondrian_Forests_for_Large-Scale_Regression_when_Uncertainty_Matters","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1506.03805","sourceName":"de.arxiv.org","hasSourceUrl":true},"publicationUid":278047923,"publicationUrl":"publication\/278047923_Mondrian_Forests_for_Large-Scale_Regression_when_Uncertainty_Matters","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/278047923_Mondrian_Forests_for_Large-Scale_Regression_when_Uncertainty_Matters\/links\/562987e508ae518e347cc2ea\/smallpreview.png","linkId":"562987e508ae518e347cc2ea","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=278047923&reference=562987e508ae518e347cc2ea&eventCode=&origin=publication_list","widgetId":"rgw40_56ab1ecceaa39"},"id":"rgw40_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=278047923&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":260089482,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/278047923_Mondrian_Forests_for_Large-Scale_Regression_when_Uncertainty_Matters\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["We refer to [5] for a recent summary of large scale Gaussian processes. Hensman et al. [10] and Gal et al. [8] use stochastic variational inference to speed up GPs, building on the variational bound developed by Titsias [24]. Deisenroth and Ng [5] present the robust Bayesian committee machine (rBCM), which combines predictions from experts that operate on subsets of data. "],"widgetId":"rgw41_56ab1ecceaa39"},"id":"rgw41_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw39_56ab1ecceaa39"},"id":"rgw39_56ab1ecceaa39","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=278047923&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":260089482,"publicationLink":"publication\/260089482_Distributed_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable_Models","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw32_56ab1ecceaa39"},"id":"rgw32_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=260089482&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=15","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":15,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw31_56ab1ecceaa39"},"id":"rgw31_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=260089482&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/260089482_Distributed_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable_Models","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1ecceaa39"},"id":"rgw2_56ab1ecceaa39","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":260089482},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=260089482&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1ecceaa39"},"id":"rgw1_56ab1ecceaa39","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"t7SXIoIFY8qSCKAmIKLKhMIZADDhe1dEcgYezsNLgZQsLAvT0V8Z1bWt2SLxd5twCyZslG7WRx5TFsC7CZDI2xlSGUClC2nlrYHw0T7Jz77dCxv13bo2LvKX0UHSrR4AWGASyG4Ez1CV932Zq4X1065DmmkZFZt5oLkw0wQhxPEWm8LSJ2KGlMigm+ljzMxEf3SM4HGWW9YpEZyt5rV36eNsUwtREUP6zgIwB2bJcndKp9XUA21g5ZmsQH4cPvjvsB30K3WrZ\/q6zLRC4Kclj3\/RTj3JR7hntUtvSNfO01Y=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/260089482_Distributed_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable_Models\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models\" \/>\n<meta property=\"og:description\" content=\"The recently developed Bayesian Gaussian process latent variable model\n(GPLVM) is a powerful generative model for discovering low dimensional\nembeddings in linear time complexity. However, modern...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/260089482_Distributed_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable_Models\/links\/02f9df6c0cf28b84cae11e25\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/260089482_Distributed_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable_Models\" \/>\n<meta property=\"rg:id\" content=\"PB:260089482\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Distributed Variational Inference in Sparse Gaussian Process Regression and Latent Variable Models\" \/>\n<meta name=\"citation_author\" content=\"Yarin Gal\" \/>\n<meta name=\"citation_author\" content=\"Mark van der Wilk\" \/>\n<meta name=\"citation_author\" content=\"Carl E. Rasmussen\" \/>\n<meta name=\"citation_publication_date\" content=\"2014\/02\/06\" \/>\n<meta name=\"citation_journal_title\" content=\"Advances in neural information processing systems\" \/>\n<meta name=\"citation_issn\" content=\"1049-5258\" \/>\n<meta name=\"citation_volume\" content=\"4\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/260089482_Distributed_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable_Models\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/260089482_Distributed_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable_Models\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-9aecc9a4-5de8-4133-b114-a19f9e8a02fa","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":1461,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw42_56ab1ecceaa39"},"id":"rgw42_56ab1ecceaa39","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-9aecc9a4-5de8-4133-b114-a19f9e8a02fa", "a49c298333ec0f6fb56fbf7d3a9791d1216aa01a");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-9aecc9a4-5de8-4133-b114-a19f9e8a02fa", "a49c298333ec0f6fb56fbf7d3a9791d1216aa01a");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw43_56ab1ecceaa39"},"id":"rgw43_56ab1ecceaa39","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/260089482_Distributed_Variational_Inference_in_Sparse_Gaussian_Process_Regression_and_Latent_Variable_Models","requestToken":"FpzFFYINX+rq3OSHElJ+cStdzWDXC0bX2WRbgSDoewEDFYqthRUSi3eR3tHMAuGX5a3\/ye+KXj81tlmg9Az6Qz6U4qPD2y7aUkZFDuu+sOsSi3huVNt2TOkFbJKrdJKSVciYo2uVGB4bdtVv9DaY5in4sSa7h\/rsz3vRxWPUgpXm8MlPpH1mZL9rgoxbZVM\/Zgb+nV4ECGScRM6yLJ7k4sm+y9b7r8XQsrRZ3ixeRGpNqa+AFKQZiu7EQZ9pO4rJg2RyQRV5NtdloGfkHvLDxdONlBjL8uZ6utAhxo6dHkQ=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=6W7weEOTqeG6npvhwzeGQQ1e94CJPKZSXIg3H5BRvojpF5YtrpG23bm__KBXnmqB","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjYwMDg5NDgyX0Rpc3RyaWJ1dGVkX1ZhcmlhdGlvbmFsX0luZmVyZW5jZV9pbl9TcGFyc2VfR2F1c3NpYW5fUHJvY2Vzc19SZWdyZXNzaW9uX2FuZF9MYXRlbnRfVmFyaWFibGVfTW9kZWxz","signupCallToAction":"Join for free","widgetId":"rgw45_56ab1ecceaa39"},"id":"rgw45_56ab1ecceaa39","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw44_56ab1ecceaa39"},"id":"rgw44_56ab1ecceaa39","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw46_56ab1ecceaa39"},"id":"rgw46_56ab1ecceaa39","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
