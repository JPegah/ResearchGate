<!DOCTYPE html> <html lang="en" class="" id="rgw23_56aba1862854c"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="pFKWsKnnXzpaSlLEMA8FQ8ry5HCJNLqs33wy+iNaWvMCyr1wnxaprgR+hdLNSMEvjyisXkNmtpwZ5hPrrDX9Z0fomZfjsdIG3hj8DhQwc4p6+PLFhpo+T/vjaR2PdIu4CyfsFTVIE2dWFNUx8ZeOr4XjlzlKpO3Pk3lp1YuMG2XAcNvxjOhDzg/VSB2+Pk4SuDdlGwbykDrcFlaCGyQYgypN/Ibx3gybZZidmgtyffWahr/xOK8lKn2614mlIUAIQL6Pb2sTkRefmyE01AWEYAuJTDvazjVB4Dn4x+GNA4U="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-8742ef76-a8b7-4d35-8b1f-28443e7159d5",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/216457235_A_SEQUENTIAL_DESIGN_FOR_APPROXIMATING_THE_PARETO_FRONT_USING_THE_EXPECTED_PARETO_IMPROVEMENT_FUNCTION" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="A SEQUENTIAL DESIGN FOR APPROXIMATING THE PARETO FRONT USING THE EXPECTED PARETO IMPROVEMENT FUNCTION" />
<meta property="og:description" content="" />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/216457235_A_SEQUENTIAL_DESIGN_FOR_APPROXIMATING_THE_PARETO_FRONT_USING_THE_EXPECTED_PARETO_IMPROVEMENT_FUNCTION/links/0ffc2f280cf2ca93ebadd973/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/216457235_A_SEQUENTIAL_DESIGN_FOR_APPROXIMATING_THE_PARETO_FRONT_USING_THE_EXPECTED_PARETO_IMPROVEMENT_FUNCTION" />
<meta property="rg:id" content="PB:216457235" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="A SEQUENTIAL DESIGN FOR APPROXIMATING THE PARETO FRONT USING THE EXPECTED PARETO IMPROVEMENT FUNCTION" />
<meta name="citation_author" content="Dianne Carrol Bautista" />
<meta name="citation_publication_date" content="2009/01/01" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/216457235_A_SEQUENTIAL_DESIGN_FOR_APPROXIMATING_THE_PARETO_FRONT_USING_THE_EXPECTED_PARETO_IMPROVEMENT_FUNCTION" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/216457235_A_SEQUENTIAL_DESIGN_FOR_APPROXIMATING_THE_PARETO_FRONT_USING_THE_EXPECTED_PARETO_IMPROVEMENT_FUNCTION" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Thesis');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>A SEQUENTIAL DESIGN FOR APPROXIMATING THE PARETO FRONT USING THE EXPECTED PARETO IMPROVEMENT FUNCTION</title>
<meta name="description" content="A SEQUENTIAL DESIGN FOR APPROXIMATING THE PARETO FRONT USING THE EXPECTED PARETO IMPROVEMENT FUNCTION on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56aba1862854c" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56aba1862854c" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56aba1862854c">  <div class="type-label"> Thesis   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=A%20SEQUENTIAL%20DESIGN%20FOR%20APPROXIMATING%20THE%20PARETO%20FRONT%20USING%20THE%20EXPECTED%20PARETO%20IMPROVEMENT%20FUNCTION&rft.date=2009&rft.au=Dianne%20Carrol%20Bautista&rft.genre=thesis"></span> <h1 class="pub-title" itemprop="name">A SEQUENTIAL DESIGN FOR APPROXIMATING THE PARETO FRONT USING THE EXPECTED PARETO IMPROVEMENT FUNCTION</h1> <meta itemprop="headline" content="A SEQUENTIAL DESIGN FOR APPROXIMATING THE PARETO FRONT USING THE EXPECTED PARETO IMPROVEMENT FUNCTION">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/216457235_A_SEQUENTIAL_DESIGN_FOR_APPROXIMATING_THE_PARETO_FRONT_USING_THE_EXPECTED_PARETO_IMPROVEMENT_FUNCTION/links/0ffc2f280cf2ca93ebadd973/smallpreview.png">  <div id="rgw7_56aba1862854c" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56aba1862854c" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Dianne_Bautista" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Dianne Bautista" alt="Dianne Bautista" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Dianne Bautista</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw9_56aba1862854c" data-account-key="Dianne_Bautista">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Dianne_Bautista"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Dianne Bautista" alt="Dianne Bautista" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Dianne_Bautista" class="display-name">Dianne Bautista</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Singapore_Clinical_Research_Institute" title="Singapore Clinical Research Institute">Singapore Clinical Research Institute</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">  <div> Ohio State University </div>           Thesis for: PhD, Advisor:     </div>       <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw22_56aba1862854c">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw21_56aba1862854c"  itemprop="articleBody">  <p>Page 1</p> <p>A SEQUENTIAL DESIGN FOR APPROXIMATING THE<br />PARETO FRONT USING THE EXPECTED PARETO<br />IMPROVEMENT FUNCTION<br />DISSERTATION<br />Presented in Partial Fulfillment of the Requirements for<br />the Degree Doctor of Philosophy in the<br />Graduate School of The Ohio State University<br />By<br />Dianne Carrol Bautista, M.S.<br />* * * * *<br />The Ohio State University<br />2009<br />Dissertation Committee:<br />Prof. Thomas J. Santner, Adviser<br />Prof. Peter F. Craigmile<br />Prof. William I. Notz<br />Approved by<br />Adviser<br />Graduate Program in<br />Statistics</p>  <p>Page 2</p> <p>c ? Copyright by<br />Dianne Carrol Bautista<br />2009</p>  <p>Page 3</p> <p>ABSTRACT<br />This thesis proposes a methodology for the simultaneous optimization of multiple<br />goal functions via computer experiments.<br />Some technical challenges associated with the black box multiobjective problem<br />(MOP) can be enumerated as follows: the presence of conflicting goals imply that<br />more optimization effort is invested to find a good range of solutions that are simul-<br />taneously optimal against these competing criteria; the highly non-linear mapping<br />between the inputs in the design space and the goal functions in objective space<br />may complicate the solution process; and in common with global optimization, the<br />run-time costs of simulation severely limit the number of evaluations that can be<br />made.<br />In view of these, the aim is to compute efficiently and identify a set of good<br />solutions that collectively provide an even coverage of the Pareto front, the set of<br />optimal solutions for a given MOP. The members of the Pareto front comprise the<br />set of compromise solutions from which a decision maker chooses a final design that<br />resonates best with his or her preferences.<br />To reduce the computational overhead, we adopt a surrogate-guided optimization<br />approach. The idea is to build fast approximations that can replace the long-running<br />simulator during optimization while also being reasonably accurate at predicting the<br />latter in the unevaluated feasible design points. This brings about a tremendous gain<br />ii</p>  <p>Page 4</p> <p>in efficiency at the price of extra uncertainty due to the speculative nature of the<br />search for optimal points. Consequently, two competing issues need to be balanced:<br />the global exploratory search for improving surrogate accuracy and local exploitative<br />search for converging rapidly to the optimal points. In a fully sequential optimization<br />design, a key ingredient for achieving this balance is the criterion for selecting the<br />next design point for costly-function evaluation.<br />Among the various surrogates considered so far, none has demonstrated a mech-<br />anism for balancing the tension between local exploitation and global exploration as<br />automatically and as naturally as Gaussian processes have done, as illustrated by<br />the Efficient Global Optimization algorithm for single-objective optimization. We<br />therefore attempt to extend the EI framework to solve the black box MOP.<br />The existing literature on Gaussian process-guided sequential designs for the MOP<br />is scarce on multivariate emulators that effectively incorporate dependencies in the<br />objective function vector. It is also scant on improvement criteria suitably defined<br />for the MOP, that can decisively localize solutions in the vicinity of the Pareto front.<br />Our proposed EmaXalgorithm addresses this lack. We implement a multivariate<br />Gaussian process emulator that guides the sequential search for optimal solutions<br />by means of the expected Pareto improvement function. We considered two models<br />of the covariance structure: a non-separable independence model and a separable<br />dependence model which exemplifies a way of accounting for the covariances within<br />the objective vector.<br />At each stage, the “current best” solutions are first identified. These solutions<br />dominate other feasible solutions in the current experimental design, but do not<br />dominate each other. Then a constrained non-linear program is solved to locate the<br />iii</p>  <p>Page 5</p> <p>design point that presents the greatest potential Pareto improvement to the current<br />non-dominated front.<br />Based on the maximin fitness function, the Pareto improvement is essentially a free<br />upgrade offered by a prospective design point to at least one of the currently identified<br />best designs, in at least one of the objectives. It bears an analogous interpretation to<br />its usage in economics as a change or action in economic management which upgrades<br />the condition of one or more members without worsening the circumstances of the<br />other members. The idea is to progressively add increments of improvements until<br />ideally, a state of Pareto equilibrium is reached where no more free upgrades are<br />possible. At that point, trading-off in the performance criteria happens when moving<br />from one Pareto solution to another.<br />We demonstrated the viability of the EmaX algorithm on five MOP’s with rel-<br />atively low dimensionality and offering various degrees of difficulty in terms of the<br />shape of the Pareto front. Three sequential algorithms were compared: the IGP-PI,<br />IGP-EmaX, and CoH-EmaX. The IGP procedures use a surrogate for the outputs<br />based on the independence model while CoH-EmaX is based on a dependence model.<br />The EmaXcriterion was contrasted with a contending improvement criterion called<br />the probability of improvement or PI.<br />On the five MOP’s tested, the EmaXcriterion generally performed better than<br />the PI in terms of efficiently and evenly covering the Pareto front. The solutions<br />obtained by the EmaXalgorithm were generally more spread out along the Pareto<br />front than the solutions obtained using the PI-directed sequential design which were<br />clustered or biased in some regions of the Pareto front, even as the latter algorithm<br />delivered bigger solution sets.<br />iv</p>  <p>Page 6</p> <p>As regards the gain of modeling dependencies, the EmaXalgorithm based on a<br />Gaussian process with a separable dependence covariance structure fared better than<br />the non-separable independence model in terms of closeness to the best approximated<br />Pareto front, as measured by the binary epsilon factor and in terms of the hypervolume<br />indicator, a measure of the size of the dominated region.<br />This endeavor has definitely inspired areas for future investigation within the scope<br />of Gaussian process-assisted sequential optimization designs. The implementation of<br />the EmaXalgorithm leaves an ample room for improving its algorithmic efficiency<br />and precision. Enhancements in the area of multivariate emulation, particularly in<br />specifying covariance structures that offer alternative ways of accounting for the con-<br />flicting structure in the objective vector can be explored. Recently, a competing infill<br />sampling criterion to the expected improvement, called the conditional minimizer<br />entropy or the CME criterion has been proposed that merits investigation. Finally,<br />Handl and Knowles (2007) advance novel applications of multiobjective optimization<br />methods or Pareto set approaches to solve “conventional” problems that are worth<br />testing.<br />v</p>  <p>Page 7</p> <p>Dedicated to Marco and Francesco Maria.<br />vi</p>  <p>Page 8</p> <p>ACKNOWLEDGMENTS<br />My profound gratitude to Marco for the Love, logistics, and inspiration...I know<br />I will never thank you enough; my family for their constant support and prayers.<br />I thank and commend my advisor, Prof. Thomas Santner for the gift of his person<br />and exemplifying a kind of mentoring that has made a positive impact on me as a<br />parent, a teacher, and statistics consultant. You have opened doors and given me<br />opportunities to outdo myself. I will not forget the time in SAMSI, specially. Thank<br />you, Prof. Santner!<br />I also thank my thesis committee members, Prof. Bill Notz and Prof. Peter<br />Craigmile for their insights and feedback which have improved my thesis significantly;<br />to Prof. Angela Dean for her encouragement, and Prof. Nagaraja for sharing time to<br />help me with a derivation. My heartfelt thanks to all professors who have influenced<br />my thinking, doing, and approach to statistics – Prof. Cressie, Prof. Critchlow, Prof.<br />Goel, Prof. Verducci, Prof. Wolfe, Prof. Fligner, Prof. Lee, Prof. Hans, Prof.<br />Lemeshow, Prof. Peruggia, and Prof. Berliner. Thank you Prof. Stasny for keeping<br />us in top form.<br />I would not have made this without the help of the department’s staff. Thank you<br />Patty Kathy, Lisa, Eric and Brian, and Paul. Thank you all for your assistance...and<br />to LaDonna.<br />vii</p>  <p>Page 9</p> <p>To my friends in the department whose company kept me grounded through the<br />years – Kimberly, Rajib, Namhee, Gang, and Soma; my friends in the Newman Center<br />– Anne-Marie, Valentine, Dinna, Jenny, Magdalena, Socorro, Joan, Fr. Vinny, Fr.<br />Chuck, Fr. Larry and Fr. Dave. You are all special! You also know that you have a<br />place to stay once in Singapore.<br />Above all, I thank GOD, Jesus, and the Holy Spirit, for everything...Their good-<br />ness on the persons, the things, both tangible and intangible, all that was given to<br />me generously and abundantly. This thesis is my gift back to YOU.<br />viii</p>  <p>Page 10</p> <p>VITA<br />February 4, 1972 ........................... Born - Philippines<br />1992 ........................................B.S. Statistics, University of the Philip-<br />pines<br />1996 ........................................M.S.Statistics,Universityof the<br />Philippines<br />1992-1998 .................................. Assistant Professor, School of Statis-<br />tics, University of the Philippines<br />1999-2004 .................................. Assistant Professor, Mathematics and<br />Natural Sciences Division, University<br />of the Philippines in the Visayas, Cebu<br />College<br />2004-2005 .................................. Graduate Research Associate, College<br />of Human Ecology, The Ohio State<br />University<br />2005-2006 .................................. Graduate Teaching Associate, Depart-<br />ment of Statistics, The Ohio State Uni-<br />versity<br />2006-2007 .................................. Graduate Research Associate, Depart-<br />ment of Statistics, The Ohio State Uni-<br />versity<br />2008-present ................................Senior Biostatistician, Singapore Clin-<br />ical Research Institute<br />ix</p>  <p>Page 11</p> <p>PUBLICATIONS<br />Majumdar, A., Debashis, P., and Bautista, D., “A Generalized Convolution Model for<br />Multivariate Non-Stationary Spatial Processes”, accepted for publication in Statistica<br />Sinica, 2009.<br />Craft, S., Delaney, R., Bautista, D., and Serovich, J., “Pregnancy Decisions Among<br />Women with HIV”, AIDS and Behavior, Vol. 11, No. 6, pp. 927-935, 2007.<br />Serovich,J., Mason, T., Bautista, D., and Toviessi, P., “Gay Men’s Report of Re-<br />gret of HIV Disclosure to Family, Friends, and Sex Partners”, AIDS Education and<br />Prevention, Vol. 18, No. 2, pp. 132-138, 2006.<br />Craft, S., Smith, S. A., Serovich, J., and Bautista, D., “Need Fulfillment in the Sexual<br />Relationships of HIV Infected Men Who Have Sex with Men”, AIDS Education and<br />Prevention, Vol. 17, No. 3, pp. 217-226, 2005.<br />FIELDS OF STUDY<br />Major Field: Statistics<br />x</p>  <p>Page 12</p> <p>TABLE OF CONTENTS<br />Page<br />Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ii<br />Dedication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .vi<br />Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .vii<br />Vita . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .ix<br />List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .xiv<br />List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .xvi<br />Chapters:<br />1. Models for Computer Experiments with Multivariate Outputs . . . . . .1<br />1.1<br />1.2<br />1.3<br />Introduction<br />Review of Computer Experiments . . . . . . . . . . . . . . . . . . .<br />Modeling Computer Experiments with Multiple Outputs . . . . . .<br />1.3.1Introduction. . . . . . . . . . . . . . . . . . . . . . . . . .<br />1.3.2Overview of Gaussian process emulation . . . . . . . . . . .<br />1.3.3Models for the Cross-Covariance . . . . . . . . . . . . . . .<br />1.3.4Predicting Output from Computer Experiments . . . . . . .<br />Restricted Maximum Likelihood Estimation of ψ . . . . . . . . . .<br />The Multiobjective Problem (MOP) . . . . . . . . . . . . . . . . .<br />1.5.1Introduction. . . . . . . . . . . . . . . . . . . . . . . . . .<br />1.5.2Pareto Optimality . . . . . . . . . . . . . . . . . . . . . . .<br />1.5.3Review of GP-guided Solution Approaches for the Black box<br />MOP’s . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<br />. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1<br />2<br />9<br />9<br />10<br />14<br />28<br />37<br />42<br />42<br />43<br />1.4<br />1.5<br />51<br />xi</p>  <p>Page 13</p> <p>2.The Expected Pareto Improvement and EmaX Algorithm for Approxi-<br />mating the Pareto Front . . . . . . . . . . . . . . . . . . . . . . . . . . .74<br />2.1<br />2.2<br />2.3<br />The Pareto or Maximin Improvement Function . . . . . . . . . . .<br />The EmaX Algorithm . . . . . . . . . . . . . . . . . . . . . . . . .<br />The Expected Pareto Improvement when m = 2 . . . . . . . . . . .<br />76<br />82<br />85<br />3. Numerical Test Results . . . . . . . . . . . . . . . . . . . . . . . . . . . .99<br />3.1 Quality Assessment of the Pareto Front Approximation . . . . . . .<br />3.1.1The Hypervolume Indicator . . . . . . . . . . . . . . . . . .<br />3.1.2The Binary Epsilon Indicator, I? . . . . . . . . . . . . . . .<br />MOP Test Functions . . . . . . . . . . . . . . . . . . . . . . . . . .<br />3.2.1The wsnl test functions . . . . . . . . . . . . . . . . . . . .<br />3.2.2 The knowles test functions<br />3.2.3The obj4d2 test function . . . . . . . . . . . . . . . . . . . .<br />3.2.4The hedarg2 test functions<br />3.2.5The dltz1a test function . . . . . . . . . . . . . . . . . . . .<br />Numerical Test Results . . . . . . . . . . . . . . . . . . . . . . . .<br />3.3.1Approximating the Pareto Front of the wsnl MOP . . . . .<br />3.3.2Approximating the Pareto Front of the knowles MOP<br />3.3.3 Approximating the Pareto Front of the obj4d2 MOP . . . .<br />3.3.4Approximating the Pareto Front of the hedarg2 MOP<br />3.3.5Approximating the Pareto Front of the dltz1a MOP<br />3.3.6 Summary of Results . . . . . . . . . . . . . . . . . . . . . .<br />100<br />105<br />107<br />108<br />110<br />112<br />113<br />115<br />118<br />119<br />123<br />127<br />130<br />135<br />138<br />141<br />3.2<br />. . . . . . . . . . . . . . . . . .<br />. . . . . . . . . . . . . . . . . .<br />3.3<br />. . .<br />. . .<br />. . . .<br />4. Summary and Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . .143<br />4.1<br />4.2<br />Summary of Performance Assessments . . . . . . . . . . . . . . . .<br />Directions for Future Work . . . . . . . . . . . . . . . . . . . . . .<br />4.2.1 Refinements to the EmaXAlgorithm . . . . . . . . . . . . .<br />4.2.2Enhancing Multivariate Emulation . . . . . . . . . . . . . .<br />4.2.3 Alternative Sequential Sampling Criteria . . . . . . . . . . .<br />4.2.4Extension of the EmaX Algorithm to more general spaces .<br />4.2.5Novel Applications of Pareto set approaches . . . . . . . . .<br />4.2.6 Systematizing the evaluation and comparison of the perfor-<br />mance of the algorithms . . . . . . . . . . . . . . . . . . . .<br />146<br />149<br />150<br />153<br />154<br />154<br />155<br />156<br />Appendices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157<br />Appendices:<br />xii</p>  <p>Page 14</p> <p>A. List of Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .158<br />A.1 Abbreviations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<br />A.2 Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .<br />159<br />161<br />xiii</p>  <p>Page 15</p> <p>LIST OF TABLES<br />Table Page<br />3.1Dominance Relations on Objective Vectors and Approximation Sets . 103<br />3.2<br />The MOP-specific grid resolution used to determine the minimum and max-<br />imum of each Yk(·) and the “best” approximation to the true Pareto front,<br />PG<br />the best approximation was based on the analytical solution. . . . . . . .<br />Ycontaining n(PG<br />Y) approximate solutions. For the dltz1a test function,<br />110<br />3.3<br />The number of solutions in Po<br />Y, o ∈ Ω. . . . . . . . . . . . . . . . . . . . 124<br />3.4<br />A comparison of the size of the dominated region. Note IH(PG<br />Y) = 0.9102<br />125<br />3.5<br />The binary epsilon indicator. Column 2 compares Po<br />reference, PG<br />Y, o ∈ Ω to a common<br />Y), o ?= o?∈ Ω.<br />Y. Columns 3 - 5 shows the I?(Po<br />Y,Po?<br />. . . . .126<br />3.6<br />The binary epsilon indicator. Column 2 compares Po<br />reference, PG<br />Y, o ∈ Ω to a common<br />Y), o ?= o?∈ Ω.<br />Y. Columns 3 - 5 shows the I?(Po<br />Y,Po?<br />. . . . .127<br />3.7<br />The number of solutions in Po<br />Y, o ∈ Ω. . . . . . . . . . . . . . . . . . . . 128<br />3.8<br />A comparison of the size of the dominated region. Note IH(PG<br />Y) = 0.8591<br />128<br />3.9<br />The binary epsilon indicator. Column 2 compares Po<br />reference, PG<br />Y, o ∈ Ω to a common<br />Y), o ?= o?∈ Ω.<br />Y. Columns 3 - 5 shows the I?(Po<br />Y,Po?<br />. . . . .129<br />3.10 The binary epsilon indicator. Column 2 compares Po<br />reference, PG<br />Y, o ∈ Ω to a common<br />Y), o ?= o?∈ Ω.<br />Y. Columns 3 - 5 shows the I?(Po<br />Y,Po?<br />. . . . .130<br />3.11 The number of solutions in Po<br />Y, o ∈ Ω. . . . . . . . . . . . . . . . . . . . 132<br />3.12 A comparison of the size of the dominated region. Note IH(PG<br />Y) = 0.4444.<br />133<br />xiv</p>  <p>Page 16</p> <p>3.13 The binary epsilon indicator. Column 2 compares Po<br />reference, PG<br />Y, o ∈ Ω to a common<br />Y), o ?= o?∈ Ω.<br />Y. Columns 3 - 5 shows the I?(Po<br />Y,Po?<br />. . . . .134<br />3.14 The binary epsilon indicator. Column 2 compares Po<br />reference, PG<br />Y, o ∈ Ω to a common<br />Y), o ?= o?∈ Ω.<br />Y. Columns 3 - 5 shows the I?(Po<br />Y,Po?<br />. . . . .135<br />3.15 The number of solutions in Po<br />Y, o ∈ Ω. . . . . . . . . . . . . . . . . . . . 135<br />3.16 A comparison of the size of the dominated region. Note IH(PG<br />Y) = 0.1942<br />137<br />3.17 The binary epsilon indicator. Column 2 compares Po<br />reference, PG<br />Y, o ∈ Ω to a common<br />Y), o ?= o?∈ Ω.<br />Y. Columns 3 - 5 shows the I?(Po<br />Y,Po?<br />. . . . .138<br />3.18 The binary epsilon indicator. Column 2 compares Po<br />reference, PG<br />Y, o ∈ Ω to a common<br />Y), o ?= o?∈ Ω.<br />Y. Columns 3 - 5 shows the I?(Po<br />Y,Po?<br />. . . . .138<br />3.19 The number of solutions in Po<br />Y, o ∈ Ω. . . . . . . . . . . . . . . . . . . .139<br />3.20 A comparison of the size of the dominated region. Note IH(PG<br />Y) = 0.9995<br />140<br />3.21 The binary epsilon indicator. Column 2 compares Po<br />reference, PG<br />Y, o ∈ Ω to a common<br />Y), o ?= o?∈ Ω.<br />Y. Columns 3 - 5 shows the I?(Po<br />Y,Po?<br />. . . . . 140<br />3.22 The binary epsilon indicator. Column 2 compares Po<br />reference, PG<br />Y, o ∈ Ω to a common<br />Y), o ?= o?∈ Ω.<br />Y. Columns 3 - 5 shows the I?(Po<br />Y,Po?<br />. . . . .141<br />3.23 Overall rank of the algorithms with respect to IH(Po<br />5d, Nmax= 20d. A rank of 1 is the most preferable and 3, least preferable. 141<br />Y), I?(Po<br />Y,PG<br />Y) for n0=<br />3.24 Overall rank of the algorithms with respect to IH(Po<br />10d, Nmax= 20d. A rank of 1 is the most preferable and 3, least preferable. 142<br />Y), I?(Po<br />Y,PG<br />Y) for n0=<br />xv</p>  <p>Page 17</p> <p>LIST OF FIGURES<br />Figure Page<br />1.1<br />Pareto dominance illustrated on a biobjective problem with two variables . 46<br />1.2<br />A geometric interpretation of the Henkenjohann-Kunert improvement func-<br />tion for m = 2 and g = 1: the shortest Euclidean distance of a potential<br />design point xi/ ∈ Dnto the curve DImax= 0.51 . . . . . . . . . . . . . .63<br />1.3<br />The regions of integration for determining the probability of improvement<br />when m = 2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71<br />1.4<br />The region of improvement partitioned according to the number of non-<br />inferior solutions in the current non-dominated front ν that can be dominated. 73<br />2.1Partitioning of IR2by two non-dominated points . . . . . . . . . . . .90<br />3.1<br />Outcomes of three hypothetical algorithms for a bi-objective problem. The<br />approximation sets are denoted as A,B and C; the (discretized) Pareto<br />Front P consists of three objective vectors. Between A,B and C, the fol-<br />lowing dominance relations hold: A ?? C ,A ? C ,B ? C ,A ? A,A ?<br />B ,A ? C ,B ? B ,B ? C ,C ? C ,A ? B ,A ? C , and B ? C . . . . . 104<br />3.2<br />In the left and center panels, A ? B, but the difference is more prominent<br />in the center. In the right panel, A and B are indifferent by definition, but<br />visual inspection suggests that A may be better . . . . . . . . . . . . . . 104<br />3.3<br />The shaded area indicates the size of the dominated zone of the approxi-<br />mation set A = {y1,y2,y3}, measured by the hypervolume indicator, IH.<br />The reference point, r, bounds the calculation of the hypervolume<br />. . . .106<br />3.4<br />The shaded volume indicates the size of the dominated zone of an approxi-<br />mation set with seven points in three dimensions. . . . . . . . . . . . . . 106<br />xvi</p>  <p>Page 18</p> <p>3.5<br />Grid approximated-Pareto optimal set and Pareto front of the wsnl MOP<br />are indicated in bold-faced dots. . . . . . . . . . . . . . . . . . . . . . . 112<br />3.6 The Pareto set and the Pareto front of the knowles MOP are both<br />disconnected. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .113<br />3.7The design space of the test function obj4d2 with an irregularly shaped<br />Pareto set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114<br />3.8Two-dimensional projections of the objective space and Pareto front<br />of test function obj4d2 . . . . . . . . . . . . . . . . . . . . . . . . . . 116<br />3.9Two-dimensional projection of design space and grid-approximated<br />Pareto optimal set for hedarg2 test functions.. . . . . . . . . . . . . 117<br />3.10 Two-dimensional projections of the objective space and Pareto front<br />of the hedarg2 test function. . . . . . . . . . . . . . . . . . . . . . . . 118<br />3.11 Design Space and Pareto optimal set for the dltz1a test functions.<br />Design variable x1is inactive while xj, j = 2,3,4,5,6 are active. Hence<br />the projection of the Pareto set on the plane (x1,xj), j = 2,3,4,5,6<br />is a line, such as shown in the graph of (x1,x2). The two-dimensional<br />projections of a pair of active design variables (xi,xj), i ?= j, is a point,<br />like the graph of x5and x6.. . . . . . . . . . . . . . . . . . . . . . .120<br />3.12 Test function dltz1a: A linear Pareto front in objective space. . . . . 121<br />3.13 A comparison of IGP-PI, IGP-EmaX, CoH-EmaX algorithms for approxi-<br />mating the Pareto Front of the function wsnl. . . . . . . . . . . . . . . .123<br />3.14 A comparison of IGP-PI, IGP-EmaX, CoH-EmaX algorithms for approxi-<br />mating the Pareto Front of the function knowles. . . . . . . . . . . . . .127<br />3.15 Comparison in y1 and y2 of IGP-PI, IGP-EmaX, CoH-EmaX algorithms<br />for approximating the Pareto Front of the obj4d2 test functions.<br />. . . . .131<br />3.16 Comparison in y1 and y4 of IGP-PI, IGP-EmaX, CoH-EmaX algorithms<br />for approximating the Pareto Front of the obj4d2 test functions.<br />. . . . . 132<br />xvii</p>  <p>Page 19</p> <p>3.17 Comparison in y2 and y4 of IGP-PI, IGP-EmaX, CoH-EmaX algorithms<br />for approximating the Pareto Front of the obj4d2 test functions.<br />. . . . .133<br />3.18 Comparison in y1 and y2 of IGP-PI, IGP-EmaX, CoH-EmaX algorithms<br />for approximating the Pareto Front of the hedarg2 test functions . . . . . 136<br />3.19 Comparison in y1 and y2 of IGP-PI, IGP-EmaX, CoH-EmaX algorithms<br />for approximating the Pareto Front of the function dltz1a . . . . . . . . . 139<br />xviii</p>  <p>Page 20</p> <p>CHAPTER 1<br />MODELS FOR COMPUTER EXPERIMENTS WITH<br />MULTIVARIATE OUTPUTS<br />1.1Introduction<br />Due to the growth in computing power and speed, computer modeling and simu-<br />lation of physical processes have become relevant in the study of many scientific and<br />engineered systems. These activities continue to expand in domains that have used<br />it for some time and new application areas are emerging. Scientists and engineers<br />have relied on computer simulation to assess the risk of high consequence systems<br />that cannot be tested physically such as the catastrophic failure of a nuclear power<br />plant, a fire spreading through a high rise building, or a nuclear weapon caught in a<br />transport accident.<br />Natural systems have also been the object of interest. Climate and weather models<br />inform today’s debate on climate change; computer simulations of environmental<br />impact have been made, for example, in the analysis of surface water quality, air toxic<br />emissions, and hazardous waste management, particularly the underground storage<br />of nuclear waste.<br />1</p>  <p>Page 21</p> <p>For engineered systems, safety and reliability concerns have motivated the devel-<br />opment of computer experiments (CE) on existing and proposed systems operating<br />at design, off-design, and failure-mode conditions in accident scenarios. Numerical<br />simulations from computational methods in structural mechanics, heat transfer, fluid<br />mechanics, shock physics are used as virtual prototypes to help produce acceptable<br />designs for systems. Virtual tests are also being carried out to develop new hardware<br />or entire systems to eliminate the causes of failure or at worst, moderate its costs to<br />the environment, the public, an individual, or a company.<br />1.2Review of Computer Experiments<br />The starting point of a computer model is a mathematical model relating the<br />systems’ outputs to a set of inputs. In engineering, such mathematical models are<br />expressed as systems of partial differential equations (PDE) and require computer<br />programs to numerically solve them. Typical programs can contain several thousands<br />of lines of code and can take anywhere from hours to days to evaluate even in the<br />world’s fastest computers. The mathematical model, or the computer program that<br />implements it, is often called a simulator.<br />The inputs and outputs of a simulator can have high dimensionality like hundreds<br />of input and output quantities. Computer codes of this size and complexity can there-<br />fore be thought of as black boxes in that not much is understood about the code’s<br />mapping from inputs to outputs. The mapping is also considered to be deterministic,<br />meaning that after all inputs have been assigned values, the simulator produces a sin-<br />gle value for each output it generates; a re-run at the same input generates duplicate<br />outputs.<br />2</p>  <p>Page 22</p> <p>This is in stark contrast with physical experiments where a response corresponding<br />to a set of treatments (the input variables) is measured along with unknown nuisance<br />factors which can cause variation in the outcome of replicated runs. In particular,<br />the measurement of treatment effects is blurred by noise–random or non-systematic<br />effects and bias–the systematic effects of nuisance or uncontrolled factors. Noise can<br />arise from discrepancies due to unavoidable inaccuracies in the measuring instruments<br />or non-homogeneity in the experimental units. Bias may be caused by an unrecog-<br />nized flaw in the design or philosophy of the experiment that offsets the measurements<br />consistently.<br />The key differentiating concept here is repeatability. The response is deterministic<br />in computer experiments but stochastic in physical experiments.<br />This contrast engenders a distinction in the meaning and management of uncer-<br />tainty as well as in the sorts of errors produced. To manage uncertainties in compar-<br />ing treatments, classical experimental design uses randomization to control for the<br />bias due to unrecognized nuisance factors and replication and blocking to mitigate or<br />model the effects of recognized factors. In computer experiments, noise and bias due<br />to uncontrolled factors does not occur so that randomization, replication, and block-<br />ing of experimental units are not required. Space-filling experimental designs that<br />allow greatest empirical investigation of the input space are more useful in computer<br />experiments than classical fractional factorial and blocked designs.<br />Any model of reality is bound to have deficiencies in the form of errors and uncer-<br />tainties which make predictions imperfect. In computer experiments, the mathemati-<br />cal model is the main source of uncertainty which is why the uncertainty is described<br />as epistemic, the kind that derives from the modeler’s partial knowledge or incomplete<br />3</p>  <p>Page 23</p> <p>understanding of the physical system and its surrounding environment. For example,<br />the modeler is not sure about the correct formulation of the mathematical model,<br />or details about the formulation like the ‘right’ coefficients to plug in, or the correct<br />boundary conditions to solve the relevant system of PDEs. On account of these rea-<br />sons and of the simplifying assumptions made, the computer model’s representation<br />of reality will inevitably be biased. The simulation error or the difference between<br />reality and the computer model’s prediction is the model bias. Non-random errors<br />also exist in the code’s implementation such as conversion from continuum partial<br />differential equations to discrete numerical modeling, e.g., errors expected from the<br />mesh resolution in a finite element solve, the computer’s finite precision arithmetic,<br />and round-off and truncation errors. Since simulation and implementation errors<br />are systematic, both can be reduced. Predictions can be sharpened by incorporat-<br />ing additional information into, or calibrating the computer model. Computations<br />can be made more accurate by reconfiguring default settings or using more powerful<br />computing machines.<br />In the last twenty years, there has been an upsurge in the engineering community’s<br />ability to build finite element models to simulate the behavior of complex processes.<br />Furthermore, the ability to rapidly adjust these simulation models to keep up with<br />design changes has also increased.<br />The net result is that the use of simulation-based optimization to develop new en-<br />gineered systems has increased. There are obstacles however, the very long running<br />times and lack of gradient information in some areas have made it less convenient,<br />despite the steady growth in computing power and speed. For instance, single evalua-<br />tions of finite element analyses to predict a structure’s performance or computational<br />4</p>  <p>Page 24</p> <p>fluid dynamics models to visualize the flow over a body – both cases can take some<br />days up to a week to evaluate. Hence any optimization algorithm applied directly<br />on those codes will be slow. Even if it were possible to optimize the simulator di-<br />rectly, this painful strategy will exhaust the budget in no time, barring any important<br />follow-up studies. Missed deadlines are also likely to delay the whole design process.<br />The rationale behind the surrogate model approach is to construct fast mathemat-<br />ical approximations that can be used in lieu of the long-running simulator to facilitate<br />such ends as input space exploration, optimization, or reliability analysis. The un-<br />derlying assumption is that, once constructed, the surrogate will be many orders of<br />magnitude faster than the simulator while also being reasonably accurate at predict-<br />ing the latter in unsampled design points. Meeting these criteria, such a surrogate can<br />be relied upon to identify active inputs, visualize functional relationships, investigate<br />trade-offs, and possibly provide fresh insights in real time. The simulator can then be<br />called to verify the tentative conclusions drawn from interfacing with the surrogate.<br />For carrying out design optimization, the surrogate model is usually embedded<br />within a sequential design–a strategy for finding optimal points by performing ex-<br />periments successively in a direction of improvement. While the basic idea of the<br />surrogate approach is simple, there are numerous details that need to be sorted out<br />for its successful implementation. In the case of surrogate-guided optimization these<br />are: (1) the selection of a surrogate model, (2) the choice of design points on which<br />to train the surrogate, (3) identification of the best method to exploit the surrogate<br />to identify new and improved designs, and (4) the use of the surrogate to examine<br />the trade-offs among conflicting objectives.<br />5</p>  <p>Page 25</p> <p>The first is a question of design for CE’s. If no prior knowledge exists about the<br />relationship between inputs and outputs, space-filling designs minimize both this un-<br />certainty and the surrogate bias, by spreading points to gather information through-<br />out the design space. Surrogate bias refers to the error of the surrogate in predicting<br />the simulator. The prediction error at any point in input space usually varies pro-<br />portionally to its distance from the closest sampled design point. Points near any<br />sampled point will generally be predicted more accurately than those that are far<br />away. This implies that uneven designs–those having poor coverage, can result in<br />predictors that are inaccurate in sparsely sampled regions of input space.<br />As for the choice of surrogate, a special class of surrogates called emulators have<br />the special advantage of enabling a probabilistic assessment of the uncertainty in-<br />duced by the prediction process. An emulator is a statistical model of a deterministic<br />function which provides, for any design point x in the input space, a predictive dis-<br />tribution for the simulator output at x, say Y (x). The mean of this distribution<br />for a given x is often regarded as the surrogate’s approximation to y(x) and the<br />distribution about the mean is a measure of the uncertainty describing how close the<br />surrogate will be to y(x). Furthermore, because the output is deterministic, it is rea-<br />sonable to seek emulators that interpolate the data to acknowledge the fact that the<br />simulator output is completely known at inputs where the simulator was run. For all<br />other points, the distribution of Y (x) should indicate a mean value that represents<br />a realistic interpolation or extrapolation of all previously acquired data and that the<br />probability distribution around the prediction (mean) should plausibly describe the<br />uncertainty about how Y (x) might interpolate or extrapolate.<br />6</p>  <p>Page 26</p> <p>The kriging predictor is a very popular emulator in the literature of CE’s (Sant-<br />ner, Williams, and Notz 2003). If the simulator is believed to behave smoothly over<br />its domain, then the choice of a GP-based surrogate is well-grounded. Other surro-<br />gates have been used like neural networks, radial basis functions, or support vector<br />machines (Wang and Shan 2007; Ponweiser, Wagner, and Vincze 2008) but none<br />of these approximations explicitly account for the uncertainty in prediction in their<br />implementation.<br />So far we have suggested implementing the surrogate model approach with a<br />design having a space-filling property and a GP model that allows an assessment of<br />the uncertainty in the prediction process, respectively. The last two details in the<br />implementation – how to use the surrogate to identify new and improved designs and<br />to examine the trade-offs among conflicting objectives, are addressed in this thesis.<br />The interest in this thesis is on simulation-based multiobjective optimization. In<br />engineering design, multiple performance targets usually make it likely for some to<br />be conflicting: minimize weight, cost, number of defects, limit a critical temperature,<br />stress, vibration response, maximize reliability, throughput, reconfigurability, agility,<br />or design robustness. Assuming that no prior preferences are given, conflicting as-<br />pirations generally imply that the solution will be non-unique. The task is then to<br />identify the best trade-offs or compromises among the competing objectives.<br />This trend of simulation-based optimization has been energized by competition in<br />a market constantly pressed by a need to decrease lead times and the cost of deliver-<br />ing products to prospective users. More efficient methods that reduce both the time<br />taken to evaluate design concepts and the number of evaluations needed for optimiza-<br />tion are continually being developed. State-of-the-art multiobjective optimization, as<br />7</p>  <p>Page 27</p> <p>evidenced by current commercial software (BOSS/Quattro, iSight, modeFRONTIER,<br />OPTIMUS, and the freely available softwares DAKOTA from Sandia Laboratories,<br />and PISA from the ETH Laboratory in Zurich) have mostly implemented population-<br />based or nature-inspired algorithms such as evolutionary and genetic algorithms, par-<br />ticle swarm or ant colony optimization (Gobbi, Haque, Papalambros, and Mastinu<br />2005; Simpson, Toropov, Balabanov, and Viana 2008; Knowles, Thiele, and Ziztler<br />2006). Some previous studies have demonstrated the efficiency or utility of these al-<br />gorithms in very high-dimensional problems. The main drawback in some is the cpu<br />time required in the processing.<br />This thesis takes a different course by formulating a multiobjective extension of<br />the Efficient Global Optimization algorithm (EGO) of Jones, Schonlau, and Welch<br />(1998). The EGO algorithm is a Gaussian process-guided sequential design that<br />finds the global optimum by locating at each stage, that design point showing the<br />greatest potential of improving on the current best one. The task at hand poses<br />two major challenges: one is developing a multivariate GP emulator that exploits the<br />conflicting structure in the objectives to emulate the multi-output simulator, and two,<br />defining a suitable improvement function for the multiobjective problem to efficiently<br />guide the sequential approximation of the Pareto front–the trade-off curve or surface<br />representing a set of optimal solutions. The current work expands the literature on<br />surrogate-guided black box optimization approaches.<br />The thesis is organized as follows: Chapter 1 introduces the Gaussian process<br />(GP) framework for analyzing computer experiments. In view of the multi-output<br />setting, two GP models are considered – the independence model which ignores the<br />cross-covariances among the objectives and a dependence model that attempts to<br />8</p>  <p>Page 28</p> <p>capture these effects. Chapter 1 also introduces the key concepts of dominance and<br />Pareto optimality in multiobjective optimization. Chapter 2 contains the main con-<br />tribution of this research namely, the expected maximin improvement or the EmaX<br />update criterion embedded in a Gaussian process-guided sequential design. Its ana-<br />lytic expression is derived for the bi-objective case and a Monte Carlo approximation<br />is proposed for the general case. Chapter 3 presents comparisons on the quality of<br />the Pareto front approximation between the EmaX-updated sequential design and<br />another GP-guided sequential design using a competing update criterion called the<br />probability of improvement. In terms of the GP emulator, the efficiency gained by<br />modeling cross-covariance effects is also investigated. The comparisons are made on<br />five multiobjective test problems covering some variety with respect to the difficulties<br />posed. Finally Chapter 4 concludes with a summary of results and directions for<br />future work.<br />1.3 Modeling Computer Experiments with Multiple Outputs<br />1.3.1 Introduction<br />A major disincentive for performing optimization via computer experiments is the<br />runtime cost of the simulator which limits the number of runs that can be made. Our<br />approach seeks to replicate the behavior of the simulator to perform optimization<br />much faster by using an approximate model.<br />In this section, we describe the theoretical and computational aspects of Gaussian<br />process emulation.<br />9</p>  <p>Page 29</p> <p>1.3.2 Overview of Gaussian process emulation<br />We regard the simulator or computer code as a deterministic function y(·) that<br />takes an input vector x ∈ X ⊂ IRd,d ≥ 1 and returns an output<br />y(x) = (y1(x),y2(x),... ,ym(x))T,m &gt; 1.<br />Let<br />Dn= {x1,...,xn},n ≥ 1<br />be a given design at which the computer code is run to produce outputs<br />y(x1),y(x2),... ,y(xn).<br />Consider the problem of predicting y(x∗) for an unseen input vector x∗∈ X on the<br />basis of the data {(xi,y(xi)) : xi∈ Dn}.<br />Koehler and Owen (1996) discuss the Bayesian and frequentist approaches to<br />computer experiments which are differentiated by the way randomness is introduced<br />to measure how much a predicted value, say ˆ y(x0), differs from the true value y(x0)<br />for a new input site x0∈ X.<br />In the Bayesian formulation, y(·) is a realization of a random process. A prior<br />distribution is placed on the space of all functions from X = [0,1]dto IRm. This prior<br />is then combined with information from training data {y(x1),...,y(xn)} to produce<br />a posterior distribution used to predict y(·) at new input sites.<br />A Bayesian approach can be based on a spatial model adapted from Matheron’s<br />kriging model which treats the bias or the systematic departure of the response surface<br />from a linear model, as the realization of a stationary Gaussian process. The classical<br />best linear unbiased predictor or BLUP is used to predict the simulator output at<br />new input sites.<br />10</p>  <p>Page 30</p> <p>A fully Bayesian approach views the simulator y(x),x ∈ X as an unknown func-<br />tion and consequently represents the uncertainty by a prior m-variate Gaussian pro-<br />cess with mean vector µ(x) and positive definite covariance matrix Σ0; the Gaussian<br />process is usually chosen for convenience, as the posterior process given a vector of<br />observed data on a set of input sites, is also a Gaussian process. Here, the posterior<br />process is the object of interest since it is used for prediction. For this reason it is also<br />referred to as the predictive process (Currin, Mitchell, Morris, and Ylvisaker 1991).<br />The actual form of the predictor ˆ y(xo) depends on the specification of a loss function<br />which quantifies the loss incurred when ˆ y(xo) is used to predict y(xo).<br />On the other hand, the frequentist approach introduces randomness via sampling<br />techniques, taking values of x1,x2,...,xnthat are partially determined by pseudo-<br />random number generators. The randomness in the xiis then propagated through<br />to randomness in the y(xi). Its approach to prediction and computer experiments is<br />based on numerical integration.<br />In this thesis, we will adopt a Bayesian perspective. We will combine prior infor-<br />mation describing the mapping from x ∈ X ⊂ IRdto y(x) ∈ IRmwith information<br />from the data to predict the simulator at unevaluated input sites. A definition of an<br />m-variate Gaussian process follows.<br />Definition 1.3.1 Suppose that X is a fixed subset of IRd,d ≥ 1 having positive d-<br />dimensional volume. We say that Y (x) = (Y1(x),Y2(x),...,Ym(x))Tis an m-variate<br />Gaussian process provided that for any n ≥ 1 and any choice of x1,x2,... ,xnin X,<br />the vector (YT(x1),YT(x2),... ,YT(xn))Thas a multivariate normal distribution.<br />The GP is a rich class of functions and we assume that y(·) can be expressed as a<br />draw from a mixture of GP’s. Using GP emulators also allows tractable derivations of<br />11</p>  <p>Page 31</p> <p>the predictive distribution on account of well-established results on multivariate nor-<br />mality. For our purpose, we consider the class of second-order stationary or stationary<br />GP’s.<br />Definition 1.3.2 An m-variate Gaussian process Y (x),x ∈ X is defined to be<br />second-order stationary if E(Y (x)) = µ ∈ IRmfor all x ∈ X, E (Y (x)−µ)(Y (x?)−<br />µ)T= Cov(Y (x),Y (x?)) = K(x − x?) for any given pair of inputs x,x?∈ X. If<br />x = x?, then K(0) = Var(Y (x)) which we denote by Σ0for all x ∈ X.<br />At any untried input vector x0∈ X, we can think of the predictive distribution<br />of Y0= Y (x0) as capturing information about Y0provided in the data {Y (x1) =<br />y(x1),Y (x2) = y(x2),...,Y (xn) = y(xn)} given the designs xi∈ Dn. We organize<br />the data as an n × m matrix Yn,mas follows:<br />Yn,m=<br /><br /><br /><br /><br /><br />Y1(x1)<br />Y1(x2)<br />...<br />Y1(xn) Y2(xn) ... Ym(xn)<br />Y2(x1) ... Ym(x1)<br />Y2(x2) ... Ym(x2)<br />...<br />...<br />...<br /><br /><br /><br /><br />=<br /><br /><br /><br /><br /><br />YT(x1)<br />YT(x2)<br />...<br />YT(xn)<br /><br /><br /><br /><br /><br />(1.1)<br />and let Yn,m= (YT(x1),YT(x2),...,YT(xn))T∈ IRmnbe the data vector. If Y0<br />and {Y (xi) : xi∈ Dn} are dependent quantities, then the conditional distribution<br />[Y0|Yn,m] is the logical choice for the predictive distribution. Accordingly, we first<br />derive the joint distribution [Y0,Yn,m] based on a two-stage hierarchical model.<br />In the first stage, we suppose that Y (x) follows an m-variate Gaussian process<br />(GP) conditional on some parameter vector (µ,Σ0,ψ) denoting respectively the<br />mean, variance, and correlation parameters of the GP. In symbols,<br />Y (x) ∼ GP (µ,K(·;Σ0,ψ))(1.2)<br />12</p>  <p>Page 32</p> <p>In the second stage we assume that a parametrization of (µ,Σ0) follows a non-<br />informative distribution, that is,<br />π(µ,Σ0) ∝ |Σo|−(m+1)<br />2<br />(1.3)<br />where π(D) denotes the distribution of a random quantity D and |A| denotes the<br />determinant of a positive definite matrix A. We treat ψ as known purely for com-<br />putational convenience. The GP specification implies a willingness to assume that<br />y(·) varies continuously over X and that for any given x ∈ X, the uncertainty about<br />y(x) can be described by a multivariate normal distribution having mean and variance<br />functions which depend upon (µ,Σ0,ψ). A constructive approach is taken to form<br />multivariate GP models with a valid cross-covariance structure–an important con-<br />sideration when using GP emulators. The non-informative second-stage distribution<br />reflects the idea that only weak information is available about (µ,Σ0). Given this<br />particular two-stage model, we show that the conditional distribution [Y0|Yn,m,ψ]<br />behaves as a multivariate t process which we then take as our emulator. In reality<br />however, ψ is unknown, so we replace it by some estimator?ψ, e.g. maximum likeli-<br />hood, restricted maximum likelihood, or posterior mode. Plugging in?ψ tends to be<br />optimistic, meaning that it causes an understatement of the prediction uncertainty<br />by disregarding the variability betweenˆψ and its true unknown value. However,<br />“plug-in” prediction is justifiable in situations where varying model parameters over<br />reasonable ranges results in marginal changes in the sizes of the associated prediction<br />variances ((Diggle and Ribeiro 2007)).<br />13</p>  <p>Page 33</p> <p>1.3.3Models for the Cross-Covariance<br />In the first stage of the model we postulate that<br />Y (x) = BTf(x) + W(x)(1.4)<br />where Y (x) = (Y1(x),Y2(x),... ,Ym(x))T∈ IRm,m &gt; 1 is a vector of computer<br />experiment outputs, f(x) = (f1(x),f2(x),... ,fp(x))T∈ IRp,p ≥ 1 is a vector of<br />known regression functions common to all the Yk’s, B = (β1,... ,βm) ∈ IRp,m is<br />a matrix of unknown regression coefficients with each βk= (βk,1,βk,2,... ,βk,p)T∈<br />IRp, and W(x) = (W1(x),... ,Wm(x))T∈ IRmis a zero mean stationary Gaussian<br />process, the key ingredient through which we capture the dependence among the<br />components Y1(x),Y2(x),... ,Ym(x). We write this m-variate process<br />W(x) ∼ GP(0,K(·))<br />where K(x − x?) is an m × m matrix whose (i,j)thelement is Cov(Wi(x),Wj(x?)).<br />Therefore, from Equation [1.4], it follows that<br />Y (x) ∼ GP(BTf(x),K(·)).(1.5)<br />For any arbitrary collection of n input sites x1,x2,... ,xnwe can also organize the<br />multivariate outcomes as an n×m matrix Wn,manalogous to Yn,min Equation (1.1).<br />We can then write the mn × 1 latent vector as Wn,m= (WT(x1),... ,WT(xn))T.<br />Then<br />Wn,m∼ MV N(0,ΣWn,m)<br />14</p>  <p>Page 34</p> <p>where ΣWn,m∈ IRmn,mncan be partitioned into n2blocks of size m × m, with the<br />(u,v)thblock equal to the cross-covariance matrix K(xu− xv) for 1 ≤ u,v ≤ n or<br /><br /><br />Then it follows that<br />ΣWn,m=<br /><br />K(0)K(x1,x2)<br />K(0)<br />...<br />... K(x1− xn)<br />... K(x2− xn)<br />...<br />K(0)<br />K(x2− x1)<br />...<br />K(xn− x1) K(xn− x2) ...<br />...<br /><br /><br /><br />(1.6)<br />vecYT<br />n,m∼ MV N((In⊗ BT)F(n),ΣWn,m) (1.7)<br />where F(n)= (fT(x1),... ,fT(xn))T, Inis the n × n identity matrix, and ⊗ is the<br />Kronecker product operator. If we denote the variance of the data vector Yn,mby<br />ΣY n,mthen<br />ΣY n,m= ΣWn,m<br />(1.8)<br />To implement the first stage, we face the challenge of choosing a valid matrix-<br />valued cross-covariance function, a more demanding task than choosing a real-valued<br />function in the case of univariate GP’s.<br />Definition 1.3.3 An n × n real matrix A is called positive definite if<br />zTAz &gt; 0<br />for all non-zero real vectors z ∈ IRn. The determinant of a positive definite matrix is<br />always positive so a positive definite matrix is always non-singular.<br />We require that for any arbitrary number n and choice of input sites, x1,... ,xn,<br />the resulting ΣWn,mbe symmetric and positive definite. By itself, K(·) need not be<br />symmetric nor positive definite but must satisfy<br />K(x − x?) = KT(x?− x)<br />15</p>  <p>Page 35</p> <p>so that ΣWn,mis symmetric. At x = x?, K(x−x?) must become symmetric and pos-<br />itive definite because it is the covariance matrix of the latent components W1,... ,Wk<br />specific to input vector x.<br />To ensure positive-definiteness, we follow a constructive approach which has been<br />used earlier in the environmental sciences (Ver Hoef and Barry 1998; Wackernagel<br />2003; Finley, Banerjee, Ek, and McRoberts 2008) as well as in the computer modeling<br />literature (Kennedy and O’Hagan 2000; Williams, Lehman, Santner, and Notz 2002).<br />One example is to set<br />W(x) = AZ(x)(1.9)<br />with Z(x) = (Z1(x),... ,Zm(x))T∈ IRmwhere (Z1(·),...,Zm(·)) are mutually in-<br />dependent, zero-centered, unit variance, stationary Gaussian processes and Zk(·) has<br />a correlation function ρk(·; ϑk) with correlation parameter vector ϑk. The matrix<br />A is selected as an m × m non-singular matrix to make sure that ΣY n,mis positive<br />definite. Since<br />Cov(Zk(x),Zj(x?)) = 0<br />for k ?= j for all x and x?are, an immediate consequence is that<br /><br /><br />Cov(Zm(x),Z1(x?)) ... Cov(Zm(x),Zm(x?))<br />= diag{ρ1(x − x?;ϑ1),... ,ρm(x − x?;ϑm)}<br />KZ(x − x?;ϑ) =<br /><br /><br /><br />Cov(Z1(x),Z1(x?))<br />Cov(Z2(x),Z1(x?))<br />...<br />...<br />...<br />...<br />Cov(Z1(x),Zm(x?))<br />Cov(Z2(x),Zm(x?))<br />...<br /><br /><br /><br /><br /><br />(1.10)<br />The cross-covariance matrix KZ(·; ϑ) with<br />ϑ = (ϑ1,... ,ϑm)<br />is positive definite since the diagonals are the values of a set of real-valued positive-<br />definite functions ρ1(·),... ,ρm(·). More precisely, since the Zk’s have unit variance,<br />16</p>  <p>Page 36</p> <p>KZ(·; ϑ) is, in fact, a cross-correlation matrix. Defining Zn,m∈ IRn,manalogously<br />as in Yn,min Equation (1.1), and letting<br />Zn,m= (ZT(x1),... ,ZT(xn))T∈ IRmn,<br />it follows that<br />Zn,m∼ MV N(0,ΣZn,m) (1.11)<br />where<br />ΣZn,m=<br /><br /><br /><br /><br /><br />KZ(0;ϑ)<br />KZ(x2− x1;ϑ)<br />...<br />KZ(xn− x1;ϑ) KZ(xn− x2;ϑ) ...<br />In Equation (1.12), the block diagonals of ΣZn,mare m × m identity matrices<br />since these equal<br />KZ(x1− x2;ϑ) ... KZ(x1− xn;ϑ)<br />KZ(0;ϑ) ... KZ(x2− xn;ϑ)<br />...<br />...<br />...<br />KZ(0;ϑ)<br /><br /><br /><br /><br /><br />(1.12)<br />KZ(0,ϑ) = diag{ρ1(0; ϑ1),...,ρm(0; ϑm)}<br />where ρk(0; ϑk) = 1 for any valid correlation function. The (u,v)thoff-diagonal block<br />is the m × m diagonal matrix<br />KZ(xu− xv; ϑ) = 0m,m<br />with xu− xv?= 0 ∈ IRd. Hence ΣZn,mis an identity matrix of order mn × mn.<br />The implications of defining the latent process W(x) by (1.9) are as follows:<br />1. Recalling that K(xu− xv; ϑ) = Cov(W(xu),W(xv)) ∈ IRm,m where ϑ is<br />the vector of correlation parameters for the process Z(x).Since W(x) =<br />AZ(x),x ∈ X we have that for any xu,xv,<br />K(xu− xv; ϑ) = Cov(W(xu),W(xv))<br />= AKZ(xu− xv; ϑ)AT<br />(1.13)<br />17</p>  <p>Page 37</p> <p>2. By (1), when xu= xv, then K(0; ϑ) = AAT. This implies that<br />A = K<br />1<br />2(0,ϑ)(1.14)<br />Since K(0,ϑ) ∈ IR+<br />outputs Y1,... ,Ymfor the same input, we can identify the non-singular matrix<br />m,mrepresents the variance-covariance matrix of the model<br />transform A as the square root or Cholesky factor of K(0,ϑ). The one-to-<br />one correspondence between a positive definite matrix and its Cholesky factor<br />is known (Harville 1997), so the matrix A is well-defined. Without loss of<br />generality, we take A as a lower triangular matrix. Hence A indirectly models<br />the within-input covariance of the components of the W(x) process.<br />3. The mn × mn cross-covariance matrix of Yn,mcan be expressed in terms of A<br />and ΣZn,mas follows:<br /><br /><br />K(xn− x1;ϑ) ...<br /><br /><br />AKZ(xn− x1;ϑ)AT<br /><br /><br />Υn,1(ϑ) Υn,2(ϑ) ...<br />(In⊗ AT)<br />ΣY n,m<br />=<br /><br /><br /><br /><br />K(0;ϑ)<br />K(x2− x1;ϑ) ... K(x2− xn;ϑ)<br />...<br />... K(x1− xn;ϑ)<br />...<br />...<br />K(0;ϑ)<br /><br /><br /><br /><br /><br />=<br /><br /><br />AKZ(0;ϑ)AT<br />AKZ(x2− x1;ϑ)AT<br />...<br />... AKZ(x1− xn;ϑ)AT<br />... AKZ(x2− xn;ϑ)AT<br />...<br />...AKZ(0;ϑ)AT<br />...<br /><br /><br /><br /><br /><br />= (In⊗ A)<br /><br /><br /><br />Im<br />Υ1,2(ϑ) ... Υ1,n(ϑ)<br />Im<br />... Υ2,n(ϑ)<br />...<br />Υ1,2(ϑ)<br />...<br />...<br />...<br />Im<br /><br /><br /><br /><br />×<br />= (In⊗ A)ΣZn,m(In⊗ AT)(1.15)<br />where<br />Υu,v(ϑ) = diag{ρ1(xu− xv;ϑ1),...,ρm(xu− xv;ϑm)} ∈ IR+<br />m,m<br />18</p>  <p>Page 38</p> <p>The constructive approach here generalizes some of the known cross-covariance<br />structures proposed for GP models of multiple output computer experiments. We<br />now show how particular specifications of the matrix A and<br />KZ(·; ϑ) = diag{ρ1(xu− xv; ϑ1),...,ρm(xu− xv; ϑm)}<br />give rise to these known models. We develop the idea that A accounts for the covari-<br />ances due to the model outputs while KZ(·; ϑ) models the covariances due to the<br />model inputs or the spatial effects which may depend upon the Yk’s.<br />If we specify A to have diagonal structure, the outcome is a model assuming<br />independence among the model outputs Yk. This model is a special case of the more<br />general dependence model which simply provides that A ∈ IR+<br />describe the independence model by letting<br />m,m. Formally, we<br />A = diag(σ1,... ,σm) where σk&gt; 0 ∀ k<br />⇒ AAT<br />= diag(σ2<br />1,... ,σ2<br />m) (1.16)<br />In the second line of Equation (1.16), we recognize the left hand side as the quantity<br />K(0,ϑ), the within-input covariance matrix of Y (x). The diagonal structure for<br />A ignores any dependencies among the components of Y (x), x ∈ X and reduces<br />multivariate emulation into building m GP emulators, one for each Yk, using what<br />has now become a standard recipe first introduced by Sacks, Welch, Mitchell, and<br />Wynn (1989). Such is the model adopted by Keane (2006), Forrester, Sobester, and<br />Keane (2008), and Knowles (2006) who all proposed sequential designs for solving a<br />multiobjective optimization problem using computer experiments.<br />If we assume a common correlation structure for the Z(·) process, that is a com-<br />mon correlation structure for all the Zk’s, the end result is a separable cross-covariance<br />19</p>  <p>Page 39</p> <p>model for the Y (·) process. The separable case is a subset of the more general non-<br />separable model which allows different correlation functions for each Zk. In symbols,<br />a common correlation structure is written as<br />ρk(·; ϑk) = ρ(·; ϑ∗) with ϑk= ϑ∗for all k (1.17)<br />Definition 1.3.4 A separable cross-covariance structure holds for the m-variate Gaus-<br />sian process Y (x), x ∈ X, if for any n ≥ 1, and any collection {x1,... ,xn} of input<br />sites, the mn×mn cross-covariance matrix of Yn,mcan be expressed as the Kronecker<br />product of matrices R(ϑ∗) ∈ IR+<br />n,nand AAT∈ IR+<br />m,m, that is,<br />ΣY n,m= R(ϑ∗) ⊗ AAT<br />(1.18)<br />where<br />R(ϑ∗) =<br /><br /><br /><br /><br /><br />1ρ(x1− x2;ϑ∗)<br />1<br />...<br />... ρ(x1− xn;ϑ∗)<br />... ρ(x2− xn;ϑ∗)<br />...<br />ρ(x1− x2;ϑ∗)<br />...<br />ρ(xn− x1;ϑ∗) ρ(xn− x2;ϑ∗) ...<br />...<br />1<br /><br /><br /><br /><br /><br />To see how a common correlation structure induces separability in the cross-covariance<br />of Yn,m, observe that<br />KZ(xu− xv;ϑ) = Im⊗ ρ(xu− xv;ϑ∗)<br />⇒ ΣZn,m<br />= R(ϑ∗) ⊗ Im<br />(1.19)<br />Substituting the last equality of Equation (1.19) into the expression for the cross-<br />covariance of Yn,min Equation (1.15) we have<br />ΣY n,m<br />= (In⊗ A)ΣZn,m(In⊗ AT)<br />= (In⊗ A)(R(ϑ∗) ⊗ Im)(In⊗ AT)<br />= R(ϑ∗) ⊗ AAT<br />(1.20)<br />20</p>  <p>Page 40</p> <p>Note that the term AATdenotes the within-input covariance matrix. The last equal-<br />ity follows from the distributive law particularly<br />(C ⊗ D)(E ⊗ G) = CE ⊗ DG<br />governing Kronecker products, provided the matrices are compatible with multipli-<br />cation. The separable cross-covariance model is used by Conti, Gosling, Oakley, and<br />O’Hagan (2007) in their work on dynamic emulation and Rougier (2008) in his con-<br />struction of a multivariate emulator called the Outer Product Emulator.<br />What has been demonstrated so far is that, specific forms for A and KZ(·; ϑ)<br />lead to currently proposed cross-covariance structures. To complete the presenta-<br />tion here, we list four possible models and give more detailed discussion in the next<br />paragraphs regarding the model implication for Yn,m.<br />• Model I:<br />A = diag{σ1,... ,σm} and KZ(·; ϑ) = ρ(·; ϑ∗)<br />• Model II:<br />A ∈ IR+<br />m,mand KZ(·; ϑ) = ρ(·; ϑ∗)<br />• Model III:<br />A = diag{σk,... ,σm} and KZ(·; ϑ) = diag{ρ1(·; ϑ1),...,ρm(·; ϑm)}<br />• Model IV:<br />A ∈ IR+<br />m,mand KZ(·; ϑ) = diag{ρ1(·; ϑ1),...,ρm(·; ϑm)}<br />Briefly, Model I is a separable independence model, and its non-separable extension is<br />Model III. Model II is a separable dependence model and its non-separable extension is<br />21</p>  <p>Page 41</p> <p>Model IV. In this thesis, only GP emulators with Models II and III cross-covariance<br />structures were implemented.These are discussed in later subsections where we<br />highlight the important implications. Since only the non-separable independence<br />model is considered, we label the emulator with Model II structure as the independent<br />Gaussian processes model or IGP for short. As for the GP emulator with Model III<br />structure, we name it CoH after Conti, Gosling, Oakley, and O’Hagan (2007) whose<br />work we apply here. Another model used for computer experiments with multiple<br />responses is the Autoregressive Model used by Kennedy and O’Hagan (2000) and<br />Williams, Lehman, Santner, and Notz (2002). This model exemplifies the Model IV<br />structure which we also discuss to underline the variety of covariance of structures<br />allowed by the constructive approach.<br />Model I: A Separable Independence Model<br />The separable independence model is the least complex among the four models for<br />the cross-covariance. It is obtained by imposing special structures on the within-input<br />covariance matrix and on the correlation structure of the elementary processes Zk.<br />Particularly, we assume independence among the model outputs Yk(·) and a common<br />correlation structure for the Zk(·). These are summarized as follows:<br />A = diag{σ1,... ,σm}<br />(1.21)<br />ρk(·;ϑk) = ρ(·;ϑ∗) ∀ k<br />where σk &gt; 0 for all k to ensure positive-definiteness. Here onwards, we use the<br />notation Λ ≡ diag{σ1,... ,σm} to differentiate this special case from the general A<br />notation. In the second line of Equation (1.21), ϑk= ϑ∗for all k, that is, ϑ∗is the<br />common correlation parameter vector. These special structures imply that given any<br />22</p>  <p>Page 42</p> <p>two scalar outputs Yj(·) and Yk(·) for all x,x?∈ X<br />Cov(Yj(x),Yk(x?)) =<br />?0, if j ?= k<br />σ2<br />jρ(x − x?;ϑ∗), if j = k<br />(1.22)<br />or in terms of vector outputs,<br />Cov(Y (x),Y (x?)) =<br />?ΛΛT,x = x?<br />x ?= x?<br />ρ(x − x?;ϑ∗)ΛΛT<br />(1.23)<br />which leads to the following expression for the cross-covariance of Yn,m:<br /><br /><br />ρn,1ΛΛT<br />= R(ϑ∗) ⊗ ΛΛT<br />ΣY n,m<br />=<br /><br /><br /><br />ΛΛT<br />ρ2,1ΛΛT<br />...<br />ρ1,2ΛΛT<br />ΛΛT<br />...<br />ρn,2ΛΛT<br />... ρ1,nΛΛT<br />... ρ2,nΛΛT<br />...<br />... ΛΛT<br />...<br /><br /><br /><br /><br /><br />(1.24)<br />where<br />R(ϑ∗) =<br /><br /><br /><br /><br /><br />1ρ1,2<br />1<br />...<br />... ρ1,n<br />... ρ2,n<br />...<br />ρ2,1<br />...<br />ρn,1 ρn,2 ...<br />...<br />1<br /><br /><br /><br /><br />∈ IR+<br />n,n,<br />and ρu,v= ρ(xu−xv;ϑ∗) for u,v = 1,...,n. The cross-covariance matrix in Equation<br />(1.24) is positive definite by construction and symmetric for as long as ρ(·;ϑ∗) is an<br />even function of its argument, that is ρ(xu− xv) = ρ(xv− xu).<br />Model II: A Non-Separable Independence Model<br />Another way to model multivariate computer output<br />Y (x) = (Y1(x),... ,Ym(x))T<br />is to emulate each component separately which means building m single-output GP<br />emulators, one for each Yk(·). This is equivalent to specifying a non-separable inde-<br />pendence model for the cross-covariance which assumes a diagonal structure for A<br />23</p>  <p>Page 43</p> <p>and allows each elementary process Zk(·) to have its own correlation structure. In<br />symbols,<br />A = Λ = diag{σ1,... ,σm}<br />KZ(·;ϑ) = diag{ρ1(·; ϑ1),...,ρm(·; ϑm)}<br />∈ IR+<br />m,m<br />(1.25)<br />where σk&gt; 0 for all k. For any 1 ≤ j,k ≤ m and for any pair of inputs, x,x?∈ X,<br />this cross-covariance model provides that<br />?0,<br />In terms of the vector outputs Y (x) and Y (x?),<br />?<br />Cov(Yj(x),Yk(x?)) =<br />j ?= k<br />σ2<br />jρj(x − x?;ϑj), j = k<br />(1.26)<br />Cov(Y (x),Y (x?)) =<br />ΛΛT,<br />Λ[⊕m<br />x = x?<br />x ?= x?<br />k=1ρk(x − x?;ϑk)]ΛT<br />(1.27)<br />where<br />[⊕m<br />k=1ρk(x − x?;ϑk)] = diag{ρ1(x − x?;ϑ1),...,ρm(x − x?;ϑm)}<br />Therefore, the covariance matrix of Yn,mis given by<br /><br /><br />ΣY n,m=<br /><br /><br /><br />ΛΛT<br />Λ∆2,1ΛT<br />...<br />Λ∆n,1ΛT<br />Λ∆1,2ΛT<br />ΛΛT<br />...<br />Λ∆n,2ΛT<br />... Λ∆1,nΛT<br />... Λ∆2,nΛT<br />...<br />...ΛΛT<br />...<br /><br /><br /><br /><br /><br />(1.28)<br />where ∆u,v= (⊕m<br />We now compare and contrast the two independence models. The idea of emulat-<br />k=1ρk(xu− xv;ϑk)) ∈ IRm,m.<br />ing the model outputs independently when the components are believed to be related<br />in some way poses potential modeling inefficiencies. Despite this, the independence<br />model continues to be used because of its simplicity and tractability which are its<br />attractive features.<br />24</p>  <p>Page 44</p> <p>If the outputs (Y1(x),Y2(x),...,Ym(x)) represent different quantities with dis-<br />parate measurement scales, the assumption of common correlation structure may<br />cause inadequacies in the emulator. In some cases, Model II is appropriate follow-<br />ing pre-processing of the values by means of a normalization. Alternatively, a non-<br />separable version can be pursued since it accommodates output-specific scale and/or<br />roughness parameters. However, when the model outputs are all measurements of a<br />single quantity, for example, temperatures across time or space or stresses at different<br />points of a body, then a common correlation structure is a rational choice with the<br />added benefit of having fewer parameters to estimate. The separable independence<br />model imposes a more stringent structure than its non-separable extension.<br />Model III: A Separable Dependence Model<br />One of the simple ways to produce a valid cross-covariance function for Y (x),x ∈<br />X is to let ρ(·;ϑ∗) be a valid correlation function of some stationary univariate GP<br />process and AAT∈ IRm,mbe a positive definite matrix. Then set<br />Cov(Y (x),Y (x?)) = ρ(x − x?;ϑ∗) × AAT<br />(1.29)<br />where AATis the within input covariance matrix associated with Y (x), for x ∈ X,<br />and ρ(·;ϑ∗) attenuates the correlation as the separation between x and x?increases.<br />This summarizes the gist of the separable dependence model. In terms of the con-<br />structive approach, we attain this cross-covariance structure by specifying a common<br />correlation structure for the elementary processes Zk(·), and taking a more general<br />form for A ∈ IR+<br />Yj(x) and Yk(x?) for any x,x?∈ X is given by<br />m,m. As a result, the covariance between any pair of model outputs,<br />Cov(Yj(x),Yk(x?)) = ρ(x − x?;ϑ∗) × σjk<br />(1.30)<br />25</p>  <p>Page 45</p> <p>where σjk= σkjis the covariance between Yj(·) and Yk(·) found as the (j,k)thentry<br />of AAT.<br />It follows from Equations (1.29) and (1.30) that the cross-covariance matrix of<br />Yn,mhas the form:<br />ΣY n,m= R(ϑ∗) ⊗ AAT. (1.31)<br />where R(ϑ∗) is defined below Equation (1.24).<br />One important feature of Equation (1.31) is that the cross-covariance factors into<br />a part that could be ascribed purely to the spatial association R(ϑ∗) and a part<br />attributable to the model outputs (AAT). This separability, which is key to tractable<br />statistical modeling, also helps in finding the determinant and inverse of the mn×mn<br />cross-covariance ΣY n,mwhich are:<br />|ΣY n,m| = |R(ϑ∗)|m|AAT|n<br />Σ−1<br />Y n,m<br />= R−1(ϑ∗) ⊗ (AAT)−1.(1.32)<br />Thus the inversion of ΣY n,mdeals with two matrices of more manageable dimensions,<br />m×m and n×n, instead of one that is mn×mn. The overall reduction in complexity<br />however comes with a price–a highly restrictive, possibly unsatisfying structure for<br />the cross-covariance. As Rougier (2007) pointed out, Equation (1.31) implies<br />Cov(Yj(x),Yk(x?))<br />Cov(Yj(x∗),Yk(x∗∗))=<br />ρ(x − x?)<br />ρ(x∗− x∗∗)<br />(1.33)<br />which depends only in the model inputs {x,x?,x∗,x∗∗} for any selected pair of<br />{Yj,Yk}. When x = x?and x∗= x∗∗, the ratio in Equation (1.33) would be a<br />constant for any pair of model outputs.<br />26</p>  <p>Page 46</p> <p>Model IV: A Non-Separable Dependence Model<br />Dropping the restriction of common correlation structure from the separable de-<br />pendence model, the non-separable dependence model is obtained. By far this is the<br />most complex among all the cross-covariance models considered. This is a general-<br />ization of the autoregressive model (AR) used by Kennedy and O’Hagan (2000) and<br />Williams, Lehman, Santner, and Notz (2002). We show here how the cross-covariance<br />induced by the AR specification falls into this class of models.<br />The AR model of order m provides<br />Yj(x) = βT<br />jfj(x) + AjZ(x)(1.34)<br />where βj∈ IRp is the jthcolumn of B, fj ∈ IRp is a vector of known regression<br />functions common to all the Yj’s, Ajis the jthrow of A, the Cholesky factor of the<br />within-input covariance matrix, and Z(x) is an m−dimensional GP with mutually<br />independent components having<br />• E(Zj(x)) = 0,<br />• V ar(Zj(x)) = 1, and<br />• Cov(Zj(x),Zj(x?)) = ρj(x − x?;ϑ∗).<br />The assumption of a shared set of regressors can be relaxed, and in most AR cases<br />used in the literature, the order is usually less than m. From Equation (1.34) it<br />follows that for any pair of outputs 1 ≤ j,k ≤ m and any pair of inputs x,x?∈ X,<br />Cov(Yj(x),Yk(x?)) =<br />m<br />?<br />l=1<br />ajlaklρl(x − x?;ϑl)(1.35)<br />27</p>  <p>Page 47</p> <p>where ajlis the lthentry in Aj= (aj1,... ,ajm) which is the jthrow of A. It follows<br />that<br />Cov(Y (x),Y (x?)) = A[⊕m<br />k=1ρk(x − x?;ϑk)]AT. (1.36)<br />The cross-covariance matrix ΣY n,mis therefore equal to<br /><br /><br />A∆n,1AT<br />ΣY n,m=<br /><br /><br /><br />AAT<br />A∆2,1AT<br />...<br />A∆1,2AT<br />AAT<br />...<br />A∆n,2AT<br />... A∆1,nAT<br />... A∆2,nAT<br />...<br />...AAT<br />...<br /><br /><br /><br /><br /><br />(1.37)<br />where ∆u,vis as defined in Equation (1.28).<br />1.3.4 Predicting Output from Computer Experiments<br />In this section we discuss techniques for predicting the output of the simulator<br />y(x) = (y1(x),y2(x),...,ym(x))Tbased on training data. By prediction we mean the<br />problem of presenting a point guess of the realization of a random quantity (Santner,<br />Williams, and Notz 2003). Our proposed sequential optimization design for the black<br />box multiobjective problem uses the prediction methods discussed here.<br />The following discussion describes optimal predictors under the classical or kriging<br />model and under a fully Bayesian approach.<br />The Kriging Model<br />Suppose X = [0,1]d⊂ IRd is the design space, and let x ∈ X be a scaled d-<br />dimensional vector of input values. The kriging approach views the deterministic<br />function y(·) as a realization of a stochastic process Y (x), x ∈ X and uses a two<br />component model for Y (x): a linear term that models the drift or trend in the<br />response and a stationary Gaussian process that models the systematic departure of<br />the response from the linear model (Sacks, Welch, Mitchell, and Wynn 1989). The<br />28</p>  <p>Page 48</p> <p>model is written as:<br />Y (x) = F(x)β + W(x) (1.38)<br />where<br />F(x) =<br /><br /><br /><br /><br /><br />fT<br />01,p1<br />1(x)01,p2<br />fT<br />2(x) ...<br />...<br />01,p2<br /><br /><br />fkpk(x)<br />...01,pm<br />01,pm<br />...<br />m(x)<br />...<br />...<br />... fT<br /><br /><br /><br /><br />01,p1<br /><br /><br /><br /><br />∈ IRm,p<br />fk(x) =<br /><br /><br /><br />fk1(x)<br />fk2(x)<br />...<br /><br /><br />∈ IRpk<br /><br />βkpk<br />β =<br /><br /><br /><br /><br /><br />β1<br />β2<br />...<br />βm<br /><br /><br /><br /><br />∈ IRp,βk=<br /><br /><br />βk1<br />βk2<br />...<br /><br /><br /><br /><br />∈ IRpk.<br />The full column rank m × p matrix F(·) contains known regression functions for<br />each of the Yk(·), reflected in each row by the pk-vector fT<br />p =<br />k(·) for 1 ≤ k ≤ m, and<br />?m<br />k=1pk is the total number of regressors; β is a p × 1 vector of unknown<br />regression parameters consisting of the pk× 1 vectors βkof regression coefficients<br />of Yk(·), W(x) is an m-variate stationary Gaussian process having the following<br />properties: E(W(x)) = 0m, V ar(W(x)) = Σ0 ∈ IR+<br />a given pair xu,xv ∈ X, Cov(W(xu),W(xv)) = K(xu− xv;Σ0,ψ), and ψ is a<br />vector of correlation parameters. The dimension of ψ depends on the assumed cross-<br />m,mfor all x ∈ X, and for<br />covariance structure, for example, imposing separability results in a ψ vector having<br />a lower dimension compared to the ψ-vector in the more general non-separable case.<br />The kriging model in (1.38) implies that for any x ∈ X, the response Y (x) is<br />m-variate multivariate normal with mean F(x)β and variance K(0;Σ0,ψ) = Σ0.<br />More importantly, the bias or systematic departure of the response surface from the<br />29</p>  <p>Page 49</p> <p>linear model is treated as a realization of an m-variate stationary Gaussian process,<br />W(·).<br />Let our data consist of simulator outputs<br />{Y (x1) = y(x1),Y (x2) = y(x2),...,Y (xn) = y(xn)}<br />evaluated on a set of input sites Dn= {x1,...,xn} ⊂ X. Recall from Subsection<br />1.3.3 our notation for the data matrix:<br /><br /><br />Y1(xn) Y2(xn) ... Ym(xn)<br />Yn,m=<br /><br /><br /><br />Y1(x1)<br />Y1(x2)<br />...<br />Y2(x1) ... Ym(x1)<br />Y2(x2) ... Ym(x2)<br />...<br />...<br />...<br /><br /><br /><br /><br /><br />We now tackle the goal of predicting y(xo) at a new input site xogiven the information<br />from Yn,m. Let Yk≡ (Yk(x1),...,Yk(xn))T∈ IRn, and Wk≡ (Wk(x1),...,Wk(xn))T∈<br />IRn, 1 ≤ k ≤ m. One way to write the data in terms of Equation (1.38) is as follows:<br /><br /><br />Ym<br />0n,p1<br />0n,p2<br />...<br />vecYn,m = F β + vecWn,m<br /><br /><br /><br />Y1<br />Y2<br />...<br /><br /><br /><br /><br /><br />=<br /><br /><br /><br /><br /><br />F1<br />0n,p1<br />...<br />0n,p1<br />F2<br />...<br />... 0n,pm<br />... 0n,pm<br />...<br />...<br />Fm<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />β1<br />β2<br />...<br />βm<br /><br /><br /><br /><br />+<br /><br /><br /><br /><br /><br />W1<br />W2<br />...<br />Wm<br /><br /><br /><br /><br /><br />(1.39)<br />The n×pkmatrix Fkcontains the known regression functions of Ykevaluated on Dn.<br />Since W(·) is a zero-mean process, it follows that<br />E(Wmn) = 0mnand E(W(xo)) = 0m.<br />30</p>  <p>Page 50</p> <p>We write Σwmn≡ V ar(Wmn) and C ≡ cov(Wmn,W(xo)), where<br /><br /><br />Σm1 Σm2 ... Σmm<br /><br /><br />Cov(Wj(xn),Wk(x1)) Cov(Wj(xn),Wk(x2)) ... Cov(Wj(xn),Wk(xn))<br />∈ IRn,n<br /><br /><br />cm1 cm2 ... cmm<br /><br /><br />Cov(Wj(xn),Wk(xo))<br />Σwmn<br />=<br /><br /><br /><br /><br />Σ11<br />Σ21<br />...<br />Σ12<br />Σ22<br />...<br />...<br />...<br />...<br />Σ1m<br />Σ2m<br />...<br /><br /><br /><br /><br />Cov(Wj(x1),Wk(x2))<br />Cov(Wj(x2),Wk(x2))<br />...<br />∈ IRmn,mn<br />Σjk<br />≡<br /><br /><br />Cov(Wj(x1),Wk(x1))<br />Cov(Wj(x2),Wk(x1))<br />...<br />... Cov(Wj(x1),Wk(xn))<br />... Cov(Wj(x2),Wk(xn))<br />...<br />...<br /><br /><br /><br /><br /><br />C=<br /><br /><br /><br /><br />c11<br />c2m<br />...<br />c12<br />c22<br />...<br />...<br />...<br />...<br />c1m<br />c1m<br />...<br /><br /><br /><br /><br />∈ IRmn,m<br /><br />cjk<br />≡<br /><br /><br />Cov(Wj(x1),Wk(xo))<br />Cov(Wj(x2),Wk(xo))<br />...<br /><br /><br /><br />∈ IRn,cj=<br /><br /><br /><br /><br /><br />c1j<br />c2j<br />...<br />cmj<br /><br /><br /><br /><br />∈ IRmn<br />A common optimality criterion for predicting in the univariate instance is the<br />mean squared error or MSE. Consider any m × 1 vector function of the data Yn,m,<br />?Y0≡?Y (x0;Yn,m)<br />to be a predictor of the m × 1 vector Y (x0). We extend the MSE criterion to the<br />multivariate case by defining the mean squared prediction error matrix of?Y0:<br />MSPE(?Y0) = E<br />The expectation in Equation (1.40) is taken over the joint distribution of Y (x0)<br />???Y0− Y (x0)<br />???Y0− Y (x0)<br />?T?<br />∈ IR+<br />m,m<br />(1.40)<br />and vecYn,m, which in the present case is an m(n + 1)-variate multivariate normal<br />distribution. The diagonals of the MSPE(?Y0) matrix equal the univariate MSE’s of<br />?Yk(x0), for 1 ≤ k ≤ m. We define?Y<br />∗(x0) to be the best MSPE predictor of Y (x0)<br />31</p>   </div> <div id="rgw14_56aba1862854c" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw15_56aba1862854c">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw16_56aba1862854c"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://etd.ohiolink.edu/send-pdf.cgi/Bautista%20Dianne%20Carrol%20Tan.pdf?osu1237600537&amp;amp;dl=y" target="_blank" rel="nofollow" class="publication-viewer" title="A SEQUENTIAL DESIGN FOR APPROXIMATING THE PARETO FRONT USING THE EXPECTED PARETO IMPROVEMENT FUNCTION">A SEQUENTIAL DESIGN FOR APPROXIMATING THE PARETO F...</a> </div>  <div class="details">   Available from <a href="http://etd.ohiolink.edu/send-pdf.cgi/Bautista%20Dianne%20Carrol%20Tan.pdf?osu1237600537&amp;amp;dl=y" target="_blank" rel="nofollow">ohiolink.edu</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw18_56aba1862854c" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw19_56aba1862854c">  </ul> </div> </div>   <div id="rgw10_56aba1862854c" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw11_56aba1862854c"> <div> <h5> <a href="publication/266204876_An_Approach_to_Continuous_Approximation_of_Pareto_Front_Using_Geometric_Support_Vector_Regression_for_Multi-Objective_Optimization_of_Fermentation_Process" class="color-inherit ga-similar-publication-title"><span class="publication-title">An Approach to Continuous Approximation of Pareto Front Using Geometric Support Vector Regression for Multi-Objective Optimization of Fermentation Process</span></a>  </h5>  <div class="authors"> <a href="researcher/2055003968_Jiahuan_Wu" class="authors ga-similar-publication-author">Jiahuan Wu</a>, <a href="researcher/71899236_Jianlin_Wang" class="authors ga-similar-publication-author">Jianlin Wang</a>, <a href="researcher/2054992412_Tao_Yu" class="authors ga-similar-publication-author">Tao Yu</a>, <a href="researcher/71969705_Liqiang_Zhao" class="authors ga-similar-publication-author">Liqiang Zhao</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw12_56aba1862854c"> <div> <h5> <a href="publication/283429025_Hierarchical_design_flow_for_heterogenous_systems_using_Pareto_front_interpolation" class="color-inherit ga-similar-publication-title"><span class="publication-title">Hierarchical design flow for heterogenous systems using Pareto front interpolation</span></a>  </h5>  <div class="authors"> <a href="researcher/34690077_L_Labrak" class="authors ga-similar-publication-author">L. Labrak</a>, <a href="researcher/69623165_I_OConnor" class="authors ga-similar-publication-author">I. O&#39;Connor</a>, <a href="researcher/2013934460_F_Frantz" class="authors ga-similar-publication-author">F. Frantz</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw13_56aba1862854c"> <div> <h5> <a href="publication/2629039_Approximating_the_Nondominated_Front_Using_the_Pareto_Archived_Evolution_Strategy" class="color-inherit ga-similar-publication-title"><span class="publication-title">Approximating the Nondominated Front Using the Pareto Archived Evolution Strategy</span></a>  </h5>  <div class="authors"> <a href="researcher/8111149_Joshua_D_Knowles" class="authors ga-similar-publication-author">Joshua D. Knowles</a>, <a href="researcher/6802819_David_W_Corne" class="authors ga-similar-publication-author">David W. Corne</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw24_56aba1862854c" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw25_56aba1862854c">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw26_56aba1862854c" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=ZSt1GRlJ2w1havReamaQJY7NrvD4u3e65ZFbVr4tvnmkCOnhrP9ngJZEr7NtxK97" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="haJc1rC3hBkG2TGmn9VcTBErPAzncRgCAI3NRN/sx60blekD8MMkPecgJCKEvgYFshEU7Vh6nMaLjYjthjZ3DX7YGy6bjNG3sCDB28POtbZj5KRPDRAIJPiecqYP/fEImHRjglIEgVCUNugknx4n+EMQoFSGQ4KNwKE8deFue1Q5yyf7vz7M9biXWwzzCAMCKVaZ+ui+sU58luG4rroSJv/TT+jO4xbTPR1MrahOHgOi+pWZ7+Uk5xykLUl0WXK6Fg5NyZxaBc2Q/3oS4UHcwQv52t87pfVmnEV1LPxyabU="/> <input type="hidden" name="urlAfterLogin" value="publication/216457235_A_SEQUENTIAL_DESIGN_FOR_APPROXIMATING_THE_PARETO_FRONT_USING_THE_EXPECTED_PARETO_IMPROVEMENT_FUNCTION"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjE2NDU3MjM1X0FfU0VRVUVOVElBTF9ERVNJR05fRk9SX0FQUFJPWElNQVRJTkdfVEhFX1BBUkVUT19GUk9OVF9VU0lOR19USEVfRVhQRUNURURfUEFSRVRPX0lNUFJPVkVNRU5UX0ZVTkNUSU9O"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjE2NDU3MjM1X0FfU0VRVUVOVElBTF9ERVNJR05fRk9SX0FQUFJPWElNQVRJTkdfVEhFX1BBUkVUT19GUk9OVF9VU0lOR19USEVfRVhQRUNURURfUEFSRVRPX0lNUFJPVkVNRU5UX0ZVTkNUSU9O"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjE2NDU3MjM1X0FfU0VRVUVOVElBTF9ERVNJR05fRk9SX0FQUFJPWElNQVRJTkdfVEhFX1BBUkVUT19GUk9OVF9VU0lOR19USEVfRVhQRUNURURfUEFSRVRPX0lNUFJPVkVNRU5UX0ZVTkNUSU9O"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw27_56aba1862854c"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 397;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Dianne Bautista","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Dianne_Bautista","institution":"Singapore Clinical Research Institute","institutionUrl":false,"widgetId":"rgw4_56aba1862854c"},"id":"rgw4_56aba1862854c","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=4004088","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56aba1862854c"},"id":"rgw3_56aba1862854c","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=216457235","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":216457235,"title":"A SEQUENTIAL DESIGN FOR APPROXIMATING THE PARETO FRONT USING THE EXPECTED PARETO IMPROVEMENT FUNCTION","journalTitle":false,"journalDetailsTooltip":false,"affiliation":"Ohio State University","type":"Thesis","details":{"thesisInfos":"Thesis for: PhD, Advisor: "},"source":false,"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"A SEQUENTIAL DESIGN FOR APPROXIMATING THE PARETO FRONT USING THE EXPECTED PARETO IMPROVEMENT FUNCTION"},{"key":"rft.date","value":"2009"},{"key":"rft.au","value":"Dianne Carrol Bautista"},{"key":"rft.genre","value":"thesis"}],"widgetId":"rgw6_56aba1862854c"},"id":"rgw6_56aba1862854c","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=216457235","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":216457235,"peopleItems":[{"data":{"authorNameOnPublication":"Dianne Bautista","accountUrl":"profile\/Dianne_Bautista","accountKey":"Dianne_Bautista","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Dianne Bautista","profile":{"professionalInstitution":{"professionalInstitutionName":"Singapore Clinical Research Institute","professionalInstitutionUrl":"institution\/Singapore_Clinical_Research_Institute"}},"professionalInstitutionName":"Singapore Clinical Research Institute","professionalInstitutionUrl":"institution\/Singapore_Clinical_Research_Institute","url":"profile\/Dianne_Bautista","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Dianne_Bautista","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw9_56aba1862854c"},"id":"rgw9_56aba1862854c","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=4004088&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Singapore Clinical Research Institute","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":1,"accountCount":1,"publicationUid":216457235,"widgetId":"rgw8_56aba1862854c"},"id":"rgw8_56aba1862854c","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=4004088&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=1&accountCount=1&publicationUid=216457235","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56aba1862854c"},"id":"rgw7_56aba1862854c","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=216457235&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":null,"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/216457235_A_SEQUENTIAL_DESIGN_FOR_APPROXIMATING_THE_PARETO_FRONT_USING_THE_EXPECTED_PARETO_IMPROVEMENT_FUNCTION\/links\/0ffc2f280cf2ca93ebadd973\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw5_56aba1862854c"},"id":"rgw5_56aba1862854c","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=216457235&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2055003968,"url":"researcher\/2055003968_Jiahuan_Wu","fullname":"Jiahuan Wu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":71899236,"url":"researcher\/71899236_Jianlin_Wang","fullname":"Jianlin Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2054992412,"url":"researcher\/2054992412_Tao_Yu","fullname":"Tao Yu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":71969705,"url":"researcher\/71969705_Liqiang_Zhao","fullname":"Liqiang Zhao","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Oct 2014","journal":"Chinese Journal of Chemical Engineering","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/266204876_An_Approach_to_Continuous_Approximation_of_Pareto_Front_Using_Geometric_Support_Vector_Regression_for_Multi-Objective_Optimization_of_Fermentation_Process","usePlainButton":true,"publicationUid":266204876,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.10","url":"publication\/266204876_An_Approach_to_Continuous_Approximation_of_Pareto_Front_Using_Geometric_Support_Vector_Regression_for_Multi-Objective_Optimization_of_Fermentation_Process","title":"An Approach to Continuous Approximation of Pareto Front Using Geometric Support Vector Regression for Multi-Objective Optimization of Fermentation Process","displayTitleAsLink":true,"authors":[{"id":2055003968,"url":"researcher\/2055003968_Jiahuan_Wu","fullname":"Jiahuan Wu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":71899236,"url":"researcher\/71899236_Jianlin_Wang","fullname":"Jianlin Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2054992412,"url":"researcher\/2054992412_Tao_Yu","fullname":"Tao Yu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":71969705,"url":"researcher\/71969705_Liqiang_Zhao","fullname":"Liqiang Zhao","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Chinese Journal of Chemical Engineering 10\/2014; 22(10). DOI:10.1016\/j.cjche.2014.09.003"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/266204876_An_Approach_to_Continuous_Approximation_of_Pareto_Front_Using_Geometric_Support_Vector_Regression_for_Multi-Objective_Optimization_of_Fermentation_Process","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/266204876_An_Approach_to_Continuous_Approximation_of_Pareto_Front_Using_Geometric_Support_Vector_Regression_for_Multi-Objective_Optimization_of_Fermentation_Process\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw11_56aba1862854c"},"id":"rgw11_56aba1862854c","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=266204876","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":34690077,"url":"researcher\/34690077_L_Labrak","fullname":"L. Labrak","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69623165,"url":"researcher\/69623165_I_OConnor","fullname":"I. O'Connor","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2013934460,"url":"researcher\/2013934460_F_Frantz","fullname":"F. Frantz","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283429025_Hierarchical_design_flow_for_heterogenous_systems_using_Pareto_front_interpolation","usePlainButton":true,"publicationUid":283429025,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/283429025_Hierarchical_design_flow_for_heterogenous_systems_using_Pareto_front_interpolation","title":"Hierarchical design flow for heterogenous systems using Pareto front interpolation","displayTitleAsLink":true,"authors":[{"id":34690077,"url":"researcher\/34690077_L_Labrak","fullname":"L. Labrak","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69623165,"url":"researcher\/69623165_I_OConnor","fullname":"I. O'Connor","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2013934460,"url":"researcher\/2013934460_F_Frantz","fullname":"F. Frantz","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283429025_Hierarchical_design_flow_for_heterogenous_systems_using_Pareto_front_interpolation","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283429025_Hierarchical_design_flow_for_heterogenous_systems_using_Pareto_front_interpolation\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw12_56aba1862854c"},"id":"rgw12_56aba1862854c","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=283429025","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":8111149,"url":"researcher\/8111149_Joshua_D_Knowles","fullname":"Joshua D. Knowles","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":6802819,"url":"researcher\/6802819_David_W_Corne","fullname":"David W. Corne","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2000","journal":"Evolutionary Computation","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/2629039_Approximating_the_Nondominated_Front_Using_the_Pareto_Archived_Evolution_Strategy","usePlainButton":true,"publicationUid":2629039,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.37","url":"publication\/2629039_Approximating_the_Nondominated_Front_Using_the_Pareto_Archived_Evolution_Strategy","title":"Approximating the Nondominated Front Using the Pareto Archived Evolution Strategy","displayTitleAsLink":true,"authors":[{"id":8111149,"url":"researcher\/8111149_Joshua_D_Knowles","fullname":"Joshua D. Knowles","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":6802819,"url":"researcher\/6802819_David_W_Corne","fullname":"David W. Corne","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Evolutionary Computation 01\/2000; 8(2). DOI:10.1162\/106365600568167"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/2629039_Approximating_the_Nondominated_Front_Using_the_Pareto_Archived_Evolution_Strategy","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/2629039_Approximating_the_Nondominated_Front_Using_the_Pareto_Archived_Evolution_Strategy\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw13_56aba1862854c"},"id":"rgw13_56aba1862854c","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=2629039","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw10_56aba1862854c"},"id":"rgw10_56aba1862854c","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=216457235&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":216457235,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":216457235,"publicationType":"thesis","linkId":"0ffc2f280cf2ca93ebadd973","fileName":"A SEQUENTIAL DESIGN FOR APPROXIMATING THE PARETO FRONT USING THE EXPECTED PARETO IMPROVEMENT FUNCTION","fileUrl":"http:\/\/etd.ohiolink.edu\/send-pdf.cgi\/Bautista%20Dianne%20Carrol%20Tan.pdf?osu1237600537&amp;dl=y","name":"ohiolink.edu","nameUrl":"http:\/\/etd.ohiolink.edu\/send-pdf.cgi\/Bautista%20Dianne%20Carrol%20Tan.pdf?osu1237600537&amp;dl=y","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw16_56aba1862854c"},"id":"rgw16_56aba1862854c","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=216457235&linkId=0ffc2f280cf2ca93ebadd973&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw15_56aba1862854c"},"id":"rgw15_56aba1862854c","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=216457235&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":1,"valueFormatted":"1","widgetId":"rgw17_56aba1862854c"},"id":"rgw17_56aba1862854c","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=216457235","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw14_56aba1862854c"},"id":"rgw14_56aba1862854c","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=216457235&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":216457235,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw19_56aba1862854c"},"id":"rgw19_56aba1862854c","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=216457235&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":1,"valueFormatted":"1","widgetId":"rgw20_56aba1862854c"},"id":"rgw20_56aba1862854c","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=216457235","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw18_56aba1862854c"},"id":"rgw18_56aba1862854c","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=216457235&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"A SEQUENTIAL DESIGN FOR APPROXIMATING THE\nPARETO FRONT USING THE EXPECTED PARETO\nIMPROVEMENT FUNCTION\nDISSERTATION\nPresented in Partial Fulfillment of the Requirements for\nthe Degree Doctor of Philosophy in the\nGraduate School of The Ohio State University\nBy\nDianne Carrol Bautista, M.S.\n* * * * *\nThe Ohio State University\n2009\nDissertation Committee:\nProf. Thomas J. Santner, Adviser\nProf. Peter F. Craigmile\nProf. William I. Notz\nApproved by\nAdviser\nGraduate Program in\nStatistics"},{"page":2,"text":"c ? Copyright by\nDianne Carrol Bautista\n2009"},{"page":3,"text":"ABSTRACT\nThis thesis proposes a methodology for the simultaneous optimization of multiple\ngoal functions via computer experiments.\nSome technical challenges associated with the black box multiobjective problem\n(MOP) can be enumerated as follows: the presence of conflicting goals imply that\nmore optimization effort is invested to find a good range of solutions that are simul-\ntaneously optimal against these competing criteria; the highly non-linear mapping\nbetween the inputs in the design space and the goal functions in objective space\nmay complicate the solution process; and in common with global optimization, the\nrun-time costs of simulation severely limit the number of evaluations that can be\nmade.\nIn view of these, the aim is to compute efficiently and identify a set of good\nsolutions that collectively provide an even coverage of the Pareto front, the set of\noptimal solutions for a given MOP. The members of the Pareto front comprise the\nset of compromise solutions from which a decision maker chooses a final design that\nresonates best with his or her preferences.\nTo reduce the computational overhead, we adopt a surrogate-guided optimization\napproach. The idea is to build fast approximations that can replace the long-running\nsimulator during optimization while also being reasonably accurate at predicting the\nlatter in the unevaluated feasible design points. This brings about a tremendous gain\nii"},{"page":4,"text":"in efficiency at the price of extra uncertainty due to the speculative nature of the\nsearch for optimal points. Consequently, two competing issues need to be balanced:\nthe global exploratory search for improving surrogate accuracy and local exploitative\nsearch for converging rapidly to the optimal points. In a fully sequential optimization\ndesign, a key ingredient for achieving this balance is the criterion for selecting the\nnext design point for costly-function evaluation.\nAmong the various surrogates considered so far, none has demonstrated a mech-\nanism for balancing the tension between local exploitation and global exploration as\nautomatically and as naturally as Gaussian processes have done, as illustrated by\nthe Efficient Global Optimization algorithm for single-objective optimization. We\ntherefore attempt to extend the EI framework to solve the black box MOP.\nThe existing literature on Gaussian process-guided sequential designs for the MOP\nis scarce on multivariate emulators that effectively incorporate dependencies in the\nobjective function vector. It is also scant on improvement criteria suitably defined\nfor the MOP, that can decisively localize solutions in the vicinity of the Pareto front.\nOur proposed EmaXalgorithm addresses this lack. We implement a multivariate\nGaussian process emulator that guides the sequential search for optimal solutions\nby means of the expected Pareto improvement function. We considered two models\nof the covariance structure: a non-separable independence model and a separable\ndependence model which exemplifies a way of accounting for the covariances within\nthe objective vector.\nAt each stage, the \u201ccurrent best\u201d solutions are first identified. These solutions\ndominate other feasible solutions in the current experimental design, but do not\ndominate each other. Then a constrained non-linear program is solved to locate the\niii"},{"page":5,"text":"design point that presents the greatest potential Pareto improvement to the current\nnon-dominated front.\nBased on the maximin fitness function, the Pareto improvement is essentially a free\nupgrade offered by a prospective design point to at least one of the currently identified\nbest designs, in at least one of the objectives. It bears an analogous interpretation to\nits usage in economics as a change or action in economic management which upgrades\nthe condition of one or more members without worsening the circumstances of the\nother members. The idea is to progressively add increments of improvements until\nideally, a state of Pareto equilibrium is reached where no more free upgrades are\npossible. At that point, trading-off in the performance criteria happens when moving\nfrom one Pareto solution to another.\nWe demonstrated the viability of the EmaX algorithm on five MOP\u2019s with rel-\natively low dimensionality and offering various degrees of difficulty in terms of the\nshape of the Pareto front. Three sequential algorithms were compared: the IGP-PI,\nIGP-EmaX, and CoH-EmaX. The IGP procedures use a surrogate for the outputs\nbased on the independence model while CoH-EmaX is based on a dependence model.\nThe EmaXcriterion was contrasted with a contending improvement criterion called\nthe probability of improvement or PI.\nOn the five MOP\u2019s tested, the EmaXcriterion generally performed better than\nthe PI in terms of efficiently and evenly covering the Pareto front. The solutions\nobtained by the EmaXalgorithm were generally more spread out along the Pareto\nfront than the solutions obtained using the PI-directed sequential design which were\nclustered or biased in some regions of the Pareto front, even as the latter algorithm\ndelivered bigger solution sets.\niv"},{"page":6,"text":"As regards the gain of modeling dependencies, the EmaXalgorithm based on a\nGaussian process with a separable dependence covariance structure fared better than\nthe non-separable independence model in terms of closeness to the best approximated\nPareto front, as measured by the binary epsilon factor and in terms of the hypervolume\nindicator, a measure of the size of the dominated region.\nThis endeavor has definitely inspired areas for future investigation within the scope\nof Gaussian process-assisted sequential optimization designs. The implementation of\nthe EmaXalgorithm leaves an ample room for improving its algorithmic efficiency\nand precision. Enhancements in the area of multivariate emulation, particularly in\nspecifying covariance structures that offer alternative ways of accounting for the con-\nflicting structure in the objective vector can be explored. Recently, a competing infill\nsampling criterion to the expected improvement, called the conditional minimizer\nentropy or the CME criterion has been proposed that merits investigation. Finally,\nHandl and Knowles (2007) advance novel applications of multiobjective optimization\nmethods or Pareto set approaches to solve \u201cconventional\u201d problems that are worth\ntesting.\nv"},{"page":7,"text":"Dedicated to Marco and Francesco Maria.\nvi"},{"page":8,"text":"ACKNOWLEDGMENTS\nMy profound gratitude to Marco for the Love, logistics, and inspiration...I know\nI will never thank you enough; my family for their constant support and prayers.\nI thank and commend my advisor, Prof. Thomas Santner for the gift of his person\nand exemplifying a kind of mentoring that has made a positive impact on me as a\nparent, a teacher, and statistics consultant. You have opened doors and given me\nopportunities to outdo myself. I will not forget the time in SAMSI, specially. Thank\nyou, Prof. Santner!\nI also thank my thesis committee members, Prof. Bill Notz and Prof. Peter\nCraigmile for their insights and feedback which have improved my thesis significantly;\nto Prof. Angela Dean for her encouragement, and Prof. Nagaraja for sharing time to\nhelp me with a derivation. My heartfelt thanks to all professors who have influenced\nmy thinking, doing, and approach to statistics \u2013 Prof. Cressie, Prof. Critchlow, Prof.\nGoel, Prof. Verducci, Prof. Wolfe, Prof. Fligner, Prof. Lee, Prof. Hans, Prof.\nLemeshow, Prof. Peruggia, and Prof. Berliner. Thank you Prof. Stasny for keeping\nus in top form.\nI would not have made this without the help of the department\u2019s staff. Thank you\nPatty Kathy, Lisa, Eric and Brian, and Paul. Thank you all for your assistance...and\nto LaDonna.\nvii"},{"page":9,"text":"To my friends in the department whose company kept me grounded through the\nyears \u2013 Kimberly, Rajib, Namhee, Gang, and Soma; my friends in the Newman Center\n\u2013 Anne-Marie, Valentine, Dinna, Jenny, Magdalena, Socorro, Joan, Fr. Vinny, Fr.\nChuck, Fr. Larry and Fr. Dave. You are all special! You also know that you have a\nplace to stay once in Singapore.\nAbove all, I thank GOD, Jesus, and the Holy Spirit, for everything...Their good-\nness on the persons, the things, both tangible and intangible, all that was given to\nme generously and abundantly. This thesis is my gift back to YOU.\nviii"},{"page":10,"text":"VITA\nFebruary 4, 1972 ........................... Born - Philippines\n1992 ........................................B.S. Statistics, University of the Philip-\npines\n1996 ........................................M.S.Statistics,Universityof the\nPhilippines\n1992-1998 .................................. Assistant Professor, School of Statis-\ntics, University of the Philippines\n1999-2004 .................................. Assistant Professor, Mathematics and\nNatural Sciences Division, University\nof the Philippines in the Visayas, Cebu\nCollege\n2004-2005 .................................. Graduate Research Associate, College\nof Human Ecology, The Ohio State\nUniversity\n2005-2006 .................................. Graduate Teaching Associate, Depart-\nment of Statistics, The Ohio State Uni-\nversity\n2006-2007 .................................. Graduate Research Associate, Depart-\nment of Statistics, The Ohio State Uni-\nversity\n2008-present ................................Senior Biostatistician, Singapore Clin-\nical Research Institute\nix"},{"page":11,"text":"PUBLICATIONS\nMajumdar, A., Debashis, P., and Bautista, D., \u201cA Generalized Convolution Model for\nMultivariate Non-Stationary Spatial Processes\u201d, accepted for publication in Statistica\nSinica, 2009.\nCraft, S., Delaney, R., Bautista, D., and Serovich, J., \u201cPregnancy Decisions Among\nWomen with HIV\u201d, AIDS and Behavior, Vol. 11, No. 6, pp. 927-935, 2007.\nSerovich,J., Mason, T., Bautista, D., and Toviessi, P., \u201cGay Men\u2019s Report of Re-\ngret of HIV Disclosure to Family, Friends, and Sex Partners\u201d, AIDS Education and\nPrevention, Vol. 18, No. 2, pp. 132-138, 2006.\nCraft, S., Smith, S. A., Serovich, J., and Bautista, D., \u201cNeed Fulfillment in the Sexual\nRelationships of HIV Infected Men Who Have Sex with Men\u201d, AIDS Education and\nPrevention, Vol. 17, No. 3, pp. 217-226, 2005.\nFIELDS OF STUDY\nMajor Field: Statistics\nx"},{"page":12,"text":"TABLE OF CONTENTS\nPage\nAbstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ii\nDedication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .vi\nAcknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .vii\nVita . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .ix\nList of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .xiv\nList of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .xvi\nChapters:\n1. Models for Computer Experiments with Multivariate Outputs . . . . . .1\n1.1\n1.2\n1.3\nIntroduction\nReview of Computer Experiments . . . . . . . . . . . . . . . . . . .\nModeling Computer Experiments with Multiple Outputs . . . . . .\n1.3.1Introduction. . . . . . . . . . . . . . . . . . . . . . . . . .\n1.3.2Overview of Gaussian process emulation . . . . . . . . . . .\n1.3.3Models for the Cross-Covariance . . . . . . . . . . . . . . .\n1.3.4Predicting Output from Computer Experiments . . . . . . .\nRestricted Maximum Likelihood Estimation of \u03c8 . . . . . . . . . .\nThe Multiobjective Problem (MOP) . . . . . . . . . . . . . . . . .\n1.5.1Introduction. . . . . . . . . . . . . . . . . . . . . . . . . .\n1.5.2Pareto Optimality . . . . . . . . . . . . . . . . . . . . . . .\n1.5.3Review of GP-guided Solution Approaches for the Black box\nMOP\u2019s . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1\n2\n9\n9\n10\n14\n28\n37\n42\n42\n43\n1.4\n1.5\n51\nxi"},{"page":13,"text":"2.The Expected Pareto Improvement and EmaX Algorithm for Approxi-\nmating the Pareto Front . . . . . . . . . . . . . . . . . . . . . . . . . . .74\n2.1\n2.2\n2.3\nThe Pareto or Maximin Improvement Function . . . . . . . . . . .\nThe EmaX Algorithm . . . . . . . . . . . . . . . . . . . . . . . . .\nThe Expected Pareto Improvement when m = 2 . . . . . . . . . . .\n76\n82\n85\n3. Numerical Test Results . . . . . . . . . . . . . . . . . . . . . . . . . . . .99\n3.1 Quality Assessment of the Pareto Front Approximation . . . . . . .\n3.1.1The Hypervolume Indicator . . . . . . . . . . . . . . . . . .\n3.1.2The Binary Epsilon Indicator, I? . . . . . . . . . . . . . . .\nMOP Test Functions . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2.1The wsnl test functions . . . . . . . . . . . . . . . . . . . .\n3.2.2 The knowles test functions\n3.2.3The obj4d2 test function . . . . . . . . . . . . . . . . . . . .\n3.2.4The hedarg2 test functions\n3.2.5The dltz1a test function . . . . . . . . . . . . . . . . . . . .\nNumerical Test Results . . . . . . . . . . . . . . . . . . . . . . . .\n3.3.1Approximating the Pareto Front of the wsnl MOP . . . . .\n3.3.2Approximating the Pareto Front of the knowles MOP\n3.3.3 Approximating the Pareto Front of the obj4d2 MOP . . . .\n3.3.4Approximating the Pareto Front of the hedarg2 MOP\n3.3.5Approximating the Pareto Front of the dltz1a MOP\n3.3.6 Summary of Results . . . . . . . . . . . . . . . . . . . . . .\n100\n105\n107\n108\n110\n112\n113\n115\n118\n119\n123\n127\n130\n135\n138\n141\n3.2\n. . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . .\n3.3\n. . .\n. . .\n. . . .\n4. Summary and Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . .143\n4.1\n4.2\nSummary of Performance Assessments . . . . . . . . . . . . . . . .\nDirections for Future Work . . . . . . . . . . . . . . . . . . . . . .\n4.2.1 Refinements to the EmaXAlgorithm . . . . . . . . . . . . .\n4.2.2Enhancing Multivariate Emulation . . . . . . . . . . . . . .\n4.2.3 Alternative Sequential Sampling Criteria . . . . . . . . . . .\n4.2.4Extension of the EmaX Algorithm to more general spaces .\n4.2.5Novel Applications of Pareto set approaches . . . . . . . . .\n4.2.6 Systematizing the evaluation and comparison of the perfor-\nmance of the algorithms . . . . . . . . . . . . . . . . . . . .\n146\n149\n150\n153\n154\n154\n155\n156\nAppendices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\nAppendices:\nxii"},{"page":14,"text":"A. List of Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .158\nA.1 Abbreviations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nA.2 Symbols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n159\n161\nxiii"},{"page":15,"text":"LIST OF TABLES\nTable Page\n3.1Dominance Relations on Objective Vectors and Approximation Sets . 103\n3.2\nThe MOP-specific grid resolution used to determine the minimum and max-\nimum of each Yk(\u00b7) and the \u201cbest\u201d approximation to the true Pareto front,\nPG\nthe best approximation was based on the analytical solution. . . . . . . .\nYcontaining n(PG\nY) approximate solutions. For the dltz1a test function,\n110\n3.3\nThe number of solutions in Po\nY, o \u2208 \u03a9. . . . . . . . . . . . . . . . . . . . 124\n3.4\nA comparison of the size of the dominated region. Note IH(PG\nY) = 0.9102\n125\n3.5\nThe binary epsilon indicator. Column 2 compares Po\nreference, PG\nY, o \u2208 \u03a9 to a common\nY), o ?= o?\u2208 \u03a9.\nY. Columns 3 - 5 shows the I?(Po\nY,Po?\n. . . . .126\n3.6\nThe binary epsilon indicator. Column 2 compares Po\nreference, PG\nY, o \u2208 \u03a9 to a common\nY), o ?= o?\u2208 \u03a9.\nY. Columns 3 - 5 shows the I?(Po\nY,Po?\n. . . . .127\n3.7\nThe number of solutions in Po\nY, o \u2208 \u03a9. . . . . . . . . . . . . . . . . . . . 128\n3.8\nA comparison of the size of the dominated region. Note IH(PG\nY) = 0.8591\n128\n3.9\nThe binary epsilon indicator. Column 2 compares Po\nreference, PG\nY, o \u2208 \u03a9 to a common\nY), o ?= o?\u2208 \u03a9.\nY. Columns 3 - 5 shows the I?(Po\nY,Po?\n. . . . .129\n3.10 The binary epsilon indicator. Column 2 compares Po\nreference, PG\nY, o \u2208 \u03a9 to a common\nY), o ?= o?\u2208 \u03a9.\nY. Columns 3 - 5 shows the I?(Po\nY,Po?\n. . . . .130\n3.11 The number of solutions in Po\nY, o \u2208 \u03a9. . . . . . . . . . . . . . . . . . . . 132\n3.12 A comparison of the size of the dominated region. Note IH(PG\nY) = 0.4444.\n133\nxiv"},{"page":16,"text":"3.13 The binary epsilon indicator. Column 2 compares Po\nreference, PG\nY, o \u2208 \u03a9 to a common\nY), o ?= o?\u2208 \u03a9.\nY. Columns 3 - 5 shows the I?(Po\nY,Po?\n. . . . .134\n3.14 The binary epsilon indicator. Column 2 compares Po\nreference, PG\nY, o \u2208 \u03a9 to a common\nY), o ?= o?\u2208 \u03a9.\nY. Columns 3 - 5 shows the I?(Po\nY,Po?\n. . . . .135\n3.15 The number of solutions in Po\nY, o \u2208 \u03a9. . . . . . . . . . . . . . . . . . . . 135\n3.16 A comparison of the size of the dominated region. Note IH(PG\nY) = 0.1942\n137\n3.17 The binary epsilon indicator. Column 2 compares Po\nreference, PG\nY, o \u2208 \u03a9 to a common\nY), o ?= o?\u2208 \u03a9.\nY. Columns 3 - 5 shows the I?(Po\nY,Po?\n. . . . .138\n3.18 The binary epsilon indicator. Column 2 compares Po\nreference, PG\nY, o \u2208 \u03a9 to a common\nY), o ?= o?\u2208 \u03a9.\nY. Columns 3 - 5 shows the I?(Po\nY,Po?\n. . . . .138\n3.19 The number of solutions in Po\nY, o \u2208 \u03a9. . . . . . . . . . . . . . . . . . . .139\n3.20 A comparison of the size of the dominated region. Note IH(PG\nY) = 0.9995\n140\n3.21 The binary epsilon indicator. Column 2 compares Po\nreference, PG\nY, o \u2208 \u03a9 to a common\nY), o ?= o?\u2208 \u03a9.\nY. Columns 3 - 5 shows the I?(Po\nY,Po?\n. . . . . 140\n3.22 The binary epsilon indicator. Column 2 compares Po\nreference, PG\nY, o \u2208 \u03a9 to a common\nY), o ?= o?\u2208 \u03a9.\nY. Columns 3 - 5 shows the I?(Po\nY,Po?\n. . . . .141\n3.23 Overall rank of the algorithms with respect to IH(Po\n5d, Nmax= 20d. A rank of 1 is the most preferable and 3, least preferable. 141\nY), I?(Po\nY,PG\nY) for n0=\n3.24 Overall rank of the algorithms with respect to IH(Po\n10d, Nmax= 20d. A rank of 1 is the most preferable and 3, least preferable. 142\nY), I?(Po\nY,PG\nY) for n0=\nxv"},{"page":17,"text":"LIST OF FIGURES\nFigure Page\n1.1\nPareto dominance illustrated on a biobjective problem with two variables . 46\n1.2\nA geometric interpretation of the Henkenjohann-Kunert improvement func-\ntion for m = 2 and g = 1: the shortest Euclidean distance of a potential\ndesign point xi\/ \u2208 Dnto the curve DImax= 0.51 . . . . . . . . . . . . . .63\n1.3\nThe regions of integration for determining the probability of improvement\nwhen m = 2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n1.4\nThe region of improvement partitioned according to the number of non-\ninferior solutions in the current non-dominated front \u03bd that can be dominated. 73\n2.1Partitioning of IR2by two non-dominated points . . . . . . . . . . . .90\n3.1\nOutcomes of three hypothetical algorithms for a bi-objective problem. The\napproximation sets are denoted as A,B and C; the (discretized) Pareto\nFront P consists of three objective vectors. Between A,B and C, the fol-\nlowing dominance relations hold: A ?? C ,A ? C ,B ? C ,A ? A,A ?\nB ,A ? C ,B ? B ,B ? C ,C ? C ,A ? B ,A ? C , and B ? C . . . . . 104\n3.2\nIn the left and center panels, A ? B, but the difference is more prominent\nin the center. In the right panel, A and B are indifferent by definition, but\nvisual inspection suggests that A may be better . . . . . . . . . . . . . . 104\n3.3\nThe shaded area indicates the size of the dominated zone of the approxi-\nmation set A = {y1,y2,y3}, measured by the hypervolume indicator, IH.\nThe reference point, r, bounds the calculation of the hypervolume\n. . . .106\n3.4\nThe shaded volume indicates the size of the dominated zone of an approxi-\nmation set with seven points in three dimensions. . . . . . . . . . . . . . 106\nxvi"},{"page":18,"text":"3.5\nGrid approximated-Pareto optimal set and Pareto front of the wsnl MOP\nare indicated in bold-faced dots. . . . . . . . . . . . . . . . . . . . . . . 112\n3.6 The Pareto set and the Pareto front of the knowles MOP are both\ndisconnected. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .113\n3.7The design space of the test function obj4d2 with an irregularly shaped\nPareto set. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n3.8Two-dimensional projections of the objective space and Pareto front\nof test function obj4d2 . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n3.9Two-dimensional projection of design space and grid-approximated\nPareto optimal set for hedarg2 test functions.. . . . . . . . . . . . . 117\n3.10 Two-dimensional projections of the objective space and Pareto front\nof the hedarg2 test function. . . . . . . . . . . . . . . . . . . . . . . . 118\n3.11 Design Space and Pareto optimal set for the dltz1a test functions.\nDesign variable x1is inactive while xj, j = 2,3,4,5,6 are active. Hence\nthe projection of the Pareto set on the plane (x1,xj), j = 2,3,4,5,6\nis a line, such as shown in the graph of (x1,x2). The two-dimensional\nprojections of a pair of active design variables (xi,xj), i ?= j, is a point,\nlike the graph of x5and x6.. . . . . . . . . . . . . . . . . . . . . . .120\n3.12 Test function dltz1a: A linear Pareto front in objective space. . . . . 121\n3.13 A comparison of IGP-PI, IGP-EmaX, CoH-EmaX algorithms for approxi-\nmating the Pareto Front of the function wsnl. . . . . . . . . . . . . . . .123\n3.14 A comparison of IGP-PI, IGP-EmaX, CoH-EmaX algorithms for approxi-\nmating the Pareto Front of the function knowles. . . . . . . . . . . . . .127\n3.15 Comparison in y1 and y2 of IGP-PI, IGP-EmaX, CoH-EmaX algorithms\nfor approximating the Pareto Front of the obj4d2 test functions.\n. . . . .131\n3.16 Comparison in y1 and y4 of IGP-PI, IGP-EmaX, CoH-EmaX algorithms\nfor approximating the Pareto Front of the obj4d2 test functions.\n. . . . . 132\nxvii"},{"page":19,"text":"3.17 Comparison in y2 and y4 of IGP-PI, IGP-EmaX, CoH-EmaX algorithms\nfor approximating the Pareto Front of the obj4d2 test functions.\n. . . . .133\n3.18 Comparison in y1 and y2 of IGP-PI, IGP-EmaX, CoH-EmaX algorithms\nfor approximating the Pareto Front of the hedarg2 test functions . . . . . 136\n3.19 Comparison in y1 and y2 of IGP-PI, IGP-EmaX, CoH-EmaX algorithms\nfor approximating the Pareto Front of the function dltz1a . . . . . . . . . 139\nxviii"},{"page":20,"text":"CHAPTER 1\nMODELS FOR COMPUTER EXPERIMENTS WITH\nMULTIVARIATE OUTPUTS\n1.1Introduction\nDue to the growth in computing power and speed, computer modeling and simu-\nlation of physical processes have become relevant in the study of many scientific and\nengineered systems. These activities continue to expand in domains that have used\nit for some time and new application areas are emerging. Scientists and engineers\nhave relied on computer simulation to assess the risk of high consequence systems\nthat cannot be tested physically such as the catastrophic failure of a nuclear power\nplant, a fire spreading through a high rise building, or a nuclear weapon caught in a\ntransport accident.\nNatural systems have also been the object of interest. Climate and weather models\ninform today\u2019s debate on climate change; computer simulations of environmental\nimpact have been made, for example, in the analysis of surface water quality, air toxic\nemissions, and hazardous waste management, particularly the underground storage\nof nuclear waste.\n1"},{"page":21,"text":"For engineered systems, safety and reliability concerns have motivated the devel-\nopment of computer experiments (CE) on existing and proposed systems operating\nat design, off-design, and failure-mode conditions in accident scenarios. Numerical\nsimulations from computational methods in structural mechanics, heat transfer, fluid\nmechanics, shock physics are used as virtual prototypes to help produce acceptable\ndesigns for systems. Virtual tests are also being carried out to develop new hardware\nor entire systems to eliminate the causes of failure or at worst, moderate its costs to\nthe environment, the public, an individual, or a company.\n1.2Review of Computer Experiments\nThe starting point of a computer model is a mathematical model relating the\nsystems\u2019 outputs to a set of inputs. In engineering, such mathematical models are\nexpressed as systems of partial differential equations (PDE) and require computer\nprograms to numerically solve them. Typical programs can contain several thousands\nof lines of code and can take anywhere from hours to days to evaluate even in the\nworld\u2019s fastest computers. The mathematical model, or the computer program that\nimplements it, is often called a simulator.\nThe inputs and outputs of a simulator can have high dimensionality like hundreds\nof input and output quantities. Computer codes of this size and complexity can there-\nfore be thought of as black boxes in that not much is understood about the code\u2019s\nmapping from inputs to outputs. The mapping is also considered to be deterministic,\nmeaning that after all inputs have been assigned values, the simulator produces a sin-\ngle value for each output it generates; a re-run at the same input generates duplicate\noutputs.\n2"},{"page":22,"text":"This is in stark contrast with physical experiments where a response corresponding\nto a set of treatments (the input variables) is measured along with unknown nuisance\nfactors which can cause variation in the outcome of replicated runs. In particular,\nthe measurement of treatment effects is blurred by noise\u2013random or non-systematic\neffects and bias\u2013the systematic effects of nuisance or uncontrolled factors. Noise can\narise from discrepancies due to unavoidable inaccuracies in the measuring instruments\nor non-homogeneity in the experimental units. Bias may be caused by an unrecog-\nnized flaw in the design or philosophy of the experiment that offsets the measurements\nconsistently.\nThe key differentiating concept here is repeatability. The response is deterministic\nin computer experiments but stochastic in physical experiments.\nThis contrast engenders a distinction in the meaning and management of uncer-\ntainty as well as in the sorts of errors produced. To manage uncertainties in compar-\ning treatments, classical experimental design uses randomization to control for the\nbias due to unrecognized nuisance factors and replication and blocking to mitigate or\nmodel the effects of recognized factors. In computer experiments, noise and bias due\nto uncontrolled factors does not occur so that randomization, replication, and block-\ning of experimental units are not required. Space-filling experimental designs that\nallow greatest empirical investigation of the input space are more useful in computer\nexperiments than classical fractional factorial and blocked designs.\nAny model of reality is bound to have deficiencies in the form of errors and uncer-\ntainties which make predictions imperfect. In computer experiments, the mathemati-\ncal model is the main source of uncertainty which is why the uncertainty is described\nas epistemic, the kind that derives from the modeler\u2019s partial knowledge or incomplete\n3"},{"page":23,"text":"understanding of the physical system and its surrounding environment. For example,\nthe modeler is not sure about the correct formulation of the mathematical model,\nor details about the formulation like the \u2018right\u2019 coefficients to plug in, or the correct\nboundary conditions to solve the relevant system of PDEs. On account of these rea-\nsons and of the simplifying assumptions made, the computer model\u2019s representation\nof reality will inevitably be biased. The simulation error or the difference between\nreality and the computer model\u2019s prediction is the model bias. Non-random errors\nalso exist in the code\u2019s implementation such as conversion from continuum partial\ndifferential equations to discrete numerical modeling, e.g., errors expected from the\nmesh resolution in a finite element solve, the computer\u2019s finite precision arithmetic,\nand round-off and truncation errors. Since simulation and implementation errors\nare systematic, both can be reduced. Predictions can be sharpened by incorporat-\ning additional information into, or calibrating the computer model. Computations\ncan be made more accurate by reconfiguring default settings or using more powerful\ncomputing machines.\nIn the last twenty years, there has been an upsurge in the engineering community\u2019s\nability to build finite element models to simulate the behavior of complex processes.\nFurthermore, the ability to rapidly adjust these simulation models to keep up with\ndesign changes has also increased.\nThe net result is that the use of simulation-based optimization to develop new en-\ngineered systems has increased. There are obstacles however, the very long running\ntimes and lack of gradient information in some areas have made it less convenient,\ndespite the steady growth in computing power and speed. For instance, single evalua-\ntions of finite element analyses to predict a structure\u2019s performance or computational\n4"},{"page":24,"text":"fluid dynamics models to visualize the flow over a body \u2013 both cases can take some\ndays up to a week to evaluate. Hence any optimization algorithm applied directly\non those codes will be slow. Even if it were possible to optimize the simulator di-\nrectly, this painful strategy will exhaust the budget in no time, barring any important\nfollow-up studies. Missed deadlines are also likely to delay the whole design process.\nThe rationale behind the surrogate model approach is to construct fast mathemat-\nical approximations that can be used in lieu of the long-running simulator to facilitate\nsuch ends as input space exploration, optimization, or reliability analysis. The un-\nderlying assumption is that, once constructed, the surrogate will be many orders of\nmagnitude faster than the simulator while also being reasonably accurate at predict-\ning the latter in unsampled design points. Meeting these criteria, such a surrogate can\nbe relied upon to identify active inputs, visualize functional relationships, investigate\ntrade-offs, and possibly provide fresh insights in real time. The simulator can then be\ncalled to verify the tentative conclusions drawn from interfacing with the surrogate.\nFor carrying out design optimization, the surrogate model is usually embedded\nwithin a sequential design\u2013a strategy for finding optimal points by performing ex-\nperiments successively in a direction of improvement. While the basic idea of the\nsurrogate approach is simple, there are numerous details that need to be sorted out\nfor its successful implementation. In the case of surrogate-guided optimization these\nare: (1) the selection of a surrogate model, (2) the choice of design points on which\nto train the surrogate, (3) identification of the best method to exploit the surrogate\nto identify new and improved designs, and (4) the use of the surrogate to examine\nthe trade-offs among conflicting objectives.\n5"},{"page":25,"text":"The first is a question of design for CE\u2019s. If no prior knowledge exists about the\nrelationship between inputs and outputs, space-filling designs minimize both this un-\ncertainty and the surrogate bias, by spreading points to gather information through-\nout the design space. Surrogate bias refers to the error of the surrogate in predicting\nthe simulator. The prediction error at any point in input space usually varies pro-\nportionally to its distance from the closest sampled design point. Points near any\nsampled point will generally be predicted more accurately than those that are far\naway. This implies that uneven designs\u2013those having poor coverage, can result in\npredictors that are inaccurate in sparsely sampled regions of input space.\nAs for the choice of surrogate, a special class of surrogates called emulators have\nthe special advantage of enabling a probabilistic assessment of the uncertainty in-\nduced by the prediction process. An emulator is a statistical model of a deterministic\nfunction which provides, for any design point x in the input space, a predictive dis-\ntribution for the simulator output at x, say Y (x). The mean of this distribution\nfor a given x is often regarded as the surrogate\u2019s approximation to y(x) and the\ndistribution about the mean is a measure of the uncertainty describing how close the\nsurrogate will be to y(x). Furthermore, because the output is deterministic, it is rea-\nsonable to seek emulators that interpolate the data to acknowledge the fact that the\nsimulator output is completely known at inputs where the simulator was run. For all\nother points, the distribution of Y (x) should indicate a mean value that represents\na realistic interpolation or extrapolation of all previously acquired data and that the\nprobability distribution around the prediction (mean) should plausibly describe the\nuncertainty about how Y (x) might interpolate or extrapolate.\n6"},{"page":26,"text":"The kriging predictor is a very popular emulator in the literature of CE\u2019s (Sant-\nner, Williams, and Notz 2003). If the simulator is believed to behave smoothly over\nits domain, then the choice of a GP-based surrogate is well-grounded. Other surro-\ngates have been used like neural networks, radial basis functions, or support vector\nmachines (Wang and Shan 2007; Ponweiser, Wagner, and Vincze 2008) but none\nof these approximations explicitly account for the uncertainty in prediction in their\nimplementation.\nSo far we have suggested implementing the surrogate model approach with a\ndesign having a space-filling property and a GP model that allows an assessment of\nthe uncertainty in the prediction process, respectively. The last two details in the\nimplementation \u2013 how to use the surrogate to identify new and improved designs and\nto examine the trade-offs among conflicting objectives, are addressed in this thesis.\nThe interest in this thesis is on simulation-based multiobjective optimization. In\nengineering design, multiple performance targets usually make it likely for some to\nbe conflicting: minimize weight, cost, number of defects, limit a critical temperature,\nstress, vibration response, maximize reliability, throughput, reconfigurability, agility,\nor design robustness. Assuming that no prior preferences are given, conflicting as-\npirations generally imply that the solution will be non-unique. The task is then to\nidentify the best trade-offs or compromises among the competing objectives.\nThis trend of simulation-based optimization has been energized by competition in\na market constantly pressed by a need to decrease lead times and the cost of deliver-\ning products to prospective users. More efficient methods that reduce both the time\ntaken to evaluate design concepts and the number of evaluations needed for optimiza-\ntion are continually being developed. State-of-the-art multiobjective optimization, as\n7"},{"page":27,"text":"evidenced by current commercial software (BOSS\/Quattro, iSight, modeFRONTIER,\nOPTIMUS, and the freely available softwares DAKOTA from Sandia Laboratories,\nand PISA from the ETH Laboratory in Zurich) have mostly implemented population-\nbased or nature-inspired algorithms such as evolutionary and genetic algorithms, par-\nticle swarm or ant colony optimization (Gobbi, Haque, Papalambros, and Mastinu\n2005; Simpson, Toropov, Balabanov, and Viana 2008; Knowles, Thiele, and Ziztler\n2006). Some previous studies have demonstrated the efficiency or utility of these al-\ngorithms in very high-dimensional problems. The main drawback in some is the cpu\ntime required in the processing.\nThis thesis takes a different course by formulating a multiobjective extension of\nthe Efficient Global Optimization algorithm (EGO) of Jones, Schonlau, and Welch\n(1998). The EGO algorithm is a Gaussian process-guided sequential design that\nfinds the global optimum by locating at each stage, that design point showing the\ngreatest potential of improving on the current best one. The task at hand poses\ntwo major challenges: one is developing a multivariate GP emulator that exploits the\nconflicting structure in the objectives to emulate the multi-output simulator, and two,\ndefining a suitable improvement function for the multiobjective problem to efficiently\nguide the sequential approximation of the Pareto front\u2013the trade-off curve or surface\nrepresenting a set of optimal solutions. The current work expands the literature on\nsurrogate-guided black box optimization approaches.\nThe thesis is organized as follows: Chapter 1 introduces the Gaussian process\n(GP) framework for analyzing computer experiments. In view of the multi-output\nsetting, two GP models are considered \u2013 the independence model which ignores the\ncross-covariances among the objectives and a dependence model that attempts to\n8"},{"page":28,"text":"capture these effects. Chapter 1 also introduces the key concepts of dominance and\nPareto optimality in multiobjective optimization. Chapter 2 contains the main con-\ntribution of this research namely, the expected maximin improvement or the EmaX\nupdate criterion embedded in a Gaussian process-guided sequential design. Its ana-\nlytic expression is derived for the bi-objective case and a Monte Carlo approximation\nis proposed for the general case. Chapter 3 presents comparisons on the quality of\nthe Pareto front approximation between the EmaX-updated sequential design and\nanother GP-guided sequential design using a competing update criterion called the\nprobability of improvement. In terms of the GP emulator, the efficiency gained by\nmodeling cross-covariance effects is also investigated. The comparisons are made on\nfive multiobjective test problems covering some variety with respect to the difficulties\nposed. Finally Chapter 4 concludes with a summary of results and directions for\nfuture work.\n1.3 Modeling Computer Experiments with Multiple Outputs\n1.3.1 Introduction\nA major disincentive for performing optimization via computer experiments is the\nruntime cost of the simulator which limits the number of runs that can be made. Our\napproach seeks to replicate the behavior of the simulator to perform optimization\nmuch faster by using an approximate model.\nIn this section, we describe the theoretical and computational aspects of Gaussian\nprocess emulation.\n9"},{"page":29,"text":"1.3.2 Overview of Gaussian process emulation\nWe regard the simulator or computer code as a deterministic function y(\u00b7) that\ntakes an input vector x \u2208 X \u2282 IRd,d \u2265 1 and returns an output\ny(x) = (y1(x),y2(x),... ,ym(x))T,m > 1.\nLet\nDn= {x1,...,xn},n \u2265 1\nbe a given design at which the computer code is run to produce outputs\ny(x1),y(x2),... ,y(xn).\nConsider the problem of predicting y(x\u2217) for an unseen input vector x\u2217\u2208 X on the\nbasis of the data {(xi,y(xi)) : xi\u2208 Dn}.\nKoehler and Owen (1996) discuss the Bayesian and frequentist approaches to\ncomputer experiments which are differentiated by the way randomness is introduced\nto measure how much a predicted value, say \u02c6 y(x0), differs from the true value y(x0)\nfor a new input site x0\u2208 X.\nIn the Bayesian formulation, y(\u00b7) is a realization of a random process. A prior\ndistribution is placed on the space of all functions from X = [0,1]dto IRm. This prior\nis then combined with information from training data {y(x1),...,y(xn)} to produce\na posterior distribution used to predict y(\u00b7) at new input sites.\nA Bayesian approach can be based on a spatial model adapted from Matheron\u2019s\nkriging model which treats the bias or the systematic departure of the response surface\nfrom a linear model, as the realization of a stationary Gaussian process. The classical\nbest linear unbiased predictor or BLUP is used to predict the simulator output at\nnew input sites.\n10"},{"page":30,"text":"A fully Bayesian approach views the simulator y(x),x \u2208 X as an unknown func-\ntion and consequently represents the uncertainty by a prior m-variate Gaussian pro-\ncess with mean vector \u00b5(x) and positive definite covariance matrix \u03a30; the Gaussian\nprocess is usually chosen for convenience, as the posterior process given a vector of\nobserved data on a set of input sites, is also a Gaussian process. Here, the posterior\nprocess is the object of interest since it is used for prediction. For this reason it is also\nreferred to as the predictive process (Currin, Mitchell, Morris, and Ylvisaker 1991).\nThe actual form of the predictor \u02c6 y(xo) depends on the specification of a loss function\nwhich quantifies the loss incurred when \u02c6 y(xo) is used to predict y(xo).\nOn the other hand, the frequentist approach introduces randomness via sampling\ntechniques, taking values of x1,x2,...,xnthat are partially determined by pseudo-\nrandom number generators. The randomness in the xiis then propagated through\nto randomness in the y(xi). Its approach to prediction and computer experiments is\nbased on numerical integration.\nIn this thesis, we will adopt a Bayesian perspective. We will combine prior infor-\nmation describing the mapping from x \u2208 X \u2282 IRdto y(x) \u2208 IRmwith information\nfrom the data to predict the simulator at unevaluated input sites. A definition of an\nm-variate Gaussian process follows.\nDefinition 1.3.1 Suppose that X is a fixed subset of IRd,d \u2265 1 having positive d-\ndimensional volume. We say that Y (x) = (Y1(x),Y2(x),...,Ym(x))Tis an m-variate\nGaussian process provided that for any n \u2265 1 and any choice of x1,x2,... ,xnin X,\nthe vector (YT(x1),YT(x2),... ,YT(xn))Thas a multivariate normal distribution.\nThe GP is a rich class of functions and we assume that y(\u00b7) can be expressed as a\ndraw from a mixture of GP\u2019s. Using GP emulators also allows tractable derivations of\n11"},{"page":31,"text":"the predictive distribution on account of well-established results on multivariate nor-\nmality. For our purpose, we consider the class of second-order stationary or stationary\nGP\u2019s.\nDefinition 1.3.2 An m-variate Gaussian process Y (x),x \u2208 X is defined to be\nsecond-order stationary if E(Y (x)) = \u00b5 \u2208 IRmfor all x \u2208 X, E (Y (x)\u2212\u00b5)(Y (x?)\u2212\n\u00b5)T= Cov(Y (x),Y (x?)) = K(x \u2212 x?) for any given pair of inputs x,x?\u2208 X. If\nx = x?, then K(0) = Var(Y (x)) which we denote by \u03a30for all x \u2208 X.\nAt any untried input vector x0\u2208 X, we can think of the predictive distribution\nof Y0= Y (x0) as capturing information about Y0provided in the data {Y (x1) =\ny(x1),Y (x2) = y(x2),...,Y (xn) = y(xn)} given the designs xi\u2208 Dn. We organize\nthe data as an n \u00d7 m matrix Yn,mas follows:\nYn,m=\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nY1(x1)\nY1(x2)\n...\nY1(xn) Y2(xn) ... Ym(xn)\nY2(x1) ... Ym(x1)\nY2(x2) ... Ym(x2)\n...\n...\n...\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8=\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nYT(x1)\nYT(x2)\n...\nYT(xn)\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n(1.1)\nand let Yn,m= (YT(x1),YT(x2),...,YT(xn))T\u2208 IRmnbe the data vector. If Y0\nand {Y (xi) : xi\u2208 Dn} are dependent quantities, then the conditional distribution\n[Y0|Yn,m] is the logical choice for the predictive distribution. Accordingly, we first\nderive the joint distribution [Y0,Yn,m] based on a two-stage hierarchical model.\nIn the first stage, we suppose that Y (x) follows an m-variate Gaussian process\n(GP) conditional on some parameter vector (\u00b5,\u03a30,\u03c8) denoting respectively the\nmean, variance, and correlation parameters of the GP. In symbols,\nY (x) \u223c GP (\u00b5,K(\u00b7;\u03a30,\u03c8))(1.2)\n12"},{"page":32,"text":"In the second stage we assume that a parametrization of (\u00b5,\u03a30) follows a non-\ninformative distribution, that is,\n\u03c0(\u00b5,\u03a30) \u221d |\u03a3o|\u2212(m+1)\n2\n(1.3)\nwhere \u03c0(D) denotes the distribution of a random quantity D and |A| denotes the\ndeterminant of a positive definite matrix A. We treat \u03c8 as known purely for com-\nputational convenience. The GP specification implies a willingness to assume that\ny(\u00b7) varies continuously over X and that for any given x \u2208 X, the uncertainty about\ny(x) can be described by a multivariate normal distribution having mean and variance\nfunctions which depend upon (\u00b5,\u03a30,\u03c8). A constructive approach is taken to form\nmultivariate GP models with a valid cross-covariance structure\u2013an important con-\nsideration when using GP emulators. The non-informative second-stage distribution\nreflects the idea that only weak information is available about (\u00b5,\u03a30). Given this\nparticular two-stage model, we show that the conditional distribution [Y0|Yn,m,\u03c8]\nbehaves as a multivariate t process which we then take as our emulator. In reality\nhowever, \u03c8 is unknown, so we replace it by some estimator?\u03c8, e.g. maximum likeli-\nhood, restricted maximum likelihood, or posterior mode. Plugging in?\u03c8 tends to be\noptimistic, meaning that it causes an understatement of the prediction uncertainty\nby disregarding the variability between\u02c6\u03c8 and its true unknown value. However,\n\u201cplug-in\u201d prediction is justifiable in situations where varying model parameters over\nreasonable ranges results in marginal changes in the sizes of the associated prediction\nvariances ((Diggle and Ribeiro 2007)).\n13"},{"page":33,"text":"1.3.3Models for the Cross-Covariance\nIn the first stage of the model we postulate that\nY (x) = BTf(x) + W(x)(1.4)\nwhere Y (x) = (Y1(x),Y2(x),... ,Ym(x))T\u2208 IRm,m > 1 is a vector of computer\nexperiment outputs, f(x) = (f1(x),f2(x),... ,fp(x))T\u2208 IRp,p \u2265 1 is a vector of\nknown regression functions common to all the Yk\u2019s, B = (\u03b21,... ,\u03b2m) \u2208 IRp,m is\na matrix of unknown regression coefficients with each \u03b2k= (\u03b2k,1,\u03b2k,2,... ,\u03b2k,p)T\u2208\nIRp, and W(x) = (W1(x),... ,Wm(x))T\u2208 IRmis a zero mean stationary Gaussian\nprocess, the key ingredient through which we capture the dependence among the\ncomponents Y1(x),Y2(x),... ,Ym(x). We write this m-variate process\nW(x) \u223c GP(0,K(\u00b7))\nwhere K(x \u2212 x?) is an m \u00d7 m matrix whose (i,j)thelement is Cov(Wi(x),Wj(x?)).\nTherefore, from Equation [1.4], it follows that\nY (x) \u223c GP(BTf(x),K(\u00b7)).(1.5)\nFor any arbitrary collection of n input sites x1,x2,... ,xnwe can also organize the\nmultivariate outcomes as an n\u00d7m matrix Wn,manalogous to Yn,min Equation (1.1).\nWe can then write the mn \u00d7 1 latent vector as Wn,m= (WT(x1),... ,WT(xn))T.\nThen\nWn,m\u223c MV N(0,\u03a3Wn,m)\n14"},{"page":34,"text":"where \u03a3Wn,m\u2208 IRmn,mncan be partitioned into n2blocks of size m \u00d7 m, with the\n(u,v)thblock equal to the cross-covariance matrix K(xu\u2212 xv) for 1 \u2264 u,v \u2264 n or\n\uf8ee\n\uf8f0\nThen it follows that\n\u03a3Wn,m=\n\uf8ef\uf8ef\uf8ef\nK(0)K(x1,x2)\nK(0)\n...\n... K(x1\u2212 xn)\n... K(x2\u2212 xn)\n...\nK(0)\nK(x2\u2212 x1)\n...\nK(xn\u2212 x1) K(xn\u2212 x2) ...\n...\n\uf8f9\n\uf8fb\n\uf8fa\uf8fa\uf8fa\n(1.6)\nvecYT\nn,m\u223c MV N((In\u2297 BT)F(n),\u03a3Wn,m) (1.7)\nwhere F(n)= (fT(x1),... ,fT(xn))T, Inis the n \u00d7 n identity matrix, and \u2297 is the\nKronecker product operator. If we denote the variance of the data vector Yn,mby\n\u03a3Y n,mthen\n\u03a3Y n,m= \u03a3Wn,m\n(1.8)\nTo implement the first stage, we face the challenge of choosing a valid matrix-\nvalued cross-covariance function, a more demanding task than choosing a real-valued\nfunction in the case of univariate GP\u2019s.\nDefinition 1.3.3 An n \u00d7 n real matrix A is called positive definite if\nzTAz > 0\nfor all non-zero real vectors z \u2208 IRn. The determinant of a positive definite matrix is\nalways positive so a positive definite matrix is always non-singular.\nWe require that for any arbitrary number n and choice of input sites, x1,... ,xn,\nthe resulting \u03a3Wn,mbe symmetric and positive definite. By itself, K(\u00b7) need not be\nsymmetric nor positive definite but must satisfy\nK(x \u2212 x?) = KT(x?\u2212 x)\n15"},{"page":35,"text":"so that \u03a3Wn,mis symmetric. At x = x?, K(x\u2212x?) must become symmetric and pos-\nitive definite because it is the covariance matrix of the latent components W1,... ,Wk\nspecific to input vector x.\nTo ensure positive-definiteness, we follow a constructive approach which has been\nused earlier in the environmental sciences (Ver Hoef and Barry 1998; Wackernagel\n2003; Finley, Banerjee, Ek, and McRoberts 2008) as well as in the computer modeling\nliterature (Kennedy and O\u2019Hagan 2000; Williams, Lehman, Santner, and Notz 2002).\nOne example is to set\nW(x) = AZ(x)(1.9)\nwith Z(x) = (Z1(x),... ,Zm(x))T\u2208 IRmwhere (Z1(\u00b7),...,Zm(\u00b7)) are mutually in-\ndependent, zero-centered, unit variance, stationary Gaussian processes and Zk(\u00b7) has\na correlation function \u03c1k(\u00b7; \u03d1k) with correlation parameter vector \u03d1k. The matrix\nA is selected as an m \u00d7 m non-singular matrix to make sure that \u03a3Y n,mis positive\ndefinite. Since\nCov(Zk(x),Zj(x?)) = 0\nfor k ?= j for all x and x?are, an immediate consequence is that\n\uf8eb\n\uf8ec\nCov(Zm(x),Z1(x?)) ... Cov(Zm(x),Zm(x?))\n= diag{\u03c11(x \u2212 x?;\u03d11),... ,\u03c1m(x \u2212 x?;\u03d1m)}\nKZ(x \u2212 x?;\u03d1) =\n\uf8ec\n\uf8ec\n\uf8ed\nCov(Z1(x),Z1(x?))\nCov(Z2(x),Z1(x?))\n...\n...\n...\n...\nCov(Z1(x),Zm(x?))\nCov(Z2(x),Zm(x?))\n...\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n(1.10)\nThe cross-covariance matrix KZ(\u00b7; \u03d1) with\n\u03d1 = (\u03d11,... ,\u03d1m)\nis positive definite since the diagonals are the values of a set of real-valued positive-\ndefinite functions \u03c11(\u00b7),... ,\u03c1m(\u00b7). More precisely, since the Zk\u2019s have unit variance,\n16"},{"page":36,"text":"KZ(\u00b7; \u03d1) is, in fact, a cross-correlation matrix. Defining Zn,m\u2208 IRn,manalogously\nas in Yn,min Equation (1.1), and letting\nZn,m= (ZT(x1),... ,ZT(xn))T\u2208 IRmn,\nit follows that\nZn,m\u223c MV N(0,\u03a3Zn,m) (1.11)\nwhere\n\u03a3Zn,m=\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nKZ(0;\u03d1)\nKZ(x2\u2212 x1;\u03d1)\n...\nKZ(xn\u2212 x1;\u03d1) KZ(xn\u2212 x2;\u03d1) ...\nIn Equation (1.12), the block diagonals of \u03a3Zn,mare m \u00d7 m identity matrices\nsince these equal\nKZ(x1\u2212 x2;\u03d1) ... KZ(x1\u2212 xn;\u03d1)\nKZ(0;\u03d1) ... KZ(x2\u2212 xn;\u03d1)\n...\n...\n...\nKZ(0;\u03d1)\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n(1.12)\nKZ(0,\u03d1) = diag{\u03c11(0; \u03d11),...,\u03c1m(0; \u03d1m)}\nwhere \u03c1k(0; \u03d1k) = 1 for any valid correlation function. The (u,v)thoff-diagonal block\nis the m \u00d7 m diagonal matrix\nKZ(xu\u2212 xv; \u03d1) = 0m,m\nwith xu\u2212 xv?= 0 \u2208 IRd. Hence \u03a3Zn,mis an identity matrix of order mn \u00d7 mn.\nThe implications of defining the latent process W(x) by (1.9) are as follows:\n1. Recalling that K(xu\u2212 xv; \u03d1) = Cov(W(xu),W(xv)) \u2208 IRm,m where \u03d1 is\nthe vector of correlation parameters for the process Z(x).Since W(x) =\nAZ(x),x \u2208 X we have that for any xu,xv,\nK(xu\u2212 xv; \u03d1) = Cov(W(xu),W(xv))\n= AKZ(xu\u2212 xv; \u03d1)AT\n(1.13)\n17"},{"page":37,"text":"2. By (1), when xu= xv, then K(0; \u03d1) = AAT. This implies that\nA = K\n1\n2(0,\u03d1)(1.14)\nSince K(0,\u03d1) \u2208 IR+\noutputs Y1,... ,Ymfor the same input, we can identify the non-singular matrix\nm,mrepresents the variance-covariance matrix of the model\ntransform A as the square root or Cholesky factor of K(0,\u03d1). The one-to-\none correspondence between a positive definite matrix and its Cholesky factor\nis known (Harville 1997), so the matrix A is well-defined. Without loss of\ngenerality, we take A as a lower triangular matrix. Hence A indirectly models\nthe within-input covariance of the components of the W(x) process.\n3. The mn \u00d7 mn cross-covariance matrix of Yn,mcan be expressed in terms of A\nand \u03a3Zn,mas follows:\n\uf8eb\n\uf8ec\nK(xn\u2212 x1;\u03d1) ...\n\uf8eb\n\uf8ec\nAKZ(xn\u2212 x1;\u03d1)AT\n\uf8eb\n\uf8ec\n\u03a5n,1(\u03d1) \u03a5n,2(\u03d1) ...\n(In\u2297 AT)\n\u03a3Y n,m\n=\n\uf8ec\n\uf8ec\n\uf8ed\n\uf8ec\nK(0;\u03d1)\nK(x2\u2212 x1;\u03d1) ... K(x2\u2212 xn;\u03d1)\n...\n... K(x1\u2212 xn;\u03d1)\n...\n...\nK(0;\u03d1)\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n=\n\uf8ec\n\uf8ed\nAKZ(0;\u03d1)AT\nAKZ(x2\u2212 x1;\u03d1)AT\n...\n... AKZ(x1\u2212 xn;\u03d1)AT\n... AKZ(x2\u2212 xn;\u03d1)AT\n...\n...AKZ(0;\u03d1)AT\n...\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n= (In\u2297 A)\n\uf8ec\n\uf8ec\n\uf8ed\nIm\n\u03a51,2(\u03d1) ... \u03a51,n(\u03d1)\nIm\n... \u03a52,n(\u03d1)\n...\n\u03a51,2(\u03d1)\n...\n...\n...\nIm\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\u00d7\n= (In\u2297 A)\u03a3Zn,m(In\u2297 AT)(1.15)\nwhere\n\u03a5u,v(\u03d1) = diag{\u03c11(xu\u2212 xv;\u03d11),...,\u03c1m(xu\u2212 xv;\u03d1m)} \u2208 IR+\nm,m\n18"},{"page":38,"text":"The constructive approach here generalizes some of the known cross-covariance\nstructures proposed for GP models of multiple output computer experiments. We\nnow show how particular specifications of the matrix A and\nKZ(\u00b7; \u03d1) = diag{\u03c11(xu\u2212 xv; \u03d11),...,\u03c1m(xu\u2212 xv; \u03d1m)}\ngive rise to these known models. We develop the idea that A accounts for the covari-\nances due to the model outputs while KZ(\u00b7; \u03d1) models the covariances due to the\nmodel inputs or the spatial effects which may depend upon the Yk\u2019s.\nIf we specify A to have diagonal structure, the outcome is a model assuming\nindependence among the model outputs Yk. This model is a special case of the more\ngeneral dependence model which simply provides that A \u2208 IR+\ndescribe the independence model by letting\nm,m. Formally, we\nA = diag(\u03c31,... ,\u03c3m) where \u03c3k> 0 \u2200 k\n\u21d2 AAT\n= diag(\u03c32\n1,... ,\u03c32\nm) (1.16)\nIn the second line of Equation (1.16), we recognize the left hand side as the quantity\nK(0,\u03d1), the within-input covariance matrix of Y (x). The diagonal structure for\nA ignores any dependencies among the components of Y (x), x \u2208 X and reduces\nmultivariate emulation into building m GP emulators, one for each Yk, using what\nhas now become a standard recipe first introduced by Sacks, Welch, Mitchell, and\nWynn (1989). Such is the model adopted by Keane (2006), Forrester, Sobester, and\nKeane (2008), and Knowles (2006) who all proposed sequential designs for solving a\nmultiobjective optimization problem using computer experiments.\nIf we assume a common correlation structure for the Z(\u00b7) process, that is a com-\nmon correlation structure for all the Zk\u2019s, the end result is a separable cross-covariance\n19"},{"page":39,"text":"model for the Y (\u00b7) process. The separable case is a subset of the more general non-\nseparable model which allows different correlation functions for each Zk. In symbols,\na common correlation structure is written as\n\u03c1k(\u00b7; \u03d1k) = \u03c1(\u00b7; \u03d1\u2217) with \u03d1k= \u03d1\u2217for all k (1.17)\nDefinition 1.3.4 A separable cross-covariance structure holds for the m-variate Gaus-\nsian process Y (x), x \u2208 X, if for any n \u2265 1, and any collection {x1,... ,xn} of input\nsites, the mn\u00d7mn cross-covariance matrix of Yn,mcan be expressed as the Kronecker\nproduct of matrices R(\u03d1\u2217) \u2208 IR+\nn,nand AAT\u2208 IR+\nm,m, that is,\n\u03a3Y n,m= R(\u03d1\u2217) \u2297 AAT\n(1.18)\nwhere\nR(\u03d1\u2217) =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n1\u03c1(x1\u2212 x2;\u03d1\u2217)\n1\n...\n... \u03c1(x1\u2212 xn;\u03d1\u2217)\n... \u03c1(x2\u2212 xn;\u03d1\u2217)\n...\n\u03c1(x1\u2212 x2;\u03d1\u2217)\n...\n\u03c1(xn\u2212 x1;\u03d1\u2217) \u03c1(xn\u2212 x2;\u03d1\u2217) ...\n...\n1\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\nTo see how a common correlation structure induces separability in the cross-covariance\nof Yn,m, observe that\nKZ(xu\u2212 xv;\u03d1) = Im\u2297 \u03c1(xu\u2212 xv;\u03d1\u2217)\n\u21d2 \u03a3Zn,m\n= R(\u03d1\u2217) \u2297 Im\n(1.19)\nSubstituting the last equality of Equation (1.19) into the expression for the cross-\ncovariance of Yn,min Equation (1.15) we have\n\u03a3Y n,m\n= (In\u2297 A)\u03a3Zn,m(In\u2297 AT)\n= (In\u2297 A)(R(\u03d1\u2217) \u2297 Im)(In\u2297 AT)\n= R(\u03d1\u2217) \u2297 AAT\n(1.20)\n20"},{"page":40,"text":"Note that the term AATdenotes the within-input covariance matrix. The last equal-\nity follows from the distributive law particularly\n(C \u2297 D)(E \u2297 G) = CE \u2297 DG\ngoverning Kronecker products, provided the matrices are compatible with multipli-\ncation. The separable cross-covariance model is used by Conti, Gosling, Oakley, and\nO\u2019Hagan (2007) in their work on dynamic emulation and Rougier (2008) in his con-\nstruction of a multivariate emulator called the Outer Product Emulator.\nWhat has been demonstrated so far is that, specific forms for A and KZ(\u00b7; \u03d1)\nlead to currently proposed cross-covariance structures. To complete the presenta-\ntion here, we list four possible models and give more detailed discussion in the next\nparagraphs regarding the model implication for Yn,m.\n\u2022 Model I:\nA = diag{\u03c31,... ,\u03c3m} and KZ(\u00b7; \u03d1) = \u03c1(\u00b7; \u03d1\u2217)\n\u2022 Model II:\nA \u2208 IR+\nm,mand KZ(\u00b7; \u03d1) = \u03c1(\u00b7; \u03d1\u2217)\n\u2022 Model III:\nA = diag{\u03c3k,... ,\u03c3m} and KZ(\u00b7; \u03d1) = diag{\u03c11(\u00b7; \u03d11),...,\u03c1m(\u00b7; \u03d1m)}\n\u2022 Model IV:\nA \u2208 IR+\nm,mand KZ(\u00b7; \u03d1) = diag{\u03c11(\u00b7; \u03d11),...,\u03c1m(\u00b7; \u03d1m)}\nBriefly, Model I is a separable independence model, and its non-separable extension is\nModel III. Model II is a separable dependence model and its non-separable extension is\n21"},{"page":41,"text":"Model IV. In this thesis, only GP emulators with Models II and III cross-covariance\nstructures were implemented.These are discussed in later subsections where we\nhighlight the important implications. Since only the non-separable independence\nmodel is considered, we label the emulator with Model II structure as the independent\nGaussian processes model or IGP for short. As for the GP emulator with Model III\nstructure, we name it CoH after Conti, Gosling, Oakley, and O\u2019Hagan (2007) whose\nwork we apply here. Another model used for computer experiments with multiple\nresponses is the Autoregressive Model used by Kennedy and O\u2019Hagan (2000) and\nWilliams, Lehman, Santner, and Notz (2002). This model exemplifies the Model IV\nstructure which we also discuss to underline the variety of covariance of structures\nallowed by the constructive approach.\nModel I: A Separable Independence Model\nThe separable independence model is the least complex among the four models for\nthe cross-covariance. It is obtained by imposing special structures on the within-input\ncovariance matrix and on the correlation structure of the elementary processes Zk.\nParticularly, we assume independence among the model outputs Yk(\u00b7) and a common\ncorrelation structure for the Zk(\u00b7). These are summarized as follows:\nA = diag{\u03c31,... ,\u03c3m}\n(1.21)\n\u03c1k(\u00b7;\u03d1k) = \u03c1(\u00b7;\u03d1\u2217) \u2200 k\nwhere \u03c3k > 0 for all k to ensure positive-definiteness. Here onwards, we use the\nnotation \u039b \u2261 diag{\u03c31,... ,\u03c3m} to differentiate this special case from the general A\nnotation. In the second line of Equation (1.21), \u03d1k= \u03d1\u2217for all k, that is, \u03d1\u2217is the\ncommon correlation parameter vector. These special structures imply that given any\n22"},{"page":42,"text":"two scalar outputs Yj(\u00b7) and Yk(\u00b7) for all x,x?\u2208 X\nCov(Yj(x),Yk(x?)) =\n?0, if j ?= k\n\u03c32\nj\u03c1(x \u2212 x?;\u03d1\u2217), if j = k\n(1.22)\nor in terms of vector outputs,\nCov(Y (x),Y (x?)) =\n?\u039b\u039bT,x = x?\nx ?= x?\n\u03c1(x \u2212 x?;\u03d1\u2217)\u039b\u039bT\n(1.23)\nwhich leads to the following expression for the cross-covariance of Yn,m:\n\uf8eb\n\uf8ec\n\u03c1n,1\u039b\u039bT\n= R(\u03d1\u2217) \u2297 \u039b\u039bT\n\u03a3Y n,m\n=\n\uf8ec\n\uf8ec\n\uf8ed\n\u039b\u039bT\n\u03c12,1\u039b\u039bT\n...\n\u03c11,2\u039b\u039bT\n\u039b\u039bT\n...\n\u03c1n,2\u039b\u039bT\n... \u03c11,n\u039b\u039bT\n... \u03c12,n\u039b\u039bT\n...\n... \u039b\u039bT\n...\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n(1.24)\nwhere\nR(\u03d1\u2217) =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n1\u03c11,2\n1\n...\n... \u03c11,n\n... \u03c12,n\n...\n\u03c12,1\n...\n\u03c1n,1 \u03c1n,2 ...\n...\n1\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\u2208 IR+\nn,n,\nand \u03c1u,v= \u03c1(xu\u2212xv;\u03d1\u2217) for u,v = 1,...,n. The cross-covariance matrix in Equation\n(1.24) is positive definite by construction and symmetric for as long as \u03c1(\u00b7;\u03d1\u2217) is an\neven function of its argument, that is \u03c1(xu\u2212 xv) = \u03c1(xv\u2212 xu).\nModel II: A Non-Separable Independence Model\nAnother way to model multivariate computer output\nY (x) = (Y1(x),... ,Ym(x))T\nis to emulate each component separately which means building m single-output GP\nemulators, one for each Yk(\u00b7). This is equivalent to specifying a non-separable inde-\npendence model for the cross-covariance which assumes a diagonal structure for A\n23"},{"page":43,"text":"and allows each elementary process Zk(\u00b7) to have its own correlation structure. In\nsymbols,\nA = \u039b = diag{\u03c31,... ,\u03c3m}\nKZ(\u00b7;\u03d1) = diag{\u03c11(\u00b7; \u03d11),...,\u03c1m(\u00b7; \u03d1m)}\n\u2208 IR+\nm,m\n(1.25)\nwhere \u03c3k> 0 for all k. For any 1 \u2264 j,k \u2264 m and for any pair of inputs, x,x?\u2208 X,\nthis cross-covariance model provides that\n?0,\nIn terms of the vector outputs Y (x) and Y (x?),\n?\nCov(Yj(x),Yk(x?)) =\nj ?= k\n\u03c32\nj\u03c1j(x \u2212 x?;\u03d1j), j = k\n(1.26)\nCov(Y (x),Y (x?)) =\n\u039b\u039bT,\n\u039b[\u2295m\nx = x?\nx ?= x?\nk=1\u03c1k(x \u2212 x?;\u03d1k)]\u039bT\n(1.27)\nwhere\n[\u2295m\nk=1\u03c1k(x \u2212 x?;\u03d1k)] = diag{\u03c11(x \u2212 x?;\u03d11),...,\u03c1m(x \u2212 x?;\u03d1m)}\nTherefore, the covariance matrix of Yn,mis given by\n\uf8eb\n\uf8ec\n\u03a3Y n,m=\n\uf8ec\n\uf8ec\n\uf8ed\n\u039b\u039bT\n\u039b\u22062,1\u039bT\n...\n\u039b\u2206n,1\u039bT\n\u039b\u22061,2\u039bT\n\u039b\u039bT\n...\n\u039b\u2206n,2\u039bT\n... \u039b\u22061,n\u039bT\n... \u039b\u22062,n\u039bT\n...\n...\u039b\u039bT\n...\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n(1.28)\nwhere \u2206u,v= (\u2295m\nWe now compare and contrast the two independence models. The idea of emulat-\nk=1\u03c1k(xu\u2212 xv;\u03d1k)) \u2208 IRm,m.\ning the model outputs independently when the components are believed to be related\nin some way poses potential modeling inefficiencies. Despite this, the independence\nmodel continues to be used because of its simplicity and tractability which are its\nattractive features.\n24"},{"page":44,"text":"If the outputs (Y1(x),Y2(x),...,Ym(x)) represent different quantities with dis-\nparate measurement scales, the assumption of common correlation structure may\ncause inadequacies in the emulator. In some cases, Model II is appropriate follow-\ning pre-processing of the values by means of a normalization. Alternatively, a non-\nseparable version can be pursued since it accommodates output-specific scale and\/or\nroughness parameters. However, when the model outputs are all measurements of a\nsingle quantity, for example, temperatures across time or space or stresses at different\npoints of a body, then a common correlation structure is a rational choice with the\nadded benefit of having fewer parameters to estimate. The separable independence\nmodel imposes a more stringent structure than its non-separable extension.\nModel III: A Separable Dependence Model\nOne of the simple ways to produce a valid cross-covariance function for Y (x),x \u2208\nX is to let \u03c1(\u00b7;\u03d1\u2217) be a valid correlation function of some stationary univariate GP\nprocess and AAT\u2208 IRm,mbe a positive definite matrix. Then set\nCov(Y (x),Y (x?)) = \u03c1(x \u2212 x?;\u03d1\u2217) \u00d7 AAT\n(1.29)\nwhere AATis the within input covariance matrix associated with Y (x), for x \u2208 X,\nand \u03c1(\u00b7;\u03d1\u2217) attenuates the correlation as the separation between x and x?increases.\nThis summarizes the gist of the separable dependence model. In terms of the con-\nstructive approach, we attain this cross-covariance structure by specifying a common\ncorrelation structure for the elementary processes Zk(\u00b7), and taking a more general\nform for A \u2208 IR+\nYj(x) and Yk(x?) for any x,x?\u2208 X is given by\nm,m. As a result, the covariance between any pair of model outputs,\nCov(Yj(x),Yk(x?)) = \u03c1(x \u2212 x?;\u03d1\u2217) \u00d7 \u03c3jk\n(1.30)\n25"},{"page":45,"text":"where \u03c3jk= \u03c3kjis the covariance between Yj(\u00b7) and Yk(\u00b7) found as the (j,k)thentry\nof AAT.\nIt follows from Equations (1.29) and (1.30) that the cross-covariance matrix of\nYn,mhas the form:\n\u03a3Y n,m= R(\u03d1\u2217) \u2297 AAT. (1.31)\nwhere R(\u03d1\u2217) is defined below Equation (1.24).\nOne important feature of Equation (1.31) is that the cross-covariance factors into\na part that could be ascribed purely to the spatial association R(\u03d1\u2217) and a part\nattributable to the model outputs (AAT). This separability, which is key to tractable\nstatistical modeling, also helps in finding the determinant and inverse of the mn\u00d7mn\ncross-covariance \u03a3Y n,mwhich are:\n|\u03a3Y n,m| = |R(\u03d1\u2217)|m|AAT|n\n\u03a3\u22121\nY n,m\n= R\u22121(\u03d1\u2217) \u2297 (AAT)\u22121.(1.32)\nThus the inversion of \u03a3Y n,mdeals with two matrices of more manageable dimensions,\nm\u00d7m and n\u00d7n, instead of one that is mn\u00d7mn. The overall reduction in complexity\nhowever comes with a price\u2013a highly restrictive, possibly unsatisfying structure for\nthe cross-covariance. As Rougier (2007) pointed out, Equation (1.31) implies\nCov(Yj(x),Yk(x?))\nCov(Yj(x\u2217),Yk(x\u2217\u2217))=\n\u03c1(x \u2212 x?)\n\u03c1(x\u2217\u2212 x\u2217\u2217)\n(1.33)\nwhich depends only in the model inputs {x,x?,x\u2217,x\u2217\u2217} for any selected pair of\n{Yj,Yk}. When x = x?and x\u2217= x\u2217\u2217, the ratio in Equation (1.33) would be a\nconstant for any pair of model outputs.\n26"},{"page":46,"text":"Model IV: A Non-Separable Dependence Model\nDropping the restriction of common correlation structure from the separable de-\npendence model, the non-separable dependence model is obtained. By far this is the\nmost complex among all the cross-covariance models considered. This is a general-\nization of the autoregressive model (AR) used by Kennedy and O\u2019Hagan (2000) and\nWilliams, Lehman, Santner, and Notz (2002). We show here how the cross-covariance\ninduced by the AR specification falls into this class of models.\nThe AR model of order m provides\nYj(x) = \u03b2T\njfj(x) + AjZ(x)(1.34)\nwhere \u03b2j\u2208 IRp is the jthcolumn of B, fj \u2208 IRp is a vector of known regression\nfunctions common to all the Yj\u2019s, Ajis the jthrow of A, the Cholesky factor of the\nwithin-input covariance matrix, and Z(x) is an m\u2212dimensional GP with mutually\nindependent components having\n\u2022 E(Zj(x)) = 0,\n\u2022 V ar(Zj(x)) = 1, and\n\u2022 Cov(Zj(x),Zj(x?)) = \u03c1j(x \u2212 x?;\u03d1\u2217).\nThe assumption of a shared set of regressors can be relaxed, and in most AR cases\nused in the literature, the order is usually less than m. From Equation (1.34) it\nfollows that for any pair of outputs 1 \u2264 j,k \u2264 m and any pair of inputs x,x?\u2208 X,\nCov(Yj(x),Yk(x?)) =\nm\n?\nl=1\najlakl\u03c1l(x \u2212 x?;\u03d1l)(1.35)\n27"},{"page":47,"text":"where ajlis the lthentry in Aj= (aj1,... ,ajm) which is the jthrow of A. It follows\nthat\nCov(Y (x),Y (x?)) = A[\u2295m\nk=1\u03c1k(x \u2212 x?;\u03d1k)]AT. (1.36)\nThe cross-covariance matrix \u03a3Y n,mis therefore equal to\n\uf8eb\n\uf8ec\nA\u2206n,1AT\n\u03a3Y n,m=\n\uf8ec\n\uf8ec\n\uf8ed\nAAT\nA\u22062,1AT\n...\nA\u22061,2AT\nAAT\n...\nA\u2206n,2AT\n... A\u22061,nAT\n... A\u22062,nAT\n...\n...AAT\n...\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n(1.37)\nwhere \u2206u,vis as defined in Equation (1.28).\n1.3.4 Predicting Output from Computer Experiments\nIn this section we discuss techniques for predicting the output of the simulator\ny(x) = (y1(x),y2(x),...,ym(x))Tbased on training data. By prediction we mean the\nproblem of presenting a point guess of the realization of a random quantity (Santner,\nWilliams, and Notz 2003). Our proposed sequential optimization design for the black\nbox multiobjective problem uses the prediction methods discussed here.\nThe following discussion describes optimal predictors under the classical or kriging\nmodel and under a fully Bayesian approach.\nThe Kriging Model\nSuppose X = [0,1]d\u2282 IRd is the design space, and let x \u2208 X be a scaled d-\ndimensional vector of input values. The kriging approach views the deterministic\nfunction y(\u00b7) as a realization of a stochastic process Y (x), x \u2208 X and uses a two\ncomponent model for Y (x): a linear term that models the drift or trend in the\nresponse and a stationary Gaussian process that models the systematic departure of\nthe response from the linear model (Sacks, Welch, Mitchell, and Wynn 1989). The\n28"},{"page":48,"text":"model is written as:\nY (x) = F(x)\u03b2 + W(x) (1.38)\nwhere\nF(x) =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nfT\n01,p1\n1(x)01,p2\nfT\n2(x) ...\n...\n01,p2\n\uf8eb\n\uf8ec\nfkpk(x)\n...01,pm\n01,pm\n...\nm(x)\n...\n...\n... fT\n\uf8f6\n\uf8f7\n\uf8eb\n\uf8ec\n01,p1\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\u2208 IRm,p\nfk(x) =\n\uf8ec\n\uf8ec\n\uf8ed\nfk1(x)\nfk2(x)\n...\n\uf8f7\n\uf8f7\n\uf8f8\u2208 IRpk\n\uf8ec\n\u03b2kpk\n\u03b2 =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n\u03b21\n\u03b22\n...\n\u03b2m\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\u2208 IRp,\u03b2k=\n\uf8ec\n\uf8ed\n\u03b2k1\n\u03b2k2\n...\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\u2208 IRpk.\nThe full column rank m \u00d7 p matrix F(\u00b7) contains known regression functions for\neach of the Yk(\u00b7), reflected in each row by the pk-vector fT\np =\nk(\u00b7) for 1 \u2264 k \u2264 m, and\n?m\nk=1pk is the total number of regressors; \u03b2 is a p \u00d7 1 vector of unknown\nregression parameters consisting of the pk\u00d7 1 vectors \u03b2kof regression coefficients\nof Yk(\u00b7), W(x) is an m-variate stationary Gaussian process having the following\nproperties: E(W(x)) = 0m, V ar(W(x)) = \u03a30 \u2208 IR+\na given pair xu,xv \u2208 X, Cov(W(xu),W(xv)) = K(xu\u2212 xv;\u03a30,\u03c8), and \u03c8 is a\nvector of correlation parameters. The dimension of \u03c8 depends on the assumed cross-\nm,mfor all x \u2208 X, and for\ncovariance structure, for example, imposing separability results in a \u03c8 vector having\na lower dimension compared to the \u03c8-vector in the more general non-separable case.\nThe kriging model in (1.38) implies that for any x \u2208 X, the response Y (x) is\nm-variate multivariate normal with mean F(x)\u03b2 and variance K(0;\u03a30,\u03c8) = \u03a30.\nMore importantly, the bias or systematic departure of the response surface from the\n29"},{"page":49,"text":"linear model is treated as a realization of an m-variate stationary Gaussian process,\nW(\u00b7).\nLet our data consist of simulator outputs\n{Y (x1) = y(x1),Y (x2) = y(x2),...,Y (xn) = y(xn)}\nevaluated on a set of input sites Dn= {x1,...,xn} \u2282 X. Recall from Subsection\n1.3.3 our notation for the data matrix:\n\uf8eb\n\uf8ec\nY1(xn) Y2(xn) ... Ym(xn)\nYn,m=\n\uf8ec\n\uf8ec\n\uf8ed\nY1(x1)\nY1(x2)\n...\nY2(x1) ... Ym(x1)\nY2(x2) ... Ym(x2)\n...\n...\n...\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\nWe now tackle the goal of predicting y(xo) at a new input site xogiven the information\nfrom Yn,m. Let Yk\u2261 (Yk(x1),...,Yk(xn))T\u2208 IRn, and Wk\u2261 (Wk(x1),...,Wk(xn))T\u2208\nIRn, 1 \u2264 k \u2264 m. One way to write the data in terms of Equation (1.38) is as follows:\n\uf8eb\n\uf8ec\nYm\n0n,p1\n0n,p2\n...\nvecYn,m = F \u03b2 + vecWn,m\n\uf8ec\n\uf8ec\n\uf8ed\nY1\nY2\n...\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n=\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nF1\n0n,p1\n...\n0n,p1\nF2\n...\n... 0n,pm\n... 0n,pm\n...\n...\nFm\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n\u03b21\n\u03b22\n...\n\u03b2m\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8+\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nW1\nW2\n...\nWm\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n(1.39)\nThe n\u00d7pkmatrix Fkcontains the known regression functions of Ykevaluated on Dn.\nSince W(\u00b7) is a zero-mean process, it follows that\nE(Wmn) = 0mnand E(W(xo)) = 0m.\n30"},{"page":50,"text":"We write \u03a3wmn\u2261 V ar(Wmn) and C \u2261 cov(Wmn,W(xo)), where\n\uf8eb\n\uf8ec\n\u03a3m1 \u03a3m2 ... \u03a3mm\n\uf8eb\n\uf8ec\nCov(Wj(xn),Wk(x1)) Cov(Wj(xn),Wk(x2)) ... Cov(Wj(xn),Wk(xn))\n\u2208 IRn,n\n\uf8eb\n\uf8ec\ncm1 cm2 ... cmm\n\uf8eb\n\uf8ec\nCov(Wj(xn),Wk(xo))\n\u03a3wmn\n=\n\uf8ec\n\uf8ec\n\uf8ed\n\uf8ec\n\u03a311\n\u03a321\n...\n\u03a312\n\u03a322\n...\n...\n...\n...\n\u03a31m\n\u03a32m\n...\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\nCov(Wj(x1),Wk(x2))\nCov(Wj(x2),Wk(x2))\n...\n\uf8f8\u2208 IRmn,mn\n\u03a3jk\n\u2261\n\uf8ec\n\uf8ed\nCov(Wj(x1),Wk(x1))\nCov(Wj(x2),Wk(x1))\n...\n... Cov(Wj(x1),Wk(xn))\n... Cov(Wj(x2),Wk(xn))\n...\n...\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\nC=\n\uf8ec\n\uf8ec\n\uf8ed\n\uf8ec\nc11\nc2m\n...\nc12\nc22\n...\n...\n...\n...\nc1m\nc1m\n...\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\u2208 IRmn,m\n\uf8f7\ncjk\n\u2261\n\uf8ec\n\uf8ed\nCov(Wj(x1),Wk(xo))\nCov(Wj(x2),Wk(xo))\n...\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8\u2208 IRn,cj=\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nc1j\nc2j\n...\ncmj\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\u2208 IRmn\nA common optimality criterion for predicting in the univariate instance is the\nmean squared error or MSE. Consider any m \u00d7 1 vector function of the data Yn,m,\n?Y0\u2261?Y (x0;Yn,m)\nto be a predictor of the m \u00d7 1 vector Y (x0). We extend the MSE criterion to the\nmultivariate case by defining the mean squared prediction error matrix of?Y0:\nMSPE(?Y0) = E\nThe expectation in Equation (1.40) is taken over the joint distribution of Y (x0)\n???Y0\u2212 Y (x0)\n???Y0\u2212 Y (x0)\n?T?\n\u2208 IR+\nm,m\n(1.40)\nand vecYn,m, which in the present case is an m(n + 1)-variate multivariate normal\ndistribution. The diagonals of the MSPE(?Y0) matrix equal the univariate MSE\u2019s of\n?Yk(x0), for 1 \u2264 k \u2264 m. We define?Y\n\u2217(x0) to be the best MSPE predictor of Y (x0)\n31"}],"widgetId":"rgw21_56aba1862854c"},"id":"rgw21_56aba1862854c","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=216457235&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw22_56aba1862854c"},"id":"rgw22_56aba1862854c","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=216457235&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":216457235,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/216457235_A_SEQUENTIAL_DESIGN_FOR_APPROXIMATING_THE_PARETO_FRONT_USING_THE_EXPECTED_PARETO_IMPROVEMENT_FUNCTION","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56aba1862854c"},"id":"rgw2_56aba1862854c","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":216457235},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=216457235&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56aba1862854c"},"id":"rgw1_56aba1862854c","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"pFKWsKnnXzpaSlLEMA8FQ8ry5HCJNLqs33wy+iNaWvMCyr1wnxaprgR+hdLNSMEvjyisXkNmtpwZ5hPrrDX9Z0fomZfjsdIG3hj8DhQwc4p6+PLFhpo+T\/vjaR2PdIu4CyfsFTVIE2dWFNUx8ZeOr4XjlzlKpO3Pk3lp1YuMG2XAcNvxjOhDzg\/VSB2+Pk4SuDdlGwbykDrcFlaCGyQYgypN\/Ibx3gybZZidmgtyffWahr\/xOK8lKn2614mlIUAIQL6Pb2sTkRefmyE01AWEYAuJTDvazjVB4Dn4x+GNA4U=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/216457235_A_SEQUENTIAL_DESIGN_FOR_APPROXIMATING_THE_PARETO_FRONT_USING_THE_EXPECTED_PARETO_IMPROVEMENT_FUNCTION\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"A SEQUENTIAL DESIGN FOR APPROXIMATING THE PARETO FRONT USING THE EXPECTED PARETO IMPROVEMENT FUNCTION\" \/>\n<meta property=\"og:description\" content=\"\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/216457235_A_SEQUENTIAL_DESIGN_FOR_APPROXIMATING_THE_PARETO_FRONT_USING_THE_EXPECTED_PARETO_IMPROVEMENT_FUNCTION\/links\/0ffc2f280cf2ca93ebadd973\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/216457235_A_SEQUENTIAL_DESIGN_FOR_APPROXIMATING_THE_PARETO_FRONT_USING_THE_EXPECTED_PARETO_IMPROVEMENT_FUNCTION\" \/>\n<meta property=\"rg:id\" content=\"PB:216457235\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"A SEQUENTIAL DESIGN FOR APPROXIMATING THE PARETO FRONT USING THE EXPECTED PARETO IMPROVEMENT FUNCTION\" \/>\n<meta name=\"citation_author\" content=\"Dianne Carrol Bautista\" \/>\n<meta name=\"citation_publication_date\" content=\"2009\/01\/01\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/216457235_A_SEQUENTIAL_DESIGN_FOR_APPROXIMATING_THE_PARETO_FRONT_USING_THE_EXPECTED_PARETO_IMPROVEMENT_FUNCTION\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/216457235_A_SEQUENTIAL_DESIGN_FOR_APPROXIMATING_THE_PARETO_FRONT_USING_THE_EXPECTED_PARETO_IMPROVEMENT_FUNCTION\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Thesis');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-8742ef76-a8b7-4d35-8b1f-28443e7159d5","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":382,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw23_56aba1862854c"},"id":"rgw23_56aba1862854c","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-8742ef76-a8b7-4d35-8b1f-28443e7159d5", "0c4b1d6fdd0ce3fc7ab481c286dd2dc448bab51c");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-8742ef76-a8b7-4d35-8b1f-28443e7159d5", "0c4b1d6fdd0ce3fc7ab481c286dd2dc448bab51c");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw24_56aba1862854c"},"id":"rgw24_56aba1862854c","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/216457235_A_SEQUENTIAL_DESIGN_FOR_APPROXIMATING_THE_PARETO_FRONT_USING_THE_EXPECTED_PARETO_IMPROVEMENT_FUNCTION","requestToken":"haJc1rC3hBkG2TGmn9VcTBErPAzncRgCAI3NRN\/sx60blekD8MMkPecgJCKEvgYFshEU7Vh6nMaLjYjthjZ3DX7YGy6bjNG3sCDB28POtbZj5KRPDRAIJPiecqYP\/fEImHRjglIEgVCUNugknx4n+EMQoFSGQ4KNwKE8deFue1Q5yyf7vz7M9biXWwzzCAMCKVaZ+ui+sU58luG4rroSJv\/TT+jO4xbTPR1MrahOHgOi+pWZ7+Uk5xykLUl0WXK6Fg5NyZxaBc2Q\/3oS4UHcwQv52t87pfVmnEV1LPxyabU=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=ZSt1GRlJ2w1havReamaQJY7NrvD4u3e65ZFbVr4tvnmkCOnhrP9ngJZEr7NtxK97","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjE2NDU3MjM1X0FfU0VRVUVOVElBTF9ERVNJR05fRk9SX0FQUFJPWElNQVRJTkdfVEhFX1BBUkVUT19GUk9OVF9VU0lOR19USEVfRVhQRUNURURfUEFSRVRPX0lNUFJPVkVNRU5UX0ZVTkNUSU9O","signupCallToAction":"Join for free","widgetId":"rgw26_56aba1862854c"},"id":"rgw26_56aba1862854c","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw25_56aba1862854c"},"id":"rgw25_56aba1862854c","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw27_56aba1862854c"},"id":"rgw27_56aba1862854c","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Thesis","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
