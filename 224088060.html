<!DOCTYPE html> <html lang="en" class="" id="rgw49_56ab19911d8dd"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="uASQoVWnRGF2owvq1enJPZ0HATznOoeg2gxuyAMB2ReZsoAmfrhx4lrVwwtzA48IfLRHTd3OF1IfaNwfkszFUAKBP985DYQ0pBSkXEgVOdOrOqjt7iNxZG1l3cGupvikrTpjyovBooMQ3fJ8AvMTVHCSLAmBBD7hDB4pJYSVCrHb0lzOgRqkobrlfVX+QiyekqRLXyBb5i36Q81rIDaF3mafTf7aZ2JPCsyLl9VEYMrG2HICv9stcOJ+S1mj4/1xG7jeaB4kkpVS4WBh3s8qaT9KRxDOdCVJbVWGNMnglAo="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-5c5615cc-686a-4f4c-95c6-017f04262cbd",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="OpenEAR - Introducing the Munich open-source emotion and affect recognition toolkit" />
<meta property="og:description" content="Various open-source toolkits exist for speech recognition and speech processing. These toolkits have brought a great benefit to the research community, i.e. speeding up research. Yet, no such..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit/links/00463514064556446b000000/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit" />
<meta property="rg:id" content="PB:224088060" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1109/ACII.2009.5349350" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="OpenEAR - Introducing the Munich open-source emotion and affect recognition toolkit" />
<meta name="citation_author" content="Florian Eyben" />
<meta name="citation_author" content="Martin Wollmer" />
<meta name="citation_author" content="Bjorn Schuller" />
<meta name="citation_conference_title" content="Affective Computing and Intelligent Interaction and Workshops, 2009. ACII 2009. 3rd International Conference on" />
<meta name="citation_publication_date" content="2009/10/12" />
<meta name="citation_firstpage" content="1" />
<meta name="citation_lastpage" content="6" />
<meta name="citation_doi" content="10.1109/ACII.2009.5349350" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Bjoern_Schuller/publication/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit/links/00463514064556446b000000.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Conference Paper');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>OpenEAR - Introducing the Munich open-source emotion and affect recognition toolkit (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: OpenEAR - Introducing the Munich open-source emotion and affect recognition toolkit on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab19911d8dd" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab19911d8dd" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab19911d8dd">  <div class="type-label"> Conference Paper   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1109%2FACII.2009.5349350&rft.atitle=OpenEAR%20-%20Introducing%20the%20Munich%20open-source%20emotion%20and%20affect%20recognition%20toolkit&rft.title=Affective%20Computing%20and%20Intelligent%20Interaction%20and%20Workshops%2C%202009.%20ACII%202009.%203rd%20International%20Conference&rft.jtitle=Affective%20Computing%20and%20Intelligent%20Interaction%20and%20Workshops%2C%202009.%20ACII%202009.%203rd%20International%20Conference&rft.date=2009&rft.pages=1%20-%206&rft.au=Florian%20Eyben%2CMartin%20Wollmer%2CBjorn%20Schuller&rft.genre=inProceedings"></span> <h1 class="pub-title" itemprop="name">OpenEAR - Introducing the Munich open-source emotion and affect recognition toolkit</h1> <meta itemprop="headline" content="OpenEAR - Introducing the Munich open-source emotion and affect recognition toolkit">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit/links/00463514064556446b000000/smallpreview.png">  <div id="rgw7_56ab19911d8dd" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab19911d8dd"> <a href="researcher/18998136_Florian_Eyben" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Florian Eyben" alt="Florian Eyben" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Florian Eyben</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw9_56ab19911d8dd">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/18998136_Florian_Eyben"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Florian Eyben" alt="Florian Eyben" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/18998136_Florian_Eyben" class="display-name">Florian Eyben</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56ab19911d8dd"> <a href="researcher/75181895_Martin_Wollmer" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Martin Wollmer" alt="Martin Wollmer" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Martin Wollmer</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw11_56ab19911d8dd">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/75181895_Martin_Wollmer"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Martin Wollmer" alt="Martin Wollmer" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/75181895_Martin_Wollmer" class="display-name">Martin Wollmer</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56ab19911d8dd" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Bjoern_Schuller" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272368136355886%401441949032980_m/Bjoern_Schuller.png" title="Björn Schuller" alt="Björn Schuller" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Björn Schuller</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw13_56ab19911d8dd" data-account-key="Bjoern_Schuller">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Bjoern_Schuller"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272368136355886%401441949032980_l/Bjoern_Schuller.png" title="Björn Schuller" alt="Björn Schuller" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Bjoern_Schuller" class="display-name">Björn Schuller</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Imperial_College_London" title="Imperial College London">Imperial College London</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">  <div> Inst. for Human-Machine Commun., Tech. Univ. Munchen, Munich, Germany </div>      DOI:&nbsp;10.1109/ACII.2009.5349350     Conference: Affective Computing and Intelligent Interaction and Workshops, 2009. ACII 2009. 3rd International Conference on      <div class="pub-source"> Source: <a href="http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=5349350" rel="nofollow">IEEE Xplore</a> </div>  </div> <div id="rgw14_56ab19911d8dd" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>Various open-source toolkits exist for speech recognition and speech processing. These toolkits have brought a great benefit to the research community, i.e. speeding up research. Yet, no such freely available toolkit exists for automatic affect recognition from speech. We herein introduce a novel open-source affect and emotion recognition engine, which integrates all necessary components in one highly efficient software package. The components include audio recording and audio file reading, state-of-the-art paralinguistic feature extraction and plugable classification modules. In this paper we introduce the engine and extensive baseline results. Pre-trained models for four affect recognition tasks are included in the openEAR distribution. The engine is tailored for multi-threaded, incremental on-line processing of live input in real-time, however it can also be used for batch processing of databases.</div> </p>  </div>  </div>   </div>      <div class="action-container"> <div id="rgw15_56ab19911d8dd" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw29_56ab19911d8dd">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw41_56ab19911d8dd">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Bjoern_Schuller/publication/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit/links/00463514064556446b000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Bjoern_Schuller">Björn Schuller</a>   </span>  </div>  <div class="social-share-container"><div id="rgw43_56ab19911d8dd" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw44_56ab19911d8dd" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw45_56ab19911d8dd" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw46_56ab19911d8dd" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw47_56ab19911d8dd" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw48_56ab19911d8dd" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw42_56ab19911d8dd" src="https://www.researchgate.net/c/o1o9o3/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FBjoern_Schuller%2Fpublication%2F224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit%2Flinks%2F00463514064556446b000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw28_56ab19911d8dd"  itemprop="articleBody">  <p>Page 1</p> <p>openEAR - Introducing the Munich Open-Source<br />Emotion and Affect Recognition Toolkit<br />Florian Eyben, Martin W¨ ollmer, and Bj¨ orn Schuller<br />Technische Universit¨ at M¨ unchen, Institute for Human-Machine Communication<br />Theresienstrasse 90, 80333 M¨ unchen<br />{eyben|woellmer|schuller}@tum.de<br />Abstract<br />Various open-source toolkits exist for speech recognition<br />and speech processing. These toolkits have brought a great<br />benefit to the research community, i.e. speeding up research.<br />Yet, no such freely available toolkit exists for automatic af-<br />fect recognition from speech. We herein introduce a novel<br />open-source affect and emotion recognition engine, which<br />integrates all necessary components in one highly efficient<br />software package. The components include audio recording<br />and audio file reading, state-of-the-art paralinguistic fea-<br />ture extraction and plugable classification modules. In this<br />paper we introduce the engine and extensive baseline re-<br />sults. Pre-trained models for four affect recognition tasks<br />are included in the openEAR distribution. The engine is tai-<br />lored for multi-threaded, incremental on-line processing of<br />live input in real-time, however it can also be used for batch<br />processing of databases.<br />1. Introduction<br />Affective Computing has become a popular area of re-<br />search in recent times [17]. Many achievements have been<br />made towards making machines detect and understand hu-<br />man affective states, such as emotion, interest or dialogue<br />role. Yet, in contrast to the field of speech recognition, only<br />very few software toolkits exist, which are tailored specif-<br />ically for affect recognition from audio or video. In this<br />paper, we introduce and describe the Munich open Affect<br />Recognition Toolkit (openEAR), the first such tool, which<br />runs on multiple platforms and is publicly available1.<br />OpenEAR in it’s initial version is introduced as an affect<br />and emotion recognition toolkit for audio and speech affect<br />recognition. However, openEAR’s architecture is modular<br />and by principle modality independent. Thus, also vision<br />features such as facial points or optical flow measures can<br />1http://sourceforge.net/projects/openear<br />be added and fused with audio features. Moreover, phys-<br />iological features such as heart rate, ECG, or EEG signals<br />from devices such as the Neural Impulse Actuator (NIA),<br />can be analysed using the same methods and algorithms<br />as for speech signals and thus can also be processed us-<br />ing openEAR – provided suitable capture interfaces and<br />databases.<br />2. Existing work<br />A few free toolkits exist, that provide various compo-<br />nents usable for emotion recognition. Most toolkits that in-<br />clude feature extraction algorithms are targeted at speech<br />recognition and speech processing, such as the Hidden<br />Markov Toolkit (HTK) [16], the PRAAT Software [1], the<br />Speech Filling System (SFS) from UCL, and the SNACK<br />package for the Tcl scripting language. These can all be<br />used to extract state-of-the-art features for emotion recog-<br />nition. However, only PRAAT and HTK include certain<br />classifiers. For further classifiers WEKA and RapidMiner,<br />for example, can be used. Moreover, only few of the listed<br />toolkits are available under a permissive Open-Source li-<br />cense, e.g. WEKA, PRAAT, and RapidMiner.<br />The most complete and task specific framework for<br />Emotion Recognition currently is EmoVoice [13]. How-<br />ever, the main design objective is to provide an emotion<br />recognition system for the non-expert. Thus it is a great<br />framework for demonstrator applications and making emo-<br />tion recognition available to the non-expert. openEAR, in<br />contrast, aims at being a stable and efficient set of tools<br />for researchers and those developing emotional aware ap-<br />plications, providing the elementary functionality for emo-<br />tion recognition, i.e. the Swiss Army Knife for research and<br />development of affect aware applications. openEAR com-<br />bines everything from audio recording, feature extraction,<br />and classification to evaluation of results, and pre-trained<br />models while being very fast and highly efficient. All fea-<br />ture extractor components are written in C++ and can be<br />used as a library, facilitating integration into custom appli-<br />978-1-4244-4799-2/09/$25.00 c ?2009 IEEE</p>  <p>Page 2</p> <p>cations. Also, openEAR can be used as an out-of-the-box<br />emotion live affect recogniser for various domains, using<br />pre-trained models which are included in the distribution.<br />Moreover, openEAR is Open-Source software, freely avail-<br />able to anybody under the terms of the GNU General Public<br />License.<br />3. openEAR’s Architecture<br />The openEAR toolkit consists of three major compo-<br />nents: the core component is the SMILE (Speech and Mu-<br />sic Interpretation by Large-Space Extraction) signal pro-<br />cessing and feature extraction tool, which is capable gen-<br />erating &gt; 500k features in real-time (Real-Time Factor<br />(RTF) &lt; 0.1), either from live audio input or from off-<br />line media. Next, there is support for classification mod-<br />ules via a plug-in interface to the feature extractor. More-<br />over, supporting scripts and tools are provided, which fa-<br />cilitate training of own models on arbitrary data sets. Fi-<br />nally, four ready-to-use model-sets are provided for recog-<br />nition of six basic emotion categories (trained on the Berlin<br />Speech Emotion Database (EMO-DB) [2] and the eNTER-<br />FACE database), for recognition of emotion in a continu-<br />ous three-dimensional feature space spanned by activation,<br />valence, and dominance (trained on the Belfast naturalistic<br />(SAL) and Vera-am-Mittag (VAM) [4] corpora), for recog-<br />nition of interest using three discrete classes taken from the<br />Audio Visual Interest Corpus (AVIC) [9], and for recogni-<br />tion of affective states such as drunkenness trained on the<br />Airplane Behaviour Corpus (ABC).<br />Signalinputcaneitherbereadoff-linefromaudiofilesor<br />recorded on-line from a capture device in real-time. Since<br />data processing is incremental (concerning signal process-<br />ing and feature extraction), there is no difference between<br />handling live input and off-line media. Independent of the<br />input method, the feature output can either be classified di-<br />rectly via built in classifiers, classifier plug-ins, or the fea-<br />tures (or even wave data) can be exported to various file<br />formats used by other popular toolkits. Currently imple-<br />mented export file formats are: WEKA Arff [14], LibSVM<br />format [3], Comma Separated Value (CSV) File, and Hid-<br />den Markov Toolkit (HTK) [16] feature files.<br />The following sub-sections describe the feature extrac-<br />tor’s modular architecture, the features currently imple-<br />mented, and the classifier interface. The model-sets will be<br />detailed along with baseline benchmark results in section 4.<br />3.1. Modular and Efficient Implementation<br />During specification of openEAR’s feature extractor ar-<br />chitecture, three main objectives were followed: speed and<br />efficiency, incremental processing of data (i.e. frame by<br />frame with minimum delay), and flexibility and modular-<br />ity. Adding new features is possible via an easy plug-in<br />interface.<br />The SMILE feature extractor is implemented from<br />scratch in C++, without crucial third party dependencies.<br />Thus, it is easy to compile, and basically platform inde-<br />pendent. It is currently known to run on Mac OS, vari-<br />ous Linux distributions, and Windows platforms. Feature<br />extraction code is optimised to avoid double computations<br />of shared values, e.g. Fast Fourier Transform (FFT) coeffi-<br />cients, which are only computed once and used for multiple<br />algorithms such as computation of energy, spectral features,<br />and cepstral features.<br />Figure 1. Concept and components of openEAR’s SMILE (Speech<br />and Music Interpretation by Large-Space Extraction) feature ex-<br />tractor.<br />Figure 1 shows a rough sketch of the data flow and sig-<br />nal processing architecture. The central component is the<br />Data Memory, which enables memory efficient incremen-<br />tal processing by managing ring-buffer storage of feature<br />data. Input data (wave files, other features, etc.) is fed to the<br />Data Memory by Data Source components, which contain<br />a Data Writer sub-component that handles the data mem-<br />ory interface. Data Processor components read data frames<br />or contours from one location of the Data Memory, pro-<br />cess the data and write new frames to a different location in<br />the Data Memory. They contain both a Data Reader and a<br />Data Writer sub-component, which handle the Data Mem-<br />ory interface. Finally, the Data Sink components read data<br />from the Data Memory and feed it to the classifier compo-<br />nents or write data to files. Each component can be run in a<br />separate thread, speeding up processing on multiple proces-<br />sors/cores.<br />The individual components can be freely instantiated,<br />configured, and connected to the Data Memory via a central<br />configuration file. To facilitate configuration file creation<br />example files are provided and configuration file conversion</p>  <p>Page 3</p> <p>scripts are included.<br />3.2. Features<br />The SMILE feature extraction tool is capable of ex-<br />tracting low-level audio features (Low-Level Descriptors<br />(LLD)) and applying various statistical functionals and<br />transformations to those features. The Low-Level Descrip-<br />tors currently implemented are listed in table 1. We hope to<br />extend this list by numerous advanced and state-of-the-art<br />features in the near future, such as Voice Quality Parameters<br />(e.g. [6]), alternative pitch detection algorithms, e.g. pitch<br />by Harmonic Product Spectrum, combination of Average<br />Magnitude Difference with Autocorrelation, and smooth-<br />ing of pitch contours via a Viterbi algorithm or ESPS pitch<br />tracker. Moreover, features such as TEAGER energy or fur-<br />ther Auditory Features are considered for integration.<br />Feature Group<br />Signal energy<br />FFT-Spectrum<br />Mel-Spectrum<br />Cepstral<br />Pitch<br />Features in Group<br />Root mean-square &amp; logarithmic<br />Bins 0-Nfft<br />Bins 0-Nmel<br />MFCC 0-Nmfcc<br />Fundamentalfrequency<br />ACF, in Hz, Bark and closest<br />semitone.<br />Probability of voicing (ACF(T0)<br />Harmonics-to-noise ratio<br />LPC Coefficients<br />Perceptual Linear Predictive Coef-<br />ficients<br />Formants and Bandwidth computed<br />from LPC analysis<br />Zero-crossing-rate,<br />value, minimum value, DC<br />Energy in bands 0-250Hz, 0-<br />650Hz, 250-650Hz, 1-4kHz, and<br />custom<br />N% roll-off point, centroid, flux,<br />and rel. pos. of spectrum max. and<br />min.<br />CHROMA (warped semitone filter-<br />bank), CENS<br />Comb-filter bank<br />F0<br />via<br />ACF(0))<br />Voice Quality<br />LPC<br />PLP<br />Formants<br />Time Signalmaximum<br />Spectral<br />Musical<br />Table 1. Low-Level Descriptors implemented in openEAR’s<br />SMILE feature extractor.<br />The Mel frequency features, Mel-Spectrum and Mel-<br />Frequency Cepstral Coefficients (MFCCs) are computed<br />exactly as described in [16], thus providing compatibility<br />to the Hidden Markov Toolkit and existing models trained<br />on HTK MFCCs. Harmonics-To-Noise Ratio computation<br />is based on equation 1, where T0is the pitch period.<br />HNRt= 10 · log<br />ACF(T0)<br />ACF(0) − ACF(T0)<br />S) at time t is computed via equation 2.<br />(1)<br />Spectral centroid (Ct<br />Xt(f) is the spectral magnitude at time t in bin f.<br />Ct<br />S=<br />?<br />∀ff · Xt(f)<br />·?<br />∀fXt(f)<br />(2)<br />Spectral Flux Ft<br />tion 3, whereby Etis the energy of the frame at time t.<br />?<br />N<br />f=1<br />Sfor N FFT bins is computed via equa-<br />Ft<br />S=<br />?<br />?<br />?1<br />N<br />?<br />?Xt(f)<br />Et<br />−Xt−1(f)<br />Et−1<br />?2<br />(3)<br />The p percent Spectral Roll-Off is determined as the fre-<br />quency (or FFT bin) below which p percent of the total<br />signal energy fall. All frequencies, i.e. for Centroid and<br />Roll-Offs are normalised to 1000Hz.<br />Delta regression coefficients dtof arbitrary order can be<br />computed from any LLD contour (xt) using equation 4,<br />where W specifies half the size of the window to be used<br />for computation of the regression coefficients. The default<br />is W = 2. In order to provide HTK compatibility, equa-<br />tion 4 was implemented as described in [16]. Typically only<br />the first and second order δ-coefficients are used, which is<br />also the default setting in openEAR.<br />?W<br />Table 2 lists the statistical functionals, regression coef-<br />ficients and transformations currently implemented. They<br />can be applied to the LLD to map a time sequence of vari-<br />able length to a static feature vector. This procedure is<br />the common technique in related emotion and affect recog-<br />nition work (cf. [7]). Moreover, hierarchical functionals<br />can be computed as “functionals of functionals” (cf. [11]),<br />which helps improve robustness against single outliers, for<br />example. Thereby there is no limit as to how many hierar-<br />chies can be computed, except by computing resources such<br />as memory and processing time.<br />As with the LLD we aim at implementing even more<br />functionals which can be applied to LLD contours or func-<br />tional contours, in order to facilitate systematic feature gen-<br />eration. Due to the modular architecture of the feature ex-<br />tractor, it will also be possible to apply any implemented<br />processing algorithm to any time series, i.e. the Mel-band<br />filter-bank could be applied as a functional to any LLD con-<br />tour. This gives researchers an efficient and customisable<br />tool to generate millions of features in order to find optimal<br />feature sets which represent affective information.<br />dt=<br />i=1i ·?xt+i− xt−i?<br />2?W<br />i=1i2<br />(4)</p>  <p>Page 4</p> <p>Functionals, etc.<br />Max./min. val. and respective rel. position<br />Range (max.-min.)<br />Arithmetic, quadratic, and geometric mean<br />Arth. mean of abs˙ and non-zero val.<br />%-age of non-zero val. wrt. tot. # of val. in contour<br />Max. and min. value - arithmetic mean<br />Quartiles and inter-quartile ranges<br />N% percentiles<br />Std. deviation, variance, kurtosis, skewness<br />Centroid of LLD contour<br />Zero-crossing and mean-crossing rate<br />25% Down-Level Time, 75% Up-Level Time,<br />Rise-Time, Fall-Time<br /># of peaks, mean dist. btwn. peaks, arth. mean of<br />peaks, arth. mean of peaks - overall arth. mean<br />Number of segments based on δ-thresholding<br />Linear reg. coefficients and corresp. approx. err.<br />Quadraticreg.coefficientsandcorresp.approx.err.<br />Discrete Cosine Transf. (DCT) coefficients 0-N<br />#<br />4<br />1<br />3<br />2<br />1<br />2<br />6<br />(N)<br />4<br />1<br />2<br />4<br />4<br />1<br />4<br />5<br />(6)<br />Table 2. Statistical functionals, regression coefficients and trans-<br />formations currently implemented in openEAR’s feature extractor.<br />3.3. Classification and Data Output<br />As mentioned in section 3.1, multiple Data Sinks can<br />be present in the feature extractor, feeding data to different<br />classifiers. There exists an implementation of a K-Nearest<br />Neighbour classifier, a Bayes classifier, and a module for<br />Support-Vector classification and regression using the effi-<br />cient and freely available LibSVM [3]. We further plan to<br />implement Discriminant Multinomial Naive Bayes [12] and<br />Long Short-Term Memory Recurrent Neural Networks [5].<br />Supporting scripts written in Perl (in order to be plat-<br />form independent) facilitate batch processing of data-sets.<br />As features can be saved in various data-formats, custom<br />experiments can easily be conducted using e.g. WEKA [14]<br />or HTK [16]. Calling of various WEKA functions from the<br />command-line, e.g. for feature selection or classification, is<br />also included among openEAR’s Perl scripts.<br />A benefit of openEAR’s modular feature extractor archi-<br />tecture is that practically any internal data can be accessed<br />and output to various formats simply by connecting a data<br />sink to the specific location in the data memory. The ex-<br />ported data can be used for visualisation purposes, for ex-<br />ample. This is helpful in finding extractor parametrisation<br />problems, or for common sense checks to see if everything<br />is working as expected. Visualising the internal data can<br />also be a helpful means for teaching and understanding the<br />process of extracting various feature types. Exemplary plots<br />of some selected LLD can be found in figure 3.3.<br />laughing if I take my mind off<br />Figure 2. Plots of selected LLD for the spoken utterance: “laugh-<br />ing, if I take my mind off ...” (SAL database, male speaker). Top<br />to bottom: original wave (3s), norm. RMS energy, probability of<br />voicing, norm. zero-crossing rate, rel. freq. of spec. max.<br />4. Benchmarks<br />We now show benchmark results obtained on popu-<br />lar databases for speech emotion and interest recognition.<br />Freely available models for all databases used for bench-<br />marking are distributed with openEAR. Moreover, compu-<br />tation time performance of openEAR’s SMILE feature ex-<br />tractor is evaluated. The results show, that openEAR yields<br />state-of-the-art recognition results with very low computa-<br />tional demand.<br />The followingsub-section<br />databases, before the results are discussed in section 4.2.<br />briefly describesthe<br />4.1. Databases<br />Six databases as listed in table 3 are used for benchmark-<br />ing openEAR and for which freely available model sets are<br />distributed with openEAR: the first three contain discrete<br />class labels for emotion, namely the Berlin Speech Emo-<br />tion Database (EMO-DB) [2], containing seven classes of<br />basic emotions (Anger, Fear, Happiness, Disgust, Boredom,<br />Sadness, Neutral), the eNTERFACE corpus with six emo-<br />tion categories (Anger, Disgust, Fear, Happiness, Sadness,<br />and Surprise), the ABC corpus with the classes (Aggres-<br />sive, Cheerful, Intoxicated, Nervous, Neutral, Tired), and<br />the Audio Visual Interest Corpus (AVIC) [9] with labels for<br />three levels of interest (-1: disinterest, 0: normal, and 1:<br />high interest). The last two databases contain continuous<br />dimensional labels for valence and activation in the range<br />from -1 to +1. These are the SAL corpus and the VAM<br />corpus [4]. The latter also has labels for the potency di-</p>  <p>Page 5</p> <p>mension. However, we found this dimension to be highly<br />correlated to activation (Corr. Coeff. 0.9). Thus, we did not<br />consider it in the evaluations. When viewing the results in<br />Database<br />ABC<br />AVIC<br />EMO-DB<br />eNTERFACE<br />SAL<br />VAM<br /># turns<br />431<br />996<br />494<br />1170<br />1692<br />947<br />discrete<br />?<br />?<br />?<br />?<br />continuous<br />?<br />?<br />Table 3. Six databases for benchmarking openEAR and generation<br />of model sets included in the openEAR distribution.<br />the following section it has to be considered that ABC, eN-<br />TERFACE, and EMO-DB contain acted and prototypical<br />emotions, while AVIC, SAL, and VAM contain natural and<br />spontaneous data. Due to the dimensional annotation for<br />SAL and VAM all recorded turns are left in the database,<br />not only prototypical turns.<br />4.2. Recognition Results<br />Table 4 shows results obtained for discrete class emo-<br />tion and interest (on AVIC) recognition. For all bench-<br />marks the feature set 5,967 tF as described in table 6 was<br />extracted. A correlation based feature subset (CFS) selec-<br />tion was performed in order to find relevant feature sets for<br />each database. However, better results with the full feature<br />set were achieved on EMO-DB, eNTERFACE, and ABC.<br />Thus, the best results without feature selection are shown<br />there. All the benchmark results are well in line with or<br />even above the current state-of-the-art.<br />Recall [%]<br />ABC (6 emo. rel. states)<br />AVIC (3 levels of interest)<br />EMO-DB (7 emotions)<br />eNTERFACE (6 emotions)<br />WA<br />71.9<br />74.5<br />89.5<br />75.2<br />UA<br />66.5<br />70.4<br />88.8<br />75.1<br />Table 4. Results obtained for discrete class emotion recognition<br />using Support-Vector Machines with polynomial kernel function<br />of degree 1. 10-fold Stratified Cross-Validation. Weighted average<br />(WA) and unweighted average (UA) of class-wise recall rates as<br />demanded in [10].<br />Finally, table 5 shows the results for the two most chal-<br />lenging tasks, the dimensional estimation of naturalistic<br />emotions. Results obtained for VAM are slightly better<br />than those reported for polynomial kernel SVM in [4]. Re-<br />sults for SAL are obtained on the same data-set partitions<br />as in [15].<br />Database<br />SAL (train/test)<br />VAM (10-f. SCV)<br />CCa<br />0.24<br />0.83<br />MLEa<br />0.28<br />0.15<br />CCv<br />0.15<br />0.42<br />MLEv<br />0.38<br />0.14<br />Table 5. Results obtained for continuous emotion recognition for<br />two dimensions (activation and valence) using Support-Vector Re-<br />gression (polynomial kernel function, degree 1). Results reported<br />are Correlation Coefficient (CC) and Mean absolute (linear) error<br />(MLE) for activation (a) and valence (v). VAM: 10-fold Strat-<br />ified Cross-Validation (SCV) after CFS feature selection. SAL:<br />pre-defined train/test sets, CFS feature selection on training set.<br />Feature Set<br />36 MFCCde<br />102 LLD<br />Description<br />MFCC 0-12, first and second order δ<br />LLD pitch, time, spectral, mfcc, and en-<br />ergy + first and second order δ<br />43 functionals (applied to complete in-<br />put) of 39 LLD + first and second order<br />δ. (No percentile functionals)<br />51 functionals (applied to complete in-<br />put) of 39 LLD + first and second order<br />δ<br />43 functionals (applied to 2s windows<br />w. 50% overlap) of 39 LLD + first and<br />second order δ. (No percentile function-<br />als)<br />51 functionals (applied to 2s windows<br />w. 50% overlap) of 39 LLD + first and<br />second order δ<br />43 functionals applied to set 5,031 2sF<br />51 functionals applied to set 5,967 2sF<br />5,031 tF<br />5,967 tF<br />5,031 2sF<br />5,967 2sF<br />216k tHRF<br />304k tHRF<br />Table 6. Feature-sets for openEAR feature extractor computation<br />time evaluation and benchmark results.<br />4.3. Computation Time<br />Since one objective of openEAR is efficiency and real-<br />time operation, we now provide run-time benchmarks for<br />various feature sets, which are summarised in table 6. Com-<br />putation time is evaluated under Ubuntu Linux on a AMD<br />Phenom 64bit CPU at 2.2GHz. All components are run in<br />a single thread for benchmarking.<br />Table 7 shows the computation time and the Real-Time<br />Factor (RTF) for extraction of various feature sets.<br />5. Conclusion and Outlook<br />We introduced openEAR, an efficient, open-source,<br />multi-threaded, real-time emotion recognition framework<br />providing an extensible, platform independent feature ex-<br />tractor implemented in C++, pre-trained models on six<br />databases which are ready-to-use for on-line emotion and<br />affect recognition, and supporting scripts for model build-<br />ing, evaluation, and visualisation. The framework is com-</p>  <p>Page 6</p> <p>Feature Set<br />36 MFCCde<br />102 LLD<br />5,031 tF<br />5,031 2sF<br />216k tHRF<br />Comp. time [s] RTF<br />0.003<br />0.009<br />0.012<br />0.014<br />0.018<br />1.3<br />4.4<br />6.1<br />7.2<br />9.2<br />Table 7. openEAR’s computation time and real-time factor (RTF)<br />for feature extraction of 8 minutes and 27 seconds 16-bit mono<br />audio sampled at 16kHz. LLD for all above feature sets computed<br />for 25ms frames at a rate of 10ms. CPU: AMD Phenom, 2.2GHz.<br />Single thread processing.<br />patible with related tool-kits, such as HTK and WEKA by<br />supporting their data-formats. The current implementation<br />was successfully evaluated on six affective speech corpora,<br />showing state-of-the-art performance. Moreover, features<br />for the Interspeech 2009 Emotion Challenge [10] were ex-<br />tracted with openEAR.<br />Development of openEAR is still in progress and more<br />features will be added soon. Due to it’s modular architec-<br />ture and the public source code, rapid addition of new, ad-<br />vanced features by the community is hopefully encouraged.<br />Although openEAR already is a fully featured emo-<br />tion and affect recognition toolkit, it can also be used for<br />other tasks such as classification of non-linguistic vocali-<br />sations [8]. In the future, decoders for continuous speech<br />recognition and linguistic features will be integrated into<br />openEAR, resulting in a highly efficient and comprehensive<br />affect recognition engine.<br />6. Acknowledgment<br />The research leading to these results has received fund-<br />ing from the European Community’s Seventh Framework<br />Programme (FP7/2007-2013) under grant agreement No.<br />211486 (SEMAINE).<br />References<br />[1] P. Boersma and D. Weenink.<br />computer (version 4.3.14). http://www.praat.org/,<br />2005.<br />[2] F. Burkhardt, A. Paeschke, M. Rolfes, W. Sendlmeier, and<br />B. Weiss. A database of german emotional speech. In Pro-<br />ceedings Interspeech 2005, Lissabon, Portugal, pages 1517–<br />1520, 2005.<br />[3] C.-C. Chang and C.-J. Lin.<br />port vector machines, 2001. Software available at http:<br />//www.csie.ntu.edu.tw/˜cjlin/libsvm.<br />[4] M. Grimm, K. Kroschel, and S. Narayanan. Support vector<br />regression for automatic recognition of spontaneous emo-<br />tions in speech. In International Conference on Acoustics,<br />Speech and Signal Processing, 2007., volume 4, pages IV–<br />1085–IV. IEEE, April 2007.<br />Praat: doing phonetics by<br />LibSVM: a library for sup-<br />[5] S. Hochreiter and J. Schmidhuber. Long short-term memory.<br />Neural Computation, 9(8):1735–1780, 1997.<br />[6] M. Lugger and B. Yang.<br />stageemotionclassificationexploitingvoicequalityfeatures.<br />In F. Mihelic and J. Zibert, editors, Speech Recognition,<br />page 1. IN-TECH, November 2008.<br />[7] B. Schuller, A. Batliner, D. Seppi, S. Steidl, T. Vogt, J. Wag-<br />ner, L. Devillers, L. Vidrascu, N. Amir, L. Kessous, and<br />V. Aharonson. The relevance of feature type for the auto-<br />matic classification of emotional user states: Low level de-<br />scriptors and functionals. In Proc. INTERSPEECH 2007,<br />pages 2253–2256, Antwerp, Belgium, 2007.<br />[8] B. Schuller, F. Eyben, and G. Rigoll. Static and dynamic<br />modelling for the recognition of non-verbal vocalisations in<br />conversational speech. In E. Andr´ e, editor, Proc. IEEE PIT<br />2008, volume LNCS 5078, pages 99–110. Springer, 2008.<br />16.-18.06.2008.<br />[9] B. Schuller, R. M¨ uller, F. Eyben, J. Gast, B. H¨ ornler,<br />M. W¨ ollmer, G. Rigoll, A. H¨ othker, and H. Konosu. Being<br />bored? recognising natural interest by extensive audiovisual<br />integration for real-life application. to appear in Image and<br />Vision Computing Journal (IMAVIS), Special Issue on Visual<br />and Multimodal Analysis of Human Spontaneous Behavior,<br />Elsevier, page 17 pages, 2009.<br />[10] B. Schuller, S. Steidl, and A. Batliner. The interspeech 2009<br />emotion challenge. In Interspeech (2009), ISCA, Brighton,<br />UK, 2009.<br />[11] B. Schuller, M. Wimmer, L. M¨ osenlechner, C. Kern, D. Ar-<br />sic, and G. Rigoll. Brute-forcing hierarchical functionals for<br />paralinguistics: A waste of feature space? In Proceedings of<br />ICASSP 2008, Las Vegas, Nevada, USA, April 2008.<br />[12] J. Su, H. Zhang, C. X. Ling, and S. Matwin.<br />native parameter learning for bayesian networks. In ICML<br />’08: Proceedings of the 25th international conference on<br />Machine learning, pages 1016–1023, New York, NY, USA,<br />2008. ACM.<br />[13] T. Vogt, E. Andr´ e, and N. Bee. Emovoice - a framework for<br />online recognition of emotions from voice. In Proc. IEEE<br />PIT 2008, volume 5078 of LNCS, pages 188–199. Springer,<br />June 2008. feature extraction, emotion recognition, GUI.<br />[14] I. H. Witten and E. Frank. Data Mining: Practical machine<br />learning tools and techniques. Morgan Kaufmann, San Fran-<br />cisco, 2nd edition edition, 2005.<br />[15] M. W¨ ollmer, F. Eyben, S. Reiter, B. Schuller, C. Cox,<br />E. Douglas-Cowie, and R. Cowie.<br />classes - towards continuous emotion recognition with mod-<br />elling of long-range dependencies.<br />speech, Brisbane, Australia, 2008.<br />[16] S. Young, G. Evermann, M. Gales, T. Hain, D. Ker-<br />shaw, X. Liu, G. Moore, J. Odell, D. Ollason, D. Povey,<br />V. Valtchev, and P. Woodland. The HTK book (v3.4). Cam-<br />bridge University Press, Cambridge, UK, December 2006.<br />[17] Z.Zeng, M.Pantic, G.I.Rosiman, andT.S.Huang. Asurvey<br />of affect recognition methods: Audio, visual, and sponta-<br />neous expressions. Trans. on Pattern Analysis and Machine<br />Intelligence, 31(1):39–58, 2009.<br />Psychological motivated multi-<br />Discrimi-<br />Abandoning emotion<br />In Proceedings Inter-</p>  <a href="https://www.researchgate.net/profile/Bjoern_Schuller/publication/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit/links/00463514064556446b000000.pdf">Download full-text</a> </div> <div id="rgw20_56ab19911d8dd" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw21_56ab19911d8dd">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw22_56ab19911d8dd"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Bjoern_Schuller/publication/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit/links/00463514064556446b000000.pdf" class="publication-viewer" title="09eyb1.pdf">09eyb1.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Bjoern_Schuller">Björn Schuller</a> &middot; Jan 20, 2016 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw23_56ab19911d8dd"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://www.mmk.ei.tum.de/publ/pdf/09/09eyb1.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="OpenEAR - Introducing the Munich open-source emotion and affect recognition toolkit">OpenEAR - Introducing the Munich open-source emoti...</a> </div>  <div class="details">   Available from <a href="http://www.mmk.ei.tum.de/publ/pdf/09/09eyb1.pdf" target="_blank" rel="nofollow">tum.de</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw30_56ab19911d8dd" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (154) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw31_56ab19911d8dd" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw32_56ab19911d8dd" >  <div class="indent-left">  <div id="rgw33_56ab19911d8dd" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/275208397_Speaker-Independent_Speech_Emotion_Recognition_using_Gaussian_and_SVM_Classifiers">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Dryousaf_Khan" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Dr.Yousaf Khan </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw34_56ab19911d8dd">  <li class="citation-context-item"> "We extracted 384 audio features that were used in the Interspeech 2009 Emotion Challenge (Schuller et al., 2009) using the openEAR toolkit (Eyben et al., 2009). The feature set consisted of low-level descriptors (LLD) including fundamental frequency (f 0 ), energy, Mel frequency cepstral coefficients (MFCCs), zero crossing rate (ZCR) and line spectral frequencies (lsf). " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/275208397_Speaker-Independent_Speech_Emotion_Recognition_using_Gaussian_and_SVM_Classifiers"> <span class="publication-title js-publication-title">Speaker-Independent Speech Emotion Recognition using Gaussian and SVM Classifiers,</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/84871601_Yousaf_Khan" class="authors js-author-name ga-publications-authors">Yousaf Khan</a> &middot;     <a href="researcher/2080911976_S_Haq" class="authors js-author-name ga-publications-authors">S. Haq</a> &middot;     <a href="researcher/2034213230_A_Ali" class="authors js-author-name ga-publications-authors">A. Ali</a> &middot;     <a href="researcher/2007832441_M_Asif" class="authors js-author-name ga-publications-authors">M. Asif</a> &middot;     <a href="researcher/2080902083_T_Jan" class="authors js-author-name ga-publications-authors">T. Jan</a> &middot;     <a href="researcher/2092734913_Naveedahmad" class="authors js-author-name ga-publications-authors">Naveedahmad</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> This paper presents a novel technique of human emotion recognition using the audio modality for the speaker-independent task. In order to achieve a high emotion classification performance a standard set of audio features were extracted. The feature selection was performed using the Plus l-Take Away r algorithm based on Bhattacharyya distance criterion. The feature selection was followed by feature reduction using PCA and LDA, and classification using the Gaussian and SVM classifiers. The emotion classification performance better or comparable to state-of-the art techniques and humans were achieved on the standard Berlin emotional speech database.
Keywords: Audio Emotion Recognition, Feature Selection, Principal Component Analysis, Gaussian and SVM Classifiers </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Jan 2015  &middot; Boston University journal  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Dryousaf_Khan/publication/275208397_Speaker-Independent_Speech_Emotion_Recognition_using_Gaussian_and_SVM_Classifiers/links/5534da330cf2df9ea6a3e89e.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw35_56ab19911d8dd" >  <div class="indent-left">  <div id="rgw36_56ab19911d8dd" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/269636311_A_Broadcast_News_Corpus_for_Evaluation_and_Tuning_of_German_LVCSR_Systems">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Felix_Weninger" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Felix Weninger </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw37_56ab19911d8dd">  <li class="citation-context-item"> "A baseline set contained Mel Frequency Cepstral Coefficients (MFCC) 1–12 and signal log-energy, and their first (δ) and second order regression coefficients (δδ), which were extracted using the openEAR feature extractor [10], and are identical to the features extracted by the HTK toolkit [11]. Next, this feature set was augmented by 12 Line Spectral Pairs (LSP), along with their δ and δδ coefficients, computed with openEAR. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/269636311_A_Broadcast_News_Corpus_for_Evaluation_and_Tuning_of_German_LVCSR_Systems"> <span class="publication-title js-publication-title">A Broadcast News Corpus for Evaluation and Tuning of German LVCSR Systems</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/29537335_Felix_Weninger" class="authors js-author-name ga-publications-authors">Felix Weninger</a> &middot;     <a href="researcher/18998135_Bjoern_Schuller" class="authors js-author-name ga-publications-authors">Björn Schuller</a> &middot;     <a href="researcher/18998136_Florian_Eyben" class="authors js-author-name ga-publications-authors">Florian Eyben</a> &middot;     <a href="researcher/48001256_Martin_Woellmer" class="authors js-author-name ga-publications-authors">Martin Wöllmer</a> &middot;     <a href="researcher/7708667_Gerhard_Rigoll" class="authors js-author-name ga-publications-authors">Gerhard Rigoll</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Transcription of broadcast news is an interesting and challenging application
for large-vocabulary continuous speech recognition (LVCSR). We present in
detail the structure of a manually segmented and annotated corpus including
over 160 hours of German broadcast news, and propose it as an evaluation
framework of LVCSR systems. We show our own experimental results on the corpus,
achieved with a state-of-the-art LVCSR decoder, measuring the effect of
different feature sets and decoding parameters, and thereby demonstrate that
real-time decoding of our test set is feasible on a desktop PC at 9.2% word
error rate. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Dec 2014  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Felix_Weninger/publication/269636311_A_Broadcast_News_Corpus_for_Evaluation_and_Tuning_of_German_LVCSR_Systems/links/551d65340cf252bc3a87a822.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw38_56ab19911d8dd" >  <div class="indent-left">  <div id="rgw39_56ab19911d8dd" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/281276446_Emotion_Recognition_In_The_Wild_Challenge_2014_Baseline_Data_and_Protocol">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Karan_Sikka" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Karan Sikka </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw40_56ab19911d8dd">  <li class="citation-context-item"> "In this challenge, a set of audio features similar to the features employed in Audio Video Emotion Recognition Challenge 2011 [17] motivated from the INTERSPEECH 2010 Paralinguistic challenge (1582 features) [16] are used. The features are extracted using the open-source Emotion and Affect Recognition (openEAR) [8] toolkit backend openS- MILE [9]. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Conference Paper:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/281276446_Emotion_Recognition_In_The_Wild_Challenge_2014_Baseline_Data_and_Protocol"> <span class="publication-title js-publication-title">Emotion Recognition In The Wild Challenge 2014: Baseline, Data and Protocol</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/58762740_Abhinav_Dhall" class="authors js-author-name ga-publications-authors">Abhinav Dhall</a> &middot;     <a href="researcher/49642779_Roland_Goecke" class="authors js-author-name ga-publications-authors">Roland Goecke</a> &middot;     <a href="researcher/2030399401_Jyoti_Joshi" class="authors js-author-name ga-publications-authors">Jyoti Joshi</a> &middot;     <a href="researcher/2047889113_Karan_Sikka" class="authors js-author-name ga-publications-authors">Karan Sikka</a> &middot;     <a href="researcher/2048102736_Tom_Gedeon" class="authors js-author-name ga-publications-authors">Tom Gedeon</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> The Second Emotion Recognition In The Wild Challenge (EmotiW) 2014 consists of an audio-video based emotion classification challenge, which mimics the real-world conditions. Traditionally, emotion recognition has been performed on data captured in constrained lab-controlled like environment. While this data was a good starting point, such lab controlled data poorly represents the environment and conditions faced in real-world situations. With the exponential increase in the number of video clips being up-loaded online, it is worthwhile to explore the performance of emotion recognition methods that work &#39;in the wild&#39;. The goal of this Grand Challenge is to carry forward the common platform defined during EmotiW 2013, for evaluation of emotion recognition methods in real-world conditions. The database in the 2014 challenge is the Acted Facial Expression In Wild (AFEW) 4.0, which has been collected from movies showing close-to-real-world conditions. The paper describes the data partitions, the baseline method and the experimental protocol. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Conference Paper &middot; Nov 2014  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Karan_Sikka/publication/281276446_Emotion_Recognition_In_The_Wild_Challenge_2014_Baseline_Data_and_Protocol/links/55de59c708ae45e825d39d10.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw25_56ab19911d8dd" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw26_56ab19911d8dd">  </ul> </div> </div>   <div id="rgw16_56ab19911d8dd" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw17_56ab19911d8dd"> <div> <h5> <a href="publication/290526966_Frames_and_Phaseless_Reconstruction" class="color-inherit ga-similar-publication-title"><span class="publication-title">Frames and Phaseless Reconstruction</span></a>  </h5>  <div class="authors"> <a href="researcher/2094136480_Radu_Balan" class="authors ga-similar-publication-author">Radu Balan</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56ab19911d8dd"> <div> <h5> <a href="publication/286476962_Speech_Production_in_Speech_Technologies_Introduction_to_the_CSL_Special_Issue" class="color-inherit ga-similar-publication-title"><span class="publication-title">Speech Production in Speech Technologies: Introduction to the CSL Special Issue</span></a>  </h5>  <div class="authors"> <a href="researcher/2075666096_Karen_Livescu" class="authors ga-similar-publication-author">Karen Livescu</a>, <a href="researcher/71649552_Frank_Rudzicz" class="authors ga-similar-publication-author">Frank Rudzicz</a>, <a href="researcher/44265487_Eric_Fosler-Lussier" class="authors ga-similar-publication-author">Eric Fosler-Lussier</a>, <a href="researcher/10650011_Mark_Hasegawa-Johnson" class="authors ga-similar-publication-author">Mark Hasegawa-Johnson</a>, <a href="researcher/34403493_Jeff_Bilmes" class="authors ga-similar-publication-author">Jeff Bilmes</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56ab19911d8dd"> <div> <h5> <a href="publication/281189960_Implementation_and_Comparison_of_Speech_Emotion_Recognition_System_Using_Gaussian_Mixture_Model_GMM_and_K-_Nearest_Neighbor_K-NN_Techniques" class="color-inherit ga-similar-publication-title"><span class="publication-title">Implementation and Comparison of Speech Emotion Recognition System Using Gaussian Mixture Model (GMM) and K- Nearest Neighbor (K-NN) Techniques</span></a>  </h5>  <div class="authors"> <a href="researcher/2058273613_Rahul_B_Lanjewar" class="authors ga-similar-publication-author">Rahul B. Lanjewar</a>, <a href="researcher/2058336774_Swarup_Mathurkar" class="authors ga-similar-publication-author">Swarup Mathurkar</a>, <a href="researcher/2079919135_Nilesh_Patel" class="authors ga-similar-publication-author">Nilesh Patel</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw50_56ab19911d8dd" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw51_56ab19911d8dd">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw52_56ab19911d8dd" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=SkQ5M3OvB21keG8ZJ1kKXo0vsyh6OBt8yGH5kTPx9SBBSGgRT_ybg2HWEmUg4_h7" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="49Wi3Wvo2dpHgzIDIrtfFs+ILAZLjf1KUBYPyHQzuwa8OVzVP6ap7/7zPK5qle+JMV+YUC+XqYd2iRqkibJu7w9xMuxypLDo42K1kuE0fmDZlTjaSvnj1/YsDUscXyvf6Bfifx0p3ICl93Z7zod0yTqMBs2KWjXkJZ9qHkzGo54qeRxSW39NqZFzPOoLDtFXcFksjF9uUeNqXDsnzo7aCMuklOzSOEgYWIAfX5ifRYoc8zS+HkgnIw7HyhiyN7DwR/OJcgf3z3Vcpc0DxkjRZJDayLfkr9ClbqC2YXnZU5s="/> <input type="hidden" name="urlAfterLogin" value="publication/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjI0MDg4MDYwX09wZW5FQVJfLV9JbnRyb2R1Y2luZ190aGVfTXVuaWNoX29wZW4tc291cmNlX2Vtb3Rpb25fYW5kX2FmZmVjdF9yZWNvZ25pdGlvbl90b29sa2l0"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjI0MDg4MDYwX09wZW5FQVJfLV9JbnRyb2R1Y2luZ190aGVfTXVuaWNoX29wZW4tc291cmNlX2Vtb3Rpb25fYW5kX2FmZmVjdF9yZWNvZ25pdGlvbl90b29sa2l0"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjI0MDg4MDYwX09wZW5FQVJfLV9JbnRyb2R1Y2luZ190aGVfTXVuaWNoX29wZW4tc291cmNlX2Vtb3Rpb25fYW5kX2FmZmVjdF9yZWNvZ25pdGlvbl90b29sa2l0"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw53_56ab19911d8dd"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 766;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Bj\u00f6rn Schuller","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272368136355886%401441949032980_m\/Bjoern_Schuller.png","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Bjoern_Schuller","institution":"Imperial College London","institutionUrl":false,"widgetId":"rgw4_56ab19911d8dd"},"id":"rgw4_56ab19911d8dd","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=1741421","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab19911d8dd"},"id":"rgw3_56ab19911d8dd","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=224088060","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":224088060,"title":"OpenEAR - Introducing the Munich open-source emotion and affect recognition toolkit","journalTitle":false,"journalDetailsTooltip":false,"affiliation":"Inst. for Human-Machine Commun., Tech. Univ. Munchen, Munich, Germany","type":"Conference Paper","details":{"doi":"10.1109\/ACII.2009.5349350","conferenceInfos":"Conference: Affective Computing and Intelligent Interaction and Workshops, 2009. ACII 2009. 3rd International Conference on"},"source":{"sourceUrl":"http:\/\/ieeexplore.ieee.org\/xpl\/freeabs_all.jsp?arnumber=5349350","sourceName":"IEEE Xplore"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1109\/ACII.2009.5349350"},{"key":"rft.atitle","value":"OpenEAR - Introducing the Munich open-source emotion and affect recognition toolkit"},{"key":"rft.title","value":"Affective Computing and Intelligent Interaction and Workshops, 2009. ACII 2009. 3rd International Conference"},{"key":"rft.jtitle","value":"Affective Computing and Intelligent Interaction and Workshops, 2009. ACII 2009. 3rd International Conference"},{"key":"rft.date","value":"2009"},{"key":"rft.pages","value":"1 - 6"},{"key":"rft.au","value":"Florian Eyben,Martin Wollmer,Bjorn Schuller"},{"key":"rft.genre","value":"inProceedings"}],"widgetId":"rgw6_56ab19911d8dd"},"id":"rgw6_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=224088060","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":224088060,"peopleItems":[{"data":{"authorUrl":"researcher\/18998136_Florian_Eyben","authorNameOnPublication":"Florian Eyben","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Florian Eyben","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/18998136_Florian_Eyben","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw9_56ab19911d8dd"},"id":"rgw9_56ab19911d8dd","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=18998136&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab19911d8dd"},"id":"rgw8_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=18998136&authorNameOnPublication=Florian%20Eyben","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/75181895_Martin_Wollmer","authorNameOnPublication":"Martin Wollmer","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Martin Wollmer","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/75181895_Martin_Wollmer","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw11_56ab19911d8dd"},"id":"rgw11_56ab19911d8dd","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=75181895&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw10_56ab19911d8dd"},"id":"rgw10_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=75181895&authorNameOnPublication=Martin%20Wollmer","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Bj\u00f6rn Schuller","accountUrl":"profile\/Bjoern_Schuller","accountKey":"Bjoern_Schuller","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272368136355886%401441949032980_m\/Bjoern_Schuller.png","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Bj\u00f6rn Schuller","profile":{"professionalInstitution":{"professionalInstitutionName":"Imperial College London","professionalInstitutionUrl":"institution\/Imperial_College_London"}},"professionalInstitutionName":"Imperial College London","professionalInstitutionUrl":"institution\/Imperial_College_London","url":"profile\/Bjoern_Schuller","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272368136355886%401441949032980_l\/Bjoern_Schuller.png","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Bjoern_Schuller","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw13_56ab19911d8dd"},"id":"rgw13_56ab19911d8dd","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1741421&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Imperial College London","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":1,"publicationUid":224088060,"widgetId":"rgw12_56ab19911d8dd"},"id":"rgw12_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1741421&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=1&publicationUid=224088060","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab19911d8dd"},"id":"rgw7_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=224088060&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":224088060,"abstract":"<noscript><\/noscript><div>Various open-source toolkits exist for speech recognition and speech processing. These toolkits have brought a great benefit to the research community, i.e. speeding up research. Yet, no such freely available toolkit exists for automatic affect recognition from speech. We herein introduce a novel open-source affect and emotion recognition engine, which integrates all necessary components in one highly efficient software package. The components include audio recording and audio file reading, state-of-the-art paralinguistic feature extraction and plugable classification modules. In this paper we introduce the engine and extensive baseline results. Pre-trained models for four affect recognition tasks are included in the openEAR distribution. The engine is tailored for multi-threaded, incremental on-line processing of live input in real-time, however it can also be used for batch processing of databases.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw14_56ab19911d8dd"},"id":"rgw14_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=224088060","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit\/links\/00463514064556446b000000\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw15_56ab19911d8dd"},"id":"rgw15_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab19911d8dd"},"id":"rgw5_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=224088060&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2094136480,"url":"researcher\/2094136480_Radu_Balan","fullname":"Radu Balan","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/290526966_Frames_and_Phaseless_Reconstruction","usePlainButton":true,"publicationUid":290526966,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/290526966_Frames_and_Phaseless_Reconstruction","title":"Frames and Phaseless Reconstruction","displayTitleAsLink":true,"authors":[{"id":2094136480,"url":"researcher\/2094136480_Radu_Balan","fullname":"Radu Balan","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/290526966_Frames_and_Phaseless_Reconstruction","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/290526966_Frames_and_Phaseless_Reconstruction\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab19911d8dd"},"id":"rgw17_56ab19911d8dd","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=290526966","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2075666096,"url":"researcher\/2075666096_Karen_Livescu","fullname":"Karen Livescu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":71649552,"url":"researcher\/71649552_Frank_Rudzicz","fullname":"Frank Rudzicz","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":44265487,"url":"researcher\/44265487_Eric_Fosler-Lussier","fullname":"Eric Fosler-Lussier","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":10650011,"url":"researcher\/10650011_Mark_Hasegawa-Johnson","fullname":"Mark Hasegawa-Johnson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Mar 2016","journal":"Computer Speech & Language","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/286476962_Speech_Production_in_Speech_Technologies_Introduction_to_the_CSL_Special_Issue","usePlainButton":true,"publicationUid":286476962,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.75","url":"publication\/286476962_Speech_Production_in_Speech_Technologies_Introduction_to_the_CSL_Special_Issue","title":"Speech Production in Speech Technologies: Introduction to the CSL Special Issue","displayTitleAsLink":true,"authors":[{"id":2075666096,"url":"researcher\/2075666096_Karen_Livescu","fullname":"Karen Livescu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":71649552,"url":"researcher\/71649552_Frank_Rudzicz","fullname":"Frank Rudzicz","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":44265487,"url":"researcher\/44265487_Eric_Fosler-Lussier","fullname":"Eric Fosler-Lussier","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10650011,"url":"researcher\/10650011_Mark_Hasegawa-Johnson","fullname":"Mark Hasegawa-Johnson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":34403493,"url":"researcher\/34403493_Jeff_Bilmes","fullname":"Jeff Bilmes","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Computer Speech & Language 03\/2016; 36:165-172. DOI:10.1016\/j.csl.2015.11.002"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/286476962_Speech_Production_in_Speech_Technologies_Introduction_to_the_CSL_Special_Issue","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/286476962_Speech_Production_in_Speech_Technologies_Introduction_to_the_CSL_Special_Issue\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab19911d8dd"},"id":"rgw18_56ab19911d8dd","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=286476962","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2058273613,"url":"researcher\/2058273613_Rahul_B_Lanjewar","fullname":"Rahul B. Lanjewar","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2058336774,"url":"researcher\/2058336774_Swarup_Mathurkar","fullname":"Swarup Mathurkar","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079919135,"url":"researcher\/2079919135_Nilesh_Patel","fullname":"Nilesh Patel","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Dec 2015","journal":"Procedia Computer Science","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/281189960_Implementation_and_Comparison_of_Speech_Emotion_Recognition_System_Using_Gaussian_Mixture_Model_GMM_and_K-_Nearest_Neighbor_K-NN_Techniques","usePlainButton":true,"publicationUid":281189960,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/281189960_Implementation_and_Comparison_of_Speech_Emotion_Recognition_System_Using_Gaussian_Mixture_Model_GMM_and_K-_Nearest_Neighbor_K-NN_Techniques","title":"Implementation and Comparison of Speech Emotion Recognition System Using Gaussian Mixture Model (GMM) and K- Nearest Neighbor (K-NN) Techniques","displayTitleAsLink":true,"authors":[{"id":2058273613,"url":"researcher\/2058273613_Rahul_B_Lanjewar","fullname":"Rahul B. Lanjewar","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2058336774,"url":"researcher\/2058336774_Swarup_Mathurkar","fullname":"Swarup Mathurkar","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079919135,"url":"researcher\/2079919135_Nilesh_Patel","fullname":"Nilesh Patel","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Procedia Computer Science 12\/2015; 49(1):50-57. DOI:10.1016\/j.procs.2015.04.226"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/281189960_Implementation_and_Comparison_of_Speech_Emotion_Recognition_System_Using_Gaussian_Mixture_Model_GMM_and_K-_Nearest_Neighbor_K-NN_Techniques","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/281189960_Implementation_and_Comparison_of_Speech_Emotion_Recognition_System_Using_Gaussian_Mixture_Model_GMM_and_K-_Nearest_Neighbor_K-NN_Techniques\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab19911d8dd"},"id":"rgw19_56ab19911d8dd","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=281189960","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw16_56ab19911d8dd"},"id":"rgw16_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=224088060&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":224088060,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":224088060,"publicationType":"inProceedings","linkId":"00463514064556446b000000","fileName":"09eyb1.pdf","fileUrl":"profile\/Bjoern_Schuller\/publication\/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit\/links\/00463514064556446b000000.pdf","name":"Bj\u00f6rn Schuller","nameUrl":"profile\/Bjoern_Schuller","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Jan 20, 2016","fileSize":"348.04 KB","widgetId":"rgw22_56ab19911d8dd"},"id":"rgw22_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=224088060&linkId=00463514064556446b000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":224088060,"publicationType":"inProceedings","linkId":"0f969cc00cf20156559a6905","fileName":"OpenEAR - Introducing the Munich open-source emotion and affect recognition toolkit","fileUrl":"http:\/\/www.mmk.ei.tum.de\/publ\/pdf\/09\/09eyb1.pdf","name":"tum.de","nameUrl":"http:\/\/www.mmk.ei.tum.de\/publ\/pdf\/09\/09eyb1.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw23_56ab19911d8dd"},"id":"rgw23_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=224088060&linkId=0f969cc00cf20156559a6905&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw21_56ab19911d8dd"},"id":"rgw21_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=224088060&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":103,"valueFormatted":"103","widgetId":"rgw24_56ab19911d8dd"},"id":"rgw24_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=224088060","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw20_56ab19911d8dd"},"id":"rgw20_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=224088060&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":224088060,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw26_56ab19911d8dd"},"id":"rgw26_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=224088060&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":103,"valueFormatted":"103","widgetId":"rgw27_56ab19911d8dd"},"id":"rgw27_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=224088060","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw25_56ab19911d8dd"},"id":"rgw25_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=224088060&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"openEAR - Introducing the Munich Open-Source\nEmotion and Affect Recognition Toolkit\nFlorian Eyben, Martin W\u00a8 ollmer, and Bj\u00a8 orn Schuller\nTechnische Universit\u00a8 at M\u00a8 unchen, Institute for Human-Machine Communication\nTheresienstrasse 90, 80333 M\u00a8 unchen\n{eyben|woellmer|schuller}@tum.de\nAbstract\nVarious open-source toolkits exist for speech recognition\nand speech processing. These toolkits have brought a great\nbenefit to the research community, i.e. speeding up research.\nYet, no such freely available toolkit exists for automatic af-\nfect recognition from speech. We herein introduce a novel\nopen-source affect and emotion recognition engine, which\nintegrates all necessary components in one highly efficient\nsoftware package. The components include audio recording\nand audio file reading, state-of-the-art paralinguistic fea-\nture extraction and plugable classification modules. In this\npaper we introduce the engine and extensive baseline re-\nsults. Pre-trained models for four affect recognition tasks\nare included in the openEAR distribution. The engine is tai-\nlored for multi-threaded, incremental on-line processing of\nlive input in real-time, however it can also be used for batch\nprocessing of databases.\n1. Introduction\nAffective Computing has become a popular area of re-\nsearch in recent times [17]. Many achievements have been\nmade towards making machines detect and understand hu-\nman affective states, such as emotion, interest or dialogue\nrole. Yet, in contrast to the field of speech recognition, only\nvery few software toolkits exist, which are tailored specif-\nically for affect recognition from audio or video. In this\npaper, we introduce and describe the Munich open Affect\nRecognition Toolkit (openEAR), the first such tool, which\nruns on multiple platforms and is publicly available1.\nOpenEAR in it\u2019s initial version is introduced as an affect\nand emotion recognition toolkit for audio and speech affect\nrecognition. However, openEAR\u2019s architecture is modular\nand by principle modality independent. Thus, also vision\nfeatures such as facial points or optical flow measures can\n1http:\/\/sourceforge.net\/projects\/openear\nbe added and fused with audio features. Moreover, phys-\niological features such as heart rate, ECG, or EEG signals\nfrom devices such as the Neural Impulse Actuator (NIA),\ncan be analysed using the same methods and algorithms\nas for speech signals and thus can also be processed us-\ning openEAR \u2013 provided suitable capture interfaces and\ndatabases.\n2. Existing work\nA few free toolkits exist, that provide various compo-\nnents usable for emotion recognition. Most toolkits that in-\nclude feature extraction algorithms are targeted at speech\nrecognition and speech processing, such as the Hidden\nMarkov Toolkit (HTK) [16], the PRAAT Software [1], the\nSpeech Filling System (SFS) from UCL, and the SNACK\npackage for the Tcl scripting language. These can all be\nused to extract state-of-the-art features for emotion recog-\nnition. However, only PRAAT and HTK include certain\nclassifiers. For further classifiers WEKA and RapidMiner,\nfor example, can be used. Moreover, only few of the listed\ntoolkits are available under a permissive Open-Source li-\ncense, e.g. WEKA, PRAAT, and RapidMiner.\nThe most complete and task specific framework for\nEmotion Recognition currently is EmoVoice [13]. How-\never, the main design objective is to provide an emotion\nrecognition system for the non-expert. Thus it is a great\nframework for demonstrator applications and making emo-\ntion recognition available to the non-expert. openEAR, in\ncontrast, aims at being a stable and efficient set of tools\nfor researchers and those developing emotional aware ap-\nplications, providing the elementary functionality for emo-\ntion recognition, i.e. the Swiss Army Knife for research and\ndevelopment of affect aware applications. openEAR com-\nbines everything from audio recording, feature extraction,\nand classification to evaluation of results, and pre-trained\nmodels while being very fast and highly efficient. All fea-\nture extractor components are written in C++ and can be\nused as a library, facilitating integration into custom appli-\n978-1-4244-4799-2\/09\/$25.00 c ?2009 IEEE"},{"page":2,"text":"cations. Also, openEAR can be used as an out-of-the-box\nemotion live affect recogniser for various domains, using\npre-trained models which are included in the distribution.\nMoreover, openEAR is Open-Source software, freely avail-\nable to anybody under the terms of the GNU General Public\nLicense.\n3. openEAR\u2019s Architecture\nThe openEAR toolkit consists of three major compo-\nnents: the core component is the SMILE (Speech and Mu-\nsic Interpretation by Large-Space Extraction) signal pro-\ncessing and feature extraction tool, which is capable gen-\nerating > 500k features in real-time (Real-Time Factor\n(RTF) < 0.1), either from live audio input or from off-\nline media. Next, there is support for classification mod-\nules via a plug-in interface to the feature extractor. More-\nover, supporting scripts and tools are provided, which fa-\ncilitate training of own models on arbitrary data sets. Fi-\nnally, four ready-to-use model-sets are provided for recog-\nnition of six basic emotion categories (trained on the Berlin\nSpeech Emotion Database (EMO-DB) [2] and the eNTER-\nFACE database), for recognition of emotion in a continu-\nous three-dimensional feature space spanned by activation,\nvalence, and dominance (trained on the Belfast naturalistic\n(SAL) and Vera-am-Mittag (VAM) [4] corpora), for recog-\nnition of interest using three discrete classes taken from the\nAudio Visual Interest Corpus (AVIC) [9], and for recogni-\ntion of affective states such as drunkenness trained on the\nAirplane Behaviour Corpus (ABC).\nSignalinputcaneitherbereadoff-linefromaudiofilesor\nrecorded on-line from a capture device in real-time. Since\ndata processing is incremental (concerning signal process-\ning and feature extraction), there is no difference between\nhandling live input and off-line media. Independent of the\ninput method, the feature output can either be classified di-\nrectly via built in classifiers, classifier plug-ins, or the fea-\ntures (or even wave data) can be exported to various file\nformats used by other popular toolkits. Currently imple-\nmented export file formats are: WEKA Arff [14], LibSVM\nformat [3], Comma Separated Value (CSV) File, and Hid-\nden Markov Toolkit (HTK) [16] feature files.\nThe following sub-sections describe the feature extrac-\ntor\u2019s modular architecture, the features currently imple-\nmented, and the classifier interface. The model-sets will be\ndetailed along with baseline benchmark results in section 4.\n3.1. Modular and Efficient Implementation\nDuring specification of openEAR\u2019s feature extractor ar-\nchitecture, three main objectives were followed: speed and\nefficiency, incremental processing of data (i.e. frame by\nframe with minimum delay), and flexibility and modular-\nity. Adding new features is possible via an easy plug-in\ninterface.\nThe SMILE feature extractor is implemented from\nscratch in C++, without crucial third party dependencies.\nThus, it is easy to compile, and basically platform inde-\npendent. It is currently known to run on Mac OS, vari-\nous Linux distributions, and Windows platforms. Feature\nextraction code is optimised to avoid double computations\nof shared values, e.g. Fast Fourier Transform (FFT) coeffi-\ncients, which are only computed once and used for multiple\nalgorithms such as computation of energy, spectral features,\nand cepstral features.\nFigure 1. Concept and components of openEAR\u2019s SMILE (Speech\nand Music Interpretation by Large-Space Extraction) feature ex-\ntractor.\nFigure 1 shows a rough sketch of the data flow and sig-\nnal processing architecture. The central component is the\nData Memory, which enables memory efficient incremen-\ntal processing by managing ring-buffer storage of feature\ndata. Input data (wave files, other features, etc.) is fed to the\nData Memory by Data Source components, which contain\na Data Writer sub-component that handles the data mem-\nory interface. Data Processor components read data frames\nor contours from one location of the Data Memory, pro-\ncess the data and write new frames to a different location in\nthe Data Memory. They contain both a Data Reader and a\nData Writer sub-component, which handle the Data Mem-\nory interface. Finally, the Data Sink components read data\nfrom the Data Memory and feed it to the classifier compo-\nnents or write data to files. Each component can be run in a\nseparate thread, speeding up processing on multiple proces-\nsors\/cores.\nThe individual components can be freely instantiated,\nconfigured, and connected to the Data Memory via a central\nconfiguration file. To facilitate configuration file creation\nexample files are provided and configuration file conversion"},{"page":3,"text":"scripts are included.\n3.2. Features\nThe SMILE feature extraction tool is capable of ex-\ntracting low-level audio features (Low-Level Descriptors\n(LLD)) and applying various statistical functionals and\ntransformations to those features. The Low-Level Descrip-\ntors currently implemented are listed in table 1. We hope to\nextend this list by numerous advanced and state-of-the-art\nfeatures in the near future, such as Voice Quality Parameters\n(e.g. [6]), alternative pitch detection algorithms, e.g. pitch\nby Harmonic Product Spectrum, combination of Average\nMagnitude Difference with Autocorrelation, and smooth-\ning of pitch contours via a Viterbi algorithm or ESPS pitch\ntracker. Moreover, features such as TEAGER energy or fur-\nther Auditory Features are considered for integration.\nFeature Group\nSignal energy\nFFT-Spectrum\nMel-Spectrum\nCepstral\nPitch\nFeatures in Group\nRoot mean-square & logarithmic\nBins 0-Nfft\nBins 0-Nmel\nMFCC 0-Nmfcc\nFundamentalfrequency\nACF, in Hz, Bark and closest\nsemitone.\nProbability of voicing (ACF(T0)\nHarmonics-to-noise ratio\nLPC Coefficients\nPerceptual Linear Predictive Coef-\nficients\nFormants and Bandwidth computed\nfrom LPC analysis\nZero-crossing-rate,\nvalue, minimum value, DC\nEnergy in bands 0-250Hz, 0-\n650Hz, 250-650Hz, 1-4kHz, and\ncustom\nN% roll-off point, centroid, flux,\nand rel. pos. of spectrum max. and\nmin.\nCHROMA (warped semitone filter-\nbank), CENS\nComb-filter bank\nF0\nvia\nACF(0))\nVoice Quality\nLPC\nPLP\nFormants\nTime Signalmaximum\nSpectral\nMusical\nTable 1. Low-Level Descriptors implemented in openEAR\u2019s\nSMILE feature extractor.\nThe Mel frequency features, Mel-Spectrum and Mel-\nFrequency Cepstral Coefficients (MFCCs) are computed\nexactly as described in [16], thus providing compatibility\nto the Hidden Markov Toolkit and existing models trained\non HTK MFCCs. Harmonics-To-Noise Ratio computation\nis based on equation 1, where T0is the pitch period.\nHNRt= 10 \u00b7 log\nACF(T0)\nACF(0) \u2212 ACF(T0)\nS) at time t is computed via equation 2.\n(1)\nSpectral centroid (Ct\nXt(f) is the spectral magnitude at time t in bin f.\nCt\nS=\n?\n\u2200ff \u00b7 Xt(f)\n\u00b7?\n\u2200fXt(f)\n(2)\nSpectral Flux Ft\ntion 3, whereby Etis the energy of the frame at time t.\n?\nN\nf=1\nSfor N FFT bins is computed via equa-\nFt\nS=\n?\n?\n?1\nN\n?\n?Xt(f)\nEt\n\u2212Xt\u22121(f)\nEt\u22121\n?2\n(3)\nThe p percent Spectral Roll-Off is determined as the fre-\nquency (or FFT bin) below which p percent of the total\nsignal energy fall. All frequencies, i.e. for Centroid and\nRoll-Offs are normalised to 1000Hz.\nDelta regression coefficients dtof arbitrary order can be\ncomputed from any LLD contour (xt) using equation 4,\nwhere W specifies half the size of the window to be used\nfor computation of the regression coefficients. The default\nis W = 2. In order to provide HTK compatibility, equa-\ntion 4 was implemented as described in [16]. Typically only\nthe first and second order \u03b4-coefficients are used, which is\nalso the default setting in openEAR.\n?W\nTable 2 lists the statistical functionals, regression coef-\nficients and transformations currently implemented. They\ncan be applied to the LLD to map a time sequence of vari-\nable length to a static feature vector. This procedure is\nthe common technique in related emotion and affect recog-\nnition work (cf. [7]). Moreover, hierarchical functionals\ncan be computed as \u201cfunctionals of functionals\u201d (cf. [11]),\nwhich helps improve robustness against single outliers, for\nexample. Thereby there is no limit as to how many hierar-\nchies can be computed, except by computing resources such\nas memory and processing time.\nAs with the LLD we aim at implementing even more\nfunctionals which can be applied to LLD contours or func-\ntional contours, in order to facilitate systematic feature gen-\neration. Due to the modular architecture of the feature ex-\ntractor, it will also be possible to apply any implemented\nprocessing algorithm to any time series, i.e. the Mel-band\nfilter-bank could be applied as a functional to any LLD con-\ntour. This gives researchers an efficient and customisable\ntool to generate millions of features in order to find optimal\nfeature sets which represent affective information.\ndt=\ni=1i \u00b7?xt+i\u2212 xt\u2212i?\n2?W\ni=1i2\n(4)"},{"page":4,"text":"Functionals, etc.\nMax.\/min. val. and respective rel. position\nRange (max.-min.)\nArithmetic, quadratic, and geometric mean\nArth. mean of abs\u02d9 and non-zero val.\n%-age of non-zero val. wrt. tot. # of val. in contour\nMax. and min. value - arithmetic mean\nQuartiles and inter-quartile ranges\nN% percentiles\nStd. deviation, variance, kurtosis, skewness\nCentroid of LLD contour\nZero-crossing and mean-crossing rate\n25% Down-Level Time, 75% Up-Level Time,\nRise-Time, Fall-Time\n# of peaks, mean dist. btwn. peaks, arth. mean of\npeaks, arth. mean of peaks - overall arth. mean\nNumber of segments based on \u03b4-thresholding\nLinear reg. coefficients and corresp. approx. err.\nQuadraticreg.coefficientsandcorresp.approx.err.\nDiscrete Cosine Transf. (DCT) coefficients 0-N\n#\n4\n1\n3\n2\n1\n2\n6\n(N)\n4\n1\n2\n4\n4\n1\n4\n5\n(6)\nTable 2. Statistical functionals, regression coefficients and trans-\nformations currently implemented in openEAR\u2019s feature extractor.\n3.3. Classification and Data Output\nAs mentioned in section 3.1, multiple Data Sinks can\nbe present in the feature extractor, feeding data to different\nclassifiers. There exists an implementation of a K-Nearest\nNeighbour classifier, a Bayes classifier, and a module for\nSupport-Vector classification and regression using the effi-\ncient and freely available LibSVM [3]. We further plan to\nimplement Discriminant Multinomial Naive Bayes [12] and\nLong Short-Term Memory Recurrent Neural Networks [5].\nSupporting scripts written in Perl (in order to be plat-\nform independent) facilitate batch processing of data-sets.\nAs features can be saved in various data-formats, custom\nexperiments can easily be conducted using e.g. WEKA [14]\nor HTK [16]. Calling of various WEKA functions from the\ncommand-line, e.g. for feature selection or classification, is\nalso included among openEAR\u2019s Perl scripts.\nA benefit of openEAR\u2019s modular feature extractor archi-\ntecture is that practically any internal data can be accessed\nand output to various formats simply by connecting a data\nsink to the specific location in the data memory. The ex-\nported data can be used for visualisation purposes, for ex-\nample. This is helpful in finding extractor parametrisation\nproblems, or for common sense checks to see if everything\nis working as expected. Visualising the internal data can\nalso be a helpful means for teaching and understanding the\nprocess of extracting various feature types. Exemplary plots\nof some selected LLD can be found in figure 3.3.\nlaughing if I take my mind off\nFigure 2. Plots of selected LLD for the spoken utterance: \u201claugh-\ning, if I take my mind off ...\u201d (SAL database, male speaker). Top\nto bottom: original wave (3s), norm. RMS energy, probability of\nvoicing, norm. zero-crossing rate, rel. freq. of spec. max.\n4. Benchmarks\nWe now show benchmark results obtained on popu-\nlar databases for speech emotion and interest recognition.\nFreely available models for all databases used for bench-\nmarking are distributed with openEAR. Moreover, compu-\ntation time performance of openEAR\u2019s SMILE feature ex-\ntractor is evaluated. The results show, that openEAR yields\nstate-of-the-art recognition results with very low computa-\ntional demand.\nThe followingsub-section\ndatabases, before the results are discussed in section 4.2.\nbriefly describesthe\n4.1. Databases\nSix databases as listed in table 3 are used for benchmark-\ning openEAR and for which freely available model sets are\ndistributed with openEAR: the first three contain discrete\nclass labels for emotion, namely the Berlin Speech Emo-\ntion Database (EMO-DB) [2], containing seven classes of\nbasic emotions (Anger, Fear, Happiness, Disgust, Boredom,\nSadness, Neutral), the eNTERFACE corpus with six emo-\ntion categories (Anger, Disgust, Fear, Happiness, Sadness,\nand Surprise), the ABC corpus with the classes (Aggres-\nsive, Cheerful, Intoxicated, Nervous, Neutral, Tired), and\nthe Audio Visual Interest Corpus (AVIC) [9] with labels for\nthree levels of interest (-1: disinterest, 0: normal, and 1:\nhigh interest). The last two databases contain continuous\ndimensional labels for valence and activation in the range\nfrom -1 to +1. These are the SAL corpus and the VAM\ncorpus [4]. The latter also has labels for the potency di-"},{"page":5,"text":"mension. However, we found this dimension to be highly\ncorrelated to activation (Corr. Coeff. 0.9). Thus, we did not\nconsider it in the evaluations. When viewing the results in\nDatabase\nABC\nAVIC\nEMO-DB\neNTERFACE\nSAL\nVAM\n# turns\n431\n996\n494\n1170\n1692\n947\ndiscrete\n?\n?\n?\n?\ncontinuous\n?\n?\nTable 3. Six databases for benchmarking openEAR and generation\nof model sets included in the openEAR distribution.\nthe following section it has to be considered that ABC, eN-\nTERFACE, and EMO-DB contain acted and prototypical\nemotions, while AVIC, SAL, and VAM contain natural and\nspontaneous data. Due to the dimensional annotation for\nSAL and VAM all recorded turns are left in the database,\nnot only prototypical turns.\n4.2. Recognition Results\nTable 4 shows results obtained for discrete class emo-\ntion and interest (on AVIC) recognition. For all bench-\nmarks the feature set 5,967 tF as described in table 6 was\nextracted. A correlation based feature subset (CFS) selec-\ntion was performed in order to find relevant feature sets for\neach database. However, better results with the full feature\nset were achieved on EMO-DB, eNTERFACE, and ABC.\nThus, the best results without feature selection are shown\nthere. All the benchmark results are well in line with or\neven above the current state-of-the-art.\nRecall [%]\nABC (6 emo. rel. states)\nAVIC (3 levels of interest)\nEMO-DB (7 emotions)\neNTERFACE (6 emotions)\nWA\n71.9\n74.5\n89.5\n75.2\nUA\n66.5\n70.4\n88.8\n75.1\nTable 4. Results obtained for discrete class emotion recognition\nusing Support-Vector Machines with polynomial kernel function\nof degree 1. 10-fold Stratified Cross-Validation. Weighted average\n(WA) and unweighted average (UA) of class-wise recall rates as\ndemanded in [10].\nFinally, table 5 shows the results for the two most chal-\nlenging tasks, the dimensional estimation of naturalistic\nemotions. Results obtained for VAM are slightly better\nthan those reported for polynomial kernel SVM in [4]. Re-\nsults for SAL are obtained on the same data-set partitions\nas in [15].\nDatabase\nSAL (train\/test)\nVAM (10-f. SCV)\nCCa\n0.24\n0.83\nMLEa\n0.28\n0.15\nCCv\n0.15\n0.42\nMLEv\n0.38\n0.14\nTable 5. Results obtained for continuous emotion recognition for\ntwo dimensions (activation and valence) using Support-Vector Re-\ngression (polynomial kernel function, degree 1). Results reported\nare Correlation Coefficient (CC) and Mean absolute (linear) error\n(MLE) for activation (a) and valence (v). VAM: 10-fold Strat-\nified Cross-Validation (SCV) after CFS feature selection. SAL:\npre-defined train\/test sets, CFS feature selection on training set.\nFeature Set\n36 MFCCde\n102 LLD\nDescription\nMFCC 0-12, first and second order \u03b4\nLLD pitch, time, spectral, mfcc, and en-\nergy + first and second order \u03b4\n43 functionals (applied to complete in-\nput) of 39 LLD + first and second order\n\u03b4. (No percentile functionals)\n51 functionals (applied to complete in-\nput) of 39 LLD + first and second order\n\u03b4\n43 functionals (applied to 2s windows\nw. 50% overlap) of 39 LLD + first and\nsecond order \u03b4. (No percentile function-\nals)\n51 functionals (applied to 2s windows\nw. 50% overlap) of 39 LLD + first and\nsecond order \u03b4\n43 functionals applied to set 5,031 2sF\n51 functionals applied to set 5,967 2sF\n5,031 tF\n5,967 tF\n5,031 2sF\n5,967 2sF\n216k tHRF\n304k tHRF\nTable 6. Feature-sets for openEAR feature extractor computation\ntime evaluation and benchmark results.\n4.3. Computation Time\nSince one objective of openEAR is efficiency and real-\ntime operation, we now provide run-time benchmarks for\nvarious feature sets, which are summarised in table 6. Com-\nputation time is evaluated under Ubuntu Linux on a AMD\nPhenom 64bit CPU at 2.2GHz. All components are run in\na single thread for benchmarking.\nTable 7 shows the computation time and the Real-Time\nFactor (RTF) for extraction of various feature sets.\n5. Conclusion and Outlook\nWe introduced openEAR, an efficient, open-source,\nmulti-threaded, real-time emotion recognition framework\nproviding an extensible, platform independent feature ex-\ntractor implemented in C++, pre-trained models on six\ndatabases which are ready-to-use for on-line emotion and\naffect recognition, and supporting scripts for model build-\ning, evaluation, and visualisation. The framework is com-"},{"page":6,"text":"Feature Set\n36 MFCCde\n102 LLD\n5,031 tF\n5,031 2sF\n216k tHRF\nComp. time [s] RTF\n0.003\n0.009\n0.012\n0.014\n0.018\n1.3\n4.4\n6.1\n7.2\n9.2\nTable 7. openEAR\u2019s computation time and real-time factor (RTF)\nfor feature extraction of 8 minutes and 27 seconds 16-bit mono\naudio sampled at 16kHz. LLD for all above feature sets computed\nfor 25ms frames at a rate of 10ms. CPU: AMD Phenom, 2.2GHz.\nSingle thread processing.\npatible with related tool-kits, such as HTK and WEKA by\nsupporting their data-formats. The current implementation\nwas successfully evaluated on six affective speech corpora,\nshowing state-of-the-art performance. Moreover, features\nfor the Interspeech 2009 Emotion Challenge [10] were ex-\ntracted with openEAR.\nDevelopment of openEAR is still in progress and more\nfeatures will be added soon. Due to it\u2019s modular architec-\nture and the public source code, rapid addition of new, ad-\nvanced features by the community is hopefully encouraged.\nAlthough openEAR already is a fully featured emo-\ntion and affect recognition toolkit, it can also be used for\nother tasks such as classification of non-linguistic vocali-\nsations [8]. In the future, decoders for continuous speech\nrecognition and linguistic features will be integrated into\nopenEAR, resulting in a highly efficient and comprehensive\naffect recognition engine.\n6. Acknowledgment\nThe research leading to these results has received fund-\ning from the European Community\u2019s Seventh Framework\nProgramme (FP7\/2007-2013) under grant agreement No.\n211486 (SEMAINE).\nReferences\n[1] P. Boersma and D. Weenink.\ncomputer (version 4.3.14). http:\/\/www.praat.org\/,\n2005.\n[2] F. Burkhardt, A. Paeschke, M. Rolfes, W. Sendlmeier, and\nB. Weiss. A database of german emotional speech. In Pro-\nceedings Interspeech 2005, Lissabon, Portugal, pages 1517\u2013\n1520, 2005.\n[3] C.-C. Chang and C.-J. Lin.\nport vector machines, 2001. Software available at http:\n\/\/www.csie.ntu.edu.tw\/\u02dccjlin\/libsvm.\n[4] M. Grimm, K. Kroschel, and S. Narayanan. Support vector\nregression for automatic recognition of spontaneous emo-\ntions in speech. In International Conference on Acoustics,\nSpeech and Signal Processing, 2007., volume 4, pages IV\u2013\n1085\u2013IV. IEEE, April 2007.\nPraat: doing phonetics by\nLibSVM: a library for sup-\n[5] S. Hochreiter and J. Schmidhuber. Long short-term memory.\nNeural Computation, 9(8):1735\u20131780, 1997.\n[6] M. Lugger and B. Yang.\nstageemotionclassificationexploitingvoicequalityfeatures.\nIn F. Mihelic and J. Zibert, editors, Speech Recognition,\npage 1. IN-TECH, November 2008.\n[7] B. Schuller, A. Batliner, D. Seppi, S. Steidl, T. Vogt, J. Wag-\nner, L. Devillers, L. Vidrascu, N. Amir, L. Kessous, and\nV. Aharonson. The relevance of feature type for the auto-\nmatic classification of emotional user states: Low level de-\nscriptors and functionals. In Proc. INTERSPEECH 2007,\npages 2253\u20132256, Antwerp, Belgium, 2007.\n[8] B. Schuller, F. Eyben, and G. Rigoll. Static and dynamic\nmodelling for the recognition of non-verbal vocalisations in\nconversational speech. In E. Andr\u00b4 e, editor, Proc. IEEE PIT\n2008, volume LNCS 5078, pages 99\u2013110. Springer, 2008.\n16.-18.06.2008.\n[9] B. Schuller, R. M\u00a8 uller, F. Eyben, J. Gast, B. H\u00a8 ornler,\nM. W\u00a8 ollmer, G. Rigoll, A. H\u00a8 othker, and H. Konosu. Being\nbored? recognising natural interest by extensive audiovisual\nintegration for real-life application. to appear in Image and\nVision Computing Journal (IMAVIS), Special Issue on Visual\nand Multimodal Analysis of Human Spontaneous Behavior,\nElsevier, page 17 pages, 2009.\n[10] B. Schuller, S. Steidl, and A. Batliner. The interspeech 2009\nemotion challenge. In Interspeech (2009), ISCA, Brighton,\nUK, 2009.\n[11] B. Schuller, M. Wimmer, L. M\u00a8 osenlechner, C. Kern, D. Ar-\nsic, and G. Rigoll. Brute-forcing hierarchical functionals for\nparalinguistics: A waste of feature space? In Proceedings of\nICASSP 2008, Las Vegas, Nevada, USA, April 2008.\n[12] J. Su, H. Zhang, C. X. Ling, and S. Matwin.\nnative parameter learning for bayesian networks. In ICML\n\u201908: Proceedings of the 25th international conference on\nMachine learning, pages 1016\u20131023, New York, NY, USA,\n2008. ACM.\n[13] T. Vogt, E. Andr\u00b4 e, and N. Bee. Emovoice - a framework for\nonline recognition of emotions from voice. In Proc. IEEE\nPIT 2008, volume 5078 of LNCS, pages 188\u2013199. Springer,\nJune 2008. feature extraction, emotion recognition, GUI.\n[14] I. H. Witten and E. Frank. Data Mining: Practical machine\nlearning tools and techniques. Morgan Kaufmann, San Fran-\ncisco, 2nd edition edition, 2005.\n[15] M. W\u00a8 ollmer, F. Eyben, S. Reiter, B. Schuller, C. Cox,\nE. Douglas-Cowie, and R. Cowie.\nclasses - towards continuous emotion recognition with mod-\nelling of long-range dependencies.\nspeech, Brisbane, Australia, 2008.\n[16] S. Young, G. Evermann, M. Gales, T. Hain, D. Ker-\nshaw, X. Liu, G. Moore, J. Odell, D. Ollason, D. Povey,\nV. Valtchev, and P. Woodland. The HTK book (v3.4). Cam-\nbridge University Press, Cambridge, UK, December 2006.\n[17] Z.Zeng, M.Pantic, G.I.Rosiman, andT.S.Huang. Asurvey\nof affect recognition methods: Audio, visual, and sponta-\nneous expressions. Trans. on Pattern Analysis and Machine\nIntelligence, 31(1):39\u201358, 2009.\nPsychological motivated multi-\nDiscrimi-\nAbandoning emotion\nIn Proceedings Inter-"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Bjoern_Schuller\/publication\/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit\/links\/00463514064556446b000000.pdf","widgetId":"rgw28_56ab19911d8dd"},"id":"rgw28_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=224088060&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw29_56ab19911d8dd"},"id":"rgw29_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=224088060&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":224088060,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":224088060,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":84871601,"url":"researcher\/84871601_Yousaf_Khan","fullname":"Yousaf Khan","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277142207254528%401443087259056_m\/Dryousaf_Khan.png"},{"id":2080911976,"url":"researcher\/2080911976_S_Haq","fullname":"S. Haq","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2034213230,"url":"researcher\/2034213230_A_Ali","fullname":"A. Ali","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2007832441,"url":"researcher\/2007832441_M_Asif","fullname":"M. Asif","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":2,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Jan 2015","journal":"Boston University journal","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/275208397_Speaker-Independent_Speech_Emotion_Recognition_using_Gaussian_and_SVM_Classifiers","usePlainButton":true,"publicationUid":275208397,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/275208397_Speaker-Independent_Speech_Emotion_Recognition_using_Gaussian_and_SVM_Classifiers","title":"Speaker-Independent Speech Emotion Recognition using Gaussian and SVM Classifiers,","displayTitleAsLink":true,"authors":[{"id":84871601,"url":"researcher\/84871601_Yousaf_Khan","fullname":"Yousaf Khan","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277142207254528%401443087259056_m\/Dryousaf_Khan.png"},{"id":2080911976,"url":"researcher\/2080911976_S_Haq","fullname":"S. Haq","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2034213230,"url":"researcher\/2034213230_A_Ali","fullname":"A. Ali","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2007832441,"url":"researcher\/2007832441_M_Asif","fullname":"M. Asif","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2080902083,"url":"researcher\/2080902083_T_Jan","fullname":"T. Jan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2092734913,"url":"researcher\/2092734913_Naveedahmad","fullname":"Naveedahmad","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Boston University journal 01\/2015; 47(1):6."],"abstract":"This paper presents a novel technique of human emotion recognition using the audio modality for the speaker-independent task. In order to achieve a high emotion classification performance a standard set of audio features were extracted. The feature selection was performed using the Plus l-Take Away r algorithm based on Bhattacharyya distance criterion. The feature selection was followed by feature reduction using PCA and LDA, and classification using the Gaussian and SVM classifiers. The emotion classification performance better or comparable to state-of-the art techniques and humans were achieved on the standard Berlin emotional speech database.\nKeywords: Audio Emotion Recognition, Feature Selection, Principal Component Analysis, Gaussian and SVM Classifiers","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/275208397_Speaker-Independent_Speech_Emotion_Recognition_using_Gaussian_and_SVM_Classifiers","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Dryousaf_Khan\/publication\/275208397_Speaker-Independent_Speech_Emotion_Recognition_using_Gaussian_and_SVM_Classifiers\/links\/5534da330cf2df9ea6a3e89e.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Dryousaf_Khan","sourceName":"Dr.Yousaf Khan","hasSourceUrl":true},"publicationUid":275208397,"publicationUrl":"publication\/275208397_Speaker-Independent_Speech_Emotion_Recognition_using_Gaussian_and_SVM_Classifiers","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/275208397_Speaker-Independent_Speech_Emotion_Recognition_using_Gaussian_and_SVM_Classifiers\/links\/5534da330cf2df9ea6a3e89e\/smallpreview.png","linkId":"5534da330cf2df9ea6a3e89e","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=275208397&reference=5534da330cf2df9ea6a3e89e&eventCode=&origin=publication_list","widgetId":"rgw33_56ab19911d8dd"},"id":"rgw33_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=275208397&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"5534da330cf2df9ea6a3e89e","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":224088060,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/275208397_Speaker-Independent_Speech_Emotion_Recognition_using_Gaussian_and_SVM_Classifiers\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["We extracted 384 audio features that were used in the Interspeech 2009 Emotion Challenge (Schuller et al., 2009) using the openEAR toolkit (Eyben et al., 2009). The feature set consisted of low-level descriptors (LLD) including fundamental frequency (f 0 ), energy, Mel frequency cepstral coefficients (MFCCs), zero crossing rate (ZCR) and line spectral frequencies (lsf). "],"widgetId":"rgw34_56ab19911d8dd"},"id":"rgw34_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw32_56ab19911d8dd"},"id":"rgw32_56ab19911d8dd","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=275208397&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":29537335,"url":"researcher\/29537335_Felix_Weninger","fullname":"Felix Weninger","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272457294675996%401441970289453_m"},{"id":18998135,"url":"researcher\/18998135_Bjoern_Schuller","fullname":"Bj\u00f6rn Schuller","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":18998136,"url":"researcher\/18998136_Florian_Eyben","fullname":"Florian Eyben","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":48001256,"url":"researcher\/48001256_Martin_Woellmer","fullname":"Martin W\u00f6llmer","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Dec 2014","journal":null,"showEnrichedPublicationItem":false,"citationCount":1,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/269636311_A_Broadcast_News_Corpus_for_Evaluation_and_Tuning_of_German_LVCSR_Systems","usePlainButton":true,"publicationUid":269636311,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/269636311_A_Broadcast_News_Corpus_for_Evaluation_and_Tuning_of_German_LVCSR_Systems","title":"A Broadcast News Corpus for Evaluation and Tuning of German LVCSR Systems","displayTitleAsLink":true,"authors":[{"id":29537335,"url":"researcher\/29537335_Felix_Weninger","fullname":"Felix Weninger","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272457294675996%401441970289453_m"},{"id":18998135,"url":"researcher\/18998135_Bjoern_Schuller","fullname":"Bj\u00f6rn Schuller","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":18998136,"url":"researcher\/18998136_Florian_Eyben","fullname":"Florian Eyben","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":48001256,"url":"researcher\/48001256_Martin_Woellmer","fullname":"Martin W\u00f6llmer","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7708667,"url":"researcher\/7708667_Gerhard_Rigoll","fullname":"Gerhard Rigoll","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Transcription of broadcast news is an interesting and challenging application\nfor large-vocabulary continuous speech recognition (LVCSR). We present in\ndetail the structure of a manually segmented and annotated corpus including\nover 160 hours of German broadcast news, and propose it as an evaluation\nframework of LVCSR systems. We show our own experimental results on the corpus,\nachieved with a state-of-the-art LVCSR decoder, measuring the effect of\ndifferent feature sets and decoding parameters, and thereby demonstrate that\nreal-time decoding of our test set is feasible on a desktop PC at 9.2% word\nerror rate.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/269636311_A_Broadcast_News_Corpus_for_Evaluation_and_Tuning_of_German_LVCSR_Systems","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Felix_Weninger\/publication\/269636311_A_Broadcast_News_Corpus_for_Evaluation_and_Tuning_of_German_LVCSR_Systems\/links\/551d65340cf252bc3a87a822.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Felix_Weninger","sourceName":"Felix Weninger","hasSourceUrl":true},"publicationUid":269636311,"publicationUrl":"publication\/269636311_A_Broadcast_News_Corpus_for_Evaluation_and_Tuning_of_German_LVCSR_Systems","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/269636311_A_Broadcast_News_Corpus_for_Evaluation_and_Tuning_of_German_LVCSR_Systems\/links\/551d65340cf252bc3a87a822\/smallpreview.png","linkId":"551d65340cf252bc3a87a822","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=269636311&reference=551d65340cf252bc3a87a822&eventCode=&origin=publication_list","widgetId":"rgw36_56ab19911d8dd"},"id":"rgw36_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=269636311&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"551d65340cf252bc3a87a822","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":224088060,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/269636311_A_Broadcast_News_Corpus_for_Evaluation_and_Tuning_of_German_LVCSR_Systems\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["A baseline set contained Mel Frequency Cepstral Coefficients (MFCC) 1\u201312 and signal log-energy, and their first (\u03b4) and second order regression coefficients (\u03b4\u03b4), which were extracted using the openEAR feature extractor [10], and are identical to the features extracted by the HTK toolkit [11]. Next, this feature set was augmented by 12 Line Spectral Pairs (LSP), along with their \u03b4 and \u03b4\u03b4 coefficients, computed with openEAR. "],"widgetId":"rgw37_56ab19911d8dd"},"id":"rgw37_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw35_56ab19911d8dd"},"id":"rgw35_56ab19911d8dd","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=269636311&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":58762740,"url":"researcher\/58762740_Abhinav_Dhall","fullname":"Abhinav Dhall","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272375044374563%401441950679548_m\/Abhinav_Dhall.png"},{"id":49642779,"url":"researcher\/49642779_Roland_Goecke","fullname":"Roland Goecke","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2030399401,"url":"researcher\/2030399401_Jyoti_Joshi","fullname":"Jyoti Joshi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2047889113,"url":"researcher\/2047889113_Karan_Sikka","fullname":"Karan Sikka","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Conference Paper","publicationDate":"Nov 2014","journal":null,"showEnrichedPublicationItem":false,"citationCount":10,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/281276446_Emotion_Recognition_In_The_Wild_Challenge_2014_Baseline_Data_and_Protocol","usePlainButton":true,"publicationUid":281276446,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/281276446_Emotion_Recognition_In_The_Wild_Challenge_2014_Baseline_Data_and_Protocol","title":"Emotion Recognition In The Wild Challenge 2014: Baseline, Data and Protocol","displayTitleAsLink":true,"authors":[{"id":58762740,"url":"researcher\/58762740_Abhinav_Dhall","fullname":"Abhinav Dhall","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272375044374563%401441950679548_m\/Abhinav_Dhall.png"},{"id":49642779,"url":"researcher\/49642779_Roland_Goecke","fullname":"Roland Goecke","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2030399401,"url":"researcher\/2030399401_Jyoti_Joshi","fullname":"Jyoti Joshi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2047889113,"url":"researcher\/2047889113_Karan_Sikka","fullname":"Karan Sikka","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2048102736,"url":"researcher\/2048102736_Tom_Gedeon","fullname":"Tom Gedeon","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["ICMI 2014; 11\/2014"],"abstract":"The Second Emotion Recognition In The Wild Challenge (EmotiW) 2014 consists of an audio-video based emotion classification challenge, which mimics the real-world conditions. Traditionally, emotion recognition has been performed on data captured in constrained lab-controlled like environment. While this data was a good starting point, such lab controlled data poorly represents the environment and conditions faced in real-world situations. With the exponential increase in the number of video clips being up-loaded online, it is worthwhile to explore the performance of emotion recognition methods that work 'in the wild'. The goal of this Grand Challenge is to carry forward the common platform defined during EmotiW 2013, for evaluation of emotion recognition methods in real-world conditions. The database in the 2014 challenge is the Acted Facial Expression In Wild (AFEW) 4.0, which has been collected from movies showing close-to-real-world conditions. The paper describes the data partitions, the baseline method and the experimental protocol.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/281276446_Emotion_Recognition_In_The_Wild_Challenge_2014_Baseline_Data_and_Protocol","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Karan_Sikka\/publication\/281276446_Emotion_Recognition_In_The_Wild_Challenge_2014_Baseline_Data_and_Protocol\/links\/55de59c708ae45e825d39d10.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Karan_Sikka","sourceName":"Karan Sikka","hasSourceUrl":true},"publicationUid":281276446,"publicationUrl":"publication\/281276446_Emotion_Recognition_In_The_Wild_Challenge_2014_Baseline_Data_and_Protocol","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/281276446_Emotion_Recognition_In_The_Wild_Challenge_2014_Baseline_Data_and_Protocol\/links\/55de59c708ae45e825d39d10\/smallpreview.png","linkId":"55de59c708ae45e825d39d10","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=281276446&reference=55de59c708ae45e825d39d10&eventCode=&origin=publication_list","widgetId":"rgw39_56ab19911d8dd"},"id":"rgw39_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=281276446&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"55de59c708ae45e825d39d10","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":224088060,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/281276446_Emotion_Recognition_In_The_Wild_Challenge_2014_Baseline_Data_and_Protocol\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["In this challenge, a set of audio features similar to the features employed in Audio Video Emotion Recognition Challenge 2011 [17] motivated from the INTERSPEECH 2010 Paralinguistic challenge (1582 features) [16] are used. The features are extracted using the open-source Emotion and Affect Recognition (openEAR) [8] toolkit backend openS- MILE [9]. "],"widgetId":"rgw40_56ab19911d8dd"},"id":"rgw40_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw38_56ab19911d8dd"},"id":"rgw38_56ab19911d8dd","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=281276446&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":224088060,"publicationLink":"publication\/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw31_56ab19911d8dd"},"id":"rgw31_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=224088060&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=154","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":154,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw30_56ab19911d8dd"},"id":"rgw30_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=224088060&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"00463514064556446b000000","name":"Bj\u00f6rn Schuller","date":null,"nameLink":"profile\/Bjoern_Schuller","filename":"09eyb1.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Bjoern_Schuller\/publication\/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit\/links\/00463514064556446b000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Bjoern_Schuller\/publication\/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit\/links\/00463514064556446b000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"d2e1450b2e0b8d260053917d24252578","showFileSizeNote":false,"fileSize":"348.04 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"00463514064556446b000000","name":"Bj\u00f6rn Schuller","date":null,"nameLink":"profile\/Bjoern_Schuller","filename":"09eyb1.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Bjoern_Schuller\/publication\/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit\/links\/00463514064556446b000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Bjoern_Schuller\/publication\/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit\/links\/00463514064556446b000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"d2e1450b2e0b8d260053917d24252578","showFileSizeNote":false,"fileSize":"348.04 KB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Conference Paper","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=xU4oVKsm6n_ANX8qEq7SS7STN4HgQzwVnzWKwtvuDmQxLURYF46wBqLZaf2vhqVR5EUJRrdnR4ZwKD89i51Qpw.6Wo8nV0K1-u75jzID3TEGQcrmm1f4VhaD7A22513WPvRtrTqkcCwCNDuKNRcZEGm1GQE_3Mr38u0RmahI0XsYw","clickOnPill":"publication.PublicationFigures.html?_sg=oQJpLGW0_pUjz1fFoKCFnAQYjFbx17zAqi87JxnbJ5e4VtMOMTjF6X6bNj3lMazTmirF5C37n_ewHuwcJrO0YQ.DTLFcE3vr_LXxgP0rnbvcPsXfxxFwf3dVtsa-5Yizh1_cU8TYPf002GYp9QjpceOa0X8RFDVBboOXARmvgR71A"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1o9o3\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FBjoern_Schuller%2Fpublication%2F224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit%2Flinks%2F00463514064556446b000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1o9o3\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=ZjgTTHLEIx0Z-zKZO_saENpK4R0_zD5mUgjj3oa6bz3V91i1Hd2SSRJFL7XpjZ0wmVub1JO7hWnSDniku8YK2w","urlHash":"5590a054b2cd3c7a1c66160545bb7eb8","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=tbyepdIzaTTVXlzooXWs2PDczmRLHfgph-bNeOJB0WJonoKNFC7AgQI2FAFnwFaXGvtOpuuwwCiJHudd5NZvHKf1QMW_7oUBN2oeHSXTzOw.H8WzRl97MhIJbO3qU_G4uLSpElcg-8cpc3b-iS5YKFBm2nclciPdrvVJ7T7Wbcu4wpUUGMECfjGiG1J-_VeNCw.redd8zGfVYrMISGYS8E7YRvILdjFbCBdF7Pv1SXNryV2QDVGBKddZMUAakZYIfqBrUaJmvhaIq8Yh4k6IZ3yGQ","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"00463514064556446b000000","trackedDownloads":{"00463514064556446b000000":{"v":false,"d":false}},"assetId":"AS:104704767954947@1401974970238","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":224088060,"commentCursorPromo":null,"widgetId":"rgw42_56ab19911d8dd"},"id":"rgw42_56ab19911d8dd","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FBjoern_Schuller%2Fpublication%2F224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit%2Flinks%2F00463514064556446b000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A104704767954947%401401974970238&publicationUid=224088060&linkId=00463514064556446b000000&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"OpenEAR - Introducing the Munich open-source emotion and affect recognition toolkit","publicationType":"Conference Paper","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=u9dqNtBi3Nm4FjZdUVJJzDQ-N90bbL3TC6hD5CvJiy8vfqkAr6G9Dz59cbH3QfvErFJ2EyHWf_YHv0SOy9iwxGJ4iVVbPwq5DKazu5_iXXs.m7MszpAvPl0utkAEu6_aioaKChO2k7lzibRJxA7u7DGsWAJxxDWz6mYFB4UFyUwz0sKFgngf9iptIhn9tHprBw.5NTAWRjn7uEXjAQ90-0HdX-mucXb4_3Os8D2EOgtk1-GtBRgCpZMP27wtdb2AtT75hmxuslgGoWkdBC3OSuyTA","publicationUid":224088060,"trackedDownloads":{"00463514064556446b000000":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw44_56ab19911d8dd"},"id":"rgw44_56ab19911d8dd","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw45_56ab19911d8dd"},"id":"rgw45_56ab19911d8dd","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw46_56ab19911d8dd"},"id":"rgw46_56ab19911d8dd","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw47_56ab19911d8dd"},"id":"rgw47_56ab19911d8dd","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw48_56ab19911d8dd"},"id":"rgw48_56ab19911d8dd","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw43_56ab19911d8dd"},"id":"rgw43_56ab19911d8dd","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw41_56ab19911d8dd"},"id":"rgw41_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab19911d8dd"},"id":"rgw2_56ab19911d8dd","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":224088060},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=224088060&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab19911d8dd"},"id":"rgw1_56ab19911d8dd","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"uASQoVWnRGF2owvq1enJPZ0HATznOoeg2gxuyAMB2ReZsoAmfrhx4lrVwwtzA48IfLRHTd3OF1IfaNwfkszFUAKBP985DYQ0pBSkXEgVOdOrOqjt7iNxZG1l3cGupvikrTpjyovBooMQ3fJ8AvMTVHCSLAmBBD7hDB4pJYSVCrHb0lzOgRqkobrlfVX+QiyekqRLXyBb5i36Q81rIDaF3mafTf7aZ2JPCsyLl9VEYMrG2HICv9stcOJ+S1mj4\/1xG7jeaB4kkpVS4WBh3s8qaT9KRxDOdCVJbVWGNMnglAo=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"OpenEAR - Introducing the Munich open-source emotion and affect recognition toolkit\" \/>\n<meta property=\"og:description\" content=\"Various open-source toolkits exist for speech recognition and speech processing. These toolkits have brought a great benefit to the research community, i.e. speeding up research. Yet, no such...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit\/links\/00463514064556446b000000\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit\" \/>\n<meta property=\"rg:id\" content=\"PB:224088060\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1109\/ACII.2009.5349350\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"OpenEAR - Introducing the Munich open-source emotion and affect recognition toolkit\" \/>\n<meta name=\"citation_author\" content=\"Florian Eyben\" \/>\n<meta name=\"citation_author\" content=\"Martin Wollmer\" \/>\n<meta name=\"citation_author\" content=\"Bjorn Schuller\" \/>\n<meta name=\"citation_conference_title\" content=\"Affective Computing and Intelligent Interaction and Workshops, 2009. ACII 2009. 3rd International Conference on\" \/>\n<meta name=\"citation_publication_date\" content=\"2009\/10\/12\" \/>\n<meta name=\"citation_firstpage\" content=\"1\" \/>\n<meta name=\"citation_lastpage\" content=\"6\" \/>\n<meta name=\"citation_doi\" content=\"10.1109\/ACII.2009.5349350\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Bjoern_Schuller\/publication\/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit\/links\/00463514064556446b000000.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Conference Paper');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-5c5615cc-686a-4f4c-95c6-017f04262cbd","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":746,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw49_56ab19911d8dd"},"id":"rgw49_56ab19911d8dd","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-5c5615cc-686a-4f4c-95c6-017f04262cbd", "54176be4ebaf57facce410c5d351e4b871b00ab0");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-5c5615cc-686a-4f4c-95c6-017f04262cbd", "54176be4ebaf57facce410c5d351e4b871b00ab0");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw50_56ab19911d8dd"},"id":"rgw50_56ab19911d8dd","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/224088060_OpenEAR_-_Introducing_the_Munich_open-source_emotion_and_affect_recognition_toolkit","requestToken":"49Wi3Wvo2dpHgzIDIrtfFs+ILAZLjf1KUBYPyHQzuwa8OVzVP6ap7\/7zPK5qle+JMV+YUC+XqYd2iRqkibJu7w9xMuxypLDo42K1kuE0fmDZlTjaSvnj1\/YsDUscXyvf6Bfifx0p3ICl93Z7zod0yTqMBs2KWjXkJZ9qHkzGo54qeRxSW39NqZFzPOoLDtFXcFksjF9uUeNqXDsnzo7aCMuklOzSOEgYWIAfX5ifRYoc8zS+HkgnIw7HyhiyN7DwR\/OJcgf3z3Vcpc0DxkjRZJDayLfkr9ClbqC2YXnZU5s=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=SkQ5M3OvB21keG8ZJ1kKXo0vsyh6OBt8yGH5kTPx9SBBSGgRT_ybg2HWEmUg4_h7","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjI0MDg4MDYwX09wZW5FQVJfLV9JbnRyb2R1Y2luZ190aGVfTXVuaWNoX29wZW4tc291cmNlX2Vtb3Rpb25fYW5kX2FmZmVjdF9yZWNvZ25pdGlvbl90b29sa2l0","signupCallToAction":"Join for free","widgetId":"rgw52_56ab19911d8dd"},"id":"rgw52_56ab19911d8dd","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw51_56ab19911d8dd"},"id":"rgw51_56ab19911d8dd","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw53_56ab19911d8dd"},"id":"rgw53_56ab19911d8dd","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Conference Paper","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
