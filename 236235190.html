<!DOCTYPE html> <html lang="en" class="" id="rgw50_56ab1a186f7c6"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="CASto3giJOsgZkmk+4mYwQPhLHO5hiwD6wbgGCMhh2TpChaIZCHU6+XkdoLjQgft6W3y9w0XjZnefQkL3cakoykFPBneztwxJlYz42ECIz034XSEUcZz5/nQIm/U/mFPlExHHc7LdIpAIyEoY53mZNSA+1pKPhr63Z+ax7GJjrmh5gyvDqrccSBBl5520uNr0eZgrItA2Gkyvs/p3Z8Lw7BghhwXkGaC/jiZUpjSPEfKhD41R4XDh4wpM9DhpZBkn6VLlCVMeIqHIYHzONlsJS0jarq3RlTUmRwV2uLLtRY="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-6f59a5cf-5383-442e-ad26-f031d182c877",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget" />
<meta property="og:description" content="Can we make Bayesian posterior MCMC sampling more efficient when faced with
very large datasets? We argue that computing the likelihood for N datapoints
twice in order to reach a single binary..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget/links/5406e5e90cf23d9765a81dca/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget" />
<meta property="rg:id" content="PB:236235190" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget" />
<meta name="citation_author" content="Anoop Korattikara" />
<meta name="citation_author" content="Yutian Chen" />
<meta name="citation_author" content="Max Welling" />
<meta name="citation_publication_date" content="2013/04/18" />
<meta name="citation_volume" content="1" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Yutian_Chen3/publication/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget/links/5406e5e90cf23d9765a81dca.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/215868066921738/styles/pow/publicliterature/FigureList.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1a186f7c6" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1a186f7c6" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab1a186f7c6">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Austerity%20in%20MCMC%20Land%3A%20Cutting%20the%20Metropolis-Hastings%20Budget&rft.title=31st%20International%20Conference%20on%20Machine%20Learning%2C%20ICML%202014&rft.jtitle=31st%20International%20Conference%20on%20Machine%20Learning%2C%20ICML%202014&rft.volume=1&rft.date=2013&rft.au=Anoop%20Korattikara%2CYutian%20Chen%2CMax%20Welling&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget</h1> <meta itemprop="headline" content="Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget/links/5406e5e90cf23d9765a81dca/smallpreview.png">  <div id="rgw7_56ab1a186f7c6" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab1a186f7c6"> <a href="researcher/80788532_Anoop_Korattikara" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Anoop Korattikara" alt="Anoop Korattikara" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Anoop Korattikara</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw9_56ab1a186f7c6">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/80788532_Anoop_Korattikara"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Anoop Korattikara" alt="Anoop Korattikara" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/80788532_Anoop_Korattikara" class="display-name">Anoop Korattikara</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56ab1a186f7c6" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Yutian_Chen3" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A278543817822209%401443421429383_m" title="Yutian Chen" alt="Yutian Chen" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Yutian Chen</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw11_56ab1a186f7c6" data-account-key="Yutian_Chen3">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Yutian_Chen3"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A278543817822209%401443421429383_l" title="Yutian Chen" alt="Yutian Chen" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Yutian_Chen3" class="display-name">Yutian Chen</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_of_Cambridge" title="University of Cambridge">University of Cambridge</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56ab1a186f7c6"> <a href="researcher/69847505_Max_Welling" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Max Welling" alt="Max Welling" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Max Welling</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw13_56ab1a186f7c6">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/69847505_Max_Welling"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Max Welling" alt="Max Welling" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/69847505_Max_Welling" class="display-name">Max Welling</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">     31st International Conference on Machine Learning, ICML 2014   <meta itemprop="datePublished" content="2013-04">  04/2013;  1.             <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1304.5299" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw14_56ab1a186f7c6" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>Can we make Bayesian posterior MCMC sampling more efficient when faced with<br />
very large datasets? We argue that computing the likelihood for N datapoints<br />
twice in order to reach a single binary decision is computationally<br />
inefficient. We introduce an approximate Metropolis-Hastings rule based on a<br />
sequential hypothesis test which allows us to accept or reject samples with<br />
high confidence using only a fraction of the data required for the exact MH<br />
rule. While this introduces an asymptotic bias, we show that this bias can be<br />
controlled and is more than offset by a decrease in variance due to our ability<br />
to draw more samples per unit of time. We show that the same idea can also be<br />
applied to Gibbs sampling in densely connected graphs.</div> </p>  </div>  </div>   </div>     <div id="rgw15_56ab1a186f7c6" class="figure-carousel"> <div class="carousel-hd"> Figures in this publication </div> <div class="carousel-bd"> <ul class="clearfix">  <li> <a href="/figure/236235190_fig1_Figure-8-An-example-of-the-random-walk-followed-by-z-with-m-std-0" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 8: An example of the random walk followed by z with μ std &gt; 0. " data-key="236235190_fig1_Figure-8-An-example-of-the-random-walk-followed-by-z-with-m-std-0"> <img class="fig" src="https://www.researchgate.net/profile/Yutian_Chen3/publication/236235190/figure/fig1/Figure-8-An-example-of-the-random-walk-followed-by-z-with-m-std-0_small.png" alt="Figure 8: An example of the random walk followed by z with μ std &gt; 0. " title="Figure 8: An example of the random walk followed by z with μ std &gt; 0. "/> </a> </li>  <li> <a href="/figure/236235190_fig2_Figure-9-Sequential-test-with-3-mini-batches-Red-dashed-line-is-the-bound-G" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 9: Sequential test with 3 mini-batches. Red dashed line is the..." data-key="236235190_fig2_Figure-9-Sequential-test-with-3-mini-batches-Red-dashed-line-is-the-bound-G"> <img class="fig" src="https://www.researchgate.net/profile/Yutian_Chen3/publication/236235190/figure/fig2/Figure-9-Sequential-test-with-3-mini-batches-Red-dashed-line-is-the-bound-G_small.png" alt="Figure 9: Sequential test with 3 mini-batches. Red dashed line is the..." title="Figure 9: Sequential test with 3 mini-batches. Red dashed line is the..."/> </a> </li>  </ul> </div> </div> <div class="action-container"> <div id="rgw16_56ab1a186f7c6" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw30_56ab1a186f7c6">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw42_56ab1a186f7c6">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Yutian_Chen3/publication/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget/links/5406e5e90cf23d9765a81dca.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Yutian_Chen3">Yutian Chen</a>, <span class="js-publication-date"> Sep 03, 2014 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw44_56ab1a186f7c6" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw45_56ab1a186f7c6" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw46_56ab1a186f7c6" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw47_56ab1a186f7c6" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw48_56ab1a186f7c6" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw49_56ab1a186f7c6" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw43_56ab1a186f7c6" src="https://www.researchgate.net/c/o1o9o3/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FYutian_Chen3%2Fpublication%2F236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget%2Flinks%2F5406e5e90cf23d9765a81dca.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw29_56ab1a186f7c6"  itemprop="articleBody">  <p>Page 1</p> <p>Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget<br />Anoop Korattikara∗1, Yutian Chen†1,2and Max Welling‡1,3<br />1Department of Computer Science, University of California, Irvine<br />2Department of Engineering, University of Cambridge<br />3Informatics Institute, University of Amsterdam<br />Abstract<br />Can we make Bayesian posterior MCMC sampling more efficient when faced with very large datasets?<br />We argue that computing the likelihood for N datapoints in the Metropolis-Hastings (MH) test to reach<br />a single binary decision is computationally inefficient. We introduce an approximate MH rule based on<br />a sequential hypothesis test that allows us to accept or reject samples with high confidence using only a<br />fraction of the data required for the exact MH rule. While this method introduces an asymptotic bias,<br />we show that this bias can be controlled and is more than offset by a decrease in variance due to our<br />ability to draw more samples per unit of time.<br />1Introduction<br />Markov chain Monte Carlo (MCMC) sampling has been the main workhorse of Bayesian computation since<br />the 1990s. A canonical MCMC algorithm proposes samples from a distribution q and then accepts or rejects<br />these proposals with a certain probability given by the Metropolis-Hastings (MH) formula [Metropolis et al.,<br />1953, Hastings, 1970]. For each proposed sample, the MH rule needs to examine the likelihood of all data-<br />items. When the number of data-cases is large this is an awful lot of computation for one bit of information,<br />namely whether to accept or reject a proposal.<br />In today’s Big Data world, we need to rethink our Bayesian inference algorithms. Standard MCMC<br />methods do not meet the Big Data challenge for the reason described above. Researchers have made some<br />progress in terms of making MCMC more efficient, mostly by focusing on parallelization. Very few question<br />the algorithm itself: is the standard MCMC paradigm really optimally efficient in achieving its goals? We<br />claim it is not.<br />Any method that includes computation as an essential ingredient should acknowledge that there is a finite<br />amount of time, T, to finish a calculation. An efficient MCMC algorithm should therefore decrease the “error”<br />(properly defined) maximally in the given time T. For MCMC algorithms, there are two contributions to this<br />error: bias and variance. Bias occurs because the chain needs to burn in during which it is sampling from<br />the wrong distribution. Bias usually decreases fast, as evidenced by the fact that practitioners are willing<br />to wait until the bias has (almost) completely vanished after which they discard these “burn-in samples”.<br />The second cause of error is sampling variance, which occurs because of the random nature of the sampling<br />process. The retained samples after burn-in will reduce the variance as O(1/T).<br />However, given a finite amount of computational time, it is not at all clear whether the strategy of<br />retaining few unbiased samples and accepting an error dominated by variance is optimal. Perhaps, by<br />decreasing the bias more slowly we could sample faster and thus reduce variance faster? In this paper we<br />illustrate this effect by cutting the computational budget of the MH accept/reject step. To achieve that,<br />we conduct sequential hypothesis tests to decide whether to accept or reject a given sample and find that<br />the majority of these decisions can be made based on a small fraction of the data with high confidence. A<br />∗akoratti@ics.uci.edu<br />†yutian.chen@eng.cam.edu<br />‡welling@ics.uci.edu<br />1<br />arXiv:1304.5299v4  [cs.LG]  14 Feb 2014</p>  <p>Page 2</p> <p>related method was used in Singh et al. [2012], where the factors of a graphical model are sub-sampled to<br />compute fixed-width confidence intervals for the log-likelihood in the MH test.<br />Our “philosophy” runs deeper than the algorithm proposed here. We advocate MCMC algorithms with<br />a “bias-knob”, allowing one to dial down the bias at a rate that optimally balances error due to bias and<br />variance. We only know of one algorithm that would also adhere to this strategy: stochastic gradient<br />Langevin dynamics [Welling and Teh, 2011] and its successor stochastic gradient Fisher scoring [Ahn et al.,<br />2012]. In their case the bias-knob was the stepsize. These algorithms do not have an MH step which resulted<br />in occasional samples with extremely low probability. We show that our approximate MH step largely<br />resolves this, still avoiding O(N) computations per iteration.<br />In the next section we introduce the MH algorithm and discuss its drawbacks. Then in Section 3, we<br />introduce the idea of approximate MCMC methods and the bias variance trade-off involved. We develop<br />approximate MH tests for Bayesian posterior sampling in Section 4 and present a theoretical analysis in<br />Section 5. Finally, we show our experimental results in Section 6 and conclude in Section 7.<br />2 The Metropolis-Hastings algorithm<br />MCMC methods generate samples from a distribution S0(θ) by simulating a Markov chain designed to have<br />stationary distribution S0(θ). A Markov chain with a given stationary distribution can be constructed using<br />the Metropolis-Hastings algorithm [Metropolis et al., 1953, Hastings, 1970], which uses the following rule for<br />transitioning from the current state θtto the next state θt+1:<br />1. Draw a candidate state θ?from a proposal distribution q(θ?|θt)<br />2. Compute the acceptance probability:<br />Pa= min<br />?<br />1,S0(θ?)q(θt|θ?)<br />S0(θt)q(θ?|θt)<br />?<br />(1)<br />3. Draw u ∼ Uniform[0,1]. If u &lt; Paset θt+1← θ?, otherwise set θt+1← θt.<br />Following this transition rule ensures that the stationary distribution of the Markov chain is S0(θ). The<br />samples from the Markov chain are usually used to estimate the expectation of a function f(θ) with respect<br />to S0(θ). To do this we collect T samples and approximate the expectation I = ?f?S0asˆI =<br />Since the stationary distribution of the Markov chain is S0,ˆI is an unbiased estimator of I (if we ignore<br />burn-in).<br />The variance ofˆI is V = E[(?f?S0−1<br />of the Markov chain. It is well known that V ≈ σ2<br />and τ is the integrated auto-correlation time, which is a measure of the interval between independent samples<br />[Gamerman and Lopes, 2006]. Usually, it is quite difficult to design a chain that mixes fast and therefore, the<br />auto-correlation time will be quite high. Also, for many important problems, evaluating S0(θ) to compute<br />the acceptance probability Pain every step is so expensive that we can collect only a very small number of<br />samples (T) in a realistic amount of computational time. Thus the variance ofˆI can be prohibitively high,<br />even though it is unbiased.<br />1<br />T<br />?T<br />t=1f(θt).<br />T<br />?T<br />t=1f(θt))2], where the expectation is over multiple simulations<br />f,S0τ/T, where σ2<br />f,S0is the variance of f with respect to S0<br />3 Approximate MCMC and the Bias-Variance Tradeoff<br />Ironically, the reason MCMC methods are so slow is that they are designed to be unbiased. If we were to<br />allow a small bias in the stationary distribution, it is possible to design a Markov chain that can be simulated<br />cheaply [Welling and Teh, 2011, Ahn et al., 2012]. That is, to estimate I = ?f?S0, we can use a Markov chain<br />with stationary distribution S?where ? is a parameter that can be used to control the bias in the algorithm.<br />Then I can be estimated asˆI =1<br />T<br />As ? → 0, S?approaches S0(the distribution of interest) but it becomes expensive to simulate the Markov<br />chain. Therefore, the bias inˆI is low, but the variance is high because we can collect only a small number<br />of samples in a given amount of computational time. As ? moves away from 0, it becomes cheap to simulate<br />?T<br />t=1f(θt), computed using samples from S?instead of S0.<br />2</p>  <p>Page 3</p> <p>the Markov chain but the difference between S?and S0grows. Therefore,ˆI will have higher bias, but lower<br />variance because we can collect a larger number of samples in the same amount of computational time. This<br />is a classical bias-variance trade-off and can be studied using the risk of the estimator.<br />The risk can be defined as the mean squared error inˆI, i.e. R = E[(I −ˆI)2], where the expectation is<br />taken over multiple simulations of the Markov chain. It is easy to show that the risk can be decomposed<br />as R = B2+ V , where B is the bias and V is the variance. If we ignore burn-in, it can be shown that<br />B = ?f?S?− ?f?S0and V = E[(?f?S?−1<br />The optimal setting of ? that minimizes the risk depends on the amount of computational time available.<br />If we have an infinite amount of computational time, we should set ? to 0. Then there is no bias, and the<br />variance can be brought down to 0 by drawing an infinite number of samples. This is the traditional MCMC<br />setting. However, given a finite amount of computational time, this setting may not be optimal. It might be<br />better to tolerate a small amount of bias in the stationary distribution if it allows us to reduce the variance<br />quickly, either by making it cheaper to collect a large number of samples or by mixing faster.<br />It is interesting to note that two recently proposed algorithms follow this paradigm: Stochastic Gradient<br />Langevin Dynamics (SGLD) [Welling and Teh, 2011] and Stochastic Gradient Fisher Scoring (SGFS) [Ahn<br />et al., 2012]. These algorithms are biased because they omit the required Metropolis-Hastings tests. However,<br />in both cases, a knob ? (the step-size of the proposal distribution) is available to control the bias. As ? → 0,<br />the acceptance probability Pa→ 1 and the bias from not conducting MH tests disappears. However, when<br />? → 0 the chain mixes very slowly and the variance increases because the auto-correlation time τ → ∞. As<br />? is increased from 0, the auto-correlation, and therefore the variance, reduces. But, at the same time, the<br />acceptance probability reduces and the bias from not conducting MH tests increases as well.<br />In the next section, we will develop another class of approximate MCMC algorithms for the case where<br />the target S0is a Bayesian posterior distribution given a very large dataset. We achieve this by developing<br />an approximate Metropolis-Hastings test, equipped with a knob for controlling the bias. Moreover, our<br />algorithm has the advantage that it can be used with any proposal distribution. For example, our method<br />allows approximate MCMC methods to be applied to problems where it is impossible to compute gradients<br />(which is necessary to apply SGLD/SGFS). Or, we can even combine our method with SGLD/SGFS, to<br />obtain the best of both worlds.<br />Tf(θt))2] ≈ σ2<br />f,S?τ/T.<br />4 Approximate Metropolis-Hastings Test for Bayesian Posterior<br />Sampling<br />An important method in the toolbox of Bayesian inference is posterior sampling. Given a dataset of N<br />independent observations XN = {x1,...,xN}, which we model using a distribution p(x;θ) parameterized<br />by θ, defined on a space Θ with measure Ω, and a prior distribution ρ(θ), the task is to sample from the<br />posterior distribution S0(θ) ∝ ρ(θ)?N<br />has to be done for each posterior sample we generate. Spending O(N) computation to get just 1 bit of<br />information, i.e. whether to accept or reject a sample, is likely not the best use of computational resources.<br />But, if we try to develop accept/reject tests that satisfy detailed balance exactly with respect to the<br />posterior distribution using only sub-samples of data, we will quickly see the no free lunch theorem kicking in.<br />For example, the pseudo marginal MCMC method [Andrieu and Roberts, 2009] and the method developed<br />by Lin et al. [2000] provide a way to conduct exact accept/reject tests using unbiased estimators of the<br />likelihood. However, unbiased estimators of the likelihood that can be computed from mini-batches of data,<br />such as the Poisson estimator [Fearnhead et al., 2008] or the Kennedy-Bhanot estimator [Lin et al., 2000]<br />have very high variance for large datasets. Because of this, once we get a very high estimate of the likelihood,<br />almost all proposed moves are rejected and the algorithm gets stuck.<br />Thus, we should be willing to tolerate some error in the stationary distribution if we want faster ac-<br />cept/reject tests. If we can offset this small bias by drawing a large number of samples cheaply and reducing<br />the variance faster, we can establish a potentially large reduction in the risk.<br />We will now show how to develop such approximate tests by reformulating the MH test as a statistical<br />decision problem. It is easy to see that the original MH test (Eqn. 1) is equivalent to the following procedure:<br />Draw u ∼ Uniform[0,1] and accept the proposal θ?if the average difference µ in the log-likelihoods of θ?and<br />i=1p(xi;θ).<br />If the dataset has a billion datapoints, it becomes very painful to compute S0(.) in the MH test, which<br />3</p>  <p>Page 4</p> <p>θtis greater than a threshold µ0, i.e. compute<br />µ0=<br />1<br />Nlog<br />?<br />uρ(θt)q(θ?|θt)<br />ρ(θ?)q(θt|θ?)<br />?<br />, and (2)<br />µ =<br />1<br />N<br />N<br />?<br />i=1<br />li where li= logp(xi;θ?) − logp(xi;θt)(3)<br />Then if µ &gt; µ0, accept the proposal and set θt+1← θ?. If µ ≤ µ0, reject the proposal and set θt+1← θt.<br />This reformulation of the MH test makes it very easy to frame it as a statistical hypothesis test. Given<br />µ0and a random sample {li1,...,lin} drawn without replacement from the population {l1,...,lN}, can we<br />decide whether the population mean µ is greater than or less than the threshold µ0? The answer to this<br />depends on the precision in the random sample. If the difference between the sample mean¯l and µ0 is<br />significantly greater than the standard deviation s of¯l, we can make the decision to accept or reject the<br />proposal confidently. If not, we should draw more data to increase the precision of¯l (reduce s) until we have<br />enough evidence to make a decision.<br />More formally, we test the hypotheses H1: µ &gt; µ0vs H2: µ &lt; µ0. To do this, we proceed as follows: We<br />compute the sample mean¯l and the sample standard deviation sl=<br />deviation of¯l can be estimated as:<br />sl<br />√n<br />?<br />without replacement from a finite-sized population. Then, we compute the test statistic:<br />?<br />(l2− (¯l)2)<br />n<br />n−1. Then the standard<br />s =<br />?<br />1 −n − 1<br />N − 1<br />(4)<br />where1 −n−1<br />N−1, the finite population correction term, is applied because we are drawing the subsample<br />t =<br />¯l − µ0<br />s<br />(5)<br />Approximate MH test<br />Input: θt, θ?, ?, µ0, XN, m<br />Output: accept<br />1: Initialize estimated means¯l ← 0 and l2← 0<br />2: Initialize n ← 0, done ← false<br />3: Draw u ∼ Uniform[0,1]<br />4: while not done do<br />5:<br />Draw mini-batch X of size min (m, N − n) without replacement from XN and set XN← XN\ X<br />6:<br />Update¯l and l2using X, and n ← n + |X|<br />7:<br />Estimate std s using Eqn. 4<br />?????<br />10:<br />accept ← true if¯l &gt; µ0and false otherwise<br />11:<br />done ← true<br />12:<br />end if<br />13: end while<br />8:<br />Compute δ ← 1 − φn−1<br />if δ &lt; ? then<br />¯l − µ0<br />s<br />????<br />?<br />9:<br />If n is large enough for the central limit theorem (CLT) to hold, the test statistic t follows a standard<br />Student-t distribution with n − 1 degrees of freedom, when µ = µ0 (see Fig. 7 in supplementary for an<br />empirical verification).Then, we compute δ = 1 − φn−1(|t|) where φn−1(.) is the cdf of the standard<br />Student-t distribution with n−1 degrees of freedom. If δ &lt; ? (a fixed threshold) we can confidently say that<br />µ is significantly different from µ0. In this case, if¯l &gt; µ0, we decide µ &gt; µ0, otherwise we decide µ &lt; µ0. If<br />δ ≥ ?, we do not have enough evidence to make a decision. In this case, we draw more data to reduce the<br />uncertainty, s, in the sample mean¯l. We keep drawing more data until we have the required confidence (i.e.<br />until δ &lt; ?). Note, that this procedure will terminate because when we have used all the available data, i.e.<br />4</p>  <p>Page 5</p> <p>n = N, the standard deviation s is 0, the sample mean¯l = µ and δ = 0 &lt; ?. So, we will make the same<br />decision as the original MH test would make. Pseudo-code for our test is shown in Algorithm . Here, we<br />start with a mini-batch of size m for the first test and increase it by m datapoints when required.<br />The advantage of our method is that often we can make confident decisions with n &lt; N datapoints and<br />save on computation, although we introduce a small bias in the stationary distribution. But, we can use<br />the computational time we save to draw more samples and reduce the variance. The bias-variance trade-off<br />can be controlled by adjusting the knob ?. When ? is high, we make decisions without sufficient evidence<br />and introduce a high bias. As ? → 0, we make more accurate decisions but are forced to examine more data<br />which results in high variance.<br />Our algorithm will behave erratically if the CLT does not hold, e.g. with very sparse datasets or datasets<br />with extreme outliers. The CLT assumption can be easily tested empirically before running the algorithm to<br />avoid such pathological situations. The sequential hypothesis testing method can also be used to speed-up<br />Gibbs sampling in densely connected Markov Random Fields. We explore this idea briefly in Section F of<br />the supplementary.<br />5 Error Analysis and Test Design<br />In 5.1, we study the relation between the parameter ?, the error E of the complete sequential test, the error<br />∆ in the acceptance probability and the error in the stationary distribution. In 5.2, we describe how to<br />design an optimal test that minimizes data usage given a bound on the error.<br />5.1Error Analysis and Estimation<br />The parameter ? is an upper-bound on the error of a single test and not the error of the complete sequential<br />test. To compute this error, we assume a) n is large enough that the t statistics can be approximated<br />with z statistics, and b) the joint distribution of the¯l’s corresponding to different mini-batches used in<br />the test is multivariate normal. Under these assumptions, we can show that the test statistic at different<br />stages of the sequential test follows a Gaussian Random Walk process. This allows us to compute the error<br />of the sequential test E(µstd,m,?), and the expected proportion of the data required to reach a decision<br />¯ π(µstd,m,?), using an efficient dynamic programming algorithm. Note that E and ¯ π depend on θ, θ?and u<br />only through the ‘standardized mean’ defined as µstd(u,θ,θ?)<br />def<br />=(µ(θ,θ?) − µ0(θ,θ?,u))√N − 1<br />σl(θ,θ?)<br />where σlis<br />the true standard deviation of the li’s. See Section A of the supplementary for a detailed derivation and an<br />empirical validation of the assumptions.<br />Fig. 1 shows the theoretical and actual error of 1000 sequential tests for the logistic regression model<br />described in Section 6.1. The error E(µstd,m,?) is highest in the worst case when µ = µ0. Therefore,<br />E(0,m,?) is an upper-bound on E. Since the error decreases sharply as µ moves away from µ0, we can get a<br />more useful estimate of E if we have some knowledge about the distribution of µstd’s that will be encountered<br />during the Markov chain simulation.<br />Now, let Pa,?(θ,θ?) be the actual acceptance probability of our algorithm and let ∆(θ,θ?)<br />Pa(θ,θ?) be the error in Pa,?. In Section B of the supplementary, we show that for any (θ,θ?):<br />?1<br />Thus, the errors corresponding to different u’s partly cancel each other. As a result, although |∆(θ,θ?)|<br />is upper-bounded by the worst-case error E(0,m,?) of the sequential test, the actual error is usually much<br />smaller. For any given (θ,θ?), ∆ can be computed easily using 1-dimensional quadrature.<br />Finally, we show that the error in the stationary distribution is bounded linearly by ∆max= supθ,θ? |∆(θ,θ?)|.<br />As noted above, ∆max≤ E(0,m,?) but is usually much smaller. Let dv(P,Q) denote the total variation dis-<br />tance1between two distributions, P and Q. If the transition kernel T0of the exact Markov chain satisfies the<br />1The total variation distance between two distributions P and Q, that are absolutely continuous w.r.t. measure Ω, is defined<br />as dv(P,Q)def<br />=<br />2<br />be more precise).<br />def<br />= Pa,?(θ,θ?)−<br />∆ =<br />Pa<br />E(µstd(u))du −<br />?Pa<br />0<br />E(µstd(u))du(6)<br />1<br />?<br />θ∈Θ|fP(θ) − fQ(θ)|dΩ(θ) where fP and fQare their respective densities (or Radon-Nikodym derivatives to<br />5</p>  <p>Page 6</p> <p>−20<br />−10<br />0<br />10<br />20<br />0.01<br />0.05<br />0.1<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br /> <br />Standardized Mean, µstd<br />ǫ<br /> <br />P(Error)<br />Simulation with 68% CI<br />Theoretical<br />Upper Bound<br />Figure 1: Error E estimated using simulation (blue cross with 1 σ error bar) and dynamic programming (red<br />line). An upper bound (black dashed line) is also shown.<br />contraction condition dv(PT0,S0) ≤ ηdv(P,S0) for all probability distributions P with a constant η ∈ [0,1),<br />we can prove (see supplementary Section C) the following upper bound on the error in the stationary distri-<br />bution:<br />Theorem 1. The distance between the posterior distribution S0and the stationary distribution of our ap-<br />proximate Markov chain S?is upper bounded as:<br />dv(S0,S?) ≤∆max<br />1 − η<br />5.2 Optimal Sequential Test Design<br />We now briefly describe how to choose the parameters of the algorithm: ?, the error of a single test and m,<br />the mini-batch size. A very simple strategy we recommend is to choose m ≈ 500 so that the Central Limit<br />Theorem holds and keep ? as small as possible while maintaining a low average data usage. This rule works<br />well in practice and is used in Experiments 6.1 - 6.4.<br />The more discerning practitioner can design an optimal test that minimizes the data used while keeping<br />the error below a given tolerance. Ideally, we want to do this based on a tolerance on the error in the<br />stationary distribution S?. Unfortunately, this error depends on the contraction parameter, η, of the exact<br />transition kernel, which is difficult to compute. A more practical choice is a bound on the error ∆ in the<br />acceptance probability, since the error in S?increases linearly with ∆. Since ∆ is a function of (θ,θ?), we<br />can try to control the average value of ∆ over the empirical distribution of (θ,θ?) that would be encountered<br />while simulating the Markov chain. Given a tolerance ∆∗on this average error, we can find the optimal m<br />and ? by solving the following optimization problem (e.g. using grid search) to minimize the average data<br />usage :<br />min<br />m,?Eθ,θ? [Eu¯ π(µstd(u,θ,θ?),m,?)]<br />s.t. Eθ,θ?|∆(m,?,θ,θ?)| ≤ ∆∗<br />(7)<br />In the above equation, we estimate the average data usage, Eu[¯ π], and the error in the acceptance probability,<br />∆, using dynamic programming with one dimensional numerical quadrature on u. The empirical distribution<br />for computing the expectation with respect to (θ,θ?) can be obtained using a trial run of the Markov chain.<br />6</p>  <p>Page 7</p> <p>050100150200250300350400<br />−12<br />−10<br />−8<br />−6<br />−4<br />−2<br />0<br />Wall Clock Time (secs)<br />Log (Risk)<br /> <br /> <br />ε =0.00, T = 75484<br />ε =0.01, T = 133069<br />ε =0.05, T = 200672<br />ε =0.10, T = 257897<br />ε =0.20, T = 422978<br />Figure 2: Logistic Regression: Risk in predictive mean.<br />Without a trial run the best we can do is to control the worst case error E(0,m,?) (which is also an upper-<br />bound on ∆) in each sequential test by solving the following minimization problem:<br />min<br />m,?¯ π(0,m,?) s.t. E(0,m,?) ≤ ∆∗<br />(8)<br />But this leads to a very conservative design as the worst case error is usually much higher than the average<br />case error. We illustrate the sequential design in Experiment 6.5. More details and a generalization of this<br />method is given in supplementary Section D.<br />6Experiments<br />6.1Random Walk - Logistic Regression<br />We first test our method using a random walk proposal q(θ?|θt) = N(θt,σ2<br />walk proposal is not efficient, it is very useful for illustrating our algorithm because the proposal does not<br />contain any information about the target distribution, unlike Langevin or Hamiltonian methods. So, the<br />responsibility of converging to the correct distribution lies solely with the MH test. Also since q is symmetric,<br />it does not appear in the MH test and we can use µ0=<br />The target distribution in this experiment was the posterior for a logistic regression model trained on the<br />MNIST dataset for classifying digits 7 vs 9. The dataset consisted of 12214 datapoints and we reduced the<br />dimensionality from 784 to 50 using PCA. We chose a zero mean spherical Gaussian prior with precision =<br />10, and set σRW= 0.01.<br />In Fig. 2, we show how the logarithm of the risk in estimating the predictive mean, decreases as a function<br />of wall clock time. The predictive mean of a test point x∗is defined as Ep(θ|XN)[p(x∗|θ)]. To calculate the<br />risk, we first estimate the true predictive mean using a long run of Hybrid Monte Carlo. Then, we compute<br />multiple estimates of the predictive mean from our approximate algorithm and obtain the risk as the mean<br />squared error in these estimates. We plot the average risk of 2037 datapoints in the test set. Since the risk<br />R = B2+V = B2+σ2f<br />T, we expect it to decrease as a function of time until the bias dominates the variance.<br />The figure shows that even after collecting a lot of samples, the risk is still dominated by the variance and<br />the minimum risk is obtained with ? &gt; 0.<br />RW). Although the random<br />1<br />Nlog[uρ(θt)/ρ(θ?)].<br />6.2Independent Component Analysis<br />Next, we use our algorithm to sample from the posterior distribution of the unmixing matrix in Inde-<br />pendent Component Analysis (ICA) [Hyv¨ arinen and Oja, 2000]. When using prewhitened data, the un-<br />mixing matrix W ∈ RD×Dis constrained to lie on the Stiefel manifold of orthonormal matrices.<br />choose a prior that is uniform over the manifold and zero elsewhere. We model the data as p(x|W) =<br />We<br />7</p>  <p>Page 8</p> <p>010002000<br />Wall Clock Time (secs)<br />30004000500060007000<br />−3.5<br />−3<br />−2.5<br />−2<br />−1.5<br />−1<br />−0.5<br />0<br />0.5<br />Log (Risk)<br /> <br /> <br />ε = 0, T = 5992<br />ε = 0.01, T = 11320<br />ε = 0.05, T = 40973<br />ε = 0.1, T = 171917<br />ε = 0.2, T = 1894000<br />Figure 3: ICA: Risk in mean of Amari distance<br />|det(W)|?D<br />distribution [Ouyang, 2008]. Since this is a symmetric proposal distribution, it does not appear in the MH<br />test and we can use µ0=<br />To perform a large scale experiment, we created a synthetic dataset by mixing 1.95 million samples of 4<br />sources: (a) a Classical music recording (b) street / traffic noise (c) &amp; (d) 2 independent Gaussian sources.<br />To measure the correctness of the sampler, we measure the risk in estimating I = Ep(W|X)[dA(W,W0)] where<br />the test function dAis the Amari distance [Amari et al., 1996] and W0is the true unmixing matrix. We<br />computed the ground truth using a long run (T = 100K samples) of the exact MH algorithm. Then we ran<br />each algorithm 10 times, each time for ≈ 6400 secs. We calculated the risk by averaging the squared error in<br />the estimate from each Markov chain, over the 10 chains. This is shown in Fig. 3. Note that even after 6400<br />secs the variance dominates the bias, as evidenced by the still decreasing risk, except for the most biased<br />algorithm with ? = 0.2. Also, the lowest risk at 6400 secs is obtained with ? = 0.1 and not the exact MH<br />algorithm (? = 0). But we expect the exact algorithm to outperform all the approximate algorithms if we<br />were to run for an infinite time.<br />j=1<br />?4cosh2(1<br />2wT<br />jx)?−1where wjare the rows of W. Since the prior is zero outside the manifold,<br />the same is true for the posterior. Therefore we use a random walk on the Stiefel manifold as a proposal<br />1<br />Nlog[u].<br />6.3Variable selection in Logistic Regression<br />Now, we apply our MH test to variable selection in a logistic regression model using the reversible jump<br />MCMC algorithm of Green [1995]. We use a model that is similar to the Bayesian LASSO model for linear<br />regression described in Chen et al. [2011]. Specifically, given D input features, our parameter θ = {β,γ}<br />where β is a vector of D regression coefficients and γ is a D dimensional binary vector that indicates whether<br />a particular feature is included in the model or not. The prior we choose for β is p(βj|γ,ν) =<br />if γj = 1. If γj = 0, βj does not appear in the model. Here ν is a shrinkage parameter that pushes βj<br />1<br />2νexp<br />?<br />−|βj|<br />ν<br />?<br />towards 0, and we choose a prior p(ν) ∝ 1/ν. We also place a right truncated Poisson prior p(γ|λ) ∝<br />on γ to control the size of the model, k =?D<br />p(β,γ|XN,yN,λ) ∝ lN(β,γ)?β?−k<br />out λ, we use it as a parameter to control the size of the model. We use the same proposal distribution as<br />in Chen et al. [2011] which is a mixture of 3 type of moves that are picked randomly in each iteration: an<br />update move, a birth move and a death move. A detailed description is given in Supplementary Section E.<br />We applied this to the MiniBooNE dataset from the UCI machine learning repository[Bache and Lichman,<br />λk<br />?k!<br />?D<br />k<br />j=1γjWe set λ = 10−10in this experiment.<br />Denoting the likelihood of the data by lN(β,γ), the posterior distribution after integrating out ν is<br />1λkB(k,D−k+1) where B(.,.) is the beta function. Instead of integrating<br />8</p>  <p>Page 9</p> <p>0 1000<br />Wall Clock Time (secs)<br />200030004000<br />−9<br />−8<br />−7<br />−6<br />−5<br />−4<br />−3<br />−2<br />−1<br />Log (Risk)<br /> <br /> <br />ε = 0, T = 24583<br />ε = 0.01, T = 137375<br />ε = 0.05, T = 245906<br />ε = 0.1, T = 419090<br />Figure 4: RJMCMC: Risk in predictive mean<br />2013]. Here the task is to classify electron neutrinos (signal) from muon neutrinos (background). There are<br />130,065 datapoints (28% in +ve class) with 50 features to which we add a constant feature of 1’s. We<br />randomly split the data into a training (80%) and testing (20%) set. To compute ground truth, we collected<br />T=400K samples using the exact reversible jump algorithm (? = 0). Then, we ran the approximate MH<br />algorithm with different values of ? for around 3500 seconds. We plot the risk in predictive mean of test data<br />(estimated from 10 Markov chains) in Fig. 4. Again we see that the lowest risk is obtained with ? &gt; 0.<br />The acceptance rates for the birth/death moves starts off at ≈ 20% but dies down to ≈ 2% once a<br />good model is found. The acceptance rate for update moves is kept at ≈ 50%. The model also suffers<br />from local minima. For the plot in Fig. 4, we started with only one variable and we ended up learning<br />models with around 12 features, giving a classification error ≈ 15%. But, if we initialize the sampler with<br />all features included and initialize β to the MAP value, we learn models with around 45 features, but with<br />a lower classification error ≈ 10%. Both the exact reversible jump algorithm and our approximate version<br />suffer from this problem. We should bear this in mind when interpreting “ground truth”. However, we<br />have observed that when initialized with the same values, we obtain similar results with the approximate<br />algorithm and the exact algorithm (see e.g. Fig. 13 in supplementary).<br />6.4 Stochastic Gradient Langevin Dynamics<br />Finally, we apply our method to Stochastic Gradient Langevin Dynamics[Welling and Teh, 2011]. In each<br />iteration, we randomly draw a mini-batch Xnof size n, and propose:<br />?<br />n<br />θ?∼ q(.|θ,Xn) = N<br />θ +α<br />2∇θ<br />?<br />N<br />?<br />x∈Xn<br />logp(x|θ) + logρ(θ)<br />?<br />,α<br />?<br />(9)<br />The proposed state θ?is always accepted (without conducting any MH test). Since the acceptance probability<br />approaches 1 as we reduce α, the bias from not conducting the MH test can be kept under control by using<br />α ≈ 0. However, we have to use a reasonably large α to keep the mixing rate high. This can be problematic<br />for some distributions, because SGLD relies solely on gradients of the log density and it can be easily thrown<br />off track by large gradients in low density regions, unless α ≈ 0.<br />As an example, consider an L1-regularized linear regression model. Given a dataset {xi,yi}N<br />xi are predictors and yi are targets, we use a Gaussian error model p(y|x,θ) ∝ exp?−λ<br />ourselves to a toy version of the problem where θ and x are one dimensional. We use a synthetic dataset with<br />N = 10000 datapoints generated as yi= 0.5xi+ ξ where ξ ∼ N(0,1/3). We choose λ = 3 and λ0= 4950,<br />so that the prior is not washed out by the likelihood. The posterior density and the gradient of the log<br />posterior are shown in figures 5(a) and 5(b) respectively.<br />i=1where<br />2(y − θTx)2?<br />and<br />choose a Laplacian prior for the parameters p(θ) ∝ exp(−λ0?θ?1). For pedagogical reasons, we will restrict<br />9</p>  <p>Page 10</p> <p>00.010.020.03<br />θ<br />0.040.050.06<br />0<br />10<br />20<br />30<br />40<br />50<br />60<br />70<br />80<br />p(θ|Data)<br />(a) Posterior density<br />00.010.020.03<br />θ<br />0.040.050.06<br />−2000<br />0<br />2000<br />4000<br />6000<br />8000<br />10000<br />∇θ log p(θ|Data)<br />(b) Gradient of log posterior<br />00.01 0.02 0.03<br />θ<br />0.040.050.06<br />0<br />10<br />20<br />30<br />40<br />50<br />60<br />70<br />80<br />p(θ|Data)<br /> <br /> <br />SGLD<br />True<br />(c) SGLD<br />00.010.020.03<br />θ<br />0.040.05 0.06<br />0<br />10<br />20<br />30<br />40<br />50<br />60<br />70<br />80<br />p(θ|Data)<br /> <br /> <br />ε = 0.5<br />True<br />(d) SGLD + MH, ? = 0.5.<br />Figure 5: Pitfalls of using uncorrected SGLD<br />An empirical histogram of samples obtained by running SGLD with α = 5×10−6is shown in Fig. 5(c). The<br />effect of omitting the MH test is quite severe here. When the sampler reaches the mode of the distribution,<br />the Langevin noise occasionally throws it into the valley to the left, where the gradient is very high. This<br />propels the sampler far off to the right, after which it takes a long time to find its way back to the mode.<br />However, if we had used an MH accept-reject test, most of these troublesome jumps into the valley would<br />be rejected because the density in the valley is much lower than that at the mode.<br />To apply an MH test, note that the SGLD proposal q(θ?|θ) can be considered a mixture of component ker-<br />nels q(θ?|θ,Xn) corresponding to different mini-batches. The mixture kernel will satisfy detailed balance with<br />respect to the posterior distribution if the MH test enforces detailed balance between the posterior and each<br />of the component kernels q(θ?|θ,Xn). Thus, we can use an MH test with µ0=<br />The result of running SGLD (keeping α = 5×10−6as before) corrected using our approximate MH test,<br />with ? = 0.5, is shown in Fig. 5(d). As expected, the MH test rejects most troublesome jumps into the valley<br />because the density in the valley is much lower than that at the mode. The stationary distribution is almost<br />indistinguishable from the true posterior. Note that when ? = 0.5, a decision is always made in the first step<br />(using just m = 500 datapoints) without querying additional data sequentially.<br />1<br />Nlog<br />?<br />uρ(θt)q(θ?|θt,Xn)<br />ρ(θ?)q(θt|θ?,Xn)<br />?<br />.<br />6.5Optimal Design of Sequential Tests<br />We illustrate the advantages of the optimal test design proposed in Section 5.2 by applying it to the ICA<br />experiment described in Section 6.2. We consider two design methods: the ‘average design’ (Eqn. 7) and<br />the ‘worst-case design’ (Eqn. 8). For the average design, we collected 100 samples of the Markov chain to<br />approximate the expectation of the error over (θ,θ?). We will call these samples the training set. The worst<br />case design does not need the training set as it does not involve the distribution of (θ,θ?). We compute the<br />optimal m and ? using grid search, for different values of the target training error, for both designs. We<br />then collect a new set of 100 samples (θ,θ?) and measure the average error and data usage on this test set<br />10</p>  <p>Page 11</p> <p>10<br />−3<br />10<br />−2<br />10<br />−1<br />10<br />−4<br />10<br />−2<br />Target Average Error<br />Test Average Error<br /> <br /> <br />Avg−Design<br />Avg−Design Fix m<br />WC−Design<br />Target<br />(a) Test Average Error<br />10<br />−3<br />10<br />−2<br />10<br />−1<br />10<br />−3<br />10<br />−2<br />10<br />−1<br />10<br />0<br />Target Average Error<br />Average Data Usage<br /> <br /> <br />Avg−Design<br />Avg−Design Fix m<br />WC−Design<br />(b) Average Data Usage<br />Figure 6: Test average error in Paand data usage Eu[¯ π] for the ICA experiment using average design over<br />both m and ? (?), with fixed m = 600 (?), and worst-case design (?).<br />(Fig. 6).<br />For the same target error on the training set, the worst-case design gives a conservative parameter setting<br />that achieves a much smaller error on the test set. In contrast, the average design achieves a test error that<br />is almost the same as the target error (Fig. 6(a)). Therefore, it uses much less data than the worst-case<br />design (Fig. 6(b)).<br />We also analyze the performance in the case where we fix m = 600 and only change ?. This is a simple<br />heuristic we recommended at the beginning of Section 5.2. Although this usually works well, using the<br />optimal test design ensures the best possible performance. In this experiment, we see that when the error is<br />large, the optimal design uses only half the data (Fig. 6(b)) used by the heuristic and is therefore twice as<br />fast.<br />7Conclusions and Future Work<br />We have taken a first step towards cutting the computational budget of the Metropolis-Hastings MCMC<br />algorithm, which takes O(N) likelihood evaluations to make the binary decision of accepting or rejecting a<br />proposed sample. In our approach, we compute the probability that a new sample will be accepted based on<br />a subset of the data. We increase the cardinality of the subset until a prescribed confidence level is reached.<br />In the process we create a bias, which is more than compensated for by a reduction in variance due to the<br />fact that we can draw more samples per unit time. Current MCMC procedures do not take these trade-offs<br />into account. In this work we use a fixed decision threshold for accepting or rejecting a sample, but in theory<br />a better algorithm can be obtained by adapting this threshold over time. An adaptive algorithm can tune<br />bias and variance contributions in such a way that at every moment our risk (the sum of squared bias and<br />variance) is as low as possible. We leave these extensions for future work.<br />Acknowledgments<br />We thank Alex Ihler, Daniel Gillen, Sungjin Ahn and Babak Shahbaba for their valuable suggestions. This<br />material is based upon work supported by the National Science Foundation under Grant No. 1216045.<br />11</p>  <p>Page 12</p> <p>ADistribution of the test statistic<br />In the sequential test, we first compute the test statistic from a mini-batch of size m. If a decision cannot be<br />made with this statistic, we keep increasing the mini-batch size by m datapoints until we reach a decision.<br />This procedure is guaranteed to terminate as explained in Section 4.<br />The parameter ? controls the probability of making an error in a single test and not the complete<br />sequential test. As the statistics across multiple tests are correlated with each other, we should first obtain<br />the joint distribution of these statistics in order to estimate the error of the complete sequential test. Let¯lj<br />and sl,jbe the sample mean and standard deviation respectively, computed using the first j mini-batches.<br />Notice that when the size of a mini-batch is large enough, e.g. n &gt; 100, the central limit theorem applies,<br />and also sl,j is an accurate estimate of the population standard deviation. Additionally, since the degrees<br />of freedom is high, the t-statistic in Eqn. 5 reduces to a z-statistic. Therefore, it is reasonable to make the<br />following assumptions:<br />Assumption 1. The joint distribution of the sequence (¯l1,¯l2,...) follows a multivariate normal distribution.<br />Assumption 2. sl= σl, where σl= std({li})<br />Fig. 7 shows that when µ = µ0the empirical marginal distribution of tj(or zj) is well fitted by both a<br />standard student-t and a standard normal distribution.<br />Under these assumptions, we state and prove the following proposition about the joint distribution of<br />the z-statistic z = (z1,z2,...), where zj<br />= (¯lj− µ0)/σl≈ tj, from different tests.<br />Proposition 2. Given Assumption 1 and 2, the sequence z follows a Gaussian random walk process:<br />def<br />P(zj|z1,...,zj−1) = N(mj(zj−1),σ2<br />z,j) (10)<br />where<br />mj(zj−1) = µstdπj− πj−1<br />1 − πj−1<br />?<br />πj<br />πj− πj−1<br />πj(1 − πj−1)<br />1<br />?πj(1 − πj)<br />1 − πj<br />1 − πj−1<br />+ zj−1<br />πj−1<br />(11)<br />σ2<br />z,j=<br />(12)<br />with µstd=<br />mini-batches.<br />(µ−µ0)√N−1<br />σl<br />being the standardized mean, and πj= jm/N the proportion of data in the first j<br />Proof of Proposition 2. Denote by xjthe average of m l’s in the j-th mini-batch. Taking into account the<br />fact that the l’s are drawn without replacement, we can compute the mean and covariance of the xj’s as:<br />E[xj] = µ(13)<br />Cov(xi,xj) =<br /><br /><br /><br /><br /><br /><br /><br />σ2<br />m<br />l<br />?<br />σ2<br />N − 1<br />1 −m − 1<br />N − 1<br />l<br />?<br />, i = j<br />−<br />, i ?= j<br />(14)<br />It is trivial to derive the expression for the mean. For the covariance, we first derive the covariance matrix<br />12</p>  <p>Page 13</p> <p>−505<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br /> <br /> <br />Empirical<br />Student−t<br />Normal<br />(a) n=500<br />−505<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br /> <br /> <br />Empirical<br />Student−t<br />Normal<br />(b) n=5000<br />−505<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br /> <br /> <br />Empirical<br />Student−t<br />Normal<br />(c) n=10000<br />Figure 7: Empirical distribution (blue bars) of the t-statistic under resampling n datapoints without replace-<br />ment from a dataset composed of digits 7 and 9 from the MNIST dataset (total N = 12214 points, mean of<br />l’s is removed). Also shown are a standard normal (green dashed) and a student-t distribution with n − 1<br />degrees of freedom (red solid).<br />of single data points as<br />Cov(lk,lk?) = Ek,k?[lklk?] − Ek[lk]Ek?[lk?]<br />if k = k?<br />= l2<br />k− µ2def<br />= σ2<br />l<br />if k ?= k?<br />= Ek?=k?[lklk?] − µ2<br />=<br />N(N − 1)(<br />N<br />N − 1µ2−<br />= −<br />N − 1<br />1<br />?<br />k,k?<br />lkl?<br />k−<br />?<br />k<br />l2<br />k) − µ2<br />=<br />l2<br />k<br />N − 1− µ2<br />σ2<br />l<br />(15)<br />Now, as xjcan be written as a linear combination of the elements in j-th mini-batch as xj=<br />expression for covariance in Eqn. 14 follows immediately from:<br />1<br />m1Tlj, the<br />Cov(xi,xj) = E[xixj] − E[xi]E[xj] =<br />1<br />m21TCov(lilT<br />j)1(16)<br />According to Assumption 1, the joint distribution of zj’s is Gaussian because zjis a linear combination<br />of¯lj’s. It is however easier to derive the mean and covariance matrix of zj’s by considering the vector z as<br />a linear function of x: z = Q(x − µ01) with<br />?????????<br />where<br />dj=<br />jσx<br />Q =<br />d1<br />d2<br />...<br />dj<br />??????????????????<br />1<br />1<br />...<br />1<br />1<br />...<br />1<br />...<br />...1<br />?????????<br />(17)<br />√N − 1<br />?<br />N−jm<br />jm<br />(18)<br />The mean and covariance can be computed as E[z] = Q1(µ − µ0) and Cov(z) = QCov(x)QTand the<br />conditional distribution P(zj|z1,...,zj−1) follows straightforwardly. We conclude the proof by plugging the<br />definition of µstdand πjinto the distribution.<br />13</p>  <p>Page 14</p> <p>Figure 8: An example of the random walk followed by z with µstd&gt; 0.<br />Fig. 8 shows the mean and 95% confidence interval of the random walk as a function of π with a few<br />realizations of the z sequence. Notice that as the proportion of observed data πjapproaches 1, the mean of<br />zjapproaches infinity with a constant variance of 1. This is consistent with the fact that when we observe<br />all the data, we will always make a correct decision.<br />It is also worth noting that given the standardized mean µstdand πj, the process is independent of the<br />actual size of a mini-batch m, population size N, or the variance of l’s σ2<br />even if we use a different size for each mini-batch. This formulation allows us to study general properties of<br />the sequential test, independent of any particular dataset.<br />Applying the individual tests δ ≷ ? ⇔ |zj| ≷ Φ(1 − ?)<br />thresholding the absolute value of zjat πjwith a bound G as shown in Fig. 9. Instead of m and ?, we will<br />use π1 = m/N and G as the parameters of the sequential test in the supplementary. The probability of<br />incorrectly deciding µ &lt; µ0when µ ≥ µ0over the whole sequential test is computed as:<br />J<br />?<br />where J = ?1/π1? is the maximum number of tests. Similarly the probability of incorrectly deciding µ ≥ µ0<br />when µ &lt; µ0can be computed similarly by replacing zj&lt; −G with zj&gt; G in Eqn. 19. We can also compute<br />the expected proportion of data that will be used in the sequential test as:<br />l. Thus, Eqns. 11 and 12 apply<br />def<br />= G at the j-th mini-batch corresponds to<br />E(µstd,π1,G) =<br />j=1<br />P(zj&lt; −G,|zi| ≤ G,∀i &lt; j)(19)<br />¯ π(µstd,π1,G) = Ez[πj?]<br />?<br />=<br />J<br />j=1<br />πjP(|zj| &gt; G,|zi| ≤ G,∀i &lt; j)(20)<br />where j?denotes the time when the sequential test terminates. Eqn. 19 and 20 can be efficiently approximated<br />together using a dynamic programming algorithm by discretizing the value of zjbetween [−G,G]. The time<br />complexity of this algorithm is O(L2J) where L is the number of discretized values.<br />The error and data usage as functions of µstdare maximum in the worst case scenario when µstd→ 0 ⇔<br />µ → µ0. In this case we have:<br />E(0,π1,G) =lim<br />µstd→0E(µstd,π1,G) = (1 − P(j?= J))/2<br />= Eworst(π1,G)<br />def<br />(21)<br />14</p>  <p>Page 15</p> <p>Figure 9: Sequential test with 3 mini-batches. Red dashed line is the bound G.<br />Figs. 1 and 10 show respectively that the theoretical value of the error (E) and the average data usage<br />(¯ π) estimated using our dynamic programming algorithm match the simulated values. Also, note that both<br />error and data usage drop off very fast as µ moves away from µ0.<br />BError in One Metropolis-Hastings Step<br />In the approximate Metropolis-Hasting test, one first draws a uniform random variable u, and then conducts<br />the sequential test. As µstdis a function of u (and µ, σl, both of which depend on θ and θ?), E measures the<br />probability that one will make a wrong decision conditioned on u. One might expect that the average error<br />in the accept/reject step of M-H using sequential test is the expected value of E w.r.t. to the distribution of<br />u. But in fact, we can usually achieve a significantly smaller error than a typical value of E. This is because<br />with a varying u, there is some probability that µ &gt; µ0(u) and also some probability that µ &lt; µ0(u). Part<br />of the error one will make given a fixed u can be canceled when we marginalize out the distribution of u.<br />Following the definition of µ0(u) for M-H in Eqn. 2, we can compute the actual error in the acceptance<br />probability as:<br />∆(µ(θ,θ?),σl(θ,θ?),π1,G) = Pa,?− Pa<br />?1<br />?1<br />?1<br />=<br />0<br />P?(µ &gt; µ0(u))du −<br />?Pa<br />?Pa<br />?Pa<br />0<br />du<br />=<br />Pa<br />P?(µ &gt; µ0(u))du −<br />0<br />(1 − P?(µ &gt; µ0(u)))du<br />=<br />Pa<br />E(µ − µ0(u))du −<br />0<br />E(µ − µ0(u))du(22)<br />Therefore, it is often observed in experiments (see Fig. 11 for example) that when Pa≈ 0.5, a typical value of<br />µstd(u) is close to 0, and the average value of the absolute error |E| can be large. But due to the cancellation<br />of errors, the actual acceptance probability Pa,?can approximate Pavery well. Fig. 12 shows the approximate<br />Pain one step of M-H. This result also suggests that making use of some (approximate) knowledge about µ<br />and σlwill help us obtain a much better estimate of the error than the worst case analysis in Eqn. 21.<br />15</p>  <p>Page 16</p> <p>−20−100102030<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />Standardized Mean, µstd<br />Average Data Usage<br /> <br /> <br />Simulation<br />Theoretical<br />Worst Case<br />Figure 10: Average data usage ¯ π estimated using simulation (blue cross) and dynamic programming (red<br />line). The worst case scenario with µstd= 0 is also shown (black dashed line).<br />0 0.2 0.4<br />Exact Pa<br />0.60.81<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />Error, |∆ Pa|<br /> <br /> <br />Average Abs Error<br />Actual Error with u marginalized<br />Upper Bound<br />Figure 11: Error ∆ in the acceptance probability (magenta circle) vs. exact acceptance probability Pa. Blue<br />crosses are the expected value of |E| w.r.t. the distribution of u. Black dashed line shows the upper bound.<br />00.20.40.60.81<br />0<br />0.2<br />0.4<br />0.6<br />0.8<br />1<br />Exact Probability, Pa<br />Approximate Probability, Pa,ε<br />Figure 12: Approximate acceptance probability vs. true acceptance probability.<br />16</p>  <p>Page 17</p> <p>CProof of Theorem 1<br />C.1Upper Bound Based on One Step Error<br />We first prove a lemma that will be used for the proof of Theorem 1.<br />Lemma 3. Given two transition kernels, T0and T?, with respective stationary distributions, S0and S?, if<br />T0satisfies the following contraction condition with a constant η ∈ [0,1) for all probability distributions P:<br />dv(PT0,S0) ≤ ηdv(P,S0)<br />and the one step error between T0and T?is upper bounded uniformly with a constant ∆ &gt; 0 as:<br />dv(PT0,PT?) ≤ ∆,∀P<br />then the distance between S0and S?is bounded as:<br />(23)<br />(24)<br />dv(S0,S?) ≤<br />∆<br />1 − η<br />(25)<br />Proof. Consider a Markov chain with transition kernel T?initialized from an arbitrary distribution P. Denote<br />the distribution after t steps by P(t)def<br />= PTt<br />P(t). According to the one step error bound in Eqn. 24, the distance between P(t+1)and the distribution<br />obtained by applying T0to P(t)is upper bounded as:<br />dv(P(t+1),P(t)T0) = dv(P(t)T?,P(t)T0) ≤ ∆<br />Following the contraction condition of T0in Eqn. 23, the distance of P(t)T0from its stationary distribution<br />S0is less than P(t)as<br />dv(P(t)T0,S0) ≤ ηdv(P(t),S0)<br />Now let us use the triangle inequality to combine Eqn. 26 and 27 to obtain an upper bounded for the distance<br />between P(t+1)and S0:<br />dv(P(t+1),S0) ≤ dv(P(t+1),P(t)T0) + dv(P(t)T0,S0)<br />≤ ∆ + ηdv(P(t),S0)<br />?. At every time step, t ≥ 0, we apply the transition kernel T?on<br />(26)<br />(27)<br />(28)<br />Let r &lt; 1−η be any positive constant and consider the ball B(S0,∆<br />outside the ball, we have ∆ ≤ rdv(P(t),S). Plugging this into Eqn. 28, we can obtain a contraction condition<br />for P(t)towards S0:<br />dv(P(t+1),S0) ≤ (r + η)dv(P(t),S0)<br />So if the initial distribution P is outside the ball, the Markov chain will move monotonically into the ball<br />within a finite number of steps. Let us denote the first time it enters the ball as tr. If the initial distribution<br />is already inside the ball, we simply let tr= 0. We then show by induction that P(t)will stay inside the ball<br />for all t ≥ tr.<br />1. At t = tr, P(t)∈ B(S0,∆<br />2. Assume P(t)∈ B(S0,∆<br />dv(P(t+1),S0) ≤ ∆ + η∆<br />=⇒ P(t+1)∈ B(S0,∆<br />r)<br />def<br />= {P : dv(P,S0) &lt;∆<br />r}. When P(t)is<br />(29)<br />r) holds by the definition of tr.<br />r) for some t ≥ tr. Then, following Eqn. 28, we have<br />r<br />=r + η<br />r<br />∆ &lt;∆<br />r<br />r)(30)<br />17</p>  <p>Page 18</p> <p>Therefore, P(t)∈ B(S0,∆<br />r) holds for all t ≥ tr. Since P(t)converges to S?, it follows that:<br />dv(S?,S0) &lt;∆<br />r,∀r &lt; 1 − η(31)<br />Taking the limit r → 1 − η, we prove the lemma:<br />dv(S?,S) ≤<br />∆<br />1 − η<br />(32)<br />C.2Proof of Theorem 1<br />We first derive an upper bound for the one step error of the approximate Metropolis-Hastings algorithm, and<br />then use Lemma 3 to prove Theorem 1. The transition kernel of the exact Metropolis-Hastings algorithm<br />can be written as<br />T0(θ,θ?) = Pa(θ,θ?)q(θ?|θ) + (1 − Pa(θ,θ?))δD(θ?− θ)<br />where δD is the Dirac delta function. For the approximate algorithm proposed in this paper, we use an<br />approximate MH test with acceptance probability˜Pa,?(θ,θ?) where the error, ∆Pa<br />bounded as |∆Pa| ≤ ∆max. Now let us look at the distance between the distributions generated by one step<br />of the exact kernel T0and the approximate kernel T?. For any P,<br />?<br />=<br />θ<br />?<br />= ∆max<br />(33)<br />def<br />= Pa,?− Pa, is upper<br />θ?dΩ(θ?)|(PT?)(θ?) − (PT0)(θ?)|<br />?<br />≤ ∆max<br />?<br />θ?dΩ(θ?)<br />????<br />?<br />dP(θ)∆Pa(θ,θ?)(q(θ?|θ) − δD(θ?− θ))<br />????<br />????<br />θ?dΩ(θ?)<br />?<br />θ<br />dP(θ)(q(θ?|θ) + δD(θ?− θ))<br />????<br />θ?dΩ(θ?)(gQ(θ?) + gP(θ?)) = 2∆max<br />(34)<br />where gQ(θ?)<br />Hastings without rejection. So we get an upper bound for the total variation distance as<br />def<br />=<br />?<br />θdP(θ)q(θ?|θ) is the density that would be obtained by applying one step of Metropolis-<br />dv(PT?,PT0) =1<br />2<br />?<br />θ?dΩ(θ?)|PT?− PT0| ≤ ∆max<br />(35)<br />Apply Lemma 3 with ∆ = ∆maxand we prove Theorem 1.<br />DOptimal Sequential Test Design<br />It is possible to design optimal tests that minimize the amount of data used while keeping the error below a<br />given tolerance. Ideally, we want to do this based on a tolerance on the error in the stationary distribution<br />S?. Unfortunately, this error depends on the contraction parameter, η, of the exact transition kernel, which<br />is difficult to compute. A more practical choice is a bound ∆maxon the error in the acceptance probability,<br />since the error in S?increases linearly with ∆max.<br />Given ∆max, we want to minimize the average data usage ¯ π over the parameters ? (or G) and/or m (or<br />π1) of the sequential test. Unfortunately, the error is a function of µ and σlwhich depend on θ and θ?, and<br />we cannot afford to change the test design at every iteration.<br />One solution is to base the design on the upper bound of the worst case error in Eqn. 21 which does<br />not rely on µstd. But we have shown in Section B that this is a rather loose bound and will lead to a very<br />conservative design that wastes the power of the sequential test. Therefore, we instead propose to design the<br />18</p>  <p>Page 19</p> <p>test by bounding the expectation of the error w.r.t. the distribution P(µ,σl). This leads to the following<br />optimization problem:<br />min<br />π1,GEµ,σlEu¯ π(µ,σl,µ0(u),π1,G)<br />s.t. Eµ,σl|∆(µ,σl,π1,G)| ≤ ∆max<br />(36)<br />The expectation w.r.t. u can be computed accurately using one dimensional quadrature. For the expectation<br />w.r.t. µ and σl, we collect a set of parameter samples (θ,θ?) during burn-in, compute the corresponding µ and<br />σlfor each sample, and use them to empirically estimate the expectation. We can also consider collecting<br />samples periodically and adapting the sequential design over time. Once we obtain a set of samples {(µ,σl)},<br />the optimization is carried out using grid search.<br />We have been using a constant bound G across all the individual tests. This is known as the Pocock design<br />[Pocock, 1977]. A more flexible sequential design can be obtained by allowing G to change as a function of π.<br />Wang and Tsiatis [1987] proposed a bound sequence Gj= G0π0.5−α<br />When α = 0, it reduces to the Pocock design, and when α = 1, it reduces to O’Brien-Fleming design [O’Brien<br />and Fleming, 1979]. We can adopt this more general form in our optimization problem straightforwardly,<br />and the grid search will now be conducted over three parameters, π1, G0, and α.<br />j<br />where α ∈ [0.5,1] is a free parameter.<br />EReversible Jump MCMC<br />We give a more detailed description of the different transition moves used in experiment 6.3. The update<br />move is the usual MCMC move which involves changing the parameter vector β without changing the model<br />γ. Specifically, we randomly pick an active component j : γj= 1 and set βj= βj+η where η ∼ N(0,σupdate).<br />The birth move involves (for k &lt; D) randomly picking an inactive component j : γj= 0 and setting γj= 1.<br />We also propose a new value for βj ∼ N(0,σbirth). The birth move is paired with a corresponding death<br />move (for k &gt; 1) which involves randomly picking an active component j : γj= 1 and setting γj= 0. The<br />corresponding βj is discarded. The probabilities of picking these moves p(γ → γ?) is the same as in Chen<br />et al. [2011]. The value of µ0used in the MH test for different moves is given below.<br />1. Update move:<br />1<br />Nlog<br />µ0=<br />?<br />u?β?−k<br />?β??−k<br />1<br />1<br />?<br />(37)<br />2. Birth move:<br />µ0=<br />1<br />Nlog<br />?<br />u?β?−k<br />1p(γ → γ?)N(βj|0,σbirth)(D − k)<br />?β??−(k+1)<br />1<br />p(γ?→ γ)λk<br />?<br />(38)<br />2. Death move:<br />µ0=<br />1<br />N×<br />log<br />?<br />u<br />?β?−k<br />?β??−(k−1)<br />1p(γ → γ?)<br />p(γ?→ γ)<br />1<br />λ(k − 1)<br />N(βj|0,σbirth)(D − k + 1)<br />?<br />(39)<br />We used σupdate= 0.01 and σbirth= 0.1 in this experiment. As mentioned in the main text, both the<br />exact reversible jump algorithm and our approximate version suffer from local minima. But, when initialized<br />with the same values, we obtain similar results with both algorithms. For example, we plot the marginal<br />posterior probability of including a feature in the model, i.e. p(γj= 1|XN,yN,λ) in figure 13.<br />FApplication to Gibbs Sampling<br />The same sequential testing method can be applied to the Gibbs sampling algorithm for discrete models. We<br />study a model with binary variables in this paper while the extension to multi-valued variables is also possible.<br />Consider running a Gibbs sampler on a probability distribution over D binary variables P(X1,...,XD). At<br />every iteration, it updates one variable Xiusing the following procedure:<br />19</p>  <p>Page 20</p> <p>010 203040 5060<br />0<br />0.01<br />0.02<br />0.03<br />0.04<br />0.05<br />0.06<br />0.07<br />0.08<br />Ground Truth<br />Feature<br />Inclusion Probability<br />010 203040 5060<br />0<br />0.01<br />0.02<br />0.03<br />0.04<br />0.05<br />0.06<br />0.07<br />0.08<br />ε = 0.01<br />Feature<br />Inclusion Probability<br />Figure 13: Marginal probability of features to be included in the model<br />1. Compute the conditional probability:<br />P(Xi= 1|x−i) =<br />P(Xi= 1,x−i)<br />P(Xi= 1,x−i) + P(Xi= 0,x−i)<br />(40)<br />where x−idenotes the value of all variables other than the ithone.<br />2. Draw u ∼ Uniform[0,1]. If u &lt; P(Xi= 1|x−i) set Xi= 1, otherwise set Xi= 0.<br />The condition in step 2 is equivalent to checking:<br />logu<br />log(1 − u)&lt;logP(Xi= 1,x−i)<br />logP(Xi= 0,x−i)<br />(41)<br />When the joint distribution is expensive to compute but can be represented as a product over multiple terms,<br />P(X) =?N<br />1<br />Nlog(1 − u)<br />1<br />N<br />n=1<br />n=1fn(X), we can apply our sequential test to speed up the Gibbs sampling algorithm. In this<br />case the variable µ0and µ is given by<br />µ0=<br />logu<br />(42)<br />µ =<br />N<br />?<br />logfn(Xi= 1,x−i)<br />fn(Xi= 0,x−i)<br />(43)<br />Similar to the Metropolis-Hastings algorithm, given an upper bound in the error of the approximate<br />conditional probability<br />∆max= max<br />i,x−i|P(Xiis assigned 1|x−i) − P(Xi= 1|x−i)|<br />we can prove the following theorem:<br />20</p>  <p>Page 21</p> <p>Theorem 4. For a Gibbs sampler with a Dobrushin coefficient η ∈ [0,1) [Br´ emaud, 1999, §7.6.2], the<br />distance between the stationary distribution and that of the approximate Gibbs sampler S?is upper bounded<br />by<br />dv(S0,S?) ≤∆max<br />1 − η<br />Proof. The proof is similar to that of Theorem 1. We first obtain an upper bound for the one step error and<br />then plug it into Lemma 3.<br />The exact transition kernel of the Gibbs sampler for variable Xican be represented by a matrix T0,iof<br />size 2D× 2D:<br />T0,i(x,y) =<br />P(Yi= yi|y−i)<br />where 1 ≤ i ≤ N,x,y ∈ {0,1}D. The approximate transition kernel T?,ican be represented similarly as<br />?<br />?<br />0if x−i?= y−i<br />otherwise<br />(44)<br />T?,i(x,y) =<br />0 if x−i?= y−i<br />otherwiseP?(Yi= yi|y−i)<br />(45)<br />where P?is the approximate conditional distribution. Define the approximation error ∆Ti(x,y)<br />T0,i(x,y). We know that ∆Ti(x,y) = 0 if y−i?= x−iand it is upper bounded by ∆maxfrom the premise of<br />Theorem 4.<br />Notice that the total variation distance reduces to a half of the L1distance for discrete distributions. For<br />any distribution P, the one step error is bounded as<br />def<br />= T?,i(x,y)−<br />dv(PT?,i,PT0,i) =1<br />=1<br />2<br />y<br />??????<br />2∆max<br />2?PT?,i− PT0,i?1<br />?????<br />P(xi,y−i)∆P(xi|y−i)<br />?<br />?<br />?????<br />?<br />x<br />P(x)∆T (x,y)<br />=1<br />2<br />y<br />?<br />?<br />xi∈{0,1}<br />??????<br />≤1<br />y<br />|P(Y−i= y−i)|<br />= ∆max<br />(46)<br />For a Gibbs sampling algorithm, we have the contraction condition [Br´ emaud, 1999, §7.6.2]:<br />dv(PT ,S) ≤ ηdv(P,S)<br />Plug ∆ = ∆maxand η into Lemma 3 and we obtain the conclusion.<br />(47)<br />F.1Experiments on Markov Random Fields<br />We illustrate the performance of our approximate Gibbs sampling algorithm on a synthetic Markov Random<br />Field. The model under consideration has D = 100 binary variables and they are densely connected by<br />potential functions of three variables ψi,j,k(Xi,Xj,Xk),∀i ?= j ?= k. There are D(D − 1)(D − 2)/6 potential<br />functions in total (we assume potential functions with permuted indices in the argument are the same<br />potential function), and every function has 23= 8 values. The entries in the potential function tables are<br />drawn randomly from a log-normal distribution, logψi,j,k(Xi,Xj,Xk) ∼ N(0,0.02). To draw a Gibbs sample<br />for one variable Xiwe have to compute (D − 1)(D − 2)/2 = 4851 pairs of potential functions as<br />P(Xi= 1|x−i)<br />P(Xi= 0|x−i)=<br />The approximate methods use a mini-batches of 500 pairs of potential functions at a time. We compare the<br />exact Gibbs sampling algorithm with approximate versions with ? ∈ {0.01,0.05,0.1,0.15,0.2,0.25}.<br />?<br />i?=j?=kψi,j,k(Xi= 1,xj,xk)<br />i?=j?=kψi,j,k(Xi= 0,xj,xk)<br />?<br />(48)<br />21</p>  <p>Page 22</p> <p>00.20.4 0.60.81<br />0<br />0.2<br />0.4<br />0.6<br />0.8<br />1<br />True Conditional Probability<br />Empirical Probability<br /> <br /> <br />True probability<br />ε = 0.01<br />ε = 0.05<br />ε = 0.1<br />ε = 0.15<br />ε = 0.2<br />ε = 0.25<br />Figure 14: Empirical conditional probability vs exact conditional probability for different values of ?. The<br />dotted black line shows the result for exact Gibbs sampling.<br />10<br />0<br />10<br />1<br />10<br />2<br />10<br />3<br />0<br />0.2<br />0.4<br />0.6<br />0.8<br />1<br />1.2<br />1.4<br />Time (s)<br />L1 error<br /> <br /> <br />ε = 0.01, T = 3429<br />ε = 0.05, T = 3979<br />ε = 0.10, T = 4494<br />ε = 0.15, T = 4889<br />ε = 0.20, T = 5507<br />ε = 0.25, T = 5959<br />Gibbs, T = 2897<br />Figure 15: Average L1error in the joint distribution over cliques of 5 variables vs running time for different<br />values of ?. The black line shows the error of Gibbs sampler with an exact acceptance probability. T in the<br />legend indicates the number of samples obtained after 1000 seconds.<br />To measure the performance in approximating P(X) with samples xt, the ideal metric would be a distance<br />between the empirical joint distribution and P. Since it is impossible to store all the 2100probabilities, we<br />instead repeatedly draw M = 1600 subsets of 5 variables, {sm}M<br />the average L1distance of the joint distribution on these subsets between the empirical distribution and P:<br />m=1,sm⊂ {1,...,D},|sm| = 5, and compute<br />Error =<br />1<br />M<br />?<br />sm<br />?ˆP(Xsm) − P(Xsm)?1<br />(49)<br />The true P is estimated by running exact Gibbs chains for a long time. We show the empirical conditional<br />probability obtained by our approximate algorithms (percentage of Xibeing assigned 1) for different ? in<br />Fig. 14. It tends to underestimate large probabilities and overestimate on the other end. When ? = 0.01,<br />the observed maximum error is within 0.01.<br />Fig. 15 shows the error for different ? as a function of the running time. For small ?, we use fewer<br />mini-batches per iteration and thus generate more samples in the same amount of time than the exact Gibbs<br />sampler. So the error decays faster in the beginning. As more samples are collected the variance is reduced.<br />We see that these plots converge towards their bias floor while the exact Gibbs sampler out-performs all the<br />approximate methods at around 1000 seconds.<br />22</p>  <p>Page 23</p> <p>References<br />S. Ahn, A. Korattikara, and M. Welling. Bayesian posterior sampling via stochastic gradient Fisher scoring.<br />In International Conference on Machine Learning, 2012.<br />S.-i. Amari, A. Cichocki, H. H. Yang, et al. A new learning algorithm for blind signal separation. Advances<br />in neural information processing systems, pages 757–763, 1996.<br />C. Andrieu and G. O. Roberts. The pseudo-marginal approach for efficient Monte Carlo computations. The<br />Annals of Statistics, 37(2):697–725, 2009.<br />K. Bache and M. Lichman. UCI machine learning repository, 2013. URL http://archive.ics.uci.edu/ml.<br />P. Br´ emaud. Markov chains: Gibbs fields, Monte Carlo simulation, and queues, volume 31. Springer, 1999.<br />X. Chen, Z. Jane Wang, and M. J. McKeown.<br />Processing, 91(8):1920–1932, 2011.<br />A Bayesian Lasso via reversible-jump MCMC.Signal<br />P. Fearnhead, O. Papaspiliopoulos, and G. O. Roberts. Particle filters for partially observed diffusions.<br />Journal of the Royal Statistical Society: Series B (Statistical Methodology), 70(4):755–777, 2008.<br />D. Gamerman and H. F. Lopes. Markov chain Monte Carlo: stochastic simulation for Bayesian inference,<br />volume 68. Chapman &amp; Hall/CRC, 2006.<br />P. J. Green. Reversible jump Markov chain Monte Carlo computation and Bayesian model determination.<br />Biometrika, 82(4):711–732, 1995.<br />W. K. Hastings. Monte Carlo sampling methods using Markov chains and their applications. Biometrika,<br />57(1):97–109, 1970.<br />A. Hyv¨ arinen and E. Oja. Independent component analysis: algorithms and applications. Neural networks,<br />13(4):411–430, 2000.<br />L. Lin, K. Liu, and J. Sloan. A noisy Monte Carlo algorithm. Physical Review D, 61(7):074505, 2000.<br />N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. Equation of state calcula-<br />tions by fast computing machines. The journal of chemical physics, 21:1087, 1953.<br />P. C. O’Brien and T. R. Fleming. A multiple testing procedure for clinical trials. Biometrics, pages 549–556,<br />1979.<br />Z. Ouyang. Bayesian Additive Regression Kernels. PhD thesis, Duke University, 2008.<br />S. J. Pocock. Group sequential methods in the design and analysis of clinical trials. Biometrika, 64(2):<br />191–199, 1977.<br />S. Singh, M. Wick, and A. McCallum. Monte Carlo MCMC: efficient inference by approximate sampling.<br />In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and<br />Computational Natural Language Learning, pages 1104–1113. Association for Computational Linguistics,<br />2012.<br />S. K. Wang and A. A. Tsiatis. Approximately optimal one-parameter boundaries for group sequential trials.<br />Biometrics, pages 193–199, 1987.<br />M. Welling and Y. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the<br />28th International Conference on Machine Learning (ICML), pages 681–688, 2011.<br />23</p>  <a href="https://www.researchgate.net/profile/Yutian_Chen3/publication/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget/links/5406e5e90cf23d9765a81dca.pdf">Download full-text</a> </div> <div id="rgw21_56ab1a186f7c6" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw22_56ab1a186f7c6">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw23_56ab1a186f7c6"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Yutian_Chen3/publication/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget/links/5406e5e90cf23d9765a81dca.pdf" class="publication-viewer" title="5406e5e90cf23d9765a81dca.pdf">5406e5e90cf23d9765a81dca.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Yutian_Chen3">Yutian Chen</a> &middot; Sep 3, 2014 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw24_56ab1a186f7c6"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://de.arxiv.org/pdf/1304.5299" target="_blank" rel="nofollow" class="publication-viewer" title="Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget">Austerity in MCMC Land: Cutting the Metropolis-Has...</a> </div>  <div class="details">   Available from <a href="http://de.arxiv.org/pdf/1304.5299" target="_blank" rel="nofollow">de.arxiv.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw31_56ab1a186f7c6" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (35) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw32_56ab1a186f7c6" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw33_56ab1a186f7c6" >  <div class="indent-left">  <div id="rgw34_56ab1a186f7c6" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/David_Dunson" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: David B Dunson </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw35_56ab1a186f7c6">  <li class="citation-context-item"> "Relying on stochastic approximation, most of these approaches use stochastic gradient descent for optimization (Welling and Teh, 2011; Broderick et al., 2013; Hoffman et al., 2013). Optimization updates are also coupled with sampling using modified Hamiltonian or Langevin Dynamics to improve posterior exploration (Ahn et al., 2012; Korattikara et al., 2014). Recent work in this area has focused on using different factorizations and parametrizations to improve the efficiency of existing methods (Wang and Blei, 2013; Tan and Nott, 2013; Wand, 2014). " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space"> <span class="publication-title js-publication-title">Scalable Bayes via Barycenter in Wasserstein Space</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2045181100_Sanvesh_Srivastava" class="authors js-author-name ga-publications-authors">Sanvesh Srivastava</a> &middot;     <a href="researcher/2084332535_Cheng_Li" class="authors js-author-name ga-publications-authors">Cheng Li</a> &middot;     <a href="researcher/9530149_David_B_Dunson" class="authors js-author-name ga-publications-authors">David B. Dunson</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> We propose a novel approach WASP for Bayesian inference when massive size of
the data prohibits posterior computations. WASP is estimated in three steps.
First, data are divided into smaller computationally tractable subsets. Second,
posterior draws of parameters are obtained for every subset after modifying
subset posteriors using stochastic approximation. Finally, the empirical
measures of samples from each subset posterior are combined through their
barycenter in the Wasserstein space of probability measures. Stochastic
approximation ensures that posterior uncertainty quantification of the
barycenter matches with that of the full data posterior distribution. The
combining step can be conducted efficiently through a sparse linear program,
which takes negligible time relative to sampling from subset posteriors,
facilitating scaling to massive data. WASP is very general and allows
application of existing sampling algorithms to massive data with minimal
modifications. We provide theoretical conditions under which rate of
convergence of WASP to the delta measure centered at the true parameter
coincides with the optimal parametric rate up to a logarithmic factor. WASP is
applied for scalable Bayesian computations in a nonparametric mixture model and
a movie recommender database containing tens of millions of ratings. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Aug 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/David_Dunson/publication/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space/links/55ffe48708ae07629e51e264.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw36_56ab1a186f7c6" >  <div class="indent-left">  <div id="rgw37_56ab1a186f7c6" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/281058916_Approximations_of_Markov_Chains_and_High-Dimensional_Bayesian_Inference">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/David_Dunson" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: David B Dunson </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw38_56ab1a186f7c6">  <li class="citation-context-item"> "An example of this class is provided in Korattikara et al. (2013), where V Ă t1, . . . , N u is a random subset of indices adaptively chosen to obtain a pre-specified type I error in a Metropolis-Hastings acceptance decision. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/281058916_Approximations_of_Markov_Chains_and_High-Dimensional_Bayesian_Inference"> <span class="publication-title js-publication-title">Approximations of Markov Chains and High-Dimensional Bayesian Inference</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2079652947_James_E_Johndrow" class="authors js-author-name ga-publications-authors">James E. Johndrow</a> &middot;     <a href="researcher/2079648813_Jonathan_C_Mattingly" class="authors js-author-name ga-publications-authors">Jonathan C. Mattingly</a> &middot;     <a href="researcher/2079631589_Sayan_Mukherjee" class="authors js-author-name ga-publications-authors">Sayan Mukherjee</a> &middot;     <a href="researcher/9530149_David_Dunson" class="authors js-author-name ga-publications-authors">David Dunson</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> The Markov Chain Monte Carlo method is the dominant paradigm for posterior
computation in Bayesian analysis. It has long been common to control
computation time by making approximations to the Markov transition kernel.
Comparatively little attention has been paid to convergence and estimation
error in these approximating Markov Chains. We propose a framework for
assessing when to use approximations in MCMC algorithms, and how much error in
the transition kernel should be tolerated to obtain optimal estimation
performance with respect to a specified loss function and computational budget.
The results require only ergodicity of the exact kernel and control of the
kernel approximation accuracy. The theoretical framework is applied to
approximations based on random subsets of data, low-rank approximations of
Gaussian processes, and a novel approximating Markov chain for discrete mixture
models. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Aug 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/David_Dunson/publication/281058916_Approximations_of_Markov_Chains_and_High-Dimensional_Bayesian_Inference/links/55ffe48708aec948c4f9c17e.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw39_56ab1a186f7c6" >  <div class="indent-left">  <div id="rgw40_56ab1a186f7c6" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/280589611_How_Can_Subsampling_Reduce_Complexity_in_Sequential_MCMC_Methods_and_Deal_with_Big_Data_in_Target_Tracking">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Lyudmila_Mihaylova" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Lyudmila Mihaylova </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw41_56ab1a186f7c6">  <li class="citation-context-item"> "The largest hindrance being long processing times which could limit usage in applications required to run in real time. There have also been several algorithms [9], [10], [11] which have been proposed to help reduce computational complexities when performing static inference with MCMC techniques on large datasets. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/280589611_How_Can_Subsampling_Reduce_Complexity_in_Sequential_MCMC_Methods_and_Deal_with_Big_Data_in_Target_Tracking"> <span class="publication-title js-publication-title">How Can Subsampling Reduce Complexity in Sequential MCMC Methods and Deal with Big Data in Target Tracking?</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2078862656_Allan_De_Freitas" class="authors js-author-name ga-publications-authors">Allan De Freitas</a> &middot;     <a href="researcher/54570685_Francois_Septier" class="authors js-author-name ga-publications-authors">François Septier</a> &middot;     <a href="researcher/8858822_Lyudmila_Mihaylova" class="authors js-author-name ga-publications-authors">Lyudmila Mihaylova</a> &middot;     <a href="researcher/5949805_Simon_J_Godsill" class="authors js-author-name ga-publications-authors">Simon J. Godsill</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Target tracking faces the challenge in coping with large volumes of data
which requires efficient methods for real time applications. The complexity
considered in this paper is when there is a large number of measurements which
are required to be processed at each time step. Sequential Markov chain Monte
Carlo (MCMC) has been shown to be a promising approach to target tracking in
complex environments, especially when dealing with clutter. However, a large
number of measurements usually results in large processing requirements. This
paper goes beyond the current state-of-the-art and presents a novel Sequential
MCMC approach that can overcome this challenge through adaptively subsampling
the set of measurements. Instead of using the whole large volume of available
data, the proposed algorithm performs a trade off between the number of
measurements to be used and the desired accuracy of the estimates to be
obtained in the presence of clutter. We show results with large improvements in
processing time, more than 40% with a negligible loss in tracking performance,
compared with the solution without subsampling. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Jul 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Lyudmila_Mihaylova/publication/280589611_How_Can_Subsampling_Reduce_Complexity_in_Sequential_MCMC_Methods_and_Deal_with_Big_Data_in_Target_Tracking/links/55bf422b08ae092e96652cf3.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw26_56ab1a186f7c6" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw27_56ab1a186f7c6">  </ul> </div> </div>   <div id="rgw17_56ab1a186f7c6" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw18_56ab1a186f7c6"> <div> <h5> <a href="publication/237433287_Sf2955_COMPUTER_INTENSIVE_METHODS_MCMC_On_the_Discrete_Metropolis-Hastings_Algorithm_2009" class="color-inherit ga-similar-publication-title"><span class="publication-title">Sf2955 COMPUTER INTENSIVE METHODS MCMC On the Discrete Metropolis-Hastings Algorithm 2009</span></a>  </h5>  <div class="authors"> <a href="researcher/4904784_Timo_Koski" class="authors ga-similar-publication-author">Timo Koski</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56ab1a186f7c6"> <div> <h5> <a href="publication/263621886_Uncertainty_Analysis_for_Parameters_of_Probability_Distribution_in_Rainfall_Frequency_Analysis_by_Bayesian_MCMC_and_Metropolis_Hastings_Algorithm" class="color-inherit ga-similar-publication-title"><span class="publication-title">Uncertainty Analysis for Parameters of Probability Distribution in Rainfall Frequency Analysis by Bayesian MCMC and Metropolis Hastings Algorithm</span></a>  </h5>  <div class="authors"> <a href="researcher/2050635997_Young-Min_Seo" class="authors ga-similar-publication-author">Young-Min Seo</a>, <a href="researcher/2050631352_Ki-Bum_Park" class="authors ga-similar-publication-author">Ki-Bum Park</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw20_56ab1a186f7c6"> <div> <h5> <a href="publication/51959512_Note_on_the_computation_of_the_Metropolis-Hastings_ratio_forBirth-or-Death_moves_in_trans-dimensional_MCMC_algorithms_for_signaldecomposition_problems" class="color-inherit ga-similar-publication-title"><span class="publication-title">Note on the computation of the Metropolis-Hastings ratio for
Birth-or-Death moves in trans-dimensional MCMC algorithms for signal
decomposition problems</span></a>  </h5>  <div class="authors"> <a href="researcher/42548324_Alireza_Roodaki" class="authors ga-similar-publication-author">Alireza Roodaki</a>, <a href="researcher/14088780_Julien_Bect" class="authors ga-similar-publication-author">Julien Bect</a>, <a href="researcher/8948246_Gilles_Fleury" class="authors ga-similar-publication-author">Gilles Fleury</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw51_56ab1a186f7c6" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw52_56ab1a186f7c6">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw53_56ab1a186f7c6" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=tgH5VL5IexZZplCXRkRcEj__uBSwlBgHza1WiytYNJV97N3e_LTRhyMau42n3wEP" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="Ow7kFdjbESs4mAgTq+asYUKw16jt1MXBYAw6MwhOLCbJKqTDYWGu81MEw8twhkMIzNvk6aBrSMk09mnjuQNEBHthCzdIZ6cRfdJ1z5eZ4ZlwYS8LWrOfv53SZmvwz+EaDxZGTVzAED3KmqRisFMeqjrPZEyO7FLzM5DYADPZR+u7PtM5k0AUVer7yfBMPjm3k/7TWOEe/cOMe1/XBdAkzYb7yltpfmYdzPWE6SrS3Vd5Bs4gtuhzhX3GaDWzrrvIsnrIZxXoD32YiP6UHx2gxgd/L6Fwxqf5KpvxZmCglQ0="/> <input type="hidden" name="urlAfterLogin" value="publication/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjM2MjM1MTkwX0F1c3Rlcml0eV9pbl9NQ01DX0xhbmRfQ3V0dGluZ190aGVfTWV0cm9wb2xpcy1IYXN0aW5nc19CdWRnZXQ%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjM2MjM1MTkwX0F1c3Rlcml0eV9pbl9NQ01DX0xhbmRfQ3V0dGluZ190aGVfTWV0cm9wb2xpcy1IYXN0aW5nc19CdWRnZXQ%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjM2MjM1MTkwX0F1c3Rlcml0eV9pbl9NQ01DX0xhbmRfQ3V0dGluZ190aGVfTWV0cm9wb2xpcy1IYXN0aW5nc19CdWRnZXQ%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw54_56ab1a186f7c6"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 809;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FigureList","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Yutian Chen","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278543817822209%401443421429383_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Yutian_Chen3","institution":"University of Cambridge","institutionUrl":false,"widgetId":"rgw4_56ab1a186f7c6"},"id":"rgw4_56ab1a186f7c6","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=4885109","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab1a186f7c6"},"id":"rgw3_56ab1a186f7c6","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=236235190","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":236235190,"title":"Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"31st International Conference on Machine Learning, ICML 2014","publicationDate":"04\/2013;","publicationDateRobot":"2013-04","article":"1."}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1304.5299","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget"},{"key":"rft.title","value":"31st International Conference on Machine Learning, ICML 2014"},{"key":"rft.jtitle","value":"31st International Conference on Machine Learning, ICML 2014"},{"key":"rft.volume","value":"1"},{"key":"rft.date","value":"2013"},{"key":"rft.au","value":"Anoop Korattikara,Yutian Chen,Max Welling"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56ab1a186f7c6"},"id":"rgw6_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=236235190","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":236235190,"peopleItems":[{"data":{"authorUrl":"researcher\/80788532_Anoop_Korattikara","authorNameOnPublication":"Anoop Korattikara","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Anoop Korattikara","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/80788532_Anoop_Korattikara","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw9_56ab1a186f7c6"},"id":"rgw9_56ab1a186f7c6","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=80788532&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab1a186f7c6"},"id":"rgw8_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=80788532&authorNameOnPublication=Anoop%20Korattikara","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Yutian Chen","accountUrl":"profile\/Yutian_Chen3","accountKey":"Yutian_Chen3","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278543817822209%401443421429383_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Yutian Chen","profile":{"professionalInstitution":{"professionalInstitutionName":"University of Cambridge","professionalInstitutionUrl":"institution\/University_of_Cambridge"}},"professionalInstitutionName":"University of Cambridge","professionalInstitutionUrl":"institution\/University_of_Cambridge","url":"profile\/Yutian_Chen3","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278543817822209%401443421429383_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Yutian_Chen3","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw11_56ab1a186f7c6"},"id":"rgw11_56ab1a186f7c6","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=4885109&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University of Cambridge","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":1,"publicationUid":236235190,"widgetId":"rgw10_56ab1a186f7c6"},"id":"rgw10_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=4885109&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=1&publicationUid=236235190","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/69847505_Max_Welling","authorNameOnPublication":"Max Welling","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Max Welling","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/69847505_Max_Welling","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw13_56ab1a186f7c6"},"id":"rgw13_56ab1a186f7c6","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=69847505&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw12_56ab1a186f7c6"},"id":"rgw12_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=69847505&authorNameOnPublication=Max%20Welling","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab1a186f7c6"},"id":"rgw7_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=236235190&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":236235190,"abstract":"<noscript><\/noscript><div>Can we make Bayesian posterior MCMC sampling more efficient when faced with<br \/>\nvery large datasets? We argue that computing the likelihood for N datapoints<br \/>\ntwice in order to reach a single binary decision is computationally<br \/>\ninefficient. We introduce an approximate Metropolis-Hastings rule based on a<br \/>\nsequential hypothesis test which allows us to accept or reject samples with<br \/>\nhigh confidence using only a fraction of the data required for the exact MH<br \/>\nrule. While this introduces an asymptotic bias, we show that this bias can be<br \/>\ncontrolled and is more than offset by a decrease in variance due to our ability<br \/>\nto draw more samples per unit of time. We show that the same idea can also be<br \/>\napplied to Gibbs sampling in densely connected graphs.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw14_56ab1a186f7c6"},"id":"rgw14_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=236235190","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":{"data":{"figures":[{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Yutian_Chen3\/publication\/236235190\/figure\/fig1\/Figure-8-An-example-of-the-random-walk-followed-by-z-with-m-std-0.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Yutian_Chen3\/publication\/236235190\/figure\/fig1\/Figure-8-An-example-of-the-random-walk-followed-by-z-with-m-std-0_small.png","figureUrl":"\/figure\/236235190_fig1_Figure-8-An-example-of-the-random-walk-followed-by-z-with-m-std-0","selected":false,"title":"Figure 8: An example of the random walk followed by z with \u03bc std > 0.\u00a0","key":"236235190_fig1_Figure-8-An-example-of-the-random-walk-followed-by-z-with-m-std-0"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Yutian_Chen3\/publication\/236235190\/figure\/fig2\/Figure-9-Sequential-test-with-3-mini-batches-Red-dashed-line-is-the-bound-G.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Yutian_Chen3\/publication\/236235190\/figure\/fig2\/Figure-9-Sequential-test-with-3-mini-batches-Red-dashed-line-is-the-bound-G_small.png","figureUrl":"\/figure\/236235190_fig2_Figure-9-Sequential-test-with-3-mini-batches-Red-dashed-line-is-the-bound-G","selected":false,"title":"Figure 9: Sequential test with 3 mini-batches. Red dashed line is the...","key":"236235190_fig2_Figure-9-Sequential-test-with-3-mini-batches-Red-dashed-line-is-the-bound-G"}],"readerDocId":"5995207","linkBehaviour":"dialog","isDialog":true,"headerText":"Figures in this publication","isNewPublicationDesign":false,"widgetId":"rgw15_56ab1a186f7c6"},"id":"rgw15_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/FigureList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FigureList.html?readerDocId=5995207&isDialog=1&linkBehaviour=dialog","viewClass":"views.publicliterature.FigureListView","yuiModules":["rg.views.publicliterature.FigureListView","css-pow-publicliterature-FigureList"],"stylesheets":["pow\/publicliterature\/FigureList.css"],"_isYUI":true},"previewImage":"https:\/\/i1.rgstatic.net\/publication\/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget\/links\/5406e5e90cf23d9765a81dca\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw16_56ab1a186f7c6"},"id":"rgw16_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab1a186f7c6"},"id":"rgw5_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=236235190&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":4904784,"url":"researcher\/4904784_Timo_Koski","fullname":"Timo Koski","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/237433287_Sf2955_COMPUTER_INTENSIVE_METHODS_MCMC_On_the_Discrete_Metropolis-Hastings_Algorithm_2009","usePlainButton":true,"publicationUid":237433287,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/237433287_Sf2955_COMPUTER_INTENSIVE_METHODS_MCMC_On_the_Discrete_Metropolis-Hastings_Algorithm_2009","title":"Sf2955 COMPUTER INTENSIVE METHODS MCMC On the Discrete Metropolis-Hastings Algorithm 2009","displayTitleAsLink":true,"authors":[{"id":4904784,"url":"researcher\/4904784_Timo_Koski","fullname":"Timo Koski","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/237433287_Sf2955_COMPUTER_INTENSIVE_METHODS_MCMC_On_the_Discrete_Metropolis-Hastings_Algorithm_2009","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/237433287_Sf2955_COMPUTER_INTENSIVE_METHODS_MCMC_On_the_Discrete_Metropolis-Hastings_Algorithm_2009\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab1a186f7c6"},"id":"rgw18_56ab1a186f7c6","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=237433287","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2050635997,"url":"researcher\/2050635997_Young-Min_Seo","fullname":"Young-Min Seo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2050631352,"url":"researcher\/2050631352_Ki-Bum_Park","fullname":"Ki-Bum Park","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Mar 2011","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/263621886_Uncertainty_Analysis_for_Parameters_of_Probability_Distribution_in_Rainfall_Frequency_Analysis_by_Bayesian_MCMC_and_Metropolis_Hastings_Algorithm","usePlainButton":true,"publicationUid":263621886,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/263621886_Uncertainty_Analysis_for_Parameters_of_Probability_Distribution_in_Rainfall_Frequency_Analysis_by_Bayesian_MCMC_and_Metropolis_Hastings_Algorithm","title":"Uncertainty Analysis for Parameters of Probability Distribution in Rainfall Frequency Analysis by Bayesian MCMC and Metropolis Hastings Algorithm","displayTitleAsLink":true,"authors":[{"id":2050635997,"url":"researcher\/2050635997_Young-Min_Seo","fullname":"Young-Min Seo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2050631352,"url":"researcher\/2050631352_Ki-Bum_Park","fullname":"Ki-Bum Park","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["03\/2011; 20(3). DOI:10.5322\/JES.2011.20.3.329"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/263621886_Uncertainty_Analysis_for_Parameters_of_Probability_Distribution_in_Rainfall_Frequency_Analysis_by_Bayesian_MCMC_and_Metropolis_Hastings_Algorithm","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/263621886_Uncertainty_Analysis_for_Parameters_of_Probability_Distribution_in_Rainfall_Frequency_Analysis_by_Bayesian_MCMC_and_Metropolis_Hastings_Algorithm\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab1a186f7c6"},"id":"rgw19_56ab1a186f7c6","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=263621886","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":42548324,"url":"researcher\/42548324_Alireza_Roodaki","fullname":"Alireza Roodaki","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":14088780,"url":"researcher\/14088780_Julien_Bect","fullname":"Julien Bect","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8948246,"url":"researcher\/8948246_Gilles_Fleury","fullname":"Gilles Fleury","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Nov 2011","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/51959512_Note_on_the_computation_of_the_Metropolis-Hastings_ratio_forBirth-or-Death_moves_in_trans-dimensional_MCMC_algorithms_for_signaldecomposition_problems","usePlainButton":true,"publicationUid":51959512,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/51959512_Note_on_the_computation_of_the_Metropolis-Hastings_ratio_forBirth-or-Death_moves_in_trans-dimensional_MCMC_algorithms_for_signaldecomposition_problems","title":"Note on the computation of the Metropolis-Hastings ratio for\nBirth-or-Death moves in trans-dimensional MCMC algorithms for signal\ndecomposition problems","displayTitleAsLink":true,"authors":[{"id":42548324,"url":"researcher\/42548324_Alireza_Roodaki","fullname":"Alireza Roodaki","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":14088780,"url":"researcher\/14088780_Julien_Bect","fullname":"Julien Bect","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8948246,"url":"researcher\/8948246_Gilles_Fleury","fullname":"Gilles Fleury","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/51959512_Note_on_the_computation_of_the_Metropolis-Hastings_ratio_forBirth-or-Death_moves_in_trans-dimensional_MCMC_algorithms_for_signaldecomposition_problems","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/51959512_Note_on_the_computation_of_the_Metropolis-Hastings_ratio_forBirth-or-Death_moves_in_trans-dimensional_MCMC_algorithms_for_signaldecomposition_problems\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56ab1a186f7c6"},"id":"rgw20_56ab1a186f7c6","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=51959512","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw17_56ab1a186f7c6"},"id":"rgw17_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=236235190&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":236235190,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":236235190,"publicationType":"article","linkId":"5406e5e90cf23d9765a81dca","fileName":"5406e5e90cf23d9765a81dca.pdf","fileUrl":"profile\/Yutian_Chen3\/publication\/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget\/links\/5406e5e90cf23d9765a81dca.pdf","name":"Yutian Chen","nameUrl":"profile\/Yutian_Chen3","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Sep 3, 2014","fileSize":"1.08 MB","widgetId":"rgw23_56ab1a186f7c6"},"id":"rgw23_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=236235190&linkId=5406e5e90cf23d9765a81dca&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":236235190,"publicationType":"article","linkId":"03314bd90cf2220c9457387c","fileName":"Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget","fileUrl":"http:\/\/de.arxiv.org\/pdf\/1304.5299","name":"de.arxiv.org","nameUrl":"http:\/\/de.arxiv.org\/pdf\/1304.5299","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw24_56ab1a186f7c6"},"id":"rgw24_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=236235190&linkId=03314bd90cf2220c9457387c&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw22_56ab1a186f7c6"},"id":"rgw22_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=236235190&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":20,"valueFormatted":"20","widgetId":"rgw25_56ab1a186f7c6"},"id":"rgw25_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=236235190","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw21_56ab1a186f7c6"},"id":"rgw21_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=236235190&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":236235190,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw27_56ab1a186f7c6"},"id":"rgw27_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=236235190&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":20,"valueFormatted":"20","widgetId":"rgw28_56ab1a186f7c6"},"id":"rgw28_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=236235190","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw26_56ab1a186f7c6"},"id":"rgw26_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=236235190&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget\nAnoop Korattikara\u22171, Yutian Chen\u20201,2and Max Welling\u20211,3\n1Department of Computer Science, University of California, Irvine\n2Department of Engineering, University of Cambridge\n3Informatics Institute, University of Amsterdam\nAbstract\nCan we make Bayesian posterior MCMC sampling more efficient when faced with very large datasets?\nWe argue that computing the likelihood for N datapoints in the Metropolis-Hastings (MH) test to reach\na single binary decision is computationally inefficient. We introduce an approximate MH rule based on\na sequential hypothesis test that allows us to accept or reject samples with high confidence using only a\nfraction of the data required for the exact MH rule. While this method introduces an asymptotic bias,\nwe show that this bias can be controlled and is more than offset by a decrease in variance due to our\nability to draw more samples per unit of time.\n1Introduction\nMarkov chain Monte Carlo (MCMC) sampling has been the main workhorse of Bayesian computation since\nthe 1990s. A canonical MCMC algorithm proposes samples from a distribution q and then accepts or rejects\nthese proposals with a certain probability given by the Metropolis-Hastings (MH) formula [Metropolis et al.,\n1953, Hastings, 1970]. For each proposed sample, the MH rule needs to examine the likelihood of all data-\nitems. When the number of data-cases is large this is an awful lot of computation for one bit of information,\nnamely whether to accept or reject a proposal.\nIn today\u2019s Big Data world, we need to rethink our Bayesian inference algorithms. Standard MCMC\nmethods do not meet the Big Data challenge for the reason described above. Researchers have made some\nprogress in terms of making MCMC more efficient, mostly by focusing on parallelization. Very few question\nthe algorithm itself: is the standard MCMC paradigm really optimally efficient in achieving its goals? We\nclaim it is not.\nAny method that includes computation as an essential ingredient should acknowledge that there is a finite\namount of time, T, to finish a calculation. An efficient MCMC algorithm should therefore decrease the \u201cerror\u201d\n(properly defined) maximally in the given time T. For MCMC algorithms, there are two contributions to this\nerror: bias and variance. Bias occurs because the chain needs to burn in during which it is sampling from\nthe wrong distribution. Bias usually decreases fast, as evidenced by the fact that practitioners are willing\nto wait until the bias has (almost) completely vanished after which they discard these \u201cburn-in samples\u201d.\nThe second cause of error is sampling variance, which occurs because of the random nature of the sampling\nprocess. The retained samples after burn-in will reduce the variance as O(1\/T).\nHowever, given a finite amount of computational time, it is not at all clear whether the strategy of\nretaining few unbiased samples and accepting an error dominated by variance is optimal. Perhaps, by\ndecreasing the bias more slowly we could sample faster and thus reduce variance faster? In this paper we\nillustrate this effect by cutting the computational budget of the MH accept\/reject step. To achieve that,\nwe conduct sequential hypothesis tests to decide whether to accept or reject a given sample and find that\nthe majority of these decisions can be made based on a small fraction of the data with high confidence. A\n\u2217akoratti@ics.uci.edu\n\u2020yutian.chen@eng.cam.edu\n\u2021welling@ics.uci.edu\n1\narXiv:1304.5299v4  [cs.LG]  14 Feb 2014"},{"page":2,"text":"related method was used in Singh et al. [2012], where the factors of a graphical model are sub-sampled to\ncompute fixed-width confidence intervals for the log-likelihood in the MH test.\nOur \u201cphilosophy\u201d runs deeper than the algorithm proposed here. We advocate MCMC algorithms with\na \u201cbias-knob\u201d, allowing one to dial down the bias at a rate that optimally balances error due to bias and\nvariance. We only know of one algorithm that would also adhere to this strategy: stochastic gradient\nLangevin dynamics [Welling and Teh, 2011] and its successor stochastic gradient Fisher scoring [Ahn et al.,\n2012]. In their case the bias-knob was the stepsize. These algorithms do not have an MH step which resulted\nin occasional samples with extremely low probability. We show that our approximate MH step largely\nresolves this, still avoiding O(N) computations per iteration.\nIn the next section we introduce the MH algorithm and discuss its drawbacks. Then in Section 3, we\nintroduce the idea of approximate MCMC methods and the bias variance trade-off involved. We develop\napproximate MH tests for Bayesian posterior sampling in Section 4 and present a theoretical analysis in\nSection 5. Finally, we show our experimental results in Section 6 and conclude in Section 7.\n2 The Metropolis-Hastings algorithm\nMCMC methods generate samples from a distribution S0(\u03b8) by simulating a Markov chain designed to have\nstationary distribution S0(\u03b8). A Markov chain with a given stationary distribution can be constructed using\nthe Metropolis-Hastings algorithm [Metropolis et al., 1953, Hastings, 1970], which uses the following rule for\ntransitioning from the current state \u03b8tto the next state \u03b8t+1:\n1. Draw a candidate state \u03b8?from a proposal distribution q(\u03b8?|\u03b8t)\n2. Compute the acceptance probability:\nPa= min\n?\n1,S0(\u03b8?)q(\u03b8t|\u03b8?)\nS0(\u03b8t)q(\u03b8?|\u03b8t)\n?\n(1)\n3. Draw u \u223c Uniform[0,1]. If u < Paset \u03b8t+1\u2190 \u03b8?, otherwise set \u03b8t+1\u2190 \u03b8t.\nFollowing this transition rule ensures that the stationary distribution of the Markov chain is S0(\u03b8). The\nsamples from the Markov chain are usually used to estimate the expectation of a function f(\u03b8) with respect\nto S0(\u03b8). To do this we collect T samples and approximate the expectation I = ?f?S0as\u02c6I =\nSince the stationary distribution of the Markov chain is S0,\u02c6I is an unbiased estimator of I (if we ignore\nburn-in).\nThe variance of\u02c6I is V = E[(?f?S0\u22121\nof the Markov chain. It is well known that V \u2248 \u03c32\nand \u03c4 is the integrated auto-correlation time, which is a measure of the interval between independent samples\n[Gamerman and Lopes, 2006]. Usually, it is quite difficult to design a chain that mixes fast and therefore, the\nauto-correlation time will be quite high. Also, for many important problems, evaluating S0(\u03b8) to compute\nthe acceptance probability Pain every step is so expensive that we can collect only a very small number of\nsamples (T) in a realistic amount of computational time. Thus the variance of\u02c6I can be prohibitively high,\neven though it is unbiased.\n1\nT\n?T\nt=1f(\u03b8t).\nT\n?T\nt=1f(\u03b8t))2], where the expectation is over multiple simulations\nf,S0\u03c4\/T, where \u03c32\nf,S0is the variance of f with respect to S0\n3 Approximate MCMC and the Bias-Variance Tradeoff\nIronically, the reason MCMC methods are so slow is that they are designed to be unbiased. If we were to\nallow a small bias in the stationary distribution, it is possible to design a Markov chain that can be simulated\ncheaply [Welling and Teh, 2011, Ahn et al., 2012]. That is, to estimate I = ?f?S0, we can use a Markov chain\nwith stationary distribution S?where ? is a parameter that can be used to control the bias in the algorithm.\nThen I can be estimated as\u02c6I =1\nT\nAs ? \u2192 0, S?approaches S0(the distribution of interest) but it becomes expensive to simulate the Markov\nchain. Therefore, the bias in\u02c6I is low, but the variance is high because we can collect only a small number\nof samples in a given amount of computational time. As ? moves away from 0, it becomes cheap to simulate\n?T\nt=1f(\u03b8t), computed using samples from S?instead of S0.\n2"},{"page":3,"text":"the Markov chain but the difference between S?and S0grows. Therefore,\u02c6I will have higher bias, but lower\nvariance because we can collect a larger number of samples in the same amount of computational time. This\nis a classical bias-variance trade-off and can be studied using the risk of the estimator.\nThe risk can be defined as the mean squared error in\u02c6I, i.e. R = E[(I \u2212\u02c6I)2], where the expectation is\ntaken over multiple simulations of the Markov chain. It is easy to show that the risk can be decomposed\nas R = B2+ V , where B is the bias and V is the variance. If we ignore burn-in, it can be shown that\nB = ?f?S?\u2212 ?f?S0and V = E[(?f?S?\u22121\nThe optimal setting of ? that minimizes the risk depends on the amount of computational time available.\nIf we have an infinite amount of computational time, we should set ? to 0. Then there is no bias, and the\nvariance can be brought down to 0 by drawing an infinite number of samples. This is the traditional MCMC\nsetting. However, given a finite amount of computational time, this setting may not be optimal. It might be\nbetter to tolerate a small amount of bias in the stationary distribution if it allows us to reduce the variance\nquickly, either by making it cheaper to collect a large number of samples or by mixing faster.\nIt is interesting to note that two recently proposed algorithms follow this paradigm: Stochastic Gradient\nLangevin Dynamics (SGLD) [Welling and Teh, 2011] and Stochastic Gradient Fisher Scoring (SGFS) [Ahn\net al., 2012]. These algorithms are biased because they omit the required Metropolis-Hastings tests. However,\nin both cases, a knob ? (the step-size of the proposal distribution) is available to control the bias. As ? \u2192 0,\nthe acceptance probability Pa\u2192 1 and the bias from not conducting MH tests disappears. However, when\n? \u2192 0 the chain mixes very slowly and the variance increases because the auto-correlation time \u03c4 \u2192 \u221e. As\n? is increased from 0, the auto-correlation, and therefore the variance, reduces. But, at the same time, the\nacceptance probability reduces and the bias from not conducting MH tests increases as well.\nIn the next section, we will develop another class of approximate MCMC algorithms for the case where\nthe target S0is a Bayesian posterior distribution given a very large dataset. We achieve this by developing\nan approximate Metropolis-Hastings test, equipped with a knob for controlling the bias. Moreover, our\nalgorithm has the advantage that it can be used with any proposal distribution. For example, our method\nallows approximate MCMC methods to be applied to problems where it is impossible to compute gradients\n(which is necessary to apply SGLD\/SGFS). Or, we can even combine our method with SGLD\/SGFS, to\nobtain the best of both worlds.\nTf(\u03b8t))2] \u2248 \u03c32\nf,S?\u03c4\/T.\n4 Approximate Metropolis-Hastings Test for Bayesian Posterior\nSampling\nAn important method in the toolbox of Bayesian inference is posterior sampling. Given a dataset of N\nindependent observations XN = {x1,...,xN}, which we model using a distribution p(x;\u03b8) parameterized\nby \u03b8, defined on a space \u0398 with measure \u03a9, and a prior distribution \u03c1(\u03b8), the task is to sample from the\nposterior distribution S0(\u03b8) \u221d \u03c1(\u03b8)?N\nhas to be done for each posterior sample we generate. Spending O(N) computation to get just 1 bit of\ninformation, i.e. whether to accept or reject a sample, is likely not the best use of computational resources.\nBut, if we try to develop accept\/reject tests that satisfy detailed balance exactly with respect to the\nposterior distribution using only sub-samples of data, we will quickly see the no free lunch theorem kicking in.\nFor example, the pseudo marginal MCMC method [Andrieu and Roberts, 2009] and the method developed\nby Lin et al. [2000] provide a way to conduct exact accept\/reject tests using unbiased estimators of the\nlikelihood. However, unbiased estimators of the likelihood that can be computed from mini-batches of data,\nsuch as the Poisson estimator [Fearnhead et al., 2008] or the Kennedy-Bhanot estimator [Lin et al., 2000]\nhave very high variance for large datasets. Because of this, once we get a very high estimate of the likelihood,\nalmost all proposed moves are rejected and the algorithm gets stuck.\nThus, we should be willing to tolerate some error in the stationary distribution if we want faster ac-\ncept\/reject tests. If we can offset this small bias by drawing a large number of samples cheaply and reducing\nthe variance faster, we can establish a potentially large reduction in the risk.\nWe will now show how to develop such approximate tests by reformulating the MH test as a statistical\ndecision problem. It is easy to see that the original MH test (Eqn. 1) is equivalent to the following procedure:\nDraw u \u223c Uniform[0,1] and accept the proposal \u03b8?if the average difference \u00b5 in the log-likelihoods of \u03b8?and\ni=1p(xi;\u03b8).\nIf the dataset has a billion datapoints, it becomes very painful to compute S0(.) in the MH test, which\n3"},{"page":4,"text":"\u03b8tis greater than a threshold \u00b50, i.e. compute\n\u00b50=\n1\nNlog\n?\nu\u03c1(\u03b8t)q(\u03b8?|\u03b8t)\n\u03c1(\u03b8?)q(\u03b8t|\u03b8?)\n?\n, and (2)\n\u00b5 =\n1\nN\nN\n?\ni=1\nli where li= logp(xi;\u03b8?) \u2212 logp(xi;\u03b8t)(3)\nThen if \u00b5 > \u00b50, accept the proposal and set \u03b8t+1\u2190 \u03b8?. If \u00b5 \u2264 \u00b50, reject the proposal and set \u03b8t+1\u2190 \u03b8t.\nThis reformulation of the MH test makes it very easy to frame it as a statistical hypothesis test. Given\n\u00b50and a random sample {li1,...,lin} drawn without replacement from the population {l1,...,lN}, can we\ndecide whether the population mean \u00b5 is greater than or less than the threshold \u00b50? The answer to this\ndepends on the precision in the random sample. If the difference between the sample mean\u00afl and \u00b50 is\nsignificantly greater than the standard deviation s of\u00afl, we can make the decision to accept or reject the\nproposal confidently. If not, we should draw more data to increase the precision of\u00afl (reduce s) until we have\nenough evidence to make a decision.\nMore formally, we test the hypotheses H1: \u00b5 > \u00b50vs H2: \u00b5 < \u00b50. To do this, we proceed as follows: We\ncompute the sample mean\u00afl and the sample standard deviation sl=\ndeviation of\u00afl can be estimated as:\nsl\n\u221an\n?\nwithout replacement from a finite-sized population. Then, we compute the test statistic:\n?\n(l2\u2212 (\u00afl)2)\nn\nn\u22121. Then the standard\ns =\n?\n1 \u2212n \u2212 1\nN \u2212 1\n(4)\nwhere1 \u2212n\u22121\nN\u22121, the finite population correction term, is applied because we are drawing the subsample\nt =\n\u00afl \u2212 \u00b50\ns\n(5)\nApproximate MH test\nInput: \u03b8t, \u03b8?, ?, \u00b50, XN, m\nOutput: accept\n1: Initialize estimated means\u00afl \u2190 0 and l2\u2190 0\n2: Initialize n \u2190 0, done \u2190 false\n3: Draw u \u223c Uniform[0,1]\n4: while not done do\n5:\nDraw mini-batch X of size min (m, N \u2212 n) without replacement from XN and set XN\u2190 XN\\ X\n6:\nUpdate\u00afl and l2using X, and n \u2190 n + |X|\n7:\nEstimate std s using Eqn. 4\n?????\n10:\naccept \u2190 true if\u00afl > \u00b50and false otherwise\n11:\ndone \u2190 true\n12:\nend if\n13: end while\n8:\nCompute \u03b4 \u2190 1 \u2212 \u03c6n\u22121\nif \u03b4 < ? then\n\u00afl \u2212 \u00b50\ns\n????\n?\n9:\nIf n is large enough for the central limit theorem (CLT) to hold, the test statistic t follows a standard\nStudent-t distribution with n \u2212 1 degrees of freedom, when \u00b5 = \u00b50 (see Fig. 7 in supplementary for an\nempirical verification).Then, we compute \u03b4 = 1 \u2212 \u03c6n\u22121(|t|) where \u03c6n\u22121(.) is the cdf of the standard\nStudent-t distribution with n\u22121 degrees of freedom. If \u03b4 < ? (a fixed threshold) we can confidently say that\n\u00b5 is significantly different from \u00b50. In this case, if\u00afl > \u00b50, we decide \u00b5 > \u00b50, otherwise we decide \u00b5 < \u00b50. If\n\u03b4 \u2265 ?, we do not have enough evidence to make a decision. In this case, we draw more data to reduce the\nuncertainty, s, in the sample mean\u00afl. We keep drawing more data until we have the required confidence (i.e.\nuntil \u03b4 < ?). Note, that this procedure will terminate because when we have used all the available data, i.e.\n4"},{"page":5,"text":"n = N, the standard deviation s is 0, the sample mean\u00afl = \u00b5 and \u03b4 = 0 < ?. So, we will make the same\ndecision as the original MH test would make. Pseudo-code for our test is shown in Algorithm . Here, we\nstart with a mini-batch of size m for the first test and increase it by m datapoints when required.\nThe advantage of our method is that often we can make confident decisions with n < N datapoints and\nsave on computation, although we introduce a small bias in the stationary distribution. But, we can use\nthe computational time we save to draw more samples and reduce the variance. The bias-variance trade-off\ncan be controlled by adjusting the knob ?. When ? is high, we make decisions without sufficient evidence\nand introduce a high bias. As ? \u2192 0, we make more accurate decisions but are forced to examine more data\nwhich results in high variance.\nOur algorithm will behave erratically if the CLT does not hold, e.g. with very sparse datasets or datasets\nwith extreme outliers. The CLT assumption can be easily tested empirically before running the algorithm to\navoid such pathological situations. The sequential hypothesis testing method can also be used to speed-up\nGibbs sampling in densely connected Markov Random Fields. We explore this idea briefly in Section F of\nthe supplementary.\n5 Error Analysis and Test Design\nIn 5.1, we study the relation between the parameter ?, the error E of the complete sequential test, the error\n\u2206 in the acceptance probability and the error in the stationary distribution. In 5.2, we describe how to\ndesign an optimal test that minimizes data usage given a bound on the error.\n5.1Error Analysis and Estimation\nThe parameter ? is an upper-bound on the error of a single test and not the error of the complete sequential\ntest. To compute this error, we assume a) n is large enough that the t statistics can be approximated\nwith z statistics, and b) the joint distribution of the\u00afl\u2019s corresponding to different mini-batches used in\nthe test is multivariate normal. Under these assumptions, we can show that the test statistic at different\nstages of the sequential test follows a Gaussian Random Walk process. This allows us to compute the error\nof the sequential test E(\u00b5std,m,?), and the expected proportion of the data required to reach a decision\n\u00af \u03c0(\u00b5std,m,?), using an efficient dynamic programming algorithm. Note that E and \u00af \u03c0 depend on \u03b8, \u03b8?and u\nonly through the \u2018standardized mean\u2019 defined as \u00b5std(u,\u03b8,\u03b8?)\ndef\n=(\u00b5(\u03b8,\u03b8?) \u2212 \u00b50(\u03b8,\u03b8?,u))\u221aN \u2212 1\n\u03c3l(\u03b8,\u03b8?)\nwhere \u03c3lis\nthe true standard deviation of the li\u2019s. See Section A of the supplementary for a detailed derivation and an\nempirical validation of the assumptions.\nFig. 1 shows the theoretical and actual error of 1000 sequential tests for the logistic regression model\ndescribed in Section 6.1. The error E(\u00b5std,m,?) is highest in the worst case when \u00b5 = \u00b50. Therefore,\nE(0,m,?) is an upper-bound on E. Since the error decreases sharply as \u00b5 moves away from \u00b50, we can get a\nmore useful estimate of E if we have some knowledge about the distribution of \u00b5std\u2019s that will be encountered\nduring the Markov chain simulation.\nNow, let Pa,?(\u03b8,\u03b8?) be the actual acceptance probability of our algorithm and let \u2206(\u03b8,\u03b8?)\nPa(\u03b8,\u03b8?) be the error in Pa,?. In Section B of the supplementary, we show that for any (\u03b8,\u03b8?):\n?1\nThus, the errors corresponding to different u\u2019s partly cancel each other. As a result, although |\u2206(\u03b8,\u03b8?)|\nis upper-bounded by the worst-case error E(0,m,?) of the sequential test, the actual error is usually much\nsmaller. For any given (\u03b8,\u03b8?), \u2206 can be computed easily using 1-dimensional quadrature.\nFinally, we show that the error in the stationary distribution is bounded linearly by \u2206max= sup\u03b8,\u03b8? |\u2206(\u03b8,\u03b8?)|.\nAs noted above, \u2206max\u2264 E(0,m,?) but is usually much smaller. Let dv(P,Q) denote the total variation dis-\ntance1between two distributions, P and Q. If the transition kernel T0of the exact Markov chain satisfies the\n1The total variation distance between two distributions P and Q, that are absolutely continuous w.r.t. measure \u03a9, is defined\nas dv(P,Q)def\n=\n2\nbe more precise).\ndef\n= Pa,?(\u03b8,\u03b8?)\u2212\n\u2206 =\nPa\nE(\u00b5std(u))du \u2212\n?Pa\n0\nE(\u00b5std(u))du(6)\n1\n?\n\u03b8\u2208\u0398|fP(\u03b8) \u2212 fQ(\u03b8)|d\u03a9(\u03b8) where fP and fQare their respective densities (or Radon-Nikodym derivatives to\n5"},{"page":6,"text":"\u221220\n\u221210\n0\n10\n20\n0.01\n0.05\n0.1\n0\n0.1\n0.2\n0.3\n0.4\n \nStandardized Mean, \u00b5std\n\u01eb\n \nP(Error)\nSimulation with 68% CI\nTheoretical\nUpper Bound\nFigure 1: Error E estimated using simulation (blue cross with 1 \u03c3 error bar) and dynamic programming (red\nline). An upper bound (black dashed line) is also shown.\ncontraction condition dv(PT0,S0) \u2264 \u03b7dv(P,S0) for all probability distributions P with a constant \u03b7 \u2208 [0,1),\nwe can prove (see supplementary Section C) the following upper bound on the error in the stationary distri-\nbution:\nTheorem 1. The distance between the posterior distribution S0and the stationary distribution of our ap-\nproximate Markov chain S?is upper bounded as:\ndv(S0,S?) \u2264\u2206max\n1 \u2212 \u03b7\n5.2 Optimal Sequential Test Design\nWe now briefly describe how to choose the parameters of the algorithm: ?, the error of a single test and m,\nthe mini-batch size. A very simple strategy we recommend is to choose m \u2248 500 so that the Central Limit\nTheorem holds and keep ? as small as possible while maintaining a low average data usage. This rule works\nwell in practice and is used in Experiments 6.1 - 6.4.\nThe more discerning practitioner can design an optimal test that minimizes the data used while keeping\nthe error below a given tolerance. Ideally, we want to do this based on a tolerance on the error in the\nstationary distribution S?. Unfortunately, this error depends on the contraction parameter, \u03b7, of the exact\ntransition kernel, which is difficult to compute. A more practical choice is a bound on the error \u2206 in the\nacceptance probability, since the error in S?increases linearly with \u2206. Since \u2206 is a function of (\u03b8,\u03b8?), we\ncan try to control the average value of \u2206 over the empirical distribution of (\u03b8,\u03b8?) that would be encountered\nwhile simulating the Markov chain. Given a tolerance \u2206\u2217on this average error, we can find the optimal m\nand ? by solving the following optimization problem (e.g. using grid search) to minimize the average data\nusage :\nmin\nm,?E\u03b8,\u03b8? [Eu\u00af \u03c0(\u00b5std(u,\u03b8,\u03b8?),m,?)]\ns.t. E\u03b8,\u03b8?|\u2206(m,?,\u03b8,\u03b8?)| \u2264 \u2206\u2217\n(7)\nIn the above equation, we estimate the average data usage, Eu[\u00af \u03c0], and the error in the acceptance probability,\n\u2206, using dynamic programming with one dimensional numerical quadrature on u. The empirical distribution\nfor computing the expectation with respect to (\u03b8,\u03b8?) can be obtained using a trial run of the Markov chain.\n6"},{"page":7,"text":"050100150200250300350400\n\u221212\n\u221210\n\u22128\n\u22126\n\u22124\n\u22122\n0\nWall Clock Time (secs)\nLog (Risk)\n \n \n\u03b5 =0.00, T = 75484\n\u03b5 =0.01, T = 133069\n\u03b5 =0.05, T = 200672\n\u03b5 =0.10, T = 257897\n\u03b5 =0.20, T = 422978\nFigure 2: Logistic Regression: Risk in predictive mean.\nWithout a trial run the best we can do is to control the worst case error E(0,m,?) (which is also an upper-\nbound on \u2206) in each sequential test by solving the following minimization problem:\nmin\nm,?\u00af \u03c0(0,m,?) s.t. E(0,m,?) \u2264 \u2206\u2217\n(8)\nBut this leads to a very conservative design as the worst case error is usually much higher than the average\ncase error. We illustrate the sequential design in Experiment 6.5. More details and a generalization of this\nmethod is given in supplementary Section D.\n6Experiments\n6.1Random Walk - Logistic Regression\nWe first test our method using a random walk proposal q(\u03b8?|\u03b8t) = N(\u03b8t,\u03c32\nwalk proposal is not efficient, it is very useful for illustrating our algorithm because the proposal does not\ncontain any information about the target distribution, unlike Langevin or Hamiltonian methods. So, the\nresponsibility of converging to the correct distribution lies solely with the MH test. Also since q is symmetric,\nit does not appear in the MH test and we can use \u00b50=\nThe target distribution in this experiment was the posterior for a logistic regression model trained on the\nMNIST dataset for classifying digits 7 vs 9. The dataset consisted of 12214 datapoints and we reduced the\ndimensionality from 784 to 50 using PCA. We chose a zero mean spherical Gaussian prior with precision =\n10, and set \u03c3RW= 0.01.\nIn Fig. 2, we show how the logarithm of the risk in estimating the predictive mean, decreases as a function\nof wall clock time. The predictive mean of a test point x\u2217is defined as Ep(\u03b8|XN)[p(x\u2217|\u03b8)]. To calculate the\nrisk, we first estimate the true predictive mean using a long run of Hybrid Monte Carlo. Then, we compute\nmultiple estimates of the predictive mean from our approximate algorithm and obtain the risk as the mean\nsquared error in these estimates. We plot the average risk of 2037 datapoints in the test set. Since the risk\nR = B2+V = B2+\u03c32f\nT, we expect it to decrease as a function of time until the bias dominates the variance.\nThe figure shows that even after collecting a lot of samples, the risk is still dominated by the variance and\nthe minimum risk is obtained with ? > 0.\nRW). Although the random\n1\nNlog[u\u03c1(\u03b8t)\/\u03c1(\u03b8?)].\n6.2Independent Component Analysis\nNext, we use our algorithm to sample from the posterior distribution of the unmixing matrix in Inde-\npendent Component Analysis (ICA) [Hyv\u00a8 arinen and Oja, 2000]. When using prewhitened data, the un-\nmixing matrix W \u2208 RD\u00d7Dis constrained to lie on the Stiefel manifold of orthonormal matrices.\nchoose a prior that is uniform over the manifold and zero elsewhere. We model the data as p(x|W) =\nWe\n7"},{"page":8,"text":"010002000\nWall Clock Time (secs)\n30004000500060007000\n\u22123.5\n\u22123\n\u22122.5\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\nLog (Risk)\n \n \n\u03b5 = 0, T = 5992\n\u03b5 = 0.01, T = 11320\n\u03b5 = 0.05, T = 40973\n\u03b5 = 0.1, T = 171917\n\u03b5 = 0.2, T = 1894000\nFigure 3: ICA: Risk in mean of Amari distance\n|det(W)|?D\ndistribution [Ouyang, 2008]. Since this is a symmetric proposal distribution, it does not appear in the MH\ntest and we can use \u00b50=\nTo perform a large scale experiment, we created a synthetic dataset by mixing 1.95 million samples of 4\nsources: (a) a Classical music recording (b) street \/ traffic noise (c) & (d) 2 independent Gaussian sources.\nTo measure the correctness of the sampler, we measure the risk in estimating I = Ep(W|X)[dA(W,W0)] where\nthe test function dAis the Amari distance [Amari et al., 1996] and W0is the true unmixing matrix. We\ncomputed the ground truth using a long run (T = 100K samples) of the exact MH algorithm. Then we ran\neach algorithm 10 times, each time for \u2248 6400 secs. We calculated the risk by averaging the squared error in\nthe estimate from each Markov chain, over the 10 chains. This is shown in Fig. 3. Note that even after 6400\nsecs the variance dominates the bias, as evidenced by the still decreasing risk, except for the most biased\nalgorithm with ? = 0.2. Also, the lowest risk at 6400 secs is obtained with ? = 0.1 and not the exact MH\nalgorithm (? = 0). But we expect the exact algorithm to outperform all the approximate algorithms if we\nwere to run for an infinite time.\nj=1\n?4cosh2(1\n2wT\njx)?\u22121where wjare the rows of W. Since the prior is zero outside the manifold,\nthe same is true for the posterior. Therefore we use a random walk on the Stiefel manifold as a proposal\n1\nNlog[u].\n6.3Variable selection in Logistic Regression\nNow, we apply our MH test to variable selection in a logistic regression model using the reversible jump\nMCMC algorithm of Green [1995]. We use a model that is similar to the Bayesian LASSO model for linear\nregression described in Chen et al. [2011]. Specifically, given D input features, our parameter \u03b8 = {\u03b2,\u03b3}\nwhere \u03b2 is a vector of D regression coefficients and \u03b3 is a D dimensional binary vector that indicates whether\na particular feature is included in the model or not. The prior we choose for \u03b2 is p(\u03b2j|\u03b3,\u03bd) =\nif \u03b3j = 1. If \u03b3j = 0, \u03b2j does not appear in the model. Here \u03bd is a shrinkage parameter that pushes \u03b2j\n1\n2\u03bdexp\n?\n\u2212|\u03b2j|\n\u03bd\n?\ntowards 0, and we choose a prior p(\u03bd) \u221d 1\/\u03bd. We also place a right truncated Poisson prior p(\u03b3|\u03bb) \u221d\non \u03b3 to control the size of the model, k =?D\np(\u03b2,\u03b3|XN,yN,\u03bb) \u221d lN(\u03b2,\u03b3)?\u03b2?\u2212k\nout \u03bb, we use it as a parameter to control the size of the model. We use the same proposal distribution as\nin Chen et al. [2011] which is a mixture of 3 type of moves that are picked randomly in each iteration: an\nupdate move, a birth move and a death move. A detailed description is given in Supplementary Section E.\nWe applied this to the MiniBooNE dataset from the UCI machine learning repository[Bache and Lichman,\n\u03bbk\n?k!\n?D\nk\nj=1\u03b3jWe set \u03bb = 10\u221210in this experiment.\nDenoting the likelihood of the data by lN(\u03b2,\u03b3), the posterior distribution after integrating out \u03bd is\n1\u03bbkB(k,D\u2212k+1) where B(.,.) is the beta function. Instead of integrating\n8"},{"page":9,"text":"0 1000\nWall Clock Time (secs)\n200030004000\n\u22129\n\u22128\n\u22127\n\u22126\n\u22125\n\u22124\n\u22123\n\u22122\n\u22121\nLog (Risk)\n \n \n\u03b5 = 0, T = 24583\n\u03b5 = 0.01, T = 137375\n\u03b5 = 0.05, T = 245906\n\u03b5 = 0.1, T = 419090\nFigure 4: RJMCMC: Risk in predictive mean\n2013]. Here the task is to classify electron neutrinos (signal) from muon neutrinos (background). There are\n130,065 datapoints (28% in +ve class) with 50 features to which we add a constant feature of 1\u2019s. We\nrandomly split the data into a training (80%) and testing (20%) set. To compute ground truth, we collected\nT=400K samples using the exact reversible jump algorithm (? = 0). Then, we ran the approximate MH\nalgorithm with different values of ? for around 3500 seconds. We plot the risk in predictive mean of test data\n(estimated from 10 Markov chains) in Fig. 4. Again we see that the lowest risk is obtained with ? > 0.\nThe acceptance rates for the birth\/death moves starts off at \u2248 20% but dies down to \u2248 2% once a\ngood model is found. The acceptance rate for update moves is kept at \u2248 50%. The model also suffers\nfrom local minima. For the plot in Fig. 4, we started with only one variable and we ended up learning\nmodels with around 12 features, giving a classification error \u2248 15%. But, if we initialize the sampler with\nall features included and initialize \u03b2 to the MAP value, we learn models with around 45 features, but with\na lower classification error \u2248 10%. Both the exact reversible jump algorithm and our approximate version\nsuffer from this problem. We should bear this in mind when interpreting \u201cground truth\u201d. However, we\nhave observed that when initialized with the same values, we obtain similar results with the approximate\nalgorithm and the exact algorithm (see e.g. Fig. 13 in supplementary).\n6.4 Stochastic Gradient Langevin Dynamics\nFinally, we apply our method to Stochastic Gradient Langevin Dynamics[Welling and Teh, 2011]. In each\niteration, we randomly draw a mini-batch Xnof size n, and propose:\n?\nn\n\u03b8?\u223c q(.|\u03b8,Xn) = N\n\u03b8 +\u03b1\n2\u2207\u03b8\n?\nN\n?\nx\u2208Xn\nlogp(x|\u03b8) + log\u03c1(\u03b8)\n?\n,\u03b1\n?\n(9)\nThe proposed state \u03b8?is always accepted (without conducting any MH test). Since the acceptance probability\napproaches 1 as we reduce \u03b1, the bias from not conducting the MH test can be kept under control by using\n\u03b1 \u2248 0. However, we have to use a reasonably large \u03b1 to keep the mixing rate high. This can be problematic\nfor some distributions, because SGLD relies solely on gradients of the log density and it can be easily thrown\noff track by large gradients in low density regions, unless \u03b1 \u2248 0.\nAs an example, consider an L1-regularized linear regression model. Given a dataset {xi,yi}N\nxi are predictors and yi are targets, we use a Gaussian error model p(y|x,\u03b8) \u221d exp?\u2212\u03bb\nourselves to a toy version of the problem where \u03b8 and x are one dimensional. We use a synthetic dataset with\nN = 10000 datapoints generated as yi= 0.5xi+ \u03be where \u03be \u223c N(0,1\/3). We choose \u03bb = 3 and \u03bb0= 4950,\nso that the prior is not washed out by the likelihood. The posterior density and the gradient of the log\nposterior are shown in figures 5(a) and 5(b) respectively.\ni=1where\n2(y \u2212 \u03b8Tx)2?\nand\nchoose a Laplacian prior for the parameters p(\u03b8) \u221d exp(\u2212\u03bb0?\u03b8?1). For pedagogical reasons, we will restrict\n9"},{"page":10,"text":"00.010.020.03\n\u03b8\n0.040.050.06\n0\n10\n20\n30\n40\n50\n60\n70\n80\np(\u03b8|Data)\n(a) Posterior density\n00.010.020.03\n\u03b8\n0.040.050.06\n\u22122000\n0\n2000\n4000\n6000\n8000\n10000\n\u2207\u03b8 log p(\u03b8|Data)\n(b) Gradient of log posterior\n00.01 0.02 0.03\n\u03b8\n0.040.050.06\n0\n10\n20\n30\n40\n50\n60\n70\n80\np(\u03b8|Data)\n \n \nSGLD\nTrue\n(c) SGLD\n00.010.020.03\n\u03b8\n0.040.05 0.06\n0\n10\n20\n30\n40\n50\n60\n70\n80\np(\u03b8|Data)\n \n \n\u03b5 = 0.5\nTrue\n(d) SGLD + MH, ? = 0.5.\nFigure 5: Pitfalls of using uncorrected SGLD\nAn empirical histogram of samples obtained by running SGLD with \u03b1 = 5\u00d710\u22126is shown in Fig. 5(c). The\neffect of omitting the MH test is quite severe here. When the sampler reaches the mode of the distribution,\nthe Langevin noise occasionally throws it into the valley to the left, where the gradient is very high. This\npropels the sampler far off to the right, after which it takes a long time to find its way back to the mode.\nHowever, if we had used an MH accept-reject test, most of these troublesome jumps into the valley would\nbe rejected because the density in the valley is much lower than that at the mode.\nTo apply an MH test, note that the SGLD proposal q(\u03b8?|\u03b8) can be considered a mixture of component ker-\nnels q(\u03b8?|\u03b8,Xn) corresponding to different mini-batches. The mixture kernel will satisfy detailed balance with\nrespect to the posterior distribution if the MH test enforces detailed balance between the posterior and each\nof the component kernels q(\u03b8?|\u03b8,Xn). Thus, we can use an MH test with \u00b50=\nThe result of running SGLD (keeping \u03b1 = 5\u00d710\u22126as before) corrected using our approximate MH test,\nwith ? = 0.5, is shown in Fig. 5(d). As expected, the MH test rejects most troublesome jumps into the valley\nbecause the density in the valley is much lower than that at the mode. The stationary distribution is almost\nindistinguishable from the true posterior. Note that when ? = 0.5, a decision is always made in the first step\n(using just m = 500 datapoints) without querying additional data sequentially.\n1\nNlog\n?\nu\u03c1(\u03b8t)q(\u03b8?|\u03b8t,Xn)\n\u03c1(\u03b8?)q(\u03b8t|\u03b8?,Xn)\n?\n.\n6.5Optimal Design of Sequential Tests\nWe illustrate the advantages of the optimal test design proposed in Section 5.2 by applying it to the ICA\nexperiment described in Section 6.2. We consider two design methods: the \u2018average design\u2019 (Eqn. 7) and\nthe \u2018worst-case design\u2019 (Eqn. 8). For the average design, we collected 100 samples of the Markov chain to\napproximate the expectation of the error over (\u03b8,\u03b8?). We will call these samples the training set. The worst\ncase design does not need the training set as it does not involve the distribution of (\u03b8,\u03b8?). We compute the\noptimal m and ? using grid search, for different values of the target training error, for both designs. We\nthen collect a new set of 100 samples (\u03b8,\u03b8?) and measure the average error and data usage on this test set\n10"},{"page":11,"text":"10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n\u22124\n10\n\u22122\nTarget Average Error\nTest Average Error\n \n \nAvg\u2212Design\nAvg\u2212Design Fix m\nWC\u2212Design\nTarget\n(a) Test Average Error\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\nTarget Average Error\nAverage Data Usage\n \n \nAvg\u2212Design\nAvg\u2212Design Fix m\nWC\u2212Design\n(b) Average Data Usage\nFigure 6: Test average error in Paand data usage Eu[\u00af \u03c0] for the ICA experiment using average design over\nboth m and ? (?), with fixed m = 600 (?), and worst-case design (?).\n(Fig. 6).\nFor the same target error on the training set, the worst-case design gives a conservative parameter setting\nthat achieves a much smaller error on the test set. In contrast, the average design achieves a test error that\nis almost the same as the target error (Fig. 6(a)). Therefore, it uses much less data than the worst-case\ndesign (Fig. 6(b)).\nWe also analyze the performance in the case where we fix m = 600 and only change ?. This is a simple\nheuristic we recommended at the beginning of Section 5.2. Although this usually works well, using the\noptimal test design ensures the best possible performance. In this experiment, we see that when the error is\nlarge, the optimal design uses only half the data (Fig. 6(b)) used by the heuristic and is therefore twice as\nfast.\n7Conclusions and Future Work\nWe have taken a first step towards cutting the computational budget of the Metropolis-Hastings MCMC\nalgorithm, which takes O(N) likelihood evaluations to make the binary decision of accepting or rejecting a\nproposed sample. In our approach, we compute the probability that a new sample will be accepted based on\na subset of the data. We increase the cardinality of the subset until a prescribed confidence level is reached.\nIn the process we create a bias, which is more than compensated for by a reduction in variance due to the\nfact that we can draw more samples per unit time. Current MCMC procedures do not take these trade-offs\ninto account. In this work we use a fixed decision threshold for accepting or rejecting a sample, but in theory\na better algorithm can be obtained by adapting this threshold over time. An adaptive algorithm can tune\nbias and variance contributions in such a way that at every moment our risk (the sum of squared bias and\nvariance) is as low as possible. We leave these extensions for future work.\nAcknowledgments\nWe thank Alex Ihler, Daniel Gillen, Sungjin Ahn and Babak Shahbaba for their valuable suggestions. This\nmaterial is based upon work supported by the National Science Foundation under Grant No. 1216045.\n11"},{"page":12,"text":"ADistribution of the test statistic\nIn the sequential test, we first compute the test statistic from a mini-batch of size m. If a decision cannot be\nmade with this statistic, we keep increasing the mini-batch size by m datapoints until we reach a decision.\nThis procedure is guaranteed to terminate as explained in Section 4.\nThe parameter ? controls the probability of making an error in a single test and not the complete\nsequential test. As the statistics across multiple tests are correlated with each other, we should first obtain\nthe joint distribution of these statistics in order to estimate the error of the complete sequential test. Let\u00aflj\nand sl,jbe the sample mean and standard deviation respectively, computed using the first j mini-batches.\nNotice that when the size of a mini-batch is large enough, e.g. n > 100, the central limit theorem applies,\nand also sl,j is an accurate estimate of the population standard deviation. Additionally, since the degrees\nof freedom is high, the t-statistic in Eqn. 5 reduces to a z-statistic. Therefore, it is reasonable to make the\nfollowing assumptions:\nAssumption 1. The joint distribution of the sequence (\u00afl1,\u00afl2,...) follows a multivariate normal distribution.\nAssumption 2. sl= \u03c3l, where \u03c3l= std({li})\nFig. 7 shows that when \u00b5 = \u00b50the empirical marginal distribution of tj(or zj) is well fitted by both a\nstandard student-t and a standard normal distribution.\nUnder these assumptions, we state and prove the following proposition about the joint distribution of\nthe z-statistic z = (z1,z2,...), where zj\n= (\u00aflj\u2212 \u00b50)\/\u03c3l\u2248 tj, from different tests.\nProposition 2. Given Assumption 1 and 2, the sequence z follows a Gaussian random walk process:\ndef\nP(zj|z1,...,zj\u22121) = N(mj(zj\u22121),\u03c32\nz,j) (10)\nwhere\nmj(zj\u22121) = \u00b5std\u03c0j\u2212 \u03c0j\u22121\n1 \u2212 \u03c0j\u22121\n?\n\u03c0j\n\u03c0j\u2212 \u03c0j\u22121\n\u03c0j(1 \u2212 \u03c0j\u22121)\n1\n?\u03c0j(1 \u2212 \u03c0j)\n1 \u2212 \u03c0j\n1 \u2212 \u03c0j\u22121\n+ zj\u22121\n\u03c0j\u22121\n(11)\n\u03c32\nz,j=\n(12)\nwith \u00b5std=\nmini-batches.\n(\u00b5\u2212\u00b50)\u221aN\u22121\n\u03c3l\nbeing the standardized mean, and \u03c0j= jm\/N the proportion of data in the first j\nProof of Proposition 2. Denote by xjthe average of m l\u2019s in the j-th mini-batch. Taking into account the\nfact that the l\u2019s are drawn without replacement, we can compute the mean and covariance of the xj\u2019s as:\nE[xj] = \u00b5(13)\nCov(xi,xj) =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\uf8f2\n\u03c32\nm\nl\n?\n\u03c32\nN \u2212 1\n1 \u2212m \u2212 1\nN \u2212 1\nl\n?\n, i = j\n\u2212\n, i ?= j\n(14)\nIt is trivial to derive the expression for the mean. For the covariance, we first derive the covariance matrix\n12"},{"page":13,"text":"\u2212505\n0\n0.1\n0.2\n0.3\n0.4\n \n \nEmpirical\nStudent\u2212t\nNormal\n(a) n=500\n\u2212505\n0\n0.1\n0.2\n0.3\n0.4\n \n \nEmpirical\nStudent\u2212t\nNormal\n(b) n=5000\n\u2212505\n0\n0.1\n0.2\n0.3\n0.4\n \n \nEmpirical\nStudent\u2212t\nNormal\n(c) n=10000\nFigure 7: Empirical distribution (blue bars) of the t-statistic under resampling n datapoints without replace-\nment from a dataset composed of digits 7 and 9 from the MNIST dataset (total N = 12214 points, mean of\nl\u2019s is removed). Also shown are a standard normal (green dashed) and a student-t distribution with n \u2212 1\ndegrees of freedom (red solid).\nof single data points as\nCov(lk,lk?) = Ek,k?[lklk?] \u2212 Ek[lk]Ek?[lk?]\nif k = k?\n= l2\nk\u2212 \u00b52def\n= \u03c32\nl\nif k ?= k?\n= Ek?=k?[lklk?] \u2212 \u00b52\n=\nN(N \u2212 1)(\nN\nN \u2212 1\u00b52\u2212\n= \u2212\nN \u2212 1\n1\n?\nk,k?\nlkl?\nk\u2212\n?\nk\nl2\nk) \u2212 \u00b52\n=\nl2\nk\nN \u2212 1\u2212 \u00b52\n\u03c32\nl\n(15)\nNow, as xjcan be written as a linear combination of the elements in j-th mini-batch as xj=\nexpression for covariance in Eqn. 14 follows immediately from:\n1\nm1Tlj, the\nCov(xi,xj) = E[xixj] \u2212 E[xi]E[xj] =\n1\nm21TCov(lilT\nj)1(16)\nAccording to Assumption 1, the joint distribution of zj\u2019s is Gaussian because zjis a linear combination\nof\u00aflj\u2019s. It is however easier to derive the mean and covariance matrix of zj\u2019s by considering the vector z as\na linear function of x: z = Q(x \u2212 \u00b501) with\n?????????\nwhere\ndj=\nj\u03c3x\nQ =\nd1\nd2\n...\ndj\n??????????????????\n1\n1\n...\n1\n1\n...\n1\n...\n...1\n?????????\n(17)\n\u221aN \u2212 1\n?\nN\u2212jm\njm\n(18)\nThe mean and covariance can be computed as E[z] = Q1(\u00b5 \u2212 \u00b50) and Cov(z) = QCov(x)QTand the\nconditional distribution P(zj|z1,...,zj\u22121) follows straightforwardly. We conclude the proof by plugging the\ndefinition of \u00b5stdand \u03c0jinto the distribution.\n13"},{"page":14,"text":"Figure 8: An example of the random walk followed by z with \u00b5std> 0.\nFig. 8 shows the mean and 95% confidence interval of the random walk as a function of \u03c0 with a few\nrealizations of the z sequence. Notice that as the proportion of observed data \u03c0japproaches 1, the mean of\nzjapproaches infinity with a constant variance of 1. This is consistent with the fact that when we observe\nall the data, we will always make a correct decision.\nIt is also worth noting that given the standardized mean \u00b5stdand \u03c0j, the process is independent of the\nactual size of a mini-batch m, population size N, or the variance of l\u2019s \u03c32\neven if we use a different size for each mini-batch. This formulation allows us to study general properties of\nthe sequential test, independent of any particular dataset.\nApplying the individual tests \u03b4 \u2277 ? \u21d4 |zj| \u2277 \u03a6(1 \u2212 ?)\nthresholding the absolute value of zjat \u03c0jwith a bound G as shown in Fig. 9. Instead of m and ?, we will\nuse \u03c01 = m\/N and G as the parameters of the sequential test in the supplementary. The probability of\nincorrectly deciding \u00b5 < \u00b50when \u00b5 \u2265 \u00b50over the whole sequential test is computed as:\nJ\n?\nwhere J = ?1\/\u03c01? is the maximum number of tests. Similarly the probability of incorrectly deciding \u00b5 \u2265 \u00b50\nwhen \u00b5 < \u00b50can be computed similarly by replacing zj< \u2212G with zj> G in Eqn. 19. We can also compute\nthe expected proportion of data that will be used in the sequential test as:\nl. Thus, Eqns. 11 and 12 apply\ndef\n= G at the j-th mini-batch corresponds to\nE(\u00b5std,\u03c01,G) =\nj=1\nP(zj< \u2212G,|zi| \u2264 G,\u2200i < j)(19)\n\u00af \u03c0(\u00b5std,\u03c01,G) = Ez[\u03c0j?]\n?\n=\nJ\nj=1\n\u03c0jP(|zj| > G,|zi| \u2264 G,\u2200i < j)(20)\nwhere j?denotes the time when the sequential test terminates. Eqn. 19 and 20 can be efficiently approximated\ntogether using a dynamic programming algorithm by discretizing the value of zjbetween [\u2212G,G]. The time\ncomplexity of this algorithm is O(L2J) where L is the number of discretized values.\nThe error and data usage as functions of \u00b5stdare maximum in the worst case scenario when \u00b5std\u2192 0 \u21d4\n\u00b5 \u2192 \u00b50. In this case we have:\nE(0,\u03c01,G) =lim\n\u00b5std\u21920E(\u00b5std,\u03c01,G) = (1 \u2212 P(j?= J))\/2\n= Eworst(\u03c01,G)\ndef\n(21)\n14"},{"page":15,"text":"Figure 9: Sequential test with 3 mini-batches. Red dashed line is the bound G.\nFigs. 1 and 10 show respectively that the theoretical value of the error (E) and the average data usage\n(\u00af \u03c0) estimated using our dynamic programming algorithm match the simulated values. Also, note that both\nerror and data usage drop off very fast as \u00b5 moves away from \u00b50.\nBError in One Metropolis-Hastings Step\nIn the approximate Metropolis-Hasting test, one first draws a uniform random variable u, and then conducts\nthe sequential test. As \u00b5stdis a function of u (and \u00b5, \u03c3l, both of which depend on \u03b8 and \u03b8?), E measures the\nprobability that one will make a wrong decision conditioned on u. One might expect that the average error\nin the accept\/reject step of M-H using sequential test is the expected value of E w.r.t. to the distribution of\nu. But in fact, we can usually achieve a significantly smaller error than a typical value of E. This is because\nwith a varying u, there is some probability that \u00b5 > \u00b50(u) and also some probability that \u00b5 < \u00b50(u). Part\nof the error one will make given a fixed u can be canceled when we marginalize out the distribution of u.\nFollowing the definition of \u00b50(u) for M-H in Eqn. 2, we can compute the actual error in the acceptance\nprobability as:\n\u2206(\u00b5(\u03b8,\u03b8?),\u03c3l(\u03b8,\u03b8?),\u03c01,G) = Pa,?\u2212 Pa\n?1\n?1\n?1\n=\n0\nP?(\u00b5 > \u00b50(u))du \u2212\n?Pa\n?Pa\n?Pa\n0\ndu\n=\nPa\nP?(\u00b5 > \u00b50(u))du \u2212\n0\n(1 \u2212 P?(\u00b5 > \u00b50(u)))du\n=\nPa\nE(\u00b5 \u2212 \u00b50(u))du \u2212\n0\nE(\u00b5 \u2212 \u00b50(u))du(22)\nTherefore, it is often observed in experiments (see Fig. 11 for example) that when Pa\u2248 0.5, a typical value of\n\u00b5std(u) is close to 0, and the average value of the absolute error |E| can be large. But due to the cancellation\nof errors, the actual acceptance probability Pa,?can approximate Pavery well. Fig. 12 shows the approximate\nPain one step of M-H. This result also suggests that making use of some (approximate) knowledge about \u00b5\nand \u03c3lwill help us obtain a much better estimate of the error than the worst case analysis in Eqn. 21.\n15"},{"page":16,"text":"\u221220\u2212100102030\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nStandardized Mean, \u00b5std\nAverage Data Usage\n \n \nSimulation\nTheoretical\nWorst Case\nFigure 10: Average data usage \u00af \u03c0 estimated using simulation (blue cross) and dynamic programming (red\nline). The worst case scenario with \u00b5std= 0 is also shown (black dashed line).\n0 0.2 0.4\nExact Pa\n0.60.81\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nError, |\u2206 Pa|\n \n \nAverage Abs Error\nActual Error with u marginalized\nUpper Bound\nFigure 11: Error \u2206 in the acceptance probability (magenta circle) vs. exact acceptance probability Pa. Blue\ncrosses are the expected value of |E| w.r.t. the distribution of u. Black dashed line shows the upper bound.\n00.20.40.60.81\n0\n0.2\n0.4\n0.6\n0.8\n1\nExact Probability, Pa\nApproximate Probability, Pa,\u03b5\nFigure 12: Approximate acceptance probability vs. true acceptance probability.\n16"},{"page":17,"text":"CProof of Theorem 1\nC.1Upper Bound Based on One Step Error\nWe first prove a lemma that will be used for the proof of Theorem 1.\nLemma 3. Given two transition kernels, T0and T?, with respective stationary distributions, S0and S?, if\nT0satisfies the following contraction condition with a constant \u03b7 \u2208 [0,1) for all probability distributions P:\ndv(PT0,S0) \u2264 \u03b7dv(P,S0)\nand the one step error between T0and T?is upper bounded uniformly with a constant \u2206 > 0 as:\ndv(PT0,PT?) \u2264 \u2206,\u2200P\nthen the distance between S0and S?is bounded as:\n(23)\n(24)\ndv(S0,S?) \u2264\n\u2206\n1 \u2212 \u03b7\n(25)\nProof. Consider a Markov chain with transition kernel T?initialized from an arbitrary distribution P. Denote\nthe distribution after t steps by P(t)def\n= PTt\nP(t). According to the one step error bound in Eqn. 24, the distance between P(t+1)and the distribution\nobtained by applying T0to P(t)is upper bounded as:\ndv(P(t+1),P(t)T0) = dv(P(t)T?,P(t)T0) \u2264 \u2206\nFollowing the contraction condition of T0in Eqn. 23, the distance of P(t)T0from its stationary distribution\nS0is less than P(t)as\ndv(P(t)T0,S0) \u2264 \u03b7dv(P(t),S0)\nNow let us use the triangle inequality to combine Eqn. 26 and 27 to obtain an upper bounded for the distance\nbetween P(t+1)and S0:\ndv(P(t+1),S0) \u2264 dv(P(t+1),P(t)T0) + dv(P(t)T0,S0)\n\u2264 \u2206 + \u03b7dv(P(t),S0)\n?. At every time step, t \u2265 0, we apply the transition kernel T?on\n(26)\n(27)\n(28)\nLet r < 1\u2212\u03b7 be any positive constant and consider the ball B(S0,\u2206\noutside the ball, we have \u2206 \u2264 rdv(P(t),S). Plugging this into Eqn. 28, we can obtain a contraction condition\nfor P(t)towards S0:\ndv(P(t+1),S0) \u2264 (r + \u03b7)dv(P(t),S0)\nSo if the initial distribution P is outside the ball, the Markov chain will move monotonically into the ball\nwithin a finite number of steps. Let us denote the first time it enters the ball as tr. If the initial distribution\nis already inside the ball, we simply let tr= 0. We then show by induction that P(t)will stay inside the ball\nfor all t \u2265 tr.\n1. At t = tr, P(t)\u2208 B(S0,\u2206\n2. Assume P(t)\u2208 B(S0,\u2206\ndv(P(t+1),S0) \u2264 \u2206 + \u03b7\u2206\n=\u21d2 P(t+1)\u2208 B(S0,\u2206\nr)\ndef\n= {P : dv(P,S0) <\u2206\nr}. When P(t)is\n(29)\nr) holds by the definition of tr.\nr) for some t \u2265 tr. Then, following Eqn. 28, we have\nr\n=r + \u03b7\nr\n\u2206 <\u2206\nr\nr)(30)\n17"},{"page":18,"text":"Therefore, P(t)\u2208 B(S0,\u2206\nr) holds for all t \u2265 tr. Since P(t)converges to S?, it follows that:\ndv(S?,S0) <\u2206\nr,\u2200r < 1 \u2212 \u03b7(31)\nTaking the limit r \u2192 1 \u2212 \u03b7, we prove the lemma:\ndv(S?,S) \u2264\n\u2206\n1 \u2212 \u03b7\n(32)\nC.2Proof of Theorem 1\nWe first derive an upper bound for the one step error of the approximate Metropolis-Hastings algorithm, and\nthen use Lemma 3 to prove Theorem 1. The transition kernel of the exact Metropolis-Hastings algorithm\ncan be written as\nT0(\u03b8,\u03b8?) = Pa(\u03b8,\u03b8?)q(\u03b8?|\u03b8) + (1 \u2212 Pa(\u03b8,\u03b8?))\u03b4D(\u03b8?\u2212 \u03b8)\nwhere \u03b4D is the Dirac delta function. For the approximate algorithm proposed in this paper, we use an\napproximate MH test with acceptance probability\u02dcPa,?(\u03b8,\u03b8?) where the error, \u2206Pa\nbounded as |\u2206Pa| \u2264 \u2206max. Now let us look at the distance between the distributions generated by one step\nof the exact kernel T0and the approximate kernel T?. For any P,\n?\n=\n\u03b8\n?\n= \u2206max\n(33)\ndef\n= Pa,?\u2212 Pa, is upper\n\u03b8?d\u03a9(\u03b8?)|(PT?)(\u03b8?) \u2212 (PT0)(\u03b8?)|\n?\n\u2264 \u2206max\n?\n\u03b8?d\u03a9(\u03b8?)\n????\n?\ndP(\u03b8)\u2206Pa(\u03b8,\u03b8?)(q(\u03b8?|\u03b8) \u2212 \u03b4D(\u03b8?\u2212 \u03b8))\n????\n????\n\u03b8?d\u03a9(\u03b8?)\n?\n\u03b8\ndP(\u03b8)(q(\u03b8?|\u03b8) + \u03b4D(\u03b8?\u2212 \u03b8))\n????\n\u03b8?d\u03a9(\u03b8?)(gQ(\u03b8?) + gP(\u03b8?)) = 2\u2206max\n(34)\nwhere gQ(\u03b8?)\nHastings without rejection. So we get an upper bound for the total variation distance as\ndef\n=\n?\n\u03b8dP(\u03b8)q(\u03b8?|\u03b8) is the density that would be obtained by applying one step of Metropolis-\ndv(PT?,PT0) =1\n2\n?\n\u03b8?d\u03a9(\u03b8?)|PT?\u2212 PT0| \u2264 \u2206max\n(35)\nApply Lemma 3 with \u2206 = \u2206maxand we prove Theorem 1.\nDOptimal Sequential Test Design\nIt is possible to design optimal tests that minimize the amount of data used while keeping the error below a\ngiven tolerance. Ideally, we want to do this based on a tolerance on the error in the stationary distribution\nS?. Unfortunately, this error depends on the contraction parameter, \u03b7, of the exact transition kernel, which\nis difficult to compute. A more practical choice is a bound \u2206maxon the error in the acceptance probability,\nsince the error in S?increases linearly with \u2206max.\nGiven \u2206max, we want to minimize the average data usage \u00af \u03c0 over the parameters ? (or G) and\/or m (or\n\u03c01) of the sequential test. Unfortunately, the error is a function of \u00b5 and \u03c3lwhich depend on \u03b8 and \u03b8?, and\nwe cannot afford to change the test design at every iteration.\nOne solution is to base the design on the upper bound of the worst case error in Eqn. 21 which does\nnot rely on \u00b5std. But we have shown in Section B that this is a rather loose bound and will lead to a very\nconservative design that wastes the power of the sequential test. Therefore, we instead propose to design the\n18"},{"page":19,"text":"test by bounding the expectation of the error w.r.t. the distribution P(\u00b5,\u03c3l). This leads to the following\noptimization problem:\nmin\n\u03c01,GE\u00b5,\u03c3lEu\u00af \u03c0(\u00b5,\u03c3l,\u00b50(u),\u03c01,G)\ns.t. E\u00b5,\u03c3l|\u2206(\u00b5,\u03c3l,\u03c01,G)| \u2264 \u2206max\n(36)\nThe expectation w.r.t. u can be computed accurately using one dimensional quadrature. For the expectation\nw.r.t. \u00b5 and \u03c3l, we collect a set of parameter samples (\u03b8,\u03b8?) during burn-in, compute the corresponding \u00b5 and\n\u03c3lfor each sample, and use them to empirically estimate the expectation. We can also consider collecting\nsamples periodically and adapting the sequential design over time. Once we obtain a set of samples {(\u00b5,\u03c3l)},\nthe optimization is carried out using grid search.\nWe have been using a constant bound G across all the individual tests. This is known as the Pocock design\n[Pocock, 1977]. A more flexible sequential design can be obtained by allowing G to change as a function of \u03c0.\nWang and Tsiatis [1987] proposed a bound sequence Gj= G0\u03c00.5\u2212\u03b1\nWhen \u03b1 = 0, it reduces to the Pocock design, and when \u03b1 = 1, it reduces to O\u2019Brien-Fleming design [O\u2019Brien\nand Fleming, 1979]. We can adopt this more general form in our optimization problem straightforwardly,\nand the grid search will now be conducted over three parameters, \u03c01, G0, and \u03b1.\nj\nwhere \u03b1 \u2208 [0.5,1] is a free parameter.\nEReversible Jump MCMC\nWe give a more detailed description of the different transition moves used in experiment 6.3. The update\nmove is the usual MCMC move which involves changing the parameter vector \u03b2 without changing the model\n\u03b3. Specifically, we randomly pick an active component j : \u03b3j= 1 and set \u03b2j= \u03b2j+\u03b7 where \u03b7 \u223c N(0,\u03c3update).\nThe birth move involves (for k < D) randomly picking an inactive component j : \u03b3j= 0 and setting \u03b3j= 1.\nWe also propose a new value for \u03b2j \u223c N(0,\u03c3birth). The birth move is paired with a corresponding death\nmove (for k > 1) which involves randomly picking an active component j : \u03b3j= 1 and setting \u03b3j= 0. The\ncorresponding \u03b2j is discarded. The probabilities of picking these moves p(\u03b3 \u2192 \u03b3?) is the same as in Chen\net al. [2011]. The value of \u00b50used in the MH test for different moves is given below.\n1. Update move:\n1\nNlog\n\u00b50=\n?\nu?\u03b2?\u2212k\n?\u03b2??\u2212k\n1\n1\n?\n(37)\n2. Birth move:\n\u00b50=\n1\nNlog\n?\nu?\u03b2?\u2212k\n1p(\u03b3 \u2192 \u03b3?)N(\u03b2j|0,\u03c3birth)(D \u2212 k)\n?\u03b2??\u2212(k+1)\n1\np(\u03b3?\u2192 \u03b3)\u03bbk\n?\n(38)\n2. Death move:\n\u00b50=\n1\nN\u00d7\nlog\n?\nu\n?\u03b2?\u2212k\n?\u03b2??\u2212(k\u22121)\n1p(\u03b3 \u2192 \u03b3?)\np(\u03b3?\u2192 \u03b3)\n1\n\u03bb(k \u2212 1)\nN(\u03b2j|0,\u03c3birth)(D \u2212 k + 1)\n?\n(39)\nWe used \u03c3update= 0.01 and \u03c3birth= 0.1 in this experiment. As mentioned in the main text, both the\nexact reversible jump algorithm and our approximate version suffer from local minima. But, when initialized\nwith the same values, we obtain similar results with both algorithms. For example, we plot the marginal\nposterior probability of including a feature in the model, i.e. p(\u03b3j= 1|XN,yN,\u03bb) in figure 13.\nFApplication to Gibbs Sampling\nThe same sequential testing method can be applied to the Gibbs sampling algorithm for discrete models. We\nstudy a model with binary variables in this paper while the extension to multi-valued variables is also possible.\nConsider running a Gibbs sampler on a probability distribution over D binary variables P(X1,...,XD). At\nevery iteration, it updates one variable Xiusing the following procedure:\n19"},{"page":20,"text":"010 203040 5060\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nGround Truth\nFeature\nInclusion Probability\n010 203040 5060\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n\u03b5 = 0.01\nFeature\nInclusion Probability\nFigure 13: Marginal probability of features to be included in the model\n1. Compute the conditional probability:\nP(Xi= 1|x\u2212i) =\nP(Xi= 1,x\u2212i)\nP(Xi= 1,x\u2212i) + P(Xi= 0,x\u2212i)\n(40)\nwhere x\u2212idenotes the value of all variables other than the ithone.\n2. Draw u \u223c Uniform[0,1]. If u < P(Xi= 1|x\u2212i) set Xi= 1, otherwise set Xi= 0.\nThe condition in step 2 is equivalent to checking:\nlogu\nlog(1 \u2212 u)<logP(Xi= 1,x\u2212i)\nlogP(Xi= 0,x\u2212i)\n(41)\nWhen the joint distribution is expensive to compute but can be represented as a product over multiple terms,\nP(X) =?N\n1\nNlog(1 \u2212 u)\n1\nN\nn=1\nn=1fn(X), we can apply our sequential test to speed up the Gibbs sampling algorithm. In this\ncase the variable \u00b50and \u00b5 is given by\n\u00b50=\nlogu\n(42)\n\u00b5 =\nN\n?\nlogfn(Xi= 1,x\u2212i)\nfn(Xi= 0,x\u2212i)\n(43)\nSimilar to the Metropolis-Hastings algorithm, given an upper bound in the error of the approximate\nconditional probability\n\u2206max= max\ni,x\u2212i|P(Xiis assigned 1|x\u2212i) \u2212 P(Xi= 1|x\u2212i)|\nwe can prove the following theorem:\n20"},{"page":21,"text":"Theorem 4. For a Gibbs sampler with a Dobrushin coefficient \u03b7 \u2208 [0,1) [Br\u00b4 emaud, 1999, \u00a77.6.2], the\ndistance between the stationary distribution and that of the approximate Gibbs sampler S?is upper bounded\nby\ndv(S0,S?) \u2264\u2206max\n1 \u2212 \u03b7\nProof. The proof is similar to that of Theorem 1. We first obtain an upper bound for the one step error and\nthen plug it into Lemma 3.\nThe exact transition kernel of the Gibbs sampler for variable Xican be represented by a matrix T0,iof\nsize 2D\u00d7 2D:\nT0,i(x,y) =\nP(Yi= yi|y\u2212i)\nwhere 1 \u2264 i \u2264 N,x,y \u2208 {0,1}D. The approximate transition kernel T?,ican be represented similarly as\n?\n?\n0if x\u2212i?= y\u2212i\notherwise\n(44)\nT?,i(x,y) =\n0 if x\u2212i?= y\u2212i\notherwiseP?(Yi= yi|y\u2212i)\n(45)\nwhere P?is the approximate conditional distribution. Define the approximation error \u2206Ti(x,y)\nT0,i(x,y). We know that \u2206Ti(x,y) = 0 if y\u2212i?= x\u2212iand it is upper bounded by \u2206maxfrom the premise of\nTheorem 4.\nNotice that the total variation distance reduces to a half of the L1distance for discrete distributions. For\nany distribution P, the one step error is bounded as\ndef\n= T?,i(x,y)\u2212\ndv(PT?,i,PT0,i) =1\n=1\n2\ny\n??????\n2\u2206max\n2?PT?,i\u2212 PT0,i?1\n?????\nP(xi,y\u2212i)\u2206P(xi|y\u2212i)\n?\n?\n?????\n?\nx\nP(x)\u2206T (x,y)\n=1\n2\ny\n?\n?\nxi\u2208{0,1}\n??????\n\u22641\ny\n|P(Y\u2212i= y\u2212i)|\n= \u2206max\n(46)\nFor a Gibbs sampling algorithm, we have the contraction condition [Br\u00b4 emaud, 1999, \u00a77.6.2]:\ndv(PT ,S) \u2264 \u03b7dv(P,S)\nPlug \u2206 = \u2206maxand \u03b7 into Lemma 3 and we obtain the conclusion.\n(47)\nF.1Experiments on Markov Random Fields\nWe illustrate the performance of our approximate Gibbs sampling algorithm on a synthetic Markov Random\nField. The model under consideration has D = 100 binary variables and they are densely connected by\npotential functions of three variables \u03c8i,j,k(Xi,Xj,Xk),\u2200i ?= j ?= k. There are D(D \u2212 1)(D \u2212 2)\/6 potential\nfunctions in total (we assume potential functions with permuted indices in the argument are the same\npotential function), and every function has 23= 8 values. The entries in the potential function tables are\ndrawn randomly from a log-normal distribution, log\u03c8i,j,k(Xi,Xj,Xk) \u223c N(0,0.02). To draw a Gibbs sample\nfor one variable Xiwe have to compute (D \u2212 1)(D \u2212 2)\/2 = 4851 pairs of potential functions as\nP(Xi= 1|x\u2212i)\nP(Xi= 0|x\u2212i)=\nThe approximate methods use a mini-batches of 500 pairs of potential functions at a time. We compare the\nexact Gibbs sampling algorithm with approximate versions with ? \u2208 {0.01,0.05,0.1,0.15,0.2,0.25}.\n?\ni?=j?=k\u03c8i,j,k(Xi= 1,xj,xk)\ni?=j?=k\u03c8i,j,k(Xi= 0,xj,xk)\n?\n(48)\n21"},{"page":22,"text":"00.20.4 0.60.81\n0\n0.2\n0.4\n0.6\n0.8\n1\nTrue Conditional Probability\nEmpirical Probability\n \n \nTrue probability\n\u03b5 = 0.01\n\u03b5 = 0.05\n\u03b5 = 0.1\n\u03b5 = 0.15\n\u03b5 = 0.2\n\u03b5 = 0.25\nFigure 14: Empirical conditional probability vs exact conditional probability for different values of ?. The\ndotted black line shows the result for exact Gibbs sampling.\n10\n0\n10\n1\n10\n2\n10\n3\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\nTime (s)\nL1 error\n \n \n\u03b5 = 0.01, T = 3429\n\u03b5 = 0.05, T = 3979\n\u03b5 = 0.10, T = 4494\n\u03b5 = 0.15, T = 4889\n\u03b5 = 0.20, T = 5507\n\u03b5 = 0.25, T = 5959\nGibbs, T = 2897\nFigure 15: Average L1error in the joint distribution over cliques of 5 variables vs running time for different\nvalues of ?. The black line shows the error of Gibbs sampler with an exact acceptance probability. T in the\nlegend indicates the number of samples obtained after 1000 seconds.\nTo measure the performance in approximating P(X) with samples xt, the ideal metric would be a distance\nbetween the empirical joint distribution and P. Since it is impossible to store all the 2100probabilities, we\ninstead repeatedly draw M = 1600 subsets of 5 variables, {sm}M\nthe average L1distance of the joint distribution on these subsets between the empirical distribution and P:\nm=1,sm\u2282 {1,...,D},|sm| = 5, and compute\nError =\n1\nM\n?\nsm\n?\u02c6P(Xsm) \u2212 P(Xsm)?1\n(49)\nThe true P is estimated by running exact Gibbs chains for a long time. We show the empirical conditional\nprobability obtained by our approximate algorithms (percentage of Xibeing assigned 1) for different ? in\nFig. 14. It tends to underestimate large probabilities and overestimate on the other end. When ? = 0.01,\nthe observed maximum error is within 0.01.\nFig. 15 shows the error for different ? as a function of the running time. For small ?, we use fewer\nmini-batches per iteration and thus generate more samples in the same amount of time than the exact Gibbs\nsampler. So the error decays faster in the beginning. As more samples are collected the variance is reduced.\nWe see that these plots converge towards their bias floor while the exact Gibbs sampler out-performs all the\napproximate methods at around 1000 seconds.\n22"},{"page":23,"text":"References\nS. Ahn, A. Korattikara, and M. Welling. Bayesian posterior sampling via stochastic gradient Fisher scoring.\nIn International Conference on Machine Learning, 2012.\nS.-i. Amari, A. Cichocki, H. H. Yang, et al. A new learning algorithm for blind signal separation. Advances\nin neural information processing systems, pages 757\u2013763, 1996.\nC. Andrieu and G. O. Roberts. The pseudo-marginal approach for efficient Monte Carlo computations. The\nAnnals of Statistics, 37(2):697\u2013725, 2009.\nK. Bache and M. Lichman. UCI machine learning repository, 2013. URL http:\/\/archive.ics.uci.edu\/ml.\nP. Br\u00b4 emaud. Markov chains: Gibbs fields, Monte Carlo simulation, and queues, volume 31. Springer, 1999.\nX. Chen, Z. Jane Wang, and M. J. McKeown.\nProcessing, 91(8):1920\u20131932, 2011.\nA Bayesian Lasso via reversible-jump MCMC.Signal\nP. Fearnhead, O. Papaspiliopoulos, and G. O. Roberts. Particle filters for partially observed diffusions.\nJournal of the Royal Statistical Society: Series B (Statistical Methodology), 70(4):755\u2013777, 2008.\nD. Gamerman and H. F. Lopes. Markov chain Monte Carlo: stochastic simulation for Bayesian inference,\nvolume 68. Chapman & Hall\/CRC, 2006.\nP. J. Green. Reversible jump Markov chain Monte Carlo computation and Bayesian model determination.\nBiometrika, 82(4):711\u2013732, 1995.\nW. K. Hastings. Monte Carlo sampling methods using Markov chains and their applications. Biometrika,\n57(1):97\u2013109, 1970.\nA. Hyv\u00a8 arinen and E. Oja. Independent component analysis: algorithms and applications. Neural networks,\n13(4):411\u2013430, 2000.\nL. Lin, K. Liu, and J. Sloan. A noisy Monte Carlo algorithm. Physical Review D, 61(7):074505, 2000.\nN. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller. Equation of state calcula-\ntions by fast computing machines. The journal of chemical physics, 21:1087, 1953.\nP. C. O\u2019Brien and T. R. Fleming. A multiple testing procedure for clinical trials. Biometrics, pages 549\u2013556,\n1979.\nZ. Ouyang. Bayesian Additive Regression Kernels. PhD thesis, Duke University, 2008.\nS. J. Pocock. Group sequential methods in the design and analysis of clinical trials. Biometrika, 64(2):\n191\u2013199, 1977.\nS. Singh, M. Wick, and A. McCallum. Monte Carlo MCMC: efficient inference by approximate sampling.\nIn Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and\nComputational Natural Language Learning, pages 1104\u20131113. Association for Computational Linguistics,\n2012.\nS. K. Wang and A. A. Tsiatis. Approximately optimal one-parameter boundaries for group sequential trials.\nBiometrics, pages 193\u2013199, 1987.\nM. Welling and Y. Teh. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the\n28th International Conference on Machine Learning (ICML), pages 681\u2013688, 2011.\n23"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Yutian_Chen3\/publication\/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget\/links\/5406e5e90cf23d9765a81dca.pdf","widgetId":"rgw29_56ab1a186f7c6"},"id":"rgw29_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=236235190&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw30_56ab1a186f7c6"},"id":"rgw30_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=236235190&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":236235190,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":236235190,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2045181100,"url":"researcher\/2045181100_Sanvesh_Srivastava","fullname":"Sanvesh Srivastava","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084332535,"url":"researcher\/2084332535_Cheng_Li","fullname":"Cheng Li","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9530149,"url":"researcher\/9530149_David_B_Dunson","fullname":"David B. Dunson","last":true,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272381621043241%401441952247912_m\/David_Dunson.png"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Aug 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space","usePlainButton":true,"publicationUid":281262237,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space","title":"Scalable Bayes via Barycenter in Wasserstein Space","displayTitleAsLink":true,"authors":[{"id":2045181100,"url":"researcher\/2045181100_Sanvesh_Srivastava","fullname":"Sanvesh Srivastava","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084332535,"url":"researcher\/2084332535_Cheng_Li","fullname":"Cheng Li","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9530149,"url":"researcher\/9530149_David_B_Dunson","fullname":"David B. Dunson","last":true,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272381621043241%401441952247912_m\/David_Dunson.png"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"We propose a novel approach WASP for Bayesian inference when massive size of\nthe data prohibits posterior computations. WASP is estimated in three steps.\nFirst, data are divided into smaller computationally tractable subsets. Second,\nposterior draws of parameters are obtained for every subset after modifying\nsubset posteriors using stochastic approximation. Finally, the empirical\nmeasures of samples from each subset posterior are combined through their\nbarycenter in the Wasserstein space of probability measures. Stochastic\napproximation ensures that posterior uncertainty quantification of the\nbarycenter matches with that of the full data posterior distribution. The\ncombining step can be conducted efficiently through a sparse linear program,\nwhich takes negligible time relative to sampling from subset posteriors,\nfacilitating scaling to massive data. WASP is very general and allows\napplication of existing sampling algorithms to massive data with minimal\nmodifications. We provide theoretical conditions under which rate of\nconvergence of WASP to the delta measure centered at the true parameter\ncoincides with the optimal parametric rate up to a logarithmic factor. WASP is\napplied for scalable Bayesian computations in a nonparametric mixture model and\na movie recommender database containing tens of millions of ratings.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/David_Dunson\/publication\/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space\/links\/55ffe48708ae07629e51e264.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/David_Dunson","sourceName":"David B Dunson","hasSourceUrl":true},"publicationUid":281262237,"publicationUrl":"publication\/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space\/links\/55ffe48708ae07629e51e264\/smallpreview.png","linkId":"55ffe48708ae07629e51e264","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=281262237&reference=55ffe48708ae07629e51e264&eventCode=&origin=publication_list","widgetId":"rgw34_56ab1a186f7c6"},"id":"rgw34_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=281262237&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"55ffe48708ae07629e51e264","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":236235190,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Relying on stochastic approximation, most of these approaches use stochastic gradient descent for optimization (Welling and Teh, 2011; Broderick et al., 2013; Hoffman et al., 2013). Optimization updates are also coupled with sampling using modified Hamiltonian or Langevin Dynamics to improve posterior exploration (Ahn et al., 2012; Korattikara et al., 2014). Recent work in this area has focused on using different factorizations and parametrizations to improve the efficiency of existing methods (Wang and Blei, 2013; Tan and Nott, 2013; Wand, 2014). "],"widgetId":"rgw35_56ab1a186f7c6"},"id":"rgw35_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw33_56ab1a186f7c6"},"id":"rgw33_56ab1a186f7c6","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=281262237&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2079652947,"url":"researcher\/2079652947_James_E_Johndrow","fullname":"James E. Johndrow","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079648813,"url":"researcher\/2079648813_Jonathan_C_Mattingly","fullname":"Jonathan C. Mattingly","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079631589,"url":"researcher\/2079631589_Sayan_Mukherjee","fullname":"Sayan Mukherjee","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9530149,"url":"researcher\/9530149_David_Dunson","fullname":"David Dunson","last":true,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272381621043241%401441952247912_m\/David_Dunson.png"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Aug 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":2,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/281058916_Approximations_of_Markov_Chains_and_High-Dimensional_Bayesian_Inference","usePlainButton":true,"publicationUid":281058916,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/281058916_Approximations_of_Markov_Chains_and_High-Dimensional_Bayesian_Inference","title":"Approximations of Markov Chains and High-Dimensional Bayesian Inference","displayTitleAsLink":true,"authors":[{"id":2079652947,"url":"researcher\/2079652947_James_E_Johndrow","fullname":"James E. Johndrow","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079648813,"url":"researcher\/2079648813_Jonathan_C_Mattingly","fullname":"Jonathan C. Mattingly","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079631589,"url":"researcher\/2079631589_Sayan_Mukherjee","fullname":"Sayan Mukherjee","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9530149,"url":"researcher\/9530149_David_Dunson","fullname":"David Dunson","last":true,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272381621043241%401441952247912_m\/David_Dunson.png"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"The Markov Chain Monte Carlo method is the dominant paradigm for posterior\ncomputation in Bayesian analysis. It has long been common to control\ncomputation time by making approximations to the Markov transition kernel.\nComparatively little attention has been paid to convergence and estimation\nerror in these approximating Markov Chains. We propose a framework for\nassessing when to use approximations in MCMC algorithms, and how much error in\nthe transition kernel should be tolerated to obtain optimal estimation\nperformance with respect to a specified loss function and computational budget.\nThe results require only ergodicity of the exact kernel and control of the\nkernel approximation accuracy. The theoretical framework is applied to\napproximations based on random subsets of data, low-rank approximations of\nGaussian processes, and a novel approximating Markov chain for discrete mixture\nmodels.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/281058916_Approximations_of_Markov_Chains_and_High-Dimensional_Bayesian_Inference","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/David_Dunson\/publication\/281058916_Approximations_of_Markov_Chains_and_High-Dimensional_Bayesian_Inference\/links\/55ffe48708aec948c4f9c17e.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/David_Dunson","sourceName":"David B Dunson","hasSourceUrl":true},"publicationUid":281058916,"publicationUrl":"publication\/281058916_Approximations_of_Markov_Chains_and_High-Dimensional_Bayesian_Inference","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/281058916_Approximations_of_Markov_Chains_and_High-Dimensional_Bayesian_Inference\/links\/55ffe48708aec948c4f9c17e\/smallpreview.png","linkId":"55ffe48708aec948c4f9c17e","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=281058916&reference=55ffe48708aec948c4f9c17e&eventCode=&origin=publication_list","widgetId":"rgw37_56ab1a186f7c6"},"id":"rgw37_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=281058916&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"55ffe48708aec948c4f9c17e","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":236235190,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/281058916_Approximations_of_Markov_Chains_and_High-Dimensional_Bayesian_Inference\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["An example of this class is provided in Korattikara et al. (2013), where V \u0102 t1, . . . , N u is a random subset of indices adaptively chosen to obtain a pre-specified type I error in a Metropolis-Hastings acceptance decision. "],"widgetId":"rgw38_56ab1a186f7c6"},"id":"rgw38_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw36_56ab1a186f7c6"},"id":"rgw36_56ab1a186f7c6","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=281058916&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2078862656,"url":"researcher\/2078862656_Allan_De_Freitas","fullname":"Allan De Freitas","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":54570685,"url":"researcher\/54570685_Francois_Septier","fullname":"Fran\u00e7ois Septier","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8858822,"url":"researcher\/8858822_Lyudmila_Mihaylova","fullname":"Lyudmila Mihaylova","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":5949805,"url":"researcher\/5949805_Simon_J_Godsill","fullname":"Simon J. Godsill","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Jul 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/280589611_How_Can_Subsampling_Reduce_Complexity_in_Sequential_MCMC_Methods_and_Deal_with_Big_Data_in_Target_Tracking","usePlainButton":true,"publicationUid":280589611,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/280589611_How_Can_Subsampling_Reduce_Complexity_in_Sequential_MCMC_Methods_and_Deal_with_Big_Data_in_Target_Tracking","title":"How Can Subsampling Reduce Complexity in Sequential MCMC Methods and Deal with Big Data in Target Tracking?","displayTitleAsLink":true,"authors":[{"id":2078862656,"url":"researcher\/2078862656_Allan_De_Freitas","fullname":"Allan De Freitas","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":54570685,"url":"researcher\/54570685_Francois_Septier","fullname":"Fran\u00e7ois Septier","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8858822,"url":"researcher\/8858822_Lyudmila_Mihaylova","fullname":"Lyudmila Mihaylova","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":5949805,"url":"researcher\/5949805_Simon_J_Godsill","fullname":"Simon J. Godsill","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Target tracking faces the challenge in coping with large volumes of data\nwhich requires efficient methods for real time applications. The complexity\nconsidered in this paper is when there is a large number of measurements which\nare required to be processed at each time step. Sequential Markov chain Monte\nCarlo (MCMC) has been shown to be a promising approach to target tracking in\ncomplex environments, especially when dealing with clutter. However, a large\nnumber of measurements usually results in large processing requirements. This\npaper goes beyond the current state-of-the-art and presents a novel Sequential\nMCMC approach that can overcome this challenge through adaptively subsampling\nthe set of measurements. Instead of using the whole large volume of available\ndata, the proposed algorithm performs a trade off between the number of\nmeasurements to be used and the desired accuracy of the estimates to be\nobtained in the presence of clutter. We show results with large improvements in\nprocessing time, more than 40% with a negligible loss in tracking performance,\ncompared with the solution without subsampling.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/280589611_How_Can_Subsampling_Reduce_Complexity_in_Sequential_MCMC_Methods_and_Deal_with_Big_Data_in_Target_Tracking","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Lyudmila_Mihaylova\/publication\/280589611_How_Can_Subsampling_Reduce_Complexity_in_Sequential_MCMC_Methods_and_Deal_with_Big_Data_in_Target_Tracking\/links\/55bf422b08ae092e96652cf3.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Lyudmila_Mihaylova","sourceName":"Lyudmila Mihaylova","hasSourceUrl":true},"publicationUid":280589611,"publicationUrl":"publication\/280589611_How_Can_Subsampling_Reduce_Complexity_in_Sequential_MCMC_Methods_and_Deal_with_Big_Data_in_Target_Tracking","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/280589611_How_Can_Subsampling_Reduce_Complexity_in_Sequential_MCMC_Methods_and_Deal_with_Big_Data_in_Target_Tracking\/links\/55bf422b08ae092e96652cf3\/smallpreview.png","linkId":"55bf422b08ae092e96652cf3","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=280589611&reference=55bf422b08ae092e96652cf3&eventCode=&origin=publication_list","widgetId":"rgw40_56ab1a186f7c6"},"id":"rgw40_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=280589611&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"55bf422b08ae092e96652cf3","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":236235190,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/280589611_How_Can_Subsampling_Reduce_Complexity_in_Sequential_MCMC_Methods_and_Deal_with_Big_Data_in_Target_Tracking\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["The largest hindrance being long processing times which could limit usage in applications required to run in real time. There have also been several algorithms [9], [10], [11] which have been proposed to help reduce computational complexities when performing static inference with MCMC techniques on large datasets. "],"widgetId":"rgw41_56ab1a186f7c6"},"id":"rgw41_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw39_56ab1a186f7c6"},"id":"rgw39_56ab1a186f7c6","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=280589611&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":236235190,"publicationLink":"publication\/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw32_56ab1a186f7c6"},"id":"rgw32_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=236235190&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=35","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":35,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw31_56ab1a186f7c6"},"id":"rgw31_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=236235190&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"5406e5e90cf23d9765a81dca","name":"Yutian Chen","date":"Sep 03, 2014 ","nameLink":"profile\/Yutian_Chen3","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Yutian_Chen3\/publication\/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget\/links\/5406e5e90cf23d9765a81dca.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Yutian_Chen3\/publication\/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget\/links\/5406e5e90cf23d9765a81dca.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"fe69a41baf3a48514c8b4308620ba1a0","showFileSizeNote":false,"fileSize":"1.08 MB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"5406e5e90cf23d9765a81dca","name":"Yutian Chen","date":"Sep 03, 2014 ","nameLink":"profile\/Yutian_Chen3","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Yutian_Chen3\/publication\/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget\/links\/5406e5e90cf23d9765a81dca.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Yutian_Chen3\/publication\/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget\/links\/5406e5e90cf23d9765a81dca.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"fe69a41baf3a48514c8b4308620ba1a0","showFileSizeNote":false,"fileSize":"1.08 MB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[{"props":{"position":"float","orientation":"portrait","coords":"pag:14:rect:149.34,297.97,308.88,9.76","ordinal":"8"},"assetId":"AS:299396106342400@1448393002277"},{"props":{"position":"float","orientation":"portrait","coords":"pag:15:rect:130.93,297.97,345.71,8.85","ordinal":"9"},"assetId":"AS:299396106342401@1448393002312"}],"figureAssetIds":["AS:299396106342400@1448393002277","AS:299396106342401@1448393002312"],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=3jKDHjeuS5Mfsn5ytxDpSo1Z3fnRptECabOkjAySsH_gZfkagV2QeBFRE9WrbOTiSH3FLP9OBXAUgVtGt6VYzA.bQYXIqkajUDGs68b4yXdRM9diTZl6qPD7SrBIqB8ocaKDtIz1h_CfRT4ASi1HxAMDoigRj7Gc47p_OCOFkCB0g","clickOnPill":"publication.PublicationFigures.html?_sg=-dTRoo6YFKz6bohss-u18cagIzNCiDKE04jvroqfvukjtAx5_iMDy2nt1GOaNrmVTAMEqLd1Tg7nGYBQcxqjMg.W_yUaVLYj6rPij2eRqu_vxmaHA784W7dTJZP_z821ps4nhiFWPEuvYvWZsAHfaYUMCGIXiQyuguX0R2dHi6BSQ"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1o9o3\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FYutian_Chen3%2Fpublication%2F236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget%2Flinks%2F5406e5e90cf23d9765a81dca.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1o9o3\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=CyGTxkJhQiQpk9eVcyUWAUcmRnz6nvoHRCeTCQB2r8qDh8E6rMvyhtduh9f_ihrvU623e6mdLtf6iwFT1zpO8g","urlHash":"3705cb3975fcafdb45bbd70b9071ccb0","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=iMDohlPibjE4VKL3je5HcrqAch0KmXdtJbNpM59N7h-mFE2BzaAvY2uTSW6x-xZV5N-60PlsH-w3j6YYT3PtPEpVVMNUqYlJ3i83wI-cmNU.KKVHtZklIdLiv5C9lfzlAEAijIs1WNtuHauKVbOlyahTIvJhhlTRHea_lpZGoHSHwKxhDN6MLPTx9SiWVkOhmw.i4p-iwK9onpqvgs5hraVjuMWCEhPIGg3n-I8pUrgFeF8XCVlX6fMO2qWJ1uzJ-yJLCo2BEdogwgzdnvk5fmB8g","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"5406e5e90cf23d9765a81dca","trackedDownloads":{"5406e5e90cf23d9765a81dca":{"v":false,"d":false}},"assetId":"AS:137266185904134@1409738217766","readerDocId":"5995207","assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":236235190,"commentCursorPromo":null,"widgetId":"rgw43_56ab1a186f7c6"},"id":"rgw43_56ab1a186f7c6","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FYutian_Chen3%2Fpublication%2F236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget%2Flinks%2F5406e5e90cf23d9765a81dca.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A137266185904134%401409738217766&publicationUid=236235190&linkId=5406e5e90cf23d9765a81dca&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=sNM62MDbUr3f8pdZpLTHY10s7rFSiFhT_-TwUn9z-ceHW489t1SaeMKV_Bdj3933LLbWuKvNetQqxgjFRIiJ2hoAW7xRuS3mx2NSvLMRn7Y.RTmN4Z2lJI26m7XAQvoZAk-v9OI0dtV0HmflVxVnN1aUE_lpr3bgzFGJR5sFFBBr42PpEk_HLZ8PCE7PKLt-MQ.qbrYpwkFbTBEkd2FsOrtVp8cZqS7n-uZTZ1wRk0GHnHj4p-_KAnNxJePnYBMcW-cfCxlemath7FrOOHqkpEdlQ","publicationUid":236235190,"trackedDownloads":{"5406e5e90cf23d9765a81dca":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw45_56ab1a186f7c6"},"id":"rgw45_56ab1a186f7c6","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw46_56ab1a186f7c6"},"id":"rgw46_56ab1a186f7c6","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw47_56ab1a186f7c6"},"id":"rgw47_56ab1a186f7c6","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw48_56ab1a186f7c6"},"id":"rgw48_56ab1a186f7c6","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw49_56ab1a186f7c6"},"id":"rgw49_56ab1a186f7c6","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw44_56ab1a186f7c6"},"id":"rgw44_56ab1a186f7c6","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw42_56ab1a186f7c6"},"id":"rgw42_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1a186f7c6"},"id":"rgw2_56ab1a186f7c6","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":236235190},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=236235190&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1a186f7c6"},"id":"rgw1_56ab1a186f7c6","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"CASto3giJOsgZkmk+4mYwQPhLHO5hiwD6wbgGCMhh2TpChaIZCHU6+XkdoLjQgft6W3y9w0XjZnefQkL3cakoykFPBneztwxJlYz42ECIz034XSEUcZz5\/nQIm\/U\/mFPlExHHc7LdIpAIyEoY53mZNSA+1pKPhr63Z+ax7GJjrmh5gyvDqrccSBBl5520uNr0eZgrItA2Gkyvs\/p3Z8Lw7BghhwXkGaC\/jiZUpjSPEfKhD41R4XDh4wpM9DhpZBkn6VLlCVMeIqHIYHzONlsJS0jarq3RlTUmRwV2uLLtRY=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget\" \/>\n<meta property=\"og:description\" content=\"Can we make Bayesian posterior MCMC sampling more efficient when faced with\nvery large datasets? We argue that computing the likelihood for N datapoints\ntwice in order to reach a single binary...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget\/links\/5406e5e90cf23d9765a81dca\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget\" \/>\n<meta property=\"rg:id\" content=\"PB:236235190\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget\" \/>\n<meta name=\"citation_author\" content=\"Anoop Korattikara\" \/>\n<meta name=\"citation_author\" content=\"Yutian Chen\" \/>\n<meta name=\"citation_author\" content=\"Max Welling\" \/>\n<meta name=\"citation_publication_date\" content=\"2013\/04\/18\" \/>\n<meta name=\"citation_volume\" content=\"1\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Yutian_Chen3\/publication\/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget\/links\/5406e5e90cf23d9765a81dca.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/215868066921738\/styles\/pow\/publicliterature\/FigureList.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-6f59a5cf-5383-442e-ad26-f031d182c877","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":792,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw50_56ab1a186f7c6"},"id":"rgw50_56ab1a186f7c6","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-6f59a5cf-5383-442e-ad26-f031d182c877", "dfd1f2ebb4014283bb95436a7132a6bcad0bdac8");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-6f59a5cf-5383-442e-ad26-f031d182c877", "dfd1f2ebb4014283bb95436a7132a6bcad0bdac8");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw51_56ab1a186f7c6"},"id":"rgw51_56ab1a186f7c6","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/236235190_Austerity_in_MCMC_Land_Cutting_the_Metropolis-Hastings_Budget","requestToken":"Ow7kFdjbESs4mAgTq+asYUKw16jt1MXBYAw6MwhOLCbJKqTDYWGu81MEw8twhkMIzNvk6aBrSMk09mnjuQNEBHthCzdIZ6cRfdJ1z5eZ4ZlwYS8LWrOfv53SZmvwz+EaDxZGTVzAED3KmqRisFMeqjrPZEyO7FLzM5DYADPZR+u7PtM5k0AUVer7yfBMPjm3k\/7TWOEe\/cOMe1\/XBdAkzYb7yltpfmYdzPWE6SrS3Vd5Bs4gtuhzhX3GaDWzrrvIsnrIZxXoD32YiP6UHx2gxgd\/L6Fwxqf5KpvxZmCglQ0=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=tgH5VL5IexZZplCXRkRcEj__uBSwlBgHza1WiytYNJV97N3e_LTRhyMau42n3wEP","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjM2MjM1MTkwX0F1c3Rlcml0eV9pbl9NQ01DX0xhbmRfQ3V0dGluZ190aGVfTWV0cm9wb2xpcy1IYXN0aW5nc19CdWRnZXQ%3D","signupCallToAction":"Join for free","widgetId":"rgw53_56ab1a186f7c6"},"id":"rgw53_56ab1a186f7c6","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw52_56ab1a186f7c6"},"id":"rgw52_56ab1a186f7c6","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw54_56ab1a186f7c6"},"id":"rgw54_56ab1a186f7c6","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
