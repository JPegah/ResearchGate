<!DOCTYPE html> <html lang="en" class="" id="rgw40_56ab1dba860c9"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="IrcAq12lhkAJHMf72j/lo0mbR9/2IjfimlIwkIwOVnW9FLe1dZrUHvbChJ5A60aaat3LiXeSHiKFipHs8FAlXKgxx0KQXbio4kSiceVUITiFGP+O9bbsI7ChSHad6rqgMyISu3Wnlzdl6WIfZWyzo+dcMn/t6KXUIYWudJ47bY59Q+GfI4DaumhJjhzsVXOzpDQcDlH3in49Wa1yuL/CjBHj4yoov2bB1JZPF6aSJE9ZBS/Rb2F3/9GRRtG3nZuXroDS/KpewM5ZwjpV1s0i79KKZ0sduCx/1IPBb3vTr3A="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-a1f4fd35-7cea-44ea-938e-a8513f258164",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/41781429_Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Assessing Approximate Inference for Binary Gaussian Process Classification" />
<meta property="og:description" content="Gaussian process priors can be used to define flexible, probabilistic classification models. Unfortunately exact Bayesian inference is analytically intractable and various approximation techniques..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/41781429_Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification/links/0e608558f0c46d4f0acd3c92/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/41781429_Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification" />
<meta property="rg:id" content="PB:41781429" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Assessing Approximate Inference for Binary Gaussian Process Classification" />
<meta name="citation_author" content="M. Kuss" />
<meta name="citation_author" content="C.E. Rasmussen" />
<meta name="citation_publication_date" content="2005/10/01" />
<meta name="citation_journal_title" content="Journal of Machine Learning Research" />
<meta name="citation_issn" content="1533-7928" />
<meta name="citation_volume" content="6" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/41781429_Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/41781429_Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Assessing Approximate Inference for Binary Gaussian Process Classification</title>
<meta name="description" content="Assessing Approximate Inference for Binary Gaussian Process Classification on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1dba860c9" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1dba860c9" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw7_56ab1dba860c9">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Assessing%20Approximate%20Inference%20for%20Binary%20Gaussian%20Process%20Classification&rft.title=Journal%20of%20Machine%20Learning%20Research%2C%20v.6%2C%201679-1704%20(2005)&rft.jtitle=Journal%20of%20Machine%20Learning%20Research%2C%20v.6%2C%201679-1704%20(2005)&rft.volume=6&rft.date=2005&rft.issn=1533-7928&rft.au=M.%20Kuss%2CC.E.%20Rasmussen&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Assessing Approximate Inference for Binary Gaussian Process Classification</h1> <meta itemprop="headline" content="Assessing Approximate Inference for Binary Gaussian Process Classification">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/41781429_Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification/links/0e608558f0c46d4f0acd3c92/smallpreview.png">  <div id="rgw10_56ab1dba860c9" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw11_56ab1dba860c9"> <a href="researcher/34887298_M_Kuss" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="M. Kuss" alt="M. Kuss" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">M. Kuss</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56ab1dba860c9">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/34887298_M_Kuss"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="M. Kuss" alt="M. Kuss" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/34887298_M_Kuss" class="display-name">M. Kuss</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw13_56ab1dba860c9"> <a href="researcher/43277170_CE_Rasmussen" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="C.E. Rasmussen" alt="C.E. Rasmussen" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">C.E. Rasmussen</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw14_56ab1dba860c9">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/43277170_CE_Rasmussen"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="C.E. Rasmussen" alt="C.E. Rasmussen" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/43277170_CE_Rasmussen" class="display-name">C.E. Rasmussen</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/1533-7928_Journal_of_Machine_Learning_Research"><span itemprop="name">Journal of Machine Learning Research</span></a> </span>    (Impact Factor: 2.47).     <meta itemprop="datePublished" content="2005-10">  10/2005;  6.             <div class="pub-source"> Source: <a href="http://edoc.mpg.de/270115" rel="nofollow">OAI</a> </div>  </div> <div id="rgw15_56ab1dba860c9" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>Gaussian process priors can be used to define flexible, probabilistic classification models. Unfortunately exact Bayesian inference is analytically intractable and various approximation techniques have been proposed. In this work we review and compare Laplace&lsquo;s method and Expectation Propagation for approximate Bayesian inference in the binary Gaussian process classification model. We present a comprehensive comparison of the approximations, their predictive performance and marginal likelihood estimates to results obtained by MCMC sampling. We explain theoretically and corroborate empirically the advantages of Expectation Propagation compared to Laplace&lsquo;s method.</div> </p>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw28_56ab1dba860c9">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw27_56ab1dba860c9"  itemprop="articleBody">  <p>Page 1</p> <p>Journal of Machine Learning Research 6 (2005) 1679–1704Submitted 8/05; Published 10/05<br />Assessing Approximate Inference for<br />Binary Gaussian Process Classification<br />Malte Kuss<br />Carl Edward Rasmussen<br />Max Planck Institute for Biological Cybernetics<br />Spemannstraße 38<br />72076 T¨ ubingen, Germany<br />KUSS@TUEBINGEN.MPG.DE<br />CARL@TUEBINGEN.MPG.DE<br />Editor: Ralf Herbrich<br />Abstract<br />Gaussian process priors can be used to define flexible, probabilistic classification models. Unfor-<br />tunately exact Bayesian inference is analytically intractable and various approximation techniques<br />have been proposed. In this work we review and compare Laplace’s method and Expectation Prop-<br />agation for approximate Bayesian inference in the binary Gaussian process classification model.<br />We present a comprehensive comparison of the approximations, their predictive performance and<br />marginal likelihood estimates to results obtained by MCMC sampling. We explain theoretically and<br />corroborate empirically the advantages of Expectation Propagation compared to Laplace’s method.<br />Keywords: Gaussian process priors, probabilistic classification, Laplace’s approximation, expec-<br />tation propagation, marginal likelihood, evidence, MCMC<br />1. Introduction<br />In recent years models based on Gaussian process (GP) priors have attracted much attention in the<br />machine learning community. Whereas inference in the GP regression model with Gaussian noise<br />can be done analytically, probabilistic classification using GPs is analytically intractable, see Ras-<br />mussen and Williams (2006) for a general overview. Several approaches to approximate Bayesian<br />inference have been suggested, including Laplace’s method, Expectation Propagation (EP), varia-<br />tional approximations and Markov chain Monte Carlo (MCMC) sampling, some of these in con-<br />junction with generalisation bounds, online learning schemes and sparse approximations (e.g. Neal,<br />1998; Williams and Barber, 1998; Gibbs and MacKay, 2000; Opper and Winther, 2000; Csat´ o and<br />Opper, 2002; Seeger, 2002; Lawrence et al., 2003).<br />Despite the abundance of recent work on probabilistic GP classifiers, most experimental studies<br />provide only anecdotal evidence, and no clear picture has yet emerged, as to when and why which<br />algorithm should be preferred. Thus, from a practitioners point of view it is unclear what the method<br />of choice is for probabilistic GP classification. In this work, we set out to understand and compare<br />two of the most wide-spread approximations: Laplace’s method and Expectation Propagation (EP).<br />We also compare to a sophisticated, but computationally demanding MCMC scheme, which be-<br />comes exact in the limit of long running times. We do not address issues of sparsification but stick<br />to comparing the two types of approximation.<br />We examine two aspects of the approximation schemes: Firstly the accuracy of approximations<br />tothemarginallikelihoodwhichisofcentralimportanceformodelselectionandmodelcomparison.<br />c ?2005 Malte Kuss and Carl Edward Rasmussen.</p>  <p>Page 2</p> <p>KUSS AND RASMUSSEN<br />In any practical application of GPs in classification (usually multiple) parameters of the covariance<br />function (hyper-parameters) have to be handled. Bayesian model selection provides a consistent<br />framework for setting such parameters. Therefore, it is essential to evaluate the accuracy of the<br />marginal likelihood approximations as a function of the hyper-parameters, in order to assess the<br />practical usefulness of the approach. The related question of whether the marginal likelihood cor-<br />relates well with the generalisation performance cannot be answered in general but depends on the<br />appropriateness of the model for a given data set. However, we do assess this empirically for two<br />data sets.<br />Secondly, we need to assess the quality of the approximate probabilistic predictions. In the<br />past, the probabilistic nature of the GP predictions has not received much attention, the focus being<br />mostly on classification error rates. This unfortunate state of affairs is caused primarily by typical<br />benchmarking problems being considered outside of a realistic context. The ability of a classifier<br />to produce class probabilities or confidences, have obvious relevance in most areas of application,<br />e.g. medical diagnosis and ROC analysis. We evaluate the predictive distributions of the approxi-<br />mate methods, and compare to the MCMC gold standard.<br />2. The Gaussian Process Model for Binary Classification<br />In this section we describe the Gaussian process model for binary classification (GPC). Let y ∈<br />{−1,1} denote the class label corresponding to an input x. The GPC model is discriminative in the<br />sense that it models p(y|x) which for fixed x is a Bernoulli distribution. The probability of success<br />p(y=1|x) is related to an unconstrained latent function f(x) which is mapped to the unit interval<br />by a sigmoidal transformation, e.g. the logit or the probit. Both mappings are relatively similar<br />around zero but show different tail behaviour. We will not examine the difference in this study.<br />For reasons of analytic convenience (for the EP algorithm) we exclusively use the probit model<br />p(y=1|x) = Φ(f(x)), where Φ denotes the cumulative density function of the standard normal<br />distribution.<br />In the GPC model Bayesian inference is performed about the latent function f in the light of<br />observed data D = {(yi,xi)|i=1,...,m}. Let fi= f(xi) and f = [f1,..., fm]?be shorthand for the<br />values of the latent function and y = [y1,...,ym]?and X = [x1,...,xm]?collect the class labels and<br />inputs respectively.<br />Given the latent function, the class labels are independent Bernoulli variables, so the joint like-<br />lihood factorises:<br />∏<br />i=1<br />p(y|f) =<br />m<br />p(yi|fi)<br />(1)<br />and depends on f only through its value at the corresponding observed inputs. For the probit model<br />the individual likelihood terms become p(yi|fi) = Φ(yifi), due to the symmetry of Φ.<br />As prior over functions f we use a zero-mean Gaussian process (GP) prior (O’Hagan, 1978).<br />A GP is a stochastic process where each input x has an associated random variable f(x). The<br />joint distribution of function values corresponding to any set of inputs X is multivariate Gaussian<br />p(f|X,θ) = N (f|0,K). The covariance matrix is defined element-wise, Kij= k(xi,xj,θ) where k<br />is a positive definite covariance function parameterised by θ. Note that by choosing a covariance<br />function we introduce hyper-parameters θ to the prior. The zero-mean GP prior encodes that a<br />priori p(y=1|x) = 1/2 and certain further beliefs about the characteristics of the latent function.<br />1680</p>  <p>Page 3</p> <p>ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION<br />Fordetailsoncovariancefunctionsandtheirimplicationsontheprioroverfunctionsseeforexample<br />Abrahamsen (1997) or Rasmussen and Williams (2006, ch. 4).<br />Using Bayes’ rule the posterior distribution over the latent function values f for given hyper-<br />parameters θ becomes:<br />p(f|D,θ) =p(y|f)p(f|X,θ)<br />p(D|θ)<br />=N (f|0,K)<br />p(D|θ)<br />m<br />∏<br />i=1<br />Φ(yifi)<br />(2)<br />which is non-Gaussian. Properties of the posterior will be described in Section 5.<br />The main purpose of classification models is to predict the class label y∗for test inputs x∗. The<br />distribution of the latent function value can be computed by marginalisation:<br />p(f∗|D,θ,x∗) =<br />Z<br />p(f∗|f,X,θ,x∗)p(f|D,θ)df,<br />(3)<br />and by computing the expectation:<br />p(y∗|D,θ,x∗) =<br />Z<br />p(y∗|f∗)p(f∗|D,θ,x∗)d f∗<br />(4)<br />the predictive distribution is obtained, which is again a Bernoulli distribution. The first term in the<br />right hand side of equation (3) is Gaussian and obtained by conditioning the joint Gaussian prior<br />distribution.<br />Unfortunately, neither the posterior eq. (2) p(f|D,θ), the predictive distribution eq. (4) p(y∗=<br />1|D,θ,x∗) nor the marginal likelihood eq. (7) p(D|θ) can be computed analytically, so approxima-<br />tions are needed. For the GPC model approximations are either based on a Gaussian approximation<br />q(f|D,θ) = N (f|m,A) to the posterior p(f|D,θ) or involve Markov chain Monte Carlo (MCMC)<br />sampling.<br />A key insight is that a Gaussian approximation to the posterior implies a GP approximation to<br />the posterior process, which gives rise to an approximate predictive distribution for test cases. Intro-<br />ducingtheapproximateGaussianposteriorintoeq.(3)givestheapproximateposteriorq(f∗|D,θ,x∗)=<br />N (f∗|µ∗,σ2<br />µ∗<br />σ2<br />∗<br />where k∗= [k(x1,x∗),...,k(xm,x∗)]?is a vector of prior covariances between x∗and the training<br />inputs X. For the probit likelihood the approximate predictive probability (4) of x∗belonging to<br />class 1 can be computed analytically:<br />∗), with mean and variance:<br />=<br />k?<br />k(x∗,x∗)−k?<br />∗K−1m<br />(5a)<br />(5b)<br />=<br />∗(K−1−K−1AK−1)k∗,<br />q(y∗=1|D,θ,x∗) =<br />Z<br />Φ(f∗)N (f∗|µ∗,σ2<br />∗)d f∗= Φ<br />?<br />µ∗<br />1+σ2<br />√<br />∗<br />?<br />.<br />(6)<br />The parameters m and A of the posterior approximation can be found using Laplace’s method<br />(Section 3) or by Expectation Propagation (Section 4).<br />We have introduced the hyper-parameters θ which we considered to be fixed. Typically very<br />little information about these parameters is available a priori. In principle inference should be done<br />jointly over f and θ which can only be approximated using Markov chain Monte Carlo sampling.<br />1681</p>  <p>Page 4</p> <p>KUSS AND RASMUSSEN<br />However, a model selection approach can be implemented by selecting θ maximising the marginal<br />likelihood (evidence):<br />p(D|θ) =<br />which can be understood as a measure of the agreement between the model and observed data<br />(Kass and Raftery, 1995; MacKay, 1999). This approach is called maximum likelihood II (ML-<br />II) type hyper-parameter estimation and motivates the need for computing the marginal likelihood.<br />Laplace’s method as well as Expectation Propagation provide an approximation to the marginal<br />likelihood (7) and so approximate ML-II hyper-parameter estimation can be implemented in both<br />approximation schemes.<br />Z<br />p(y|f)p(f|X,θ)df<br />(7)<br />3. Laplace’s Method<br />WilliamsandBarber(1998)describeLaplace’smethodtofindaGaussianN (f|m,A)approximation<br />to the posterior over latent function values (2) for fixed θ (although they use the logit likelihood).<br />Let lnL(f) = lnp(y|f) denote the log likelihood and:<br />lnQ(f|D,θ) = lnL(f)−1<br />2ln|K|−1<br />2f?K−1f−m<br />2ln(2π)<br />(8)<br />the unnormalised log posterior. Laplace’s approximation is found by a second order Taylor expan-<br />sion:<br />lnQ(f|D,θ) ? lnQ(m)−1<br />around the mode of the (log) posterior:<br />2(m−f)?A−1(m−f)<br />(9)<br />m = argmax<br />f∈Rm<br />lnQ(f|D,θ).<br />(10)<br />Since both the likelihood and the prior are log-concave the posterior is also log-concave and uni-<br />modal. Let:<br />∇flnQ<br />∇∇flnQ<br />=<br />∇flnL(f)−K−1f<br />∇∇flnL(f)−K−1<br />(11a)<br />(11b)<br />=<br />denote the gradient and the Hessian. The mode is conveniently found using Newton’s method,<br />iterating:<br />f ← f−(∇∇flnQ(f))−1∇flnQ(f),<br />which usually converges rapidly to m. The covariance matrix:<br />(12)<br />A = −?∇∇flnQ(m)?−1= (K−1+W)−1<br />(13)<br />is approximated by the curvature at the mode, equal to the negative inverse Hessian, where W =<br />−∇∇flnL.<br />This approximation also facilitates an approximation to the marginal likelihood:<br />p(D|θ) =<br />Z<br />p(y|f)p(f|X,θ)df =<br />Z<br />exp(lnQ(f))df.<br />(14)<br />1682</p>  <p>Page 5</p> <p>ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION<br />Algorithm 1 Laplace’s approximation for GPC<br />Given: θ, D, x∗<br />Initialise f (e.g. f ← 0), compute K from θ and X<br />repeat<br />f ← f−(∇∇flnQ(f))−1∇flnQ(f)<br />until convergence of f<br />m ← f<br />A ← (K−1−∇∇flnQ(m))−1<br />Compute log marginal likelihood lnq(D|θ) by (15), and predictions q(y∗=1|D,θ,x∗) using (6).<br />Substituting lnQ by its Taylor approximation (9) the Gaussian integral can be solved. The resulting<br />approximate log marginal likelihood is:<br />lnp(D|θ) ? lnq(D|θ) = lnQ(m)+m<br />2ln(2π)+1<br />2ln|A|<br />(15)<br />and the derivative of this quantity w.r.t. θ can be derived and used for optimisation (e.g. using conju-<br />gate gradient methods) in an ML-II type setting. See Algorithm 1 for an overview and Appendix A<br />for details about our implementation.<br />4. Expectation Propagation<br />Minka (2001) proposed the iterative Expectation Propagation (EP) algorithm which can by applied<br />to GPC. EP finds a Gaussian approximation q(f|D,θ) = N (f|m,A) to the posterior p(f|D,θ) by<br />moment matching of approximate marginal distributions. The starting point is an approximation<br />mimicking the factorising structure:<br />p(f|D,θ) =<br />p(f|X,θ)<br />p(D|θ)<br />m<br />∏<br />i=1<br />p(yi|fi) ?<br />p(f|X,θ)<br />q(D|θ)<br />m<br />∏<br />i=1<br />t(fi,µi,σ2<br />i,Zi) = q(f|D,θ),<br />(16)<br />where throughout we use p to denote exact quantities and q approximations, and the terms:<br />t(fi,µi,σ2<br />i,Zi) = ZiN (fi|µi,σ2<br />i)<br />(17)<br />are called site functions. Note that the site functions are approximating the likelihood (which nor-<br />malizes over observations yi), with a Gaussian in fi, so we cannot expect the site functions to<br />normalize, hence the explicit term Ziis necessary. For notational convenience we hide the site pa-<br />rameters µi, σ2<br />and covariance:<br />iand Ziand write t(fi) instead. From (17) the Gaussian approximation (16) has mean<br />q(f|D,θ) = N (f|m,A), where m = AΣ−1µ, and A = (K−1+Σ−1)−1,<br />where µ = (µ1,...,µm)?and Σ = diag(σ2<br />rithm iteratively visits each site function in turn, and adjusts the site parameters to match moments<br />of an approximation to the posterior marginals. The kth moment of fiunder the posterior is:<br />(18)<br />1,...,σ2<br />m) collect site function parameters. The EP algo-<br />?fk<br />i? =<br />1<br />p(D|θ)<br />Z<br />fk<br />ip(y|f)p(f|X,θ)df =<br />1<br />p(D|θ)<br />Z<br />fk<br />ip(yi|fi)p\i(fi)d fi<br />(19)<br />1683</p>  <p>Page 6</p> <p>KUSS AND RASMUSSEN<br />where:<br />p\i(fi) =<br />Z<br />∏<br />j?=i<br />p(yj|fj)p(f|X,θ)df\i<br />(20)<br />is called the cavity distribution and f\idenotes f without fi. The marginalisation required to compute<br />the exact cavity distribution is intractable for the GPC model. The key step in the EP algorithm is<br />to replace the intractable exact cavity distribution with a tractable approximation based on the site<br />functions:<br />q\i(fi) =<br />∏<br />j?=i<br />TheapproximatecavityfunctioncomesintheformofanunnormalisedGaussianq\i(fi)∝N (fi|µ\i,σ2<br />Multiplying both sides by t(fi):<br />Z<br />t(fj)p(f|X,θ)df\i.<br />(21)<br />\i).<br />q\i(fi)t(fi) =<br />Z<br />N (f|0,K)<br />m<br />∏<br />j=1<br />t(fj)df\i∝ N (fi|mi,Aii),<br />(22)<br />and basic Gaussian identities give the parameters:<br />σ2<br />\i=?(Aii)−1−σ−2<br />i<br />?−1<br />and<br />µ\i= σ2<br />\i<br />?mi<br />Aii−µi<br />σ2<br />i<br />?<br />,<br />(23)<br />of the approximate cavity function.<br />The core idea of EP is to adjust the site parameters µi, σiand Ziso that the approximate posterior<br />marginal using the exact likelihood approximates as well as possible the posterior marginal based<br />on the site function:<br />q\i(fi)p(yi|fi) ? q\i(fi)t(fi,µi,σ2<br />by matching the zeroth, first and second moments. Recall that matching of moments minimizes<br />Kullback-Leibler (KL) divergence.1For the probit likelihood p(yi|fi) = Φ(yifi) the k = 0,1,2 mo-<br />ments of the left hand side can be computed analytically<br />yµ\i<br />√<br />1+σ2<br />\i<br />m1<br />=<br />µ\i+<br />Φ(z)y<br />i,Zi)<br />(24)<br />m0<br />=<br />Φ<br />?<br />?<br />= Φ(z),<br />(25a)<br />σ2<br />\iN (z|0,1)<br />√<br />1+σ2<br />\i<br />,<br />(25b)<br />m2<br />=<br />2µ\im1−µ2<br />\i+σ2<br />\i−zσ4<br />\iN (z|0,1)<br />Φ(z)(1+σ2<br />\i),<br />(25c)<br />where z = yµ\i/<br />update equations for the site parameters become<br />√<br />1+σ2<br />\i. By equating these moments with those of the right hand side of (24) the<br />σ2<br />i<br />=<br />?(m2−m2<br />σ2<br />i<br />m1(σ−2<br />1)−1−σ−2<br />\i+σ−2<br />\i<br />?−1,<br />σ2<br />?<br />(26a)<br />µi<br />=<br />?<br />?<br />i)−µ\i<br />\i<br />?<br />(µi−µ\i)2<br />2(σ2<br />,<br />(26b)<br />Zi<br />=<br />m0<br />2π(σ2<br />\i+σ2<br />i)exp<br />\i+σ2<br />i)<br />?<br />.<br />(26c)<br />1. Although, the classical KL argument only applies to the first and second (and higher) moments for normalized<br />distributions, it seems natural also to match zeroth moment.<br />1684</p>  <p>Page 7</p> <p>ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION<br />Algorithm 2 EP for Gaussian process classification<br />Given: θ, D, x∗<br />Initialise: A ← K and site parameters σ2<br />repeat<br />for i=1,...,m do<br />Compute parameters (23) of cavity<br />Compute moments (25)<br />Update the site parameters using (26)<br />Update m and A according to (18)<br />end for<br />until The site parameters converged<br />Compute log marginal likelihood lnq(D|θ) by (27), and predictions q(y∗=1|D,θ,x∗) using (6).<br />iand µi<br />In the application of EP, one may generally not have a guarantee that the new site variance in (26a)<br />is non-negative; however, in the GPC model with probit likelihood, one can show that variance is<br />always positive. Once we have new values for µiand σ2<br />(18), which in practise is done using rank-one updates, to save computation.<br />The EP algorithm iteratively updates the site parameters as shown in Algorithm 2. Although<br />we cannot prove the convergence of EP, we conjecture that it always converges for GPC with probit<br />likelihood, and have never encountered an exception.<br />Finally the approximate log marginal likelihood can be obtained from the normalization of (16),<br />giving<br />iwe have to update m and A according to<br />lnp(D|θ) ? lnq(D|θ) = ln<br />Z<br />q(f|X,θ)<br />lnZi−1<br />m<br />∏<br />i=1<br />t(fi)df<br />(27)<br />=<br />n<br />∑<br />i=1<br />2ln|K+Σ|−1<br />2µ?(K+Σ)−1µ−m<br />2ln(2π).<br />Perhaps this is not the standard way to compute an approximation to the marginal likelihood used<br />elsewhere, butitseemsthemostnaturalgiventheapproximation. Thederivativesofthelogmarginal<br />likelihood can be computed in order to implement ML-II parameter estimation of θ. Algorithm 2<br />summarises the computations, more details on implementing EP for GPC can be found in Ap-<br />pendix B.<br />5. Structural Properties of the Posterior<br />In the previous sections we described the GPC model and two alternative approximation schemes<br />for finding a Gaussian approximation to the posterior. This section provides more details on the<br />properties of the posterior which is compared to the structure of the respective approximations.<br />Figure 1(a) provides a one-dimensional illustration. The prior N (f|0,52) combined with the<br />probit likelihood (y = 1) results in a skewed posterior. Intuitively, the likelihood cuts off the f<br />values which have the opposite sign of y. The mode of the posterior remains relatively close to the<br />origin, while the mass is placed over positive values in accordance with the observation. Laplace’s<br />approximation peaks at the posterior mode, but places far too much mass over negative values of<br />f and too little over large positive values. The EP approximation attempts to match the first two<br />1685</p>  <p>Page 8</p> <p>KUSS AND RASMUSSEN<br />−4 −4004488<br />0<br />0.02<br />0.04<br />0.06<br />0.08<br />0.1<br />0.12<br />0.14<br />0.16<br />p(f|y)<br />0<br />0.2<br />0.4<br />0.6<br />0.8<br />1<br />f<br />p(y|f)<br />Likelihood p(y|f)<br />Prior p(f)<br />Posterior p(f|y)<br />Laplace q(f|y)<br />EP q(f|y)<br />??<br />??<br />?<br />?<br />(a)(b)<br />Figure 1: Panel(a)providesaone-dimensionalillustrationofapproximations. ThepriorN (f|0,52)<br />combined with the probit likelihood (y = 1) results in a skewed posterior. The likelihood<br />uses the right axis, all other curves use the left axis. In Panel (b) we caricature a high<br />dimensional zero-mean Gaussian prior as an ellipse. The gray shadow indicates that for<br />a high dimension Gaussian most of the mass lies in a thin shell. For large latent signals,<br />the likelihood essentially cuts off regions which are incompatible with the training labels<br />(hatched area), leaving the upper right orthant as the posterior. The dot represents the<br />mode of the posterior, which is relatively unaffected by the truncation and remains close<br />to the origin.<br />posterior moments, which results in a larger mean and a more accurate placement of probability<br />mass compared to Laplace’s approximation.<br />Structural properties of the posterior in higher dimensions can best be understood by examining<br />its construction. The prior is a correlated m-dimensional Gaussian N (f|0,K) centred at the origin.<br />Each likelihood term p(yi|fi) softly truncates the half-space from the prior that is incompatible with<br />the observed label, see Figure 1(b). The resulting posterior is unimodal and skewed, similar to a<br />multivariate Gaussian truncated to the orthant containing y. The mode of the posterior remains<br />close to the origin, while the mass is placed in accordance with the observed class labels. Addi-<br />tionally, high dimensional Gaussian distributions exhibit the property that most probability mass is<br />contained in a thin ellipsoidal shell—depending on the covariance structure—away from the mean<br />(MacKay, 2003, ch. 29.2). Intuitively this occurs since in high dimensions the volume grows ex-<br />tremely rapidly with the radius. As an effect the mode becomes less representative (typical) for the<br />prior distribution as the dimension increases. For the GPC posterior this property persists: the mode<br />of the posterior distribution stays relatively close to the origin, still being unrepresentative for the<br />posterior distribution, while the mean moves to the mass of the posterior making mean and mode<br />differ significantly.<br />As described, we cannot generally assume the posterior to be close to Gaussian, as in the often<br />studied limit of low-dimensional parametric models with large amounts of data. Therefore in GPC<br />we must be aware of making a Gaussian approximation to a non-Gaussian posterior. Laplace’s ap-<br />proximation is centred around the mode of the posterior, which lies in the right orthant but too close<br />1686</p>  <p>Page 9</p> <p>ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION<br />0<br />1<br />2<br />x1<br />30<br />0.5<br />1<br />1.5<br />2<br />2.5<br />3<br />0<br />x2<br />p(x1,x2)<br />0123<br />x1<br />p(x1)<br />0246<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />0.35<br />0.4<br />0.45<br />xi<br />p(xi)<br />(a)(b)(c)<br />Figure 2: Panel (a) illustrates a bivariate normal distribution truncated to the positive quadrant. The<br />lines describe slices through the probability density function for fixed x2-values. Panel (b)<br />shows the marginal distribution of p(x1) (thick line) obtained by (numerical) integration<br />over x2, which—intuitively speaking—corresponds to an averaging of the slices (thin<br />lines) from Panel (a). Panel (c) shows a histogram of samples of a marginal distribution<br />of an high-dimensional truncated Gaussian. The line describes a Gaussian with mean and<br />variance estimated from the samples.<br />to the origin, such that the approximation will overlap with regions having practically zero posterior<br />mass. As an effect the amplitude of the approximate latent posterior GP will be underestimated<br />systematically, leading to overly cautious predictive distributions.<br />The EP approximation does not rely on a local expansion, but assumes that the marginal distri-<br />butions of the posterior can be well approximated by Gaussians. As described above the posterior<br />is similar to a high dimensional multivariate normal distribution truncated to one orthant. Although<br />the posterior is skew and truncated, marginals of such a distribution can be relatively similar to a<br />Gaussian.<br />As a low dimensional illustration the marginal distribution of a bivariate normal is shown in<br />Figure 2(a-b). Depending on the covariance structure, the mode of the marginal distribution moves<br />away from the origin and the distribution appear similar to a truncated univariate Gaussian.<br />In order to inspect the marginals of a truncated high-dimensional multivariate normal distri-<br />bution we made an additional synthetic experiment. We constructed a 767 dimensional Gaussian<br />N (x|0,C) with a covariance matrix having one eigenvalue of 100 with eigenvector 1, and all other<br />eigenvalues are 1. We then truncate this distribution such that all xi≥ 0. Note that the mode<br />of the truncated Gaussian is still at zero, whereas the mean moved towards the remaining mass.<br />Metropolis-Hastings sampling was used to generate samples from this truncated multivariate distri-<br />bution. Figure 2(c) shows a normalised histogram of samples from a marginal distribution of one<br />xi. The samples agree very well with a Gaussian approximation. Note that Laplace’s method would<br />be completely inappropriate for approximating a truncated multivariate normal distribution.<br />In order to validate the above arguments we will use Markov chain Monte Carlo methods to<br />generate samples from the posterior and also to estimate the marginal likelihood.<br />1687</p>  <p>Page 10</p> <p>KUSS AND RASMUSSEN<br />6. Markov Chain Monte Carlo<br />Markov chain Monte Carlo (MCMC) may be too slow for many practical applications, but has the<br />advantage that it becomes exact in the limit of long runs. Thus, MCMC can provide a gold standard<br />by which to measure the two analytic methods of the previous sections. Computing the predictions<br />via an MCMC estimate of (3) and (4) is relatively straight forward and covered in Section 6.1.<br />Good MCMC estimates of the marginal likelihood are, however, notoriously difficult to obtain,<br />being equivalent to the free-energy estimation problem in physics (Gelman and Meng, 1998). In<br />Section 6.2 we explain the use of Annealed Importance Sampling (AIS), which can be seen as a<br />sophisticated elaboration of Thermodynamic Integration, for this task.<br />6.1 Hybrid MCMC Sampling<br />Hybrid Monte Carlo (HMC) sampling as proposed by Duane et al. (1987) is a computationally<br />efficient sampling technique which exploits gradient information of the target distribution. Detailed<br />accounts are given by Neal (1993, ch. 5.2) and Liu (2001, ch. 9). MacKay (2003, ch. 30) also<br />provides pseudo-code; we do not repeat the details here.<br />HMCcanbeusedtogeneratesamplesfromtheposterior p(f|θ,D), whileonlytheunnormalised<br />log posterior (8) and its derivatives are required. As described in the previous section, the exact<br />posterior (2) takes the form of a (correlated) Gaussian (the GP prior), which is (softly) truncated by<br />the constraints imposed by the training labels through the likelihood. To ease the sampling task by<br />reducing correlations, we first do a linear transformation into new g = L−1f variables, such that g is<br />white w.r.t. K, where K = LL?is the Cholesky decomposition. Given samples from the posterior,<br />we generate test-latents from the Gaussian p(f∗|f,X,θ,x∗) for use in a simple Monte Carlo estimate<br />of (4).<br />6.2 Annealed Importance Sampling<br />The marginal likelihood (7) comes in the form of an m dimensional integral where m is the number<br />of data points. A simple approach would be to use importance sampling with the EP or Laplace’s<br />approximation of the posterior as proposal distribution. However, for the GPC model the resulting<br />importance weights show enormous variances, making simple importance sampling useless for this<br />task (MacKay, 2003, ch. 29).<br />Neal (2001) describes Annealed Importance Sampling (AIS), which we will use to estimate the<br />marginal likelihood in the GPC model. Instead of solving the integral (7) directly, a sequence of<br />easier quantities is computed. We define:<br />Zt =<br />Z<br />p(y|f)τ(t)p(f|X,θ)df<br />(28)<br />where τ(t) is an inverse temperature schedule such that τ(0) = 0 and τ(T) = 1. The trick is to<br />rewrite the marginal likelihood Z = p(D|θ) as a fraction and expand:<br />Z =ZT<br />Z0<br />=<br />ZT<br />ZT−1<br />ZT−1<br />ZT−2···Z1<br />Z0,<br />(29)<br />1688</p>  <p>Page 11</p> <p>ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION<br />Algorithm 3 Annealed Importance Sampling<br />Given: Temperature schedule τ<br />for r = 1,...,R do<br />Sample f0from the prior N (f|0,K)<br />for t = 1,...,T do<br />Sample ftfrom q(f|D,τ(t),θ) by HMC<br />Compute ln(Zt/Zt−1) using (31)<br />end for<br />Compute Zrusing (32)<br />end for<br />Return lnZ = ln?1<br />where Z0= 1 since the prior normalises. Each term in (29) is approximated using importance<br />sampling using samples from q(f|D,θ,τ(t)) ∝ p(y|f)τ(t)p(f|X,θ):<br />Zt<br />Zt−1<br />1<br />S<br />∑<br />i=1<br />where fiare samples from q(f|D,θ,τ(t)), which we generate using HMC. Using a single sample<br />S = 1 and a large number of temperatures, the log of each ratio is:<br />R∑R<br />r=1Zr<br />?<br />=<br />Z<br />p(y|f)τ(t)p(f|X,θ)<br />p(y|f)τ(t−1)p(f|X,θ)q(f|D,θ,τ(t −1))df<br />S<br />p(y|fi)τ(t)−τ(t−1)<br />(30a)<br />?<br />(30b)<br />ln(Zt/Zt−1) ?<br />?τ(t)−τ(t −1)?lnp(y|ft)<br />(31)<br />where ftis the only sample at temperature τ(t). Combining (29) with (31) we obtain the desired:<br />lnZ ?<br />T<br />∑<br />t=1<br />ln(Zt/Zt−1).<br />(32)<br />In all our experiments we use τ(t) = (t/T)4for t = 0,...,8000. Using this temperature schedule<br />we found that the sampling spends most of its efforts at temperatures with high variance of (31)<br />such that the variance of (32) is relatively small. Note that this was only examined on the data<br />sets we use below and only for certain values of θ. So far, we have described Thermodynamic<br />Integration, which gives an unbiased estimate in the limit of slow temperature changes. In AIS the<br />bias caused by finite temperature schedules is removed by combining multiple estimates by their<br />geometric mean (see Algorithm 3). In the experiments we combine the estimates of R = 3 runs of<br />Thermodynamic Integration.<br />7. Experiments<br />In this section we compare and inspect approximations for GPC using various benchmark data sets.<br />The primary focus is not to optimise the absolute performance of GPC models but to compare the<br />relative accuracy of approximations and to validate the arguments given in Section 5.<br />In all the GPC experiments we use a covariance function of the form:<br />k(x,x?,θ) = σ2exp?−1<br />2?2<br />??x−x???2?,<br />(33)<br />1689</p>  <p>Page 12</p> <p>KUSS AND RASMUSSEN<br />−8 −6−4 −2024<br />0<br />0.2<br />0.4<br />0.6<br />0.8<br />1<br />x<br />(a)<br />p(y = 1|x)<br />Class 1<br />Class −1<br />Laplace p(y|X)<br />EP p(y|X)<br />True p(y|X)<br />−8 −6 −4 −2024<br />−10<br />−5<br />0<br />5<br />10<br />15<br />x<br />f(x)<br />Laplace p(f|X)<br />EP p(f|X)<br />(b)<br />Figure 3: Synthetic classification problem: Panel (a) illustrates the classification task, the gen-<br />erating p(y|x) and two approximations thereof obtained by Laplace’s method and<br />EP. Panel (b) illustrates the approximate predictive distributions p(f∗|D,θ,x∗) ?<br />N (f∗|µ∗,σ2<br />∗) of latent function values showing the mean µ∗and the range of ±2σ∗.<br />such that θ = [σ,?]. We refer to σ2as the signal variance and to ? as the characteristic length-<br />scale. Note that for many classification tasks it may be reasonable to use an individual length scale<br />parameter for every input dimension (ARD). Nevertheless, for the sake of presentability we use the<br />above covariance function and we believe the conclusions to be independent of this choice.<br />Both analytic approximations have a computational complexity which is cubic O(m3) as com-<br />mon among non-sparse GP models due to inversions m×m matrices. In our implementations<br />Laplace’s method and EP need similar running times, on the order of a few minutes for several<br />hundred data-points. Making AIS work efficiently requires some fine-tuning and a single estimate<br />of p(D|θ) can take several hours for data sets of a few hundred examples, but this could conceivably<br />be improved upon.<br />7.1 Synthetic Classification Problem<br />The first experiment is a synthetic classification problem with scalar inputs. The observations for<br />class 1 were generated from two normal distributions with means −6 and 2, each with a standard<br />deviation of 0.8. For class −1 the mean is 0 and the same standard deviation was used.<br />1690</p>  <p>Page 13</p> <p>ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION<br />We computed Laplace’s and the EP approximation for the ML-II estimated value of θ that max-<br />imised Laplace’s approximation to the marginal likelihood (15). Note that this particular choice of θ<br />should be in favour of Laplace’s method. Figure 3 shows the resulting classifiers and the underlying<br />latent functions. In Figure 3(a) the approximations to p(y|x) appear to be similar for positive x but<br />we observe an appreciable discrepancy for negative values. Laplace’s approximation gives an un-<br />reasonably high predictive uncertainty, which is caused by a significant overlap of the approximate<br />predictive distribution p(f∗|D,θ,x∗) ? N (f∗|µ∗,σ2<br />note that both approximations agree on the sign of the predictive mean.<br />∗) with zero as shown in Figure 3(b). However,<br />7.2 Ionosphere Data<br />The data consists of 351 examples in 34 dimensions. We standardised the inputs X to zero mean<br />and unit variance. The training set is a random subset of size m = 200 leaving the remaining 151<br />instances out as a test set.<br />Wedoanexhaustiveinvestigationonaregular21×21gridofvaluesfortheloghyper-parameters.<br />For each θ on the grid we compute the approximated log marginal likelihood by Laplace’s method<br />(15), EP (27) and AIS. Additionally we compute the predictive performance on the test set. As<br />performance measure we use the average information in bits of the predictions about the test targets<br />in excess of that of random guessing. Let p∗= p(y∗=1|x∗) be the model’s prediction, then we<br />average:<br />I(p∗<br />i,yi) =yi+1<br />2log2(p∗<br />i)+1−yi<br />2log2(1− p∗<br />i)+H<br />(34)<br />over all test cases, where H is the entropy of the training set labels. Results are shown in Figure 4.<br />For all three approximation techniques we see an agreement between marginal likelihood esti-<br />mates and test performance, which justifies the use of ML-II parameter estimation. But the shape of<br />the contours and the values differ between the methods. The contours for Laplace’s method appear<br />to be slanted compared to EP. The estimated marginal likelihood estimates of EP and AIS agree<br />very well.2The EP predictions contain as much information about the test cases as the MCMC<br />predictions and significantly more than for Laplace’s method.<br />Note that for small signal variances (roughly ln(σ2) &lt; 0) Laplace’s method and EP give very<br />similar results. A possible explanation is that for small signal variances the likelihood does not<br />truncate the prior but only down-weights the tail that disagrees with the observation. As an effect<br />the posterior will be less skewed and both approximations will lead to similar results.<br />7.3 USPS Digits<br />We define a binary sub-problem from the USPS digit data3by considering 3’s vs. 5’s. We repeated<br />the experiments described in the previous section for a slightly modified grid of θ. Comparing the<br />results shown in Figure 5 leads to similar results as mentioned above. The EP and MCMC results<br />agree very well, given that the marginal likelihood comes as a 767 dimensional integral.<br />We now take a closer look at the approximations q(f|D,θ) =N (f|m,A) for a given value of θ.<br />We have chosen the values ln(σ)=3.35 and ln(?)=2.85 which are between the ML-II estimates of<br />EP and Laplace’s method. Comparing the respective means of the approximations in Figure 6(a) we<br />2. Note that the agreement between the two seems to be limited by the accuracy of the AIS runs, as judged by the<br />regularity of the contour lines; the tolerance is less than one unit on a (natural) log scale.<br />3. Because the training and test partitions in the original data differ significantly, we pooled cases and randomly divided<br />them into new sets, with 767 cases for training and 773 for testing.<br />1691</p>  <p>Page 14</p> <p>KUSS AND RASMUSSEN<br />Log marginal likelihood Information about test targets<br />Laplace’s Approximation<br />−200<br />−150<br />−120<br />−120<br />−100<br />−100<br />−90<br />−80<br />−75<br />−70<br />log lengthscale, log(l)<br />log magnitude, log(σf)<br />−1012345<br />−1<br />0<br />1<br />2<br />3<br />4<br />5<br />0<br />0<br />0.1<br />0.1<br />0<br />0.2<br />0.2<br />0.3<br />0.3<br />0.4<br />0.4<br />0.5<br />0.55<br />log lengthscale, log(l)<br />log magnitude, log(σf)<br />−112345<br />−1<br />0<br />1<br />2<br />3<br />4<br />5<br />Expectation Propagation<br />−120<br />−120<br />−100<br />−100<br />−90<br />−90<br />−80<br />−75<br />−75<br />−70<br />−65<br />log lengthscale, log(l)<br />log magnitude, log(σf)<br />−1012345<br />−1<br />0<br />1<br />2<br />3<br />4<br />5<br />0<br />0<br />0.1<br />0.1<br />0.2<br />0.2<br />0.3<br />0.3<br />0.4<br />0.5<br />0.5<br />0.55<br />0.6<br />log lengthscale, log(l)<br />log magnitude, log(σf)<br />−1012345<br />−1<br />0<br />1<br />2<br />3<br />4<br />5<br />Markov chain Monte Carlo<br />−120<br />−120<br />−100<br />−100<br />−90<br />−90<br />−80<br />−75<br />−75<br />−70<br />−65<br />log lengthscale, log(l)<br />log magnitude, log(σf)<br />−1012345<br />−1<br />0<br />1<br />2<br />3<br />4<br />5<br />0<br />0<br />0.1<br />0.1<br />0.2<br />0.2<br />0.3<br />0.3<br />0.4<br />0.5<br />0.5<br />0.5<br />0.5<br />0.55<br />0.6<br />log lengthscale, log(l)<br />log magnitude, log(σf)<br />−1012345<br />−1<br />0<br />1<br />2<br />3<br />4<br />5<br />Figure 4: Comparison of marginal likelihood approximations and predictive performances for the<br />Ionosphere data set. The first column shows the estimates of log marginal likelihood,<br />while the second column shows the performance on the test set measured by the informa-<br />tion about test targets in bits (34).<br />1692</p>  <p>Page 15</p> <p>ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION<br />Log marginal likelihood Information about test targets<br />Laplace’s Approximation<br />−200<br />−200<br />−150<br />−130<br />−115<br />−105<br />−100<br />log lengthscale, log(l)<br />log magnitude, log(σf)<br />2345<br />0<br />1<br />2<br />3<br />4<br />5<br />0.25<br />0.25<br />0.5<br />0.5<br />0.7<br />0.7<br />0.8<br />0.8<br />0.84<br />log lengthscale, log(l)<br />log magnitude, log(σf)<br />2345<br />0<br />1<br />2<br />3<br />4<br />5<br />Expectation Propagation<br />−200<br />−200<br />1<br />−160<br />−160<br />−130<br />−130<br />−115<br />−105<br />−105<br />−100<br />−95<br />−92<br />log lengthscale, log(l)<br />log magnitude, log(σf)<br />2345<br />0<br />2<br />3<br />4<br />5<br />0.25<br />0.5<br />0.7<br />0.7<br />0.8<br />0.8<br />0.84<br />0.84<br />0.86 0.86<br />0.88<br />0.89<br />log lengthscale, log(l)<br />log magnitude, log(σf)<br />2345<br />0<br />1<br />2<br />3<br />4<br />5<br />Markov chain Monte Carlo<br />2345<br />0<br />1<br />2<br />3<br />4<br />5<br />log lengthscale, log(l)<br />log magnitude, log(σf)<br />−92−95−100<br />−105<br />−105<br />−115<br />−130<br />−160<br />−160<br />−200<br />−200<br />0.25<br />0.5<br />0.7<br />3<br />0.7<br />0.8<br />0.84<br />0.84<br />0.86<br />0.86<br />0.88<br />0.89<br />log lengthscale, log(l)<br />log magnitude, log(σf)<br />2345<br />0<br />1<br />2<br />4<br />5<br />Figure 5: Comparison of marginal likelihood approximations and predictive performances of the<br />different methods for classifying 3’s vs. 5’s from the USPS image database. The plots are<br />arranged as in Figure 4.<br />1693</p>  <p>Page 16</p> <p>KUSS AND RASMUSSEN<br />−40−2002040<br />−40<br />−30<br />−20<br />−10<br />0<br />10<br />20<br />30<br />40<br />m Laplace<br />(a)<br />m EP<br />Class 5<br />Class 3<br />−30−20<br />ln(w) Laplace<br />(b)<br />−100<br />−20<br />−15<br />−10<br />−5<br />0<br />−ln(σ2) EP<br />Class 5<br />Class 3<br />00.51<br />0<br />0.5<br />1<br />p* MCMC<br />(c)<br />p* Laplace &amp; EP<br />Laplace p*<br />EP p*<br />00.51<br />0<br />0.5<br />1<br />p* MCMC<br />(d)<br />p* Laplace &amp; EP<br />Laplace p*<br />EP p*<br />Figure 6: Comparison of approximations q(f|D,θ) = N (f|m,A) for a given value of θ. Panel (a)<br />shows a comparison of the means mi. In Panel (b) we compare the elements of the diago-<br />nal matrices Wiiand Σii. Panels (c) and (d) compare predictions p∗obtained by MCMC<br />(abscissa) to predictions obtained from Laplace’s method and EP (ordinate). Panel (c)<br />shows predictions on training cases and (d) shows predictions on test cases.<br />see that the magnitude of the means from the Laplace approximation is much smaller than from EP.<br />The relation appears to be roughly linear. In Figure 6(b) we compare the elements of W and Σ−1<br />which cause the difference in the approximations (13) and (18) of the posterior covariance matrix A.<br />We observe that the relatively large entries in W are larger than the corresponding entries in Σ−1,<br />but in total W contains more small values than Σ−1. The exact effect on the posterior covariance<br />is difficult to characterise due to the inversion, but intuitively the smaller the values the more the<br />posterior covariance will be similar to the prior.<br />1694</p>  <p>Page 17</p> <p>ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION<br />Figures 6(c-d) compare the predictive uncertainty p∗resulting from the respective approxima-<br />tions to MCMC predictions. For both training and test set we observe that EP and MCMC agree<br />very well, while Laplace’s method shows over-conservative predictions.<br />−15−10−505<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />f<br />MCMC samples<br />Laplace p(f|D)<br />EP p(f|D)<br />−40−30−20−10010<br />0<br />0.01<br />0.02<br />0.03<br />0.04<br />0.05<br />0.06<br />0.07<br />f<br />MCMC samples<br />Laplace p(f|D)<br />EP p(f|D)<br />(a)(b)<br />Figure 7: Two marginal distributions p(fi|D,θ) from the posterior. For Panel (a) we picked the<br />fifor which the posterior marginal is maximally skewed (see again Figure 1). The true<br />posterior is approximated by a normalised histogram of 9000 samples of fiobtained by<br />MCMC sampling. Panel (b) shows a case where EP and Laplace’s approximation differ<br />significantly.<br />We now inspect the marginal distributions p(fi|D,θ) of single latent function values under the<br />posterior approximation. We use hybrid MCMC to generate 9000 samples from the posterior of f<br />for the above θ. For Laplace’s method and EP the approximated distribution is N (fi|mi,Aii) where<br />m and A are found by the respective approximation techniques.<br />In general we observe that the marginal distributions of MCMC samples agree very well with<br />the respective marginal distributions of the EP approximation. This supports the claim made in<br />Section 5 where we argued that the marginal distributions of the posterior can be very similar to<br />Gaussians, even if the posterior is a skew distribution. For Laplace’s approximation we find the<br />mean to be underestimated and the marginal distributions to overlap with zero far more than the<br />EP approximations. Figure 7(a) displays the marginal distribution and its approximations for which<br />the MCMC samples show maximal skewness. Figure 7(b) shows a typical example where the<br />EP approximation agrees very well with the MCMC samples. We show this particular example<br />because under the EP approximation q(yi= 1|D,θ) &lt; 0.1% but Laplace’s approximation gives<br />q(yi= 1|D,θ) ? 18%.<br />7.4 Lower Bound Approximation<br />In the context of sparse EP approximations Seeger (2003) proposed a lower bound on the marginal<br />likelihood. The bound is obtained from the EP approximation of the posterior using Jensen’s in-<br />1695</p>  <p>Page 18</p> <p>KUSS AND RASMUSSEN<br />−250<br />−200<br />−150<br />−120<br />−120<br />−100<br />−100<br />−90<br />−90<br />−80<br />−80<br />−75<br />−75<br />−70<br />log lengthscale, log(l)<br />(a) Ionosphere<br />log magnitude, log(σf)<br />Lower bound on log marginal likelihood<br />−1012345<br />−1<br />0<br />1<br />2<br />3<br />4<br />5<br />−200<br />−200<br />−160<br />−160<br />−130<br />−130<br />−115<br />−115<br />−105<br />−100<br />log lengthscale, log(l)<br />(b) USPS 3’s vs. 5’s<br />log magnitude, log(σf)<br />Lower bound on log marginal likelihood<br />2345<br />0<br />1<br />2<br />3<br />4<br />5<br />Figure 8: Lower bound on marginal likelihood. Panel (a) shows the lower bound eq. (35) on the<br />marginal likelihood for the Ionosphere data set (compare to left column of Figure 4).<br />Panel (b) shows the value of the lower bound for the USPS 3’s vs. 5’s (compare to left<br />column of Figure 5)<br />equality:<br />lnp(D|θ)=<br />ln<br />Z<br />N (f|m,A)lnp(y|f)N (f|0,K)<br />∑<br />i=1<br />−1<br />p(y|f)N (f|0,K)df<br />(35a)<br />≥<br />Z<br />N (f|m,A)<br />df<br />(35b)<br />=<br />m<br />Z<br />N (fi|mi,Aii)lnΦ(yifi)d fi<br />2m?K−1m−1<br />2tr(K−1A)+1<br />2ln|K−1A|+m<br />2.<br />(35c)<br />Note that the one dimensional integrals in eq. (35c) have to be solved using numerical integration<br />methods.<br />In sparse EP methods the Gaussian approximation is based on only a subset of observations<br />and so the evidence (27) may be a bad approximation of the total evidence since it does not take<br />all available data into account. Assume that the m points are only a subset of of a total of m?<br />observations. The lower bound (35c) can be extended to a lower bound on all m?observations by<br />including all points in the one dimensional integrals over the individual log likelihood terms.<br />Severalauthorsmaximisethislowerboundinsteadofmaximising(27)forML-IIhyper-parameter<br />estimation also in the case of non-sparse EP approximations, e.g. Chu and Ghahramani (2005). In<br />Figure 8 we show the value of the lower bound as a function of the hyper-parameters for the Iono-<br />sphere and USPS data described in the previous sections (for the full EP approximation). Interest-<br />ingly, for both data sets the lower bounds appear to be more similar to the approximate evidence<br />obtained by Laplace’s method than by EP (compare to the upper left panel in Figures 4 and 5<br />1696</p>  <p>Page 19</p> <p>ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION<br />respectively). However, the maxima of the lower bounds correspond to sub-optimal predictive per-<br />formances compared to the maxima of the approximate marginal likelihood (27) (compare to the<br />second row in Figures 4 and 5 respectively). Therefore for non-sparse EP approximations the use<br />of (27) seems advisable, which is also computationally advantageous.<br />7.5 Benchmark Data Sets<br />In this section we compare the performance of Laplace’s method and Expectation Propagation for<br />GPC on several well known benchmark problems for binary classification.<br />The Ionosphere, the Wisconsin Breast Cancer, and the Sonar data sets are taken from Hettich<br />et al. (1998). The Leptograpsus Crabs and the Pima Indians Diabetes data sets were described by<br />Ripley (1996). Note that for the Crabs data set we use the sex (not the colour) of the crabs as target<br />variable. The largest data set in the comparison are the 3’s vs. 5’s from the USPS handwritten digits<br />described above.<br />We standardise the inputs X to zero mean and unit variance. All data sets are randomly split<br />into 10 folds of which one at a time is left out as a test set to measure the predictive performance of<br />a model trained (or selected) on the remaining nine folds.<br />For GPC we implement model selection by ML-II hyper-parameter estimation. We use a con-<br />jugate gradient optimisation routine to find a minimum<br />θML= argmin<br />θ<br />−lnq(D|θ)<br />(36)<br />of the negative log marginal likelihood approximated by Laplace’s method (15) and EP (27) respec-<br />tively. FortherespectiveθMLtheapproximationsN (f|m,A)arecomputedandpredictionsaremade<br />for the left out test set. From the predictive distributions the average information (34) is computed<br />and averaged over the ten folds. Furthermore the average error rate E is reported, which equals the<br />average percentage of erroneous class assignments if prediction is understood as a decision problem<br />with symmetric costs (thresholding the predictive uncertainty at 1/2).<br />In order to have a better absolute impression of the predictive performance we report the results<br />of support vector machines (SVM) (Sch¨ olkopf and Smola, 2002). We use the LIBSVM implemen-<br />tation of C-SVM by Chang and Lin (2001) with a radial basis function kernel which is equivalent<br />to the covariance function (33) up to the signal variance parameter. The values of the length scale<br />parameter ? and the regularisation parameterC are found by an inner loop of 5-fold cross-validation<br />on the nine training folds respectively. We manually refine the parameter grids and repeat the cross-<br />validation procedure until the performance stabilises.<br />We use the technique described by Platt (2000) to estimate predictive probabilities from an<br />SVM. This is implemented by fitting a sigmoidal mapping from the unthresholded output of the<br />SVM to the unit interval. The parameters of the mapping are estimated on the test set in the inner<br />loop of 5-fold cross-validation.<br />Results are summarised in Table 1. Comparing Laplace’s method to EP the latter shows to be<br />more accurate both in terms of error rate and information. While the error rates are relatively similar<br />the predictive distribution obtained by EP shows to be more informative about the test targets. As<br />to be expected by now, the length of the mean vector ?m? shows much larger values for the EP<br />approximations. Comparing EP and SVM the results are mixed.<br />At first sight it may seem surprising that Laplace’s method gives relatively similar error rates<br />compared to EP. Note that for both methods the error rate only depends on the sign of the latent<br />1697</p>  <p>Page 20</p> <p>KUSS AND RASMUSSEN<br />Laplace<br />I<br />EPSVM<br />Data Set<br />Ionosphere<br />Wisconsin<br />Pima Indians<br />m<br />351<br />683<br />768<br />200<br />208<br />n<br />34<br />9<br />8 22.77 0.252<br />72.0 0.682 112.34<br />60 15.36 0.439<br />2.27 0.849 163.05<br />E<br />?m?<br />49.96<br />62.62<br />29.05 22.63 0.253<br />2.0 0.908<br />26.86 13.85 0.537 15678.55 11.14 0.567<br />2.21 0.902 22011.70<br />EI<br />?m?<br />124.94<br />84.95<br />47.49 23.01 0.232<br />2552.97<br />EI<br />8.84 0.591<br />3.21 0.804<br />7.99 0.661<br />3.21 0.805<br />5.69 0.681<br />3.21 0.795<br />Crabs<br />Sonar<br />2.0 0.047<br />USPS 3 vs 5 1540 2562.01 0.918<br />Table 1: Results for benchmark data sets. The first three columns give the name of the data set,<br />number of observation m and dimension of inputs n. For Laplace’s method and EP the<br />table reports the average error rate E, the average information I (34) and the average length<br />?m? of the mean vector of the Gaussian approximation. For SVMs the error rate and the<br />average information about the test targets are reported.<br />mean function (5a) at the test locations, which in turn depend on m only. Therefore the error rate<br />is less sensitive to the accuracy of the approximation to the posterior, but of course depends on the<br />ML-II estimated hyper-parameters, which differ between the methods. Also in the example shown<br />in Figure 3(b) it can be observed that the latent mean functions differ but their sign matches very<br />accurately.<br />For the Crabs data set all methods show the same error rate but the information content of the<br />predictive distributions differs dramatically. For some test cases the SVM predicts the wrong class<br />with large certainty. Because the mapping of the unthresholded output of the SVM to the predictive<br />probability is estimated from a left out set, the mapping can be poor if too few errors are observed<br />on this.<br />8. Conclusions<br />Our experiments reveal serious differences between Laplace’s method and EP when used in GPC<br />models. The results corroborate the considerations about the two approximations based on the<br />structure of the posterior given in Section 5. Although only a handful of data sets have been used in<br />the study, we believe the conclusions to be well-founded and generally valid.<br />From the structural properties of the posterior we described why Laplace’s method systemati-<br />cally underestimates the mean m. The resulting approximate posterior GP over latent functions will<br />have too small amplitude, although the sign of the mean function will be mostly correct. As an ef-<br />fect Laplace’s method gives over-conservative predictive probabilities, and diminished information<br />about the test labels. This effect has been shown empirically on several real world examples. Large<br />resulting discrepancies in the actual posterior probabilities were found, even at the training loca-<br />tions, which renders the predictive class probabilities produced under this approximation grossly<br />inaccurate. Note, the difference becomes less dramatic if we only consider the classification error<br />rates obtained by thresholding p∗at 1/2. For this particular task, we have seen the sign of the la-<br />tent function tends to be correct (at least at the training locations). However, the performance on<br />benchmark data sets also revealed the error rates obtained by Laplace’s method to be inferior to EP<br />results.<br />1698</p>  <p>Page 21</p> <p>ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION<br />The EP approximation has shown to give results very close to MCMC both in terms of predictive<br />distributions and marginal likelihood estimates. We have shown and explained why the marginal<br />distributions of the posterior can be well approximated by Gaussians.<br />Further, the marginal likelihood values obtained by Laplace’s method and EP differ systemat-<br />ically which will lead to different results of ML-II hyper-parameter estimation. The discrepancies<br />are similar for different tasks. We were able to exemplify that the EP approximation of the marginal<br />likelihood is accurate. To show this we described how AIS can be used to obtain unbiased estimates<br />of the marginal likelihood for Gaussian process models.<br />In the experiments summarised in Table 1 we compared the predictive accuracy of GPC to sup-<br />port vector machines. While the SVMs show a tendency to give lower error rates, the information<br />contained in predictive distributions seems comparable. Conceptually GPC comes with the advan-<br />tage that the Bayesian model selection can be used to set hyper-parameters by ML-II estimation,<br />while the parameters of an SVM usually have to be set by cross-validation (gradient based methods<br />exist, see e.g. Chapelle et al. (2002)).<br />In summary, we found that EP is the method of choice for approximate inference in binary GPC<br />models, when the computational cost of MCMC is prohibitive. Very good agreement is achieved<br />for both predictive probabilities and marginal likelihood estimates. In contrast, the Laplace approx-<br />imation is so inaccurate that we advise against its use, especially when predictive probabilities are<br />to be taken seriously.<br />Acknowledgments<br />Both authors acknowledge support by the German Research Foundation (DFG) through grant RA<br />1030/1. We would like to thank Tobias Pfingsten, Jeremy Hill and Matthias Seeger for comments<br />and discussions.<br />Appendix A. Implementation of Laplace’s Approximation<br />In Sections 3 we described Laplace’s method for approximate inference in the GPC model and<br />sketched the corresponding computations in Algorithm 1. In this appendix we describe our imple-<br />mentation of the method in more detail. See also the appendices of Williams and Barber (1998).<br />Computing Laplace’s approximation N (f|m,A) for given θ the main computational effort is<br />involved in finding the maximum of the unnormalised log posterior lnQ (eq. (8)). Our implementa-<br />tion uses Newton’s method to find the mode. In each Newton step the vector f is updated according<br />to<br />ft+1<br />=<br />ft−(∇∇flnQ(ft))−1∇flnQ(ft)<br />(K−1+W)−1(Wft+∇flnL(ft))<br />(37a)<br />(37b)<br />=<br />until convergence of f to the mode m. To ensure convergence the update is accepted if the value of<br />the target function increases, otherwise the the step size is shortened until lnQ(ft+1) &gt; lnQ(ft).<br />Computationally Newtons’s method is dominated by the repeated inversion of the Hessian.<br />Since K can be poorly conditioned we use the identity<br />(K−1+W)−1= K−KW<br />1<br />2(I+W<br />1<br />2KW<br />1<br />2)−1W<br />1<br />2K<br />(38)<br />1699</p>  <p>Page 22</p> <p>KUSS AND RASMUSSEN<br />such that only the well conditioned, positive definite matrix (I+W<br />our implementation the inverse is computed from a Cholesky decomposition of this matrix. Note<br />that W is a diagonal matrix with positive entries, so computing W<br />Note that implementing the Newton updates (37) only requires the product of the inverse Hes-<br />sian times the gradient which can be computed more efficiently using an iterative conjugate gradient<br />method (Golub and Van Loan, 1989, ch. 10).<br />Having found the mode m the marginal likelihood approximation (15) and its derivatives can be<br />computed. The approximate marginal likelihood takes the form<br />1<br />2KW<br />1<br />2) has to be inverted. In<br />1<br />2 is trivial.<br />lnq(D|θ)=<br />lnQ(m)+m<br />2ln(2π)+1<br />2m?K−1m−1<br />2ln|A|<br />2ln|I+KW| .<br />(39a)<br />=<br />lnL(m)−1<br />(39b)<br />To avoid the direct inversion of K in the second term of (39b) we use the recurrence relation (37b).<br />Let a = K−1m then by substituting (38) into (37b) we obtain:<br />a = (I−W<br />1<br />2(I+W<br />1<br />2KW<br />1<br />2)−1W<br />1<br />2K)(Wm+∇flnL(m))<br />(40)<br />such that m?K−1m = m?a. The determinant in eq. (39b) can be rewritten<br />ln|I+KW| = ln??I+W<br />1<br />2KW<br />1<br />2??<br />(41)<br />and computed from the Cholesky decomposition, that was used to calculate the inverse in eq. (38).<br />Note that if M = LL?is a Cholesky decomposition then ln|M| = 2∑lnLii.<br />During ML-II estimation (36) of hyper-parameters the approximate log marginal likelihood (39)<br />is maximised as a function of θ. Our implementation is based on a conjugate gradient optimisation<br />routine such that we also need to compute the derivatives of (39b) with respect to the elements of θ.<br />The dependency of the approximate marginal likelihood on θ is two-fold:<br />∂lnq(D|θ)<br />∂θi<br />=∑<br />k,l<br />∂lnq(D|θ)<br />∂Kkl<br />∂Kkl<br />∂θi<br />+∂lnq(D|θ)<br />∂m?<br />∂m<br />∂θi<br />(42)<br />there is a direct dependency via the terms involving K and an implicit dependency through the<br />change in m (see also Williams and Barber (1998, Appendix B)).<br />The explicit derivative of eq. (39b) due to the direct dependency of the covariance matrix is<br />∑<br />k,l<br />∂lnq(D|θ)<br />∂Kkl<br />∂Kkl<br />∂θi<br />=1<br />2m?K−1∂K<br />∂θiK−1m−1<br />2tr<br />?<br />(I+KW)−1∂K<br />∂θiW<br />?<br />(43)<br />where the first term is computed using a (40) and the inverse in the second term can be rewritten as<br />(I+KW)−1= I−(K−1+W)−1W<br />(44)<br />where the inverse (38) is already known.<br />The implicit derivative accounts for the dependency of eq. (39b) on θ due to change in the mode<br />m. Differentiating eq. (39a) with respect to m reduces to ∂ln|A|/∂m since m is the maximum of<br />lnQ and therefore ∂lnQ/∂m vanishes.<br />∂lnq(D|θ)<br />∂m?<br />∂m<br />∂θi<br />= −1<br />2<br />∂|K−1+W|<br />∂m?<br />∂m<br />∂θi<br />= −1<br />2(K−1+W)−1∂W<br />∂m?<br />∂m<br />∂θi<br />(45)<br />1700</p>  <p>Page 23</p> <p>ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION<br />The dependency of m on θiis obtained by differentiating (11a) at m:<br />0 = ∇flnL(m)−K−1m<br />=⇒<br />m = K∇flnL(m)<br />(46)<br />so<br />∂m<br />∂θi<br />=∂K<br />∂θi∇flnL(m)+K∇∇flnL(m)∂m<br />∂θi<br />= (I+KW)−1∂K<br />∂θi∇flnL(m)<br />(47)<br />and we have both terms necessary to compute the gradient (42).<br />To compute the predictive probability p∗= p(y∗=1|x∗) for a test input x∗the predictive distri-<br />bution (5) of the latent function value is N (f∗|µ∗,σ2<br />µ∗<br />σ2<br />∗<br />and p∗can be computed from eq. (6).<br />Due to the Cholesky decomposition in (38) computing Laplace’s approximation isO(m3). How-<br />ever, following the implementation we described in this section a Cholesky decomposition has to be<br />computed once per Newton step and all other quantities can be computed from it in at most O(m2).<br />The number of Newton steps necessary depends on the convergence criterion, the initialisation of f<br />and the hyper-parameters θ.<br />∗) where<br />=<br />k?<br />k(x∗,x∗)−k?<br />∗K−1m = k?<br />∗a<br />∗W<br />(48a)<br />=<br />1<br />2(I+W<br />1<br />2KW<br />1<br />2)−1W<br />1<br />2k∗<br />(48b)<br />Appendix B. Implementation of Expectation Propagation<br />In this appendix we describe details of our implementation of EP as described in Section 4 and<br />summarised in Algorithm 2. See also the appendices of Seeger (2003).<br />In our implementation the site functions (17) are parameterised in terms of natural parameters<br />σ−2<br />i<br />The algorithm proceeds by updating the site parameters in random order. In each sweep every site<br />function is updated following equations (23), (25), and (26). After each update of a site function the<br />effect on m and A has to be computed according to (18). The change in A can be computed using a<br />rank one update. Let δ be the change in σ−2<br />i<br />due to the update and eithe vector whose ith entry is 1<br />and all other 0. The relation<br />and σ−2<br />iµi. For given θ the algorithm starts by initialising A = K and σ−2<br />i<br />= 0 and σ−2<br />iµi= 0.<br />(K−1+Σ−1+δeie?<br />i)−1= A−Aei(Aii+δ−1)−1e?<br />iA<br />(49)<br />can be used to update A. Each single update isO(m2) and repeated m times per sweep, such that the<br />EP algorithm is O(m3) in time. Because of accumulating numerical errors, after a complete sweep<br />over all site functions we recompute the matrix A from scratch. For numerical stability we rewrite<br />A = (K−1+Σ−1)−1= K−KΣ−1<br />2(I+Σ−1<br />2KΣ−1<br />2)−1Σ−1<br />2K<br />(50)<br />and compute the inverse from the Cholesky decomposition of (I+Σ−1<br />After convergence the approximate log marginal likelihood (27) can be computed and its partial<br />derivatives with respect to the hyper-parameters:<br />2KΣ−1<br />2).<br />∂lnq(D|θ)<br />∂θi<br />= −1<br />2tr<br />?∂K<br />∂θi<br />?<br />(K+Σ)−1−(K+Σ)−1µµ?(K+Σ)−1??<br />.<br />(51)<br />1701</p>  <p>Page 24</p> <p>KUSS AND RASMUSSEN<br />which do not depend on the Zi(Seeger, 2005).<br />The inverse of K+Σ can be computed from the inverse in eq. (50):<br />(K+Σ)−1= Σ−1<br />2(I+Σ−1<br />2KΣ−1<br />2)−1Σ−1<br />2.<br />(52)<br />For computing the log marginal likelihood (27) also the determinant |K+Σ| has to be computed.<br />By rewriting<br />ln|K+Σ| = ln(|Σ||I+Σ−1K|) = ln|Σ|+ln|I+Σ−1<br />we obtain an expression in which the first term is a determinant of a diagonal matrix and the second<br />term can be computed from the Cholesky decomposition that was used to compute the inverse in<br />eq. (50).<br />To compute the predictive probability p∗= p(y∗=1|x∗) for a test input x∗the predictive distri-<br />bution (5) of the latent function value is N (f∗|µ∗,σ2<br />µ∗<br />σ2<br />∗<br />and p∗can be computed from eq. (6).<br />The EP algorithm is of computational complexity O(m3) due to the computations for updat-<br />ing A. However, per sweep the computation of A (50) and the m rank one updates sum to more<br />computational effort compared to Laplace’s method.<br />Usinga covariancefunction ofthe form(33)forsomedata sets we observednumerical problems<br />during ML-II hyper-parameter estimation because the optimisation algorithm asked to evaluate the<br />marginal likelihood for extremely large signal variances σ2. The problem stems from the property<br />that for large values of σ2the marginal likelihood becomes insensitive to changes in σ2. At this<br />point it is recommended to take another look at Figure 1(b). Intuitively, for large signal variances<br />the prior becomes more spread, such that the likelihood becomes more and more similar to a hard<br />truncation. The marginal likelihood equals the probability mass of the prior in the orthant that is left<br />after truncation. But the probability mass in any of the orthants remains constant if only the signal<br />variance is changed for fixed correlation structure. This argument is based on the assumption that<br />the likelihood implements a hard truncation, which is only an approximation, but this approximation<br />becomes better the larger σ2is. Note that this insensitivity of the marginal likelihood with respect<br />to changes in the signal variance can already be observed in the upper parts of of the marginal<br />likelihood plots for EP in Figures 4 and 5. A possible solution to this problem is to limit σ2&lt; 105,<br />say, since we wouldn’t typically expect any new interesting behaviour beyond this.<br />2KΣ−1<br />2|<br />(53)<br />∗) where<br />=<br />k?<br />k(x∗,x∗)−k?<br />∗(K+Σ)−1µ<br />(54a)<br />(54b)<br />=<br />∗(K+Σ)−1k∗<br />1702</p>  <p>Page 25</p> <p>ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION<br />References<br />P. Abrahamsen. A review of Gaussian random fields and correlation functions. Technical Report<br />917, Norwegian Computing Center, Oslo, 1997.<br />C.-C. Chang and C.-J. Lin.<br />http://www.csie.ntu.edu.tw/∼cjlin/libsvm.<br />O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support<br />vector machines. Machine Learning, 46(1):131–159, 2002.<br />LIBSVM: A library for Support Vector Machines, 2001.<br />W. Chu and Z. Ghahramani. Gaussian processes for ordinal regression. Journal of Machine Learn-<br />ing Research, 6:1019–1041, 2005.<br />L. Csat´ o and M. Opper. Sparse online Gaussian processes. Neural Computation, 14(2):641–669,<br />2002.<br />S. Duane, A. D. Kennedy, B. J. Pendleton, and D. Roweth. Hybrid Monte Carlo. Physics Letters B,<br />195(2):216–222, 1987.<br />A. Gelman and X.-L. Meng. Simulating normalizing constants: From importance sampling to<br />bridge sampling to path sampling. Statistical Science, 13(2):163–185, 1998.<br />M. N. Gibbs and D. J. C. MacKay. Variational Gaussian process classifiers. IEEE Transactions on<br />Neural Networks, 11(6):1458–1464, 2000.<br />G. H. Golub and C. F. Van Loan. Matrix Computations. John Hopkins University Press, Baltimore,<br />second edition, 1989.<br />S. Hettich, C. L. Blake, and C. J. Merz. UCI repository of machine learning databases, 1998.<br />http://www.ics.uci.edu/∼mlearn/MLRepository.html.<br />R. E. Kass and A. E. Raftery. Bayes factors. Journal of the American Statistical Association, 90<br />(430):773–795, 1995.<br />N. Lawrence, M. Seeger, and R. Herbrich. Fast sparse Gaussian process methods: The informa-<br />tive vector machine. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural<br />Information Processing Systems 15, pages 609–616, Cambridge, MA, 2003. The MIT Press.<br />J. S. Liu. Monte Carlo Strategies in Scientific Computing. Springer, New York, 2001.<br />D. J. C. MacKay. Comparison of approximate methods for handling hyperparameters. Neural<br />Computation, 11(5):1035–1068, 1999.<br />D. J. C. MacKay. Information Theory, Inference and Learning Algorithms. Cambridge University<br />Press, Cambridge, UK, 2003.<br />T. P. Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD thesis, Department<br />of Electrical Engineering and Computer Science, MIT, 2001.<br />R. M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. Technical Report<br />CRG-TR-93-1, Department of Computer Science, University of Toronto, 1993.<br />1703</p>  <p>Page 26</p> <p>KUSS AND RASMUSSEN<br />R. M. Neal. Regression and classification using Gaussian process priors. In J. M. Bernardo, J. O.<br />Berger, A. P. Dawid, and A. F. M. Smith, editors, Bayesian Statistics 6, pages 475–501. Oxford<br />University Press, 1998.<br />R. M. Neal. Annealed importance sampling. Statistics and Computing, 11:125–139, 2001.<br />A.O’Hagan. Curvefittingandoptimaldesignforprediction. JournaloftheRoyalStatisticalSociety,<br />Series B, 40(1):1–42, 1978.<br />M. Opper and O. Winther. Gaussian processes for classification: Mean-field algorithms. Neural<br />Computation, 12(11):2655–2684, 2000.<br />J. C. Platt. Probabilities for SV machines. In A. J. Smola, P. L. Bartlett, B. Sch¨ olkopf, and D. Schu-<br />urmans, editors, Advances in Large Margin Classifiers, pages 61–73. The MIT Press, Cambridge,<br />MA, 2000.<br />C. E. Rasmussen and C. K. I Williams. Gaussian Processes for Machine Learning. The MIT Press,<br />Cambridge, MA, 2006. In press.<br />B. D. Ripley. Pattern Recognition and Neural Newtorks. Cambridge University Press, Cambridge,<br />UK, 1996.<br />B. Sch¨ olkopf and A. J. Smola. Learning with Kernels. The MIT Press, Cambridge, MA, 2002.<br />M. Seeger. PAC-Bayesian generalisation error bounds for Gaussian process classification. Journal<br />of Machine Learning Research, 3:233–269, 2002.<br />M. Seeger. Bayesian Gaussian Process Models: PAC-Bayesian Generalisation Error Bounds and<br />Sparse Approximations. PhD thesis, University of Edinburgh, 2003.<br />M. Seeger. Expectation propagation for exponential families, 2005. Note obtainable from<br />http://www.kyb.tuebingen.mpg.de/∼seeger.<br />C. K. I. Williams and D. Barber. Bayesian classification with Gaussian processes. IEEE Transac-<br />tions on Pattern Analysis and Machine Intelligence, 20(12):1342–1351, 1998.<br />1704</p>   </div> <div id="rgw20_56ab1dba860c9" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw21_56ab1dba860c9">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw22_56ab1dba860c9"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.74.2668&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Assessing Approximate Inference for Binary Gaussian Process Classification">Assessing Approximate Inference for Binary Gaussia...</a> </div>  <div class="details">   Available from <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.74.2668&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow">psu.edu</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw29_56ab1dba860c9" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (91) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw30_56ab1dba860c9" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw31_56ab1dba860c9" >  <div class="indent-left">  <div id="rgw32_56ab1dba860c9" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Kostas_Stathis" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Kostas Stathis </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw33_56ab1dba860c9">  <li class="citation-context-item"> "Gaussian Processes (GPs) are widely used tools in statistics (Barry, 1986), machine learning (Neal, 1995; Williams and Barber, 1998; Kuss and Rasmussen, 2005; Rasmussen and Williams, 2006; Damianou and Lawrence, 2013), robotics (Ferris et al., 2006), computer vision (Kemmler et al., 2013), and scientific computation (Kennedy and O&amp;apos;Hagan, 2001; Schneider et al., 2008; Kwan et al., 2013). They are also central to probabilistic numerics, an emerging effort to develop more computationally efficient numerical procedures, and to Bayesian optimization, a family of meta-optimization techniques that are widely used to tune parameters for deep learning algorithms (Snoek et al., 2012; Gelbart et al., 2014). " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization"> <span class="publication-title js-publication-title">Probabilistic Programming with Gaussian Process Memoization</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2034167984_Ulrich_Schaechtle" class="authors js-author-name ga-publications-authors">Ulrich Schaechtle</a> &middot;     <a href="researcher/2089456342_Ben_Zinberg" class="authors js-author-name ga-publications-authors">Ben Zinberg</a> &middot;     <a href="researcher/2089399536_Alexey_Radul" class="authors js-author-name ga-publications-authors">Alexey Radul</a> &middot;     <a href="researcher/8074903_Kostas_Stathis" class="authors js-author-name ga-publications-authors">Kostas Stathis</a> &middot;     <a href="researcher/2089392273_Vikash_K_Mansinghka" class="authors js-author-name ga-publications-authors">Vikash K. Mansinghka</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Gaussian Processes (GPs) are widely used tools in statistics, machine
learning, robotics, computer vision, and scientific computation. However,
despite their popularity, they can be difficult to apply; all but the simplest
classification or regression applications require specification and inference
over complex covariance functions that do not admit simple analytical
posteriors. This paper shows how to embed Gaussian processes in any
higher-order probabilistic programming language, using an idiom based on
memoization, and demonstrates its utility by implementing and extending classic
and state-of-the-art GP applications. The interface to Gaussian processes,
called gpmem, takes an arbitrary real-valued computational process as input and
returns a statistical emulator that automatically improve as the original
process is invoked and its input-output behavior is recorded. The flexibility
of gpmem is illustrated via three applications: (i) robust GP regression with
hierarchical hyper-parameter learning, (ii) discovering symbolic expressions
from time-series data by fully Bayesian structure learning over kernels
generated by a stochastic grammar, and (iii) a bandit formulation of Bayesian
optimization with automatic inference and action selection. All applications
share a single 50-line Python library and require fewer than 20 lines of
probabilistic code each. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Dec 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Kostas_Stathis/publication/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization/links/5687eb7108ae051f9af5a28a.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw34_56ab1dba860c9" >  <div class="indent-left">  <div id="rgw35_56ab1dba860c9" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/281145119_Non-Stationary_Gaussian_Process_Regression_with_Hamiltonian_Monte_Carlo">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Samuel_Kaski" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Samuel Kaski </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw36_56ab1dba860c9">  <li class="citation-context-item"> "The posterior of the latent vectors is by definition highly correlated due to Gaussian priors, leading to inefficient Monte Carlo sampling. To ease the sampling, we perform the sampling over the whitened latent vectors (Kuss and Rasmussen, 2005) " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/281145119_Non-Stationary_Gaussian_Process_Regression_with_Hamiltonian_Monte_Carlo"> <span class="publication-title js-publication-title">Non-Stationary Gaussian Process Regression with Hamiltonian Monte Carlo</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/31451560_Markus_Heinonen" class="authors js-author-name ga-publications-authors">Markus Heinonen</a> &middot;     <a href="researcher/2079800486_Henrik_Mannerstroem" class="authors js-author-name ga-publications-authors">Henrik Mannerström</a> &middot;     <a href="researcher/7729922_Juho_Rousu" class="authors js-author-name ga-publications-authors">Juho Rousu</a> &middot;     <a href="researcher/2079807374_Samuel_Kaski" class="authors js-author-name ga-publications-authors">Samuel Kaski</a> &middot;     <a href="researcher/38617824_Harri_Laehdesmaeki" class="authors js-author-name ga-publications-authors">Harri Lähdesmäki</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> We present a novel approach for fully non-stationary Gaussian process
regression (GPR), where all three key parameters -- noise variance, signal
variance and lengthscale -- can be simultaneously input-dependent. We develop
gradient-based inference methods to learn the unknown function and the
non-stationary model parameters, without requiring any model approximations. We
propose to infer full parameter posterior with Hamiltonian Monte Carlo (HMC),
which conveniently extends the analytical gradient-based GPR learning by
guiding the sampling with model gradients. We also learn the MAP solution from
the posterior by gradient ascent. In experiments on several synthetic datasets
and in modelling of temporal gene expression, the nonstationary GPR is shown to
be necessary for modeling realistic input-dependent dynamics, while it performs
comparably to conventional stationary or previous non-stationary GPR models
otherwise. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Aug 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Samuel_Kaski/publication/281145119_Non-Stationary_Gaussian_Process_Regression_with_Hamiltonian_Monte_Carlo/links/55f31f8708ae6a34f6605292.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw37_56ab1dba860c9" >  <div class="indent-left">  <div id="rgw38_56ab1dba860c9" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Vasek_Smidl" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Vasek Smidl </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw39_56ab1dba860c9">  <li class="citation-context-item"> "Inference of GP covariance parameters is analytically intractable, and standard inference methods require repeatedly calculating the so called marginal likelihood . When the likelihood function is not Gaussian, e.g., in classification, in ordinal regression, in modeling of stochastic volatility, in Cox-processes, the marginal likelihood cannot be computed analytically, and this has motivated a large body of the literature to develop approximate inference methods [56] [43] [31] [47] [41] [24], reparameterization techniques [36] [34] [54] [14], and exact inference with unbiased computations of the marginal likelihood [12] [10]. Even in the case of a Gaussian likelihood, which makes the marginal likelihood computable, inference is generally costly because the computation of the marginal likelihood has time complexity scaling with the cube of the number of input vectors [11]. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes"> <span class="publication-title js-publication-title">Adaptive Multiple Importance Sampling for Gaussian Processes</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2079162962_Xiaoyu_Xiong" class="authors js-author-name ga-publications-authors">Xiaoyu Xiong</a> &middot;     <a href="researcher/82161005_Vaclav_Smidl" class="authors js-author-name ga-publications-authors">Václav Šmídl</a> &middot;     <a href="researcher/70871340_Maurizio_Filippone" class="authors js-author-name ga-publications-authors">Maurizio Filippone</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> In applications of Gaussian processes where quantification of uncertainty is
a strict requirement, it is necessary to accurately characterize the posterior
distribution over Gaussian process covariance parameters. Normally, this is
done by means of Markov chain Monte Carlo (MCMC) algorithms. Focusing on
Gaussian process regression where the marginal likelihood is computable but
expensive to evaluate, this paper studies algorithms based on importance
sampling to carry out expectations under the posterior distribution over
covariance parameters. The results indicate that expectations computed using
Adaptive Multiple Importance Sampling converge faster per unit of computation
than those computed with MCMC algorithms for models with few covariance
parameters, and converge as fast as MCMC for models with up to around twenty
covariance parameters. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Aug 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Vasek_Smidl/publication/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes/links/55e94dff08ae65b6389aee89.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw24_56ab1dba860c9" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw25_56ab1dba860c9">  </ul> </div> </div>   <div id="rgw16_56ab1dba860c9" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw17_56ab1dba860c9"> <div> <h5> <a href="publication/281487568_Transitional_annealed_adaptive_slice_sampling_for_Gaussian_process_hyper-parameter_estimation" class="color-inherit ga-similar-publication-title"><span class="publication-title">Transitional annealed adaptive slice sampling for Gaussian process hyper-parameter estimation</span></a>  </h5>  <div class="authors"> <a href="researcher/2076894930_A_Garbuno-Inigo" class="authors ga-similar-publication-author">A. Garbuno-Inigo</a>, <a href="researcher/84241605_F_A_DiazDelaO" class="authors ga-similar-publication-author">F. A. DiazDelaO</a>, <a href="researcher/81367029_K_M_Zuev" class="authors ga-similar-publication-author">K. M. Zuev</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56ab1dba860c9"> <div> <h5> <a href="publication/281058916_Approximations_of_Markov_Chains_and_High-Dimensional_Bayesian_Inference" class="color-inherit ga-similar-publication-title"><span class="publication-title">Approximations of Markov Chains and High-Dimensional Bayesian Inference</span></a>  </h5>  <div class="authors"> <a href="researcher/2079652947_James_E_Johndrow" class="authors ga-similar-publication-author">James E. Johndrow</a>, <a href="researcher/2079648813_Jonathan_C_Mattingly" class="authors ga-similar-publication-author">Jonathan C. Mattingly</a>, <a href="researcher/2079631589_Sayan_Mukherjee" class="authors ga-similar-publication-author">Sayan Mukherjee</a>, <a href="researcher/9530149_David_Dunson" class="authors ga-similar-publication-author">David Dunson</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56ab1dba860c9"> <div> <h5> <a href="publication/281605264_A_novel_multimode_process_monitoring_method_integrating_LDRSKM_with_Bayesian_inference" class="color-inherit ga-similar-publication-title"><span class="publication-title">A novel multimode process monitoring method integrating LDRSKM with Bayesian inference</span></a>  </h5>  <div class="authors"> <a href="researcher/2080702727_Shi-jin_Ren" class="authors ga-similar-publication-author">Shi-jin Ren</a>, <a href="researcher/2080735388_Yin_Liang" class="authors ga-similar-publication-author">Yin Liang</a>, <a href="researcher/2059068776_Xiang-jun_Zhao" class="authors ga-similar-publication-author">Xiang-jun Zhao</a>, <a href="researcher/2080701433_Mao-yun_Yang" class="authors ga-similar-publication-author">Mao-yun Yang</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw41_56ab1dba860c9" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw42_56ab1dba860c9">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw43_56ab1dba860c9" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=6evqR5dZ4ytqyVnDkH9AjmiVGvphgrqaYl__DfpX4Eoq_DTg0AIKfPiiH47xA0Ug" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="iFjewQG8CgEQq1qJa1BsolORYmztGdClOdmDAvXnrsWT/pDHay4M/62oYXN05tsYXGW5DMzQo3LJTNS7cwHCCrkMR592jLYBFmr2kEHZzPyOqcTWsQXz8DIBzlRHJBnd/S21xEi3/nT/DMn2eshKSvgE5hu0GiA5CLx3LYh2esyIN6ts8mj+e64BenLrCRjNiRkQb+ZIQ0E0FOh0v120XeFd8RitHZ3sMIFN56cenWqRVkMXRoAhsAOUyZ4+q6mlAoOVhHsyPkRzGntzsghYfZ3/+t5ldwcM9kL+YmVEics="/> <input type="hidden" name="urlAfterLogin" value="publication/41781429_Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vNDE3ODE0MjlfQXNzZXNzaW5nX0FwcHJveGltYXRlX0luZmVyZW5jZV9mb3JfQmluYXJ5X0dhdXNzaWFuX1Byb2Nlc3NfQ2xhc3NpZmljYXRpb24%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vNDE3ODE0MjlfQXNzZXNzaW5nX0FwcHJveGltYXRlX0luZmVyZW5jZV9mb3JfQmluYXJ5X0dhdXNzaWFuX1Byb2Nlc3NfQ2xhc3NpZmljYXRpb24%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vNDE3ODE0MjlfQXNzZXNzaW5nX0FwcHJveGltYXRlX0luZmVyZW5jZV9mb3JfQmluYXJ5X0dhdXNzaWFuX1Byb2Nlc3NfQ2xhc3NpZmljYXRpb24%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw44_56ab1dba860c9"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 650;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"profileUrl":"researcher\/34887298_Malte_Kuss","fullname":"Malte Kuss","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2549355721578\/images\/template\/default\/profile\/profile_default_m.png","profileStats":[{"data":{"impactPoints":"7.34","widgetId":"rgw5_56ab1dba860c9"},"id":"rgw5_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorImpactPoints.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorImpactPoints.html?authorUid=34887298","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationCount":13,"widgetId":"rgw6_56ab1dba860c9"},"id":"rgw6_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorPublicationCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorPublicationCount.html?authorUid=34887298","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},null],"widgetId":"rgw4_56ab1dba860c9"},"id":"rgw4_56ab1dba860c9","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorBadge.html?authorUid=34887298","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw3_56ab1dba860c9"},"id":"rgw3_56ab1dba860c9","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=41781429","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":41781429,"title":"Assessing Approximate Inference for Binary Gaussian Process Classification","journalTitle":"Journal of Machine Learning Research","journalDetailsTooltip":{"data":{"journalTitle":"Journal of Machine Learning Research","journalAbbrev":"J MACH LEARN RES","publisher":false,"issn":"1533-7928","impactFactor":"2.47","fiveYearImpactFactor":"4.77","citedHalfLife":"8.30","immediacyIndex":"0.31","eigenFactor":"0.03","articleInfluence":"3.23","widgetId":"rgw8_56ab1dba860c9"},"id":"rgw8_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=1533-7928","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"10\/2005;","publicationDateRobot":"2005-10","article":"6.","journalTitle":"Journal of Machine Learning Research","journalUrl":"journal\/1533-7928_Journal_of_Machine_Learning_Research","impactFactor":2.47}},"source":{"sourceUrl":"http:\/\/edoc.mpg.de\/270115","sourceName":"OAI"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Assessing Approximate Inference for Binary Gaussian Process Classification"},{"key":"rft.title","value":"Journal of Machine Learning Research, v.6, 1679-1704 (2005)"},{"key":"rft.jtitle","value":"Journal of Machine Learning Research, v.6, 1679-1704 (2005)"},{"key":"rft.volume","value":"6"},{"key":"rft.date","value":"2005"},{"key":"rft.issn","value":"1533-7928"},{"key":"rft.au","value":"M. Kuss,C.E. Rasmussen"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw9_56ab1dba860c9"},"id":"rgw9_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=41781429","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":41781429,"peopleItems":[{"data":{"authorUrl":"researcher\/34887298_M_Kuss","authorNameOnPublication":"M. Kuss","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"M. Kuss","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/34887298_M_Kuss","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56ab1dba860c9"},"id":"rgw12_56ab1dba860c9","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=34887298&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56ab1dba860c9"},"id":"rgw11_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=34887298&authorNameOnPublication=M.%20Kuss","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/43277170_CE_Rasmussen","authorNameOnPublication":"C.E. Rasmussen","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"C.E. Rasmussen","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/43277170_CE_Rasmussen","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw14_56ab1dba860c9"},"id":"rgw14_56ab1dba860c9","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=43277170&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw13_56ab1dba860c9"},"id":"rgw13_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=43277170&authorNameOnPublication=C.E.%20Rasmussen","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw10_56ab1dba860c9"},"id":"rgw10_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=41781429&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":41781429,"abstract":"<noscript><\/noscript><div>Gaussian process priors can be used to define flexible, probabilistic classification models. Unfortunately exact Bayesian inference is analytically intractable and various approximation techniques have been proposed. In this work we review and compare Laplace&lsquo;s method and Expectation Propagation for approximate Bayesian inference in the binary Gaussian process classification model. We present a comprehensive comparison of the approximations, their predictive performance and marginal likelihood estimates to results obtained by MCMC sampling. We explain theoretically and corroborate empirically the advantages of Expectation Propagation compared to Laplace&lsquo;s method.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw15_56ab1dba860c9"},"id":"rgw15_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=41781429","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/41781429_Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification\/links\/0e608558f0c46d4f0acd3c92\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw7_56ab1dba860c9"},"id":"rgw7_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=41781429&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2076894930,"url":"researcher\/2076894930_A_Garbuno-Inigo","fullname":"A. Garbuno-Inigo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":84241605,"url":"researcher\/84241605_F_A_DiazDelaO","fullname":"F. A. DiazDelaO","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":81367029,"url":"researcher\/81367029_K_M_Zuev","fullname":"K. M. Zuev","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Sep 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/281487568_Transitional_annealed_adaptive_slice_sampling_for_Gaussian_process_hyper-parameter_estimation","usePlainButton":true,"publicationUid":281487568,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/281487568_Transitional_annealed_adaptive_slice_sampling_for_Gaussian_process_hyper-parameter_estimation","title":"Transitional annealed adaptive slice sampling for Gaussian process hyper-parameter estimation","displayTitleAsLink":true,"authors":[{"id":2076894930,"url":"researcher\/2076894930_A_Garbuno-Inigo","fullname":"A. Garbuno-Inigo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":84241605,"url":"researcher\/84241605_F_A_DiazDelaO","fullname":"F. A. DiazDelaO","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":81367029,"url":"researcher\/81367029_K_M_Zuev","fullname":"K. M. Zuev","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/281487568_Transitional_annealed_adaptive_slice_sampling_for_Gaussian_process_hyper-parameter_estimation","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/281487568_Transitional_annealed_adaptive_slice_sampling_for_Gaussian_process_hyper-parameter_estimation\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab1dba860c9"},"id":"rgw17_56ab1dba860c9","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=281487568","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2079652947,"url":"researcher\/2079652947_James_E_Johndrow","fullname":"James E. Johndrow","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079648813,"url":"researcher\/2079648813_Jonathan_C_Mattingly","fullname":"Jonathan C. Mattingly","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079631589,"url":"researcher\/2079631589_Sayan_Mukherjee","fullname":"Sayan Mukherjee","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9530149,"url":"researcher\/9530149_David_Dunson","fullname":"David Dunson","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Aug 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/281058916_Approximations_of_Markov_Chains_and_High-Dimensional_Bayesian_Inference","usePlainButton":true,"publicationUid":281058916,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/281058916_Approximations_of_Markov_Chains_and_High-Dimensional_Bayesian_Inference","title":"Approximations of Markov Chains and High-Dimensional Bayesian Inference","displayTitleAsLink":true,"authors":[{"id":2079652947,"url":"researcher\/2079652947_James_E_Johndrow","fullname":"James E. Johndrow","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079648813,"url":"researcher\/2079648813_Jonathan_C_Mattingly","fullname":"Jonathan C. Mattingly","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079631589,"url":"researcher\/2079631589_Sayan_Mukherjee","fullname":"Sayan Mukherjee","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9530149,"url":"researcher\/9530149_David_Dunson","fullname":"David Dunson","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/281058916_Approximations_of_Markov_Chains_and_High-Dimensional_Bayesian_Inference","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/281058916_Approximations_of_Markov_Chains_and_High-Dimensional_Bayesian_Inference\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab1dba860c9"},"id":"rgw18_56ab1dba860c9","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=281058916","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2080702727,"url":"researcher\/2080702727_Shi-jin_Ren","fullname":"Shi-jin Ren","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2080735388,"url":"researcher\/2080735388_Yin_Liang","fullname":"Yin Liang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2059068776,"url":"researcher\/2059068776_Xiang-jun_Zhao","fullname":"Xiang-jun Zhao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2080701433,"url":"researcher\/2080701433_Mao-yun_Yang","fullname":"Mao-yun Yang","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Aug 2015","journal":"Frontiers of Information Technology & Electronic Engineering","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/281605264_A_novel_multimode_process_monitoring_method_integrating_LDRSKM_with_Bayesian_inference","usePlainButton":true,"publicationUid":281605264,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"0.42","url":"publication\/281605264_A_novel_multimode_process_monitoring_method_integrating_LDRSKM_with_Bayesian_inference","title":"A novel multimode process monitoring method integrating LDRSKM with Bayesian inference","displayTitleAsLink":true,"authors":[{"id":2080702727,"url":"researcher\/2080702727_Shi-jin_Ren","fullname":"Shi-jin Ren","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2080735388,"url":"researcher\/2080735388_Yin_Liang","fullname":"Yin Liang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2059068776,"url":"researcher\/2059068776_Xiang-jun_Zhao","fullname":"Xiang-jun Zhao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2080701433,"url":"researcher\/2080701433_Mao-yun_Yang","fullname":"Mao-yun Yang","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Frontiers of Information Technology & Electronic Engineering 08\/2015; 16(8):617-633. DOI:10.1631\/FITEE.1400263"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/281605264_A_novel_multimode_process_monitoring_method_integrating_LDRSKM_with_Bayesian_inference","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/281605264_A_novel_multimode_process_monitoring_method_integrating_LDRSKM_with_Bayesian_inference\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab1dba860c9"},"id":"rgw19_56ab1dba860c9","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=281605264","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw16_56ab1dba860c9"},"id":"rgw16_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=41781429&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":41781429,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":41781429,"publicationType":"article","linkId":"0e608558f0c46d4f0acd3c92","fileName":"Assessing Approximate Inference for Binary Gaussian Process Classification","fileUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.74.2668&amp;rep=rep1&amp;type=pdf","name":"psu.edu","nameUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.74.2668&amp;rep=rep1&amp;type=pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw22_56ab1dba860c9"},"id":"rgw22_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=41781429&linkId=0e608558f0c46d4f0acd3c92&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw21_56ab1dba860c9"},"id":"rgw21_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=41781429&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":0,"valueFormatted":"0","widgetId":"rgw23_56ab1dba860c9"},"id":"rgw23_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=41781429","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw20_56ab1dba860c9"},"id":"rgw20_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=41781429&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":41781429,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw25_56ab1dba860c9"},"id":"rgw25_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=41781429&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":0,"valueFormatted":"0","widgetId":"rgw26_56ab1dba860c9"},"id":"rgw26_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=41781429","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw24_56ab1dba860c9"},"id":"rgw24_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=41781429&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Journal of Machine Learning Research 6 (2005) 1679\u20131704Submitted 8\/05; Published 10\/05\nAssessing Approximate Inference for\nBinary Gaussian Process Classification\nMalte Kuss\nCarl Edward Rasmussen\nMax Planck Institute for Biological Cybernetics\nSpemannstra\u00dfe 38\n72076 T\u00a8 ubingen, Germany\nKUSS@TUEBINGEN.MPG.DE\nCARL@TUEBINGEN.MPG.DE\nEditor: Ralf Herbrich\nAbstract\nGaussian process priors can be used to define flexible, probabilistic classification models. Unfor-\ntunately exact Bayesian inference is analytically intractable and various approximation techniques\nhave been proposed. In this work we review and compare Laplace\u2019s method and Expectation Prop-\nagation for approximate Bayesian inference in the binary Gaussian process classification model.\nWe present a comprehensive comparison of the approximations, their predictive performance and\nmarginal likelihood estimates to results obtained by MCMC sampling. We explain theoretically and\ncorroborate empirically the advantages of Expectation Propagation compared to Laplace\u2019s method.\nKeywords: Gaussian process priors, probabilistic classification, Laplace\u2019s approximation, expec-\ntation propagation, marginal likelihood, evidence, MCMC\n1. Introduction\nIn recent years models based on Gaussian process (GP) priors have attracted much attention in the\nmachine learning community. Whereas inference in the GP regression model with Gaussian noise\ncan be done analytically, probabilistic classification using GPs is analytically intractable, see Ras-\nmussen and Williams (2006) for a general overview. Several approaches to approximate Bayesian\ninference have been suggested, including Laplace\u2019s method, Expectation Propagation (EP), varia-\ntional approximations and Markov chain Monte Carlo (MCMC) sampling, some of these in con-\njunction with generalisation bounds, online learning schemes and sparse approximations (e.g. Neal,\n1998; Williams and Barber, 1998; Gibbs and MacKay, 2000; Opper and Winther, 2000; Csat\u00b4 o and\nOpper, 2002; Seeger, 2002; Lawrence et al., 2003).\nDespite the abundance of recent work on probabilistic GP classifiers, most experimental studies\nprovide only anecdotal evidence, and no clear picture has yet emerged, as to when and why which\nalgorithm should be preferred. Thus, from a practitioners point of view it is unclear what the method\nof choice is for probabilistic GP classification. In this work, we set out to understand and compare\ntwo of the most wide-spread approximations: Laplace\u2019s method and Expectation Propagation (EP).\nWe also compare to a sophisticated, but computationally demanding MCMC scheme, which be-\ncomes exact in the limit of long running times. We do not address issues of sparsification but stick\nto comparing the two types of approximation.\nWe examine two aspects of the approximation schemes: Firstly the accuracy of approximations\ntothemarginallikelihoodwhichisofcentralimportanceformodelselectionandmodelcomparison.\nc ?2005 Malte Kuss and Carl Edward Rasmussen."},{"page":2,"text":"KUSS AND RASMUSSEN\nIn any practical application of GPs in classification (usually multiple) parameters of the covariance\nfunction (hyper-parameters) have to be handled. Bayesian model selection provides a consistent\nframework for setting such parameters. Therefore, it is essential to evaluate the accuracy of the\nmarginal likelihood approximations as a function of the hyper-parameters, in order to assess the\npractical usefulness of the approach. The related question of whether the marginal likelihood cor-\nrelates well with the generalisation performance cannot be answered in general but depends on the\nappropriateness of the model for a given data set. However, we do assess this empirically for two\ndata sets.\nSecondly, we need to assess the quality of the approximate probabilistic predictions. In the\npast, the probabilistic nature of the GP predictions has not received much attention, the focus being\nmostly on classification error rates. This unfortunate state of affairs is caused primarily by typical\nbenchmarking problems being considered outside of a realistic context. The ability of a classifier\nto produce class probabilities or confidences, have obvious relevance in most areas of application,\ne.g. medical diagnosis and ROC analysis. We evaluate the predictive distributions of the approxi-\nmate methods, and compare to the MCMC gold standard.\n2. The Gaussian Process Model for Binary Classification\nIn this section we describe the Gaussian process model for binary classification (GPC). Let y \u2208\n{\u22121,1} denote the class label corresponding to an input x. The GPC model is discriminative in the\nsense that it models p(y|x) which for fixed x is a Bernoulli distribution. The probability of success\np(y=1|x) is related to an unconstrained latent function f(x) which is mapped to the unit interval\nby a sigmoidal transformation, e.g. the logit or the probit. Both mappings are relatively similar\naround zero but show different tail behaviour. We will not examine the difference in this study.\nFor reasons of analytic convenience (for the EP algorithm) we exclusively use the probit model\np(y=1|x) = \u03a6(f(x)), where \u03a6 denotes the cumulative density function of the standard normal\ndistribution.\nIn the GPC model Bayesian inference is performed about the latent function f in the light of\nobserved data D = {(yi,xi)|i=1,...,m}. Let fi= f(xi) and f = [f1,..., fm]?be shorthand for the\nvalues of the latent function and y = [y1,...,ym]?and X = [x1,...,xm]?collect the class labels and\ninputs respectively.\nGiven the latent function, the class labels are independent Bernoulli variables, so the joint like-\nlihood factorises:\n\u220f\ni=1\np(y|f) =\nm\np(yi|fi)\n(1)\nand depends on f only through its value at the corresponding observed inputs. For the probit model\nthe individual likelihood terms become p(yi|fi) = \u03a6(yifi), due to the symmetry of \u03a6.\nAs prior over functions f we use a zero-mean Gaussian process (GP) prior (O\u2019Hagan, 1978).\nA GP is a stochastic process where each input x has an associated random variable f(x). The\njoint distribution of function values corresponding to any set of inputs X is multivariate Gaussian\np(f|X,\u03b8) = N (f|0,K). The covariance matrix is defined element-wise, Kij= k(xi,xj,\u03b8) where k\nis a positive definite covariance function parameterised by \u03b8. Note that by choosing a covariance\nfunction we introduce hyper-parameters \u03b8 to the prior. The zero-mean GP prior encodes that a\npriori p(y=1|x) = 1\/2 and certain further beliefs about the characteristics of the latent function.\n1680"},{"page":3,"text":"ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION\nFordetailsoncovariancefunctionsandtheirimplicationsontheprioroverfunctionsseeforexample\nAbrahamsen (1997) or Rasmussen and Williams (2006, ch. 4).\nUsing Bayes\u2019 rule the posterior distribution over the latent function values f for given hyper-\nparameters \u03b8 becomes:\np(f|D,\u03b8) =p(y|f)p(f|X,\u03b8)\np(D|\u03b8)\n=N (f|0,K)\np(D|\u03b8)\nm\n\u220f\ni=1\n\u03a6(yifi)\n(2)\nwhich is non-Gaussian. Properties of the posterior will be described in Section 5.\nThe main purpose of classification models is to predict the class label y\u2217for test inputs x\u2217. The\ndistribution of the latent function value can be computed by marginalisation:\np(f\u2217|D,\u03b8,x\u2217) =\nZ\np(f\u2217|f,X,\u03b8,x\u2217)p(f|D,\u03b8)df,\n(3)\nand by computing the expectation:\np(y\u2217|D,\u03b8,x\u2217) =\nZ\np(y\u2217|f\u2217)p(f\u2217|D,\u03b8,x\u2217)d f\u2217\n(4)\nthe predictive distribution is obtained, which is again a Bernoulli distribution. The first term in the\nright hand side of equation (3) is Gaussian and obtained by conditioning the joint Gaussian prior\ndistribution.\nUnfortunately, neither the posterior eq. (2) p(f|D,\u03b8), the predictive distribution eq. (4) p(y\u2217=\n1|D,\u03b8,x\u2217) nor the marginal likelihood eq. (7) p(D|\u03b8) can be computed analytically, so approxima-\ntions are needed. For the GPC model approximations are either based on a Gaussian approximation\nq(f|D,\u03b8) = N (f|m,A) to the posterior p(f|D,\u03b8) or involve Markov chain Monte Carlo (MCMC)\nsampling.\nA key insight is that a Gaussian approximation to the posterior implies a GP approximation to\nthe posterior process, which gives rise to an approximate predictive distribution for test cases. Intro-\nducingtheapproximateGaussianposteriorintoeq.(3)givestheapproximateposteriorq(f\u2217|D,\u03b8,x\u2217)=\nN (f\u2217|\u00b5\u2217,\u03c32\n\u00b5\u2217\n\u03c32\n\u2217\nwhere k\u2217= [k(x1,x\u2217),...,k(xm,x\u2217)]?is a vector of prior covariances between x\u2217and the training\ninputs X. For the probit likelihood the approximate predictive probability (4) of x\u2217belonging to\nclass 1 can be computed analytically:\n\u2217), with mean and variance:\n=\nk?\nk(x\u2217,x\u2217)\u2212k?\n\u2217K\u22121m\n(5a)\n(5b)\n=\n\u2217(K\u22121\u2212K\u22121AK\u22121)k\u2217,\nq(y\u2217=1|D,\u03b8,x\u2217) =\nZ\n\u03a6(f\u2217)N (f\u2217|\u00b5\u2217,\u03c32\n\u2217)d f\u2217= \u03a6\n?\n\u00b5\u2217\n1+\u03c32\n\u221a\n\u2217\n?\n.\n(6)\nThe parameters m and A of the posterior approximation can be found using Laplace\u2019s method\n(Section 3) or by Expectation Propagation (Section 4).\nWe have introduced the hyper-parameters \u03b8 which we considered to be fixed. Typically very\nlittle information about these parameters is available a priori. In principle inference should be done\njointly over f and \u03b8 which can only be approximated using Markov chain Monte Carlo sampling.\n1681"},{"page":4,"text":"KUSS AND RASMUSSEN\nHowever, a model selection approach can be implemented by selecting \u03b8 maximising the marginal\nlikelihood (evidence):\np(D|\u03b8) =\nwhich can be understood as a measure of the agreement between the model and observed data\n(Kass and Raftery, 1995; MacKay, 1999). This approach is called maximum likelihood II (ML-\nII) type hyper-parameter estimation and motivates the need for computing the marginal likelihood.\nLaplace\u2019s method as well as Expectation Propagation provide an approximation to the marginal\nlikelihood (7) and so approximate ML-II hyper-parameter estimation can be implemented in both\napproximation schemes.\nZ\np(y|f)p(f|X,\u03b8)df\n(7)\n3. Laplace\u2019s Method\nWilliamsandBarber(1998)describeLaplace\u2019smethodtofindaGaussianN (f|m,A)approximation\nto the posterior over latent function values (2) for fixed \u03b8 (although they use the logit likelihood).\nLet lnL(f) = lnp(y|f) denote the log likelihood and:\nlnQ(f|D,\u03b8) = lnL(f)\u22121\n2ln|K|\u22121\n2f?K\u22121f\u2212m\n2ln(2\u03c0)\n(8)\nthe unnormalised log posterior. Laplace\u2019s approximation is found by a second order Taylor expan-\nsion:\nlnQ(f|D,\u03b8) ? lnQ(m)\u22121\naround the mode of the (log) posterior:\n2(m\u2212f)?A\u22121(m\u2212f)\n(9)\nm = argmax\nf\u2208Rm\nlnQ(f|D,\u03b8).\n(10)\nSince both the likelihood and the prior are log-concave the posterior is also log-concave and uni-\nmodal. Let:\n\u2207flnQ\n\u2207\u2207flnQ\n=\n\u2207flnL(f)\u2212K\u22121f\n\u2207\u2207flnL(f)\u2212K\u22121\n(11a)\n(11b)\n=\ndenote the gradient and the Hessian. The mode is conveniently found using Newton\u2019s method,\niterating:\nf \u2190 f\u2212(\u2207\u2207flnQ(f))\u22121\u2207flnQ(f),\nwhich usually converges rapidly to m. The covariance matrix:\n(12)\nA = \u2212?\u2207\u2207flnQ(m)?\u22121= (K\u22121+W)\u22121\n(13)\nis approximated by the curvature at the mode, equal to the negative inverse Hessian, where W =\n\u2212\u2207\u2207flnL.\nThis approximation also facilitates an approximation to the marginal likelihood:\np(D|\u03b8) =\nZ\np(y|f)p(f|X,\u03b8)df =\nZ\nexp(lnQ(f))df.\n(14)\n1682"},{"page":5,"text":"ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION\nAlgorithm 1 Laplace\u2019s approximation for GPC\nGiven: \u03b8, D, x\u2217\nInitialise f (e.g. f \u2190 0), compute K from \u03b8 and X\nrepeat\nf \u2190 f\u2212(\u2207\u2207flnQ(f))\u22121\u2207flnQ(f)\nuntil convergence of f\nm \u2190 f\nA \u2190 (K\u22121\u2212\u2207\u2207flnQ(m))\u22121\nCompute log marginal likelihood lnq(D|\u03b8) by (15), and predictions q(y\u2217=1|D,\u03b8,x\u2217) using (6).\nSubstituting lnQ by its Taylor approximation (9) the Gaussian integral can be solved. The resulting\napproximate log marginal likelihood is:\nlnp(D|\u03b8) ? lnq(D|\u03b8) = lnQ(m)+m\n2ln(2\u03c0)+1\n2ln|A|\n(15)\nand the derivative of this quantity w.r.t. \u03b8 can be derived and used for optimisation (e.g. using conju-\ngate gradient methods) in an ML-II type setting. See Algorithm 1 for an overview and Appendix A\nfor details about our implementation.\n4. Expectation Propagation\nMinka (2001) proposed the iterative Expectation Propagation (EP) algorithm which can by applied\nto GPC. EP finds a Gaussian approximation q(f|D,\u03b8) = N (f|m,A) to the posterior p(f|D,\u03b8) by\nmoment matching of approximate marginal distributions. The starting point is an approximation\nmimicking the factorising structure:\np(f|D,\u03b8) =\np(f|X,\u03b8)\np(D|\u03b8)\nm\n\u220f\ni=1\np(yi|fi) ?\np(f|X,\u03b8)\nq(D|\u03b8)\nm\n\u220f\ni=1\nt(fi,\u00b5i,\u03c32\ni,Zi) = q(f|D,\u03b8),\n(16)\nwhere throughout we use p to denote exact quantities and q approximations, and the terms:\nt(fi,\u00b5i,\u03c32\ni,Zi) = ZiN (fi|\u00b5i,\u03c32\ni)\n(17)\nare called site functions. Note that the site functions are approximating the likelihood (which nor-\nmalizes over observations yi), with a Gaussian in fi, so we cannot expect the site functions to\nnormalize, hence the explicit term Ziis necessary. For notational convenience we hide the site pa-\nrameters \u00b5i, \u03c32\nand covariance:\niand Ziand write t(fi) instead. From (17) the Gaussian approximation (16) has mean\nq(f|D,\u03b8) = N (f|m,A), where m = A\u03a3\u22121\u00b5, and A = (K\u22121+\u03a3\u22121)\u22121,\nwhere \u00b5 = (\u00b51,...,\u00b5m)?and \u03a3 = diag(\u03c32\nrithm iteratively visits each site function in turn, and adjusts the site parameters to match moments\nof an approximation to the posterior marginals. The kth moment of fiunder the posterior is:\n(18)\n1,...,\u03c32\nm) collect site function parameters. The EP algo-\n?fk\ni? =\n1\np(D|\u03b8)\nZ\nfk\nip(y|f)p(f|X,\u03b8)df =\n1\np(D|\u03b8)\nZ\nfk\nip(yi|fi)p\\i(fi)d fi\n(19)\n1683"},{"page":6,"text":"KUSS AND RASMUSSEN\nwhere:\np\\i(fi) =\nZ\n\u220f\nj?=i\np(yj|fj)p(f|X,\u03b8)df\\i\n(20)\nis called the cavity distribution and f\\idenotes f without fi. The marginalisation required to compute\nthe exact cavity distribution is intractable for the GPC model. The key step in the EP algorithm is\nto replace the intractable exact cavity distribution with a tractable approximation based on the site\nfunctions:\nq\\i(fi) =\n\u220f\nj?=i\nTheapproximatecavityfunctioncomesintheformofanunnormalisedGaussianq\\i(fi)\u221dN (fi|\u00b5\\i,\u03c32\nMultiplying both sides by t(fi):\nZ\nt(fj)p(f|X,\u03b8)df\\i.\n(21)\n\\i).\nq\\i(fi)t(fi) =\nZ\nN (f|0,K)\nm\n\u220f\nj=1\nt(fj)df\\i\u221d N (fi|mi,Aii),\n(22)\nand basic Gaussian identities give the parameters:\n\u03c32\n\\i=?(Aii)\u22121\u2212\u03c3\u22122\ni\n?\u22121\nand\n\u00b5\\i= \u03c32\n\\i\n?mi\nAii\u2212\u00b5i\n\u03c32\ni\n?\n,\n(23)\nof the approximate cavity function.\nThe core idea of EP is to adjust the site parameters \u00b5i, \u03c3iand Ziso that the approximate posterior\nmarginal using the exact likelihood approximates as well as possible the posterior marginal based\non the site function:\nq\\i(fi)p(yi|fi) ? q\\i(fi)t(fi,\u00b5i,\u03c32\nby matching the zeroth, first and second moments. Recall that matching of moments minimizes\nKullback-Leibler (KL) divergence.1For the probit likelihood p(yi|fi) = \u03a6(yifi) the k = 0,1,2 mo-\nments of the left hand side can be computed analytically\ny\u00b5\\i\n\u221a\n1+\u03c32\n\\i\nm1\n=\n\u00b5\\i+\n\u03a6(z)y\ni,Zi)\n(24)\nm0\n=\n\u03a6\n?\n?\n= \u03a6(z),\n(25a)\n\u03c32\n\\iN (z|0,1)\n\u221a\n1+\u03c32\n\\i\n,\n(25b)\nm2\n=\n2\u00b5\\im1\u2212\u00b52\n\\i+\u03c32\n\\i\u2212z\u03c34\n\\iN (z|0,1)\n\u03a6(z)(1+\u03c32\n\\i),\n(25c)\nwhere z = y\u00b5\\i\/\nupdate equations for the site parameters become\n\u221a\n1+\u03c32\n\\i. By equating these moments with those of the right hand side of (24) the\n\u03c32\ni\n=\n?(m2\u2212m2\n\u03c32\ni\nm1(\u03c3\u22122\n1)\u22121\u2212\u03c3\u22122\n\\i+\u03c3\u22122\n\\i\n?\u22121,\n\u03c32\n?\n(26a)\n\u00b5i\n=\n?\n?\ni)\u2212\u00b5\\i\n\\i\n?\n(\u00b5i\u2212\u00b5\\i)2\n2(\u03c32\n,\n(26b)\nZi\n=\nm0\n2\u03c0(\u03c32\n\\i+\u03c32\ni)exp\n\\i+\u03c32\ni)\n?\n.\n(26c)\n1. Although, the classical KL argument only applies to the first and second (and higher) moments for normalized\ndistributions, it seems natural also to match zeroth moment.\n1684"},{"page":7,"text":"ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION\nAlgorithm 2 EP for Gaussian process classification\nGiven: \u03b8, D, x\u2217\nInitialise: A \u2190 K and site parameters \u03c32\nrepeat\nfor i=1,...,m do\nCompute parameters (23) of cavity\nCompute moments (25)\nUpdate the site parameters using (26)\nUpdate m and A according to (18)\nend for\nuntil The site parameters converged\nCompute log marginal likelihood lnq(D|\u03b8) by (27), and predictions q(y\u2217=1|D,\u03b8,x\u2217) using (6).\niand \u00b5i\nIn the application of EP, one may generally not have a guarantee that the new site variance in (26a)\nis non-negative; however, in the GPC model with probit likelihood, one can show that variance is\nalways positive. Once we have new values for \u00b5iand \u03c32\n(18), which in practise is done using rank-one updates, to save computation.\nThe EP algorithm iteratively updates the site parameters as shown in Algorithm 2. Although\nwe cannot prove the convergence of EP, we conjecture that it always converges for GPC with probit\nlikelihood, and have never encountered an exception.\nFinally the approximate log marginal likelihood can be obtained from the normalization of (16),\ngiving\niwe have to update m and A according to\nlnp(D|\u03b8) ? lnq(D|\u03b8) = ln\nZ\nq(f|X,\u03b8)\nlnZi\u22121\nm\n\u220f\ni=1\nt(fi)df\n(27)\n=\nn\n\u2211\ni=1\n2ln|K+\u03a3|\u22121\n2\u00b5?(K+\u03a3)\u22121\u00b5\u2212m\n2ln(2\u03c0).\nPerhaps this is not the standard way to compute an approximation to the marginal likelihood used\nelsewhere, butitseemsthemostnaturalgiventheapproximation. Thederivativesofthelogmarginal\nlikelihood can be computed in order to implement ML-II parameter estimation of \u03b8. Algorithm 2\nsummarises the computations, more details on implementing EP for GPC can be found in Ap-\npendix B.\n5. Structural Properties of the Posterior\nIn the previous sections we described the GPC model and two alternative approximation schemes\nfor finding a Gaussian approximation to the posterior. This section provides more details on the\nproperties of the posterior which is compared to the structure of the respective approximations.\nFigure 1(a) provides a one-dimensional illustration. The prior N (f|0,52) combined with the\nprobit likelihood (y = 1) results in a skewed posterior. Intuitively, the likelihood cuts off the f\nvalues which have the opposite sign of y. The mode of the posterior remains relatively close to the\norigin, while the mass is placed over positive values in accordance with the observation. Laplace\u2019s\napproximation peaks at the posterior mode, but places far too much mass over negative values of\nf and too little over large positive values. The EP approximation attempts to match the first two\n1685"},{"page":8,"text":"KUSS AND RASMUSSEN\n\u22124 \u22124004488\n0\n0.02\n0.04\n0.06\n0.08\n0.1\n0.12\n0.14\n0.16\np(f|y)\n0\n0.2\n0.4\n0.6\n0.8\n1\nf\np(y|f)\nLikelihood p(y|f)\nPrior p(f)\nPosterior p(f|y)\nLaplace q(f|y)\nEP q(f|y)\n??\n??\n?\n?\n(a)(b)\nFigure 1: Panel(a)providesaone-dimensionalillustrationofapproximations. ThepriorN (f|0,52)\ncombined with the probit likelihood (y = 1) results in a skewed posterior. The likelihood\nuses the right axis, all other curves use the left axis. In Panel (b) we caricature a high\ndimensional zero-mean Gaussian prior as an ellipse. The gray shadow indicates that for\na high dimension Gaussian most of the mass lies in a thin shell. For large latent signals,\nthe likelihood essentially cuts off regions which are incompatible with the training labels\n(hatched area), leaving the upper right orthant as the posterior. The dot represents the\nmode of the posterior, which is relatively unaffected by the truncation and remains close\nto the origin.\nposterior moments, which results in a larger mean and a more accurate placement of probability\nmass compared to Laplace\u2019s approximation.\nStructural properties of the posterior in higher dimensions can best be understood by examining\nits construction. The prior is a correlated m-dimensional Gaussian N (f|0,K) centred at the origin.\nEach likelihood term p(yi|fi) softly truncates the half-space from the prior that is incompatible with\nthe observed label, see Figure 1(b). The resulting posterior is unimodal and skewed, similar to a\nmultivariate Gaussian truncated to the orthant containing y. The mode of the posterior remains\nclose to the origin, while the mass is placed in accordance with the observed class labels. Addi-\ntionally, high dimensional Gaussian distributions exhibit the property that most probability mass is\ncontained in a thin ellipsoidal shell\u2014depending on the covariance structure\u2014away from the mean\n(MacKay, 2003, ch. 29.2). Intuitively this occurs since in high dimensions the volume grows ex-\ntremely rapidly with the radius. As an effect the mode becomes less representative (typical) for the\nprior distribution as the dimension increases. For the GPC posterior this property persists: the mode\nof the posterior distribution stays relatively close to the origin, still being unrepresentative for the\nposterior distribution, while the mean moves to the mass of the posterior making mean and mode\ndiffer significantly.\nAs described, we cannot generally assume the posterior to be close to Gaussian, as in the often\nstudied limit of low-dimensional parametric models with large amounts of data. Therefore in GPC\nwe must be aware of making a Gaussian approximation to a non-Gaussian posterior. Laplace\u2019s ap-\nproximation is centred around the mode of the posterior, which lies in the right orthant but too close\n1686"},{"page":9,"text":"ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION\n0\n1\n2\nx1\n30\n0.5\n1\n1.5\n2\n2.5\n3\n0\nx2\np(x1,x2)\n0123\nx1\np(x1)\n0246\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\nxi\np(xi)\n(a)(b)(c)\nFigure 2: Panel (a) illustrates a bivariate normal distribution truncated to the positive quadrant. The\nlines describe slices through the probability density function for fixed x2-values. Panel (b)\nshows the marginal distribution of p(x1) (thick line) obtained by (numerical) integration\nover x2, which\u2014intuitively speaking\u2014corresponds to an averaging of the slices (thin\nlines) from Panel (a). Panel (c) shows a histogram of samples of a marginal distribution\nof an high-dimensional truncated Gaussian. The line describes a Gaussian with mean and\nvariance estimated from the samples.\nto the origin, such that the approximation will overlap with regions having practically zero posterior\nmass. As an effect the amplitude of the approximate latent posterior GP will be underestimated\nsystematically, leading to overly cautious predictive distributions.\nThe EP approximation does not rely on a local expansion, but assumes that the marginal distri-\nbutions of the posterior can be well approximated by Gaussians. As described above the posterior\nis similar to a high dimensional multivariate normal distribution truncated to one orthant. Although\nthe posterior is skew and truncated, marginals of such a distribution can be relatively similar to a\nGaussian.\nAs a low dimensional illustration the marginal distribution of a bivariate normal is shown in\nFigure 2(a-b). Depending on the covariance structure, the mode of the marginal distribution moves\naway from the origin and the distribution appear similar to a truncated univariate Gaussian.\nIn order to inspect the marginals of a truncated high-dimensional multivariate normal distri-\nbution we made an additional synthetic experiment. We constructed a 767 dimensional Gaussian\nN (x|0,C) with a covariance matrix having one eigenvalue of 100 with eigenvector 1, and all other\neigenvalues are 1. We then truncate this distribution such that all xi\u2265 0. Note that the mode\nof the truncated Gaussian is still at zero, whereas the mean moved towards the remaining mass.\nMetropolis-Hastings sampling was used to generate samples from this truncated multivariate distri-\nbution. Figure 2(c) shows a normalised histogram of samples from a marginal distribution of one\nxi. The samples agree very well with a Gaussian approximation. Note that Laplace\u2019s method would\nbe completely inappropriate for approximating a truncated multivariate normal distribution.\nIn order to validate the above arguments we will use Markov chain Monte Carlo methods to\ngenerate samples from the posterior and also to estimate the marginal likelihood.\n1687"},{"page":10,"text":"KUSS AND RASMUSSEN\n6. Markov Chain Monte Carlo\nMarkov chain Monte Carlo (MCMC) may be too slow for many practical applications, but has the\nadvantage that it becomes exact in the limit of long runs. Thus, MCMC can provide a gold standard\nby which to measure the two analytic methods of the previous sections. Computing the predictions\nvia an MCMC estimate of (3) and (4) is relatively straight forward and covered in Section 6.1.\nGood MCMC estimates of the marginal likelihood are, however, notoriously difficult to obtain,\nbeing equivalent to the free-energy estimation problem in physics (Gelman and Meng, 1998). In\nSection 6.2 we explain the use of Annealed Importance Sampling (AIS), which can be seen as a\nsophisticated elaboration of Thermodynamic Integration, for this task.\n6.1 Hybrid MCMC Sampling\nHybrid Monte Carlo (HMC) sampling as proposed by Duane et al. (1987) is a computationally\nefficient sampling technique which exploits gradient information of the target distribution. Detailed\naccounts are given by Neal (1993, ch. 5.2) and Liu (2001, ch. 9). MacKay (2003, ch. 30) also\nprovides pseudo-code; we do not repeat the details here.\nHMCcanbeusedtogeneratesamplesfromtheposterior p(f|\u03b8,D), whileonlytheunnormalised\nlog posterior (8) and its derivatives are required. As described in the previous section, the exact\nposterior (2) takes the form of a (correlated) Gaussian (the GP prior), which is (softly) truncated by\nthe constraints imposed by the training labels through the likelihood. To ease the sampling task by\nreducing correlations, we first do a linear transformation into new g = L\u22121f variables, such that g is\nwhite w.r.t. K, where K = LL?is the Cholesky decomposition. Given samples from the posterior,\nwe generate test-latents from the Gaussian p(f\u2217|f,X,\u03b8,x\u2217) for use in a simple Monte Carlo estimate\nof (4).\n6.2 Annealed Importance Sampling\nThe marginal likelihood (7) comes in the form of an m dimensional integral where m is the number\nof data points. A simple approach would be to use importance sampling with the EP or Laplace\u2019s\napproximation of the posterior as proposal distribution. However, for the GPC model the resulting\nimportance weights show enormous variances, making simple importance sampling useless for this\ntask (MacKay, 2003, ch. 29).\nNeal (2001) describes Annealed Importance Sampling (AIS), which we will use to estimate the\nmarginal likelihood in the GPC model. Instead of solving the integral (7) directly, a sequence of\neasier quantities is computed. We define:\nZt =\nZ\np(y|f)\u03c4(t)p(f|X,\u03b8)df\n(28)\nwhere \u03c4(t) is an inverse temperature schedule such that \u03c4(0) = 0 and \u03c4(T) = 1. The trick is to\nrewrite the marginal likelihood Z = p(D|\u03b8) as a fraction and expand:\nZ =ZT\nZ0\n=\nZT\nZT\u22121\nZT\u22121\nZT\u22122\u00b7\u00b7\u00b7Z1\nZ0,\n(29)\n1688"},{"page":11,"text":"ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION\nAlgorithm 3 Annealed Importance Sampling\nGiven: Temperature schedule \u03c4\nfor r = 1,...,R do\nSample f0from the prior N (f|0,K)\nfor t = 1,...,T do\nSample ftfrom q(f|D,\u03c4(t),\u03b8) by HMC\nCompute ln(Zt\/Zt\u22121) using (31)\nend for\nCompute Zrusing (32)\nend for\nReturn lnZ = ln?1\nwhere Z0= 1 since the prior normalises. Each term in (29) is approximated using importance\nsampling using samples from q(f|D,\u03b8,\u03c4(t)) \u221d p(y|f)\u03c4(t)p(f|X,\u03b8):\nZt\nZt\u22121\n1\nS\n\u2211\ni=1\nwhere fiare samples from q(f|D,\u03b8,\u03c4(t)), which we generate using HMC. Using a single sample\nS = 1 and a large number of temperatures, the log of each ratio is:\nR\u2211R\nr=1Zr\n?\n=\nZ\np(y|f)\u03c4(t)p(f|X,\u03b8)\np(y|f)\u03c4(t\u22121)p(f|X,\u03b8)q(f|D,\u03b8,\u03c4(t \u22121))df\nS\np(y|fi)\u03c4(t)\u2212\u03c4(t\u22121)\n(30a)\n?\n(30b)\nln(Zt\/Zt\u22121) ?\n?\u03c4(t)\u2212\u03c4(t \u22121)?lnp(y|ft)\n(31)\nwhere ftis the only sample at temperature \u03c4(t). Combining (29) with (31) we obtain the desired:\nlnZ ?\nT\n\u2211\nt=1\nln(Zt\/Zt\u22121).\n(32)\nIn all our experiments we use \u03c4(t) = (t\/T)4for t = 0,...,8000. Using this temperature schedule\nwe found that the sampling spends most of its efforts at temperatures with high variance of (31)\nsuch that the variance of (32) is relatively small. Note that this was only examined on the data\nsets we use below and only for certain values of \u03b8. So far, we have described Thermodynamic\nIntegration, which gives an unbiased estimate in the limit of slow temperature changes. In AIS the\nbias caused by finite temperature schedules is removed by combining multiple estimates by their\ngeometric mean (see Algorithm 3). In the experiments we combine the estimates of R = 3 runs of\nThermodynamic Integration.\n7. Experiments\nIn this section we compare and inspect approximations for GPC using various benchmark data sets.\nThe primary focus is not to optimise the absolute performance of GPC models but to compare the\nrelative accuracy of approximations and to validate the arguments given in Section 5.\nIn all the GPC experiments we use a covariance function of the form:\nk(x,x?,\u03b8) = \u03c32exp?\u22121\n2?2\n??x\u2212x???2?,\n(33)\n1689"},{"page":12,"text":"KUSS AND RASMUSSEN\n\u22128 \u22126\u22124 \u22122024\n0\n0.2\n0.4\n0.6\n0.8\n1\nx\n(a)\np(y = 1|x)\nClass 1\nClass \u22121\nLaplace p(y|X)\nEP p(y|X)\nTrue p(y|X)\n\u22128 \u22126 \u22124 \u22122024\n\u221210\n\u22125\n0\n5\n10\n15\nx\nf(x)\nLaplace p(f|X)\nEP p(f|X)\n(b)\nFigure 3: Synthetic classification problem: Panel (a) illustrates the classification task, the gen-\nerating p(y|x) and two approximations thereof obtained by Laplace\u2019s method and\nEP. Panel (b) illustrates the approximate predictive distributions p(f\u2217|D,\u03b8,x\u2217) ?\nN (f\u2217|\u00b5\u2217,\u03c32\n\u2217) of latent function values showing the mean \u00b5\u2217and the range of \u00b12\u03c3\u2217.\nsuch that \u03b8 = [\u03c3,?]. We refer to \u03c32as the signal variance and to ? as the characteristic length-\nscale. Note that for many classification tasks it may be reasonable to use an individual length scale\nparameter for every input dimension (ARD). Nevertheless, for the sake of presentability we use the\nabove covariance function and we believe the conclusions to be independent of this choice.\nBoth analytic approximations have a computational complexity which is cubic O(m3) as com-\nmon among non-sparse GP models due to inversions m\u00d7m matrices. In our implementations\nLaplace\u2019s method and EP need similar running times, on the order of a few minutes for several\nhundred data-points. Making AIS work efficiently requires some fine-tuning and a single estimate\nof p(D|\u03b8) can take several hours for data sets of a few hundred examples, but this could conceivably\nbe improved upon.\n7.1 Synthetic Classification Problem\nThe first experiment is a synthetic classification problem with scalar inputs. The observations for\nclass 1 were generated from two normal distributions with means \u22126 and 2, each with a standard\ndeviation of 0.8. For class \u22121 the mean is 0 and the same standard deviation was used.\n1690"},{"page":13,"text":"ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION\nWe computed Laplace\u2019s and the EP approximation for the ML-II estimated value of \u03b8 that max-\nimised Laplace\u2019s approximation to the marginal likelihood (15). Note that this particular choice of \u03b8\nshould be in favour of Laplace\u2019s method. Figure 3 shows the resulting classifiers and the underlying\nlatent functions. In Figure 3(a) the approximations to p(y|x) appear to be similar for positive x but\nwe observe an appreciable discrepancy for negative values. Laplace\u2019s approximation gives an un-\nreasonably high predictive uncertainty, which is caused by a significant overlap of the approximate\npredictive distribution p(f\u2217|D,\u03b8,x\u2217) ? N (f\u2217|\u00b5\u2217,\u03c32\nnote that both approximations agree on the sign of the predictive mean.\n\u2217) with zero as shown in Figure 3(b). However,\n7.2 Ionosphere Data\nThe data consists of 351 examples in 34 dimensions. We standardised the inputs X to zero mean\nand unit variance. The training set is a random subset of size m = 200 leaving the remaining 151\ninstances out as a test set.\nWedoanexhaustiveinvestigationonaregular21\u00d721gridofvaluesfortheloghyper-parameters.\nFor each \u03b8 on the grid we compute the approximated log marginal likelihood by Laplace\u2019s method\n(15), EP (27) and AIS. Additionally we compute the predictive performance on the test set. As\nperformance measure we use the average information in bits of the predictions about the test targets\nin excess of that of random guessing. Let p\u2217= p(y\u2217=1|x\u2217) be the model\u2019s prediction, then we\naverage:\nI(p\u2217\ni,yi) =yi+1\n2log2(p\u2217\ni)+1\u2212yi\n2log2(1\u2212 p\u2217\ni)+H\n(34)\nover all test cases, where H is the entropy of the training set labels. Results are shown in Figure 4.\nFor all three approximation techniques we see an agreement between marginal likelihood esti-\nmates and test performance, which justifies the use of ML-II parameter estimation. But the shape of\nthe contours and the values differ between the methods. The contours for Laplace\u2019s method appear\nto be slanted compared to EP. The estimated marginal likelihood estimates of EP and AIS agree\nvery well.2The EP predictions contain as much information about the test cases as the MCMC\npredictions and significantly more than for Laplace\u2019s method.\nNote that for small signal variances (roughly ln(\u03c32) < 0) Laplace\u2019s method and EP give very\nsimilar results. A possible explanation is that for small signal variances the likelihood does not\ntruncate the prior but only down-weights the tail that disagrees with the observation. As an effect\nthe posterior will be less skewed and both approximations will lead to similar results.\n7.3 USPS Digits\nWe define a binary sub-problem from the USPS digit data3by considering 3\u2019s vs. 5\u2019s. We repeated\nthe experiments described in the previous section for a slightly modified grid of \u03b8. Comparing the\nresults shown in Figure 5 leads to similar results as mentioned above. The EP and MCMC results\nagree very well, given that the marginal likelihood comes as a 767 dimensional integral.\nWe now take a closer look at the approximations q(f|D,\u03b8) =N (f|m,A) for a given value of \u03b8.\nWe have chosen the values ln(\u03c3)=3.35 and ln(?)=2.85 which are between the ML-II estimates of\nEP and Laplace\u2019s method. Comparing the respective means of the approximations in Figure 6(a) we\n2. Note that the agreement between the two seems to be limited by the accuracy of the AIS runs, as judged by the\nregularity of the contour lines; the tolerance is less than one unit on a (natural) log scale.\n3. Because the training and test partitions in the original data differ significantly, we pooled cases and randomly divided\nthem into new sets, with 767 cases for training and 773 for testing.\n1691"},{"page":14,"text":"KUSS AND RASMUSSEN\nLog marginal likelihood Information about test targets\nLaplace\u2019s Approximation\n\u2212200\n\u2212150\n\u2212120\n\u2212120\n\u2212100\n\u2212100\n\u221290\n\u221280\n\u221275\n\u221270\nlog lengthscale, log(l)\nlog magnitude, log(\u03c3f)\n\u22121012345\n\u22121\n0\n1\n2\n3\n4\n5\n0\n0\n0.1\n0.1\n0\n0.2\n0.2\n0.3\n0.3\n0.4\n0.4\n0.5\n0.55\nlog lengthscale, log(l)\nlog magnitude, log(\u03c3f)\n\u2212112345\n\u22121\n0\n1\n2\n3\n4\n5\nExpectation Propagation\n\u2212120\n\u2212120\n\u2212100\n\u2212100\n\u221290\n\u221290\n\u221280\n\u221275\n\u221275\n\u221270\n\u221265\nlog lengthscale, log(l)\nlog magnitude, log(\u03c3f)\n\u22121012345\n\u22121\n0\n1\n2\n3\n4\n5\n0\n0\n0.1\n0.1\n0.2\n0.2\n0.3\n0.3\n0.4\n0.5\n0.5\n0.55\n0.6\nlog lengthscale, log(l)\nlog magnitude, log(\u03c3f)\n\u22121012345\n\u22121\n0\n1\n2\n3\n4\n5\nMarkov chain Monte Carlo\n\u2212120\n\u2212120\n\u2212100\n\u2212100\n\u221290\n\u221290\n\u221280\n\u221275\n\u221275\n\u221270\n\u221265\nlog lengthscale, log(l)\nlog magnitude, log(\u03c3f)\n\u22121012345\n\u22121\n0\n1\n2\n3\n4\n5\n0\n0\n0.1\n0.1\n0.2\n0.2\n0.3\n0.3\n0.4\n0.5\n0.5\n0.5\n0.5\n0.55\n0.6\nlog lengthscale, log(l)\nlog magnitude, log(\u03c3f)\n\u22121012345\n\u22121\n0\n1\n2\n3\n4\n5\nFigure 4: Comparison of marginal likelihood approximations and predictive performances for the\nIonosphere data set. The first column shows the estimates of log marginal likelihood,\nwhile the second column shows the performance on the test set measured by the informa-\ntion about test targets in bits (34).\n1692"},{"page":15,"text":"ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION\nLog marginal likelihood Information about test targets\nLaplace\u2019s Approximation\n\u2212200\n\u2212200\n\u2212150\n\u2212130\n\u2212115\n\u2212105\n\u2212100\nlog lengthscale, log(l)\nlog magnitude, log(\u03c3f)\n2345\n0\n1\n2\n3\n4\n5\n0.25\n0.25\n0.5\n0.5\n0.7\n0.7\n0.8\n0.8\n0.84\nlog lengthscale, log(l)\nlog magnitude, log(\u03c3f)\n2345\n0\n1\n2\n3\n4\n5\nExpectation Propagation\n\u2212200\n\u2212200\n1\n\u2212160\n\u2212160\n\u2212130\n\u2212130\n\u2212115\n\u2212105\n\u2212105\n\u2212100\n\u221295\n\u221292\nlog lengthscale, log(l)\nlog magnitude, log(\u03c3f)\n2345\n0\n2\n3\n4\n5\n0.25\n0.5\n0.7\n0.7\n0.8\n0.8\n0.84\n0.84\n0.86 0.86\n0.88\n0.89\nlog lengthscale, log(l)\nlog magnitude, log(\u03c3f)\n2345\n0\n1\n2\n3\n4\n5\nMarkov chain Monte Carlo\n2345\n0\n1\n2\n3\n4\n5\nlog lengthscale, log(l)\nlog magnitude, log(\u03c3f)\n\u221292\u221295\u2212100\n\u2212105\n\u2212105\n\u2212115\n\u2212130\n\u2212160\n\u2212160\n\u2212200\n\u2212200\n0.25\n0.5\n0.7\n3\n0.7\n0.8\n0.84\n0.84\n0.86\n0.86\n0.88\n0.89\nlog lengthscale, log(l)\nlog magnitude, log(\u03c3f)\n2345\n0\n1\n2\n4\n5\nFigure 5: Comparison of marginal likelihood approximations and predictive performances of the\ndifferent methods for classifying 3\u2019s vs. 5\u2019s from the USPS image database. The plots are\narranged as in Figure 4.\n1693"},{"page":16,"text":"KUSS AND RASMUSSEN\n\u221240\u22122002040\n\u221240\n\u221230\n\u221220\n\u221210\n0\n10\n20\n30\n40\nm Laplace\n(a)\nm EP\nClass 5\nClass 3\n\u221230\u221220\nln(w) Laplace\n(b)\n\u2212100\n\u221220\n\u221215\n\u221210\n\u22125\n0\n\u2212ln(\u03c32) EP\nClass 5\nClass 3\n00.51\n0\n0.5\n1\np* MCMC\n(c)\np* Laplace & EP\nLaplace p*\nEP p*\n00.51\n0\n0.5\n1\np* MCMC\n(d)\np* Laplace & EP\nLaplace p*\nEP p*\nFigure 6: Comparison of approximations q(f|D,\u03b8) = N (f|m,A) for a given value of \u03b8. Panel (a)\nshows a comparison of the means mi. In Panel (b) we compare the elements of the diago-\nnal matrices Wiiand \u03a3ii. Panels (c) and (d) compare predictions p\u2217obtained by MCMC\n(abscissa) to predictions obtained from Laplace\u2019s method and EP (ordinate). Panel (c)\nshows predictions on training cases and (d) shows predictions on test cases.\nsee that the magnitude of the means from the Laplace approximation is much smaller than from EP.\nThe relation appears to be roughly linear. In Figure 6(b) we compare the elements of W and \u03a3\u22121\nwhich cause the difference in the approximations (13) and (18) of the posterior covariance matrix A.\nWe observe that the relatively large entries in W are larger than the corresponding entries in \u03a3\u22121,\nbut in total W contains more small values than \u03a3\u22121. The exact effect on the posterior covariance\nis difficult to characterise due to the inversion, but intuitively the smaller the values the more the\nposterior covariance will be similar to the prior.\n1694"},{"page":17,"text":"ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION\nFigures 6(c-d) compare the predictive uncertainty p\u2217resulting from the respective approxima-\ntions to MCMC predictions. For both training and test set we observe that EP and MCMC agree\nvery well, while Laplace\u2019s method shows over-conservative predictions.\n\u221215\u221210\u2212505\n0\n0.05\n0.1\n0.15\n0.2\nf\nMCMC samples\nLaplace p(f|D)\nEP p(f|D)\n\u221240\u221230\u221220\u221210010\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nf\nMCMC samples\nLaplace p(f|D)\nEP p(f|D)\n(a)(b)\nFigure 7: Two marginal distributions p(fi|D,\u03b8) from the posterior. For Panel (a) we picked the\nfifor which the posterior marginal is maximally skewed (see again Figure 1). The true\nposterior is approximated by a normalised histogram of 9000 samples of fiobtained by\nMCMC sampling. Panel (b) shows a case where EP and Laplace\u2019s approximation differ\nsignificantly.\nWe now inspect the marginal distributions p(fi|D,\u03b8) of single latent function values under the\nposterior approximation. We use hybrid MCMC to generate 9000 samples from the posterior of f\nfor the above \u03b8. For Laplace\u2019s method and EP the approximated distribution is N (fi|mi,Aii) where\nm and A are found by the respective approximation techniques.\nIn general we observe that the marginal distributions of MCMC samples agree very well with\nthe respective marginal distributions of the EP approximation. This supports the claim made in\nSection 5 where we argued that the marginal distributions of the posterior can be very similar to\nGaussians, even if the posterior is a skew distribution. For Laplace\u2019s approximation we find the\nmean to be underestimated and the marginal distributions to overlap with zero far more than the\nEP approximations. Figure 7(a) displays the marginal distribution and its approximations for which\nthe MCMC samples show maximal skewness. Figure 7(b) shows a typical example where the\nEP approximation agrees very well with the MCMC samples. We show this particular example\nbecause under the EP approximation q(yi= 1|D,\u03b8) < 0.1% but Laplace\u2019s approximation gives\nq(yi= 1|D,\u03b8) ? 18%.\n7.4 Lower Bound Approximation\nIn the context of sparse EP approximations Seeger (2003) proposed a lower bound on the marginal\nlikelihood. The bound is obtained from the EP approximation of the posterior using Jensen\u2019s in-\n1695"},{"page":18,"text":"KUSS AND RASMUSSEN\n\u2212250\n\u2212200\n\u2212150\n\u2212120\n\u2212120\n\u2212100\n\u2212100\n\u221290\n\u221290\n\u221280\n\u221280\n\u221275\n\u221275\n\u221270\nlog lengthscale, log(l)\n(a) Ionosphere\nlog magnitude, log(\u03c3f)\nLower bound on log marginal likelihood\n\u22121012345\n\u22121\n0\n1\n2\n3\n4\n5\n\u2212200\n\u2212200\n\u2212160\n\u2212160\n\u2212130\n\u2212130\n\u2212115\n\u2212115\n\u2212105\n\u2212100\nlog lengthscale, log(l)\n(b) USPS 3\u2019s vs. 5\u2019s\nlog magnitude, log(\u03c3f)\nLower bound on log marginal likelihood\n2345\n0\n1\n2\n3\n4\n5\nFigure 8: Lower bound on marginal likelihood. Panel (a) shows the lower bound eq. (35) on the\nmarginal likelihood for the Ionosphere data set (compare to left column of Figure 4).\nPanel (b) shows the value of the lower bound for the USPS 3\u2019s vs. 5\u2019s (compare to left\ncolumn of Figure 5)\nequality:\nlnp(D|\u03b8)=\nln\nZ\nN (f|m,A)lnp(y|f)N (f|0,K)\n\u2211\ni=1\n\u22121\np(y|f)N (f|0,K)df\n(35a)\n\u2265\nZ\nN (f|m,A)\ndf\n(35b)\n=\nm\nZ\nN (fi|mi,Aii)ln\u03a6(yifi)d fi\n2m?K\u22121m\u22121\n2tr(K\u22121A)+1\n2ln|K\u22121A|+m\n2.\n(35c)\nNote that the one dimensional integrals in eq. (35c) have to be solved using numerical integration\nmethods.\nIn sparse EP methods the Gaussian approximation is based on only a subset of observations\nand so the evidence (27) may be a bad approximation of the total evidence since it does not take\nall available data into account. Assume that the m points are only a subset of of a total of m?\nobservations. The lower bound (35c) can be extended to a lower bound on all m?observations by\nincluding all points in the one dimensional integrals over the individual log likelihood terms.\nSeveralauthorsmaximisethislowerboundinsteadofmaximising(27)forML-IIhyper-parameter\nestimation also in the case of non-sparse EP approximations, e.g. Chu and Ghahramani (2005). In\nFigure 8 we show the value of the lower bound as a function of the hyper-parameters for the Iono-\nsphere and USPS data described in the previous sections (for the full EP approximation). Interest-\ningly, for both data sets the lower bounds appear to be more similar to the approximate evidence\nobtained by Laplace\u2019s method than by EP (compare to the upper left panel in Figures 4 and 5\n1696"},{"page":19,"text":"ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION\nrespectively). However, the maxima of the lower bounds correspond to sub-optimal predictive per-\nformances compared to the maxima of the approximate marginal likelihood (27) (compare to the\nsecond row in Figures 4 and 5 respectively). Therefore for non-sparse EP approximations the use\nof (27) seems advisable, which is also computationally advantageous.\n7.5 Benchmark Data Sets\nIn this section we compare the performance of Laplace\u2019s method and Expectation Propagation for\nGPC on several well known benchmark problems for binary classification.\nThe Ionosphere, the Wisconsin Breast Cancer, and the Sonar data sets are taken from Hettich\net al. (1998). The Leptograpsus Crabs and the Pima Indians Diabetes data sets were described by\nRipley (1996). Note that for the Crabs data set we use the sex (not the colour) of the crabs as target\nvariable. The largest data set in the comparison are the 3\u2019s vs. 5\u2019s from the USPS handwritten digits\ndescribed above.\nWe standardise the inputs X to zero mean and unit variance. All data sets are randomly split\ninto 10 folds of which one at a time is left out as a test set to measure the predictive performance of\na model trained (or selected) on the remaining nine folds.\nFor GPC we implement model selection by ML-II hyper-parameter estimation. We use a con-\njugate gradient optimisation routine to find a minimum\n\u03b8ML= argmin\n\u03b8\n\u2212lnq(D|\u03b8)\n(36)\nof the negative log marginal likelihood approximated by Laplace\u2019s method (15) and EP (27) respec-\ntively. Fortherespective\u03b8MLtheapproximationsN (f|m,A)arecomputedandpredictionsaremade\nfor the left out test set. From the predictive distributions the average information (34) is computed\nand averaged over the ten folds. Furthermore the average error rate E is reported, which equals the\naverage percentage of erroneous class assignments if prediction is understood as a decision problem\nwith symmetric costs (thresholding the predictive uncertainty at 1\/2).\nIn order to have a better absolute impression of the predictive performance we report the results\nof support vector machines (SVM) (Sch\u00a8 olkopf and Smola, 2002). We use the LIBSVM implemen-\ntation of C-SVM by Chang and Lin (2001) with a radial basis function kernel which is equivalent\nto the covariance function (33) up to the signal variance parameter. The values of the length scale\nparameter ? and the regularisation parameterC are found by an inner loop of 5-fold cross-validation\non the nine training folds respectively. We manually refine the parameter grids and repeat the cross-\nvalidation procedure until the performance stabilises.\nWe use the technique described by Platt (2000) to estimate predictive probabilities from an\nSVM. This is implemented by fitting a sigmoidal mapping from the unthresholded output of the\nSVM to the unit interval. The parameters of the mapping are estimated on the test set in the inner\nloop of 5-fold cross-validation.\nResults are summarised in Table 1. Comparing Laplace\u2019s method to EP the latter shows to be\nmore accurate both in terms of error rate and information. While the error rates are relatively similar\nthe predictive distribution obtained by EP shows to be more informative about the test targets. As\nto be expected by now, the length of the mean vector ?m? shows much larger values for the EP\napproximations. Comparing EP and SVM the results are mixed.\nAt first sight it may seem surprising that Laplace\u2019s method gives relatively similar error rates\ncompared to EP. Note that for both methods the error rate only depends on the sign of the latent\n1697"},{"page":20,"text":"KUSS AND RASMUSSEN\nLaplace\nI\nEPSVM\nData Set\nIonosphere\nWisconsin\nPima Indians\nm\n351\n683\n768\n200\n208\nn\n34\n9\n8 22.77 0.252\n72.0 0.682 112.34\n60 15.36 0.439\n2.27 0.849 163.05\nE\n?m?\n49.96\n62.62\n29.05 22.63 0.253\n2.0 0.908\n26.86 13.85 0.537 15678.55 11.14 0.567\n2.21 0.902 22011.70\nEI\n?m?\n124.94\n84.95\n47.49 23.01 0.232\n2552.97\nEI\n8.84 0.591\n3.21 0.804\n7.99 0.661\n3.21 0.805\n5.69 0.681\n3.21 0.795\nCrabs\nSonar\n2.0 0.047\nUSPS 3 vs 5 1540 2562.01 0.918\nTable 1: Results for benchmark data sets. The first three columns give the name of the data set,\nnumber of observation m and dimension of inputs n. For Laplace\u2019s method and EP the\ntable reports the average error rate E, the average information I (34) and the average length\n?m? of the mean vector of the Gaussian approximation. For SVMs the error rate and the\naverage information about the test targets are reported.\nmean function (5a) at the test locations, which in turn depend on m only. Therefore the error rate\nis less sensitive to the accuracy of the approximation to the posterior, but of course depends on the\nML-II estimated hyper-parameters, which differ between the methods. Also in the example shown\nin Figure 3(b) it can be observed that the latent mean functions differ but their sign matches very\naccurately.\nFor the Crabs data set all methods show the same error rate but the information content of the\npredictive distributions differs dramatically. For some test cases the SVM predicts the wrong class\nwith large certainty. Because the mapping of the unthresholded output of the SVM to the predictive\nprobability is estimated from a left out set, the mapping can be poor if too few errors are observed\non this.\n8. Conclusions\nOur experiments reveal serious differences between Laplace\u2019s method and EP when used in GPC\nmodels. The results corroborate the considerations about the two approximations based on the\nstructure of the posterior given in Section 5. Although only a handful of data sets have been used in\nthe study, we believe the conclusions to be well-founded and generally valid.\nFrom the structural properties of the posterior we described why Laplace\u2019s method systemati-\ncally underestimates the mean m. The resulting approximate posterior GP over latent functions will\nhave too small amplitude, although the sign of the mean function will be mostly correct. As an ef-\nfect Laplace\u2019s method gives over-conservative predictive probabilities, and diminished information\nabout the test labels. This effect has been shown empirically on several real world examples. Large\nresulting discrepancies in the actual posterior probabilities were found, even at the training loca-\ntions, which renders the predictive class probabilities produced under this approximation grossly\ninaccurate. Note, the difference becomes less dramatic if we only consider the classification error\nrates obtained by thresholding p\u2217at 1\/2. For this particular task, we have seen the sign of the la-\ntent function tends to be correct (at least at the training locations). However, the performance on\nbenchmark data sets also revealed the error rates obtained by Laplace\u2019s method to be inferior to EP\nresults.\n1698"},{"page":21,"text":"ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION\nThe EP approximation has shown to give results very close to MCMC both in terms of predictive\ndistributions and marginal likelihood estimates. We have shown and explained why the marginal\ndistributions of the posterior can be well approximated by Gaussians.\nFurther, the marginal likelihood values obtained by Laplace\u2019s method and EP differ systemat-\nically which will lead to different results of ML-II hyper-parameter estimation. The discrepancies\nare similar for different tasks. We were able to exemplify that the EP approximation of the marginal\nlikelihood is accurate. To show this we described how AIS can be used to obtain unbiased estimates\nof the marginal likelihood for Gaussian process models.\nIn the experiments summarised in Table 1 we compared the predictive accuracy of GPC to sup-\nport vector machines. While the SVMs show a tendency to give lower error rates, the information\ncontained in predictive distributions seems comparable. Conceptually GPC comes with the advan-\ntage that the Bayesian model selection can be used to set hyper-parameters by ML-II estimation,\nwhile the parameters of an SVM usually have to be set by cross-validation (gradient based methods\nexist, see e.g. Chapelle et al. (2002)).\nIn summary, we found that EP is the method of choice for approximate inference in binary GPC\nmodels, when the computational cost of MCMC is prohibitive. Very good agreement is achieved\nfor both predictive probabilities and marginal likelihood estimates. In contrast, the Laplace approx-\nimation is so inaccurate that we advise against its use, especially when predictive probabilities are\nto be taken seriously.\nAcknowledgments\nBoth authors acknowledge support by the German Research Foundation (DFG) through grant RA\n1030\/1. We would like to thank Tobias Pfingsten, Jeremy Hill and Matthias Seeger for comments\nand discussions.\nAppendix A. Implementation of Laplace\u2019s Approximation\nIn Sections 3 we described Laplace\u2019s method for approximate inference in the GPC model and\nsketched the corresponding computations in Algorithm 1. In this appendix we describe our imple-\nmentation of the method in more detail. See also the appendices of Williams and Barber (1998).\nComputing Laplace\u2019s approximation N (f|m,A) for given \u03b8 the main computational effort is\ninvolved in finding the maximum of the unnormalised log posterior lnQ (eq. (8)). Our implementa-\ntion uses Newton\u2019s method to find the mode. In each Newton step the vector f is updated according\nto\nft+1\n=\nft\u2212(\u2207\u2207flnQ(ft))\u22121\u2207flnQ(ft)\n(K\u22121+W)\u22121(Wft+\u2207flnL(ft))\n(37a)\n(37b)\n=\nuntil convergence of f to the mode m. To ensure convergence the update is accepted if the value of\nthe target function increases, otherwise the the step size is shortened until lnQ(ft+1) > lnQ(ft).\nComputationally Newtons\u2019s method is dominated by the repeated inversion of the Hessian.\nSince K can be poorly conditioned we use the identity\n(K\u22121+W)\u22121= K\u2212KW\n1\n2(I+W\n1\n2KW\n1\n2)\u22121W\n1\n2K\n(38)\n1699"},{"page":22,"text":"KUSS AND RASMUSSEN\nsuch that only the well conditioned, positive definite matrix (I+W\nour implementation the inverse is computed from a Cholesky decomposition of this matrix. Note\nthat W is a diagonal matrix with positive entries, so computing W\nNote that implementing the Newton updates (37) only requires the product of the inverse Hes-\nsian times the gradient which can be computed more efficiently using an iterative conjugate gradient\nmethod (Golub and Van Loan, 1989, ch. 10).\nHaving found the mode m the marginal likelihood approximation (15) and its derivatives can be\ncomputed. The approximate marginal likelihood takes the form\n1\n2KW\n1\n2) has to be inverted. In\n1\n2 is trivial.\nlnq(D|\u03b8)=\nlnQ(m)+m\n2ln(2\u03c0)+1\n2m?K\u22121m\u22121\n2ln|A|\n2ln|I+KW| .\n(39a)\n=\nlnL(m)\u22121\n(39b)\nTo avoid the direct inversion of K in the second term of (39b) we use the recurrence relation (37b).\nLet a = K\u22121m then by substituting (38) into (37b) we obtain:\na = (I\u2212W\n1\n2(I+W\n1\n2KW\n1\n2)\u22121W\n1\n2K)(Wm+\u2207flnL(m))\n(40)\nsuch that m?K\u22121m = m?a. The determinant in eq. (39b) can be rewritten\nln|I+KW| = ln??I+W\n1\n2KW\n1\n2??\n(41)\nand computed from the Cholesky decomposition, that was used to calculate the inverse in eq. (38).\nNote that if M = LL?is a Cholesky decomposition then ln|M| = 2\u2211lnLii.\nDuring ML-II estimation (36) of hyper-parameters the approximate log marginal likelihood (39)\nis maximised as a function of \u03b8. Our implementation is based on a conjugate gradient optimisation\nroutine such that we also need to compute the derivatives of (39b) with respect to the elements of \u03b8.\nThe dependency of the approximate marginal likelihood on \u03b8 is two-fold:\n\u2202lnq(D|\u03b8)\n\u2202\u03b8i\n=\u2211\nk,l\n\u2202lnq(D|\u03b8)\n\u2202Kkl\n\u2202Kkl\n\u2202\u03b8i\n+\u2202lnq(D|\u03b8)\n\u2202m?\n\u2202m\n\u2202\u03b8i\n(42)\nthere is a direct dependency via the terms involving K and an implicit dependency through the\nchange in m (see also Williams and Barber (1998, Appendix B)).\nThe explicit derivative of eq. (39b) due to the direct dependency of the covariance matrix is\n\u2211\nk,l\n\u2202lnq(D|\u03b8)\n\u2202Kkl\n\u2202Kkl\n\u2202\u03b8i\n=1\n2m?K\u22121\u2202K\n\u2202\u03b8iK\u22121m\u22121\n2tr\n?\n(I+KW)\u22121\u2202K\n\u2202\u03b8iW\n?\n(43)\nwhere the first term is computed using a (40) and the inverse in the second term can be rewritten as\n(I+KW)\u22121= I\u2212(K\u22121+W)\u22121W\n(44)\nwhere the inverse (38) is already known.\nThe implicit derivative accounts for the dependency of eq. (39b) on \u03b8 due to change in the mode\nm. Differentiating eq. (39a) with respect to m reduces to \u2202ln|A|\/\u2202m since m is the maximum of\nlnQ and therefore \u2202lnQ\/\u2202m vanishes.\n\u2202lnq(D|\u03b8)\n\u2202m?\n\u2202m\n\u2202\u03b8i\n= \u22121\n2\n\u2202|K\u22121+W|\n\u2202m?\n\u2202m\n\u2202\u03b8i\n= \u22121\n2(K\u22121+W)\u22121\u2202W\n\u2202m?\n\u2202m\n\u2202\u03b8i\n(45)\n1700"},{"page":23,"text":"ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION\nThe dependency of m on \u03b8iis obtained by differentiating (11a) at m:\n0 = \u2207flnL(m)\u2212K\u22121m\n=\u21d2\nm = K\u2207flnL(m)\n(46)\nso\n\u2202m\n\u2202\u03b8i\n=\u2202K\n\u2202\u03b8i\u2207flnL(m)+K\u2207\u2207flnL(m)\u2202m\n\u2202\u03b8i\n= (I+KW)\u22121\u2202K\n\u2202\u03b8i\u2207flnL(m)\n(47)\nand we have both terms necessary to compute the gradient (42).\nTo compute the predictive probability p\u2217= p(y\u2217=1|x\u2217) for a test input x\u2217the predictive distri-\nbution (5) of the latent function value is N (f\u2217|\u00b5\u2217,\u03c32\n\u00b5\u2217\n\u03c32\n\u2217\nand p\u2217can be computed from eq. (6).\nDue to the Cholesky decomposition in (38) computing Laplace\u2019s approximation isO(m3). How-\never, following the implementation we described in this section a Cholesky decomposition has to be\ncomputed once per Newton step and all other quantities can be computed from it in at most O(m2).\nThe number of Newton steps necessary depends on the convergence criterion, the initialisation of f\nand the hyper-parameters \u03b8.\n\u2217) where\n=\nk?\nk(x\u2217,x\u2217)\u2212k?\n\u2217K\u22121m = k?\n\u2217a\n\u2217W\n(48a)\n=\n1\n2(I+W\n1\n2KW\n1\n2)\u22121W\n1\n2k\u2217\n(48b)\nAppendix B. Implementation of Expectation Propagation\nIn this appendix we describe details of our implementation of EP as described in Section 4 and\nsummarised in Algorithm 2. See also the appendices of Seeger (2003).\nIn our implementation the site functions (17) are parameterised in terms of natural parameters\n\u03c3\u22122\ni\nThe algorithm proceeds by updating the site parameters in random order. In each sweep every site\nfunction is updated following equations (23), (25), and (26). After each update of a site function the\neffect on m and A has to be computed according to (18). The change in A can be computed using a\nrank one update. Let \u03b4 be the change in \u03c3\u22122\ni\ndue to the update and eithe vector whose ith entry is 1\nand all other 0. The relation\nand \u03c3\u22122\ni\u00b5i. For given \u03b8 the algorithm starts by initialising A = K and \u03c3\u22122\ni\n= 0 and \u03c3\u22122\ni\u00b5i= 0.\n(K\u22121+\u03a3\u22121+\u03b4eie?\ni)\u22121= A\u2212Aei(Aii+\u03b4\u22121)\u22121e?\niA\n(49)\ncan be used to update A. Each single update isO(m2) and repeated m times per sweep, such that the\nEP algorithm is O(m3) in time. Because of accumulating numerical errors, after a complete sweep\nover all site functions we recompute the matrix A from scratch. For numerical stability we rewrite\nA = (K\u22121+\u03a3\u22121)\u22121= K\u2212K\u03a3\u22121\n2(I+\u03a3\u22121\n2K\u03a3\u22121\n2)\u22121\u03a3\u22121\n2K\n(50)\nand compute the inverse from the Cholesky decomposition of (I+\u03a3\u22121\nAfter convergence the approximate log marginal likelihood (27) can be computed and its partial\nderivatives with respect to the hyper-parameters:\n2K\u03a3\u22121\n2).\n\u2202lnq(D|\u03b8)\n\u2202\u03b8i\n= \u22121\n2tr\n?\u2202K\n\u2202\u03b8i\n?\n(K+\u03a3)\u22121\u2212(K+\u03a3)\u22121\u00b5\u00b5?(K+\u03a3)\u22121??\n.\n(51)\n1701"},{"page":24,"text":"KUSS AND RASMUSSEN\nwhich do not depend on the Zi(Seeger, 2005).\nThe inverse of K+\u03a3 can be computed from the inverse in eq. (50):\n(K+\u03a3)\u22121= \u03a3\u22121\n2(I+\u03a3\u22121\n2K\u03a3\u22121\n2)\u22121\u03a3\u22121\n2.\n(52)\nFor computing the log marginal likelihood (27) also the determinant |K+\u03a3| has to be computed.\nBy rewriting\nln|K+\u03a3| = ln(|\u03a3||I+\u03a3\u22121K|) = ln|\u03a3|+ln|I+\u03a3\u22121\nwe obtain an expression in which the first term is a determinant of a diagonal matrix and the second\nterm can be computed from the Cholesky decomposition that was used to compute the inverse in\neq. (50).\nTo compute the predictive probability p\u2217= p(y\u2217=1|x\u2217) for a test input x\u2217the predictive distri-\nbution (5) of the latent function value is N (f\u2217|\u00b5\u2217,\u03c32\n\u00b5\u2217\n\u03c32\n\u2217\nand p\u2217can be computed from eq. (6).\nThe EP algorithm is of computational complexity O(m3) due to the computations for updat-\ning A. However, per sweep the computation of A (50) and the m rank one updates sum to more\ncomputational effort compared to Laplace\u2019s method.\nUsinga covariancefunction ofthe form(33)forsomedata sets we observednumerical problems\nduring ML-II hyper-parameter estimation because the optimisation algorithm asked to evaluate the\nmarginal likelihood for extremely large signal variances \u03c32. The problem stems from the property\nthat for large values of \u03c32the marginal likelihood becomes insensitive to changes in \u03c32. At this\npoint it is recommended to take another look at Figure 1(b). Intuitively, for large signal variances\nthe prior becomes more spread, such that the likelihood becomes more and more similar to a hard\ntruncation. The marginal likelihood equals the probability mass of the prior in the orthant that is left\nafter truncation. But the probability mass in any of the orthants remains constant if only the signal\nvariance is changed for fixed correlation structure. This argument is based on the assumption that\nthe likelihood implements a hard truncation, which is only an approximation, but this approximation\nbecomes better the larger \u03c32is. Note that this insensitivity of the marginal likelihood with respect\nto changes in the signal variance can already be observed in the upper parts of of the marginal\nlikelihood plots for EP in Figures 4 and 5. A possible solution to this problem is to limit \u03c32< 105,\nsay, since we wouldn\u2019t typically expect any new interesting behaviour beyond this.\n2K\u03a3\u22121\n2|\n(53)\n\u2217) where\n=\nk?\nk(x\u2217,x\u2217)\u2212k?\n\u2217(K+\u03a3)\u22121\u00b5\n(54a)\n(54b)\n=\n\u2217(K+\u03a3)\u22121k\u2217\n1702"},{"page":25,"text":"ASSESSING APPROXIMATIONS FOR GAUSSIAN PROCESS CLASSIFICATION\nReferences\nP. Abrahamsen. A review of Gaussian random fields and correlation functions. Technical Report\n917, Norwegian Computing Center, Oslo, 1997.\nC.-C. Chang and C.-J. Lin.\nhttp:\/\/www.csie.ntu.edu.tw\/\u223ccjlin\/libsvm.\nO. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing multiple parameters for support\nvector machines. Machine Learning, 46(1):131\u2013159, 2002.\nLIBSVM: A library for Support Vector Machines, 2001.\nW. Chu and Z. Ghahramani. Gaussian processes for ordinal regression. Journal of Machine Learn-\ning Research, 6:1019\u20131041, 2005.\nL. Csat\u00b4 o and M. Opper. Sparse online Gaussian processes. Neural Computation, 14(2):641\u2013669,\n2002.\nS. Duane, A. D. Kennedy, B. J. Pendleton, and D. Roweth. Hybrid Monte Carlo. Physics Letters B,\n195(2):216\u2013222, 1987.\nA. Gelman and X.-L. Meng. Simulating normalizing constants: From importance sampling to\nbridge sampling to path sampling. Statistical Science, 13(2):163\u2013185, 1998.\nM. N. Gibbs and D. J. C. MacKay. Variational Gaussian process classifiers. IEEE Transactions on\nNeural Networks, 11(6):1458\u20131464, 2000.\nG. H. Golub and C. F. Van Loan. Matrix Computations. John Hopkins University Press, Baltimore,\nsecond edition, 1989.\nS. Hettich, C. L. Blake, and C. J. Merz. UCI repository of machine learning databases, 1998.\nhttp:\/\/www.ics.uci.edu\/\u223cmlearn\/MLRepository.html.\nR. E. Kass and A. E. Raftery. Bayes factors. Journal of the American Statistical Association, 90\n(430):773\u2013795, 1995.\nN. Lawrence, M. Seeger, and R. Herbrich. Fast sparse Gaussian process methods: The informa-\ntive vector machine. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural\nInformation Processing Systems 15, pages 609\u2013616, Cambridge, MA, 2003. The MIT Press.\nJ. S. Liu. Monte Carlo Strategies in Scientific Computing. Springer, New York, 2001.\nD. J. C. MacKay. Comparison of approximate methods for handling hyperparameters. Neural\nComputation, 11(5):1035\u20131068, 1999.\nD. J. C. MacKay. Information Theory, Inference and Learning Algorithms. Cambridge University\nPress, Cambridge, UK, 2003.\nT. P. Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD thesis, Department\nof Electrical Engineering and Computer Science, MIT, 2001.\nR. M. Neal. Probabilistic inference using Markov chain Monte Carlo methods. Technical Report\nCRG-TR-93-1, Department of Computer Science, University of Toronto, 1993.\n1703"},{"page":26,"text":"KUSS AND RASMUSSEN\nR. M. Neal. Regression and classification using Gaussian process priors. In J. M. Bernardo, J. O.\nBerger, A. P. Dawid, and A. F. M. Smith, editors, Bayesian Statistics 6, pages 475\u2013501. Oxford\nUniversity Press, 1998.\nR. M. Neal. Annealed importance sampling. Statistics and Computing, 11:125\u2013139, 2001.\nA.O\u2019Hagan. Curvefittingandoptimaldesignforprediction. JournaloftheRoyalStatisticalSociety,\nSeries B, 40(1):1\u201342, 1978.\nM. Opper and O. Winther. Gaussian processes for classification: Mean-field algorithms. Neural\nComputation, 12(11):2655\u20132684, 2000.\nJ. C. Platt. Probabilities for SV machines. In A. J. Smola, P. L. Bartlett, B. Sch\u00a8 olkopf, and D. Schu-\nurmans, editors, Advances in Large Margin Classifiers, pages 61\u201373. The MIT Press, Cambridge,\nMA, 2000.\nC. E. Rasmussen and C. K. I Williams. Gaussian Processes for Machine Learning. The MIT Press,\nCambridge, MA, 2006. In press.\nB. D. Ripley. Pattern Recognition and Neural Newtorks. Cambridge University Press, Cambridge,\nUK, 1996.\nB. Sch\u00a8 olkopf and A. J. Smola. Learning with Kernels. The MIT Press, Cambridge, MA, 2002.\nM. Seeger. PAC-Bayesian generalisation error bounds for Gaussian process classification. Journal\nof Machine Learning Research, 3:233\u2013269, 2002.\nM. Seeger. Bayesian Gaussian Process Models: PAC-Bayesian Generalisation Error Bounds and\nSparse Approximations. PhD thesis, University of Edinburgh, 2003.\nM. Seeger. Expectation propagation for exponential families, 2005. Note obtainable from\nhttp:\/\/www.kyb.tuebingen.mpg.de\/\u223cseeger.\nC. K. I. Williams and D. Barber. Bayesian classification with Gaussian processes. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, 20(12):1342\u20131351, 1998.\n1704"}],"widgetId":"rgw27_56ab1dba860c9"},"id":"rgw27_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=41781429&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw28_56ab1dba860c9"},"id":"rgw28_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=41781429&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":41781429,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":41781429,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2034167984,"url":"researcher\/2034167984_Ulrich_Schaechtle","fullname":"Ulrich Schaechtle","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277112939401216%401443080281088_m"},{"id":2089456342,"url":"researcher\/2089456342_Ben_Zinberg","fullname":"Ben Zinberg","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089399536,"url":"researcher\/2089399536_Alexey_Radul","fullname":"Alexey Radul","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":8074903,"url":"researcher\/8074903_Kostas_Stathis","fullname":"Kostas Stathis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Dec 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization","usePlainButton":true,"publicationUid":287249271,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization","title":"Probabilistic Programming with Gaussian Process Memoization","displayTitleAsLink":true,"authors":[{"id":2034167984,"url":"researcher\/2034167984_Ulrich_Schaechtle","fullname":"Ulrich Schaechtle","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277112939401216%401443080281088_m"},{"id":2089456342,"url":"researcher\/2089456342_Ben_Zinberg","fullname":"Ben Zinberg","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089399536,"url":"researcher\/2089399536_Alexey_Radul","fullname":"Alexey Radul","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8074903,"url":"researcher\/8074903_Kostas_Stathis","fullname":"Kostas Stathis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089392273,"url":"researcher\/2089392273_Vikash_K_Mansinghka","fullname":"Vikash K. Mansinghka","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Gaussian Processes (GPs) are widely used tools in statistics, machine\nlearning, robotics, computer vision, and scientific computation. However,\ndespite their popularity, they can be difficult to apply; all but the simplest\nclassification or regression applications require specification and inference\nover complex covariance functions that do not admit simple analytical\nposteriors. This paper shows how to embed Gaussian processes in any\nhigher-order probabilistic programming language, using an idiom based on\nmemoization, and demonstrates its utility by implementing and extending classic\nand state-of-the-art GP applications. The interface to Gaussian processes,\ncalled gpmem, takes an arbitrary real-valued computational process as input and\nreturns a statistical emulator that automatically improve as the original\nprocess is invoked and its input-output behavior is recorded. The flexibility\nof gpmem is illustrated via three applications: (i) robust GP regression with\nhierarchical hyper-parameter learning, (ii) discovering symbolic expressions\nfrom time-series data by fully Bayesian structure learning over kernels\ngenerated by a stochastic grammar, and (iii) a bandit formulation of Bayesian\noptimization with automatic inference and action selection. All applications\nshare a single 50-line Python library and require fewer than 20 lines of\nprobabilistic code each.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Kostas_Stathis\/publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization\/links\/5687eb7108ae051f9af5a28a.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Kostas_Stathis","sourceName":"Kostas Stathis","hasSourceUrl":true},"publicationUid":287249271,"publicationUrl":"publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization\/links\/5687eb7108ae051f9af5a28a\/smallpreview.png","linkId":"5687eb7108ae051f9af5a28a","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=287249271&reference=5687eb7108ae051f9af5a28a&eventCode=&origin=publication_list","widgetId":"rgw32_56ab1dba860c9"},"id":"rgw32_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=287249271&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"5687eb7108ae051f9af5a28a","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":41781429,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Gaussian Processes (GPs) are widely used tools in statistics (Barry, 1986), machine learning (Neal, 1995; Williams and Barber, 1998; Kuss and Rasmussen, 2005; Rasmussen and Williams, 2006; Damianou and Lawrence, 2013), robotics (Ferris et al., 2006), computer vision (Kemmler et al., 2013), and scientific computation (Kennedy and O&apos;Hagan, 2001; Schneider et al., 2008; Kwan et al., 2013). They are also central to probabilistic numerics, an emerging effort to develop more computationally efficient numerical procedures, and to Bayesian optimization, a family of meta-optimization techniques that are widely used to tune parameters for deep learning algorithms (Snoek et al., 2012; Gelbart et al., 2014). "],"widgetId":"rgw33_56ab1dba860c9"},"id":"rgw33_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw31_56ab1dba860c9"},"id":"rgw31_56ab1dba860c9","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=287249271&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":31451560,"url":"researcher\/31451560_Markus_Heinonen","fullname":"Markus Heinonen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079800486,"url":"researcher\/2079800486_Henrik_Mannerstroem","fullname":"Henrik Mannerstr\u00f6m","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7729922,"url":"researcher\/7729922_Juho_Rousu","fullname":"Juho Rousu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2079807374,"url":"researcher\/2079807374_Samuel_Kaski","fullname":"Samuel Kaski","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Aug 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/281145119_Non-Stationary_Gaussian_Process_Regression_with_Hamiltonian_Monte_Carlo","usePlainButton":true,"publicationUid":281145119,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/281145119_Non-Stationary_Gaussian_Process_Regression_with_Hamiltonian_Monte_Carlo","title":"Non-Stationary Gaussian Process Regression with Hamiltonian Monte Carlo","displayTitleAsLink":true,"authors":[{"id":31451560,"url":"researcher\/31451560_Markus_Heinonen","fullname":"Markus Heinonen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079800486,"url":"researcher\/2079800486_Henrik_Mannerstroem","fullname":"Henrik Mannerstr\u00f6m","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7729922,"url":"researcher\/7729922_Juho_Rousu","fullname":"Juho Rousu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079807374,"url":"researcher\/2079807374_Samuel_Kaski","fullname":"Samuel Kaski","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38617824,"url":"researcher\/38617824_Harri_Laehdesmaeki","fullname":"Harri L\u00e4hdesm\u00e4ki","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"We present a novel approach for fully non-stationary Gaussian process\nregression (GPR), where all three key parameters -- noise variance, signal\nvariance and lengthscale -- can be simultaneously input-dependent. We develop\ngradient-based inference methods to learn the unknown function and the\nnon-stationary model parameters, without requiring any model approximations. We\npropose to infer full parameter posterior with Hamiltonian Monte Carlo (HMC),\nwhich conveniently extends the analytical gradient-based GPR learning by\nguiding the sampling with model gradients. We also learn the MAP solution from\nthe posterior by gradient ascent. In experiments on several synthetic datasets\nand in modelling of temporal gene expression, the nonstationary GPR is shown to\nbe necessary for modeling realistic input-dependent dynamics, while it performs\ncomparably to conventional stationary or previous non-stationary GPR models\notherwise.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/281145119_Non-Stationary_Gaussian_Process_Regression_with_Hamiltonian_Monte_Carlo","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Samuel_Kaski\/publication\/281145119_Non-Stationary_Gaussian_Process_Regression_with_Hamiltonian_Monte_Carlo\/links\/55f31f8708ae6a34f6605292.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Samuel_Kaski","sourceName":"Samuel Kaski","hasSourceUrl":true},"publicationUid":281145119,"publicationUrl":"publication\/281145119_Non-Stationary_Gaussian_Process_Regression_with_Hamiltonian_Monte_Carlo","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/281145119_Non-Stationary_Gaussian_Process_Regression_with_Hamiltonian_Monte_Carlo\/links\/55f31f8708ae6a34f6605292\/smallpreview.png","linkId":"55f31f8708ae6a34f6605292","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=281145119&reference=55f31f8708ae6a34f6605292&eventCode=&origin=publication_list","widgetId":"rgw35_56ab1dba860c9"},"id":"rgw35_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=281145119&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"55f31f8708ae6a34f6605292","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":41781429,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/281145119_Non-Stationary_Gaussian_Process_Regression_with_Hamiltonian_Monte_Carlo\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["The posterior of the latent vectors is by definition highly correlated due to Gaussian priors, leading to inefficient Monte Carlo sampling. To ease the sampling, we perform the sampling over the whitened latent vectors (Kuss and Rasmussen, 2005) "],"widgetId":"rgw36_56ab1dba860c9"},"id":"rgw36_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw34_56ab1dba860c9"},"id":"rgw34_56ab1dba860c9","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=281145119&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2079162962,"url":"researcher\/2079162962_Xiaoyu_Xiong","fullname":"Xiaoyu Xiong","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A309409379487746%401450780352548_m\/Xiaoyu_Xiong.png"},{"id":82161005,"url":"researcher\/82161005_Vaclav_Smidl","fullname":"V\u00e1clav \u0160m\u00eddl","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70871340,"url":"researcher\/70871340_Maurizio_Filippone","fullname":"Maurizio Filippone","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Aug 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes","usePlainButton":true,"publicationUid":280773011,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes","title":"Adaptive Multiple Importance Sampling for Gaussian Processes","displayTitleAsLink":true,"authors":[{"id":2079162962,"url":"researcher\/2079162962_Xiaoyu_Xiong","fullname":"Xiaoyu Xiong","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A309409379487746%401450780352548_m\/Xiaoyu_Xiong.png"},{"id":82161005,"url":"researcher\/82161005_Vaclav_Smidl","fullname":"V\u00e1clav \u0160m\u00eddl","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70871340,"url":"researcher\/70871340_Maurizio_Filippone","fullname":"Maurizio Filippone","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"In applications of Gaussian processes where quantification of uncertainty is\na strict requirement, it is necessary to accurately characterize the posterior\ndistribution over Gaussian process covariance parameters. Normally, this is\ndone by means of Markov chain Monte Carlo (MCMC) algorithms. Focusing on\nGaussian process regression where the marginal likelihood is computable but\nexpensive to evaluate, this paper studies algorithms based on importance\nsampling to carry out expectations under the posterior distribution over\ncovariance parameters. The results indicate that expectations computed using\nAdaptive Multiple Importance Sampling converge faster per unit of computation\nthan those computed with MCMC algorithms for models with few covariance\nparameters, and converge as fast as MCMC for models with up to around twenty\ncovariance parameters.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Vasek_Smidl\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/links\/55e94dff08ae65b6389aee89.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Vasek_Smidl","sourceName":"Vasek Smidl","hasSourceUrl":true},"publicationUid":280773011,"publicationUrl":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/links\/55e94dff08ae65b6389aee89\/smallpreview.png","linkId":"55e94dff08ae65b6389aee89","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=280773011&reference=55e94dff08ae65b6389aee89&eventCode=&origin=publication_list","widgetId":"rgw38_56ab1dba860c9"},"id":"rgw38_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=280773011&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"55e94dff08ae65b6389aee89","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":41781429,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/280773011_Adaptive_Multiple_Importance_Sampling_for_Gaussian_Processes\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Inference of GP covariance parameters is analytically intractable, and standard inference methods require repeatedly calculating the so called marginal likelihood . When the likelihood function is not Gaussian, e.g., in classification, in ordinal regression, in modeling of stochastic volatility, in Cox-processes, the marginal likelihood cannot be computed analytically, and this has motivated a large body of the literature to develop approximate inference methods [56] [43] [31] [47] [41] [24], reparameterization techniques [36] [34] [54] [14], and exact inference with unbiased computations of the marginal likelihood [12] [10]. Even in the case of a Gaussian likelihood, which makes the marginal likelihood computable, inference is generally costly because the computation of the marginal likelihood has time complexity scaling with the cube of the number of input vectors [11]. "],"widgetId":"rgw39_56ab1dba860c9"},"id":"rgw39_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw37_56ab1dba860c9"},"id":"rgw37_56ab1dba860c9","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=280773011&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":41781429,"publicationLink":"publication\/41781429_Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw30_56ab1dba860c9"},"id":"rgw30_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=41781429&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=91","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":91,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw29_56ab1dba860c9"},"id":"rgw29_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=41781429&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/41781429_Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1dba860c9"},"id":"rgw2_56ab1dba860c9","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":41781429},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=41781429&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1dba860c9"},"id":"rgw1_56ab1dba860c9","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"IrcAq12lhkAJHMf72j\/lo0mbR9\/2IjfimlIwkIwOVnW9FLe1dZrUHvbChJ5A60aaat3LiXeSHiKFipHs8FAlXKgxx0KQXbio4kSiceVUITiFGP+O9bbsI7ChSHad6rqgMyISu3Wnlzdl6WIfZWyzo+dcMn\/t6KXUIYWudJ47bY59Q+GfI4DaumhJjhzsVXOzpDQcDlH3in49Wa1yuL\/CjBHj4yoov2bB1JZPF6aSJE9ZBS\/Rb2F3\/9GRRtG3nZuXroDS\/KpewM5ZwjpV1s0i79KKZ0sduCx\/1IPBb3vTr3A=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/41781429_Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Assessing Approximate Inference for Binary Gaussian Process Classification\" \/>\n<meta property=\"og:description\" content=\"Gaussian process priors can be used to define flexible, probabilistic classification models. Unfortunately exact Bayesian inference is analytically intractable and various approximation techniques...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/41781429_Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification\/links\/0e608558f0c46d4f0acd3c92\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/41781429_Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification\" \/>\n<meta property=\"rg:id\" content=\"PB:41781429\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Assessing Approximate Inference for Binary Gaussian Process Classification\" \/>\n<meta name=\"citation_author\" content=\"M. Kuss\" \/>\n<meta name=\"citation_author\" content=\"C.E. Rasmussen\" \/>\n<meta name=\"citation_publication_date\" content=\"2005\/10\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"Journal of Machine Learning Research\" \/>\n<meta name=\"citation_issn\" content=\"1533-7928\" \/>\n<meta name=\"citation_volume\" content=\"6\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/41781429_Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/41781429_Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-a1f4fd35-7cea-44ea-938e-a8513f258164","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":627,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw40_56ab1dba860c9"},"id":"rgw40_56ab1dba860c9","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-a1f4fd35-7cea-44ea-938e-a8513f258164", "38d96e929d10d458727d79d25e092e7706c64a9a");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-a1f4fd35-7cea-44ea-938e-a8513f258164", "38d96e929d10d458727d79d25e092e7706c64a9a");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw41_56ab1dba860c9"},"id":"rgw41_56ab1dba860c9","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/41781429_Assessing_Approximate_Inference_for_Binary_Gaussian_Process_Classification","requestToken":"iFjewQG8CgEQq1qJa1BsolORYmztGdClOdmDAvXnrsWT\/pDHay4M\/62oYXN05tsYXGW5DMzQo3LJTNS7cwHCCrkMR592jLYBFmr2kEHZzPyOqcTWsQXz8DIBzlRHJBnd\/S21xEi3\/nT\/DMn2eshKSvgE5hu0GiA5CLx3LYh2esyIN6ts8mj+e64BenLrCRjNiRkQb+ZIQ0E0FOh0v120XeFd8RitHZ3sMIFN56cenWqRVkMXRoAhsAOUyZ4+q6mlAoOVhHsyPkRzGntzsghYfZ3\/+t5ldwcM9kL+YmVEics=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=6evqR5dZ4ytqyVnDkH9AjmiVGvphgrqaYl__DfpX4Eoq_DTg0AIKfPiiH47xA0Ug","encodedUrlAfterLogin":"cHVibGljYXRpb24vNDE3ODE0MjlfQXNzZXNzaW5nX0FwcHJveGltYXRlX0luZmVyZW5jZV9mb3JfQmluYXJ5X0dhdXNzaWFuX1Byb2Nlc3NfQ2xhc3NpZmljYXRpb24%3D","signupCallToAction":"Join for free","widgetId":"rgw43_56ab1dba860c9"},"id":"rgw43_56ab1dba860c9","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw42_56ab1dba860c9"},"id":"rgw42_56ab1dba860c9","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw44_56ab1dba860c9"},"id":"rgw44_56ab1dba860c9","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
