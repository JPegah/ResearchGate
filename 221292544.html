<!DOCTYPE html> <html lang="en" class="" id="rgw53_56ab1977ab01e"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="3FLETPVRO5igd5yKumfBAHTP7dxi5P5fKSHwkdTo5I+DBrnwznEl8inQSmGSh7MUmPlurbowkG743JBAK9RLH5my/CDUY/+Sf8j0XH9vUs6XSbBlFZ9ggRekw5gwHu8oxH24e27qJB27B+sPDD5ZO8pCiyae9aIgny4j4ITPa4DGvOmI0W4S0fbQl09h8pt+g14vNOZO83SrJBM375hqaTUCwjoTyZFL9nxOfE4oU5ha3iIasEU4BTZT0Ot6u1C20th9Dvz3LT8jdAlTSYzTzTDJSJgJaHjXpUi1wwOip7k="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-4c6a8ade-39d2-40cc-822e-3e6ca525ce0c",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Painful data: The UNBC-McMaster shoulder pain expression archive database" />
<meta property="og:description" content="A major factor hindering the deployment of a fully functional automatic facial expression detection system is the lack of representative data. A solution to this is to narrow the context of the..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database/links/00b495220a93e5795f000000/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database" />
<meta property="rg:id" content="PB:221292544" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1109/FG.2011.5771462" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Painful data: The UNBC-McMaster shoulder pain expression archive database" />
<meta name="citation_author" content="Patrick Lucey" />
<meta name="citation_author" content="Jeffrey F. Cohn" />
<meta name="citation_author" content="Kenneth M. Prkachin" />
<meta name="citation_author" content="Patricia E. Solomon" />
<meta name="citation_author" content="Iain Matthews" />
<meta name="citation_conference_title" content="Ninth IEEE International Conference on Automatic Face and Gesture Recognition (FG 2011), Santa Barbara, CA, USA, 21-25 March 2011" />
<meta name="citation_publication_date" content="2011/03/25" />
<meta name="citation_firstpage" content="57" />
<meta name="citation_lastpage" content="64" />
<meta name="citation_doi" content="10.1109/FG.2011.5771462" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Kenneth_Prkachin/publication/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database/links/00b495220a93e5795f000000.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Conference Paper');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Painful data: The UNBC-McMaster shoulder pain expression archive database (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Painful data: The UNBC-McMaster shoulder pain expression archive database on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1977ab01e" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1977ab01e" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab1977ab01e">  <div class="type-label"> Conference Paper   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1109%2FFG.2011.5771462&rft.atitle=Painful%20data%3A%20The%20UNBC-McMaster%20shoulder%20pain%20expression%20archive%20database&rft.title=2011%20IEEE%20International%20Conference%20on%20Automatic%20Face%20and%20Gesture%20Recognition%20and%20Workshops%2C%20FG%202011&rft.jtitle=2011%20IEEE%20International%20Conference%20on%20Automatic%20Face%20and%20Gesture%20Recognition%20and%20Workshops%2C%20FG%202011&rft.date=2011&rft.pages=57-64&rft.au=Patrick%20Lucey%2CJeffrey%20F.%20Cohn%2CKenneth%20M.%20Prkachin%2CPatricia%20E.%20Solomon%2CIain%20Matthews&rft.genre=inProceedings"></span> <h1 class="pub-title" itemprop="name">Painful data: The UNBC-McMaster shoulder pain expression archive database</h1> <meta itemprop="headline" content="Painful data: The UNBC-McMaster shoulder pain expression archive database">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database/links/00b495220a93e5795f000000/smallpreview.png">  <div id="rgw7_56ab1977ab01e" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab1977ab01e" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Patrick_Lucey" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Patrick Lucey" alt="Patrick Lucey" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Patrick Lucey</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw9_56ab1977ab01e" data-account-key="Patrick_Lucey">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Patrick_Lucey"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Patrick Lucey" alt="Patrick Lucey" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Patrick_Lucey" class="display-name">Patrick Lucey</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Disney_Research" title="Disney Research">Disney Research</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56ab1977ab01e" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Jeffrey_Cohn" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272368778084361%401441949186005_m/Jeffrey_Cohn.png" title="Jeffrey F Cohn" alt="Jeffrey F Cohn" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Jeffrey F Cohn</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw11_56ab1977ab01e" data-account-key="Jeffrey_Cohn">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Jeffrey_Cohn"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272368778084361%401441949186005_l/Jeffrey_Cohn.png" title="Jeffrey F Cohn" alt="Jeffrey F Cohn" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Jeffrey_Cohn" class="display-name">Jeffrey F Cohn</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_of_Pittsburgh" title="University of Pittsburgh">University of Pittsburgh</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56ab1977ab01e" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Kenneth_Prkachin" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A283801621155842%401444674987468_m/Kenneth_Prkachin.png" title="Kenneth M Prkachin" alt="Kenneth M Prkachin" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Kenneth M Prkachin</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw13_56ab1977ab01e" data-account-key="Kenneth_Prkachin">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Kenneth_Prkachin"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A283801621155842%401444674987468_l/Kenneth_Prkachin.png" title="Kenneth M Prkachin" alt="Kenneth M Prkachin" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Kenneth_Prkachin" class="display-name">Kenneth M Prkachin</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_of_Northern_British_Columbia" title="University of Northern British Columbia">University of Northern British Columbia</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw14_56ab1977ab01e" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Patricia_Solomon2" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Patricia Solomon" alt="Patricia Solomon" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Patricia Solomon</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw15_56ab1977ab01e" data-account-key="Patricia_Solomon2">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Patricia_Solomon2"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Patricia Solomon" alt="Patricia Solomon" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Patricia_Solomon2" class="display-name">Patricia Solomon</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/McMaster_University" title="McMaster University">McMaster University</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw16_56ab1977ab01e" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Iain_Matthews2" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A277565160869889%401443188099330_m/Iain_Matthews2.png" title="Iain Matthews" alt="Iain Matthews" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Iain Matthews</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw17_56ab1977ab01e" data-account-key="Iain_Matthews2">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Iain_Matthews2"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A277565160869889%401443188099330_l/Iain_Matthews2.png" title="Iain Matthews" alt="Iain Matthews" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Iain_Matthews2" class="display-name">Iain Matthews</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Disney_Research" title="Disney Research">Disney Research</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">  <div> Dept. of Psychol., Univ. of Pittsburgh, Pittsburgh, PA, USA </div>      DOI:&nbsp;10.1109/FG.2011.5771462     Conference: Ninth IEEE International Conference on Automatic Face and Gesture Recognition (FG 2011), Santa Barbara, CA, USA, 21-25 March 2011      <div class="pub-source"> Source: <a href="http://dblp.uni-trier.de/db/conf/fgr/fg2011.html#LuceyCPSM11" rel="nofollow">DBLP</a> </div>  </div> <div id="rgw18_56ab1977ab01e" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>A major factor hindering the deployment of a fully functional automatic facial expression detection system is the lack of representative data. A solution to this is to narrow the context of the target application, so enough data is available to build robust models so high performance can be gained. Automatic pain detection from a patient's face represents one such application. To facilitate this work, researchers at McMaster University and University of Northern British Columbia captured video of participant's faces (who were suffering from shoulder pain) while they were performing a series of active and passive range-of-motion tests to their affected and unaffected limbs on two separate occasions. Each frame of this data was AU coded by certified FACS coders, and self-report and observer measures at the sequence level were taken as well. This database is called the UNBC-McMaster Shoulder Pain Expression Archive Database. To promote and facilitate research into pain and augment current datasets, we have publicly made available a portion of this database which includes: (1) 200 video sequences containing spontaneous facial expressions, (2) 48,398 FACS coded frames, (3) associated pain frame-by-frame scores and sequence-level self-report and observer measures, and (4) 66-point AAM landmarks. This paper documents this data distribution in addition to describing baseline results of our AAM/SVM system. This data will be available for distribution in March 2011.</div> </p>  </div>  </div>   </div>      <div class="action-container"> <div id="rgw19_56ab1977ab01e" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw33_56ab1977ab01e">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw45_56ab1977ab01e">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Kenneth_Prkachin/publication/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database/links/00b495220a93e5795f000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Kenneth_Prkachin">Kenneth M Prkachin</a>   </span>  </div>  <div class="social-share-container"><div id="rgw47_56ab1977ab01e" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw48_56ab1977ab01e" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw49_56ab1977ab01e" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw50_56ab1977ab01e" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw51_56ab1977ab01e" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw52_56ab1977ab01e" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw46_56ab1977ab01e" src="https://www.researchgate.net/c/o1o9o3/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FKenneth_Prkachin%2Fpublication%2F221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database%2Flinks%2F00b495220a93e5795f000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw32_56ab1977ab01e"  itemprop="articleBody">  <p>Page 1</p> <p>PAINFUL DATA: The UNBC-McMaster Shoulder Pain Expression<br />Archive Database<br />Patrick Lucey, Jeffrey F. Cohn, Kenneth M. Prkachin, Patricia E. Solomon and Iain Matthews<br />Abstract—A major factor hindering the deployment of a<br />fully functional automatic facial expression detection system<br />is the lack of representative data. A solution to this is to<br />narrow the context of the target application, so enough data<br />is available to build robust models so high performance can<br />be gained. Automatic pain detection from a patient’s face<br />represents one such application. To facilitate this work, re-<br />searchers at McMaster University and University of Northern<br />British Columbia captured video of participant’s faces (who<br />were suffering from shoulder pain) while they were performing<br />a series of active and passive range-of-motion tests to their<br />affected and unaffected limbs on two separate occasions. Each<br />frame of this data was AU coded by certified FACS coders, and<br />self-report and observer measures at the sequence level were<br />taken as well. This database is called the UNBC-McMaster<br />Shoulder Pain Expression Archive Database. To promote and<br />facilitate research into pain and augment current datasets,<br />we have publicly made available a portion of this database<br />which includes: 1) 200 video sequences containing spontaneous<br />facial expressions, 2) 48,398 FACS coded frames, 3) associated<br />pain frame-by-frame scores and sequence-level self-report and<br />observer measures, and 4) 66-point AAM landmarks. This<br />paper documents this data distribution in addition to describing<br />baseline results of our AAM/SVM system. This data will be<br />available for distribution in March 2011.<br />I. INTRODUCTION<br />Automated facial expression detection has made great<br />strides over the past decade [1], [2], [3]. From initial focus<br />on posed images recorded from a frontal view, current efforts<br />extend to posed data recorded from multiple views [4], [5],<br />3D imaging [6], and, increasingly, non-posed facial behavior<br />in real-world settings in which non-frontal pose and head<br />motion are common [7], [8]. With respect to the latter, lack<br />of well-annotated, ecologically valid, representative data has<br />been a significant limitation.<br />In many real-world applications, the goal is to recognize<br />or infer intention or other psychological states rather than<br />facial actions alone. For this purpose, both narrowing the<br />number of facial actions of interest and paying attention<br />to context may be critical to the success of an automated<br />system. For example, if one were to apply an automated<br />This project was supported in part by CIHR Operating Grant MOP77799<br />and National Institute of Mental Health grant R01 MH51435<br />P. Lucey (now with Disney Research Pittsburgh) conducted this work<br />with J.F. Cohn at the Department of Psychology, University of Pitts-<br />burgh/Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, 15260.<br />Email: {patrick.lucey@disneyresearch.com, jeffcohn@cs.cmu.edu}<br />K.M. Prkachin is with the Department of Psychology at the Univer-<br />sity of Northern British Columbia; P.E. Solomon is with the School of<br />Rehabilitation Science, McMaster University, Hamilton, Ontario, Canada;<br />and I. Matthews is with Disney Research Pittsburgh/Robotics Institute,<br />Carnegie Mellon University, Pittsburgh, PA. Email: {kmprk@unbc.ca,<br />solomon@mcmaster.ca, iainm@disneyresearch.com }<br />facial analysis system to detect hostile intent in airports, a<br />very large number of facial actions could occur and the shear<br />number of permutations that could explain a person’s facial<br />expression would be far too great to have any confidence in<br />detecting intention. The range of facial action variability in<br />this setting is considerable. Hundreds of facial action units<br />in combinations could occur and their signal value highly<br />diverse. A target person could be running late, afraid of<br />flying, upset at leaving a loved one, or agitated over visa<br />problems or missed connections, among other states. Facial<br />expression analysis would have to be combined with other<br />modalities, including speech, in an iterative process to have<br />confidence in intention detection.<br />Detection of psychologically meaningful states from facial<br />behavior alone can be improved by knowing the context (e.g.,<br />clinical interview or assessment) and number of outcomes.<br />(say two, i.e. yes/no). With these constraints, the application<br />of an automatic facial expression detection system could<br />be very successful. A recent example is that of automatic<br />smile detection in digital cameras where Whitehill et al.<br />[8] constrained the goal to detecting only smile or no-<br />smile. Employing a Gabor filter approach, they were able to<br />achieve performance of up to 98% on a challenging dataset<br />consisting of frontal faces spontaneously smiling or not in<br />various environments, although no inferences were made<br />about psychological state (e.g., enjoyment) and no temporal<br />segmenting was required. Other examples of well-specified<br />problems or contexts in which facial expression detection<br />would be useful include driver fatigue detection, clinical<br />status (e.g., symptomatic or not) and approach/avoidance in<br />consumers (e.g., interested, disgusted or neutral).<br />A. Painful Motivation<br />An application that would be of great benefit is that<br />of pain/no-pain detection [9], [10], [11], [12]. In Atul<br />Gawande’s recent book entitled ”The Checklist Manifesto”<br />[13], he notes that massive improvements in patient outcomes<br />in intensive care unit (ICU) settings have been achieved<br />through adhering to standardized hygiene and monitoring<br />per a priori checklists. One of these is pain monitoring,<br />in which a nurse checks on a patient every 4 hours or<br />so to evaluate whether they are suffering from pain and<br />to make any needed adjustments in pain medication that<br />may be warranted. Pain monitoring especially beyond the<br />ICU has been hard to implement due to competing demands<br />on nursing staff. Automatic monitoring could be an ideal<br />solution.</p>  <p>Page 2</p> <p>(a)<br />(b)<br />(c)<br />(d)<br />Monday, September 20, 2010<br />Fig. 1.<br />Intensity (OPI) = 5, Visual Analog Scale (VAS) = 9, Sensory Scale (SEN) = 11, Affective-Motivational Scale (AFF) = 10, the peak-frame (60) had AU<br />codes of 6c + 9b +43 which was equal to a Prkachin and Solomon Pain Intensity (PSPI) rating of 6 for that frame; (b) the ratings were OPI = 4, VAS =<br />6,SEN = 10,AFF = 7, the peak-frame (322) had AU codes of 4a + 6d + 7d+ 12d + 43 which was equal to a PSPI rating of 6 for that frame; (c) the ratings<br />were OPI=3, VAS=7,SEN=7,AFF=7, the peak-frame (352) had AU codes of 4e + 6a + 7e + 9d + 10d + 25c + 43 which was equal to a PSPI rating of 14<br />for that frame; (d) the ratings were OPI = 2, VAS = 6, SEN = 8, AFF = 5, the peak-frame (129) had AU codes of 4b + 6c + 12c + 43 which was equal<br />to a PSPI rating of 6 for that frame.<br />Examples of some of the sequences from the UNBC-McMaster Pain Shoulder Archive: (a) the sequence-level ratings were Observer Rated Pain<br />Outside of the ICU, most pain assessment is via self-<br />report. Self-reported pain is convenient and requires no<br />special skills or staffing, but has several limitations. Self-<br />report is subjective, lacks specific timing information about<br />whether pain is increasing, decreasing, or spiking, and cannot<br />be used when patients are impaired. Breathing tubes interfere<br />with speech, consciousness may be transient, and patients<br />may yet to have achieved functional speech (e.g., infants).<br />Over the past twenty years, significant efforts have been<br />made in identifying such facial actions<br />Recently, Prkachin and Solomon<br />Action Coding System (FACS) [17] based measure of pain<br />that can be applied on a frame-by-frame basis. A caveat on<br />this approach is that it must be performed offline, where<br />manual observations are both timely and costly, which makes<br />clinical use prohibitive. However, such information can be<br />used to train a real-time automatic system which could<br />potentially provide significant advantage in patient care and<br />cost reduction.<br />Researchers at the McMaster University and University<br />of Northern British Columbia (UNBC) captured video of<br />patient’s faces (who were suffering from shoulder pain)<br />while they were performing a series of active and passive<br />range-of-motion tests to their affected and unaffected limbs<br />on two separate occasions. Each video frame was fully<br />AU coded by certified FACS coders, and both observer<br />and self-report measures at the sequence level were taken<br />as well. To promote and facilitate research into pain as<br />well as facial expression detection, the first portion of this<br />dataset is now available for computer vision and pattern<br />recognition researchers. With their particular needs in mind<br />[14], [15], [16].<br />[16] validated a Facial<br />and through collaboration with CMU and University of<br />Pittsburgh, the UNBC-McMaster Shoulder Pain Expression<br />Archive includes:<br />1) Temporal Spontaneous Expressions: 200 video se-<br />quences containing spontaneous facial expressions re-<br />lating to genuine pain,<br />2) Manual FACS codes: 48,398 FACS coded frames,<br />3) Self-Report and Observer Ratings: associated pain<br />self-report and observer ratings at the sequence level,<br />4) Tracked Landmarks: 66 point AAM landmarks.<br />This paper documents this database and describes baseline<br />results and protocol based on our AAM/SVM system.<br />II. THE UNBC-MCMASTER SHOULDER PAIN<br />EXPRESSION ARCHIVE DATABASE<br />A total of 129 participants (63 male, 66 female) who were<br />self-identified as having a problem with shoulder pain were<br />recruited from 3 physiotherapy clinics and by advertisements<br />posted on the campus of the McMaster University. One<br />fourth were students and others were from the community<br />and included a wide variety of occupations. Diagnosis of<br />the shoulder pain varied, with participants suffering from<br />arthritis, bursitis, tendonitis, subluxation, rotator cuff injuries,<br />impingement syndromes, bone spur, capsulitis and disloca-<br />tion. Over half of the participants reported use of medication<br />for their pain.<br />All participants were tested in a laboratory room that<br />included a bed for performing passive range-of-motion tests.<br />After informed consent and information procedures were<br />completed, participants underwent eight standard range-of-<br />motion tests: abduction, flexion, and internal and external</p>  <p>Page 3</p> <p>Number of Images<br />Pitch (deg)Yaw (deg)Roll (deg)<br />?40?30?20?100 1020 30 40<br />0<br />500<br />1000<br />1500<br />?40?30?20?100 102030 40<br />0<br />500<br />1000<br />1500<br />?40?30?20?100 1020 3040<br />0<br />500<br />1000<br />1500<br />Friday, August 27, 2010<br />rotation is the same except that the arm is turned externally.<br />Abduction, flexion, and internal and external rotations were<br />performed under active and passive conditions. Active tests<br />differed from the passive tests in being under the control<br />of the patient who was instructed to move the limb as far<br />as possible. Active tests were performed with the patient<br />in a standing position. Passive tests were performed by a<br />physiotherapist who moved the limb until the maximum<br />range was achieved or was asked to stop by the patient.<br />During passive tests, the participant was resting in a supine<br />position on the bed with his or her head supported and<br />stabilized by a pillow. Active tests were performed prior to<br />passive tests because that is the usual sequence in which they<br />are conducted clinically. The order of tests within active and<br />passive conditions was randomized. Tests were performed<br />on both the affected and the unaffected limb to provide a<br />within-subject control.<br />During both active and passive tests, two Sony digital<br />cameras recorded participants’ facial expressions. Camera<br />orientation was initially frontal in the active condition, al-<br />though change in pose was common. Camera orientation<br />in the passive condition was approximately 70 degrees off<br />frontal, with the face viewed from below.<br />A card, listing verbal pain descriptors was available to help<br />participants provide verbal ratings of the pain produced on<br />each test. Each card displayed two Likert-type scales [19].<br />One consisted of words reflecting the sensory intensity of<br />pain. The other consisted of words reflecting the affective-<br />motivational dimension. These scales have been subject to<br />extensive psychophysical analyses, which have established<br />their properties as ratio-scale measures of the respective un-<br />derlying dimensions. Each scale had 15 items labelled from<br />“A” to “O”. The sensory scale (SEN) started at “extremely<br />weak” and finished at “extremely intense”; the affective-<br />motivational scale (AFF) started at “bearable” and finished<br />at “excruciating”. In addition, participants completed a series<br />of 10cm Visual Analog Scales (VAS), anchored at each end<br />with the words, “No pain” and “Pain as bad as could be”. The<br />Fig. 2. Histograms of the pitch, yaw and roll taken from the 3D AAM parameters across the UNBC-McMaster Shoulder Pain Expression Archive database.<br />rotation of each arm separately [18]. Abduction movements<br />involve lifting the arm forward and up in the sagittal plane.<br />In internal rotation, the arm is bent 90 degrees at the<br />elbow, abducted 90 degrees, and turned internally. External<br />three scales were completed by participants after each test.<br />Specifically, after each test, participants rated the maximum<br />pain it had produced using the sensory and affective verbal<br />pain descriptors and the VAS.<br />Offline, independent observers rated pain intensity (OPI)<br />from the recorded video. Observers had considerable training<br />in the identification of pain expression. Observer ratings<br />were performed on a 6-point Likert-type scale that ranged<br />from 0 (no pain) to 5 (strong pain). To assess inter-observer<br />reliability of the OPI pain ratings, 210 randomly selected<br />trials were independently rated by a second rater. The<br />Pearson correlation between the observers OPI was 0.80,<br />(p &lt; 0.001), which represents high inter-observer reliability<br />[20]. Correlation between the observers rating on the OPI and<br />subjects self-reported pain on the VAS was 0.74, (p &lt; 0.001)<br />for the trials used in the current study. A value of 0.70 is<br />considered a large effect [21] and is commonly taken as<br />indicating high concurrent validity. Thus, the inter-method<br />correlation found here suggests moderate to high concurrent<br />validity for pain intensity. Examples of the active portion of<br />the dataset with the associated self-report measures, along<br />with the FACS codes and frame-by-frame level pain rating<br />(see next subsection) is given in Figure 1.<br />A. FACS coding<br />Each test was extracted from the video and coded using<br />FACS [17]. Each facial action is described in terms of one<br />of 44 individual action units (AUs). Because there is a<br />considerable amount of literature in which FACS has been<br />applied to pain expression, only the actions that have been<br />implicated as possibly related to pain were focussed on:<br />brow-lowering (AU4), cheek-raising (AU6), eyelid tightening<br />(AU7), nose wrinkling (AU9), upper-lip raising (AU10),<br />oblique lip raising (AU12), horizontal lip stretch (AU20),<br />lips parting (AU25), jaw dropping (AU26), mouth stretching<br />(AU27) and eye-closure (AU43). With the exception of AU<br />43, each action was coded on a 5 level intensity dimension<br />(A-E) by one of three coders who were certified FACS<br />coders. Actions were coded on a frame-by-frame basis. All<br />coding was then reviewed by a fourth certified FACS coder.<br />To assess inter-observer agreement, 1738 frames selected<br />from one affected-side trial and one unaffected-side trial of</p>  <p>Page 4</p> <p>20 participants were randomly sampled and independently<br />coded. Intercoder percent agreement as calculated by the<br />Ekman-Friesen formula [17] was 95%, which compares<br />favorably with other research in the FACS literature.<br />B. Prkachin and Solomon Pain Intensity Scale<br />Beginning in 1992, Prkachin [15] found that four actions<br />- brow lowering (AU4), orbital tightening (AU6 and AU7),<br />levator contraction (AU9 and AU10) and eye closure (AU43)<br />- carried the bulk of information about pain. In a recent<br />follow up to this work, Prkachin and Solomon [16] confirmed<br />these four “core” actions contained the majority of pain<br />information. They defined pain as the sum of intensities of<br />brow lowering, orbital tightening, levator contraction and eye<br />closure. The Prkachin and Solomon pain intensity (PSPI)<br />metric is defined as:<br />Pain= AU4 + (AU6orAU7) +<br />(AU9orAU10) + AU43<br />(1)<br />That is, the sum of AU4, AU6 or AU7 (whichever is higher in<br />intensity), AU9 or AU10 (whichever is higher in intensity)<br />and AU43 to yield a 16-point scale1. For the example in<br />Figure 1(a), the peak frame here (60) has been coded as AU<br />6c + 9b + 43. This would result in a PSPI of 3+2+1 = 6.<br />Similarly in Figure 1(b), the peak frame has been coded as<br />AU 4a + 6d + 7d+ 12d + 43, which equals 1 + 4 + 1 =<br />6, as AU4 has an intensity of 1, AU6 and AU7 both have<br />intensity of 4 so just the maximum 4 is taken and AU43 has<br />an intensity of 1 (eyes are shut).<br />The PSPI [16] FACS pain scale is currently the only<br />metric which can define pain on a frame-by-frame basis. All<br />frames in this dataset were coded using the PSPI. For more<br />information on the relative merits of the particular self-report<br />measures and how they relate to PSPI and FACS, please refer<br />to [16].<br />C. Analysis of Distributed Portion of the Pain Corpora<br />From the entire available UNBC-McMaster Pain Shoulder<br />Archive, 200 sequences from 25 different subjects in the ac-<br />tive portion of the dataset has been prepared for distribution<br />to the research community. From these 200 sequences there<br />is a total of 48398 frames that have been FACS coded and<br />AAM tracked. The inventory of the total number of frames<br />which have been coded from each AU and their intensity is<br />given in Table I. The number of frames and the associated<br />PSPI score is given in Table II. From this, it can be seen that<br />83.6% of the frames had a PSPI score of 0, and 16.4% had<br />frames in which had a PSPI of score ≥ 1.<br />Examples of this data are given in Figure 1 and it is<br />apparent that there is some head movement that occurs dur-<br />ing these sequences. To quantify how much head movement<br />1The intensity of action units (AUs) are scored on a 6-point intensity scale<br />that ranges from 0 (absent) to 5 (maximum intensity). Eye closing (AU43)<br />binary (0 = absent, 1 = present). In FACS terminology, ordinal intensity is<br />denoted by letters rather than numeric weights, i.e., 1 = A, 2 = B, . . . 5 =<br />E.<br />TABLE I<br />The AU inventory on the UNBC-McMaster Shoulder Pain Archive, where<br />the frequency of each AU and its intensity is given along with the total.<br />Note that for AU43, the only intensity is A (i.e. they eye can only be open<br />or shut).<br />AU<br />4<br />6<br />7<br />9<br />10<br />12<br />20<br />25<br />26<br />43<br />ABCD<br />74<br />681<br />305<br />76<br />61<br />736<br />0<br />138<br />478<br />–<br />E<br />64<br />110<br />100<br />35<br />22<br />49<br />20<br />88<br />1<br />–<br />Total<br />1074<br />5557<br />3366<br />423<br />525<br />6887<br />706<br />2407<br />2093<br />2434<br />202<br />1776<br />1362<br />93<br />171<br />2145<br />286<br />767<br />431<br />2434<br />509<br />1663<br />991<br />151<br />208<br />1799<br />282<br />803<br />918<br />–<br />225<br />1327<br />608<br />68<br />63<br />2158<br />118<br />611<br />265<br />–<br />TABLE II<br />The inventory on the UNBC-McMaster Shoulder Pain Archive according<br />to the Prkachin-Solomon Pain Intensity (PSPI) pain metric, where the<br />frequency of each pain intensity is given.<br />PSPI Score<br />0<br />1-2<br />3-4<br />5-6<br />7-8<br />9-10<br />11-12<br />13-14<br />15-16<br />Frequency<br />40029<br />5260<br />2214<br />512<br />132<br />99<br />124<br />23<br />5<br />occurred, we used the 3D parameters from the AAM to<br />estimate the pitch, yaw and roll [22]. The histograms of these<br />parameters are shown in Figure 2. In terms of pitch, yaw<br />and roll the mean was -0.38, -0.21 and -0.23 degrees and<br />the variance was 23.58, 40.82 and 33.28. However, these<br />parameters differed quite a bit when a person was in no-<br />pain (PSPI=0) and in pain (PSPI≥1). When the PSPI was<br />equal to 0 the variance in terms of pitch, yaw and roll was<br />22.69, 37.03 and 29.19. When the PSPI was ≥ 1, the variance<br />increased to 26.72, 55.61 and 48.52 which suggested that<br />head movement coincided with painful facial expression.<br />Overall, close to 90% of all frames in this distribution were<br />within 10 degrees of being fully frontal and over 99% were<br />within 20 degrees from the fully frontal view.<br />At the sequence level we show the inventory of some<br />self-report and observer measures. Table III shows the in-<br />ventory of the visual analogue scale (VAS) and observer<br />pain intensity (OPI) measures for the 200 sequences. On<br />the left side of the table, it can be said that there is a nice<br />spread of VAS measures from 0-10. With the OPI measures,<br />there is slightly less than half with no observable pain. For<br />the sequence-level experiments, we will be using the OPI</p>  <p>Page 5</p> <p>TABLE III<br />The inventory on the self-report and observer measures of the<br />UNBC-McMaster Shoulder Pain Archive at the sequence level. The<br />self-report Visual Analogue Scale (VAS), ranging from 0 (no-pain) to 10<br />(extreme pain) and the Observed Pain Intensity (OPI), ranging from 0<br />(no-pain observed) to 5 (extreme pain observed).<br />VAS<br />0<br />1<br />2<br />3<br />4<br />5<br />6<br />7<br />8<br />9<br />10<br />Total<br />Frequency<br />35<br />42<br />24<br />20<br />21<br />11<br />11<br />6<br />18<br />10<br />2<br />200<br />OPI<br />0<br />1<br />2<br />3<br />4<br />5<br />Total<br />Frequency<br />92<br />25<br />26<br />34<br />16<br />7<br />200<br />ratings so as we can have our automatic system to mimic that<br />of a human observer. The affective and sensory self-report<br />measures across the 200 sequences will also be available in<br />the distribution.<br />III. AAM LANDMARKS<br />In our system, we employ an Active Appearance Model<br />(AAM) based system which uses AAMs to track the face and<br />extract visual features. In the data distribution we include the<br />66 point AAM landmark points for each image. This section<br />describes how these landmarks were generated.<br />A. Active Appearance Models (AAMs)<br />Active Appearance Models (AAMs) have been shown to<br />be a good method of aligning a pre-defined linear shape<br />model that also has linear appearance variation, to a previ-<br />ously unseen source image containing the object of interest.<br />In general, AAMs fit their shape and appearance components<br />through a gradient-descent search, although other optimiza-<br />tion methods have been employed with similar results [23].<br />The shape s of an AAM [23] is described by a 2D<br />triangulated mesh. In particular, the coordinates of the mesh<br />vertices define the shape s = [x1,y1,x2,y2,...,xn,yn],<br />where n is the number of vertices. These vertex locations<br />correspond to a source appearance image, from which the<br />shape was aligned. Since AAMs allow linear shape variation,<br />the shape s can be expressed as a base shape s0plus a linear<br />combination of m shape vectors si:<br />s = s0+<br />m<br />?<br />i=1<br />pisi<br />(2)<br />where the coefficients p = (p1,...,pm)Tare the shape<br />parameters. These shape parameters can typically be divided<br />into rigid similarity parameters ps and non-rigid object<br />deformation parameters po, such that pT= [pT<br />ilarity parameters are associated with a geometric similarity<br />s,pT<br />o]. Sim-<br />Shape RMS error<br />Proportion of Images<br />0510 1520<br />0<br />0.2<br />0.4<br />0.6<br />0.8<br />1<br />   0                       5                       10                      15                     20<br />1<br />0.8<br />0.6<br />0.4<br />0.2<br />0<br />Monday, September 20, 2010<br />Fig. 3.<br />landmark points. The Shape RMS error refers to the total shape RMS error<br />in terms of pixels after all meshes were similarity normalized with an inter-<br />occular distance of 50 pixels.<br />Fitting curve for the AAM compared against the manual 68 point<br />transform (i.e. translation, rotation and scale). The object-<br />specific parameters, are the residual parameters representing<br />non-rigid geometric variations associated with the determing<br />object shape (e.g., mouth opening, eyes shutting, etc.). Pro-<br />crustes alignment [23] is employed to estimate the base shape<br />s0.<br />Keyframes within each video sequence were manually<br />labelled, while the remaining frames were automatically<br />aligned using a gradient descent AAM fitting algorithm<br />described in [24]. Figure 4 shows the AAM in action, with<br />the 68 point mesh being fitted to the patient’s face in every<br />frame.From the 2D shape model we can derive the 3D<br />parameters using non-rigid structure from motion. See [22]<br />for full details.<br />B. AAM Accuracy<br />In checking the AAM alignment accuracy to manually<br />landmarked images, we first similarity normalized all tracked<br />AAM points and manual landmarks to a common mesh<br />size and rotation, with an inter-occular distance of 50 pixels<br />and aligned to the centre of the eye coordinates. We then<br />compared 2584 manually landmarked images against their<br />AAM counterpart. The fitting curve for the AAM is shown<br />in Figure 3. As can be seen in this curve, nearly all of the<br />AAM landmarks are within 2 pixels RMS error of the manual<br />landmarks, which is negligible when one considers that this<br />is based on a distance of 50 pixels between the center of<br />the eyes. This highlights the benefit of employing person-<br />specific model such as an AAM, as near perfect alignment<br />can result.<br />IV. EXPERIMENTS<br />In this section, we describe two experiments that we<br />conducted for i) AU and ii) pain detection at a frame-level.<br />We first describe our baseline AAM/SVM system.</p>  <p>Page 6</p> <p>−60−40−200 20 4060<br />−60<br />−40<br />−20<br />0<br />20<br />40<br />60<br />80<br />AAM TrackingAAM Features<br />tober 4, 2010<br />Fig. 4.<br />some feature representations: (top) SPTS - similarity normalized shape and<br />(bottom) CAPP - canonical normalized appearance.<br />Once the AAM has tracked a person’s face we can derive<br />A. AAM/SVM Baseline System<br />Once we have tracked the patient’s face by estimating the<br />shape and appearance AAM parameters, we can use this<br />information to derive features from the face. From the initial<br />work conducted in [9], [25], [12], we extracted the following<br />features:<br />• SPTS: The similarity normalized shape or points, sn,<br />refers to the 66 vertex points in snfor both the x- and y-<br />coordinates, resulting in a raw 132 dimensional feature<br />vector. These points are the vertex locations after all<br />the rigid geometric variation (translation, rotation and<br />scale), relative to the base shape, has been removed.<br />The similarity normalized shape sncan be obtained by<br />synthesizing a shape instance of s, using Equation 2,<br />that ignores the similarity parameters p.<br />• CAPP: The canonical normalized appearance, a0<br />refers to where all the non-rigid shape variation has been<br />normalized with respect to the base shape s0. This is<br />accomplished by applying a piece-wise affine warp on<br />each triangle patch appearance in the source image so<br />that it aligns with the base face shape. For this study,<br />the resulting 87 × 93 synthesized grayscale image was<br />used.<br />Support vector machines (SVMs) were then used to clas-<br />sify individual action units as well as pain. SVMs attempt<br />to find the hyperplane that maximizes the margin between<br />positive and negative observations for a specified class. A<br />linear kernel was used in our experiments due to its ability<br />to generalize well to unseen data in many pattern recognition<br />tasks [26]. LIBSVM was used for the training and testing of<br />SVMs [27].<br />In all experiments conducted, a leave-one-subject-out<br />strategy was used and each AU and pain detector was trained<br />using positive examples which consisted of the frames that<br />the FACS coder labelled containing that particular AU (re-<br />gardless of intensity, i.e. A-E) or pain intensity of 1 or more.<br />The negative examples consisted of all the other frames that<br />were not labelled with that particular AU or had a pain<br />intensity of 0.<br />In order to predict whether or not a video frame contained<br />an AU or pain, the output score from the SVM was used. As<br />TABLE IV<br />Results showing the area underneath the ROC curve for the<br />similarity-normalized shape (SPTS) and appearance (SAPP) as well as the<br />canonical appearance (CAPP) features. Note the average is a weighted<br />one, depending on the number of positive examples.<br />AU<br />4<br />6<br />7<br />9<br />10<br />12<br />20<br />25<br />26<br />43<br />AVG<br />SPTS CAPP<br />60.0 ± 1.5<br />85.1 ± 0.5<br />82.6 ± 0.8<br />84.1 ± 1.6<br />83.2 ± 1.9<br />84.6 ± 0.5<br />61.7 ± 1.9<br />70.9 ± 1.0<br />54.7 ± 1.1<br />86.7 ± 0.7<br />79.2 ± 0.8<br />SPTS&amp;CAPP<br />57.1 ± 1.5<br />85.4 ± 0.5<br />80.4 ± 0.7<br />85.3 ± 1.7<br />89.2 ± 1.4<br />85.7 ± 0.4<br />77.9 ± 1.6<br />78.0 ± 0.8<br />71.0 ± 1.0<br />87.5 ± 0.7<br />81.8 ± 0.8<br />72.5 ± 3.1<br />80.1 ± 1.7<br />71.3 ± 0.8<br />75.1 ± 2.4<br />87.9 ± 1.7<br />79.4 ± 0.5<br />75.7 ± 1.7<br />78.8 ± 0.9<br />73.5 ± 1.1<br />83.1 ± 0.6<br />78.0 ± 0.8<br />there are many more frames with no behavior of interest than<br />frames of interest, the overall agreement between correctly<br />classified frames can skew the results somewhat. As such<br />we used the receiver-operator characteristic (ROC) curve,<br />which is a more reliable performance measure. This curve is<br />obtained by plotting the hit-rate (true positives) against the<br />false alarm rate (false positives) as the decision threshold<br />varies. From the ROC curves, we used the area under the<br />ROC curve (A?), to assess the performance. The A?metric<br />ranges from 50 (pure chance) to 100 (ideal classification)2.<br />An upper-bound on the uncertainty of the A?statistic was<br />obtained using the formula s =<br />are the number of positive and negative examples [28], [8].<br />?<br />A?(100−A?)<br />min{np,nn}where np,nn<br />B. AU Detection Results<br />We conducted detection for ten AUs (4, 6, 7, 9, 10, 12,<br />20, 25, 26 and 43). The results for the AU detection with<br />respect to the similarity-normalized shape (SPTS), the canon-<br />ical appearance (CAPP) and the combined (SPTS+CAPP)<br />features are shown in Table IV. In terms of the overall<br />average accuracy of the AU detection, the performance is<br />rather good with combined representation gaining the best<br />overall performance of 81.8, slightly better than CAPP (79.2)<br />and SPTS (78.0).<br />In terms of individual AU detection, it can be seen that<br />best performance is gained for the strong expressions such<br />as AU6, 10, 12 and 43. Due to the amount of very strong<br />examples in the distribution (i.e. AU intensity is greater than<br />A), it can be seen that robust performance can be gained.<br />For full analysis of AU experiments see [12].<br />C. Pain Detection at Frame-level<br />The results for automatically detecting pain are given<br />in Figure 5, which shows a clearer view of the trend we<br />observed in the AU detection results. For the individual<br />2In literature, the A?metric varies from 0.5 to 1, but for this work we<br />have multiplied the metric by 100 for improved readability of results</p>  <p>Page 7</p> <p>CAPP<br />of pain, the processes by which pain expression is perceived<br />and the role of pain expression in clinical assessment of<br />people suffering from pain conditions. Participants provided<br />informed consent for use of their video images for scientific<br />study of the perception of pain including pain detection.<br />Distribution of the database is governed by the terms of their<br />informed consent. Investigators who for scientific purposes<br />are interested in undertaking studies that can be clearly<br />construed as having the potential to advance understanding<br />of the perception of pain expression or contributing to the<br />development of improved techniques for clinical assessment<br />of pain conditions may make application for access to the<br />database. Computer vision studies, which provide a means<br />of modeling human decoding of pain expression, fall into<br />the category of perception of pain expression. Applications<br />should indicate how the proposed work addresses advance-<br />ment of knowledge in the perception of pain expression or<br />improved clinical assessment. Approved recipients of the<br />data may not redistribute it and agree to the terms of con-<br />fidentiality restrictions. Use of the database for commercial<br />purposes is strictly prohibited.<br />This data will be available from March 2011. If interested<br />in obtaining the database, please sign and return an agree-<br />ment form available from<br />˜jeffcohn/PainArchive/. Once the signed form has<br />been received, you may expect to receive instructions within<br />5 business days.<br />TS<br />Area Underneath ROC <br />(A’)<br />75<br />78<br />81<br />84<br />SPTSCAPPSPTS+CAPP<br />Fig. 5.<br />detection at the frame-level (yellow = SPTS, green = CAPP). The upper-<br />bound error for all feature sets varied from approximately ±0.67 to 0.80.<br />The performance of the various features for the task of pain<br />feature sets, SPTS achieved 76.9 area underneath the ROC<br />curve and then the CAPP features yielding the best results<br />with 80.9. When we combine the different feature sets, we<br />again see the benefit of fusing the various representations<br />together showing that there exists complimentary information<br />with the performance increasing to 83.9%.<br />V. DISTRIBUTION DETAILS<br />The data was collected in the course of a research program<br />devoted to understanding the properties of facial expressions<br />http://www.pitt.edu/<br />VI. CONCLUSIONS AND FUTURE WORK<br />In this paper we have described the UNBC-McMaster<br />Shoulder Pain Expression Archive which contains, 1) 200<br />video sequences containing spontaneous facial expressions;<br />2) 48,398 FACS coded frames, 3) pain frame-by-frame<br />scores, sequence-level self-report and observer measures; and<br />4) 66-point AAM landmarks. We have released this data in<br />an effort to address the lack of FACS coded spontaneous<br />expressions available for researchers as well as promoting<br />and facilitating research into the perception of pain. We have<br />also included baseline results from our AAM/SVM system.<br />Pain detection represents a key application in which facial<br />expression recognition could be applied successfully, espe-<br />cially if applied in the context of an heavily constrained situ-<br />ation such as an ICU ward where the number of expressions<br />is greatly limited. This is in compared to the situation where<br />a person is mobile and expresses a broad gamut of emotions,<br />where the approach we have taken here would be of little use<br />as the painful facial actions are easily confused with other<br />emotions (such as sadness, fear and surprise). For this to<br />occur, a very large dataset which is captured in conditions<br />that are indicative of the behavior to be expected in addition<br />to being accurately coded needs to be collected. Another<br />issue is the requirement of the detection in terms of timing<br />accuracy. In our system presented here, we detect pain at<br />every frame. However, at what level does this need to be<br />accurate at - milliseconds, seconds or minutes? Again this is<br />depends on the context in which this system will be used.<br />A more likely scenario would be to detect pain as an event<br />or at a sequence level (i.e. if a person was in pain over a<br />window of 1 to 2 mins). We plan to look into this area in<br />the future.<br />VII. ACKNOWLEDGMENTS<br />Zara Ambadar, Nicole Grochowina, Amy Johnson, David<br />Nordstokke, Nicole Ridgeway, Racquel Kueffner, Shawn<br />Zuratovic and Nathan Unger provided technical assistance.<br />REFERENCES<br />[1] Y. Tian, J. Cohn, and T. Kanade, “Facial expression analysis,” in The<br />handbook of emotion elicitation and assessment, S. Li and A. Jain,<br />Eds.New York, NY, USA: Springer, pp. 247–276.<br />[2] Y. Tong, W. Liao, and Q. Ji, “Facial Action Unit Recognition by<br />Exploiting Their Dynamic and Semantic Relationships,” IEEE Trans-<br />actions on Pattern Analysis and Machine Intelligence, vol. 29, no. 10,<br />pp. 1683–1699, 2007.<br />[3] Z. Zeng, M. Pantic, G. Roisman, and T. Huang, “A Survey of Affect<br />Recognition Methods: Audio, Visual and Spontaneous Expressions,”<br />IEEE Transactions on Pattern Analysis and Machine Intelligence,<br />vol. 31, no. 1, pp. 39–58, 2009.<br />[4] R. Gross, I. Matthews, S. Baker, and T. Kanade, “The CMU Multiple<br />Pose, Illumination, and Expression (MultiPIE),” Robotics Institute,<br />Carnegie Mellon University, Tech. Rep., 2007.<br />[5] M. Valstar and M. Pantic, “Induced Disgust, Happiness and Surprise:<br />an Addition to the MMI Facial Expression Database,” in Proceed-<br />ings of the 3rd International Workshop on EMOTION: Corpora for<br />Research on Emotion and Affect, 2010.<br />[6] L. Yin, X. Chen, Y. Sun, T. Worm, and M. Reale, “A High-resolution<br />3D Dynamic Facial Expression Database ,” in Proceedings of the<br />International Conference on Automatic Face and Gesture Recognition,<br />2008.<br />[7] M. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and<br />J. Movellan, “Automatic Recognition of Facial Actions in Spontaneous<br />Expressions,” Journal of Multimedia, 2006.</p>  <p>Page 8</p> <p>[8] J. Whitehill, G. Littlewort, I. Fasel, M. Bartlett, and J. Movellan,<br />“Towards Practical Smile Detection,” IEEE Transactions on Pattern<br />Analysis and Machine Intelligence, vol. 31, no. 11, pp. 2106–2111,<br />2009.<br />[9] A. Ashraf, S. Lucey, J. Cohn, T. Chen, Z. Ambadar, K. Prkachin,<br />P. . Solomon, and B.-J. Theobald, “The painful face: pain expression<br />recognition using active appearance models,” in Proceedings of the 9th<br />international conference on Multimodal interfaces.<br />Japan: ACM, 2007, pp. 9–14.<br />[10] A. Ashraf, S. Lucey, J. Cohn, K. M. Prkachin, and P. Solomon, “The<br />Painful Face II– Pain Expression Recognition using Active Appearance<br />Models,” Image and Vision Computing, vol. 27, no. 12, pp. 1788–1796,<br />2009.<br />[11] P. Lucey, J. Cohn, S. Lucey, I. Matthews, S. Sridharan, and<br />K. Prkachin, “Automatically Detecting Pain Using Facial Actions,” in<br />Proceedings of the International Conference on Affective Computing<br />and Intelligent Interaction, 2009, pp. 1–8.<br />[12] P. Lucey, J. Cohn, I. Matthews, S. Lucey, J. Howlett, S. Sridharan, and<br />K. Prkachin, “Automatically Detecting Pain in Video Through Facial<br />Action Units,” IEEE Transactions on Systems Man and Cybernetics,<br />Part B, 2010.<br />[13] A. Gawande, The Checklist Manifesto: How to Get Things Right.<br />Metropolitan Books, 2010.<br />[14] K. Craig, K. Prkachin, and R. Grunau, “The facial expression of pain,”<br />in Handbook of pain assessment.<br />[15] K. Prkachin, “The consistency of facial expressions of pain: a com-<br />parison across modalities,” Pain, vol. 51, pp. 297–306, 1992.<br />[16] K. Prkachin and P. Solomon, “The structure, reliability and validity<br />of pain expression: Evidence from patients with shoulder pain,” Pain,<br />vol. 139, pp. 267–274, 2008.<br />[17] P. Ekman, W. Friesen, and J. Hager, Facial Action Coding System:<br />Research Nexus. Salt Lake City, UT, USA: Network Research<br />Information, 2002.<br />[18] K. Prkachin and S. Mercer, “Pain expression in patients with shoulder<br />pathology: validity, coding properties and relation to sickness impact,”<br />Pain, vol. 39, pp. 257–265, 1989.<br />[19] M. Heft, R. Gracely, R. Dubner, and P. McGrath, “A validation model<br />for verbal descriptor scaling of human clinical pain,” Pain, vol. 9, pp.<br />363–373, 1980.<br />[20] A. Anastasi, Psychological Testing.<br />[21] J. Cohen, Statistical Power Analysis for the Social Sciences. Lawrence<br />Erlbaum Associates, NJ, USA, 1988.<br />[22] J. Xiao, S. Baker, I. Matthews, and T. Kanade, “Real-Time Combined<br />2D+3D Active Appearance Models,” in Proceedings of the IEEE<br />Conference on Computer Vision and Pattern Recognition, 2004, pp.<br />535–542.<br />[23] T. Cootes, G. Edwards, and C. Taylor, “Active Appearance Models,”<br />IEEE Transactions on Pattern Analysis and Machine Intelligence,<br />vol. 23, no. 6, pp. 681–685, 2001.<br />[24] I. Matthews and S. Baker, “Active appearance models revisited,”<br />International Journal of Computer Vision, vol. 60, no. 2, pp. 135–<br />164, 2004.<br />[25] S. Lucey, A. Ashraf, and J. Cohn, “Investigating spontaneous facial<br />action recognition through aam representations of the face,” in Face<br />Recognition Book, K. Kurihara, Ed.<br />[26] C. Hsu, C. C. Chang, and C. J. Lin, “A practical guide to support<br />vector classification,” Tech. Rep., 2005.<br />[27] C.-C. Chang and C.-J. Lin, LIBSVM: a library for support vector ma-<br />chines, 2001, software available at http://www.csie.ntu.edu.tw/∼cjlin/<br />libsvm.<br />[28] C. Cortes and M. Mohri, “Confidence Intervals for the Area Under<br />the ROC curve,” Advances in Neural Information Processing Systems,<br />2004.<br />Nagoya, Aichi,<br />Macmillan, NY, USA, 1982.<br />Pro Literatur Verlag, 2007.</p>  <a href="https://www.researchgate.net/profile/Kenneth_Prkachin/publication/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database/links/00b495220a93e5795f000000.pdf">Download full-text</a> </div> <div id="rgw24_56ab1977ab01e" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw25_56ab1977ab01e">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw26_56ab1977ab01e"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Kenneth_Prkachin/publication/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database/links/00b495220a93e5795f000000.pdf" class="publication-viewer" title="2011_PAINFUL.pdf">2011_PAINFUL.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Kenneth_Prkachin">Kenneth M Prkachin</a> &middot; Jan 21, 2016 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw27_56ab1977ab01e"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://www.iainm.com/iainm/Publications_files/2011_PAINFUL.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Painful data: The UNBC-McMaster shoulder pain expression archive database">Painful data: The UNBC-McMaster shoulder pain expr...</a> </div>  <div class="details">   Available from <a href="http://www.iainm.com/iainm/Publications_files/2011_PAINFUL.pdf" target="_blank" rel="nofollow">iainm.com</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw34_56ab1977ab01e" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (53) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw35_56ab1977ab01e" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw36_56ab1977ab01e" >  <div class="indent-left">  <div id="rgw37_56ab1977ab01e" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/267155572_Automatic_Behaviour_Understanding_in_Medicine">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Michel_Valstar" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Michel Valstar </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw38_56ab1977ab01e">  <li class="citation-context-item"> "Automatic recognition of pain focuses almost entirely on cues from the face. Lucey et al. report baseline recognition models based on AAM features and Support Vector Machine (SVM) classifiers to recognise pain versus non pain with an area under the ROC curve (AUC) score of 83.9% on the UNBC-McMaster dataset [23]. Many further studies on this dataset followed, e.g. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Conference Paper:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/267155572_Automatic_Behaviour_Understanding_in_Medicine"> <span class="publication-title js-publication-title">Automatic Behaviour Understanding in Medicine</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/70485381_Michel_Valstar" class="authors js-author-name ga-publications-authors">Michel Valstar</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Now that Affective Computing and Social Signal Process-ing methods are becoming increasingly robust and accu-rate, novel areas of applications with significant societal im-pact are opening up for exploration. Perhaps one of the most promising areas is the application of automatic expres-sive behaviour understanding to help diagnose, monitor, and treat medical conditions that themselves alter a person&#39;s so-cial and affective signals. This work argues that this is now essentially a new area of research, called behaviomedics. It gives a definition of the area, discusses the most important groups of medical conditions that could benefit from this, and makes suggestions for future directions. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Conference Paper &middot; Nov 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Michel_Valstar/publication/267155572_Automatic_Behaviour_Understanding_in_Medicine/links/5446ca730cf22b3c14e0b418.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw39_56ab1977ab01e" >  <div class="indent-left">  <div id="rgw40_56ab1977ab01e" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/281811172_Handling_Data_Imbalance_in_Automatic_Facial_Action_Intensity_Estimation">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Frerk_Saxen" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Frerk Saxen </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw41_56ab1977ab01e">  <li class="citation-context-item"> "), 12 AUs (DISFA [16]), resp. 10 AUs (UNBC-McMaster [15]). (b) Mean cross-dataset performance of the seven common AUs (SVR Ensemble with α = 0.5). " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Conference Paper:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/281811172_Handling_Data_Imbalance_in_Automatic_Facial_Action_Intensity_Estimation"> <span class="publication-title js-publication-title">Handling Data Imbalance in Automatic Facial Action Intensity Estimation</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2029052295_Frerk_Saxen" class="authors js-author-name ga-publications-authors">Frerk Saxen</a> &middot;     <a href="researcher/2043200184_Philipp_Werner" class="authors js-author-name ga-publications-authors">Philipp Werner</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Automatic Action Unit (AU) intensity estimation is a key problem in facial expression analysis. But limited research attention has been paid to the inherent class imbalance, which usually leads to suboptimal performance. To handle the imbalance, we propose (1) a novel multiclass under-sampling method and (2) its use in an ensemble. We compare our approach with state of the art sampling methods used for AU intensity estimation. Multiple datasets and widely varying performance measures are used in the literature, making direct comparison difficult. To address these shortcomings, we compare different performance measures for AU intensity estimation and evaluate our proposed approach on three publicly available datasets, with a comparison to state of the art methods along with a cross dataset evaluation. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Conference Paper &middot; Sep 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Frerk_Saxen/publication/281811172_Handling_Data_Imbalance_in_Automatic_Facial_Action_Intensity_Estimation/links/55f920c508aec948c48d3a75.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw42_56ab1977ab01e" >  <div class="indent-left">  <div id="rgw43_56ab1977ab01e" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/277312235_Ensemble_of_Hankel_Matrices_for_Face_Emotion_Recognition">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Liliana_Lo_Presti" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Liliana Lo Presti </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw44_56ab1977ab01e">  <li class="citation-context-item"> "Nonetheless, recognition of face expressions and emotions is of great interest in many fields such as assistive technologies [21], [10], socially assistive robotics [23], computational behavioral science [25], [18], [35], and the emerging field of audience measurement [11]. A vast literature on affective computing [35], [27], [21], has shown that an emotion can be identified by a subset of detected action units. This suggests that face emotion results as combination of movements of various facial muscles. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Conference Paper:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/277312235_Ensemble_of_Hankel_Matrices_for_Face_Emotion_Recognition"> <span class="publication-title js-publication-title">Ensemble of Hankel Matrices for Face Emotion Recognition</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/71126356_Liliana_Lo_Presti" class="authors js-author-name ga-publications-authors">Liliana Lo Presti</a> &middot;     <a href="researcher/7142168_Marco_La_Cascia" class="authors js-author-name ga-publications-authors">Marco La Cascia</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> In this paper, a face emotion is considered as the result of the composition of multiple concurrent signals, each corresponding to the movements of a specific facial muscle. These concurrent signals are represented by means of a set of multi-scale appearance features that might be correlated with one or more concurrent signals. 
The extraction of these appearance features from a sequence of face images yields to a set of time series. 
This paper proposes to use the dynamics regulating each appearance feature time series to recognize among different face emotions. To this purpose, an ensemble of Hankel matrices corresponding to the extracted time series is used for emotion classification within a framework that combines nearest neighbor and a majority vote schema.
Experimental results on a public available dataset shows that the adopted representation is promising and yields state-of-the-art accuracy in emotion classification. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Conference Paper &middot; Sep 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Liliana_Lo_Presti/publication/277312235_Ensemble_of_Hankel_Matrices_for_Face_Emotion_Recognition/links/557fe48608aeb61eae26314d.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw29_56ab1977ab01e" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw30_56ab1977ab01e">  </ul> </div> </div>   <div id="rgw20_56ab1977ab01e" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw21_56ab1977ab01e"> <div> <h5> <a href="publication/257093317_Painful_monitoring_Automatic_pain_monitoring_using_the_UNBC-McMaster_shoulder_pain_expression_archive_database" class="color-inherit ga-similar-publication-title"><span class="publication-title">Painful monitoring: Automatic pain monitoring using the UNBC-McMaster shoulder pain expression archive database</span></a>  </h5>  <div class="authors"> <a href="researcher/35740604_Patrick_Lucey" class="authors ga-similar-publication-author">Patrick Lucey</a>, <a href="researcher/9304721_Jeffrey_F_Cohn" class="authors ga-similar-publication-author">Jeffrey F. Cohn</a>, <a href="researcher/38482384_Kenneth_M_Prkachin" class="authors ga-similar-publication-author">Kenneth M. Prkachin</a>, <a href="researcher/13688935_Patricia_E_Solomon" class="authors ga-similar-publication-author">Patricia E. Solomon</a>, <a href="researcher/71055482_Sien_Chew" class="authors ga-similar-publication-author">Sien Chew</a>, <a href="researcher/38465491_Iain_Matthews" class="authors ga-similar-publication-author">Iain Matthews</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw22_56ab1977ab01e"> <div> <h5> <a href="publication/224006540_Italian_cross-cultural_adaptation_and_validation_of_three_different_scales_for_the_evaluation_of_shoulder_pain_and_dysfunction_after_neck_dissection_University_of_California_-_Los_Angeles_UCLA_Shoulde" class="color-inherit ga-similar-publication-title"><span class="publication-title">Italian cross-cultural adaptation and validation of three different scales for the evaluation of shoulder pain and dysfunction after neck dissection: University of California - Los Angeles (UCLA) Shoulder Scale, Shoulder Pain and Disability Index (SPADI) and Simple Shoulder Test (SST)</span></a>  </h5>  <div class="authors"> <a href="researcher/73736103_C_Marchese" class="authors ga-similar-publication-author">C Marchese</a>, <a href="researcher/38876090_G_Cristalli" class="authors ga-similar-publication-author">G Cristalli</a>, <a href="researcher/39060131_B_Pichi" class="authors ga-similar-publication-author">B Pichi</a>, <a href="researcher/8780514_V_Manciocco" class="authors ga-similar-publication-author">V Manciocco</a>, <a href="researcher/58600042_G_Mercante" class="authors ga-similar-publication-author">G Mercante</a>, <a href="researcher/39913246_R_Pellini" class="authors ga-similar-publication-author">R Pellini</a>, <a href="researcher/39060543_P_Marchesi" class="authors ga-similar-publication-author">P Marchesi</a>, <a href="researcher/45533784_I_Sperduti" class="authors ga-similar-publication-author">I Sperduti</a>, <a href="researcher/38238104_P_Ruscito" class="authors ga-similar-publication-author">P Ruscito</a>, <a href="researcher/39834216_G_Spriano" class="authors ga-similar-publication-author">G Spriano</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw23_56ab1977ab01e"> <div> <h5> <a href="publication/230613508_Measures_of_adult_shoulder_function_Disabilities_of_the_Arm_Shoulder_and_Hand_Questionnaire_DASH_and_its_short_version_QuickDASH_Shoulder_Pain_and_Disability_Index_SPADI_American_Shoulder_and_Elbow_Su" class="color-inherit ga-similar-publication-title"><span class="publication-title">Measures of adult shoulder function: Disabilities of the Arm, Shoulder, and Hand Questionnaire (DASH) and its short version (QuickDASH), Shoulder Pain and Disability Index (SPADI), American Shoulder and Elbow Surgeons (ASES) Society standardized shoulder assessment form, Constant (Murley) Score (CS), Simple Shoulder Test (SST), Oxford Shoulder Score (OSS), Shoulder Disability Questionnaire (SDQ), and Western Ontario Shoulder Instability Index (WOSI)</span></a>  </h5>  <div class="authors"> <a href="researcher/39806846_Felix_Angst" class="authors ga-similar-publication-author">Felix Angst</a>, <a href="researcher/6828475_Hans-Kaspar_Schwyzer" class="authors ga-similar-publication-author">Hans-Kaspar Schwyzer</a>, <a href="researcher/39604159_Andre_Aeschlimann" class="authors ga-similar-publication-author">André Aeschlimann</a>, <a href="researcher/38325822_Beat_R_Simmen" class="authors ga-similar-publication-author">Beat R Simmen</a>, <a href="researcher/14927642_Joerg_Goldhahn" class="authors ga-similar-publication-author">Jörg Goldhahn</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw54_56ab1977ab01e" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw55_56ab1977ab01e">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw56_56ab1977ab01e" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=kzXWBrKa4PFgMAITXHWsiC2E8wo0m2KOSLUKkddv4KvXQmD1_R8HXkYiLC8n1TTf" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="SG/VDMX3uQCqw65vKg32jIE7Jb86r+9M0oxm0yw8sjvooj1tdjvXRl3mORaglpaDfHQK5yvMblFWTEkisoH6WVLMkA2Nej9BQI3Ssr4xdnTamx8e2PwzVjAiulwwUTsHRtsBzcRjIp4TWZsXoQ/wnOaXawNBF+9ok+N4Nh1rj+WQeL8LcvCRuhi6odLl3oJURDlE906Wijg4dHHyh3/HwI5nKm+bh7GIaIqFfGhCu4iCiOinkLETI5U1pbyfec9BdGC0CKysqR7KivEdOHoCnNDrrTBAK0RmhbsLcpnBlSU="/> <input type="hidden" name="urlAfterLogin" value="publication/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjIxMjkyNTQ0X1BhaW5mdWxfZGF0YV9UaGVfVU5CQy1NY01hc3Rlcl9zaG91bGRlcl9wYWluX2V4cHJlc3Npb25fYXJjaGl2ZV9kYXRhYmFzZQ%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjIxMjkyNTQ0X1BhaW5mdWxfZGF0YV9UaGVfVU5CQy1NY01hc3Rlcl9zaG91bGRlcl9wYWluX2V4cHJlc3Npb25fYXJjaGl2ZV9kYXRhYmFzZQ%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjIxMjkyNTQ0X1BhaW5mdWxfZGF0YV9UaGVfVU5CQy1NY01hc3Rlcl9zaG91bGRlcl9wYWluX2V4cHJlc3Npb25fYXJjaGl2ZV9kYXRhYmFzZQ%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw57_56ab1977ab01e"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 549;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Patrick Lucey","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Patrick_Lucey","institution":"Disney Research","institutionUrl":false,"widgetId":"rgw4_56ab1977ab01e"},"id":"rgw4_56ab1977ab01e","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=3694529","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab1977ab01e"},"id":"rgw3_56ab1977ab01e","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=221292544","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":221292544,"title":"Painful data: The UNBC-McMaster shoulder pain expression archive database","journalTitle":false,"journalDetailsTooltip":false,"affiliation":"Dept. of Psychol., Univ. of Pittsburgh, Pittsburgh, PA, USA","type":"Conference Paper","details":{"doi":"10.1109\/FG.2011.5771462","conferenceInfos":"Conference: Ninth IEEE International Conference on Automatic Face and Gesture Recognition (FG 2011), Santa Barbara, CA, USA, 21-25 March 2011"},"source":{"sourceUrl":"http:\/\/dblp.uni-trier.de\/db\/conf\/fgr\/fg2011.html#LuceyCPSM11","sourceName":"DBLP"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1109\/FG.2011.5771462"},{"key":"rft.atitle","value":"Painful data: The UNBC-McMaster shoulder pain expression archive database"},{"key":"rft.title","value":"2011 IEEE International Conference on Automatic Face and Gesture Recognition and Workshops, FG 2011"},{"key":"rft.jtitle","value":"2011 IEEE International Conference on Automatic Face and Gesture Recognition and Workshops, FG 2011"},{"key":"rft.date","value":"2011"},{"key":"rft.pages","value":"57-64"},{"key":"rft.au","value":"Patrick Lucey,Jeffrey F. Cohn,Kenneth M. Prkachin,Patricia E. Solomon,Iain Matthews"},{"key":"rft.genre","value":"inProceedings"}],"widgetId":"rgw6_56ab1977ab01e"},"id":"rgw6_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=221292544","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":221292544,"peopleItems":[{"data":{"authorNameOnPublication":"Patrick Lucey","accountUrl":"profile\/Patrick_Lucey","accountKey":"Patrick_Lucey","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Patrick Lucey","profile":{"professionalInstitution":{"professionalInstitutionName":"Disney Research","professionalInstitutionUrl":"institution\/Disney_Research"}},"professionalInstitutionName":"Disney Research","professionalInstitutionUrl":"institution\/Disney_Research","url":"profile\/Patrick_Lucey","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Patrick_Lucey","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw9_56ab1977ab01e"},"id":"rgw9_56ab1977ab01e","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=3694529&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Disney Research","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":5,"accountCount":5,"publicationUid":221292544,"widgetId":"rgw8_56ab1977ab01e"},"id":"rgw8_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=3694529&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=5&accountCount=5&publicationUid=221292544","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Jeffrey F Cohn","accountUrl":"profile\/Jeffrey_Cohn","accountKey":"Jeffrey_Cohn","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272368778084361%401441949186005_m\/Jeffrey_Cohn.png","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Jeffrey F Cohn","profile":{"professionalInstitution":{"professionalInstitutionName":"University of Pittsburgh","professionalInstitutionUrl":"institution\/University_of_Pittsburgh"}},"professionalInstitutionName":"University of Pittsburgh","professionalInstitutionUrl":"institution\/University_of_Pittsburgh","url":"profile\/Jeffrey_Cohn","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272368778084361%401441949186005_l\/Jeffrey_Cohn.png","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Jeffrey_Cohn","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw11_56ab1977ab01e"},"id":"rgw11_56ab1977ab01e","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1754447&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University of Pittsburgh","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":5,"accountCount":5,"publicationUid":221292544,"widgetId":"rgw10_56ab1977ab01e"},"id":"rgw10_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1754447&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=5&accountCount=5&publicationUid=221292544","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Kenneth M Prkachin","accountUrl":"profile\/Kenneth_Prkachin","accountKey":"Kenneth_Prkachin","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A283801621155842%401444674987468_m\/Kenneth_Prkachin.png","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Kenneth M Prkachin","profile":{"professionalInstitution":{"professionalInstitutionName":"University of Northern British Columbia","professionalInstitutionUrl":"institution\/University_of_Northern_British_Columbia"}},"professionalInstitutionName":"University of Northern British Columbia","professionalInstitutionUrl":"institution\/University_of_Northern_British_Columbia","url":"profile\/Kenneth_Prkachin","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A283801621155842%401444674987468_l\/Kenneth_Prkachin.png","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Kenneth_Prkachin","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw13_56ab1977ab01e"},"id":"rgw13_56ab1977ab01e","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=3368081&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University of Northern British Columbia","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":5,"accountCount":5,"publicationUid":221292544,"widgetId":"rgw12_56ab1977ab01e"},"id":"rgw12_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=3368081&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=5&accountCount=5&publicationUid=221292544","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Patricia Solomon","accountUrl":"profile\/Patricia_Solomon2","accountKey":"Patricia_Solomon2","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Patricia Solomon","profile":{"professionalInstitution":{"professionalInstitutionName":"McMaster University","professionalInstitutionUrl":"institution\/McMaster_University"}},"professionalInstitutionName":"McMaster University","professionalInstitutionUrl":"institution\/McMaster_University","url":"profile\/Patricia_Solomon2","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Patricia_Solomon2","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw15_56ab1977ab01e"},"id":"rgw15_56ab1977ab01e","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1765706&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"McMaster University","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":5,"accountCount":5,"publicationUid":221292544,"widgetId":"rgw14_56ab1977ab01e"},"id":"rgw14_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1765706&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=5&accountCount=5&publicationUid=221292544","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Iain Matthews","accountUrl":"profile\/Iain_Matthews2","accountKey":"Iain_Matthews2","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277565160869889%401443188099330_m\/Iain_Matthews2.png","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Iain Matthews","profile":{"professionalInstitution":{"professionalInstitutionName":"Disney Research","professionalInstitutionUrl":"institution\/Disney_Research"}},"professionalInstitutionName":"Disney Research","professionalInstitutionUrl":"institution\/Disney_Research","url":"profile\/Iain_Matthews2","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277565160869889%401443188099330_l\/Iain_Matthews2.png","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Iain_Matthews2","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw17_56ab1977ab01e"},"id":"rgw17_56ab1977ab01e","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=4076838&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Disney Research","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":5,"accountCount":5,"publicationUid":221292544,"widgetId":"rgw16_56ab1977ab01e"},"id":"rgw16_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=4076838&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=5&accountCount=5&publicationUid=221292544","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab1977ab01e"},"id":"rgw7_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=221292544&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":221292544,"abstract":"<noscript><\/noscript><div>A major factor hindering the deployment of a fully functional automatic facial expression detection system is the lack of representative data. A solution to this is to narrow the context of the target application, so enough data is available to build robust models so high performance can be gained. Automatic pain detection from a patient's face represents one such application. To facilitate this work, researchers at McMaster University and University of Northern British Columbia captured video of participant's faces (who were suffering from shoulder pain) while they were performing a series of active and passive range-of-motion tests to their affected and unaffected limbs on two separate occasions. Each frame of this data was AU coded by certified FACS coders, and self-report and observer measures at the sequence level were taken as well. This database is called the UNBC-McMaster Shoulder Pain Expression Archive Database. To promote and facilitate research into pain and augment current datasets, we have publicly made available a portion of this database which includes: (1) 200 video sequences containing spontaneous facial expressions, (2) 48,398 FACS coded frames, (3) associated pain frame-by-frame scores and sequence-level self-report and observer measures, and (4) 66-point AAM landmarks. This paper documents this data distribution in addition to describing baseline results of our AAM\/SVM system. This data will be available for distribution in March 2011.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw18_56ab1977ab01e"},"id":"rgw18_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=221292544","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database\/links\/00b495220a93e5795f000000\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw19_56ab1977ab01e"},"id":"rgw19_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab1977ab01e"},"id":"rgw5_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=221292544&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":35740604,"url":"researcher\/35740604_Patrick_Lucey","fullname":"Patrick Lucey","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9304721,"url":"researcher\/9304721_Jeffrey_F_Cohn","fullname":"Jeffrey F. Cohn","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38482384,"url":"researcher\/38482384_Kenneth_M_Prkachin","fullname":"Kenneth M. Prkachin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":13688935,"url":"researcher\/13688935_Patricia_E_Solomon","fullname":"Patricia E. Solomon","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":2,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Mar 2012","journal":"Image and Vision Computing","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/257093317_Painful_monitoring_Automatic_pain_monitoring_using_the_UNBC-McMaster_shoulder_pain_expression_archive_database","usePlainButton":true,"publicationUid":257093317,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.59","url":"publication\/257093317_Painful_monitoring_Automatic_pain_monitoring_using_the_UNBC-McMaster_shoulder_pain_expression_archive_database","title":"Painful monitoring: Automatic pain monitoring using the UNBC-McMaster shoulder pain expression archive database","displayTitleAsLink":true,"authors":[{"id":35740604,"url":"researcher\/35740604_Patrick_Lucey","fullname":"Patrick Lucey","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9304721,"url":"researcher\/9304721_Jeffrey_F_Cohn","fullname":"Jeffrey F. Cohn","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38482384,"url":"researcher\/38482384_Kenneth_M_Prkachin","fullname":"Kenneth M. Prkachin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":13688935,"url":"researcher\/13688935_Patricia_E_Solomon","fullname":"Patricia E. Solomon","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":71055482,"url":"researcher\/71055482_Sien_Chew","fullname":"Sien Chew","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38465491,"url":"researcher\/38465491_Iain_Matthews","fullname":"Iain Matthews","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Image and Vision Computing 03\/2012; 30(3):197\u2013205. DOI:10.1016\/j.imavis.2011.12.003"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/257093317_Painful_monitoring_Automatic_pain_monitoring_using_the_UNBC-McMaster_shoulder_pain_expression_archive_database","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/257093317_Painful_monitoring_Automatic_pain_monitoring_using_the_UNBC-McMaster_shoulder_pain_expression_archive_database\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw21_56ab1977ab01e"},"id":"rgw21_56ab1977ab01e","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=257093317","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":73736103,"url":"researcher\/73736103_C_Marchese","fullname":"C Marchese","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38876090,"url":"researcher\/38876090_G_Cristalli","fullname":"G Cristalli","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39060131,"url":"researcher\/39060131_B_Pichi","fullname":"B Pichi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":8780514,"url":"researcher\/8780514_V_Manciocco","fullname":"V Manciocco","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":6,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2012","journal":"Acta otorhinolaryngologica Italica: organo ufficiale della Societ\u00e0 italiana di otorinolaringologia e chirurgia cervico-facciale","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/224006540_Italian_cross-cultural_adaptation_and_validation_of_three_different_scales_for_the_evaluation_of_shoulder_pain_and_dysfunction_after_neck_dissection_University_of_California_-_Los_Angeles_UCLA_Shoulde","usePlainButton":true,"publicationUid":224006540,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.64","url":"publication\/224006540_Italian_cross-cultural_adaptation_and_validation_of_three_different_scales_for_the_evaluation_of_shoulder_pain_and_dysfunction_after_neck_dissection_University_of_California_-_Los_Angeles_UCLA_Shoulde","title":"Italian cross-cultural adaptation and validation of three different scales for the evaluation of shoulder pain and dysfunction after neck dissection: University of California - Los Angeles (UCLA) Shoulder Scale, Shoulder Pain and Disability Index (SPADI) and Simple Shoulder Test (SST)","displayTitleAsLink":true,"authors":[{"id":73736103,"url":"researcher\/73736103_C_Marchese","fullname":"C Marchese","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38876090,"url":"researcher\/38876090_G_Cristalli","fullname":"G Cristalli","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39060131,"url":"researcher\/39060131_B_Pichi","fullname":"B Pichi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8780514,"url":"researcher\/8780514_V_Manciocco","fullname":"V Manciocco","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":58600042,"url":"researcher\/58600042_G_Mercante","fullname":"G Mercante","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39913246,"url":"researcher\/39913246_R_Pellini","fullname":"R Pellini","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39060543,"url":"researcher\/39060543_P_Marchesi","fullname":"P Marchesi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":45533784,"url":"researcher\/45533784_I_Sperduti","fullname":"I Sperduti","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38238104,"url":"researcher\/38238104_P_Ruscito","fullname":"P Ruscito","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39834216,"url":"researcher\/39834216_G_Spriano","fullname":"G Spriano","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Acta otorhinolaryngologica Italica: organo ufficiale della Societ\u00e0 italiana di otorinolaringologia e chirurgia cervico-facciale 02\/2012; 32(1):12-7."],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/224006540_Italian_cross-cultural_adaptation_and_validation_of_three_different_scales_for_the_evaluation_of_shoulder_pain_and_dysfunction_after_neck_dissection_University_of_California_-_Los_Angeles_UCLA_Shoulde","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/224006540_Italian_cross-cultural_adaptation_and_validation_of_three_different_scales_for_the_evaluation_of_shoulder_pain_and_dysfunction_after_neck_dissection_University_of_California_-_Los_Angeles_UCLA_Shoulde\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw22_56ab1977ab01e"},"id":"rgw22_56ab1977ab01e","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=224006540","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":39806846,"url":"researcher\/39806846_Felix_Angst","fullname":"Felix Angst","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":6828475,"url":"researcher\/6828475_Hans-Kaspar_Schwyzer","fullname":"Hans-Kaspar Schwyzer","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39604159,"url":"researcher\/39604159_Andre_Aeschlimann","fullname":"Andr\u00e9 Aeschlimann","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":38325822,"url":"researcher\/38325822_Beat_R_Simmen","fullname":"Beat R Simmen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Nov 2011","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/230613508_Measures_of_adult_shoulder_function_Disabilities_of_the_Arm_Shoulder_and_Hand_Questionnaire_DASH_and_its_short_version_QuickDASH_Shoulder_Pain_and_Disability_Index_SPADI_American_Shoulder_and_Elbow_Su","usePlainButton":true,"publicationUid":230613508,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/230613508_Measures_of_adult_shoulder_function_Disabilities_of_the_Arm_Shoulder_and_Hand_Questionnaire_DASH_and_its_short_version_QuickDASH_Shoulder_Pain_and_Disability_Index_SPADI_American_Shoulder_and_Elbow_Su","title":"Measures of adult shoulder function: Disabilities of the Arm, Shoulder, and Hand Questionnaire (DASH) and its short version (QuickDASH), Shoulder Pain and Disability Index (SPADI), American Shoulder and Elbow Surgeons (ASES) Society standardized shoulder assessment form, Constant (Murley) Score (CS), Simple Shoulder Test (SST), Oxford Shoulder Score (OSS), Shoulder Disability Questionnaire (SDQ), and Western Ontario Shoulder Instability Index (WOSI)","displayTitleAsLink":true,"authors":[{"id":39806846,"url":"researcher\/39806846_Felix_Angst","fullname":"Felix Angst","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":6828475,"url":"researcher\/6828475_Hans-Kaspar_Schwyzer","fullname":"Hans-Kaspar Schwyzer","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39604159,"url":"researcher\/39604159_Andre_Aeschlimann","fullname":"Andr\u00e9 Aeschlimann","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38325822,"url":"researcher\/38325822_Beat_R_Simmen","fullname":"Beat R Simmen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":14927642,"url":"researcher\/14927642_Joerg_Goldhahn","fullname":"J\u00f6rg Goldhahn","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["11\/2011; 63 Suppl 11(S11):S174-88. DOI:10.1002\/acr.20630"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/230613508_Measures_of_adult_shoulder_function_Disabilities_of_the_Arm_Shoulder_and_Hand_Questionnaire_DASH_and_its_short_version_QuickDASH_Shoulder_Pain_and_Disability_Index_SPADI_American_Shoulder_and_Elbow_Su","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/230613508_Measures_of_adult_shoulder_function_Disabilities_of_the_Arm_Shoulder_and_Hand_Questionnaire_DASH_and_its_short_version_QuickDASH_Shoulder_Pain_and_Disability_Index_SPADI_American_Shoulder_and_Elbow_Su\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw23_56ab1977ab01e"},"id":"rgw23_56ab1977ab01e","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=230613508","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw20_56ab1977ab01e"},"id":"rgw20_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=221292544&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":221292544,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":221292544,"publicationType":"inProceedings","linkId":"00b495220a93e5795f000000","fileName":"2011_PAINFUL.pdf","fileUrl":"profile\/Kenneth_Prkachin\/publication\/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database\/links\/00b495220a93e5795f000000.pdf","name":"Kenneth M Prkachin","nameUrl":"profile\/Kenneth_Prkachin","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Jan 21, 2016","fileSize":"3.56 MB","widgetId":"rgw26_56ab1977ab01e"},"id":"rgw26_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=221292544&linkId=00b495220a93e5795f000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":221292544,"publicationType":"inProceedings","linkId":"0fa8dbbe0cf2bd28793e38ca","fileName":"Painful data: The UNBC-McMaster shoulder pain expression archive database","fileUrl":"http:\/\/www.iainm.com\/iainm\/Publications_files\/2011_PAINFUL.pdf","name":"iainm.com","nameUrl":"http:\/\/www.iainm.com\/iainm\/Publications_files\/2011_PAINFUL.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw27_56ab1977ab01e"},"id":"rgw27_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=221292544&linkId=0fa8dbbe0cf2bd28793e38ca&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw25_56ab1977ab01e"},"id":"rgw25_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=221292544&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":66,"valueFormatted":"66","widgetId":"rgw28_56ab1977ab01e"},"id":"rgw28_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=221292544","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw24_56ab1977ab01e"},"id":"rgw24_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=221292544&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":221292544,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw30_56ab1977ab01e"},"id":"rgw30_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=221292544&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":66,"valueFormatted":"66","widgetId":"rgw31_56ab1977ab01e"},"id":"rgw31_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=221292544","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw29_56ab1977ab01e"},"id":"rgw29_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=221292544&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"PAINFUL DATA: The UNBC-McMaster Shoulder Pain Expression\nArchive Database\nPatrick Lucey, Jeffrey F. Cohn, Kenneth M. Prkachin, Patricia E. Solomon and Iain Matthews\nAbstract\u2014A major factor hindering the deployment of a\nfully functional automatic facial expression detection system\nis the lack of representative data. A solution to this is to\nnarrow the context of the target application, so enough data\nis available to build robust models so high performance can\nbe gained. Automatic pain detection from a patient\u2019s face\nrepresents one such application. To facilitate this work, re-\nsearchers at McMaster University and University of Northern\nBritish Columbia captured video of participant\u2019s faces (who\nwere suffering from shoulder pain) while they were performing\na series of active and passive range-of-motion tests to their\naffected and unaffected limbs on two separate occasions. Each\nframe of this data was AU coded by certified FACS coders, and\nself-report and observer measures at the sequence level were\ntaken as well. This database is called the UNBC-McMaster\nShoulder Pain Expression Archive Database. To promote and\nfacilitate research into pain and augment current datasets,\nwe have publicly made available a portion of this database\nwhich includes: 1) 200 video sequences containing spontaneous\nfacial expressions, 2) 48,398 FACS coded frames, 3) associated\npain frame-by-frame scores and sequence-level self-report and\nobserver measures, and 4) 66-point AAM landmarks. This\npaper documents this data distribution in addition to describing\nbaseline results of our AAM\/SVM system. This data will be\navailable for distribution in March 2011.\nI. INTRODUCTION\nAutomated facial expression detection has made great\nstrides over the past decade [1], [2], [3]. From initial focus\non posed images recorded from a frontal view, current efforts\nextend to posed data recorded from multiple views [4], [5],\n3D imaging [6], and, increasingly, non-posed facial behavior\nin real-world settings in which non-frontal pose and head\nmotion are common [7], [8]. With respect to the latter, lack\nof well-annotated, ecologically valid, representative data has\nbeen a significant limitation.\nIn many real-world applications, the goal is to recognize\nor infer intention or other psychological states rather than\nfacial actions alone. For this purpose, both narrowing the\nnumber of facial actions of interest and paying attention\nto context may be critical to the success of an automated\nsystem. For example, if one were to apply an automated\nThis project was supported in part by CIHR Operating Grant MOP77799\nand National Institute of Mental Health grant R01 MH51435\nP. Lucey (now with Disney Research Pittsburgh) conducted this work\nwith J.F. Cohn at the Department of Psychology, University of Pitts-\nburgh\/Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, 15260.\nEmail: {patrick.lucey@disneyresearch.com, jeffcohn@cs.cmu.edu}\nK.M. Prkachin is with the Department of Psychology at the Univer-\nsity of Northern British Columbia; P.E. Solomon is with the School of\nRehabilitation Science, McMaster University, Hamilton, Ontario, Canada;\nand I. Matthews is with Disney Research Pittsburgh\/Robotics Institute,\nCarnegie Mellon University, Pittsburgh, PA. Email: {kmprk@unbc.ca,\nsolomon@mcmaster.ca, iainm@disneyresearch.com }\nfacial analysis system to detect hostile intent in airports, a\nvery large number of facial actions could occur and the shear\nnumber of permutations that could explain a person\u2019s facial\nexpression would be far too great to have any confidence in\ndetecting intention. The range of facial action variability in\nthis setting is considerable. Hundreds of facial action units\nin combinations could occur and their signal value highly\ndiverse. A target person could be running late, afraid of\nflying, upset at leaving a loved one, or agitated over visa\nproblems or missed connections, among other states. Facial\nexpression analysis would have to be combined with other\nmodalities, including speech, in an iterative process to have\nconfidence in intention detection.\nDetection of psychologically meaningful states from facial\nbehavior alone can be improved by knowing the context (e.g.,\nclinical interview or assessment) and number of outcomes.\n(say two, i.e. yes\/no). With these constraints, the application\nof an automatic facial expression detection system could\nbe very successful. A recent example is that of automatic\nsmile detection in digital cameras where Whitehill et al.\n[8] constrained the goal to detecting only smile or no-\nsmile. Employing a Gabor filter approach, they were able to\nachieve performance of up to 98% on a challenging dataset\nconsisting of frontal faces spontaneously smiling or not in\nvarious environments, although no inferences were made\nabout psychological state (e.g., enjoyment) and no temporal\nsegmenting was required. Other examples of well-specified\nproblems or contexts in which facial expression detection\nwould be useful include driver fatigue detection, clinical\nstatus (e.g., symptomatic or not) and approach\/avoidance in\nconsumers (e.g., interested, disgusted or neutral).\nA. Painful Motivation\nAn application that would be of great benefit is that\nof pain\/no-pain detection [9], [10], [11], [12]. In Atul\nGawande\u2019s recent book entitled \u201dThe Checklist Manifesto\u201d\n[13], he notes that massive improvements in patient outcomes\nin intensive care unit (ICU) settings have been achieved\nthrough adhering to standardized hygiene and monitoring\nper a priori checklists. One of these is pain monitoring,\nin which a nurse checks on a patient every 4 hours or\nso to evaluate whether they are suffering from pain and\nto make any needed adjustments in pain medication that\nmay be warranted. Pain monitoring especially beyond the\nICU has been hard to implement due to competing demands\non nursing staff. Automatic monitoring could be an ideal\nsolution."},{"page":2,"text":"(a)\n(b)\n(c)\n(d)\nMonday, September 20, 2010\nFig. 1.\nIntensity (OPI) = 5, Visual Analog Scale (VAS) = 9, Sensory Scale (SEN) = 11, Affective-Motivational Scale (AFF) = 10, the peak-frame (60) had AU\ncodes of 6c + 9b +43 which was equal to a Prkachin and Solomon Pain Intensity (PSPI) rating of 6 for that frame; (b) the ratings were OPI = 4, VAS =\n6,SEN = 10,AFF = 7, the peak-frame (322) had AU codes of 4a + 6d + 7d+ 12d + 43 which was equal to a PSPI rating of 6 for that frame; (c) the ratings\nwere OPI=3, VAS=7,SEN=7,AFF=7, the peak-frame (352) had AU codes of 4e + 6a + 7e + 9d + 10d + 25c + 43 which was equal to a PSPI rating of 14\nfor that frame; (d) the ratings were OPI = 2, VAS = 6, SEN = 8, AFF = 5, the peak-frame (129) had AU codes of 4b + 6c + 12c + 43 which was equal\nto a PSPI rating of 6 for that frame.\nExamples of some of the sequences from the UNBC-McMaster Pain Shoulder Archive: (a) the sequence-level ratings were Observer Rated Pain\nOutside of the ICU, most pain assessment is via self-\nreport. Self-reported pain is convenient and requires no\nspecial skills or staffing, but has several limitations. Self-\nreport is subjective, lacks specific timing information about\nwhether pain is increasing, decreasing, or spiking, and cannot\nbe used when patients are impaired. Breathing tubes interfere\nwith speech, consciousness may be transient, and patients\nmay yet to have achieved functional speech (e.g., infants).\nOver the past twenty years, significant efforts have been\nmade in identifying such facial actions\nRecently, Prkachin and Solomon\nAction Coding System (FACS) [17] based measure of pain\nthat can be applied on a frame-by-frame basis. A caveat on\nthis approach is that it must be performed offline, where\nmanual observations are both timely and costly, which makes\nclinical use prohibitive. However, such information can be\nused to train a real-time automatic system which could\npotentially provide significant advantage in patient care and\ncost reduction.\nResearchers at the McMaster University and University\nof Northern British Columbia (UNBC) captured video of\npatient\u2019s faces (who were suffering from shoulder pain)\nwhile they were performing a series of active and passive\nrange-of-motion tests to their affected and unaffected limbs\non two separate occasions. Each video frame was fully\nAU coded by certified FACS coders, and both observer\nand self-report measures at the sequence level were taken\nas well. To promote and facilitate research into pain as\nwell as facial expression detection, the first portion of this\ndataset is now available for computer vision and pattern\nrecognition researchers. With their particular needs in mind\n[14], [15], [16].\n[16] validated a Facial\nand through collaboration with CMU and University of\nPittsburgh, the UNBC-McMaster Shoulder Pain Expression\nArchive includes:\n1) Temporal Spontaneous Expressions: 200 video se-\nquences containing spontaneous facial expressions re-\nlating to genuine pain,\n2) Manual FACS codes: 48,398 FACS coded frames,\n3) Self-Report and Observer Ratings: associated pain\nself-report and observer ratings at the sequence level,\n4) Tracked Landmarks: 66 point AAM landmarks.\nThis paper documents this database and describes baseline\nresults and protocol based on our AAM\/SVM system.\nII. THE UNBC-MCMASTER SHOULDER PAIN\nEXPRESSION ARCHIVE DATABASE\nA total of 129 participants (63 male, 66 female) who were\nself-identified as having a problem with shoulder pain were\nrecruited from 3 physiotherapy clinics and by advertisements\nposted on the campus of the McMaster University. One\nfourth were students and others were from the community\nand included a wide variety of occupations. Diagnosis of\nthe shoulder pain varied, with participants suffering from\narthritis, bursitis, tendonitis, subluxation, rotator cuff injuries,\nimpingement syndromes, bone spur, capsulitis and disloca-\ntion. Over half of the participants reported use of medication\nfor their pain.\nAll participants were tested in a laboratory room that\nincluded a bed for performing passive range-of-motion tests.\nAfter informed consent and information procedures were\ncompleted, participants underwent eight standard range-of-\nmotion tests: abduction, flexion, and internal and external"},{"page":3,"text":"Number of Images\nPitch (deg)Yaw (deg)Roll (deg)\n?40?30?20?100 1020 30 40\n0\n500\n1000\n1500\n?40?30?20?100 102030 40\n0\n500\n1000\n1500\n?40?30?20?100 1020 3040\n0\n500\n1000\n1500\nFriday, August 27, 2010\nrotation is the same except that the arm is turned externally.\nAbduction, flexion, and internal and external rotations were\nperformed under active and passive conditions. Active tests\ndiffered from the passive tests in being under the control\nof the patient who was instructed to move the limb as far\nas possible. Active tests were performed with the patient\nin a standing position. Passive tests were performed by a\nphysiotherapist who moved the limb until the maximum\nrange was achieved or was asked to stop by the patient.\nDuring passive tests, the participant was resting in a supine\nposition on the bed with his or her head supported and\nstabilized by a pillow. Active tests were performed prior to\npassive tests because that is the usual sequence in which they\nare conducted clinically. The order of tests within active and\npassive conditions was randomized. Tests were performed\non both the affected and the unaffected limb to provide a\nwithin-subject control.\nDuring both active and passive tests, two Sony digital\ncameras recorded participants\u2019 facial expressions. Camera\norientation was initially frontal in the active condition, al-\nthough change in pose was common. Camera orientation\nin the passive condition was approximately 70 degrees off\nfrontal, with the face viewed from below.\nA card, listing verbal pain descriptors was available to help\nparticipants provide verbal ratings of the pain produced on\neach test. Each card displayed two Likert-type scales [19].\nOne consisted of words reflecting the sensory intensity of\npain. The other consisted of words reflecting the affective-\nmotivational dimension. These scales have been subject to\nextensive psychophysical analyses, which have established\ntheir properties as ratio-scale measures of the respective un-\nderlying dimensions. Each scale had 15 items labelled from\n\u201cA\u201d to \u201cO\u201d. The sensory scale (SEN) started at \u201cextremely\nweak\u201d and finished at \u201cextremely intense\u201d; the affective-\nmotivational scale (AFF) started at \u201cbearable\u201d and finished\nat \u201cexcruciating\u201d. In addition, participants completed a series\nof 10cm Visual Analog Scales (VAS), anchored at each end\nwith the words, \u201cNo pain\u201d and \u201cPain as bad as could be\u201d. The\nFig. 2. Histograms of the pitch, yaw and roll taken from the 3D AAM parameters across the UNBC-McMaster Shoulder Pain Expression Archive database.\nrotation of each arm separately [18]. Abduction movements\ninvolve lifting the arm forward and up in the sagittal plane.\nIn internal rotation, the arm is bent 90 degrees at the\nelbow, abducted 90 degrees, and turned internally. External\nthree scales were completed by participants after each test.\nSpecifically, after each test, participants rated the maximum\npain it had produced using the sensory and affective verbal\npain descriptors and the VAS.\nOffline, independent observers rated pain intensity (OPI)\nfrom the recorded video. Observers had considerable training\nin the identification of pain expression. Observer ratings\nwere performed on a 6-point Likert-type scale that ranged\nfrom 0 (no pain) to 5 (strong pain). To assess inter-observer\nreliability of the OPI pain ratings, 210 randomly selected\ntrials were independently rated by a second rater. The\nPearson correlation between the observers OPI was 0.80,\n(p < 0.001), which represents high inter-observer reliability\n[20]. Correlation between the observers rating on the OPI and\nsubjects self-reported pain on the VAS was 0.74, (p < 0.001)\nfor the trials used in the current study. A value of 0.70 is\nconsidered a large effect [21] and is commonly taken as\nindicating high concurrent validity. Thus, the inter-method\ncorrelation found here suggests moderate to high concurrent\nvalidity for pain intensity. Examples of the active portion of\nthe dataset with the associated self-report measures, along\nwith the FACS codes and frame-by-frame level pain rating\n(see next subsection) is given in Figure 1.\nA. FACS coding\nEach test was extracted from the video and coded using\nFACS [17]. Each facial action is described in terms of one\nof 44 individual action units (AUs). Because there is a\nconsiderable amount of literature in which FACS has been\napplied to pain expression, only the actions that have been\nimplicated as possibly related to pain were focussed on:\nbrow-lowering (AU4), cheek-raising (AU6), eyelid tightening\n(AU7), nose wrinkling (AU9), upper-lip raising (AU10),\noblique lip raising (AU12), horizontal lip stretch (AU20),\nlips parting (AU25), jaw dropping (AU26), mouth stretching\n(AU27) and eye-closure (AU43). With the exception of AU\n43, each action was coded on a 5 level intensity dimension\n(A-E) by one of three coders who were certified FACS\ncoders. Actions were coded on a frame-by-frame basis. All\ncoding was then reviewed by a fourth certified FACS coder.\nTo assess inter-observer agreement, 1738 frames selected\nfrom one affected-side trial and one unaffected-side trial of"},{"page":4,"text":"20 participants were randomly sampled and independently\ncoded. Intercoder percent agreement as calculated by the\nEkman-Friesen formula [17] was 95%, which compares\nfavorably with other research in the FACS literature.\nB. Prkachin and Solomon Pain Intensity Scale\nBeginning in 1992, Prkachin [15] found that four actions\n- brow lowering (AU4), orbital tightening (AU6 and AU7),\nlevator contraction (AU9 and AU10) and eye closure (AU43)\n- carried the bulk of information about pain. In a recent\nfollow up to this work, Prkachin and Solomon [16] confirmed\nthese four \u201ccore\u201d actions contained the majority of pain\ninformation. They defined pain as the sum of intensities of\nbrow lowering, orbital tightening, levator contraction and eye\nclosure. The Prkachin and Solomon pain intensity (PSPI)\nmetric is defined as:\nPain= AU4 + (AU6orAU7) +\n(AU9orAU10) + AU43\n(1)\nThat is, the sum of AU4, AU6 or AU7 (whichever is higher in\nintensity), AU9 or AU10 (whichever is higher in intensity)\nand AU43 to yield a 16-point scale1. For the example in\nFigure 1(a), the peak frame here (60) has been coded as AU\n6c + 9b + 43. This would result in a PSPI of 3+2+1 = 6.\nSimilarly in Figure 1(b), the peak frame has been coded as\nAU 4a + 6d + 7d+ 12d + 43, which equals 1 + 4 + 1 =\n6, as AU4 has an intensity of 1, AU6 and AU7 both have\nintensity of 4 so just the maximum 4 is taken and AU43 has\nan intensity of 1 (eyes are shut).\nThe PSPI [16] FACS pain scale is currently the only\nmetric which can define pain on a frame-by-frame basis. All\nframes in this dataset were coded using the PSPI. For more\ninformation on the relative merits of the particular self-report\nmeasures and how they relate to PSPI and FACS, please refer\nto [16].\nC. Analysis of Distributed Portion of the Pain Corpora\nFrom the entire available UNBC-McMaster Pain Shoulder\nArchive, 200 sequences from 25 different subjects in the ac-\ntive portion of the dataset has been prepared for distribution\nto the research community. From these 200 sequences there\nis a total of 48398 frames that have been FACS coded and\nAAM tracked. The inventory of the total number of frames\nwhich have been coded from each AU and their intensity is\ngiven in Table I. The number of frames and the associated\nPSPI score is given in Table II. From this, it can be seen that\n83.6% of the frames had a PSPI score of 0, and 16.4% had\nframes in which had a PSPI of score \u2265 1.\nExamples of this data are given in Figure 1 and it is\napparent that there is some head movement that occurs dur-\ning these sequences. To quantify how much head movement\n1The intensity of action units (AUs) are scored on a 6-point intensity scale\nthat ranges from 0 (absent) to 5 (maximum intensity). Eye closing (AU43)\nbinary (0 = absent, 1 = present). In FACS terminology, ordinal intensity is\ndenoted by letters rather than numeric weights, i.e., 1 = A, 2 = B, . . . 5 =\nE.\nTABLE I\nThe AU inventory on the UNBC-McMaster Shoulder Pain Archive, where\nthe frequency of each AU and its intensity is given along with the total.\nNote that for AU43, the only intensity is A (i.e. they eye can only be open\nor shut).\nAU\n4\n6\n7\n9\n10\n12\n20\n25\n26\n43\nABCD\n74\n681\n305\n76\n61\n736\n0\n138\n478\n\u2013\nE\n64\n110\n100\n35\n22\n49\n20\n88\n1\n\u2013\nTotal\n1074\n5557\n3366\n423\n525\n6887\n706\n2407\n2093\n2434\n202\n1776\n1362\n93\n171\n2145\n286\n767\n431\n2434\n509\n1663\n991\n151\n208\n1799\n282\n803\n918\n\u2013\n225\n1327\n608\n68\n63\n2158\n118\n611\n265\n\u2013\nTABLE II\nThe inventory on the UNBC-McMaster Shoulder Pain Archive according\nto the Prkachin-Solomon Pain Intensity (PSPI) pain metric, where the\nfrequency of each pain intensity is given.\nPSPI Score\n0\n1-2\n3-4\n5-6\n7-8\n9-10\n11-12\n13-14\n15-16\nFrequency\n40029\n5260\n2214\n512\n132\n99\n124\n23\n5\noccurred, we used the 3D parameters from the AAM to\nestimate the pitch, yaw and roll [22]. The histograms of these\nparameters are shown in Figure 2. In terms of pitch, yaw\nand roll the mean was -0.38, -0.21 and -0.23 degrees and\nthe variance was 23.58, 40.82 and 33.28. However, these\nparameters differed quite a bit when a person was in no-\npain (PSPI=0) and in pain (PSPI\u22651). When the PSPI was\nequal to 0 the variance in terms of pitch, yaw and roll was\n22.69, 37.03 and 29.19. When the PSPI was \u2265 1, the variance\nincreased to 26.72, 55.61 and 48.52 which suggested that\nhead movement coincided with painful facial expression.\nOverall, close to 90% of all frames in this distribution were\nwithin 10 degrees of being fully frontal and over 99% were\nwithin 20 degrees from the fully frontal view.\nAt the sequence level we show the inventory of some\nself-report and observer measures. Table III shows the in-\nventory of the visual analogue scale (VAS) and observer\npain intensity (OPI) measures for the 200 sequences. On\nthe left side of the table, it can be said that there is a nice\nspread of VAS measures from 0-10. With the OPI measures,\nthere is slightly less than half with no observable pain. For\nthe sequence-level experiments, we will be using the OPI"},{"page":5,"text":"TABLE III\nThe inventory on the self-report and observer measures of the\nUNBC-McMaster Shoulder Pain Archive at the sequence level. The\nself-report Visual Analogue Scale (VAS), ranging from 0 (no-pain) to 10\n(extreme pain) and the Observed Pain Intensity (OPI), ranging from 0\n(no-pain observed) to 5 (extreme pain observed).\nVAS\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nTotal\nFrequency\n35\n42\n24\n20\n21\n11\n11\n6\n18\n10\n2\n200\nOPI\n0\n1\n2\n3\n4\n5\nTotal\nFrequency\n92\n25\n26\n34\n16\n7\n200\nratings so as we can have our automatic system to mimic that\nof a human observer. The affective and sensory self-report\nmeasures across the 200 sequences will also be available in\nthe distribution.\nIII. AAM LANDMARKS\nIn our system, we employ an Active Appearance Model\n(AAM) based system which uses AAMs to track the face and\nextract visual features. In the data distribution we include the\n66 point AAM landmark points for each image. This section\ndescribes how these landmarks were generated.\nA. Active Appearance Models (AAMs)\nActive Appearance Models (AAMs) have been shown to\nbe a good method of aligning a pre-defined linear shape\nmodel that also has linear appearance variation, to a previ-\nously unseen source image containing the object of interest.\nIn general, AAMs fit their shape and appearance components\nthrough a gradient-descent search, although other optimiza-\ntion methods have been employed with similar results [23].\nThe shape s of an AAM [23] is described by a 2D\ntriangulated mesh. In particular, the coordinates of the mesh\nvertices define the shape s = [x1,y1,x2,y2,...,xn,yn],\nwhere n is the number of vertices. These vertex locations\ncorrespond to a source appearance image, from which the\nshape was aligned. Since AAMs allow linear shape variation,\nthe shape s can be expressed as a base shape s0plus a linear\ncombination of m shape vectors si:\ns = s0+\nm\n?\ni=1\npisi\n(2)\nwhere the coefficients p = (p1,...,pm)Tare the shape\nparameters. These shape parameters can typically be divided\ninto rigid similarity parameters ps and non-rigid object\ndeformation parameters po, such that pT= [pT\nilarity parameters are associated with a geometric similarity\ns,pT\no]. Sim-\nShape RMS error\nProportion of Images\n0510 1520\n0\n0.2\n0.4\n0.6\n0.8\n1\n   0                       5                       10                      15                     20\n1\n0.8\n0.6\n0.4\n0.2\n0\nMonday, September 20, 2010\nFig. 3.\nlandmark points. The Shape RMS error refers to the total shape RMS error\nin terms of pixels after all meshes were similarity normalized with an inter-\noccular distance of 50 pixels.\nFitting curve for the AAM compared against the manual 68 point\ntransform (i.e. translation, rotation and scale). The object-\nspecific parameters, are the residual parameters representing\nnon-rigid geometric variations associated with the determing\nobject shape (e.g., mouth opening, eyes shutting, etc.). Pro-\ncrustes alignment [23] is employed to estimate the base shape\ns0.\nKeyframes within each video sequence were manually\nlabelled, while the remaining frames were automatically\naligned using a gradient descent AAM fitting algorithm\ndescribed in [24]. Figure 4 shows the AAM in action, with\nthe 68 point mesh being fitted to the patient\u2019s face in every\nframe.From the 2D shape model we can derive the 3D\nparameters using non-rigid structure from motion. See [22]\nfor full details.\nB. AAM Accuracy\nIn checking the AAM alignment accuracy to manually\nlandmarked images, we first similarity normalized all tracked\nAAM points and manual landmarks to a common mesh\nsize and rotation, with an inter-occular distance of 50 pixels\nand aligned to the centre of the eye coordinates. We then\ncompared 2584 manually landmarked images against their\nAAM counterpart. The fitting curve for the AAM is shown\nin Figure 3. As can be seen in this curve, nearly all of the\nAAM landmarks are within 2 pixels RMS error of the manual\nlandmarks, which is negligible when one considers that this\nis based on a distance of 50 pixels between the center of\nthe eyes. This highlights the benefit of employing person-\nspecific model such as an AAM, as near perfect alignment\ncan result.\nIV. EXPERIMENTS\nIn this section, we describe two experiments that we\nconducted for i) AU and ii) pain detection at a frame-level.\nWe first describe our baseline AAM\/SVM system."},{"page":6,"text":"\u221260\u221240\u2212200 20 4060\n\u221260\n\u221240\n\u221220\n0\n20\n40\n60\n80\nAAM TrackingAAM Features\ntober 4, 2010\nFig. 4.\nsome feature representations: (top) SPTS - similarity normalized shape and\n(bottom) CAPP - canonical normalized appearance.\nOnce the AAM has tracked a person\u2019s face we can derive\nA. AAM\/SVM Baseline System\nOnce we have tracked the patient\u2019s face by estimating the\nshape and appearance AAM parameters, we can use this\ninformation to derive features from the face. From the initial\nwork conducted in [9], [25], [12], we extracted the following\nfeatures:\n\u2022 SPTS: The similarity normalized shape or points, sn,\nrefers to the 66 vertex points in snfor both the x- and y-\ncoordinates, resulting in a raw 132 dimensional feature\nvector. These points are the vertex locations after all\nthe rigid geometric variation (translation, rotation and\nscale), relative to the base shape, has been removed.\nThe similarity normalized shape sncan be obtained by\nsynthesizing a shape instance of s, using Equation 2,\nthat ignores the similarity parameters p.\n\u2022 CAPP: The canonical normalized appearance, a0\nrefers to where all the non-rigid shape variation has been\nnormalized with respect to the base shape s0. This is\naccomplished by applying a piece-wise affine warp on\neach triangle patch appearance in the source image so\nthat it aligns with the base face shape. For this study,\nthe resulting 87 \u00d7 93 synthesized grayscale image was\nused.\nSupport vector machines (SVMs) were then used to clas-\nsify individual action units as well as pain. SVMs attempt\nto find the hyperplane that maximizes the margin between\npositive and negative observations for a specified class. A\nlinear kernel was used in our experiments due to its ability\nto generalize well to unseen data in many pattern recognition\ntasks [26]. LIBSVM was used for the training and testing of\nSVMs [27].\nIn all experiments conducted, a leave-one-subject-out\nstrategy was used and each AU and pain detector was trained\nusing positive examples which consisted of the frames that\nthe FACS coder labelled containing that particular AU (re-\ngardless of intensity, i.e. A-E) or pain intensity of 1 or more.\nThe negative examples consisted of all the other frames that\nwere not labelled with that particular AU or had a pain\nintensity of 0.\nIn order to predict whether or not a video frame contained\nan AU or pain, the output score from the SVM was used. As\nTABLE IV\nResults showing the area underneath the ROC curve for the\nsimilarity-normalized shape (SPTS) and appearance (SAPP) as well as the\ncanonical appearance (CAPP) features. Note the average is a weighted\none, depending on the number of positive examples.\nAU\n4\n6\n7\n9\n10\n12\n20\n25\n26\n43\nAVG\nSPTS CAPP\n60.0 \u00b1 1.5\n85.1 \u00b1 0.5\n82.6 \u00b1 0.8\n84.1 \u00b1 1.6\n83.2 \u00b1 1.9\n84.6 \u00b1 0.5\n61.7 \u00b1 1.9\n70.9 \u00b1 1.0\n54.7 \u00b1 1.1\n86.7 \u00b1 0.7\n79.2 \u00b1 0.8\nSPTS&CAPP\n57.1 \u00b1 1.5\n85.4 \u00b1 0.5\n80.4 \u00b1 0.7\n85.3 \u00b1 1.7\n89.2 \u00b1 1.4\n85.7 \u00b1 0.4\n77.9 \u00b1 1.6\n78.0 \u00b1 0.8\n71.0 \u00b1 1.0\n87.5 \u00b1 0.7\n81.8 \u00b1 0.8\n72.5 \u00b1 3.1\n80.1 \u00b1 1.7\n71.3 \u00b1 0.8\n75.1 \u00b1 2.4\n87.9 \u00b1 1.7\n79.4 \u00b1 0.5\n75.7 \u00b1 1.7\n78.8 \u00b1 0.9\n73.5 \u00b1 1.1\n83.1 \u00b1 0.6\n78.0 \u00b1 0.8\nthere are many more frames with no behavior of interest than\nframes of interest, the overall agreement between correctly\nclassified frames can skew the results somewhat. As such\nwe used the receiver-operator characteristic (ROC) curve,\nwhich is a more reliable performance measure. This curve is\nobtained by plotting the hit-rate (true positives) against the\nfalse alarm rate (false positives) as the decision threshold\nvaries. From the ROC curves, we used the area under the\nROC curve (A?), to assess the performance. The A?metric\nranges from 50 (pure chance) to 100 (ideal classification)2.\nAn upper-bound on the uncertainty of the A?statistic was\nobtained using the formula s =\nare the number of positive and negative examples [28], [8].\n?\nA?(100\u2212A?)\nmin{np,nn}where np,nn\nB. AU Detection Results\nWe conducted detection for ten AUs (4, 6, 7, 9, 10, 12,\n20, 25, 26 and 43). The results for the AU detection with\nrespect to the similarity-normalized shape (SPTS), the canon-\nical appearance (CAPP) and the combined (SPTS+CAPP)\nfeatures are shown in Table IV. In terms of the overall\naverage accuracy of the AU detection, the performance is\nrather good with combined representation gaining the best\noverall performance of 81.8, slightly better than CAPP (79.2)\nand SPTS (78.0).\nIn terms of individual AU detection, it can be seen that\nbest performance is gained for the strong expressions such\nas AU6, 10, 12 and 43. Due to the amount of very strong\nexamples in the distribution (i.e. AU intensity is greater than\nA), it can be seen that robust performance can be gained.\nFor full analysis of AU experiments see [12].\nC. Pain Detection at Frame-level\nThe results for automatically detecting pain are given\nin Figure 5, which shows a clearer view of the trend we\nobserved in the AU detection results. For the individual\n2In literature, the A?metric varies from 0.5 to 1, but for this work we\nhave multiplied the metric by 100 for improved readability of results"},{"page":7,"text":"CAPP\nof pain, the processes by which pain expression is perceived\nand the role of pain expression in clinical assessment of\npeople suffering from pain conditions. Participants provided\ninformed consent for use of their video images for scientific\nstudy of the perception of pain including pain detection.\nDistribution of the database is governed by the terms of their\ninformed consent. Investigators who for scientific purposes\nare interested in undertaking studies that can be clearly\nconstrued as having the potential to advance understanding\nof the perception of pain expression or contributing to the\ndevelopment of improved techniques for clinical assessment\nof pain conditions may make application for access to the\ndatabase. Computer vision studies, which provide a means\nof modeling human decoding of pain expression, fall into\nthe category of perception of pain expression. Applications\nshould indicate how the proposed work addresses advance-\nment of knowledge in the perception of pain expression or\nimproved clinical assessment. Approved recipients of the\ndata may not redistribute it and agree to the terms of con-\nfidentiality restrictions. Use of the database for commercial\npurposes is strictly prohibited.\nThis data will be available from March 2011. If interested\nin obtaining the database, please sign and return an agree-\nment form available from\n\u02dcjeffcohn\/PainArchive\/. Once the signed form has\nbeen received, you may expect to receive instructions within\n5 business days.\nTS\nArea Underneath ROC \n(A\u2019)\n75\n78\n81\n84\nSPTSCAPPSPTS+CAPP\nFig. 5.\ndetection at the frame-level (yellow = SPTS, green = CAPP). The upper-\nbound error for all feature sets varied from approximately \u00b10.67 to 0.80.\nThe performance of the various features for the task of pain\nfeature sets, SPTS achieved 76.9 area underneath the ROC\ncurve and then the CAPP features yielding the best results\nwith 80.9. When we combine the different feature sets, we\nagain see the benefit of fusing the various representations\ntogether showing that there exists complimentary information\nwith the performance increasing to 83.9%.\nV. DISTRIBUTION DETAILS\nThe data was collected in the course of a research program\ndevoted to understanding the properties of facial expressions\nhttp:\/\/www.pitt.edu\/\nVI. CONCLUSIONS AND FUTURE WORK\nIn this paper we have described the UNBC-McMaster\nShoulder Pain Expression Archive which contains, 1) 200\nvideo sequences containing spontaneous facial expressions;\n2) 48,398 FACS coded frames, 3) pain frame-by-frame\nscores, sequence-level self-report and observer measures; and\n4) 66-point AAM landmarks. We have released this data in\nan effort to address the lack of FACS coded spontaneous\nexpressions available for researchers as well as promoting\nand facilitating research into the perception of pain. We have\nalso included baseline results from our AAM\/SVM system.\nPain detection represents a key application in which facial\nexpression recognition could be applied successfully, espe-\ncially if applied in the context of an heavily constrained situ-\nation such as an ICU ward where the number of expressions\nis greatly limited. This is in compared to the situation where\na person is mobile and expresses a broad gamut of emotions,\nwhere the approach we have taken here would be of little use\nas the painful facial actions are easily confused with other\nemotions (such as sadness, fear and surprise). For this to\noccur, a very large dataset which is captured in conditions\nthat are indicative of the behavior to be expected in addition\nto being accurately coded needs to be collected. Another\nissue is the requirement of the detection in terms of timing\naccuracy. In our system presented here, we detect pain at\nevery frame. However, at what level does this need to be\naccurate at - milliseconds, seconds or minutes? Again this is\ndepends on the context in which this system will be used.\nA more likely scenario would be to detect pain as an event\nor at a sequence level (i.e. if a person was in pain over a\nwindow of 1 to 2 mins). We plan to look into this area in\nthe future.\nVII. ACKNOWLEDGMENTS\nZara Ambadar, Nicole Grochowina, Amy Johnson, David\nNordstokke, Nicole Ridgeway, Racquel Kueffner, Shawn\nZuratovic and Nathan Unger provided technical assistance.\nREFERENCES\n[1] Y. Tian, J. Cohn, and T. Kanade, \u201cFacial expression analysis,\u201d in The\nhandbook of emotion elicitation and assessment, S. Li and A. Jain,\nEds.New York, NY, USA: Springer, pp. 247\u2013276.\n[2] Y. Tong, W. Liao, and Q. Ji, \u201cFacial Action Unit Recognition by\nExploiting Their Dynamic and Semantic Relationships,\u201d IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, vol. 29, no. 10,\npp. 1683\u20131699, 2007.\n[3] Z. Zeng, M. Pantic, G. Roisman, and T. Huang, \u201cA Survey of Affect\nRecognition Methods: Audio, Visual and Spontaneous Expressions,\u201d\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\nvol. 31, no. 1, pp. 39\u201358, 2009.\n[4] R. Gross, I. Matthews, S. Baker, and T. Kanade, \u201cThe CMU Multiple\nPose, Illumination, and Expression (MultiPIE),\u201d Robotics Institute,\nCarnegie Mellon University, Tech. Rep., 2007.\n[5] M. Valstar and M. Pantic, \u201cInduced Disgust, Happiness and Surprise:\nan Addition to the MMI Facial Expression Database,\u201d in Proceed-\nings of the 3rd International Workshop on EMOTION: Corpora for\nResearch on Emotion and Affect, 2010.\n[6] L. Yin, X. Chen, Y. Sun, T. Worm, and M. Reale, \u201cA High-resolution\n3D Dynamic Facial Expression Database ,\u201d in Proceedings of the\nInternational Conference on Automatic Face and Gesture Recognition,\n2008.\n[7] M. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and\nJ. Movellan, \u201cAutomatic Recognition of Facial Actions in Spontaneous\nExpressions,\u201d Journal of Multimedia, 2006."},{"page":8,"text":"[8] J. Whitehill, G. Littlewort, I. Fasel, M. Bartlett, and J. Movellan,\n\u201cTowards Practical Smile Detection,\u201d IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. 31, no. 11, pp. 2106\u20132111,\n2009.\n[9] A. Ashraf, S. Lucey, J. Cohn, T. Chen, Z. Ambadar, K. Prkachin,\nP. . Solomon, and B.-J. Theobald, \u201cThe painful face: pain expression\nrecognition using active appearance models,\u201d in Proceedings of the 9th\ninternational conference on Multimodal interfaces.\nJapan: ACM, 2007, pp. 9\u201314.\n[10] A. Ashraf, S. Lucey, J. Cohn, K. M. Prkachin, and P. Solomon, \u201cThe\nPainful Face II\u2013 Pain Expression Recognition using Active Appearance\nModels,\u201d Image and Vision Computing, vol. 27, no. 12, pp. 1788\u20131796,\n2009.\n[11] P. Lucey, J. Cohn, S. Lucey, I. Matthews, S. Sridharan, and\nK. Prkachin, \u201cAutomatically Detecting Pain Using Facial Actions,\u201d in\nProceedings of the International Conference on Affective Computing\nand Intelligent Interaction, 2009, pp. 1\u20138.\n[12] P. Lucey, J. Cohn, I. Matthews, S. Lucey, J. Howlett, S. Sridharan, and\nK. Prkachin, \u201cAutomatically Detecting Pain in Video Through Facial\nAction Units,\u201d IEEE Transactions on Systems Man and Cybernetics,\nPart B, 2010.\n[13] A. Gawande, The Checklist Manifesto: How to Get Things Right.\nMetropolitan Books, 2010.\n[14] K. Craig, K. Prkachin, and R. Grunau, \u201cThe facial expression of pain,\u201d\nin Handbook of pain assessment.\n[15] K. Prkachin, \u201cThe consistency of facial expressions of pain: a com-\nparison across modalities,\u201d Pain, vol. 51, pp. 297\u2013306, 1992.\n[16] K. Prkachin and P. Solomon, \u201cThe structure, reliability and validity\nof pain expression: Evidence from patients with shoulder pain,\u201d Pain,\nvol. 139, pp. 267\u2013274, 2008.\n[17] P. Ekman, W. Friesen, and J. Hager, Facial Action Coding System:\nResearch Nexus. Salt Lake City, UT, USA: Network Research\nInformation, 2002.\n[18] K. Prkachin and S. Mercer, \u201cPain expression in patients with shoulder\npathology: validity, coding properties and relation to sickness impact,\u201d\nPain, vol. 39, pp. 257\u2013265, 1989.\n[19] M. Heft, R. Gracely, R. Dubner, and P. McGrath, \u201cA validation model\nfor verbal descriptor scaling of human clinical pain,\u201d Pain, vol. 9, pp.\n363\u2013373, 1980.\n[20] A. Anastasi, Psychological Testing.\n[21] J. Cohen, Statistical Power Analysis for the Social Sciences. Lawrence\nErlbaum Associates, NJ, USA, 1988.\n[22] J. Xiao, S. Baker, I. Matthews, and T. Kanade, \u201cReal-Time Combined\n2D+3D Active Appearance Models,\u201d in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2004, pp.\n535\u2013542.\n[23] T. Cootes, G. Edwards, and C. Taylor, \u201cActive Appearance Models,\u201d\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\nvol. 23, no. 6, pp. 681\u2013685, 2001.\n[24] I. Matthews and S. Baker, \u201cActive appearance models revisited,\u201d\nInternational Journal of Computer Vision, vol. 60, no. 2, pp. 135\u2013\n164, 2004.\n[25] S. Lucey, A. Ashraf, and J. Cohn, \u201cInvestigating spontaneous facial\naction recognition through aam representations of the face,\u201d in Face\nRecognition Book, K. Kurihara, Ed.\n[26] C. Hsu, C. C. Chang, and C. J. Lin, \u201cA practical guide to support\nvector classification,\u201d Tech. Rep., 2005.\n[27] C.-C. Chang and C.-J. Lin, LIBSVM: a library for support vector ma-\nchines, 2001, software available at http:\/\/www.csie.ntu.edu.tw\/\u223ccjlin\/\nlibsvm.\n[28] C. Cortes and M. Mohri, \u201cConfidence Intervals for the Area Under\nthe ROC curve,\u201d Advances in Neural Information Processing Systems,\n2004.\nNagoya, Aichi,\nMacmillan, NY, USA, 1982.\nPro Literatur Verlag, 2007."}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Kenneth_Prkachin\/publication\/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database\/links\/00b495220a93e5795f000000.pdf","widgetId":"rgw32_56ab1977ab01e"},"id":"rgw32_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=221292544&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw33_56ab1977ab01e"},"id":"rgw33_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=221292544&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":221292544,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":221292544,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":70485381,"url":"researcher\/70485381_Michel_Valstar","fullname":"Michel Valstar","last":true,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278647685566470%401443446193460_m\/Michel_Valstar.png"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Conference Paper","publicationDate":"Nov 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":5,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/267155572_Automatic_Behaviour_Understanding_in_Medicine","usePlainButton":true,"publicationUid":267155572,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/267155572_Automatic_Behaviour_Understanding_in_Medicine","title":"Automatic Behaviour Understanding in Medicine","displayTitleAsLink":true,"authors":[{"id":70485381,"url":"researcher\/70485381_Michel_Valstar","fullname":"Michel Valstar","last":true,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278647685566470%401443446193460_m\/Michel_Valstar.png"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["International Workshop on Roadmapping the Future of Multimodal Research; 11\/2015"],"abstract":"Now that Affective Computing and Social Signal Process-ing methods are becoming increasingly robust and accu-rate, novel areas of applications with significant societal im-pact are opening up for exploration. Perhaps one of the most promising areas is the application of automatic expres-sive behaviour understanding to help diagnose, monitor, and treat medical conditions that themselves alter a person's so-cial and affective signals. This work argues that this is now essentially a new area of research, called behaviomedics. It gives a definition of the area, discusses the most important groups of medical conditions that could benefit from this, and makes suggestions for future directions.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/267155572_Automatic_Behaviour_Understanding_in_Medicine","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Michel_Valstar\/publication\/267155572_Automatic_Behaviour_Understanding_in_Medicine\/links\/5446ca730cf22b3c14e0b418.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Michel_Valstar","sourceName":"Michel Valstar","hasSourceUrl":true},"publicationUid":267155572,"publicationUrl":"publication\/267155572_Automatic_Behaviour_Understanding_in_Medicine","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/267155572_Automatic_Behaviour_Understanding_in_Medicine\/links\/5446ca730cf22b3c14e0b418\/smallpreview.png","linkId":"5446ca730cf22b3c14e0b418","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=267155572&reference=5446ca730cf22b3c14e0b418&eventCode=&origin=publication_list","widgetId":"rgw37_56ab1977ab01e"},"id":"rgw37_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=267155572&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"5446ca730cf22b3c14e0b418","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":221292544,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/267155572_Automatic_Behaviour_Understanding_in_Medicine\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Automatic recognition of pain focuses almost entirely on cues from the face. Lucey et al. report baseline recognition models based on AAM features and Support Vector Machine (SVM) classifiers to recognise pain versus non pain with an area under the ROC curve (AUC) score of 83.9% on the UNBC-McMaster dataset [23]. Many further studies on this dataset followed, e.g. "],"widgetId":"rgw38_56ab1977ab01e"},"id":"rgw38_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw36_56ab1977ab01e"},"id":"rgw36_56ab1977ab01e","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=267155572&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2029052295,"url":"researcher\/2029052295_Frerk_Saxen","fullname":"Frerk Saxen","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272841702637596%401442061939390_m\/Frerk_Saxen.png"},{"id":2043200184,"url":"researcher\/2043200184_Philipp_Werner","fullname":"Philipp Werner","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Conference Paper","publicationDate":"Sep 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/281811172_Handling_Data_Imbalance_in_Automatic_Facial_Action_Intensity_Estimation","usePlainButton":true,"publicationUid":281811172,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/281811172_Handling_Data_Imbalance_in_Automatic_Facial_Action_Intensity_Estimation","title":"Handling Data Imbalance in Automatic Facial Action Intensity Estimation","displayTitleAsLink":true,"authors":[{"id":2029052295,"url":"researcher\/2029052295_Frerk_Saxen","fullname":"Frerk Saxen","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272841702637596%401442061939390_m\/Frerk_Saxen.png"},{"id":2043200184,"url":"researcher\/2043200184_Philipp_Werner","fullname":"Philipp Werner","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["British Machine Vision Conference (BMVC); 09\/2015"],"abstract":"Automatic Action Unit (AU) intensity estimation is a key problem in facial expression analysis. But limited research attention has been paid to the inherent class imbalance, which usually leads to suboptimal performance. To handle the imbalance, we propose (1) a novel multiclass under-sampling method and (2) its use in an ensemble. We compare our approach with state of the art sampling methods used for AU intensity estimation. Multiple datasets and widely varying performance measures are used in the literature, making direct comparison difficult. To address these shortcomings, we compare different performance measures for AU intensity estimation and evaluate our proposed approach on three publicly available datasets, with a comparison to state of the art methods along with a cross dataset evaluation.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/281811172_Handling_Data_Imbalance_in_Automatic_Facial_Action_Intensity_Estimation","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Frerk_Saxen\/publication\/281811172_Handling_Data_Imbalance_in_Automatic_Facial_Action_Intensity_Estimation\/links\/55f920c508aec948c48d3a75.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Frerk_Saxen","sourceName":"Frerk Saxen","hasSourceUrl":true},"publicationUid":281811172,"publicationUrl":"publication\/281811172_Handling_Data_Imbalance_in_Automatic_Facial_Action_Intensity_Estimation","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/281811172_Handling_Data_Imbalance_in_Automatic_Facial_Action_Intensity_Estimation\/links\/55f920c508aec948c48d3a75\/smallpreview.png","linkId":"55f920c508aec948c48d3a75","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=281811172&reference=55f920c508aec948c48d3a75&eventCode=&origin=publication_list","widgetId":"rgw40_56ab1977ab01e"},"id":"rgw40_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=281811172&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"55f920c508aec948c48d3a75","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":221292544,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/281811172_Handling_Data_Imbalance_in_Automatic_Facial_Action_Intensity_Estimation\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["), 12 AUs (DISFA [16]), resp. 10 AUs (UNBC-McMaster [15]). (b) Mean cross-dataset performance of the seven common AUs (SVR Ensemble with \u03b1 = 0.5). "],"widgetId":"rgw41_56ab1977ab01e"},"id":"rgw41_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw39_56ab1977ab01e"},"id":"rgw39_56ab1977ab01e","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=281811172&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":71126356,"url":"researcher\/71126356_Liliana_Lo_Presti","fullname":"Liliana Lo Presti","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272208606003208%401441910997240_m\/Liliana_Lo_Presti.png"},{"id":7142168,"url":"researcher\/7142168_Marco_La_Cascia","fullname":"Marco La Cascia","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Conference Paper","publicationDate":"Sep 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":1,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/277312235_Ensemble_of_Hankel_Matrices_for_Face_Emotion_Recognition","usePlainButton":true,"publicationUid":277312235,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/277312235_Ensemble_of_Hankel_Matrices_for_Face_Emotion_Recognition","title":"Ensemble of Hankel Matrices for Face Emotion Recognition","displayTitleAsLink":true,"authors":[{"id":71126356,"url":"researcher\/71126356_Liliana_Lo_Presti","fullname":"Liliana Lo Presti","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272208606003208%401441910997240_m\/Liliana_Lo_Presti.png"},{"id":7142168,"url":"researcher\/7142168_Marco_La_Cascia","fullname":"Marco La Cascia","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["International Conference on Image Analysis and Processing 2015; 09\/2015"],"abstract":"In this paper, a face emotion is considered as the result of the composition of multiple concurrent signals, each corresponding to the movements of a specific facial muscle. These concurrent signals are represented by means of a set of multi-scale appearance features that might be correlated with one or more concurrent signals. \nThe extraction of these appearance features from a sequence of face images yields to a set of time series. \nThis paper proposes to use the dynamics regulating each appearance feature time series to recognize among different face emotions. To this purpose, an ensemble of Hankel matrices corresponding to the extracted time series is used for emotion classification within a framework that combines nearest neighbor and a majority vote schema.\nExperimental results on a public available dataset shows that the adopted representation is promising and yields state-of-the-art accuracy in emotion classification.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/277312235_Ensemble_of_Hankel_Matrices_for_Face_Emotion_Recognition","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Liliana_Lo_Presti\/publication\/277312235_Ensemble_of_Hankel_Matrices_for_Face_Emotion_Recognition\/links\/557fe48608aeb61eae26314d.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Liliana_Lo_Presti","sourceName":"Liliana Lo Presti","hasSourceUrl":true},"publicationUid":277312235,"publicationUrl":"publication\/277312235_Ensemble_of_Hankel_Matrices_for_Face_Emotion_Recognition","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/277312235_Ensemble_of_Hankel_Matrices_for_Face_Emotion_Recognition\/links\/557fe48608aeb61eae26314d\/smallpreview.png","linkId":"557fe48608aeb61eae26314d","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=277312235&reference=557fe48608aeb61eae26314d&eventCode=&origin=publication_list","widgetId":"rgw43_56ab1977ab01e"},"id":"rgw43_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=277312235&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"557fe48608aeb61eae26314d","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":221292544,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/277312235_Ensemble_of_Hankel_Matrices_for_Face_Emotion_Recognition\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Nonetheless, recognition of face expressions and emotions is of great interest in many fields such as assistive technologies [21], [10], socially assistive robotics [23], computational behavioral science [25], [18], [35], and the emerging field of audience measurement [11]. A vast literature on affective computing [35], [27], [21], has shown that an emotion can be identified by a subset of detected action units. This suggests that face emotion results as combination of movements of various facial muscles. "],"widgetId":"rgw44_56ab1977ab01e"},"id":"rgw44_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw42_56ab1977ab01e"},"id":"rgw42_56ab1977ab01e","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=277312235&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":221292544,"publicationLink":"publication\/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw35_56ab1977ab01e"},"id":"rgw35_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=221292544&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=53","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":53,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw34_56ab1977ab01e"},"id":"rgw34_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=221292544&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"00b495220a93e5795f000000","name":"Kenneth M Prkachin","date":null,"nameLink":"profile\/Kenneth_Prkachin","filename":"2011_PAINFUL.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Kenneth_Prkachin\/publication\/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database\/links\/00b495220a93e5795f000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Kenneth_Prkachin\/publication\/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database\/links\/00b495220a93e5795f000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"0770ff1ccac33cfcc9bd45e41c3b4cc3","showFileSizeNote":false,"fileSize":"3.56 MB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"00b495220a93e5795f000000","name":"Kenneth M Prkachin","date":null,"nameLink":"profile\/Kenneth_Prkachin","filename":"2011_PAINFUL.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Kenneth_Prkachin\/publication\/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database\/links\/00b495220a93e5795f000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Kenneth_Prkachin\/publication\/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database\/links\/00b495220a93e5795f000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"0770ff1ccac33cfcc9bd45e41c3b4cc3","showFileSizeNote":false,"fileSize":"3.56 MB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Conference Paper","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=bi05ieutQeyNleyE5gbaRPRyisF4v6u_cxSjowqTtMylvHidt6KwdxclFbCQJoV9oEJVyQuQPJsR1j1wueaTSQ.SJhMA4R0WeZTn6AaurnUKRE4zY5PKRpQ7LkXK3a9NScObRbjk2hgUvKD49a07v2ghfuy4jf7wFFGgyM8Z8sytA","clickOnPill":"publication.PublicationFigures.html?_sg=QItH_h-q_gmt5qii8QOadQVtbss_wb7sEBN53VExJ1sB38tO85GuVuFgcuoCUImVWKF41emUKgHeyffy7Za_0w.FNepnSG5RTnj5USRS6bu43iHDJYUa1Y-yOt0pMHVD7Ac6QRn-A8Vq4c1EUtJV3fAEpqCUCdJkNWeVgt97nA71w"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1o9o3\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FKenneth_Prkachin%2Fpublication%2F221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database%2Flinks%2F00b495220a93e5795f000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1o9o3\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=Tf6UI5Bm6Qd5PMWMrS-eKPUuLfct6zRxcWqTsp_3bvZ5GIT9qnOuPPwB4HsEhQ40JqdXBcBIqDT0TRHdRVZB6A","urlHash":"7ef728c64a2273699287d085ea0b875a","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=wk5fmQbGTAJB16hagwi5aSTL7qV3UYj1tx4LoX9YliX8g4a0gOpDxdsUo70yHvQFJN9k1ZW6iFVDhvBHjCEMmUS8Vw8FfIfTyOsl8GiCh34.bAncA7rcbpZMB6xEjHB-_tTeU5_HhKK5Nqj-PRw4vca51WSh1CqNjkhctmuu0qmJCm4GWBVPPLsQ7ApKcsPVVg.XHMRXK_C-xa8NMi58vARlxD2MqT-bUlxgpxLVYJjzU9mCFfcUeIQR5ThrfHLADmQTJzUpBtHkqmItmg2tM031Q","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"00b495220a93e5795f000000","trackedDownloads":{"00b495220a93e5795f000000":{"v":false,"d":false}},"assetId":"AS:97531870580737@1400264818214","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":221292544,"commentCursorPromo":null,"widgetId":"rgw46_56ab1977ab01e"},"id":"rgw46_56ab1977ab01e","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FKenneth_Prkachin%2Fpublication%2F221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database%2Flinks%2F00b495220a93e5795f000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A97531870580737%401400264818214&publicationUid=221292544&linkId=00b495220a93e5795f000000&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Painful data: The UNBC-McMaster shoulder pain expression archive database","publicationType":"Conference Paper","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=GvbygsVWrzI2P8kl9QBAVe1C4i8knxEF1MkQKK-8ks9-OujZUuGYSf4L03aYGz1ZaObltAOvbV9U8iuB1ohYmPw1j9FKcpZl9nA9k-Rxm_U.bJsAFvwXXwv0pAd7TKoezs0UGQ7UZOKLsjRpWO_xBjbTjDC903OBZ_lbbqkShYKjbK9h2eSJouglezfbmFq8aA.tu5X5tRf81KZnaFmZmKcQJ7Q3_aTgNfJqwBXvEZOHhUue3YsyTVfwJkgI3bfFw49qKuNsrMythTgdGh1ISGCPg","publicationUid":221292544,"trackedDownloads":{"00b495220a93e5795f000000":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw48_56ab1977ab01e"},"id":"rgw48_56ab1977ab01e","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw49_56ab1977ab01e"},"id":"rgw49_56ab1977ab01e","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw50_56ab1977ab01e"},"id":"rgw50_56ab1977ab01e","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw51_56ab1977ab01e"},"id":"rgw51_56ab1977ab01e","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw52_56ab1977ab01e"},"id":"rgw52_56ab1977ab01e","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw47_56ab1977ab01e"},"id":"rgw47_56ab1977ab01e","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw45_56ab1977ab01e"},"id":"rgw45_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1977ab01e"},"id":"rgw2_56ab1977ab01e","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":221292544},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=221292544&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1977ab01e"},"id":"rgw1_56ab1977ab01e","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"3FLETPVRO5igd5yKumfBAHTP7dxi5P5fKSHwkdTo5I+DBrnwznEl8inQSmGSh7MUmPlurbowkG743JBAK9RLH5my\/CDUY\/+Sf8j0XH9vUs6XSbBlFZ9ggRekw5gwHu8oxH24e27qJB27B+sPDD5ZO8pCiyae9aIgny4j4ITPa4DGvOmI0W4S0fbQl09h8pt+g14vNOZO83SrJBM375hqaTUCwjoTyZFL9nxOfE4oU5ha3iIasEU4BTZT0Ot6u1C20th9Dvz3LT8jdAlTSYzTzTDJSJgJaHjXpUi1wwOip7k=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Painful data: The UNBC-McMaster shoulder pain expression archive database\" \/>\n<meta property=\"og:description\" content=\"A major factor hindering the deployment of a fully functional automatic facial expression detection system is the lack of representative data. A solution to this is to narrow the context of the...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database\/links\/00b495220a93e5795f000000\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database\" \/>\n<meta property=\"rg:id\" content=\"PB:221292544\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1109\/FG.2011.5771462\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Painful data: The UNBC-McMaster shoulder pain expression archive database\" \/>\n<meta name=\"citation_author\" content=\"Patrick Lucey\" \/>\n<meta name=\"citation_author\" content=\"Jeffrey F. Cohn\" \/>\n<meta name=\"citation_author\" content=\"Kenneth M. Prkachin\" \/>\n<meta name=\"citation_author\" content=\"Patricia E. Solomon\" \/>\n<meta name=\"citation_author\" content=\"Iain Matthews\" \/>\n<meta name=\"citation_conference_title\" content=\"Ninth IEEE International Conference on Automatic Face and Gesture Recognition (FG 2011), Santa Barbara, CA, USA, 21-25 March 2011\" \/>\n<meta name=\"citation_publication_date\" content=\"2011\/03\/25\" \/>\n<meta name=\"citation_firstpage\" content=\"57\" \/>\n<meta name=\"citation_lastpage\" content=\"64\" \/>\n<meta name=\"citation_doi\" content=\"10.1109\/FG.2011.5771462\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Kenneth_Prkachin\/publication\/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database\/links\/00b495220a93e5795f000000.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Conference Paper');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-4c6a8ade-39d2-40cc-822e-3e6ca525ce0c","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":532,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw53_56ab1977ab01e"},"id":"rgw53_56ab1977ab01e","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-4c6a8ade-39d2-40cc-822e-3e6ca525ce0c", "c6cd80261b4e58ecc5d2c0e55c06ca06f4a06582");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-4c6a8ade-39d2-40cc-822e-3e6ca525ce0c", "c6cd80261b4e58ecc5d2c0e55c06ca06f4a06582");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw54_56ab1977ab01e"},"id":"rgw54_56ab1977ab01e","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/221292544_Painful_data_The_UNBC-McMaster_shoulder_pain_expression_archive_database","requestToken":"SG\/VDMX3uQCqw65vKg32jIE7Jb86r+9M0oxm0yw8sjvooj1tdjvXRl3mORaglpaDfHQK5yvMblFWTEkisoH6WVLMkA2Nej9BQI3Ssr4xdnTamx8e2PwzVjAiulwwUTsHRtsBzcRjIp4TWZsXoQ\/wnOaXawNBF+9ok+N4Nh1rj+WQeL8LcvCRuhi6odLl3oJURDlE906Wijg4dHHyh3\/HwI5nKm+bh7GIaIqFfGhCu4iCiOinkLETI5U1pbyfec9BdGC0CKysqR7KivEdOHoCnNDrrTBAK0RmhbsLcpnBlSU=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=kzXWBrKa4PFgMAITXHWsiC2E8wo0m2KOSLUKkddv4KvXQmD1_R8HXkYiLC8n1TTf","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjIxMjkyNTQ0X1BhaW5mdWxfZGF0YV9UaGVfVU5CQy1NY01hc3Rlcl9zaG91bGRlcl9wYWluX2V4cHJlc3Npb25fYXJjaGl2ZV9kYXRhYmFzZQ%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw56_56ab1977ab01e"},"id":"rgw56_56ab1977ab01e","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw55_56ab1977ab01e"},"id":"rgw55_56ab1977ab01e","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw57_56ab1977ab01e"},"id":"rgw57_56ab1977ab01e","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Conference Paper","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
