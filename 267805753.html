<!DOCTYPE html> <html lang="en" class="" id="rgw35_56ab1ccce0ea8"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="mzY2N2BF+LMoeo6vPGzCR64yHTl9oCCmfbBJlAGOAWN4bc6I9zdYYk2vvme0ASXJEgfQWmC+YH4xmy7o4cSgCGHWIRmhCHPNbFRhZmfK62vV2JUkCKFxLP7WTxItw5U0YJFZtAMZh/jgBRYN7QptYgsZeB55EsVC8PGJU9/6k1o5zbF2pO91ieAVX8QD7alOGAX+qLsduVI02qGDOXqVyjk6KHuXkuiWHVFx2u12CsVbaF2Ap9q8GsnAKOPIHe2DaiMvagZi87JKCMAtng4T/jSjgZ+RyMXZWkQaTAzMLB0="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-c318daf5-9b14-4788-830a-2d8fb85940b7",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/267805753_Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning" />
<meta property="og:description" content="We introduce a variational Bayesian inference algorithm which can be widely applied to sparse linear models. The algorithm is based on the spike and slab prior which, from a Bayesian perspective,..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/267805753_Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning/links/545c772c0cf2f1dbcbcb28eb/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/267805753_Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning" />
<meta property="rg:id" content="PB:267805753" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning" />
<meta name="citation_author" content="Michalis K Titsias" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/267805753_Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/267805753_Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning</title>
<meta name="description" content="Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1ccce0ea8" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1ccce0ea8" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab1ccce0ea8">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Spike%20and%20Slab%20Variational%20Inference%20for%20Multi-Task%20and%20Multiple%20Kernel%20Learning&rft.au=Michalis%20K%20Titsias&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning</h1> <meta itemprop="headline" content="Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/267805753_Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning/links/545c772c0cf2f1dbcbcb28eb/smallpreview.png">  <div id="rgw7_56ab1ccce0ea8" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab1ccce0ea8" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Michalis_Titsias2" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A279280278884356%401443597015332_m" title="Michalis Titsias" alt="Michalis Titsias" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Michalis Titsias</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw9_56ab1ccce0ea8" data-account-key="Michalis_Titsias2">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Michalis_Titsias2"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A279280278884356%401443597015332_l" title="Michalis Titsias" alt="Michalis Titsias" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Michalis_Titsias2" class="display-name">Michalis Titsias</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Athens_University_of_Economics_and_Business" title="Athens University of Economics and Business">Athens University of Economics and Business</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">                       </div> <div id="rgw10_56ab1ccce0ea8" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>We introduce a variational Bayesian inference algorithm which can be widely applied to sparse linear models. The algorithm is based on the spike and slab prior which, from a Bayesian perspective, is the golden standard for sparse inference. We apply the method to a general multi-task and multiple kernel learning model in which a common set of Gaussian process functions is linearly combined with task-specific sparse weights, thus inducing relation between tasks. This model unifies several sparse linear models, such as generalized linear models, sparse factor analysis and matrix factorization with missing values, so that the variational algorithm can be applied to all these cases. We demonstrate our approach in multi-output Gaussian process regression, multi-class classification, image processing applications and collaborative filtering.</div> </p>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw23_56ab1ccce0ea8">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw22_56ab1ccce0ea8"  itemprop="articleBody">  <p>Page 1</p> <p>Spike and Slab Variational Inference for Multi-Task<br />and Multiple Kernel Learning<br />Michalis K. Titsias<br />University of Manchester<br />mtitsias@gmail.com<br />Miguel L´ azaro-Gredilla<br />Univ. de Cantabria &amp; Univ. Carlos III de Madrid<br />miguel@tsc.uc3m.es<br />Abstract<br />We introduce a variational Bayesian inference algorithm which can be widely<br />applied to sparse linear models. The algorithm is based on the spike and slab prior<br />which, from a Bayesian perspective, is the golden standard for sparse inference.<br />We apply the method to a general multi-task and multiple kernel learning model<br />in which a common set of Gaussian process functions is linearly combined with<br />task-specific sparse weights, thus inducing relation between tasks. This model<br />unifies several sparse linear models, such as generalized linear models, sparse<br />factor analysis and matrix factorization with missing values, so that the variational<br />algorithm can be applied to all these cases. We demonstrate our approach in multi-<br />output Gaussian process regression, multi-class classification, image processing<br />applications and collaborative filtering.<br />1 Introduction<br />Sparse inference has found numerous applications in statistics and machine learning [1, 2, 3]. It is a<br />generic idea that can be combined with popular models, such as linear regression, factor analysis and<br />more recently multi-task and multiple kernel learning models. In the regularization theory literature<br />sparse inference is tackled via ?1regularization [2], which requires expensive cross-validation for<br />model selection. From a Bayesian perspective, the spike and slab prior [1, 4, 5], also called two-<br />groups prior [6], is the golden standard for sparse linear models. However, the discrete nature of<br />the prior makes Bayesian inference a very challenging problem. Specifically, for M linear weights,<br />inference under a spike and slab prior distribution on those weights requires a combinatorial search<br />over 2Mpossible models. The problems found when working with the spike and slab prior led<br />several researchers to consider soft-sparse or shrinkage priors such as the Laplace and other related<br />scale mixtures of normals [3, 7, 8, 9, 10]. However, such priors are not ideal since they assign zero<br />probability mass to events associated with weights having zero value.<br />In this paper, we introduce a simple and efficient variational inference algorithm based on the spike<br />and slab prior which can be widely applied to sparse linear models. The novel characteristic of this<br />algorithm is that the variational distribution over sparse weights has a factorial nature, i.e., it can be<br />written as a mixture of 2Mcomponents where M is the number of weights. Unlike the standard<br />mean field approximation which uses a unimodal variational distribution, our variational algorithm<br />can more precisely match the combinational nature of the posterior distribution over the weights.<br />We will show that the proposed variational approach is more accurate and robust to unfavorable<br />initializations than the standard mean field variational approximation.<br />We apply the variational method to a general multi-task and multiple kernel learning model that<br />expresses the correlation between tasks by letting them share a common set of Gaussian process<br />latent functions. Each task is modeled by linearly combining these latent functions with task-<br />specific weights which are given a spike and slab prior distribution. This model is a spike and<br />slab Bayesian reformulation of previous Gaussian process-based single-task multiple kernel learning<br />1</p>  <p>Page 2</p> <p>methods [11, 12, 13] and multi-task Gaussian processes (GPs) [14, 15, 16, 17]. Further, this model<br />unifies several sparse linear models, such as generalized linear models, factor analysis, probabilistic<br />PCA and matrix factorization with missing values. In the experiments, we apply the variational in-<br />ference algorithms to all the above models and present results in multi-output regression, multi-class<br />classification, image denoising, image inpainting and collaborative filtering.<br />2 Spike and slab multi-task and multiple kernel learning<br />Section 2.1 discusses the spike and slab multi-task and multiple kernel learning (MTMKL) model<br />that linearly combines Gaussian process latent functions. Spike and slab factor analysis and proba-<br />bilistic PCA is discussed in Section 2.2, while missing values are dealt with in Section 2.3.<br />2.1The model<br />Let D = {X,Y}, with X ∈ RN×Dand Y ∈ RN×Q, be a dataset such that the n-th row of X is<br />an input vector xnand the n-th row of Y is the set of Q corresponding tasks or outputs. We use<br />yqto refer to the q-th column of Y and ynqto the (n,q) entry. Outputs Y are then assumed to be<br />generated according to the following hierarchical Bayesian model:<br />ynq∼ N(ynq|fq(xn),σ2<br />M<br />?<br />wqm∼ πN(wqm|0,σ2<br />φm(x) ∼ GP(µm(x),km(xi,xj)),<br />Here, each µm(x) is a mean function, km(xi,xj) a covariance function, wq= [wq1,...,wqM]?,<br />φ(x) = [φ1(x),...,φM(x)]?and δ0(wqm) denotes the Dirac delta function centered at zero. Since<br />each of the Q tasks is a linear combination of the same set of latent functions {φm(x)}M<br />typically M &lt; Q ), correlation is induced in the outputs. Sharing a common set of features means<br />that “knowledge transfer” between tasks can occur and latent functions are inferred more accurately,<br />since data belonging to all tasks are used.<br />Several linear models can be expressed as special cases of the above. For instance, a generalized<br />linear model is obtained when the GPs are Dirac delta measures (with zero covariance functions)<br />that deterministically assign each φm(x) to its mean function µm(x). However, the model in (1) has<br />a number of additional features not present in standard linear models. Firstly, the basis functions are<br />no longer deterministic, but they are instead drawn from different GPs, so an extra layer of flexibility<br />is added to the model. Thus, a posterior distribution over the basis functions of the generalized linear<br />model can be inferred from data. Secondly, a truly sparse prior, the spike and slab prior (1c), is<br />placed over the weights of the model. Specifically, with probability 1−π, each wqmis zero, and with<br />probability π, it is drawn from a Gaussian. This contrasts with previous approaches [3, 7, 8, 9, 13]<br />in which soft-sparse priors that assign zero probability mass to the weights being exactly zero were<br />used. Hyperparameters π and σ2<br />the discrepancy of nonzero weights, respectively. Thirdly, the number of basis functions M can be<br />inferred from data, since the sparse prior on the weights allows basis functions to be “switched off”<br />as necessary by setting the corresponding weights to zero.<br />Further, the model in (1) can be considered as a spike and slab Bayesian reformulation of multi-<br />task [14, 15] and multiple kernel learning previous methods [11, 12] that learn the weights using<br />maximum likelihood. By assuming the weights wqare given, each output function yq(x) is a GP<br />with covariance function<br />q),<br />∀n,q<br />(1a)<br />fq(x) =<br />m=1<br />wqmφm(x) = w?<br />qφ(x),<br />∀q<br />(1b)<br />w) + (1 − π)δ0(wqm),<br />∀q,m<br />∀m.<br />(1c)<br />(1d)<br />m=1(where<br />ware learnable in order to determine the amount of sparsity and<br />Cov[(yq(xi),yq(xj)] =<br />M<br />?<br />m=1<br />w2<br />qmkm(xi,xj),<br />which clearly consists of a conic combination of kernel functions. Therefore, the proposed model<br />can be reinterpreted as multiple kernel learning in which the weights of each kernel are assigned<br />spike and slab priors in a full Bayesian formulation.<br />2</p>  <p>Page 3</p> <p>2.2<br />An interesting case arises when µm(x) = 0 and km(xi,xj) = δij∀m, where δijis the Kronecker<br />delta. This says that each latent function is drawn from a white process so that it consists of indepen-<br />dent values each following the standard normal distribution. We first define matrices Φ ∈ RN×M<br />and W ∈ RQ×M, whose elements are, respectively, φnm= φm(xn) and wqm. Then, the model in<br />(1) reduces to<br />Y = ΦW?+ ξ,<br />wqm∼ πN(wqm|0,σ2<br />φnm∼ N(φnm|0,1),<br />ξnq∼ N(ξnq|0,σ2<br />where ξ is an N × Q noise matrix with entries ξnq. The resulting model thus corresponds to sparse<br />factor analysis or sparse probabilistic PCA (when the noise is homoscedastic, i.e., σ2<br />all q). Observe that the sparse spike and slab prior is placed on the factor loadings W.<br />Sparse factor and principal component analysis<br />(2a)<br />(2b)<br />(2c)<br />(2d)<br />w) + (1 − π)δ0(wqm),<br />∀q,m<br />∀n,m<br />∀n,q,<br />q),<br />qis constant for<br />2.3<br />The method can easily handle missing values and thus be applied to problems involving matrix<br />completion and collaborative filtering. More precisely, in the presence of missing values we have<br />a binary matrix Z ∈ RN×Qthat indicates the observed elements in Y. Using Z the likelihood in<br />(1a) is modified according to ynq∼ N(ynq|fq(xn),σ2<br />consider missing values in applications such as image inpainting and collaborative filtering.<br />Missing values<br />q), ∀n,qs.t. [Z]nq= 1. In the experiments we<br />3Efficient variational inference<br />The presence of the Dirac delta mass function makes the application of variational approximate<br />inference algorithms in spike and slab Bayesian models troublesome. However, there exists a sim-<br />ple reparameterization of the spike and slab prior that is more amenable to approximate inference<br />methods. Specifically, assume a Gaussian random variable ? wqm∼ N(? wqm|0,σ2<br />wqm= sqm? wqmand assign the above prior distributions on sqmand ? wqm. Thus, the reparameter-<br />p(? wqm,sqm) = N(wqm|0,σ2<br />sqm? wqm. After the above reparameterization, a standard mean field variational method that uses the<br />q=1q(? wq,sq), where<br />q(? wq,sq) = q(? wq)q(sq) = N(? wq|µwq,Σwq)<br />and where (µwq,Σwq,γq) are variational parameters. Such an approach has extensively used in [18]<br />and also considered in [19]. However, the above variational distribution leads to a very inefficient<br />approximation. This is because (4) is a unimodal distribution, and therefore has limited capacity<br />when approximating the factorial true posterior distribution which can have exponentially many<br />modes. To analyze the nature of the true posterior distribution, we consider the following two<br />properties derived by assuming for simplicity a single output (Q = 1) so index q is dropped.<br />Property 1: The true marginal posterior p(? w|Y) can be written as a mixture distribution having 2M<br />The second property characterizes the nature of each conditional p(? w|s,Y) in the above sum.<br />s1 denotes the elements in s with value one and s0 the elements with value zero. Using the<br />w) and a Bernoulli<br />random variable sqm ∼ πsqm(1 − π)1−sqm. The product sqm? wqmforms a new random variable<br />ized spike and slab prior takes the form:<br />w)πsqm(1 − π)1−sqm,<br />Notice that the presence of wqmin the likelihood function in (1a) is now replaced by the product<br />that follows the probability distribution in eq. (1c). This allows to reparameterize wqmaccording to<br />∀q,m.<br />(3)<br />factorized variational distribution over?<br />W = {? wq}Q<br />q=1and S = {sq}Q<br />q=1takes the form q(?<br />W,S) =<br />?Q<br />M<br />?<br />m=1<br />γsqm<br />qm(1 − γqm)1−sqm<br />(4)<br />components. This is an obvious fact since p(? w|Y) =?<br />sp(? w|s,Y)p(s|Y), where the summation<br />involves all 2Mpossible values of the binary vector s.<br />Property 2: Assume the conditional distribution p(? w|s,Y). We can write s = s1∪ s0, where<br />3</p>  <p>Page 4</p> <p>correspondence between s and ? w, we have ? w = ? w1∪ ? w0.<br />as an elementwise product ? w0◦ s0, thus when s0= 0, ? w0becomes disconnected from the data.<br />p(? w|Y), which is a mixture with 2Mcomponents, with a single Gaussian distribution. Next we<br />3.1The proposed variational method<br />Then, p(? w|s,Y) factorizes as<br />p(? w|s,Y) = p(? w1|Y)N(? w0|0,σ2<br />The standard variational distribution in (4) ignores these properties and approximates the marginal<br />wI|? w0|), which says that the posterior over ? w0given s0 = 0<br />is equal to the prior over ? w0. This property is obvious because ? w0and s0appear in the likelihood<br />present an alternative variational approximation that takes into account the above properties.<br />In the reparameterized spike and slab prior, each pair of variables {? wqm,sqm} is strongly correlated<br />of the variational distribution. The simplest factorization that achieves this is:<br />since their product is the underlying variable that interacts with the data. Thus, a sensible approxi-<br />mation must treat each pair {? wqm,sqm} as a unit so that {? wqm,sqm} are placed in the same factor<br />q(? wq,sq) =<br />Thisvariationaldistributionyieldsamarginalq(? wq)whichhas2Mcomponents. Thiscanbeseenby<br />a mixture of 2Mcomponents is obtained. Therefore, Property 1 is satisfied by (5). In turns out<br />that Property 2 is also satisfied. This can be shown by taking the stationary condition for the factor<br />q(? wqm,sqm) when maximizing the variational lower bound (on the true marginal likelihood):<br />q(? wqm,sqm)q(Θ)<br />their variational distribution. The stationary condition for q(? wqm,sqm) is<br />Ze?log p(Y,? wqm,sqm,Θ)?q(Θ)N(? wqm|0,σ2<br />have q(? wqm|sqm = 0) ∝ q(? wqm,sqm = 0) =<br />we obtain q(? wqm|sqm= 0) = N(? wqm|0,σ2<br />slab probability models as long as the weights ? w and binary variables s interact inside the likelihood<br />3.2Application to the multi-task and multiple kernel learning model<br />M<br />?<br />m=1<br />q(? wqm,sqm).<br />(5)<br />writing q(? wq) =?M<br />m=1[q(? wqm,sqm= 1) + q(? wqm,sqm= 0)] and then by multiplying the terms<br />?<br />logp(Y, ? wqm,sqm,Θ)p(Θ)N(? wqm|0,σ2<br />where Θ are the remaining random variables in the model (i.e., excluding {? wqm,sqm}) and q(Θ)<br />q(? wqm,sqm) =1<br />w)πsqm(1 − π)1−sqm<br />?<br />q(? wqm,sqm)q(Θ)<br />,<br />(6)<br />w)πsqm(1 − π)1−sqm,<br />(7)<br />where Z is a normalizing constant that does not depend on {? wqm,sqm}.<br />e?log p(Y,? wqm,sqm=0,Θ)?q(Θ)is a constant that does not depend on ? wqm. From the last expression<br />The above remarks regarding variational distribution (5) are general and can hold for many spike and<br />Therefore, we<br />C<br />ZN(? wqm|0,σ2<br />w)(1 − π), where C =<br />w) which implies that Property 2 is satisfied.<br />function according to ? w ◦ s.<br />Here, we briefly discuss the variational method applied to the multi-task and multiple kernel model<br />described in Section 2.1 and refer to supplementary material for variational EM update equations.<br />The explicit form of the joint probability density function on the training data of model (1) is<br />p(Y,?<br />where {?<br />using the following variational distribution<br />W,S,Φ) = N(Y|Φ(?<br />W,S,Φ} is the whole set of random variables that need to be marginalized out to compute<br />the marginal likelihood. The marginal likelihood is analytically intractable, so we lower bound it<br />W◦S)?,Σ)<br />?<br />q,m<br />?N(? wqm|0,σ2<br />w)πsqm(1 − π)sqm?<br />M<br />?<br />m=1<br />N(φm|µm,Km),<br />q(?<br />W,S,Φ) =<br />Q<br />?<br />q=1<br />M<br />?<br />m=1<br />q(? wqm,sqm)<br />4<br />M<br />?<br />m=1<br />q(φm).<br />(8)</p>  <p>Page 5</p> <p>The stationary conditions of the lower bound result in analytical updates for all factors above. More<br />precisely, q(φm) is an N-dimensional Gaussian distribution and each factor q(? wqm,sqm) leads to<br />algorithm that at the E-step updates the factors in (8) and at the M-step updates hyperparameters<br />{{σq}Q<br />surprise in these updates. The GP hyperparameters θmare strongly dependent on the factor q(φm)<br />of the corresponding GP latent vector, so updating θmby keeping fixed the factor q(φm) exhibits<br />slow convergence. This problem is efficiently resolved by applying a Marginalized Variational step<br />[20] which jointly updates the pair (q(φm),θm). This more advanced update together with all<br />remaining updates of the EM algorithm are discussed in detail in the supplementary material.<br />a marginal q(? wqm) which is a mixture of two Gaussians where one component is q(? wqm|sqm =<br />q=1,σ2<br />0) = N(? wqm|0,σ2<br />w), as shown in the previous section. The optimization proceeds using an EM<br />w,π,{θm}M<br />m=1} where θmparameterize kernel matrix Km. There is, however, one<br />4<br />In this section we compare the proposed variational inference method, in the following called<br />paired mean field (PMF), against the standard mean field (MF) approximation. For simplicity,<br />we consider a single-output linear regression problem where the data are generated according to:<br />y = (? w ◦ s)Tx + ξ. Moreover, to remove the effect of hyperparameter learning from the com-<br />where the expectation is under the true posterior distribution. wtris obtained by running a very<br />long run of Gibbs sampling. PMF and MF provide alternative approximations wPMFand wMF, and<br />absolute errors between these approximations and wtrare used to measure accuracy. Since initial-<br />ization is crucial for variational non-convex algorithms, the accuracy of PMF and MF is averaged<br />over many random initializations of their respective variational distributions.<br />Assessing the accuracy of the approximation<br />parison, (σ2,π,σ2<br />accuracy when approximating the true posterior mean value for the parameter vector wtr= E[? w◦s]<br />w) are fixed to known values. The objective of the comparison is to measure the<br />soft-error soft-boundextreme-error<br />1.880 [0.965, 2.561]<br />0.204 [0.002, 0.454]<br />extreme-bound<br />-895.0 [-618.9,-1483.3]<br />-560.6 [-557.8, -564.0]<br />MF<br />PMF<br />0.917 [0.002,1.930]<br />0.208 [0.002,0.454]<br />-628.9 [-554.6,-793.5]<br />-560.7 [-557.8, -564.1]<br />Table 1: Comparison of MF and PMF in Boston-housing data in terms of approximating the ground-truth.<br />Average errors (?13<br />For the purpose of the comparison we also derived an efficient paired Gibbs sampler that follows<br />exactly the same principle as PMF. This Gibbs sampler iteratively samples the pair (? wm,sm) from<br />given in the supplementary material.<br />We considered the Boston-housing dataset which consists of 456 training examples and 13 inputs.<br />Hyperparameters were fixed to values (σ2= 0.1×var(y),π = 0.25,σ2<br />the variance of the data. We performed two types of experiments each repeated 300 times. Each<br />repetition of the first type uses a soft random initialization of each q(sm= 1) = γmfrom the range<br />(0,1). The second type uses an extreme random initialization so that each γmis initialized to either<br />0 or 1. For each run PMF and MF are initialized to the same variational parameters.<br />Table 1 reports average absolute errors and also average values of the variational lower bounds.<br />Clearly, PMF is more accurate than MF, achieves significantly higher values for the lower bound<br />and exhibits smaller variance under different initializations. Further, for the more difficult case<br />of extreme initializations the performance of MF becomes worse, while the performance of PMF<br />remains unchanged. This shows that optimization in PMF, although non-convex, is very robust to<br />unfavorable initializations. Similar experiments in other datasets have confirmed the above remarks.<br />m=1|wtr<br />m−wappr<br />m |) together with 95% confidence intervals (given by percentiles) are shown<br />for soft and extreme initializations. Average values for the variational lower bound are also shown.<br />the conditional p(? wm,sm|? w\m,s\m,y) and has been observed to mix much faster than the standard<br />Gibbs sampler that samples ? w and s separately. More details about the paired Gibbs sampler are<br />w= 1) where var(y) denotes<br />5<br />Toy multi-output regression dataset. To illustrate the capabilities of the proposed model, we first<br />apply it to a toy multi-output dataset with missing observations. Toy data is generated as follows:<br />Experiments<br />5</p>  <p>Page 6</p> <p>Ten random latent functions are generated by sampling i.i.d. from zero-mean GPs with the following<br />non-stationary covariance function<br />k(xi,xj) = exp<br />?−x2<br />i− x2<br />20<br />j<br />?<br />(4cos(0.5(xi− xj)) + cos(2(xi− xj))),<br />at 201 evenly spaced points in the interval x ∈ [−10,10]. Ten tasks are then generated by adding<br />Gaussiannoisewithstandarddeviation0.2tothoserandomlatentfunctions, andtwoadditionaltasks<br />consist only of Gaussian noise with standard deviations 0.1 and 0.4. Finally, for each of the 12 tasks,<br />we artificially simulate missing data by removing 41 contiguous observations, as shown in Figure<br />1. Missing data are not available to any learning algorithm, and will be used to test performance<br />only. Note that the above covariance function is rank-4, so ten out of the twelve tasks will be related,<br />though we do not know how, or which ones.<br />All tasks are then learned using both independent GPs with squared exponential (SE) covariance<br />function kSE(xi,xj) = exp(−(xi− xj)2/(2?)) and the proposed MTMKL with M = 7 latent<br />functions, each of them also using the SE prior. Hyperparameter ?, as well as noise levels are<br />learned independently for each latent function. Figure 1 shows the inferred posterior means.<br />−10−8−6−4−20246810<br />−2<br />−1.5<br />−1<br />−0.5<br />0<br />0.5<br />1<br />1.5<br />2<br />2.5<br />−10−8−6−4−20246810<br />−2.5<br />−2<br />−1.5<br />−1<br />−0.5<br />0<br />0.5<br />1<br />1.5<br />2<br />−10−8−6−4 −20246810<br />−1.5<br />−1<br />−0.5<br />0<br />0.5<br />1<br />1.5<br />−10−8−6−4−20246810<br />−2<br />−1<br />0<br />1<br />2<br />3<br />4<br />−10 −8−6−4−20246810<br />−4<br />−3<br />−2<br />−1<br />0<br />1<br />2<br />−10−8 −6−4−20246810<br />−3<br />−2<br />−1<br />0<br />1<br />2<br />3<br />−10−8 −6−4−20246810<br />−0.4<br />−0.3<br />−0.2<br />−0.1<br />0<br />0.1<br />0.2<br />0.3<br />−10−8−6−4−20246810<br />−2<br />−1.5<br />−1<br />−0.5<br />0<br />0.5<br />1<br />1.5<br />2<br />2.5<br />−10 −8−6−4−20246810<br />−3<br />−2<br />−1<br />0<br />1<br />2<br />3<br />4<br />5<br />6<br />−10−8−6−4−20246810<br />−1.5<br />−1<br />−0.5<br />0<br />0.5<br />1<br />1.5<br />2<br />2.5<br />−10−8−6−4−20246810<br />−2.5<br />−2<br />−1.5<br />−1<br />−0.5<br />0<br />0.5<br />1<br />1.5<br />2<br />2.5<br />−10−8 −6 −4−20246810<br />−3<br />−2.5<br />−2<br />−1.5<br />−1<br />−0.5<br />0<br />0.5<br />1<br />1.5<br />Figure 1: Twelve related tasks and predictions according to independent GPs (blue, continuous line) and<br />MTMKL (red, dashed line). Missing data for each task is represented using green circles.<br />The mean square error (MSE) between predictions and missing observations for each task are dis-<br />played in Table 2. MTMKL is able to infer how tasks are related and then exploit that information<br />to make much better predictions. After learning, only 4 out of the 7 available latent functions re-<br />main active, while the other ones are pruned by setting the corresponding weights to zero. This is in<br />correspondence with the generating covariance function, which only had 4 eigenfunctions, showing<br />how model order selection is automatic.<br />Method \ Task #<br />Independent GPs<br />MTMKL<br />123456789 101112<br />6.51<br />1.97<br />11.70<br />4.57<br />7.52<br />7.71<br />2.49<br />1.94<br />1.53<br />1.98<br />18.25<br />2.09<br />0.41<br />0.41<br />7.43<br />1.96<br />2.73<br />1.90<br />1.81<br />1.57<br />19.93<br />1.20<br />93.80<br />2.83<br />Table 2: MSE performance of independent GPs vs. MTMKL on the missing observations for each task.<br />6</p>  <p>Page 7</p> <p>Inferred noise standard deviations for the noise-only tasks are 0.10 and 0.45, and the average for the<br />remaining tasks is 0.22, which agrees well with the stated actual values.<br />The flowers dataset. Though the proposed model has been designed as a tool for regression, it<br />can also be used approximately to solve classification problems by using output values to identify<br />class membership. In this section we will apply it to the challenging flower identification problem<br />posed in [21]. There are 2040 instances of flowers for training and 6149 for testing, mainly acquired<br />from the web, with varying scales, resolutions, etc., which are labeled into 102 categories. In [21],<br />four relevant features are identified: Color, histogram of gradient orientations and the scale invariant<br />feature transform, sampled on both the foreground region and its boundary. More information is<br />available at http://www.robots.ox.ac.uk/˜vgg/data/flowers/.<br />For this type of dataset, state of the art performance has been achieved using a weighted linear<br />combination of kernels (one per feature) in a support vector machine (SVM) classifier. A different<br />set of weights is learned for each class. In [22] it is shown that these weights can be learned by<br />solving a convex optimization problem. I.e., the standard approach to tackle the flower classification<br />problem would correspond to solving 102 independent binary classification problems, each using a<br />linear combination of 4 kernels. We take a different approach: Since all the 102 binary classification<br />tasks are related, we learn all of them at once as a multi-task multiple-kernel problem, hoping that<br />knowledge transfer between them will enhance performance.<br />For each training instance, we set the corresponding output to +1 for the desired task, whereas the<br />output for the remaining tasks is set to -1. Then we consider both using 10 and 13 latent functions<br />per feature (i.e., M = 40 and M = 52). We measure performance in terms of the recognition<br />rate (RR), which is the average of break-even points (where precision equals recall) for each class;<br />average area under the curve (AUC); and the multi-class accuracy (MA) which is the rate of correctly<br />classified instances. As baseline, recall that a random classifier would yield a RR and AUC of 0.5<br />and a MA of 1/102 = 0.0098. Results are reported in Table 3.<br />Method<br />MTMKL<br />MTMKL<br />MKL from [21]<br />MKL from [13]<br />Latent function #AUC on test set<br />0.944<br />0.952<br />-<br />0.957<br />RR on test set<br />0.889<br />0.893<br />0.728<br />-<br />MA on test set<br />0.329<br />0.400<br />-<br />-<br />M = 40<br />M = 52<br />M = 408<br />M = 408<br />Table 3: Performance of the different multiple kernel learning algorithms on the flowers dataset.<br />MTMKL significantly outperforms the state-of-the-art method in [21], yielding a performance in<br />line with [13], due to its ability to share information across tasks.<br />Image denoising and dictionary learning. Here we illustrate denoising on the 256 × 256 “house”<br />image used in [19]. Three noise levels (standard deviations 15, 25 and 50) are considered. Follow-<br />ing [19], we partition the noisy image in 62,001 overlapping 8 × 8 blocks and regard each block<br />as a different task. MTMKL is then run using M = 64 “latent blocks”, also known as “dictionary<br />elements” (bigger dictionaries do not result in significant performance increase). For the covariance<br />of the latent functions, we consider two possible choices: Either a white covariance function (as<br />in [19]) or an exponential covariance of the form kEXP(xi,xj) = e−<br />coordinates within each block. The first option is equivalent to placing an independent standard nor-<br />mal prior on each pixel of the dictionary. The second one, on the other hand, introduces correlation<br />between neighboring pixels in the dictionary. Results are shown in Table 4. The exponential co-<br />variance clearly enhances performance and produces a more structured dictionary, as can be seen in<br />Figure 3.(a). The Peak-to-Signal Ratio (PSNR) obtained using the proposed approach is comparable<br />to the state-of-the-art results obtained in [19].<br />Image inpainting and dictionary learning. We now address the inpainting problem in color im-<br />ages. Following [19], we consider a color image in which a random 80% of the RGB components<br />are missing. Using an analogous partitioning scheme as in the previous section we obtain 148,836<br />blocks of size 8×8×3, each of which is regarded as a different task. A dictionary size of M = 100<br />and a white covariance function (which is used in [19]) are selected. Note that we do not apply any<br />other preprocessing to data or any specific initialization as it is done in [19]. The PSNR of the image<br />|xi−xj|<br />?<br />, where x are the pixel<br />7</p>  <p>Page 8</p> <p>Figure 2: Noisy “house” image with σ = 25 and re-<br />stored version using Exponential cov. function.<br />PSNR (dB)<br />Noise stdNoisy image<br />24.66<br />20.22<br />14.20<br />White<br />33.98<br />30.98<br />26.14<br />Expon.<br />34.29<br />31.88<br />28.08<br />σ = 15<br />σ = 25<br />σ = 50<br />Table4: PSNRfornoisyandrestoredimageusing<br />several noise levels and covariance functions.<br />after it is restored using MTMKL is 28.94 dB, see Figure 3.(b). This result is similar to the results<br />reported in [19] and close to the state-of-the-art result of 29.65 dB achieved in [23].<br />(a) House: Dict. for white and Exponential (b) Castle: Missing values, restored and original<br />Figure 3: Dictionaries inferred from noisy (σ = 25) “house” image; and “castle” inpainting results.<br />Collaborative filtering.<br />set that consists of 10 million ratings for 71,567 users and 10,681 films, with ratings ranging<br />{1,0.5,2,...,4.5,5}. We followed the setup in [24] and used the raand rbpartitions provided<br />with the database, that split the data into a training and testing, so that they are 10 ratings per user<br />in the test set. We applied the sparse factor analysis model (i.e. sparse PCA but with heteroscedastic<br />noise for the columns of the observation matrix Y which corresponds to films) with M = 20 latent<br />dimensions. The RMSE for the rapartition was 0.88 for the rbpartition was 0.85 so one average<br />0.865. This result is slightly better than 0.8740 RMSE reported in [24] using GP-LVM.<br />Finally, we performed an experiment on the 10M MovieLens data<br />6<br />In this work we have proposed a spike and slab multi-task and multiple kernel learning model. A<br />novel variational algorithm to perform inference in this model has been derived. The key contri-<br />bution in this regard that explains the good performance of the algorithm is the choice of a joint<br />distribution over ˜ wqmand sqmin the variational posterior, as opposed to the usual independence<br />assumption. This has the effect of using exponentially many modes to approximate the posterior,<br />thus rendering it more accurate and much more robust to poor initializations of the variational pa-<br />rameters. The relevance and wide applicability of the proposed model has been illustrated by using<br />it on very diverse tasks: multi-output regression, multi-class classification, image denoising, image<br />inpainting and collaborative filtering. Prior structure beliefs were introduced in image dictionaries,<br />which is also a novel contribution to the best of our knowledge. Finally an interesting topic for future<br />research is to optimize the variational distribution proposed here with alternative approximate infer-<br />ence frameworks such as belief propagation or expectation propagation. This could allow to extend<br />current methodologies within such frameworks that assume unimodal approximations [25, 26].<br />Discussion<br />Acknowledgments<br />We thank the reviewers for insightful comments.<br />EP/F005687/1 “Gaussian Processes for Systems Identification with Applications in Systems Bi-<br />ology”. MLG gratefully acknowledges funding from CAM project CCG10-UC3M/TIC-5511 and<br />CONSOLIDER-INGENIO 2010 CSD2008-00010 (COMONSENS).<br />MKT was supported by EPSRC Grant No<br />8</p>  <p>Page 9</p> <p>References<br />[1] T.J. Mitchell and J.J. Beauchamp. Bayesian variable selection in linear regression. Journal of the Ameri-<br />can Statistical Association, 83(404):1023–1032, 1988.<br />[2] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society,<br />Series B, 58:267–288, 1994.<br />[3] M.E. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning<br />Research, 1:211–244, 2001.<br />[4] E.I. George and R.E. Mcculloch. Variable selection via Gibbs sampling. Journal of the American Statis-<br />tical Association, 88(423):881–889, 1993.<br />[5] M. West. Bayesian factor regression models in the ”large p, small n” paradigm. In Bayesian Statistics,<br />pages 723–732. Oxford University Press, 2003.<br />[6] B. Efron. Microarrays, empirical Bayes and the two-groups model. Statistical Science, 23:1–22, 2008.<br />[7] C. Archambeau and F. Bach. Sparse probabilistic projections. In D. Koller, D. Schuurmans, Y. Bengio,<br />and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 73–80. 2009.<br />[8] F. Caron and A. Doucet. Sparse Bayesian nonparametric regression. In In 25th International Conference<br />on Machine Learning (ICML). ACM, 2008.<br />[9] Matthias W. Seeger and Hannes Nickisch. Compressed sensing and Bayesian experimental design. In<br />ICML, pages 912–919, 2008.<br />[10] C.M. Carvalho, N.G. Polson, and J.G. Scott. The horseshoe estimator for sparse signals. Biometrika,<br />97:465–480, 2010.<br />[11] T. Damoulas and M.A. Girolami. Probabilistic multi-class multi-kernel learning: on protein fold recogni-<br />tion and remote homology detection. Bioinformatics, 24:1264–1270, 2008.<br />[12] M.Christoudias, R.Urtasun, andT. Darrell. Bayesian localizedmultiple kernel learning. Technical report,<br />EECS Department, University of California, Berkeley, Jul 2009.<br />[13] C. Archambeau and F. Bach. Multiple Gaussian process models. In NIPS 23 workshop on New Directions<br />in Multiple Kernel Learning. 2010.<br />[14] Y.W. Teh, M. Seeger, and M.I. Jordan. Semiparametric latent factor models. In Proceedings of the<br />International Workshop on Artificial Intelligence and Statistics, volume 10, 2005.<br />[15] E.V. Bonilla, K.M.A. Chai, and C.K.I. Williams. Multi-task Gaussian process prediction. In Advances<br />Neural Information Processing Systems 20, 2008.<br />[16] P Boyle and M. Frean. Dependent Gaussian processes. In Advances in Neural Information Processing<br />Systems 17, pages 217–224. MIT Press, 2005.<br />[17] M. Alvarez and N.D. Lawrence. Sparse convolved Gaussian processes for multi-output regression. In<br />Advances in Neural Information Processing Systems 20, pages 57–64, 2008.<br />[18] R. Yoshida and M. West. Bayesian learning in sparse graphical factor models via variational mean-field<br />annealing. Journal of Machine Learning Research, 11:1771–1798, 2010.<br />[19] M. Zhou, H. Chen, J. Paisley, L. Ren, G. Sapiro, and L. Carin. Non-parametric Bayesian dictionary<br />learning for sparse image represent ations. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams,<br />and A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 2295–2303. 2009.<br />[20] M. L´ azaro-Gredilla and M. Titsias. Variational heteroscedastic Gaussian process regression. In 28th<br />International Conference on Machine Learning (ICML-11), pages 841–848, New York, NY, USA, June<br />2011. ACM.<br />[21] M.E. Nilsback and A. Zisserman. Automated flower classification over a large number of classes. In<br />Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing, Dec 2008.<br />[22] M. Varma and D. Ray. Learning the discriminative power invariance trade-off. In International Confer-<br />ence on Computer Vision. 2007.<br />[23] J. Mairal, M. Elad, and G. Sapiro. Sparse representation for color image restoration. IEEE Trans. Image<br />Processing, 17, 2008.<br />[24] N.D. Lawrence and R. Urtasun. Non-linear matrix factorization with Gaussian processes. In Proceedings<br />of the 26th Annual International Conference on Machine Learning, pages 601–608, 2009.<br />[25] K. Sharp and M. Rattray. Dense message passing for sparse principal component analysis. In 13th<br />International Conference on Artificial Intelligence and Statistics (AISTATS), pages 725–732, 2010.<br />[26] J.M. Hern´ andez-Lobato, D. Hern´ andez-Lobato, and A. Su´ arez. Network-based sparse Bayesian classifi-<br />cation. Pattern Recognition, 44(4):886–900, 2011.<br />9</p>   </div> <div id="rgw15_56ab1ccce0ea8" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw16_56ab1ccce0ea8">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw17_56ab1ccce0ea8"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://eprints.pascal-network.org/archive/00009091/01/vmtmkl_nips.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning">Spike and Slab Variational Inference for Multi-Tas...</a> </div>  <div class="details">   Available from <a href="http://eprints.pascal-network.org/archive/00009091/01/vmtmkl_nips.pdf" target="_blank" rel="nofollow">eprints.pascal-network.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw24_56ab1ccce0ea8" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (15) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw25_56ab1ccce0ea8" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw26_56ab1ccce0ea8" >  <div class="indent-left">  <div id="rgw27_56ab1ccce0ea8" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/268504229_Multi-Task_Bayesian_Compressive_Sensing_Exploiting_Intra-Task_Dependency">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Yimin_Zhang3" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Yimin D. Zhang </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw28_56ab1ccce0ea8">  <li class="citation-context-item"> "However, and are strongly correlated since their product interacts with the data. The following paired spike-andslab prior is introduced [20]: " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/268504229_Multi-Task_Bayesian_Compressive_Sensing_Exploiting_Intra-Task_Dependency"> <span class="publication-title js-publication-title">Multi-Task Bayesian Compressive Sensing Exploiting Intra-Task Dependency</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2058288188_Qisong_Wu" class="authors js-author-name ga-publications-authors">Qisong Wu</a> &middot;     <a href="researcher/7767377_Yimin_D_Zhang" class="authors js-author-name ga-publications-authors">Yimin D Zhang</a> &middot;     <a href="researcher/7767376_Moeness_G_Amin" class="authors js-author-name ga-publications-authors">Moeness G Amin</a> &middot;     <a href="researcher/7730065_Braham_Himed" class="authors js-author-name ga-publications-authors">Braham Himed</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> In this letter, we propose a multi-task compressive sensing algorithm for the reconstruction of clustered sparse en-tries based on hierarchical Bayesian framework. By extending a paired spike-and-slab prior to a general multi-task model, the proposed algorithm has the capability of modeling both inter-task and intra-task dependencies of the observation data. The latter is achieved by imposing a clustered prior on non-zero entries and finds applications in radar where targets exhibit spatial extent. Simulation results verify that the proposed algorithm outperforms state-of-the-art group sparse Bayesian learning algorithms. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Apr 2015  &middot; IEEE Signal Processing Letters  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Yimin_Zhang3/publication/268504229_Multi-Task_Bayesian_Compressive_Sensing_Exploiting_Intra-Task_Dependency/links/546cb7740cf284dbf190eb34.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw29_56ab1ccce0ea8" >  <div class="indent-left">  <div id="rgw30_56ab1ccce0ea8" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/272521367_ICR_Iterative_Convex_Refinement_for_Sparse_Signal_Recovery_Using_Spike_and_Slab_Priors">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1502.04726" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: de.arxiv.org </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw31_56ab1ccce0ea8">  <li class="citation-context-item"> "Optimization Problem (Hierarchical Bayesian Framework): Any inference from the posterior density for this model will be ill-defined because the Dirac&#39;s delta function is unbounded. Some ways to handle this issue include approximations [35], such as approximation of spike term with a narrow Gaussian [36], approximating the whole posterior function with product of Gaussian(s) and Bernoulli(s) density functions [16], [37]– [40], etc. In this work, we focus on the setup of Yen et al. [29] which is an approximate spike and slab prior for inducing sparsity on x x x. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/272521367_ICR_Iterative_Convex_Refinement_for_Sparse_Signal_Recovery_Using_Spike_and_Slab_Priors"> <span class="publication-title js-publication-title">ICR: Iterative Convex Refinement for Sparse Signal Recovery Using Spike and Slab Priors</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2047544144_Hojjat_S_Mousavi" class="authors js-author-name ga-publications-authors">Hojjat S. Mousavi</a> &middot;     <a href="researcher/10074977_Vishal_Monga" class="authors js-author-name ga-publications-authors">Vishal Monga</a> &middot;     <a href="researcher/2050299069_Trac_D_Tran" class="authors js-author-name ga-publications-authors">Trac D. Tran</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> In this letter, we address sparse signal recovery using spike and slab
priors. In particular, we focus on a Bayesian framework where sparsity is
enforced on reconstruction coefficients via probabilistic priors. The
optimization resulting from spike and slab prior maximization is known to be a
hard non-convex problem, and existing solutions involve simplifying assumptions
and/or relaxations. We propose an approach called Iterative Convex Refinement
(ICR) that aims to solve the aforementioned optimization problem directly
allowing for greater generality in the sparse structure. Essentially, ICR
solves a sequence of convex optimization problems such that sequence of
solutions converges to a sub-optimal solution of the original hard optimization
problem. We propose two versions of our algorithm: a.) an unconstrained
version, and b.) with a non-negativity constraint on sparse coefficients, which
may be required in some real-world problems. Experimental validation is
performed on both synthetic data and for a real-world image recovery problem,
which illustrates merits of ICR over state of the art alternatives. </span> </div>    <div class="publication-meta publication-meta">  <span class="ico-publication-preview reset-background"></span> Preview    &middot; Article &middot; Feb 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw32_56ab1ccce0ea8" >  <div class="indent-left">  <div id="rgw33_56ab1ccce0ea8" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/271710414_Multi-task_Image_Classification_via_Collaborative_Hierarchical_Spike-and-Slab_Priors">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Minh_Dao7" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Minh Dao </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw34_56ab1ccce0ea8">  <li class="citation-context-item"> "well-suited structured prior for capturing sparsity [25] [26] [27]. δ 0 is a point mass concentrated at zero (known as &quot; spike &quot; ) and the other term is the distribution of nonzero coefficients of sparse vector also known as &quot; slab &quot; . " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/271710414_Multi-task_Image_Classification_via_Collaborative_Hierarchical_Spike-and-Slab_Priors"> <span class="publication-title js-publication-title">Multi-task Image Classification via Collaborative, Hierarchical Spike-and-Slab Priors</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2047544144_Hojjat_Seyed_Mousavi" class="authors js-author-name ga-publications-authors">Hojjat Seyed Mousavi</a> &middot;     <a href="researcher/59363400_Umamahesh_Srinivas" class="authors js-author-name ga-publications-authors">Umamahesh Srinivas</a> &middot;     <a href="researcher/10074977_Vishal_Monga" class="authors js-author-name ga-publications-authors">Vishal Monga</a> &middot;     <a href="researcher/2044831369_Yuanming_Suo" class="authors js-author-name ga-publications-authors">Yuanming Suo</a> &middot;     <a href="researcher/70534368_Minh_Dao" class="authors js-author-name ga-publications-authors">Minh Dao</a> &middot;     <a href="researcher/2050299069_Trac_D_Tran" class="authors js-author-name ga-publications-authors">Trac. D. Tran</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Promising results have been achieved in image classification problems by
exploiting the discriminative power of sparse representations for
classification (SRC). Recently, it has been shown that the use of
\emph{class-specific} spike-and-slab priors in conjunction with the
class-specific dictionaries from SRC is particularly effective in low training
scenarios. As a logical extension, we build on this framework for multitask
scenarios, wherein multiple representations of the same physical phenomena are
available. We experimentally demonstrate the benefits of mining joint
information from different camera views for multi-view face recognition. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Jan 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Minh_Dao7/publication/271710414_Multi-task_Image_Classification_via_Collaborative_Hierarchical_Spike-and-Slab_Priors/links/5538cbfe0cf226723ab63f23.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw19_56ab1ccce0ea8" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw20_56ab1ccce0ea8">  </ul> </div> </div>   <div id="rgw11_56ab1ccce0ea8" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw12_56ab1ccce0ea8"> <div> <h5> <a href="publication/290789307_Vision-based_structural_displacement_measurement_system_performance_evaluation_and_influence_factor_analysis" class="color-inherit ga-similar-publication-title"><span class="publication-title">Vision-based structural displacement measurement: system performance evaluation and influence factor analysis</span></a>  </h5>  <div class="authors"> <a href="researcher/2011747419_XW_Ye" class="authors ga-similar-publication-author">X.W. Ye</a>, <a href="researcher/2074920454_Ting-Hua_Yi" class="authors ga-similar-publication-author">Ting-Hua Yi</a>, <a href="researcher/2074895744_CZ_Dong" class="authors ga-similar-publication-author">C.Z. Dong</a>, <a href="researcher/2095126194_T_Liu" class="authors ga-similar-publication-author">T. Liu</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw13_56ab1ccce0ea8"> <div> <h5> <a href="publication/289524660_The_Changes_of_Ethical_Dilemmas_in_Palliative_Care_A_Lesson_Learned_from_Comparison_Between_1998_and_2013_in_Taiwan" class="color-inherit ga-similar-publication-title"><span class="publication-title">The Changes of Ethical Dilemmas in Palliative Care A Lesson Learned from Comparison Between 1998 and 2013 in Taiwan</span></a>  </h5>  <div class="authors"> <a href="researcher/2036172803_An-Hsuan_Chih" class="authors ga-similar-publication-author">An-Hsuan Chih</a>, <a href="researcher/2092696038_Peijen_Su" class="authors ga-similar-publication-author">Peijen Su</a>, <a href="researcher/39827376_Wen-Yu_Hu" class="authors ga-similar-publication-author">Wen-Yu Hu</a>, <a href="researcher/38688645_Chien-An_Yao" class="authors ga-similar-publication-author">Chien-An Yao</a>, <a href="researcher/39333238_Shao-Yi_Cheng" class="authors ga-similar-publication-author">Shao-Yi Cheng</a>, <a href="researcher/2072633108_Yen-Chun_Lin" class="authors ga-similar-publication-author">Yen-Chun Lin</a>, <a href="researcher/38812347_Tai-Yuan_Chiu" class="authors ga-similar-publication-author">Tai-Yuan Chiu</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw14_56ab1ccce0ea8"> <div> <h5> <a href="publication/284921012_Higher_order_density_approximations_for_solutions_to_estimating_equations" class="color-inherit ga-similar-publication-title"><span class="publication-title">Higher order density approximations for solutions to estimating equations</span></a>  </h5>  <div class="authors"> <a href="researcher/2086176704_A_Almudevar" class="authors ga-similar-publication-author">A. Almudevar</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw36_56ab1ccce0ea8" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw37_56ab1ccce0ea8">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw38_56ab1ccce0ea8" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=-PUTNMvC22S5Mi_NwneCdGzcfe98k7cCQwnFDV9sWTaHZq1C0v3KJrcDJ9V7kCdT" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="Z7bWoH46aUgCCAMIVNqMxCpQV2cJqDXqb7dKThDLnMVVAXFKjH+dzwx9k9lx8kqsqdm/VTpfeS43YKVSt5h88l2a6WJeBDO2PwVa+/WWgGUgwM7j2nPOOjNhw3S2QtUMKUwoN7PT+UCAjP9HE8kms6tfIHS3SoGqiB8JMKgWBQnwkXiMhWKnKc66fjeCZKT4nt2w0KRJ4934qidaCil1JhAjkjrbGlYy537j5wLUh1iijxZ2I19d3G4CkV0Gyggrpik9L548W1I0GMyAbRUgJsWKR6MtN2ESYZoMm4vaDWk="/> <input type="hidden" name="urlAfterLogin" value="publication/267805753_Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjY3ODA1NzUzX1NwaWtlX2FuZF9TbGFiX1ZhcmlhdGlvbmFsX0luZmVyZW5jZV9mb3JfTXVsdGktVGFza19hbmRfTXVsdGlwbGVfS2VybmVsX0xlYXJuaW5n"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjY3ODA1NzUzX1NwaWtlX2FuZF9TbGFiX1ZhcmlhdGlvbmFsX0luZmVyZW5jZV9mb3JfTXVsdGktVGFza19hbmRfTXVsdGlwbGVfS2VybmVsX0xlYXJuaW5n"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjY3ODA1NzUzX1NwaWtlX2FuZF9TbGFiX1ZhcmlhdGlvbmFsX0luZmVyZW5jZV9mb3JfTXVsdGktVGFza19hbmRfTXVsdGlwbGVfS2VybmVsX0xlYXJuaW5n"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw39_56ab1ccce0ea8"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 496;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Michalis Titsias","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A279280278884356%401443597015332_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Michalis_Titsias2","institution":"Athens University of Economics and Business","institutionUrl":false,"widgetId":"rgw4_56ab1ccce0ea8"},"id":"rgw4_56ab1ccce0ea8","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=6761812","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab1ccce0ea8"},"id":"rgw3_56ab1ccce0ea8","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=267805753","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":267805753,"title":"Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"","publicationDateRobot":false,"article":""}},"source":false,"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning"},{"key":"rft.au","value":"Michalis K Titsias"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56ab1ccce0ea8"},"id":"rgw6_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=267805753","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":267805753,"peopleItems":[{"data":{"authorNameOnPublication":"Michalis Titsias","accountUrl":"profile\/Michalis_Titsias2","accountKey":"Michalis_Titsias2","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A279280278884356%401443597015332_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Michalis Titsias","profile":{"professionalInstitution":{"professionalInstitutionName":"Athens University of Economics and Business","professionalInstitutionUrl":"institution\/Athens_University_of_Economics_and_Business"}},"professionalInstitutionName":"Athens University of Economics and Business","professionalInstitutionUrl":"institution\/Athens_University_of_Economics_and_Business","url":"profile\/Michalis_Titsias2","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A279280278884356%401443597015332_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Michalis_Titsias2","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw9_56ab1ccce0ea8"},"id":"rgw9_56ab1ccce0ea8","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=6761812&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Athens University of Economics and Business","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":1,"accountCount":1,"publicationUid":267805753,"widgetId":"rgw8_56ab1ccce0ea8"},"id":"rgw8_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=6761812&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=1&accountCount=1&publicationUid=267805753","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab1ccce0ea8"},"id":"rgw7_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=267805753&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":267805753,"abstract":"<noscript><\/noscript><div>We introduce a variational Bayesian inference algorithm which can be widely applied to sparse linear models. The algorithm is based on the spike and slab prior which, from a Bayesian perspective, is the golden standard for sparse inference. We apply the method to a general multi-task and multiple kernel learning model in which a common set of Gaussian process functions is linearly combined with task-specific sparse weights, thus inducing relation between tasks. This model unifies several sparse linear models, such as generalized linear models, sparse factor analysis and matrix factorization with missing values, so that the variational algorithm can be applied to all these cases. We demonstrate our approach in multi-output Gaussian process regression, multi-class classification, image processing applications and collaborative filtering.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw10_56ab1ccce0ea8"},"id":"rgw10_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=267805753","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/267805753_Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning\/links\/545c772c0cf2f1dbcbcb28eb\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw5_56ab1ccce0ea8"},"id":"rgw5_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=267805753&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2011747419,"url":"researcher\/2011747419_XW_Ye","fullname":"X.W. Ye","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2074920454,"url":"researcher\/2074920454_Ting-Hua_Yi","fullname":"Ting-Hua Yi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2074895744,"url":"researcher\/2074895744_CZ_Dong","fullname":"C.Z. Dong","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095126194,"url":"researcher\/2095126194_T_Liu","fullname":"T. Liu","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/290789307_Vision-based_structural_displacement_measurement_system_performance_evaluation_and_influence_factor_analysis","usePlainButton":true,"publicationUid":290789307,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/290789307_Vision-based_structural_displacement_measurement_system_performance_evaluation_and_influence_factor_analysis","title":"Vision-based structural displacement measurement: system performance evaluation and influence factor analysis","displayTitleAsLink":true,"authors":[{"id":2011747419,"url":"researcher\/2011747419_XW_Ye","fullname":"X.W. Ye","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2074920454,"url":"researcher\/2074920454_Ting-Hua_Yi","fullname":"Ting-Hua Yi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2074895744,"url":"researcher\/2074895744_CZ_Dong","fullname":"C.Z. Dong","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095126194,"url":"researcher\/2095126194_T_Liu","fullname":"T. Liu","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/290789307_Vision-based_structural_displacement_measurement_system_performance_evaluation_and_influence_factor_analysis","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/290789307_Vision-based_structural_displacement_measurement_system_performance_evaluation_and_influence_factor_analysis\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw12_56ab1ccce0ea8"},"id":"rgw12_56ab1ccce0ea8","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=290789307","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2036172803,"url":"researcher\/2036172803_An-Hsuan_Chih","fullname":"An-Hsuan Chih","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2092696038,"url":"researcher\/2092696038_Peijen_Su","fullname":"Peijen Su","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39827376,"url":"researcher\/39827376_Wen-Yu_Hu","fullname":"Wen-Yu Hu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":38688645,"url":"researcher\/38688645_Chien-An_Yao","fullname":"Chien-An Yao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":3,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Medicine","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/289524660_The_Changes_of_Ethical_Dilemmas_in_Palliative_Care_A_Lesson_Learned_from_Comparison_Between_1998_and_2013_in_Taiwan","usePlainButton":true,"publicationUid":289524660,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"5.72","url":"publication\/289524660_The_Changes_of_Ethical_Dilemmas_in_Palliative_Care_A_Lesson_Learned_from_Comparison_Between_1998_and_2013_in_Taiwan","title":"The Changes of Ethical Dilemmas in Palliative Care A Lesson Learned from Comparison Between 1998 and 2013 in Taiwan","displayTitleAsLink":true,"authors":[{"id":2036172803,"url":"researcher\/2036172803_An-Hsuan_Chih","fullname":"An-Hsuan Chih","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2092696038,"url":"researcher\/2092696038_Peijen_Su","fullname":"Peijen Su","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39827376,"url":"researcher\/39827376_Wen-Yu_Hu","fullname":"Wen-Yu Hu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38688645,"url":"researcher\/38688645_Chien-An_Yao","fullname":"Chien-An Yao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39333238,"url":"researcher\/39333238_Shao-Yi_Cheng","fullname":"Shao-Yi Cheng","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2072633108,"url":"researcher\/2072633108_Yen-Chun_Lin","fullname":"Yen-Chun Lin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38812347,"url":"researcher\/38812347_Tai-Yuan_Chiu","fullname":"Tai-Yuan Chiu","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Medicine 01\/2016; 95(1):e2323. DOI:10.1097\/MD.0000000000002323"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/289524660_The_Changes_of_Ethical_Dilemmas_in_Palliative_Care_A_Lesson_Learned_from_Comparison_Between_1998_and_2013_in_Taiwan","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/289524660_The_Changes_of_Ethical_Dilemmas_in_Palliative_Care_A_Lesson_Learned_from_Comparison_Between_1998_and_2013_in_Taiwan\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw13_56ab1ccce0ea8"},"id":"rgw13_56ab1ccce0ea8","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=289524660","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2086176704,"url":"researcher\/2086176704_A_Almudevar","fullname":"A. Almudevar","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/284921012_Higher_order_density_approximations_for_solutions_to_estimating_equations","usePlainButton":true,"publicationUid":284921012,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/284921012_Higher_order_density_approximations_for_solutions_to_estimating_equations","title":"Higher order density approximations for solutions to estimating equations","displayTitleAsLink":true,"authors":[{"id":2086176704,"url":"researcher\/2086176704_A_Almudevar","fullname":"A. Almudevar","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/284921012_Higher_order_density_approximations_for_solutions_to_estimating_equations","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/284921012_Higher_order_density_approximations_for_solutions_to_estimating_equations\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw14_56ab1ccce0ea8"},"id":"rgw14_56ab1ccce0ea8","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=284921012","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw11_56ab1ccce0ea8"},"id":"rgw11_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=267805753&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":267805753,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":267805753,"publicationType":"article","linkId":"545c772c0cf2f1dbcbcb28eb","fileName":"Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning","fileUrl":"http:\/\/eprints.pascal-network.org\/archive\/00009091\/01\/vmtmkl_nips.pdf","name":"eprints.pascal-network.org","nameUrl":"http:\/\/eprints.pascal-network.org\/archive\/00009091\/01\/vmtmkl_nips.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw17_56ab1ccce0ea8"},"id":"rgw17_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=267805753&linkId=545c772c0cf2f1dbcbcb28eb&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw16_56ab1ccce0ea8"},"id":"rgw16_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=267805753&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":1,"valueFormatted":"1","widgetId":"rgw18_56ab1ccce0ea8"},"id":"rgw18_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=267805753","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw15_56ab1ccce0ea8"},"id":"rgw15_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=267805753&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":267805753,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw20_56ab1ccce0ea8"},"id":"rgw20_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=267805753&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":1,"valueFormatted":"1","widgetId":"rgw21_56ab1ccce0ea8"},"id":"rgw21_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=267805753","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw19_56ab1ccce0ea8"},"id":"rgw19_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=267805753&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Spike and Slab Variational Inference for Multi-Task\nand Multiple Kernel Learning\nMichalis K. Titsias\nUniversity of Manchester\nmtitsias@gmail.com\nMiguel L\u00b4 azaro-Gredilla\nUniv. de Cantabria & Univ. Carlos III de Madrid\nmiguel@tsc.uc3m.es\nAbstract\nWe introduce a variational Bayesian inference algorithm which can be widely\napplied to sparse linear models. The algorithm is based on the spike and slab prior\nwhich, from a Bayesian perspective, is the golden standard for sparse inference.\nWe apply the method to a general multi-task and multiple kernel learning model\nin which a common set of Gaussian process functions is linearly combined with\ntask-specific sparse weights, thus inducing relation between tasks. This model\nunifies several sparse linear models, such as generalized linear models, sparse\nfactor analysis and matrix factorization with missing values, so that the variational\nalgorithm can be applied to all these cases. We demonstrate our approach in multi-\noutput Gaussian process regression, multi-class classification, image processing\napplications and collaborative filtering.\n1 Introduction\nSparse inference has found numerous applications in statistics and machine learning [1, 2, 3]. It is a\ngeneric idea that can be combined with popular models, such as linear regression, factor analysis and\nmore recently multi-task and multiple kernel learning models. In the regularization theory literature\nsparse inference is tackled via ?1regularization [2], which requires expensive cross-validation for\nmodel selection. From a Bayesian perspective, the spike and slab prior [1, 4, 5], also called two-\ngroups prior [6], is the golden standard for sparse linear models. However, the discrete nature of\nthe prior makes Bayesian inference a very challenging problem. Specifically, for M linear weights,\ninference under a spike and slab prior distribution on those weights requires a combinatorial search\nover 2Mpossible models. The problems found when working with the spike and slab prior led\nseveral researchers to consider soft-sparse or shrinkage priors such as the Laplace and other related\nscale mixtures of normals [3, 7, 8, 9, 10]. However, such priors are not ideal since they assign zero\nprobability mass to events associated with weights having zero value.\nIn this paper, we introduce a simple and efficient variational inference algorithm based on the spike\nand slab prior which can be widely applied to sparse linear models. The novel characteristic of this\nalgorithm is that the variational distribution over sparse weights has a factorial nature, i.e., it can be\nwritten as a mixture of 2Mcomponents where M is the number of weights. Unlike the standard\nmean field approximation which uses a unimodal variational distribution, our variational algorithm\ncan more precisely match the combinational nature of the posterior distribution over the weights.\nWe will show that the proposed variational approach is more accurate and robust to unfavorable\ninitializations than the standard mean field variational approximation.\nWe apply the variational method to a general multi-task and multiple kernel learning model that\nexpresses the correlation between tasks by letting them share a common set of Gaussian process\nlatent functions. Each task is modeled by linearly combining these latent functions with task-\nspecific weights which are given a spike and slab prior distribution. This model is a spike and\nslab Bayesian reformulation of previous Gaussian process-based single-task multiple kernel learning\n1"},{"page":2,"text":"methods [11, 12, 13] and multi-task Gaussian processes (GPs) [14, 15, 16, 17]. Further, this model\nunifies several sparse linear models, such as generalized linear models, factor analysis, probabilistic\nPCA and matrix factorization with missing values. In the experiments, we apply the variational in-\nference algorithms to all the above models and present results in multi-output regression, multi-class\nclassification, image denoising, image inpainting and collaborative filtering.\n2 Spike and slab multi-task and multiple kernel learning\nSection 2.1 discusses the spike and slab multi-task and multiple kernel learning (MTMKL) model\nthat linearly combines Gaussian process latent functions. Spike and slab factor analysis and proba-\nbilistic PCA is discussed in Section 2.2, while missing values are dealt with in Section 2.3.\n2.1The model\nLet D = {X,Y}, with X \u2208 RN\u00d7Dand Y \u2208 RN\u00d7Q, be a dataset such that the n-th row of X is\nan input vector xnand the n-th row of Y is the set of Q corresponding tasks or outputs. We use\nyqto refer to the q-th column of Y and ynqto the (n,q) entry. Outputs Y are then assumed to be\ngenerated according to the following hierarchical Bayesian model:\nynq\u223c N(ynq|fq(xn),\u03c32\nM\n?\nwqm\u223c \u03c0N(wqm|0,\u03c32\n\u03c6m(x) \u223c GP(\u00b5m(x),km(xi,xj)),\nHere, each \u00b5m(x) is a mean function, km(xi,xj) a covariance function, wq= [wq1,...,wqM]?,\n\u03c6(x) = [\u03c61(x),...,\u03c6M(x)]?and \u03b40(wqm) denotes the Dirac delta function centered at zero. Since\neach of the Q tasks is a linear combination of the same set of latent functions {\u03c6m(x)}M\ntypically M < Q ), correlation is induced in the outputs. Sharing a common set of features means\nthat \u201cknowledge transfer\u201d between tasks can occur and latent functions are inferred more accurately,\nsince data belonging to all tasks are used.\nSeveral linear models can be expressed as special cases of the above. For instance, a generalized\nlinear model is obtained when the GPs are Dirac delta measures (with zero covariance functions)\nthat deterministically assign each \u03c6m(x) to its mean function \u00b5m(x). However, the model in (1) has\na number of additional features not present in standard linear models. Firstly, the basis functions are\nno longer deterministic, but they are instead drawn from different GPs, so an extra layer of flexibility\nis added to the model. Thus, a posterior distribution over the basis functions of the generalized linear\nmodel can be inferred from data. Secondly, a truly sparse prior, the spike and slab prior (1c), is\nplaced over the weights of the model. Specifically, with probability 1\u2212\u03c0, each wqmis zero, and with\nprobability \u03c0, it is drawn from a Gaussian. This contrasts with previous approaches [3, 7, 8, 9, 13]\nin which soft-sparse priors that assign zero probability mass to the weights being exactly zero were\nused. Hyperparameters \u03c0 and \u03c32\nthe discrepancy of nonzero weights, respectively. Thirdly, the number of basis functions M can be\ninferred from data, since the sparse prior on the weights allows basis functions to be \u201cswitched off\u201d\nas necessary by setting the corresponding weights to zero.\nFurther, the model in (1) can be considered as a spike and slab Bayesian reformulation of multi-\ntask [14, 15] and multiple kernel learning previous methods [11, 12] that learn the weights using\nmaximum likelihood. By assuming the weights wqare given, each output function yq(x) is a GP\nwith covariance function\nq),\n\u2200n,q\n(1a)\nfq(x) =\nm=1\nwqm\u03c6m(x) = w?\nq\u03c6(x),\n\u2200q\n(1b)\nw) + (1 \u2212 \u03c0)\u03b40(wqm),\n\u2200q,m\n\u2200m.\n(1c)\n(1d)\nm=1(where\nware learnable in order to determine the amount of sparsity and\nCov[(yq(xi),yq(xj)] =\nM\n?\nm=1\nw2\nqmkm(xi,xj),\nwhich clearly consists of a conic combination of kernel functions. Therefore, the proposed model\ncan be reinterpreted as multiple kernel learning in which the weights of each kernel are assigned\nspike and slab priors in a full Bayesian formulation.\n2"},{"page":3,"text":"2.2\nAn interesting case arises when \u00b5m(x) = 0 and km(xi,xj) = \u03b4ij\u2200m, where \u03b4ijis the Kronecker\ndelta. This says that each latent function is drawn from a white process so that it consists of indepen-\ndent values each following the standard normal distribution. We first define matrices \u03a6 \u2208 RN\u00d7M\nand W \u2208 RQ\u00d7M, whose elements are, respectively, \u03c6nm= \u03c6m(xn) and wqm. Then, the model in\n(1) reduces to\nY = \u03a6W?+ \u03be,\nwqm\u223c \u03c0N(wqm|0,\u03c32\n\u03c6nm\u223c N(\u03c6nm|0,1),\n\u03benq\u223c N(\u03benq|0,\u03c32\nwhere \u03be is an N \u00d7 Q noise matrix with entries \u03benq. The resulting model thus corresponds to sparse\nfactor analysis or sparse probabilistic PCA (when the noise is homoscedastic, i.e., \u03c32\nall q). Observe that the sparse spike and slab prior is placed on the factor loadings W.\nSparse factor and principal component analysis\n(2a)\n(2b)\n(2c)\n(2d)\nw) + (1 \u2212 \u03c0)\u03b40(wqm),\n\u2200q,m\n\u2200n,m\n\u2200n,q,\nq),\nqis constant for\n2.3\nThe method can easily handle missing values and thus be applied to problems involving matrix\ncompletion and collaborative filtering. More precisely, in the presence of missing values we have\na binary matrix Z \u2208 RN\u00d7Qthat indicates the observed elements in Y. Using Z the likelihood in\n(1a) is modified according to ynq\u223c N(ynq|fq(xn),\u03c32\nconsider missing values in applications such as image inpainting and collaborative filtering.\nMissing values\nq), \u2200n,qs.t. [Z]nq= 1. In the experiments we\n3Efficient variational inference\nThe presence of the Dirac delta mass function makes the application of variational approximate\ninference algorithms in spike and slab Bayesian models troublesome. However, there exists a sim-\nple reparameterization of the spike and slab prior that is more amenable to approximate inference\nmethods. Specifically, assume a Gaussian random variable ? wqm\u223c N(? wqm|0,\u03c32\nwqm= sqm? wqmand assign the above prior distributions on sqmand ? wqm. Thus, the reparameter-\np(? wqm,sqm) = N(wqm|0,\u03c32\nsqm? wqm. After the above reparameterization, a standard mean field variational method that uses the\nq=1q(? wq,sq), where\nq(? wq,sq) = q(? wq)q(sq) = N(? wq|\u00b5wq,\u03a3wq)\nand where (\u00b5wq,\u03a3wq,\u03b3q) are variational parameters. Such an approach has extensively used in [18]\nand also considered in [19]. However, the above variational distribution leads to a very inefficient\napproximation. This is because (4) is a unimodal distribution, and therefore has limited capacity\nwhen approximating the factorial true posterior distribution which can have exponentially many\nmodes. To analyze the nature of the true posterior distribution, we consider the following two\nproperties derived by assuming for simplicity a single output (Q = 1) so index q is dropped.\nProperty 1: The true marginal posterior p(? w|Y) can be written as a mixture distribution having 2M\nThe second property characterizes the nature of each conditional p(? w|s,Y) in the above sum.\ns1 denotes the elements in s with value one and s0 the elements with value zero. Using the\nw) and a Bernoulli\nrandom variable sqm \u223c \u03c0sqm(1 \u2212 \u03c0)1\u2212sqm. The product sqm? wqmforms a new random variable\nized spike and slab prior takes the form:\nw)\u03c0sqm(1 \u2212 \u03c0)1\u2212sqm,\nNotice that the presence of wqmin the likelihood function in (1a) is now replaced by the product\nthat follows the probability distribution in eq. (1c). This allows to reparameterize wqmaccording to\n\u2200q,m.\n(3)\nfactorized variational distribution over?\nW = {? wq}Q\nq=1and S = {sq}Q\nq=1takes the form q(?\nW,S) =\n?Q\nM\n?\nm=1\n\u03b3sqm\nqm(1 \u2212 \u03b3qm)1\u2212sqm\n(4)\ncomponents. This is an obvious fact since p(? w|Y) =?\nsp(? w|s,Y)p(s|Y), where the summation\ninvolves all 2Mpossible values of the binary vector s.\nProperty 2: Assume the conditional distribution p(? w|s,Y). We can write s = s1\u222a s0, where\n3"},{"page":4,"text":"correspondence between s and ? w, we have ? w = ? w1\u222a ? w0.\nas an elementwise product ? w0\u25e6 s0, thus when s0= 0, ? w0becomes disconnected from the data.\np(? w|Y), which is a mixture with 2Mcomponents, with a single Gaussian distribution. Next we\n3.1The proposed variational method\nThen, p(? w|s,Y) factorizes as\np(? w|s,Y) = p(? w1|Y)N(? w0|0,\u03c32\nThe standard variational distribution in (4) ignores these properties and approximates the marginal\nwI|? w0|), which says that the posterior over ? w0given s0 = 0\nis equal to the prior over ? w0. This property is obvious because ? w0and s0appear in the likelihood\npresent an alternative variational approximation that takes into account the above properties.\nIn the reparameterized spike and slab prior, each pair of variables {? wqm,sqm} is strongly correlated\nof the variational distribution. The simplest factorization that achieves this is:\nsince their product is the underlying variable that interacts with the data. Thus, a sensible approxi-\nmation must treat each pair {? wqm,sqm} as a unit so that {? wqm,sqm} are placed in the same factor\nq(? wq,sq) =\nThisvariationaldistributionyieldsamarginalq(? wq)whichhas2Mcomponents. Thiscanbeseenby\na mixture of 2Mcomponents is obtained. Therefore, Property 1 is satisfied by (5). In turns out\nthat Property 2 is also satisfied. This can be shown by taking the stationary condition for the factor\nq(? wqm,sqm) when maximizing the variational lower bound (on the true marginal likelihood):\nq(? wqm,sqm)q(\u0398)\ntheir variational distribution. The stationary condition for q(? wqm,sqm) is\nZe?log p(Y,? wqm,sqm,\u0398)?q(\u0398)N(? wqm|0,\u03c32\nhave q(? wqm|sqm = 0) \u221d q(? wqm,sqm = 0) =\nwe obtain q(? wqm|sqm= 0) = N(? wqm|0,\u03c32\nslab probability models as long as the weights ? w and binary variables s interact inside the likelihood\n3.2Application to the multi-task and multiple kernel learning model\nM\n?\nm=1\nq(? wqm,sqm).\n(5)\nwriting q(? wq) =?M\nm=1[q(? wqm,sqm= 1) + q(? wqm,sqm= 0)] and then by multiplying the terms\n?\nlogp(Y, ? wqm,sqm,\u0398)p(\u0398)N(? wqm|0,\u03c32\nwhere \u0398 are the remaining random variables in the model (i.e., excluding {? wqm,sqm}) and q(\u0398)\nq(? wqm,sqm) =1\nw)\u03c0sqm(1 \u2212 \u03c0)1\u2212sqm\n?\nq(? wqm,sqm)q(\u0398)\n,\n(6)\nw)\u03c0sqm(1 \u2212 \u03c0)1\u2212sqm,\n(7)\nwhere Z is a normalizing constant that does not depend on {? wqm,sqm}.\ne?log p(Y,? wqm,sqm=0,\u0398)?q(\u0398)is a constant that does not depend on ? wqm. From the last expression\nThe above remarks regarding variational distribution (5) are general and can hold for many spike and\nTherefore, we\nC\nZN(? wqm|0,\u03c32\nw)(1 \u2212 \u03c0), where C =\nw) which implies that Property 2 is satisfied.\nfunction according to ? w \u25e6 s.\nHere, we briefly discuss the variational method applied to the multi-task and multiple kernel model\ndescribed in Section 2.1 and refer to supplementary material for variational EM update equations.\nThe explicit form of the joint probability density function on the training data of model (1) is\np(Y,?\nwhere {?\nusing the following variational distribution\nW,S,\u03a6) = N(Y|\u03a6(?\nW,S,\u03a6} is the whole set of random variables that need to be marginalized out to compute\nthe marginal likelihood. The marginal likelihood is analytically intractable, so we lower bound it\nW\u25e6S)?,\u03a3)\n?\nq,m\n?N(? wqm|0,\u03c32\nw)\u03c0sqm(1 \u2212 \u03c0)sqm?\nM\n?\nm=1\nN(\u03c6m|\u00b5m,Km),\nq(?\nW,S,\u03a6) =\nQ\n?\nq=1\nM\n?\nm=1\nq(? wqm,sqm)\n4\nM\n?\nm=1\nq(\u03c6m).\n(8)"},{"page":5,"text":"The stationary conditions of the lower bound result in analytical updates for all factors above. More\nprecisely, q(\u03c6m) is an N-dimensional Gaussian distribution and each factor q(? wqm,sqm) leads to\nalgorithm that at the E-step updates the factors in (8) and at the M-step updates hyperparameters\n{{\u03c3q}Q\nsurprise in these updates. The GP hyperparameters \u03b8mare strongly dependent on the factor q(\u03c6m)\nof the corresponding GP latent vector, so updating \u03b8mby keeping fixed the factor q(\u03c6m) exhibits\nslow convergence. This problem is efficiently resolved by applying a Marginalized Variational step\n[20] which jointly updates the pair (q(\u03c6m),\u03b8m). This more advanced update together with all\nremaining updates of the EM algorithm are discussed in detail in the supplementary material.\na marginal q(? wqm) which is a mixture of two Gaussians where one component is q(? wqm|sqm =\nq=1,\u03c32\n0) = N(? wqm|0,\u03c32\nw), as shown in the previous section. The optimization proceeds using an EM\nw,\u03c0,{\u03b8m}M\nm=1} where \u03b8mparameterize kernel matrix Km. There is, however, one\n4\nIn this section we compare the proposed variational inference method, in the following called\npaired mean field (PMF), against the standard mean field (MF) approximation. For simplicity,\nwe consider a single-output linear regression problem where the data are generated according to:\ny = (? w \u25e6 s)Tx + \u03be. Moreover, to remove the effect of hyperparameter learning from the com-\nwhere the expectation is under the true posterior distribution. wtris obtained by running a very\nlong run of Gibbs sampling. PMF and MF provide alternative approximations wPMFand wMF, and\nabsolute errors between these approximations and wtrare used to measure accuracy. Since initial-\nization is crucial for variational non-convex algorithms, the accuracy of PMF and MF is averaged\nover many random initializations of their respective variational distributions.\nAssessing the accuracy of the approximation\nparison, (\u03c32,\u03c0,\u03c32\naccuracy when approximating the true posterior mean value for the parameter vector wtr= E[? w\u25e6s]\nw) are fixed to known values. The objective of the comparison is to measure the\nsoft-error soft-boundextreme-error\n1.880 [0.965, 2.561]\n0.204 [0.002, 0.454]\nextreme-bound\n-895.0 [-618.9,-1483.3]\n-560.6 [-557.8, -564.0]\nMF\nPMF\n0.917 [0.002,1.930]\n0.208 [0.002,0.454]\n-628.9 [-554.6,-793.5]\n-560.7 [-557.8, -564.1]\nTable 1: Comparison of MF and PMF in Boston-housing data in terms of approximating the ground-truth.\nAverage errors (?13\nFor the purpose of the comparison we also derived an efficient paired Gibbs sampler that follows\nexactly the same principle as PMF. This Gibbs sampler iteratively samples the pair (? wm,sm) from\ngiven in the supplementary material.\nWe considered the Boston-housing dataset which consists of 456 training examples and 13 inputs.\nHyperparameters were fixed to values (\u03c32= 0.1\u00d7var(y),\u03c0 = 0.25,\u03c32\nthe variance of the data. We performed two types of experiments each repeated 300 times. Each\nrepetition of the first type uses a soft random initialization of each q(sm= 1) = \u03b3mfrom the range\n(0,1). The second type uses an extreme random initialization so that each \u03b3mis initialized to either\n0 or 1. For each run PMF and MF are initialized to the same variational parameters.\nTable 1 reports average absolute errors and also average values of the variational lower bounds.\nClearly, PMF is more accurate than MF, achieves significantly higher values for the lower bound\nand exhibits smaller variance under different initializations. Further, for the more difficult case\nof extreme initializations the performance of MF becomes worse, while the performance of PMF\nremains unchanged. This shows that optimization in PMF, although non-convex, is very robust to\nunfavorable initializations. Similar experiments in other datasets have confirmed the above remarks.\nm=1|wtr\nm\u2212wappr\nm |) together with 95% confidence intervals (given by percentiles) are shown\nfor soft and extreme initializations. Average values for the variational lower bound are also shown.\nthe conditional p(? wm,sm|? w\\m,s\\m,y) and has been observed to mix much faster than the standard\nGibbs sampler that samples ? w and s separately. More details about the paired Gibbs sampler are\nw= 1) where var(y) denotes\n5\nToy multi-output regression dataset. To illustrate the capabilities of the proposed model, we first\napply it to a toy multi-output dataset with missing observations. Toy data is generated as follows:\nExperiments\n5"},{"page":6,"text":"Ten random latent functions are generated by sampling i.i.d. from zero-mean GPs with the following\nnon-stationary covariance function\nk(xi,xj) = exp\n?\u2212x2\ni\u2212 x2\n20\nj\n?\n(4cos(0.5(xi\u2212 xj)) + cos(2(xi\u2212 xj))),\nat 201 evenly spaced points in the interval x \u2208 [\u221210,10]. Ten tasks are then generated by adding\nGaussiannoisewithstandarddeviation0.2tothoserandomlatentfunctions, andtwoadditionaltasks\nconsist only of Gaussian noise with standard deviations 0.1 and 0.4. Finally, for each of the 12 tasks,\nwe artificially simulate missing data by removing 41 contiguous observations, as shown in Figure\n1. Missing data are not available to any learning algorithm, and will be used to test performance\nonly. Note that the above covariance function is rank-4, so ten out of the twelve tasks will be related,\nthough we do not know how, or which ones.\nAll tasks are then learned using both independent GPs with squared exponential (SE) covariance\nfunction kSE(xi,xj) = exp(\u2212(xi\u2212 xj)2\/(2?)) and the proposed MTMKL with M = 7 latent\nfunctions, each of them also using the SE prior. Hyperparameter ?, as well as noise levels are\nlearned independently for each latent function. Figure 1 shows the inferred posterior means.\n\u221210\u22128\u22126\u22124\u221220246810\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n2\n2.5\n\u221210\u22128\u22126\u22124\u221220246810\n\u22122.5\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n2\n\u221210\u22128\u22126\u22124 \u221220246810\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n\u221210\u22128\u22126\u22124\u221220246810\n\u22122\n\u22121\n0\n1\n2\n3\n4\n\u221210 \u22128\u22126\u22124\u221220246810\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n2\n\u221210\u22128 \u22126\u22124\u221220246810\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n\u221210\u22128 \u22126\u22124\u221220246810\n\u22120.4\n\u22120.3\n\u22120.2\n\u22120.1\n0\n0.1\n0.2\n0.3\n\u221210\u22128\u22126\u22124\u221220246810\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n2\n2.5\n\u221210 \u22128\u22126\u22124\u221220246810\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n4\n5\n6\n\u221210\u22128\u22126\u22124\u221220246810\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n2\n2.5\n\u221210\u22128\u22126\u22124\u221220246810\n\u22122.5\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n2\n2.5\n\u221210\u22128 \u22126 \u22124\u221220246810\n\u22123\n\u22122.5\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\nFigure 1: Twelve related tasks and predictions according to independent GPs (blue, continuous line) and\nMTMKL (red, dashed line). Missing data for each task is represented using green circles.\nThe mean square error (MSE) between predictions and missing observations for each task are dis-\nplayed in Table 2. MTMKL is able to infer how tasks are related and then exploit that information\nto make much better predictions. After learning, only 4 out of the 7 available latent functions re-\nmain active, while the other ones are pruned by setting the corresponding weights to zero. This is in\ncorrespondence with the generating covariance function, which only had 4 eigenfunctions, showing\nhow model order selection is automatic.\nMethod \\ Task #\nIndependent GPs\nMTMKL\n123456789 101112\n6.51\n1.97\n11.70\n4.57\n7.52\n7.71\n2.49\n1.94\n1.53\n1.98\n18.25\n2.09\n0.41\n0.41\n7.43\n1.96\n2.73\n1.90\n1.81\n1.57\n19.93\n1.20\n93.80\n2.83\nTable 2: MSE performance of independent GPs vs. MTMKL on the missing observations for each task.\n6"},{"page":7,"text":"Inferred noise standard deviations for the noise-only tasks are 0.10 and 0.45, and the average for the\nremaining tasks is 0.22, which agrees well with the stated actual values.\nThe flowers dataset. Though the proposed model has been designed as a tool for regression, it\ncan also be used approximately to solve classification problems by using output values to identify\nclass membership. In this section we will apply it to the challenging flower identification problem\nposed in [21]. There are 2040 instances of flowers for training and 6149 for testing, mainly acquired\nfrom the web, with varying scales, resolutions, etc., which are labeled into 102 categories. In [21],\nfour relevant features are identified: Color, histogram of gradient orientations and the scale invariant\nfeature transform, sampled on both the foreground region and its boundary. More information is\navailable at http:\/\/www.robots.ox.ac.uk\/\u02dcvgg\/data\/flowers\/.\nFor this type of dataset, state of the art performance has been achieved using a weighted linear\ncombination of kernels (one per feature) in a support vector machine (SVM) classifier. A different\nset of weights is learned for each class. In [22] it is shown that these weights can be learned by\nsolving a convex optimization problem. I.e., the standard approach to tackle the flower classification\nproblem would correspond to solving 102 independent binary classification problems, each using a\nlinear combination of 4 kernels. We take a different approach: Since all the 102 binary classification\ntasks are related, we learn all of them at once as a multi-task multiple-kernel problem, hoping that\nknowledge transfer between them will enhance performance.\nFor each training instance, we set the corresponding output to +1 for the desired task, whereas the\noutput for the remaining tasks is set to -1. Then we consider both using 10 and 13 latent functions\nper feature (i.e., M = 40 and M = 52). We measure performance in terms of the recognition\nrate (RR), which is the average of break-even points (where precision equals recall) for each class;\naverage area under the curve (AUC); and the multi-class accuracy (MA) which is the rate of correctly\nclassified instances. As baseline, recall that a random classifier would yield a RR and AUC of 0.5\nand a MA of 1\/102 = 0.0098. Results are reported in Table 3.\nMethod\nMTMKL\nMTMKL\nMKL from [21]\nMKL from [13]\nLatent function #AUC on test set\n0.944\n0.952\n-\n0.957\nRR on test set\n0.889\n0.893\n0.728\n-\nMA on test set\n0.329\n0.400\n-\n-\nM = 40\nM = 52\nM = 408\nM = 408\nTable 3: Performance of the different multiple kernel learning algorithms on the flowers dataset.\nMTMKL significantly outperforms the state-of-the-art method in [21], yielding a performance in\nline with [13], due to its ability to share information across tasks.\nImage denoising and dictionary learning. Here we illustrate denoising on the 256 \u00d7 256 \u201chouse\u201d\nimage used in [19]. Three noise levels (standard deviations 15, 25 and 50) are considered. Follow-\ning [19], we partition the noisy image in 62,001 overlapping 8 \u00d7 8 blocks and regard each block\nas a different task. MTMKL is then run using M = 64 \u201clatent blocks\u201d, also known as \u201cdictionary\nelements\u201d (bigger dictionaries do not result in significant performance increase). For the covariance\nof the latent functions, we consider two possible choices: Either a white covariance function (as\nin [19]) or an exponential covariance of the form kEXP(xi,xj) = e\u2212\ncoordinates within each block. The first option is equivalent to placing an independent standard nor-\nmal prior on each pixel of the dictionary. The second one, on the other hand, introduces correlation\nbetween neighboring pixels in the dictionary. Results are shown in Table 4. The exponential co-\nvariance clearly enhances performance and produces a more structured dictionary, as can be seen in\nFigure 3.(a). The Peak-to-Signal Ratio (PSNR) obtained using the proposed approach is comparable\nto the state-of-the-art results obtained in [19].\nImage inpainting and dictionary learning. We now address the inpainting problem in color im-\nages. Following [19], we consider a color image in which a random 80% of the RGB components\nare missing. Using an analogous partitioning scheme as in the previous section we obtain 148,836\nblocks of size 8\u00d78\u00d73, each of which is regarded as a different task. A dictionary size of M = 100\nand a white covariance function (which is used in [19]) are selected. Note that we do not apply any\nother preprocessing to data or any specific initialization as it is done in [19]. The PSNR of the image\n|xi\u2212xj|\n?\n, where x are the pixel\n7"},{"page":8,"text":"Figure 2: Noisy \u201chouse\u201d image with \u03c3 = 25 and re-\nstored version using Exponential cov. function.\nPSNR (dB)\nNoise stdNoisy image\n24.66\n20.22\n14.20\nWhite\n33.98\n30.98\n26.14\nExpon.\n34.29\n31.88\n28.08\n\u03c3 = 15\n\u03c3 = 25\n\u03c3 = 50\nTable4: PSNRfornoisyandrestoredimageusing\nseveral noise levels and covariance functions.\nafter it is restored using MTMKL is 28.94 dB, see Figure 3.(b). This result is similar to the results\nreported in [19] and close to the state-of-the-art result of 29.65 dB achieved in [23].\n(a) House: Dict. for white and Exponential (b) Castle: Missing values, restored and original\nFigure 3: Dictionaries inferred from noisy (\u03c3 = 25) \u201chouse\u201d image; and \u201ccastle\u201d inpainting results.\nCollaborative filtering.\nset that consists of 10 million ratings for 71,567 users and 10,681 films, with ratings ranging\n{1,0.5,2,...,4.5,5}. We followed the setup in [24] and used the raand rbpartitions provided\nwith the database, that split the data into a training and testing, so that they are 10 ratings per user\nin the test set. We applied the sparse factor analysis model (i.e. sparse PCA but with heteroscedastic\nnoise for the columns of the observation matrix Y which corresponds to films) with M = 20 latent\ndimensions. The RMSE for the rapartition was 0.88 for the rbpartition was 0.85 so one average\n0.865. This result is slightly better than 0.8740 RMSE reported in [24] using GP-LVM.\nFinally, we performed an experiment on the 10M MovieLens data\n6\nIn this work we have proposed a spike and slab multi-task and multiple kernel learning model. A\nnovel variational algorithm to perform inference in this model has been derived. The key contri-\nbution in this regard that explains the good performance of the algorithm is the choice of a joint\ndistribution over \u02dc wqmand sqmin the variational posterior, as opposed to the usual independence\nassumption. This has the effect of using exponentially many modes to approximate the posterior,\nthus rendering it more accurate and much more robust to poor initializations of the variational pa-\nrameters. The relevance and wide applicability of the proposed model has been illustrated by using\nit on very diverse tasks: multi-output regression, multi-class classification, image denoising, image\ninpainting and collaborative filtering. Prior structure beliefs were introduced in image dictionaries,\nwhich is also a novel contribution to the best of our knowledge. Finally an interesting topic for future\nresearch is to optimize the variational distribution proposed here with alternative approximate infer-\nence frameworks such as belief propagation or expectation propagation. This could allow to extend\ncurrent methodologies within such frameworks that assume unimodal approximations [25, 26].\nDiscussion\nAcknowledgments\nWe thank the reviewers for insightful comments.\nEP\/F005687\/1 \u201cGaussian Processes for Systems Identification with Applications in Systems Bi-\nology\u201d. MLG gratefully acknowledges funding from CAM project CCG10-UC3M\/TIC-5511 and\nCONSOLIDER-INGENIO 2010 CSD2008-00010 (COMONSENS).\nMKT was supported by EPSRC Grant No\n8"},{"page":9,"text":"References\n[1] T.J. Mitchell and J.J. Beauchamp. Bayesian variable selection in linear regression. Journal of the Ameri-\ncan Statistical Association, 83(404):1023\u20131032, 1988.\n[2] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society,\nSeries B, 58:267\u2013288, 1994.\n[3] M.E. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning\nResearch, 1:211\u2013244, 2001.\n[4] E.I. George and R.E. Mcculloch. Variable selection via Gibbs sampling. Journal of the American Statis-\ntical Association, 88(423):881\u2013889, 1993.\n[5] M. West. Bayesian factor regression models in the \u201dlarge p, small n\u201d paradigm. In Bayesian Statistics,\npages 723\u2013732. Oxford University Press, 2003.\n[6] B. Efron. Microarrays, empirical Bayes and the two-groups model. Statistical Science, 23:1\u201322, 2008.\n[7] C. Archambeau and F. Bach. Sparse probabilistic projections. In D. Koller, D. Schuurmans, Y. Bengio,\nand L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 73\u201380. 2009.\n[8] F. Caron and A. Doucet. Sparse Bayesian nonparametric regression. In In 25th International Conference\non Machine Learning (ICML). ACM, 2008.\n[9] Matthias W. Seeger and Hannes Nickisch. Compressed sensing and Bayesian experimental design. In\nICML, pages 912\u2013919, 2008.\n[10] C.M. Carvalho, N.G. Polson, and J.G. Scott. The horseshoe estimator for sparse signals. Biometrika,\n97:465\u2013480, 2010.\n[11] T. Damoulas and M.A. Girolami. Probabilistic multi-class multi-kernel learning: on protein fold recogni-\ntion and remote homology detection. Bioinformatics, 24:1264\u20131270, 2008.\n[12] M.Christoudias, R.Urtasun, andT. Darrell. Bayesian localizedmultiple kernel learning. Technical report,\nEECS Department, University of California, Berkeley, Jul 2009.\n[13] C. Archambeau and F. Bach. Multiple Gaussian process models. In NIPS 23 workshop on New Directions\nin Multiple Kernel Learning. 2010.\n[14] Y.W. Teh, M. Seeger, and M.I. Jordan. Semiparametric latent factor models. In Proceedings of the\nInternational Workshop on Artificial Intelligence and Statistics, volume 10, 2005.\n[15] E.V. Bonilla, K.M.A. Chai, and C.K.I. Williams. Multi-task Gaussian process prediction. In Advances\nNeural Information Processing Systems 20, 2008.\n[16] P Boyle and M. Frean. Dependent Gaussian processes. In Advances in Neural Information Processing\nSystems 17, pages 217\u2013224. MIT Press, 2005.\n[17] M. Alvarez and N.D. Lawrence. Sparse convolved Gaussian processes for multi-output regression. In\nAdvances in Neural Information Processing Systems 20, pages 57\u201364, 2008.\n[18] R. Yoshida and M. West. Bayesian learning in sparse graphical factor models via variational mean-field\nannealing. Journal of Machine Learning Research, 11:1771\u20131798, 2010.\n[19] M. Zhou, H. Chen, J. Paisley, L. Ren, G. Sapiro, and L. Carin. Non-parametric Bayesian dictionary\nlearning for sparse image represent ations. In Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I. Williams,\nand A. Culotta, editors, Advances in Neural Information Processing Systems 22, pages 2295\u20132303. 2009.\n[20] M. L\u00b4 azaro-Gredilla and M. Titsias. Variational heteroscedastic Gaussian process regression. In 28th\nInternational Conference on Machine Learning (ICML-11), pages 841\u2013848, New York, NY, USA, June\n2011. ACM.\n[21] M.E. Nilsback and A. Zisserman. Automated flower classification over a large number of classes. In\nProceedings of the Indian Conference on Computer Vision, Graphics and Image Processing, Dec 2008.\n[22] M. Varma and D. Ray. Learning the discriminative power invariance trade-off. In International Confer-\nence on Computer Vision. 2007.\n[23] J. Mairal, M. Elad, and G. Sapiro. Sparse representation for color image restoration. IEEE Trans. Image\nProcessing, 17, 2008.\n[24] N.D. Lawrence and R. Urtasun. Non-linear matrix factorization with Gaussian processes. In Proceedings\nof the 26th Annual International Conference on Machine Learning, pages 601\u2013608, 2009.\n[25] K. Sharp and M. Rattray. Dense message passing for sparse principal component analysis. In 13th\nInternational Conference on Artificial Intelligence and Statistics (AISTATS), pages 725\u2013732, 2010.\n[26] J.M. Hern\u00b4 andez-Lobato, D. Hern\u00b4 andez-Lobato, and A. Su\u00b4 arez. Network-based sparse Bayesian classifi-\ncation. Pattern Recognition, 44(4):886\u2013900, 2011.\n9"}],"widgetId":"rgw22_56ab1ccce0ea8"},"id":"rgw22_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=267805753&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw23_56ab1ccce0ea8"},"id":"rgw23_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=267805753&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":267805753,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":267805753,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2058288188,"url":"researcher\/2058288188_Qisong_Wu","fullname":"Qisong Wu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7767377,"url":"researcher\/7767377_Yimin_D_Zhang","fullname":"Yimin D Zhang","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A300402269540364%401448632890515_m\/Yimin_Zhang3.png"},{"id":7767376,"url":"researcher\/7767376_Moeness_G_Amin","fullname":"Moeness G Amin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7730065,"url":"researcher\/7730065_Braham_Himed","fullname":"Braham Himed","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Apr 2015","journal":"IEEE Signal Processing Letters","showEnrichedPublicationItem":false,"citationCount":9,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/268504229_Multi-Task_Bayesian_Compressive_Sensing_Exploiting_Intra-Task_Dependency","usePlainButton":true,"publicationUid":268504229,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.75","url":"publication\/268504229_Multi-Task_Bayesian_Compressive_Sensing_Exploiting_Intra-Task_Dependency","title":"Multi-Task Bayesian Compressive Sensing Exploiting Intra-Task Dependency","displayTitleAsLink":true,"authors":[{"id":2058288188,"url":"researcher\/2058288188_Qisong_Wu","fullname":"Qisong Wu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7767377,"url":"researcher\/7767377_Yimin_D_Zhang","fullname":"Yimin D Zhang","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A300402269540364%401448632890515_m\/Yimin_Zhang3.png"},{"id":7767376,"url":"researcher\/7767376_Moeness_G_Amin","fullname":"Moeness G Amin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7730065,"url":"researcher\/7730065_Braham_Himed","fullname":"Braham Himed","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["IEEE Signal Processing Letters 04\/2015; 22(4):430-434. DOI:10.1109\/LSP.2014.2360688"],"abstract":"In this letter, we propose a multi-task compressive sensing algorithm for the reconstruction of clustered sparse en-tries based on hierarchical Bayesian framework. By extending a paired spike-and-slab prior to a general multi-task model, the proposed algorithm has the capability of modeling both inter-task and intra-task dependencies of the observation data. The latter is achieved by imposing a clustered prior on non-zero entries and finds applications in radar where targets exhibit spatial extent. Simulation results verify that the proposed algorithm outperforms state-of-the-art group sparse Bayesian learning algorithms.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/268504229_Multi-Task_Bayesian_Compressive_Sensing_Exploiting_Intra-Task_Dependency","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Yimin_Zhang3\/publication\/268504229_Multi-Task_Bayesian_Compressive_Sensing_Exploiting_Intra-Task_Dependency\/links\/546cb7740cf284dbf190eb34.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Yimin_Zhang3","sourceName":"Yimin D. Zhang","hasSourceUrl":true},"publicationUid":268504229,"publicationUrl":"publication\/268504229_Multi-Task_Bayesian_Compressive_Sensing_Exploiting_Intra-Task_Dependency","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/268504229_Multi-Task_Bayesian_Compressive_Sensing_Exploiting_Intra-Task_Dependency\/links\/546cb7740cf284dbf190eb34\/smallpreview.png","linkId":"546cb7740cf284dbf190eb34","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=268504229&reference=546cb7740cf284dbf190eb34&eventCode=&origin=publication_list","widgetId":"rgw27_56ab1ccce0ea8"},"id":"rgw27_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=268504229&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"546cb7740cf284dbf190eb34","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":267805753,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/268504229_Multi-Task_Bayesian_Compressive_Sensing_Exploiting_Intra-Task_Dependency\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["However, and are strongly correlated since their product interacts with the data. The following paired spike-andslab prior is introduced [20]: "],"widgetId":"rgw28_56ab1ccce0ea8"},"id":"rgw28_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw26_56ab1ccce0ea8"},"id":"rgw26_56ab1ccce0ea8","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=268504229&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithSlurp","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2047544144,"url":"researcher\/2047544144_Hojjat_S_Mousavi","fullname":"Hojjat S. Mousavi","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A297589623803904%401447962303052_m\/Hojjat_Seyed_Mousavi.png"},{"id":10074977,"url":"researcher\/10074977_Vishal_Monga","fullname":"Vishal Monga","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2050299069,"url":"researcher\/2050299069_Trac_D_Tran","fullname":"Trac D. Tran","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":false,"isSlurp":true,"isNoText":false,"publicationType":"Article","publicationDate":"Feb 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/272521367_ICR_Iterative_Convex_Refinement_for_Sparse_Signal_Recovery_Using_Spike_and_Slab_Priors","usePlainButton":true,"publicationUid":272521367,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/272521367_ICR_Iterative_Convex_Refinement_for_Sparse_Signal_Recovery_Using_Spike_and_Slab_Priors","title":"ICR: Iterative Convex Refinement for Sparse Signal Recovery Using Spike and Slab Priors","displayTitleAsLink":true,"authors":[{"id":2047544144,"url":"researcher\/2047544144_Hojjat_S_Mousavi","fullname":"Hojjat S. Mousavi","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A297589623803904%401447962303052_m\/Hojjat_Seyed_Mousavi.png"},{"id":10074977,"url":"researcher\/10074977_Vishal_Monga","fullname":"Vishal Monga","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2050299069,"url":"researcher\/2050299069_Trac_D_Tran","fullname":"Trac D. Tran","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"In this letter, we address sparse signal recovery using spike and slab\npriors. In particular, we focus on a Bayesian framework where sparsity is\nenforced on reconstruction coefficients via probabilistic priors. The\noptimization resulting from spike and slab prior maximization is known to be a\nhard non-convex problem, and existing solutions involve simplifying assumptions\nand\/or relaxations. We propose an approach called Iterative Convex Refinement\n(ICR) that aims to solve the aforementioned optimization problem directly\nallowing for greater generality in the sparse structure. Essentially, ICR\nsolves a sequence of convex optimization problems such that sequence of\nsolutions converges to a sub-optimal solution of the original hard optimization\nproblem. We propose two versions of our algorithm: a.) an unconstrained\nversion, and b.) with a non-negativity constraint on sparse coefficients, which\nmay be required in some real-world problems. Experimental validation is\nperformed on both synthetic data and for a real-world image recovery problem,\nwhich illustrates merits of ICR over state of the art alternatives.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/272521367_ICR_Iterative_Convex_Refinement_for_Sparse_Signal_Recovery_Using_Spike_and_Slab_Priors","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1502.04726","sourceName":"de.arxiv.org","hasSourceUrl":true},"publicationUid":272521367,"publicationUrl":"publication\/272521367_ICR_Iterative_Convex_Refinement_for_Sparse_Signal_Recovery_Using_Spike_and_Slab_Priors","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/272521367_ICR_Iterative_Convex_Refinement_for_Sparse_Signal_Recovery_Using_Spike_and_Slab_Priors\/links\/54ed2f2f0cf27fbfd7723d11\/smallpreview.png","linkId":"54ed2f2f0cf27fbfd7723d11","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=272521367&reference=54ed2f2f0cf27fbfd7723d11&eventCode=&origin=publication_list","widgetId":"rgw30_56ab1ccce0ea8"},"id":"rgw30_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=272521367&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":267805753,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/272521367_ICR_Iterative_Convex_Refinement_for_Sparse_Signal_Recovery_Using_Spike_and_Slab_Priors\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Optimization Problem (Hierarchical Bayesian Framework): Any inference from the posterior density for this model will be ill-defined because the Dirac's delta function is unbounded. Some ways to handle this issue include approximations [35], such as approximation of spike term with a narrow Gaussian [36], approximating the whole posterior function with product of Gaussian(s) and Bernoulli(s) density functions [16], [37]\u2013 [40], etc. In this work, we focus on the setup of Yen et al. [29] which is an approximate spike and slab prior for inducing sparsity on x x x. "],"widgetId":"rgw31_56ab1ccce0ea8"},"id":"rgw31_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw29_56ab1ccce0ea8"},"id":"rgw29_56ab1ccce0ea8","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=272521367&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2047544144,"url":"researcher\/2047544144_Hojjat_Seyed_Mousavi","fullname":"Hojjat Seyed Mousavi","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A297589623803904%401447962303052_m\/Hojjat_Seyed_Mousavi.png"},{"id":59363400,"url":"researcher\/59363400_Umamahesh_Srinivas","fullname":"Umamahesh Srinivas","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10074977,"url":"researcher\/10074977_Vishal_Monga","fullname":"Vishal Monga","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2044831369,"url":"researcher\/2044831369_Yuanming_Suo","fullname":"Yuanming Suo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":2,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Jan 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":4,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/271710414_Multi-task_Image_Classification_via_Collaborative_Hierarchical_Spike-and-Slab_Priors","usePlainButton":true,"publicationUid":271710414,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/271710414_Multi-task_Image_Classification_via_Collaborative_Hierarchical_Spike-and-Slab_Priors","title":"Multi-task Image Classification via Collaborative, Hierarchical Spike-and-Slab Priors","displayTitleAsLink":true,"authors":[{"id":2047544144,"url":"researcher\/2047544144_Hojjat_Seyed_Mousavi","fullname":"Hojjat Seyed Mousavi","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A297589623803904%401447962303052_m\/Hojjat_Seyed_Mousavi.png"},{"id":59363400,"url":"researcher\/59363400_Umamahesh_Srinivas","fullname":"Umamahesh Srinivas","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10074977,"url":"researcher\/10074977_Vishal_Monga","fullname":"Vishal Monga","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2044831369,"url":"researcher\/2044831369_Yuanming_Suo","fullname":"Yuanming Suo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70534368,"url":"researcher\/70534368_Minh_Dao","fullname":"Minh Dao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2050299069,"url":"researcher\/2050299069_Trac_D_Tran","fullname":"Trac. D. Tran","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Promising results have been achieved in image classification problems by\nexploiting the discriminative power of sparse representations for\nclassification (SRC). Recently, it has been shown that the use of\n\\emph{class-specific} spike-and-slab priors in conjunction with the\nclass-specific dictionaries from SRC is particularly effective in low training\nscenarios. As a logical extension, we build on this framework for multitask\nscenarios, wherein multiple representations of the same physical phenomena are\navailable. We experimentally demonstrate the benefits of mining joint\ninformation from different camera views for multi-view face recognition.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/271710414_Multi-task_Image_Classification_via_Collaborative_Hierarchical_Spike-and-Slab_Priors","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Minh_Dao7\/publication\/271710414_Multi-task_Image_Classification_via_Collaborative_Hierarchical_Spike-and-Slab_Priors\/links\/5538cbfe0cf226723ab63f23.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Minh_Dao7","sourceName":"Minh Dao","hasSourceUrl":true},"publicationUid":271710414,"publicationUrl":"publication\/271710414_Multi-task_Image_Classification_via_Collaborative_Hierarchical_Spike-and-Slab_Priors","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/271710414_Multi-task_Image_Classification_via_Collaborative_Hierarchical_Spike-and-Slab_Priors\/links\/5538cbfe0cf226723ab63f23\/smallpreview.png","linkId":"5538cbfe0cf226723ab63f23","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=271710414&reference=5538cbfe0cf226723ab63f23&eventCode=&origin=publication_list","widgetId":"rgw33_56ab1ccce0ea8"},"id":"rgw33_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=271710414&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"5538cbfe0cf226723ab63f23","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":267805753,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/271710414_Multi-task_Image_Classification_via_Collaborative_Hierarchical_Spike-and-Slab_Priors\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["well-suited structured prior for capturing sparsity [25] [26] [27]. \u03b4 0 is a point mass concentrated at zero (known as \" spike \" ) and the other term is the distribution of nonzero coefficients of sparse vector also known as \" slab \" . "],"widgetId":"rgw34_56ab1ccce0ea8"},"id":"rgw34_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw32_56ab1ccce0ea8"},"id":"rgw32_56ab1ccce0ea8","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=271710414&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":267805753,"publicationLink":"publication\/267805753_Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw25_56ab1ccce0ea8"},"id":"rgw25_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=267805753&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=15","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":15,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw24_56ab1ccce0ea8"},"id":"rgw24_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=267805753&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/267805753_Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1ccce0ea8"},"id":"rgw2_56ab1ccce0ea8","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":267805753},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=267805753&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1ccce0ea8"},"id":"rgw1_56ab1ccce0ea8","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"mzY2N2BF+LMoeo6vPGzCR64yHTl9oCCmfbBJlAGOAWN4bc6I9zdYYk2vvme0ASXJEgfQWmC+YH4xmy7o4cSgCGHWIRmhCHPNbFRhZmfK62vV2JUkCKFxLP7WTxItw5U0YJFZtAMZh\/jgBRYN7QptYgsZeB55EsVC8PGJU9\/6k1o5zbF2pO91ieAVX8QD7alOGAX+qLsduVI02qGDOXqVyjk6KHuXkuiWHVFx2u12CsVbaF2Ap9q8GsnAKOPIHe2DaiMvagZi87JKCMAtng4T\/jSjgZ+RyMXZWkQaTAzMLB0=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/267805753_Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning\" \/>\n<meta property=\"og:description\" content=\"We introduce a variational Bayesian inference algorithm which can be widely applied to sparse linear models. The algorithm is based on the spike and slab prior which, from a Bayesian perspective,...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/267805753_Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning\/links\/545c772c0cf2f1dbcbcb28eb\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/267805753_Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning\" \/>\n<meta property=\"rg:id\" content=\"PB:267805753\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning\" \/>\n<meta name=\"citation_author\" content=\"Michalis K Titsias\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/267805753_Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/267805753_Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-c318daf5-9b14-4788-830a-2d8fb85940b7","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":481,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw35_56ab1ccce0ea8"},"id":"rgw35_56ab1ccce0ea8","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-c318daf5-9b14-4788-830a-2d8fb85940b7", "c31716fd653c11ac57be2223bea6a3d81da2f4b9");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-c318daf5-9b14-4788-830a-2d8fb85940b7", "c31716fd653c11ac57be2223bea6a3d81da2f4b9");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw36_56ab1ccce0ea8"},"id":"rgw36_56ab1ccce0ea8","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/267805753_Spike_and_Slab_Variational_Inference_for_Multi-Task_and_Multiple_Kernel_Learning","requestToken":"Z7bWoH46aUgCCAMIVNqMxCpQV2cJqDXqb7dKThDLnMVVAXFKjH+dzwx9k9lx8kqsqdm\/VTpfeS43YKVSt5h88l2a6WJeBDO2PwVa+\/WWgGUgwM7j2nPOOjNhw3S2QtUMKUwoN7PT+UCAjP9HE8kms6tfIHS3SoGqiB8JMKgWBQnwkXiMhWKnKc66fjeCZKT4nt2w0KRJ4934qidaCil1JhAjkjrbGlYy537j5wLUh1iijxZ2I19d3G4CkV0Gyggrpik9L548W1I0GMyAbRUgJsWKR6MtN2ESYZoMm4vaDWk=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=-PUTNMvC22S5Mi_NwneCdGzcfe98k7cCQwnFDV9sWTaHZq1C0v3KJrcDJ9V7kCdT","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjY3ODA1NzUzX1NwaWtlX2FuZF9TbGFiX1ZhcmlhdGlvbmFsX0luZmVyZW5jZV9mb3JfTXVsdGktVGFza19hbmRfTXVsdGlwbGVfS2VybmVsX0xlYXJuaW5n","signupCallToAction":"Join for free","widgetId":"rgw38_56ab1ccce0ea8"},"id":"rgw38_56ab1ccce0ea8","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw37_56ab1ccce0ea8"},"id":"rgw37_56ab1ccce0ea8","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw39_56ab1ccce0ea8"},"id":"rgw39_56ab1ccce0ea8","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
