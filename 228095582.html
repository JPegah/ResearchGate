<!DOCTYPE html> <html lang="en" class="" id="rgw39_56ab1c85d5e07"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="fQOh/hA6jLeDe4DjDqSi2p2cQc1bwvCzcEs78/5UGH3pMYS73d0NdPWiVei57E3AjXIjE4jfCdtSS7SEKDT7y1yfPp20onqIh53KgjOGu5D2aqOLE2EL6ruET3kJtXNNoj7p6EgLCM2fJnmaS+mMA97p5jEBp7KIsPvyKxoB9KfVTwoWJaKTPJlgOF5v/kjjwcMIj/JWNUHpb9HysXn0Z33D82xbCnG+brENbRI/kdfrPY8DtfAsWxJxGh2j5tdzzdPacLxo9jEjypECm2r0cYnh9Iusk6LEkBJTtd9EUsY="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-b7a3a1a3-9b58-44a7-95cb-6a96d2cfc7eb",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/228095582_Bayesian_Posterior_Sampling_via_Stochastic_Gradient_Fisher_Scoring" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring" />
<meta property="og:description" content="In this paper we address the following question: Can we approximately sample
from a Bayesian posterior distribution if we are only allowed to touch a small
mini-batch of data-items for every..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/228095582_Bayesian_Posterior_Sampling_via_Stochastic_Gradient_Fisher_Scoring/links/02b0457d0cf27908e9dda46b/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/228095582_Bayesian_Posterior_Sampling_via_Stochastic_Gradient_Fisher_Scoring" />
<meta property="rg:id" content="PB:228095582" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring" />
<meta name="citation_author" content="Sungjin Ahn" />
<meta name="citation_author" content="Anoop Korattikara" />
<meta name="citation_author" content="Max Welling" />
<meta name="citation_publication_date" content="2012/06/27" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/228095582_Bayesian_Posterior_Sampling_via_Stochastic_Gradient_Fisher_Scoring" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/228095582_Bayesian_Posterior_Sampling_via_Stochastic_Gradient_Fisher_Scoring" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring</title>
<meta name="description" content="Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1c85d5e07" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1c85d5e07" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab1c85d5e07">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Bayesian%20Posterior%20Sampling%20via%20Stochastic%20Gradient%20Fisher%20Scoring&rft.date=2012&rft.au=Sungjin%20Ahn%2CAnoop%20Korattikara%2CMax%20Welling&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring</h1> <meta itemprop="headline" content="Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/228095582_Bayesian_Posterior_Sampling_via_Stochastic_Gradient_Fisher_Scoring/links/02b0457d0cf27908e9dda46b/smallpreview.png">  <div id="rgw7_56ab1c85d5e07" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab1c85d5e07" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Sungjin_Ahn" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Sungjin Ahn" alt="Sungjin Ahn" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Sungjin Ahn</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw9_56ab1c85d5e07" data-account-key="Sungjin_Ahn">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Sungjin_Ahn"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Sungjin Ahn" alt="Sungjin Ahn" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Sungjin_Ahn" class="display-name">Sungjin Ahn</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_of_California_Irvine" title="University of California, Irvine">University of California, Irvine</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56ab1c85d5e07"> <a href="researcher/80788532_Anoop_Korattikara" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Anoop Korattikara" alt="Anoop Korattikara" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Anoop Korattikara</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw11_56ab1c85d5e07">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/80788532_Anoop_Korattikara"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Anoop Korattikara" alt="Anoop Korattikara" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/80788532_Anoop_Korattikara" class="display-name">Anoop Korattikara</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56ab1c85d5e07"> <a href="researcher/69847505_Max_Welling" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Max Welling" alt="Max Welling" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Max Welling</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw13_56ab1c85d5e07">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/69847505_Max_Welling"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Max Welling" alt="Max Welling" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/69847505_Max_Welling" class="display-name">Max Welling</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">        <meta itemprop="datePublished" content="2012-06">  06/2012;               <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1206.6380" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw14_56ab1c85d5e07" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>In this paper we address the following question: Can we approximately sample<br />
from a Bayesian posterior distribution if we are only allowed to touch a small<br />
mini-batch of data-items for every sample we generate?. An algorithm based on<br />
the Langevin equation with stochastic gradients (SGLD) was previously proposed<br />
to solve this, but its mixing rate was slow. By leveraging the Bayesian Central<br />
Limit Theorem, we extend the SGLD algorithm so that at high mixing rates it<br />
will sample from a normal approximation of the posterior, while for slow mixing<br />
rates it will mimic the behavior of SGLD with a pre-conditioner matrix. As a<br />
bonus, the proposed algorithm is reminiscent of Fisher scoring (with stochastic<br />
gradients) and as such an efficient optimizer during burn-in.</div> </p>  </div>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw27_56ab1c85d5e07">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw26_56ab1c85d5e07"  itemprop="articleBody">  <p>Page 1</p> <p>Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring<br />Sungjin Ahn<br />Dept. of Computer Science, UC Irvine, Irvine, CA 92697-3425, USA<br />SUNGJIA@ICS.UCI.EDU<br />Anoop Korattikara<br />Dept. of Computer Science, UC Irvine, Irvine, CA 92697-3425, USA<br />AKORATTI@ICS.UCI.EDU<br />Max Welling<br />Dept. of Computer Science, UC Irvine, Irvine, CA 92697-3425, USA<br />WELLING@ICS.UCI.EDU<br />Abstract<br />In this paper we address the following question:<br />“Can we approximately sample from a Bayesian<br />posterior distribution if we are only allowed to<br />touch a small mini-batch of data-items for ev-<br />ery sample we generate?”. An algorithm based<br />on the Langevin equation with stochastic gradi-<br />ents (SGLD) was previously proposed to solve<br />this, but its mixing rate was slow. By leverag-<br />ing the Bayesian Central Limit Theorem, we ex-<br />tend the SGLD algorithm so that at high mix-<br />ing rates it will sample from a normal approx-<br />imation of the posterior, while for slow mixing<br />rates it will mimic the behavior of SGLD with a<br />pre-conditioner matrix. As a bonus, the proposed<br />algorithm is reminiscent of Fisher scoring (with<br />stochastic gradients) and as such an efficient op-<br />timizer during burn-in.<br />1. Motivation<br />When a dataset has a billion data-cases (as is not uncom-<br />monthesedays)MCMCalgorithmswillnotevenhavegen-<br />erated a single (burn-in) sample when a clever learning al-<br />gorithm based on stochastic gradients may already be mak-<br />ing fairly good predictions. In fact, the intriguing results of<br />Bottou and Bousquet (2008) seem to indicate that in terms<br />of “number of bits learned per unit of computation”, an al-<br />gorithm as simple as stochastic gradient descent is almost<br />optimally efficient. We therefore argue that for Bayesian<br />methods to remain useful in an age when the datasets grow<br />at an exponential rate, they need to embrace the ideas of the<br />stochastic optimization literature.<br />Appearing in Proceedings of the 29thInternational Conference<br />on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright<br />2012 by the author(s)/owner(s).<br />A first attempt in this direction was proposed by Welling<br />and Teh (2011) where the authors show that (uncorrected)<br />Langevin dynamics with stochastic gradients (SGLD) will<br />sample from the correct posterior distribution when the<br />stepsizes are annealed to zero at a certain rate.<br />SGLD succeeds in (asymptotically) generating samples<br />from the posterior at O(n) computational cost with (n ?<br />N) it’s mixing rate is unnecessarily slow. This can be<br />traced back to its lack of a proper pre-conditioner: SGLD<br />takes large steps in directions of small variance and re-<br />versely, small steps in directions of large variance which<br />hinders convergence of the Markov chain.<br />builds on top of Welling and Teh (2011). We leverage the<br />“Bayesian Central Limit Theorem” which states that when<br />N is large (and under certain conditions) the posterior will<br />be well approximated by a normal distribution. Our al-<br />gorithm is designed so that for large stepsizes (and thus<br />at high mixing rates) it will sample from this approximate<br />normal distribution, while at smaller stepsizes (and thus at<br />slower mixing rates) it will generate samples from an in-<br />creasingly accurate (non-Gaussian) approximation of the<br />posterior. Our main claim is therefore that we can trade-in<br />a usually small bias in our estimate of the posterior distri-<br />bution against a potentially very large computational gain,<br />which could in turn be used to draw more samples and re-<br />duce sampling variance.<br />While<br />Our work<br />From an optimization perspective one may view this algo-<br />rithm as a Fisher scoring method based on stochastic gradi-<br />ents (see e.g. (Schraudolph et al., 2007)) but in such a way<br />that the randomness introduced in the subsampling process<br />is used to sample from the posterior distribution when we<br />arrive at its mode. Hence, it is an efficient optimization al-<br />gorithmthatsmoothlyturnsintoasamplerwhenthecorrect<br />(statistical) scale of precision is reached.</p>  <p>Page 2</p> <p>Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring<br />2. Preliminaries<br />We will start with some notation, definitions and prelimi-<br />naries. We have a large dataset XNconsisting of N i.i.d.<br />data-points {x1...xN} and we use a family of distributions<br />parametrized by θ ∈ RDto model the distribution of the<br />xi’s. We choose a prior distribution p(θ) and are inter-<br />ested in obtaining samples from the posterior distribution,<br />p(θ|XN) ∝ p(XN|θ)p(θ).<br />As is common in Bayesian asymptotic theory, we will also<br />make use of some frequentist concepts in the develop-<br />ment of our method. We assume that the true data gen-<br />erating distribution is in our family of models and denote<br />the true parameter which generated the dataset XNby θ0.<br />We denote the score or the gradient of the log likelihood<br />w.r.t. data-point xiby gi(θ) = g(θ;xi) = ∇θlogp(θ;xi).<br />We denote the sum of scores of a batch of n data-points<br />Xr = {xr1...xrn} by Gn(θ;Xr) =?n<br />drop the argument Xrand instead simply write Gn(θ) and<br />gn(θ) for convenience.<br />The covariance of the gradients is called the Fisher infor-<br />mation defined as I(θ) = Ex[g(θ;x)g(θ;x)T], where Ex<br />denotes expectation w.r.t the distribution p(x;θ) and we<br />have used the fact that Ex[g(θ;x)] = 0. It can also be<br />shown that I(θ) = −Ex[H(θ;x)], where H is the Hessian<br />of the log likelihood.<br />i=1g(θ;xri) and<br />the average by gn(θ;Xr) =1<br />nGn(θ;Xr). Sometimes we will<br />Since we are dealing with a dataset with samples only<br />from p(x;θ0) we will henceforth be interested only in<br />I(θ0) which we will denote by I1. It is easy to see that<br />the Fisher information of n data-points, In = nI1. The<br />empirical covariance of the scores computed from a batch<br />ofndata-pointsiscalledtheempirical Fisherinformation,<br />V (θ;Xr) =<br />n−1<br />(Scott, 2002). Also, it can be shown that V (θ0) is a consis-<br />tent estimator of I1= I(θ0).<br />1<br />?n<br />i=1(gri(θ) − gn(θ))(gri(θ) − gn(θ))T<br />We now introduce an important result in Bayesian asymp-<br />totic theory. As N becomes large, the posterior distribution<br />becomes concentrated in a small neighbourhood around θ0<br />and becomes asymptotically Gaussian. This is formalized<br />by the Bernstein-von Mises theorem, a.k.a the Bayesian<br />Central Limit Theorem, (Le Cam, 1986), which states that<br />under suitable regularity conditions, p(θ|{x1...xN}) ap-<br />proximately equals N(θ0,I−1<br />N) as N becomes very large.<br />3. Stochastic Gradient Fisher Scoring<br />We are now ready to derive our Stochastic Gradient Fisher<br />Scoring (SGFS) algorithm. The starting point in the deriva-<br />tion of our method is the Stochastic Gradient Langevin Dy-<br />namics (SGLD) algorithm (Welling &amp; Teh, 2011) which<br />we describe in section 3.1. SGLD can sample accurately<br />from the posterior but suffers from a low mixing rate. In<br />section 3.2, we show that it is easy to construct a Markov<br />chain that can sample from a normal approximation of the<br />posterior at any mixing rate. We will then combine these<br />methods to develop our Stochastic Gradient Fisher Scoring<br />(SGFS) algorithm in section 3.3.<br />3.1. Stochastic Gradient Langevin Dynamics<br />The SGLD algorithm has the following update equation:<br />θt+1← θt+?C<br />2<br />?∇logp(θt) + Ngn(θt;Xt<br />where<br />n)?+ ν<br />ν ∼ N(0,?C) (1)<br />Here ? is the step size, C is called the preconditioning ma-<br />trix (Girolami &amp; Calderhead, 2010) and ν is a random vari-<br />able representing injected Gaussian noise. The gradient of<br />the log likelihood GN(θ;XN) over the whole dataset is ap-<br />proximated by scaling the mean gradient gn(θt;Xt<br />puted from a mini-batch Xt<br />Welling &amp; Teh (2011) showed that Eqn. (1) generates sam-<br />ples from the posterior distribution if the step size is an-<br />nealedtozeroatacertainrate. Asthestepsizegoestozero,<br />the discretization error in the Langevin equation disap-<br />pears and we do not need to conduct expensive Metropolis-<br />Hasting(MH) accept/reject tests that use the whole dataset.<br />Thus, this algorithm requires only O(n) computations to<br />generateeachsample, unliketraditionalMCMCalgorithms<br />which require O(N) computations per sample.<br />However, since the step sizes are reduced to zero, the mix-<br />ing rate is reduced as well, and a large number of iterations<br />are required to obtain a good coverage of the parameter<br />space. One way to make SGLD work at higher step sizes is<br />tointroduceMHaccept/rejectstepstocorrectforthehigher<br />discretization error, but our initial attempts using only a<br />mini-batch instead of the whole dataset were unsuccessful.<br />n) com-<br />n= {xt1...xtn} of size n ? N.<br />3.2. Sampling from the Approximate Posterior<br />Since it is not clear how to use Eqn. (1) at high step sizes,<br />we will move away from Langevin dynamics and explore<br />a different approach. As mentioned in section 2, the poste-<br />rior distribution can be shown to approach a normal distri-<br />bution, N(θ0,I−1<br />large. It is easy to construct a Markov chain which will<br />sample from this approximation of the posterior at any step<br />size. We will now show that the following update equation<br />achieves this:<br />N), as the size of the dataset becomes very<br />θt+1← θt+?C<br />2<br />{−IN(θt− θ0)} + ω<br />ω ∼ N(0,?C −?2<br />where<br />4CINC)<br />(2)</p>  <p>Page 3</p> <p>Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring<br />The update is an affine transformation of θtplus injected<br />independent Gaussian noise, ω. Thus if θthas a Gaussian<br />distribution N(µt,Σt), θt+1will also have a Gaussian dis-<br />tribution, which we will denote as N(µt+1,Σt+1). These<br />distributions are related by:<br />µt+1= (I −?C<br />Σt+1= (I −?C<br />2IN)µt+?C<br />2IN)Σt(I −?C<br />2INθ0<br />2IN)T+ ?C −?2<br />4CINC<br />(3)<br />If we choose C to be symmetric, it is easy to see that the<br />approximate posterior distribution, N(θ0,I−1<br />variant distribution of this Markov chain. Since Eqn. (2) is<br />not a Langevin equation, it samples from the approximate<br />posterior at large step-size and does not require any MH<br />accept/reject steps. The only requirement is that C should<br />be symmetric and should be chosen so that the covariance<br />matrix of the injected noise in Eqn. (2) is positive-definite.<br />N), is an in-<br />3.3. Stochastic Gradient Fisher Scoring<br />In practical problems both sampling accuracy and mixing<br />rate are important, and the extreme regimes dictated by<br />both the above methods are very limiting. If the posterior<br />is close to Gaussian (as is usually the case), we would like<br />to take advantage of the high mixing rate. However, if we<br />need to capture a highly non-Gaussian posterior, we should<br />be able to trade-off mixing rate for sampling accuracy. One<br />could also think about doing this in an “anytime” fashion<br />where if the posterior is somewhat close to Gaussian, we<br />can start by sampling from a Gaussian approximation at<br />high mixing rates, but slow down the mixing rate to capture<br />the non-Gaussian structure if more computation becomes<br />available. In other words, one should have the freedom to<br />manage the right trade off between sampling accuracy and<br />mixing rate depending on the problem at hand.<br />With this goal in mind, we combine the above methods to<br />develop our Stochastic Gradient Fisher Scoring (SGFS) al-<br />gorithm. We accomplish this using a Markov chain with<br />the following update equation:<br />?∇logp(θt) + Ngn(θt;Xt<br />θt+1← θt+?C<br />2<br />n)?+ τ<br />where<br />τ ∼ N(0,Q) (4)<br />When the step size is small, we want to choose Q = ?C so<br />that it behaves like the Markov chain in Eqn (1). Now we<br />will see how to choose Q so that when the step size is large<br />and the posterior is approximately Gaussian, our algorithm<br />behaves like the Markov chain in Eqn. (2). First, note that<br />if n is large enough for the central limit theorem to hold,<br />we have:<br />?<br />gn(θt;Xt<br />n) ∼ N<br />Ex[g(θt;x)],1<br />nCov[g(θt;x)]<br />?<br />(5)<br />Here Cov[g(θt;x)] is the covariance of the scores at<br />θt. Using NCov[g(θt;x)] ≈ IN and NEx[g(θt;x)] ≈<br />GN(θt;XN), we have:<br />∇logp(θt) + Ngn(θt;Xt<br />≈ ∇logp(θt) + GN(θt;XN) + φ<br />where<br />φ ∼ N<br />Now, ∇logp(θt) + GN(θt;XN) = ∇logp(θt|XN), the<br />gradientofthelogposterior. Ifweassumethattheposterior<br />is close to its Bernstein-von Mises approximation, we have<br />∇logp(θt|XN) = −IN(θt− θ0). Using this in Eqn. (6)<br />and then substituting in Eqn. (4), we have:<br />θt+1← θt+?C<br />2<br />where,<br />?<br />Comparing Eqn. (7) and Eqn. (2), we see that at high step<br />sizes, we need:<br />Q +?2<br />4<br />Q = ?C −?2<br />Thus, we should choose Q such that:<br />??C<br />where we have defined γ =<br />when ? is small, we can choose Q = ?C −?2<br />both the cases above. With this, our update equation be-<br />comes:<br />θt+1← θt+?C<br />2<br />?<br />Now, we have to choose C so that the covariance matrix of<br />the injected noise in Eqn. (9) is positive-definite. One way<br />to enforce this, is by setting:<br />?C −?2<br />n)<br />?<br />0,NIN<br />n<br />?<br />(6)<br />{−IN(θt− θ0)} + ψ + τ<br />(7)<br />ψ ∼ N<br />0,?2<br />4<br />N<br />nCINC<br />?<br />and τ ∼ N(0,Q)<br />N<br />nCINC = ?C −?2<br />4CINC ⇒<br />N + n<br />n4<br />CINC<br />(8)<br />Q =<br />for small ?<br />for large ??C −?2<br />4γCINC<br />N+n<br />n. Since ? dominates ?2<br />4γCINC for<br />?∇logp(θt) + Ngn(θt;Xt<br />where τ ∼ N<br />n)?+ τ<br />0,?C −?2<br />4γCINC<br />?<br />(9)<br />4γCINC = ?CBC ⇒ C = 4[?γIN+ 4B]−1<br />(10)<br />where B is any symmetric positive-definite matrix. Plug-<br />ging in this choice of C in Eqn. 9, we get:<br />?<br />{∇logp(θt) + Ngn(θt;Xt) + η}<br />where η ∼ N<br />θt+1← θt+ 2γIN+4B<br />?<br />?−1<br />×<br />?<br />0,4B<br />?<br />?<br />(11)</p>  <p>Page 4</p> <p>Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring<br />However, the above method considers IN to be a known<br />constant. In practice, we use NˆI1,tas an estimate of IN,<br />whereˆI1,t is an online average of the empirical covari-<br />ance of gradients (empirical Fisher information) computed<br />at each θt.<br />ˆI1,t= (1 − κt)ˆI1,t−1+ κtV (θt;Xt<br />where κt= 1/t. In the supplementary material we prove<br />that this online average converges to I1plus O(1/N) cor-<br />rections if we assume that the samples are actually drawn<br />from the posterior:<br />Theorem 1. Consider a sampling algorithm which<br />generates a sample θt from the posterior distribution<br />of the model parameters p(θ|XN) in each itera-<br />tion t.In each iteration, we draw a random mini-<br />batch of size n, Xt<br />n<br />=<br />the empirical covariance of the scores V (θt;Xt<br />1<br />n−1<br />Let VT be the average of V (θt) across T iterations.<br />For large N, as T → ∞, VT converges to the Fisher<br />information I(θ0) plus O(1<br />?<br />T<br />t=1<br />n)<br />(12)<br />{xt1...xtn}, and compute<br />n)=<br />?n<br />i=1{g(θt;xti) − gn(θt)}{g(θt;xti) − gn(θt)}T.<br />N) corrections, i.e.<br />?<br />lim<br />T→∞<br />VT?1<br />T<br />?<br />V (θt;Xt<br />n)= I(θ0) + O(1<br />N) (13)<br />Note that this is not a proof of convergence of the Markov<br />chain to the correct distribution. Rather, assuming that the<br />samples are from the posterior, it shows that the online av-<br />erage of the covariance of the gradients converges to the<br />Fisher information (as desired). Thus, it strengthens our<br />confidencethatifthesamplesarealmostfromtheposterior,<br />the learned pre-conditioner converges to something sensi-<br />ble. What we do know is that if we anneal the stepsizes<br />according to a certain polynomial schedule, and we keep<br />the pre-conditioner fixed, then SGFS is a version of SGLD<br />which was shown to converge to the correct equilibrium<br />distribution (Welling &amp; Teh, 2011). We believe the adap-<br />tation of the Fisher information through an online average<br />is slow enough for the resulting Markov chain to still be<br />valid, but a proof is currently lacking. The theory of adap-<br />tive MCMC (Andrieu &amp; Thoms, 2009) or two time scale<br />stochastic approximations (Borkar, 1997) might hold the<br />key to such a proof which we leave for future work. Putting<br />it all together, we arrive at algorithm 1 below.<br />The general method still has a free symmetric positive-<br />definite matrix, B, which may be chosen according to our<br />convenience. Examine the limit ? → 0. In this case our<br />method becomes SGLD with preconditioning matrix B−1<br />and step size ?.<br />IftheposteriorisGaussian, asisusuallythecasewhenN is<br />large, the proposed SGFS algorithm will sample correctly<br />for arbitrary choice of B even when the step size ? is large.<br />Algorithm 1: Stochastic Gradient Fisher Scoring (SGFS)<br />Input: n, B, {κt}t=1:T<br />Output: {θt}t=1:T<br />1: Initialize θ1,ˆI1,0<br />2: γ ←n+N<br />3: for t = 1 : T do<br />4:<br />Choose random minibatch Xt<br />5:<br />gn(θt) ←1<br />6:<br />V (θt) ←<br />1<br />n−1<br />7:<br />ˆI1,t← (1 − κt)ˆI1,t−1+ κtV (θt)<br />8:<br />Draw η ∼ N[0,4B<br />9:<br />θt+1← θt+<br />2 γNˆI1,t+4B<br />?<br />{∇logp(θt) + Ngn(θt) + η}<br />10: end for<br />n<br />n= {xt1...xtn}<br />n<br />?n<br />i=1gti(θt)<br />?n<br />i=1{gti(θt) − gn(θt)}{gti(θt) − gn(θt)}T<br />?]<br />?−1<br />?<br />However, for some models the conditions of the Bernstein-<br />von Mises theorem are violated and the posterior may not<br />be well approximated by a Gaussian. This is the case for<br />e.g. neural networks and discriminative RBMs, where the<br />identifiability condition of the parameters do not hold. In<br />this case, we have to choose a small ? to achieve accurate<br />sampling (see section 5). These two extremes can be com-<br />bined in a single “anytime” algorithm by slowly annealing<br />the stepsize. For a non-adaptive version of our algorithm<br />(i.e. where we would stop changingˆI1) after a fixed num-<br />ber of iterations) this would according to the results from<br />Welling and Teh (2011) lead to a valid Markov chain for<br />posterior sampling.<br />We recommend choosing B ∝ IN. With this choice, our<br />method is highly reminiscent of “Fisher scoring” which<br />is why we named it “Stochastic Gradient Fisher Scoring”<br />(SGFS). In fact we can think of the proposed updates as a<br />stochastic version of Fisher scoring based on small mini-<br />batches of gradients. But remarkably, the proposed algo-<br />rithm is not only much faster than Fisher scoring (because<br />it only requires small minibatches to compute an update), it<br />also samples approximately from the posterior distribution.<br />So the knife cuts on both sides: SGFS is a faster optimiza-<br />tion algorithm but also doesn’t overfit due to the fact that<br />it switches to sampling when the right statistical scale of<br />precision is reached.<br />4. Computational Efficiency<br />Clearly, the main computational benefit relative to stan-<br />dard MCMC algorithms comes from the fact that we use<br />stochastic minibatches instead of the entire dataset at every<br />iteration. However, for a model with a large number of pa-<br />rameters another source of significant computational effort<br />is the computation of the D × D matrix γNˆI1,t+4B<br />?and</p>  <p>Page 5</p> <p>Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring<br />multiplying its inverse with the mean gradient resulting in a<br />total computational complexity of O(D3) per iteration. In<br />the case n &lt; D the computational complexity per iteration<br />can be brought down to O(nD2) by using the Sherman-<br />Morrison-Woodbury equation. A more numerically stable<br />alternative is to update Cholesky factors (Seeger, 2004).<br />In case even this is infeasible one can factor the Fisher in-<br />formation into k independent blocks of variables of, say<br />size d, in which case we have brought down the complexity<br />to O(kd3). The extreme case of this is when we treat every<br />parameter as independent which boils down to replacing<br />the Fisher information by a diagonal matrix with the vari-<br />ances of the individual parameters populating the diagonal.<br />While for a large stepsize this algorithm will not sample<br />from the correct Gaussian approximation, it will still sam-<br />ple correctly from the posterior for very small stepsizes. In<br />fact, it is expected to do this more efficiently than SGLD<br />which does not rescale its stepsizes at all. We have used<br />thefullcovariancealgorithm(SGFS-f)andthediagonalco-<br />variance algorithm (SGFS-d) in the experiments section.<br />5. Experiments<br />Below we report experimental results where we test SGFS-<br />f, SGFS-d, SGLD, SGD and HMC on three different mod-<br />els: logistic regression, neural networks and discriminative<br />RBMs. The experiments share the following practice in<br />common. Stepsizes for SGD and SGLD are always se-<br />lected through cross-validation for at least five settings.<br />The minibatch size n is set to either 300 or 500, but the<br />results are not sensitive to the precise value as long as it<br />is large enough for the central limit theorem to hold (typi-<br />cally, n &gt; 100 is recommended). Also, we used κt=1<br />t.<br />5.1. Logistic Regression<br />A logistic regression model (LR) was trained on the<br />MNIST dataset for binary classification of two digits 7 and<br />9 using a total of 10,000 data-items. We used a 50 dimen-<br />sional random projection of the original features and ran<br />SGFS with λ = 1. We used B = γINand tested the algo-<br />rithm for a number of α values (where α =<br />thealgorithmfor3,000burn-initerationsandthencollected<br />100,000 samples. We compare the algorithm to Hamil-<br />tonian Monte Carlo sampling (Neal, 1993) and to SGLD<br />(Welling &amp; Teh, 2011). For HMC, the “leapfrogstep” size<br />was adapted during burn-in so that the acceptance ratio was<br />around 0.8. For SGLD we also used a range of fixed step-<br />sizes.<br />2<br />√?). We ran<br />In figure 1 we show 2-d marginal distributions of SGFS<br />compared to the ground truth from a long HMC run where<br />we used α = 0 for SGFS. From this we conclude that even<br />for the largest possible stepsize the fit for SGFS-f is al-<br />0.50.6<br />SGFS−f (BEST)<br />0.70.8<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />0.35<br />0.4<br />0.45<br />−0.4−0.3 −0.2 −0.1<br />−0.75<br />−0.7<br />−0.65<br />−0.6<br />−0.55<br />−0.5<br />−0.45<br />−0.4<br />SGFS−f (WORST)<br />0.5 0.6<br />SGFS−d (BEST)<br />0.7 0.8<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />0.35<br />0.4<br />0.45<br />−0.4−0.3 −0.2−0.1<br />−0.75<br />−0.7<br />−0.65<br />−0.6<br />−0.55<br />−0.5<br />−0.45<br />−0.4<br />SGFS−d (WORST)<br />Figure 1. 2-d marginal posterior distributions for logistic regres-<br />sion. Grey colors correspond to samples from SGFS. Red solid<br />and blue dotted ellipses represent iso-probability contours at two<br />standard deviations away from the mean computed from HMC<br />and SGFS, respectively. Top plots are the results for SGFS-f and<br />bottom plots represent SGFS-d. Plots on the left represent the 2-d<br />marginals with the smallest difference between HMC and SGFS<br />while the plots on the right represent the 2-d marginals with the<br />largest difference. Value for α is 0 meaning that no additional<br />noise was added.<br />most perfect while SGFS-d underestimates the variance in<br />this case (note however that for smaller stepsizes (larger α)<br />SGFS-d becomes very similar to SGLD and is thus guaran-<br />teed to sample correctly albeit with a low mixing rate).<br />Next, we studied the inverse autocorrelation time per unit<br />computation (ATUC)1averaged over the 51 parameters and<br />compared this with the relative error after a fixed amount of<br />computation time. The relative error is computed as fol-<br />lows: first we compute the mean and covariance of the<br />parameter samples up to time t : θt=<br />Ct=<br />t<br />the long HMC run which we indicate with θ∞and C∞.<br />Finally we compute<br />1<br />t<br />?t<br />t?=1θt? and<br />1<br />?t<br />t?=1(θt? − θt)(θt? − θt)T. We do the same for<br />E1t=<br />?<br />i|θt<br />?<br />i− θ∞<br />i|θ∞<br />i|<br />i|<br />,E2t=<br />?<br />ij|Ct<br />?<br />ij− C∞<br />ij|C∞<br />ij|<br />ij|<br />(14)<br />In Figure 2 we plot the “Error at time T” for two val-<br />ues of T (T=100, T=3000) as a function of the inverse<br />ATUC, which is a measure of the mixing rate. Top plots<br />show the results for the mean and bottom plots for the<br />covariance. Each point denoted by a cross is obtained<br />1ATUC = Autocorrelation Time × Time per Sample. Auto-<br />correlation time is defined as 1 + 2?∞<br />s=1ρ(s) with ρ(s) the au-<br />tocorrelation at lag s Neal (1993).</p>  <p>Page 6</p> <p>Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring<br />10<br />−2<br />10<br />−1<br />10<br />0<br />10<br />1<br />10<br />−3<br />10<br />−2<br />10<br />−1<br />Relative Error in Mean at 100 sec.<br />Mixing Rate (1/ATUC)<br />SGLD<br />HMC<br />SGFS−f<br />SGFS−d<br />10<br />−2<br />10<br />−1<br />10<br />0<br />10<br />1<br />10<br />−3<br />10<br />−2<br />10<br />−1<br />Relative Error in Mean at 3000 sec.<br />Mixing Rate (1/ATUC)<br />HMC<br />SGLD<br />SGFS−f<br />SGFS−d<br />10<br />−2<br />10<br />0<br />10<br />−1<br />10<br />0<br />Relative Error in Cov at 100 sec.<br />Mixing Rate (1/ATUC)<br />HMC<br />SGLD<br />SGFS−d<br />SGFS−f<br />10<br />−2<br />10<br />0<br />10<br />−1<br />10<br />0<br />Relative Error in Cov at 3000 sec.<br />Mixing Rate (1/ATUC)<br />HMC<br />SGLD<br />SGFS−d<br />SGFS−f<br />Figure 2. Final error of logistic regression at time T versus mixing<br />rate for the mean (top) and covariance (bottom) estimates after<br />100 (left) and 3000 (right) seconds of computation. See main text<br />for detailed explanation.<br />from a different setting of parameters that control the mix-<br />ing rate: α = [0,1,2,3,4,5,6] for SGFS, stepsizes ? =<br />[1e−3,5e−4,1e−4,5e−5,1e−5,5e−6,1e−6]forSGLD,<br />and number of leapfrog steps s = [50,40,30,20,10,1] for<br />HMC. The circle is the result for the fastest mixing chain.<br />For SGFS and SGLD, if the slope of the curve is nega-<br />tive (downward trend) then the corresponding algorithm<br />was still in the phase of reducing error by reducing sam-<br />pling variance at time T. However, when the curve bends<br />upwards and develops a positive slope the algorithm has<br />reached its error floor corresponding to the approximation<br />bias. The situation is different for HMC, (which has no<br />bias) but where the bending occurs because the number of<br />leapfrog steps has become so large that it is turning back<br />on itself. HMC is not faring well because it is computa-<br />tionally expensive to run (which hurts both its mixing rate<br />and error at time T). We also observe that in the allowed<br />running time SGFS-f has not reached its error floor (both<br />for the mean and the covariance). SGFS-d is reaching its<br />error floor only for the covariance (which is consistent with<br />Figure 1 bottom) but still fares well in terms of the mean.<br />Finally, for SGLD we clearly see that in order to obtain a<br />high mixing rate (low ATUC) it has to pay the price of a<br />large bias. These plots clearly illustrate the advantage of<br />SGFS over both HMC as well as SGLD.<br />5.2. SGFS on Neural Networks<br />We also applied our methods to a 3 layer neural network<br />(NN) with logistic activation functions. Below we describe<br />classification results for two datasets.<br />104<br />105<br />10-0.35<br />10-0.34<br />10-0.33<br />10-0.32<br />Iteration Number (Log)<br />Classification Error (Log)<br /> <br /> <br />SGD-Avg ( = 0)<br />SGD-Avg ( = 0.1)<br />SGLD-Avg ( = 0)<br />SGLD-Avg ( = 0.1)<br />SGFS-d-Avg ( = 0,  = 6)<br />SGFS-d-Avg ( = 0.1,  = 6)<br />SGD<br />SGFS-d<br />SGLD<br />10<br />0<br />10<br />2<br />10<br />4<br />10<br />-0.7<br />10<br />-0.5<br />10<br />-0.3<br />10<br />-0.1<br />Iteration Number (Log)<br />Classification Error (Log)<br /> <br /> <br />SGFS-f-Avg ( = 0.001,  = 2)<br />SGFS-d-Avg ( = 0.001,  = 2)<br />SGLD-Avg ( = 0.001,  = [10-3,10-7])<br />SGD-Avg ( = 0.001,  = [10-3,10-7])<br />SGLD<br />SGD<br />SGFS-f<br />SGFS-d<br />Figure 3. Test-set classification error of NNs trained with SGFS-<br />f, SGFS-d, SGLD and SGD on the HHP dataset (left) and the<br />MNIST dataset (right)<br />5.2.1. HERITAGE HEALTH PRIZE (HHP)<br />The goal of this competition is to predict how many days<br />between [0 − 15] a person will stay in a hospital given<br />his/her past three years of hospitalization records2. We<br />used the same features as the team market makers that won<br />the first milestone prize. Integrating the first and second<br />year data, we obtained 147,473 data-items with 139 fea-<br />ture dimensions and then used a randomly selected 70%<br />for training and the remainder for testing. NNs with 30<br />hidden units were used because more hidden units did not<br />noticeably improve the results. Although we used α = 6<br />for SGFS-d, there was no significant difference for values<br />in the range 3 ≤ α ≤ 6. However, α &lt; 3 did not work for<br />this dataset due to the fact that many features had values 0.<br />For SGD, we used stepsizes from a polynomial anneal-<br />ing schedule a(b + t)−δ. Because the training error de-<br />creased slowly in a valid range δ = [0.5,1], we used δ = 3,<br />a = 1014, b = 2.2 × 105instead which was found optimal<br />through cross-validation. (This setting reduced the stepsize<br />from 10−2to 10−6during 1e+7 iterations). For SGLD,<br />a = 1, b = 104, and δ = 1 reducing the step size from<br />10−4to 10−6was used. Figure 3 (left) shows the classi-<br />fication errors averaged over the posterior samples for two<br />regularizer values, λ = 0 and the best regularizer value λ<br />found through cross-validation. First, we clearly see that<br />SGD severely overfits without a regularizer while SGLD<br />and SGFS prevent it because they average predictions over<br />samples from a posterior mode. Furthermore, we see that<br />when the best regularizer is used, SGFS (marginally) out-<br />performs both SGD and SGLD. The result from SGFS-d<br />submitted to the actual competition leaderboard gave us an<br />error of 0.4635 which is comparable to 0.4632 obtained by<br />the milestone winner with a fine-tuned Gradient Boosting<br />Machine.<br />2http://www.heritagehealthprize.com</p>  <p>Page 7</p> <p>Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring<br />−2.2−2−1.8<br />SGFS−f (BEST)<br />−1.6−1.4 −1.2−1−0.8−0.6<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />0.8<br />0.9<br />1<br />−0.25−0.2<br />SGFS−f (WORST)<br />−0.15−0.1−0.0500.05 0.10.15<br />−0.2<br />−0.15<br />−0.1<br />−0.05<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />−2.2−2−1.8−1.6−1.4−1.2−1−0.8−0.6<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />0.8<br />0.9<br />1<br />SGLD (BEST)<br />−2−1.5−1−0.50 0.5<br />−4<br />−3.5<br />−3<br />−2.5<br />−2<br />−1.5<br />−1<br />−0.5<br />0<br />0.5<br />SGLD (WORST)<br />Figure 4. 2-d marginal posterior distributions of DRBM. Grey<br />colors correspond to samples from SGFS/SGLD. Thick red solid<br />lines correspond to iso-probability contours at two standard de-<br />viations away from the mean computed from HMC samples.<br />Thin red solid lines correspond to HMC results based on sub-<br />sets of the samples. The thick blue dashed lines correspond to<br />SGFS-f (top) and SGLD (bottom) runs. Plots on the left repre-<br />sent the 2-d marginals with the smallest difference between HMC<br />and SGFS/SGLD while the plots on the right represent the 2-d<br />marginals with the largest difference.<br />5.2.2. CHARACTER RECOGNITION<br />We also tested our methods on the MNIST dataset for 10<br />digit classification which has 60,000 training instances and<br />10,000 test instances. In order to test with SGFS-f, we used<br />inputs from 20 dimensional random projections and 30 hid-<br />den units so that the number of parameters equals 940.<br />Moreover, we increased the mini-batch size to 2,000 to re-<br />ducethe timerequiredtoreach agoodapproximation ofthe<br />940×940 covariance matrix. The classification error aver-<br />aged over the samples is shown in Figure 3 (right). Here,<br />we used a small regularization parameter of λ = 0.001<br />for all methods as overfitting was not an issue. For SGFS,<br />α = 2 is used while for both SGD and SGLD the stepsizes<br />were annealed from 10−3to 10−7using a = 1, b = 1000,<br />and γ = 1.<br />5.3. Discriminative Restricted Boltzmann Machine<br />(DRBM)<br />We trained a DRBM (Larochelle &amp; Bengio, 2008) on the<br />KDD99 dataset which consists of 4,898,430 datapoints<br />with 40 features, belonging to a total of 23 classes. We<br />first tested the classification performance by training the<br />DRBM using SGLD, SGFS-f, SGFS-d and SGD. For this<br />experiment the dataset was divided into a 90% training set,<br />5% validation and 5% test set. We used 41 hidden units<br />giving us a total of 2647 parameters in the model. We used<br />0.51 1.52 2.53 3.544.5<br />?5<br />x 10<br />0.05<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />0.35<br />0.4<br />Rel. Err. in Mean at 6790 sec.<br />Mixing Rate (1/ATUC)<br />SGFS?f<br />SGLD<br />0.51 1.522.53 3.54 4.5<br />?5<br />x 10<br />1<br />2<br />3<br />4<br />5<br />6<br />7<br />Rel. Err. in Cov at 6790 sec.<br />Mixing Rate (1/ATUC)<br />SGFS?f<br />SGLD<br />Figure 5. Final error for DRBM at time T versus mixing rate for<br />the mean (left) and covariance (right) estimates after 6790 sec-<br />onds of computation on a subset of KDD99.<br />SGD<br />8.010−4<br />SGLD<br />6.610−4<br />SGFS-d<br />4.210−4<br />SGFS-f<br />4.410−4<br />Table 1. Final test error rate on the KDD99 dataset.<br />λ = 10 and B = γIN. We tried 6 different (α,?) com-<br />binations for SGFS-f and SGFS-d and tried 18 annealing<br />schedules for SGD and SGLD, and used the validation set<br />to pick the best one. The best results were obtained with an<br />α value of 8.95 for SGFS-f and SGFS-d, and [a = 0.1, b =<br />100000, δ = 0.9] for SGD and SGLD. We ran all algorithms<br />for 100,000 iterations. Although we experimented with dif-<br />ferent burn-in iterations, the algorithms were insensitive to<br />this choice. The final error rates are given in table 1 from<br />which we conclude that the samplers based on stochastic<br />gradients can act as effective optimizers whereas HMC on<br />the full dataset becomes completely impractical because it<br />has to compute 11.7 billion gradients per iteration which<br />takes around 7.5 minutes per sample (4408587 datapoints<br />× 2647 parameters).<br />To compare the quality of the samples drawn after burn-in,<br />we created a 10% subset of the original dataset. This time<br />we picked only the 6 most populous classes. We tested all<br />algorithms with 41, 10 and 5 hidden units, but since the<br />posterior is highly multi-modal, the different algorithms<br />ended up sampling from different modes. In an attempt<br />to get a meaningful comparison, we therefore reduced the<br />number of hidden units to 2. This improved the situation to<br />some degree, but did not entirely get rid of the multi-modal<br />and non-Gaussian structure of the posterior. We compare<br />results of SGFS-f/SGLD with 30 independent HMC runs,<br />each providing 4000 samples for a total of 120,000 sam-<br />ples. Since HMC was very slow (even on the reduced set)<br />we initialized at a mode and used the Fisher information<br />at the mode as a pre-conditioner. We used 1 leapfrog step<br />and tuned the step-size to get an acceptance rate of 0.8.<br />We ran SGFS-f with α = [2,3,4,5,10] and SGLD with<br />fixed step sizes of [5e-4, 1e-4, 5e-5, 1e-5, 5e-6]. Both<br />algorithms were initialized at the same mode and ran for<br />1 million iterations. We looked at the marginal distribu-</p>  <p>Page 8</p> <p>Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring<br />tions of the top 25 pairs of variables which had the highest<br />correlation coefficient. In Figure 4 (top-left and bottom-<br />left) we show a set of parameters where both SGFS-f and<br />SGLD obtained an accurate estimate of the marginal poste-<br />rior. In 4 (top-right and bottom-right) we show an example<br />where SGLD failed. The thin solid red lines correspond<br />to HMC runs computed from various subsets of the sam-<br />ples, whereas the thick solid red line is computed using the<br />all samples from all HMC runs. We have shown marginal<br />posterior estimates of the SGFS-f/SGLD algorithms with a<br />thick dashed blue ellipse. After inspection, it seemed that<br />the posterior structure was highly non-Gaussian with re-<br />gions where the probability very sharply decreased. SGLD<br />regularly stepped into these regions and then got catapulted<br />away due to the large gradients there. SGFS-f presumably<br />avoided those regions by adapting to the local covariance<br />structure. We found that in this region even the HMC runs<br />are not consistent with one another. Note that the SGFS-f<br />contours seem to agree with the HMC contours as much as<br />the HMC contours agree with the results of its own subsets,<br />in both the easy and the hard case.<br />Finally, we plot the error after 6790 seconds of computa-<br />tion versus the mixing rate. Figure 5-left shows the results<br />for the mean and the right for the covariance (for an ex-<br />planation of the various quantities see discussion in section<br />5.1). We note again that SGLD incurs a significantly larger<br />approximation bias at the same mixing rate as SGFS-f.<br />6. Conclusions<br />We have introduced a novel method, “Stochastic Gradient<br />Fisher Scoring” (SGFS) for approximate Bayesian learn-<br />ing. The main idea is to use stochastic gradients in the<br />Langevin equation and leverage the central limit theorem<br />to estimate the noise induced by the subsampling process.<br />This subsampling noise is combined with artificially in-<br />jected noise and multiplied by the estimated inverse Fisher<br />information matrix to approximately sample from the pos-<br />terior. This leads to the following desirable properties.<br />• Unlike regular MCMC methods, SGFS is fast because it<br />uses only stochastic gradients based on small mini-batches<br />to draw samples.<br />• Unlike stochastic gradient descent, SGFS samples (ap-<br />proximately) from the posterior distribution.<br />• Unlike SGLD, SGFS samples from a Gaussian approx-<br />imation of the posterior distribution (that is correct for<br />N → ∞) for large stepsizes.<br />• By annealing the stepsize, SGFS becomes an any-<br />time method capturing more non-Gaussian structure with<br />smaller stepsizes but at the cost of slower mixing.<br />• During its burn-in phase, SGFS is an efficient optimizer<br />because like Fisher scoring and Gauss-Newton methods, it<br />is based on the natural gradient.<br />For an appropriate annealing schedule, SGFS thus goes<br />through three distinct phases: 1) during burn-in we use<br />a large stepsize and the method is similar to a stochastic<br />gradient version of Fisher scoring, 2) when the stepsize<br />is still large, but when we have reached the mode of the<br />distribution, SGFS samples from the asymptotic Gaussian<br />approximation of the posterior, and 3) when the stepsize<br />is further annealed, SGFS will behave like SGLD with a<br />pre-conditioning matrix and generate increasingly accurate<br />samples from the true posterior.<br />Acknowledgements<br />This material is based upon work supported by the National Sci-<br />ence Foundation under Grant No. 0447903, 0914783, 0928427.<br />References<br />Andrieu, C. and Thoms, J. A tutorial on adaptive mcmc. Statistics<br />and Computing, 18(4):343–373, 2009.<br />Borkar, V.S. Stochastic approximation with two time scales. Sys-<br />tems and Control Letters, 29(5):291–294, 1997.<br />Bottou, L. and Bousquet, O. The tradeoffs of large scale learn-<br />ing. In Advances in Neural Information Processing Systems,<br />volume 20, pp. 161–168, 2008.<br />Girolami, M. and Calderhead, B. Riemann manifold langevin<br />and hamiltonian monte carlo. Journal of the Royal Statistical<br />Society B, 73 (2):1–37, 2010.<br />Larochelle, H. and Bengio, Y. Classification using discriminative<br />Restricted Boltzmann Machines. In Proceedings of the 25th<br />International Conference on Machine learning, pp. 536–543.<br />ACM, 2008.<br />Le Cam, L.M. Asymptotic methods in statistical decision theory.<br />Springer, 1986.<br />Neal, R.M. Probabilistic inference using markov chain monte<br />carlo methods. Technical Report CRG-TR-93-1, University of<br />Toronto, Computer Science, 1993.<br />Schraudolph, N. N., Yu, J., and G¨ unter, S. A stochastic quasi-<br />Newton method for online convex optimization. In Meila, Ma-<br />rina and Shen, Xiaotong (eds.), Proc. 11thIntl. Conf. Artificial<br />Intelligence and Statistics (AIstats), pp. 436–443, San Juan,<br />Puerto Rico, 2007.<br />Scott, W.A. Maximum likelihood estimation using the empirical<br />fisher information matrix. Journal of Statistical Computation<br />and Simulation, 72(8):599–611, 2002.<br />Seeger, M.<br />sition.<br />ley, 2004.<br />cholupdate.shtml.<br />Low rank updates for the cholesky decompo-<br />Technical report, University of California Berke-<br />URL http://lapmal.epfl.ch/papers/<br />Welling, M. and Teh, Y.W. Bayesian learning via stochastic gra-<br />dient langevin dynamics. In Proceedings of the 28th Interna-<br />tional Conference on Machine Learning (ICML), pp. 681–688,<br />2011.</p>   </div> <div id="rgw19_56ab1c85d5e07" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw20_56ab1c85d5e07">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw21_56ab1c85d5e07"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://arxiv.org/pdf/1206.6380" target="_blank" rel="nofollow" class="publication-viewer" title="Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring">Bayesian Posterior Sampling via Stochastic Gradien...</a> </div>  <div class="details">   Available from <a href="http://arxiv.org/pdf/1206.6380" target="_blank" rel="nofollow">arxiv.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw28_56ab1c85d5e07" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (31) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw29_56ab1c85d5e07" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw30_56ab1c85d5e07" >  <div class="indent-left">  <div id="rgw31_56ab1c85d5e07" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/288059869_Preconditioned_Stochastic_Gradient_Langevin_Dynamics_for_Deep_Neural_Networks">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1512.07666" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: de.arxiv.org </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw32_56ab1c85d5e07">  <li class="citation-context-item"> "This encourages the algorithm to explore the full posterior, instead of simply converging to a maximum a posterior (MAP) solution. Later, SGLD was extended by (Ahn, Korattikara, and Welling 2012), (Patterson and Teh 2013) and (Korattikara et al. 2015). Furthermore , higher-order versions of the SGLD with momentum have also been proposed, including stochastic " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/288059869_Preconditioned_Stochastic_Gradient_Langevin_Dynamics_for_Deep_Neural_Networks"> <span class="publication-title js-publication-title">Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2081618664_Chunyuan_Li" class="authors js-author-name ga-publications-authors">Chunyuan Li</a> &middot;     <a href="researcher/2075738569_Changyou_Chen" class="authors js-author-name ga-publications-authors">Changyou Chen</a> &middot;     <a href="researcher/71191078_David_Carlson" class="authors js-author-name ga-publications-authors">David Carlson</a> &middot;     <a href="researcher/10135830_Lawrence_Carin" class="authors js-author-name ga-publications-authors">Lawrence Carin</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Effective training of deep neural networks suffers from two main issues. The
first is that the parameter spaces of these models exhibit pathological
curvature. Recent methods address this problem by using adaptive
preconditioning for Stochastic Gradient Descent (SGD). These methods improve
convergence by adapting to the local geometry of parameter space. A second
issue is overfitting, which is typically addressed by early stopping. However,
recent work has demonstrated that Bayesian model averaging mitigates this
problem. The posterior can be sampled by using Stochastic Gradient Langevin
Dynamics (SGLD). However, the rapidly changing curvature renders default SGLD
methods inefficient. Here, we propose combining adaptive preconditioners with
SGLD. In support of this idea, we give theoretical properties on asymptotic
convergence and predictive risk. We also provide empirical results for Logistic
Regression, Feedforward Neural Nets, and Convolutional Neural Nets,
demonstrating that our preconditioned SGLD method gives state-of-the-art
performance on these models. </span> </div>    <div class="publication-meta publication-meta">  <span class="ico-publication-preview reset-background"></span> Preview    &middot; Article &middot; Dec 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw33_56ab1c85d5e07" >  <div class="indent-left">  <div id="rgw34_56ab1c85d5e07" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/283043210_A_General_Method_for_Robust_Bayesian_Modeling">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1510.05078" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: de.arxiv.org </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw35_56ab1c85d5e07">  <li class="citation-context-item"> "We use a model to encode the types of patterns we want to discover in the data—either to predict about future data or explore existing data—and then use a posterior inference algorithm to uncover the realization of those patterns that underlie the observations. Innovations in scalable inference allow us to use Bayesian models to analyze massive data (Hoffman et al., 2013; Welling and Teh, 2011; Ahn et al., 2012; Xing et al., 2013); innovations in generic inference allow us to easily explore a wide variety of models (Ranganath et al., 2014; Wood et al., 2014; Hoffman and Gelman, 2014). Consequently, modern Bayesian modeling has had an impact on many fields, including natural language processing (Teh, 2006), computer vision (Fei-Fei and Perona, 2005), the natural sciences (Pritchard et al., 2000), and the social sciences (Grimmer, 2009). " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/283043210_A_General_Method_for_Robust_Bayesian_Modeling"> <span class="publication-title js-publication-title">A General Method for Robust Bayesian Modeling</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2083133647_Chong_Wang" class="authors js-author-name ga-publications-authors">Chong Wang</a> &middot;     <a href="researcher/2064238818_David_M_Blei" class="authors js-author-name ga-publications-authors">David M. Blei</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Robust Bayesian models are appealing alternatives to standard models,
providing protection from data that contains outliers or other departures from
the model assumptions. Historically, robust models were mostly developed on a
case-by-case basis; examples include robust linear regression, robust mixture
models, and bursty topic models. In this paper we develop a general approach to
robust Bayesian modeling. We show how to turn an existing Bayesian model into a
robust model, and then develop a generic strategy for computing with it. We use
our method to study robust variants of several models, including linear
regression, Poisson regression, logistic regression, and probabilistic topic
models. We discuss the connections between our methods and existing approaches,
especially empirical Bayes and James-Stein estimation. </span> </div>    <div class="publication-meta publication-meta">  <span class="ico-publication-preview reset-background"></span> Preview    &middot; Article &middot; Oct 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw36_56ab1c85d5e07" >  <div class="indent-left">  <div id="rgw37_56ab1c85d5e07" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/David_Dunson" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: David B Dunson </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw38_56ab1c85d5e07">  <li class="citation-context-item"> "Relying on stochastic approximation, most of these approaches use stochastic gradient descent for optimization (Welling and Teh, 2011; Broderick et al., 2013; Hoffman et al., 2013). Optimization updates are also coupled with sampling using modified Hamiltonian or Langevin Dynamics to improve posterior exploration (Ahn et al., 2012; Korattikara et al., 2014). Recent work in this area has focused on using different factorizations and parametrizations to improve the efficiency of existing methods (Wang and Blei, 2013; Tan and Nott, 2013; Wand, 2014). " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space"> <span class="publication-title js-publication-title">Scalable Bayes via Barycenter in Wasserstein Space</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2045181100_Sanvesh_Srivastava" class="authors js-author-name ga-publications-authors">Sanvesh Srivastava</a> &middot;     <a href="researcher/2084332535_Cheng_Li" class="authors js-author-name ga-publications-authors">Cheng Li</a> &middot;     <a href="researcher/9530149_David_B_Dunson" class="authors js-author-name ga-publications-authors">David B. Dunson</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> We propose a novel approach WASP for Bayesian inference when massive size of
the data prohibits posterior computations. WASP is estimated in three steps.
First, data are divided into smaller computationally tractable subsets. Second,
posterior draws of parameters are obtained for every subset after modifying
subset posteriors using stochastic approximation. Finally, the empirical
measures of samples from each subset posterior are combined through their
barycenter in the Wasserstein space of probability measures. Stochastic
approximation ensures that posterior uncertainty quantification of the
barycenter matches with that of the full data posterior distribution. The
combining step can be conducted efficiently through a sparse linear program,
which takes negligible time relative to sampling from subset posteriors,
facilitating scaling to massive data. WASP is very general and allows
application of existing sampling algorithms to massive data with minimal
modifications. We provide theoretical conditions under which rate of
convergence of WASP to the delta measure centered at the true parameter
coincides with the optimal parametric rate up to a logarithmic factor. WASP is
applied for scalable Bayesian computations in a nonparametric mixture model and
a movie recommender database containing tens of millions of ratings. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Aug 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/David_Dunson/publication/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space/links/55ffe48708ae07629e51e264.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw23_56ab1c85d5e07" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw24_56ab1c85d5e07">  </ul> </div> </div>   <div id="rgw15_56ab1c85d5e07" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw16_56ab1c85d5e07"> <div> <h5> <a href="publication/286711053_Distributed_Bayesian_posterior_sampling_via_moment_sharing" class="color-inherit ga-similar-publication-title"><span class="publication-title">Distributed Bayesian posterior sampling via moment sharing</span></a>  </h5>  <div class="authors"> <a href="researcher/2088669749_M_Xu" class="authors ga-similar-publication-author">M. Xu</a>, <a href="researcher/2088510817_B_Lakshminarayanan" class="authors ga-similar-publication-author">B. Lakshminarayanan</a>, <a href="researcher/2088558800_YW_Teh" class="authors ga-similar-publication-author">Y.W. Teh</a>, <a href="researcher/2088675909_J_Zhu" class="authors ga-similar-publication-author">J. Zhu</a>, <a href="researcher/2088464513_B_Zhang" class="authors ga-similar-publication-author">B. Zhang</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56ab1c85d5e07"> <div> <h5> <a href="publication/221346425_Bayesian_Learning_via_Stochastic_Gradient_Langevin_Dynamics" class="color-inherit ga-similar-publication-title"><span class="publication-title">Bayesian Learning via Stochastic Gradient Langevin Dynamics</span></a>  </h5>  <div class="authors"> <a href="researcher/69847505_Max_Welling" class="authors ga-similar-publication-author">Max Welling</a>, <a href="researcher/9164246_Yee_Whye_Teh" class="authors ga-similar-publication-author">Yee Whye Teh</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56ab1c85d5e07"> <div> <h5> <a href="publication/272844713_Privacy_for_Free_Posterior_Sampling_and_Stochastic_Gradient_Monte_Carlo" class="color-inherit ga-similar-publication-title"><span class="publication-title">Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo</span></a>  </h5>  <div class="authors"> <a href="researcher/2047781529_Yu-Xiang_Wang" class="authors ga-similar-publication-author">Yu-Xiang Wang</a>, <a href="researcher/9777225_Stephen_E_Fienberg" class="authors ga-similar-publication-author">Stephen E. Fienberg</a>, <a href="researcher/3222757_Alex_Smola" class="authors ga-similar-publication-author">Alex Smola</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw40_56ab1c85d5e07" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw41_56ab1c85d5e07">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw42_56ab1c85d5e07" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=Fxu9Sza8I_OYmU-03QaUFHLXvgqpCYSWE4aqhgk85SEEt6Kh0572WUQV0fxwNJcg" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="ceSWGSoYFYcYBU2LTcgPy+QAJ0EkiaTiukggyZOufVgeo3Tg1S7vWd2sF+ENv42FuJQV8YEpnx7jEK7Rn5zXbjhHKaixBthXNpxMC3X+Wp7/5YIUZPI5ftgfPi5Fd3IjV4YOYL80KEyC6zwmOqXP8zGMjpDoy5BlU6G9HOqERqyrq9jiaCsL+fjRiEgywvQCepdoi/lqZkSbkOk5Tdc0OSaCbGOFl6QQpKs0LxeDoHH5UIAdE2EdxLya6mXmMeLAwyJT9rIe/JstCYeJGglj/g0NLV7+hj4zYW63UPNzhQY="/> <input type="hidden" name="urlAfterLogin" value="publication/228095582_Bayesian_Posterior_Sampling_via_Stochastic_Gradient_Fisher_Scoring"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjI4MDk1NTgyX0JheWVzaWFuX1Bvc3Rlcmlvcl9TYW1wbGluZ192aWFfU3RvY2hhc3RpY19HcmFkaWVudF9GaXNoZXJfU2NvcmluZw%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjI4MDk1NTgyX0JheWVzaWFuX1Bvc3Rlcmlvcl9TYW1wbGluZ192aWFfU3RvY2hhc3RpY19HcmFkaWVudF9GaXNoZXJfU2NvcmluZw%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjI4MDk1NTgyX0JheWVzaWFuX1Bvc3Rlcmlvcl9TYW1wbGluZ192aWFfU3RvY2hhc3RpY19HcmFkaWVudF9GaXNoZXJfU2NvcmluZw%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw43_56ab1c85d5e07"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 960;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Sungjin Ahn","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Sungjin_Ahn","institution":"University of California, Irvine","institutionUrl":false,"widgetId":"rgw4_56ab1c85d5e07"},"id":"rgw4_56ab1c85d5e07","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=4026253","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab1c85d5e07"},"id":"rgw3_56ab1c85d5e07","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=228095582","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":228095582,"title":"Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"06\/2012;","publicationDateRobot":"2012-06","article":""}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1206.6380","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring"},{"key":"rft.date","value":"2012"},{"key":"rft.au","value":"Sungjin Ahn,Anoop Korattikara,Max Welling"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56ab1c85d5e07"},"id":"rgw6_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=228095582","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":228095582,"peopleItems":[{"data":{"authorNameOnPublication":"Sungjin Ahn","accountUrl":"profile\/Sungjin_Ahn","accountKey":"Sungjin_Ahn","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Sungjin Ahn","profile":{"professionalInstitution":{"professionalInstitutionName":"University of California, Irvine","professionalInstitutionUrl":"institution\/University_of_California_Irvine"}},"professionalInstitutionName":"University of California, Irvine","professionalInstitutionUrl":"institution\/University_of_California_Irvine","url":"profile\/Sungjin_Ahn","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Sungjin_Ahn","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw9_56ab1c85d5e07"},"id":"rgw9_56ab1c85d5e07","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=4026253&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University of California, Irvine","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":1,"publicationUid":228095582,"widgetId":"rgw8_56ab1c85d5e07"},"id":"rgw8_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=4026253&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=1&publicationUid=228095582","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/80788532_Anoop_Korattikara","authorNameOnPublication":"Anoop Korattikara","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Anoop Korattikara","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/80788532_Anoop_Korattikara","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw11_56ab1c85d5e07"},"id":"rgw11_56ab1c85d5e07","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=80788532&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw10_56ab1c85d5e07"},"id":"rgw10_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=80788532&authorNameOnPublication=Anoop%20Korattikara","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/69847505_Max_Welling","authorNameOnPublication":"Max Welling","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Max Welling","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/69847505_Max_Welling","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw13_56ab1c85d5e07"},"id":"rgw13_56ab1c85d5e07","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=69847505&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw12_56ab1c85d5e07"},"id":"rgw12_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=69847505&authorNameOnPublication=Max%20Welling","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab1c85d5e07"},"id":"rgw7_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=228095582&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":228095582,"abstract":"<noscript><\/noscript><div>In this paper we address the following question: Can we approximately sample<br \/>\nfrom a Bayesian posterior distribution if we are only allowed to touch a small<br \/>\nmini-batch of data-items for every sample we generate?. An algorithm based on<br \/>\nthe Langevin equation with stochastic gradients (SGLD) was previously proposed<br \/>\nto solve this, but its mixing rate was slow. By leveraging the Bayesian Central<br \/>\nLimit Theorem, we extend the SGLD algorithm so that at high mixing rates it<br \/>\nwill sample from a normal approximation of the posterior, while for slow mixing<br \/>\nrates it will mimic the behavior of SGLD with a pre-conditioner matrix. As a<br \/>\nbonus, the proposed algorithm is reminiscent of Fisher scoring (with stochastic<br \/>\ngradients) and as such an efficient optimizer during burn-in.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw14_56ab1c85d5e07"},"id":"rgw14_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=228095582","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/228095582_Bayesian_Posterior_Sampling_via_Stochastic_Gradient_Fisher_Scoring\/links\/02b0457d0cf27908e9dda46b\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw5_56ab1c85d5e07"},"id":"rgw5_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=228095582&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2088669749,"url":"researcher\/2088669749_M_Xu","fullname":"M. Xu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2088510817,"url":"researcher\/2088510817_B_Lakshminarayanan","fullname":"B. Lakshminarayanan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2088558800,"url":"researcher\/2088558800_YW_Teh","fullname":"Y.W. Teh","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2088675909,"url":"researcher\/2088675909_J_Zhu","fullname":"J. Zhu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2014","journal":"Advances in neural information processing systems","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/286711053_Distributed_Bayesian_posterior_sampling_via_moment_sharing","usePlainButton":true,"publicationUid":286711053,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/286711053_Distributed_Bayesian_posterior_sampling_via_moment_sharing","title":"Distributed Bayesian posterior sampling via moment sharing","displayTitleAsLink":true,"authors":[{"id":2088669749,"url":"researcher\/2088669749_M_Xu","fullname":"M. Xu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2088510817,"url":"researcher\/2088510817_B_Lakshminarayanan","fullname":"B. Lakshminarayanan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2088558800,"url":"researcher\/2088558800_YW_Teh","fullname":"Y.W. Teh","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2088675909,"url":"researcher\/2088675909_J_Zhu","fullname":"J. Zhu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2088464513,"url":"researcher\/2088464513_B_Zhang","fullname":"B. Zhang","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Advances in neural information processing systems 01\/2014; 4:3356-3364."],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/286711053_Distributed_Bayesian_posterior_sampling_via_moment_sharing","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/286711053_Distributed_Bayesian_posterior_sampling_via_moment_sharing\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56ab1c85d5e07"},"id":"rgw16_56ab1c85d5e07","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=286711053","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":69847505,"url":"researcher\/69847505_Max_Welling","fullname":"Max Welling","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9164246,"url":"researcher\/9164246_Yee_Whye_Teh","fullname":"Yee Whye Teh","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Conference Paper","publicationDate":"Jan 2011","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/221346425_Bayesian_Learning_via_Stochastic_Gradient_Langevin_Dynamics","usePlainButton":true,"publicationUid":221346425,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/221346425_Bayesian_Learning_via_Stochastic_Gradient_Langevin_Dynamics","title":"Bayesian Learning via Stochastic Gradient Langevin Dynamics","displayTitleAsLink":true,"authors":[{"id":69847505,"url":"researcher\/69847505_Max_Welling","fullname":"Max Welling","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9164246,"url":"researcher\/9164246_Yee_Whye_Teh","fullname":"Yee Whye Teh","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Proceedings of the 28th International Conference on Machine Learning, ICML 2011, Bellevue, Washington, USA, June 28 - July 2, 2011; 01\/2011"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/221346425_Bayesian_Learning_via_Stochastic_Gradient_Langevin_Dynamics","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/221346425_Bayesian_Learning_via_Stochastic_Gradient_Langevin_Dynamics\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab1c85d5e07"},"id":"rgw17_56ab1c85d5e07","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=221346425","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2047781529,"url":"researcher\/2047781529_Yu-Xiang_Wang","fullname":"Yu-Xiang Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9777225,"url":"researcher\/9777225_Stephen_E_Fienberg","fullname":"Stephen E. Fienberg","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":3222757,"url":"researcher\/3222757_Alex_Smola","fullname":"Alex Smola","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/272844713_Privacy_for_Free_Posterior_Sampling_and_Stochastic_Gradient_Monte_Carlo","usePlainButton":true,"publicationUid":272844713,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/272844713_Privacy_for_Free_Posterior_Sampling_and_Stochastic_Gradient_Monte_Carlo","title":"Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo","displayTitleAsLink":true,"authors":[{"id":2047781529,"url":"researcher\/2047781529_Yu-Xiang_Wang","fullname":"Yu-Xiang Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9777225,"url":"researcher\/9777225_Stephen_E_Fienberg","fullname":"Stephen E. Fienberg","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":3222757,"url":"researcher\/3222757_Alex_Smola","fullname":"Alex Smola","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/272844713_Privacy_for_Free_Posterior_Sampling_and_Stochastic_Gradient_Monte_Carlo","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/272844713_Privacy_for_Free_Posterior_Sampling_and_Stochastic_Gradient_Monte_Carlo\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab1c85d5e07"},"id":"rgw18_56ab1c85d5e07","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=272844713","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw15_56ab1c85d5e07"},"id":"rgw15_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=228095582&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":228095582,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":228095582,"publicationType":"article","linkId":"02b0457d0cf27908e9dda46b","fileName":"Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring","fileUrl":"http:\/\/arxiv.org\/pdf\/1206.6380","name":"arxiv.org","nameUrl":"http:\/\/arxiv.org\/pdf\/1206.6380","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw21_56ab1c85d5e07"},"id":"rgw21_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=228095582&linkId=02b0457d0cf27908e9dda46b&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw20_56ab1c85d5e07"},"id":"rgw20_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=228095582&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":4,"valueFormatted":"4","widgetId":"rgw22_56ab1c85d5e07"},"id":"rgw22_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=228095582","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw19_56ab1c85d5e07"},"id":"rgw19_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=228095582&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":228095582,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw24_56ab1c85d5e07"},"id":"rgw24_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=228095582&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":4,"valueFormatted":"4","widgetId":"rgw25_56ab1c85d5e07"},"id":"rgw25_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=228095582","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw23_56ab1c85d5e07"},"id":"rgw23_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=228095582&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring\nSungjin Ahn\nDept. of Computer Science, UC Irvine, Irvine, CA 92697-3425, USA\nSUNGJIA@ICS.UCI.EDU\nAnoop Korattikara\nDept. of Computer Science, UC Irvine, Irvine, CA 92697-3425, USA\nAKORATTI@ICS.UCI.EDU\nMax Welling\nDept. of Computer Science, UC Irvine, Irvine, CA 92697-3425, USA\nWELLING@ICS.UCI.EDU\nAbstract\nIn this paper we address the following question:\n\u201cCan we approximately sample from a Bayesian\nposterior distribution if we are only allowed to\ntouch a small mini-batch of data-items for ev-\nery sample we generate?\u201d. An algorithm based\non the Langevin equation with stochastic gradi-\nents (SGLD) was previously proposed to solve\nthis, but its mixing rate was slow. By leverag-\ning the Bayesian Central Limit Theorem, we ex-\ntend the SGLD algorithm so that at high mix-\ning rates it will sample from a normal approx-\nimation of the posterior, while for slow mixing\nrates it will mimic the behavior of SGLD with a\npre-conditioner matrix. As a bonus, the proposed\nalgorithm is reminiscent of Fisher scoring (with\nstochastic gradients) and as such an efficient op-\ntimizer during burn-in.\n1. Motivation\nWhen a dataset has a billion data-cases (as is not uncom-\nmonthesedays)MCMCalgorithmswillnotevenhavegen-\nerated a single (burn-in) sample when a clever learning al-\ngorithm based on stochastic gradients may already be mak-\ning fairly good predictions. In fact, the intriguing results of\nBottou and Bousquet (2008) seem to indicate that in terms\nof \u201cnumber of bits learned per unit of computation\u201d, an al-\ngorithm as simple as stochastic gradient descent is almost\noptimally efficient. We therefore argue that for Bayesian\nmethods to remain useful in an age when the datasets grow\nat an exponential rate, they need to embrace the ideas of the\nstochastic optimization literature.\nAppearing in Proceedings of the 29thInternational Conference\non Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright\n2012 by the author(s)\/owner(s).\nA first attempt in this direction was proposed by Welling\nand Teh (2011) where the authors show that (uncorrected)\nLangevin dynamics with stochastic gradients (SGLD) will\nsample from the correct posterior distribution when the\nstepsizes are annealed to zero at a certain rate.\nSGLD succeeds in (asymptotically) generating samples\nfrom the posterior at O(n) computational cost with (n ?\nN) it\u2019s mixing rate is unnecessarily slow. This can be\ntraced back to its lack of a proper pre-conditioner: SGLD\ntakes large steps in directions of small variance and re-\nversely, small steps in directions of large variance which\nhinders convergence of the Markov chain.\nbuilds on top of Welling and Teh (2011). We leverage the\n\u201cBayesian Central Limit Theorem\u201d which states that when\nN is large (and under certain conditions) the posterior will\nbe well approximated by a normal distribution. Our al-\ngorithm is designed so that for large stepsizes (and thus\nat high mixing rates) it will sample from this approximate\nnormal distribution, while at smaller stepsizes (and thus at\nslower mixing rates) it will generate samples from an in-\ncreasingly accurate (non-Gaussian) approximation of the\nposterior. Our main claim is therefore that we can trade-in\na usually small bias in our estimate of the posterior distri-\nbution against a potentially very large computational gain,\nwhich could in turn be used to draw more samples and re-\nduce sampling variance.\nWhile\nOur work\nFrom an optimization perspective one may view this algo-\nrithm as a Fisher scoring method based on stochastic gradi-\nents (see e.g. (Schraudolph et al., 2007)) but in such a way\nthat the randomness introduced in the subsampling process\nis used to sample from the posterior distribution when we\narrive at its mode. Hence, it is an efficient optimization al-\ngorithmthatsmoothlyturnsintoasamplerwhenthecorrect\n(statistical) scale of precision is reached."},{"page":2,"text":"Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring\n2. Preliminaries\nWe will start with some notation, definitions and prelimi-\nnaries. We have a large dataset XNconsisting of N i.i.d.\ndata-points {x1...xN} and we use a family of distributions\nparametrized by \u03b8 \u2208 RDto model the distribution of the\nxi\u2019s. We choose a prior distribution p(\u03b8) and are inter-\nested in obtaining samples from the posterior distribution,\np(\u03b8|XN) \u221d p(XN|\u03b8)p(\u03b8).\nAs is common in Bayesian asymptotic theory, we will also\nmake use of some frequentist concepts in the develop-\nment of our method. We assume that the true data gen-\nerating distribution is in our family of models and denote\nthe true parameter which generated the dataset XNby \u03b80.\nWe denote the score or the gradient of the log likelihood\nw.r.t. data-point xiby gi(\u03b8) = g(\u03b8;xi) = \u2207\u03b8logp(\u03b8;xi).\nWe denote the sum of scores of a batch of n data-points\nXr = {xr1...xrn} by Gn(\u03b8;Xr) =?n\ndrop the argument Xrand instead simply write Gn(\u03b8) and\ngn(\u03b8) for convenience.\nThe covariance of the gradients is called the Fisher infor-\nmation defined as I(\u03b8) = Ex[g(\u03b8;x)g(\u03b8;x)T], where Ex\ndenotes expectation w.r.t the distribution p(x;\u03b8) and we\nhave used the fact that Ex[g(\u03b8;x)] = 0. It can also be\nshown that I(\u03b8) = \u2212Ex[H(\u03b8;x)], where H is the Hessian\nof the log likelihood.\ni=1g(\u03b8;xri) and\nthe average by gn(\u03b8;Xr) =1\nnGn(\u03b8;Xr). Sometimes we will\nSince we are dealing with a dataset with samples only\nfrom p(x;\u03b80) we will henceforth be interested only in\nI(\u03b80) which we will denote by I1. It is easy to see that\nthe Fisher information of n data-points, In = nI1. The\nempirical covariance of the scores computed from a batch\nofndata-pointsiscalledtheempirical Fisherinformation,\nV (\u03b8;Xr) =\nn\u22121\n(Scott, 2002). Also, it can be shown that V (\u03b80) is a consis-\ntent estimator of I1= I(\u03b80).\n1\n?n\ni=1(gri(\u03b8) \u2212 gn(\u03b8))(gri(\u03b8) \u2212 gn(\u03b8))T\nWe now introduce an important result in Bayesian asymp-\ntotic theory. As N becomes large, the posterior distribution\nbecomes concentrated in a small neighbourhood around \u03b80\nand becomes asymptotically Gaussian. This is formalized\nby the Bernstein-von Mises theorem, a.k.a the Bayesian\nCentral Limit Theorem, (Le Cam, 1986), which states that\nunder suitable regularity conditions, p(\u03b8|{x1...xN}) ap-\nproximately equals N(\u03b80,I\u22121\nN) as N becomes very large.\n3. Stochastic Gradient Fisher Scoring\nWe are now ready to derive our Stochastic Gradient Fisher\nScoring (SGFS) algorithm. The starting point in the deriva-\ntion of our method is the Stochastic Gradient Langevin Dy-\nnamics (SGLD) algorithm (Welling & Teh, 2011) which\nwe describe in section 3.1. SGLD can sample accurately\nfrom the posterior but suffers from a low mixing rate. In\nsection 3.2, we show that it is easy to construct a Markov\nchain that can sample from a normal approximation of the\nposterior at any mixing rate. We will then combine these\nmethods to develop our Stochastic Gradient Fisher Scoring\n(SGFS) algorithm in section 3.3.\n3.1. Stochastic Gradient Langevin Dynamics\nThe SGLD algorithm has the following update equation:\n\u03b8t+1\u2190 \u03b8t+?C\n2\n?\u2207logp(\u03b8t) + Ngn(\u03b8t;Xt\nwhere\nn)?+ \u03bd\n\u03bd \u223c N(0,?C) (1)\nHere ? is the step size, C is called the preconditioning ma-\ntrix (Girolami & Calderhead, 2010) and \u03bd is a random vari-\nable representing injected Gaussian noise. The gradient of\nthe log likelihood GN(\u03b8;XN) over the whole dataset is ap-\nproximated by scaling the mean gradient gn(\u03b8t;Xt\nputed from a mini-batch Xt\nWelling & Teh (2011) showed that Eqn. (1) generates sam-\nples from the posterior distribution if the step size is an-\nnealedtozeroatacertainrate. Asthestepsizegoestozero,\nthe discretization error in the Langevin equation disap-\npears and we do not need to conduct expensive Metropolis-\nHasting(MH) accept\/reject tests that use the whole dataset.\nThus, this algorithm requires only O(n) computations to\ngenerateeachsample, unliketraditionalMCMCalgorithms\nwhich require O(N) computations per sample.\nHowever, since the step sizes are reduced to zero, the mix-\ning rate is reduced as well, and a large number of iterations\nare required to obtain a good coverage of the parameter\nspace. One way to make SGLD work at higher step sizes is\ntointroduceMHaccept\/rejectstepstocorrectforthehigher\ndiscretization error, but our initial attempts using only a\nmini-batch instead of the whole dataset were unsuccessful.\nn) com-\nn= {xt1...xtn} of size n ? N.\n3.2. Sampling from the Approximate Posterior\nSince it is not clear how to use Eqn. (1) at high step sizes,\nwe will move away from Langevin dynamics and explore\na different approach. As mentioned in section 2, the poste-\nrior distribution can be shown to approach a normal distri-\nbution, N(\u03b80,I\u22121\nlarge. It is easy to construct a Markov chain which will\nsample from this approximation of the posterior at any step\nsize. We will now show that the following update equation\nachieves this:\nN), as the size of the dataset becomes very\n\u03b8t+1\u2190 \u03b8t+?C\n2\n{\u2212IN(\u03b8t\u2212 \u03b80)} + \u03c9\n\u03c9 \u223c N(0,?C \u2212?2\nwhere\n4CINC)\n(2)"},{"page":3,"text":"Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring\nThe update is an affine transformation of \u03b8tplus injected\nindependent Gaussian noise, \u03c9. Thus if \u03b8thas a Gaussian\ndistribution N(\u00b5t,\u03a3t), \u03b8t+1will also have a Gaussian dis-\ntribution, which we will denote as N(\u00b5t+1,\u03a3t+1). These\ndistributions are related by:\n\u00b5t+1= (I \u2212?C\n\u03a3t+1= (I \u2212?C\n2IN)\u00b5t+?C\n2IN)\u03a3t(I \u2212?C\n2IN\u03b80\n2IN)T+ ?C \u2212?2\n4CINC\n(3)\nIf we choose C to be symmetric, it is easy to see that the\napproximate posterior distribution, N(\u03b80,I\u22121\nvariant distribution of this Markov chain. Since Eqn. (2) is\nnot a Langevin equation, it samples from the approximate\nposterior at large step-size and does not require any MH\naccept\/reject steps. The only requirement is that C should\nbe symmetric and should be chosen so that the covariance\nmatrix of the injected noise in Eqn. (2) is positive-definite.\nN), is an in-\n3.3. Stochastic Gradient Fisher Scoring\nIn practical problems both sampling accuracy and mixing\nrate are important, and the extreme regimes dictated by\nboth the above methods are very limiting. If the posterior\nis close to Gaussian (as is usually the case), we would like\nto take advantage of the high mixing rate. However, if we\nneed to capture a highly non-Gaussian posterior, we should\nbe able to trade-off mixing rate for sampling accuracy. One\ncould also think about doing this in an \u201canytime\u201d fashion\nwhere if the posterior is somewhat close to Gaussian, we\ncan start by sampling from a Gaussian approximation at\nhigh mixing rates, but slow down the mixing rate to capture\nthe non-Gaussian structure if more computation becomes\navailable. In other words, one should have the freedom to\nmanage the right trade off between sampling accuracy and\nmixing rate depending on the problem at hand.\nWith this goal in mind, we combine the above methods to\ndevelop our Stochastic Gradient Fisher Scoring (SGFS) al-\ngorithm. We accomplish this using a Markov chain with\nthe following update equation:\n?\u2207logp(\u03b8t) + Ngn(\u03b8t;Xt\n\u03b8t+1\u2190 \u03b8t+?C\n2\nn)?+ \u03c4\nwhere\n\u03c4 \u223c N(0,Q) (4)\nWhen the step size is small, we want to choose Q = ?C so\nthat it behaves like the Markov chain in Eqn (1). Now we\nwill see how to choose Q so that when the step size is large\nand the posterior is approximately Gaussian, our algorithm\nbehaves like the Markov chain in Eqn. (2). First, note that\nif n is large enough for the central limit theorem to hold,\nwe have:\n?\ngn(\u03b8t;Xt\nn) \u223c N\nEx[g(\u03b8t;x)],1\nnCov[g(\u03b8t;x)]\n?\n(5)\nHere Cov[g(\u03b8t;x)] is the covariance of the scores at\n\u03b8t. Using NCov[g(\u03b8t;x)] \u2248 IN and NEx[g(\u03b8t;x)] \u2248\nGN(\u03b8t;XN), we have:\n\u2207logp(\u03b8t) + Ngn(\u03b8t;Xt\n\u2248 \u2207logp(\u03b8t) + GN(\u03b8t;XN) + \u03c6\nwhere\n\u03c6 \u223c N\nNow, \u2207logp(\u03b8t) + GN(\u03b8t;XN) = \u2207logp(\u03b8t|XN), the\ngradientofthelogposterior. Ifweassumethattheposterior\nis close to its Bernstein-von Mises approximation, we have\n\u2207logp(\u03b8t|XN) = \u2212IN(\u03b8t\u2212 \u03b80). Using this in Eqn. (6)\nand then substituting in Eqn. (4), we have:\n\u03b8t+1\u2190 \u03b8t+?C\n2\nwhere,\n?\nComparing Eqn. (7) and Eqn. (2), we see that at high step\nsizes, we need:\nQ +?2\n4\nQ = ?C \u2212?2\nThus, we should choose Q such that:\n??C\nwhere we have defined \u03b3 =\nwhen ? is small, we can choose Q = ?C \u2212?2\nboth the cases above. With this, our update equation be-\ncomes:\n\u03b8t+1\u2190 \u03b8t+?C\n2\n?\nNow, we have to choose C so that the covariance matrix of\nthe injected noise in Eqn. (9) is positive-definite. One way\nto enforce this, is by setting:\n?C \u2212?2\nn)\n?\n0,NIN\nn\n?\n(6)\n{\u2212IN(\u03b8t\u2212 \u03b80)} + \u03c8 + \u03c4\n(7)\n\u03c8 \u223c N\n0,?2\n4\nN\nnCINC\n?\nand \u03c4 \u223c N(0,Q)\nN\nnCINC = ?C \u2212?2\n4CINC \u21d2\nN + n\nn4\nCINC\n(8)\nQ =\nfor small ?\nfor large ??C \u2212?2\n4\u03b3CINC\nN+n\nn. Since ? dominates ?2\n4\u03b3CINC for\n?\u2207logp(\u03b8t) + Ngn(\u03b8t;Xt\nwhere \u03c4 \u223c N\nn)?+ \u03c4\n0,?C \u2212?2\n4\u03b3CINC\n?\n(9)\n4\u03b3CINC = ?CBC \u21d2 C = 4[?\u03b3IN+ 4B]\u22121\n(10)\nwhere B is any symmetric positive-definite matrix. Plug-\nging in this choice of C in Eqn. 9, we get:\n?\n{\u2207logp(\u03b8t) + Ngn(\u03b8t;Xt) + \u03b7}\nwhere \u03b7 \u223c N\n\u03b8t+1\u2190 \u03b8t+ 2\u03b3IN+4B\n?\n?\u22121\n\u00d7\n?\n0,4B\n?\n?\n(11)"},{"page":4,"text":"Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring\nHowever, the above method considers IN to be a known\nconstant. In practice, we use N\u02c6I1,tas an estimate of IN,\nwhere\u02c6I1,t is an online average of the empirical covari-\nance of gradients (empirical Fisher information) computed\nat each \u03b8t.\n\u02c6I1,t= (1 \u2212 \u03bat)\u02c6I1,t\u22121+ \u03batV (\u03b8t;Xt\nwhere \u03bat= 1\/t. In the supplementary material we prove\nthat this online average converges to I1plus O(1\/N) cor-\nrections if we assume that the samples are actually drawn\nfrom the posterior:\nTheorem 1. Consider a sampling algorithm which\ngenerates a sample \u03b8t from the posterior distribution\nof the model parameters p(\u03b8|XN) in each itera-\ntion t.In each iteration, we draw a random mini-\nbatch of size n, Xt\nn\n=\nthe empirical covariance of the scores V (\u03b8t;Xt\n1\nn\u22121\nLet VT be the average of V (\u03b8t) across T iterations.\nFor large N, as T \u2192 \u221e, VT converges to the Fisher\ninformation I(\u03b80) plus O(1\n?\nT\nt=1\nn)\n(12)\n{xt1...xtn}, and compute\nn)=\n?n\ni=1{g(\u03b8t;xti) \u2212 gn(\u03b8t)}{g(\u03b8t;xti) \u2212 gn(\u03b8t)}T.\nN) corrections, i.e.\n?\nlim\nT\u2192\u221e\nVT?1\nT\n?\nV (\u03b8t;Xt\nn)= I(\u03b80) + O(1\nN) (13)\nNote that this is not a proof of convergence of the Markov\nchain to the correct distribution. Rather, assuming that the\nsamples are from the posterior, it shows that the online av-\nerage of the covariance of the gradients converges to the\nFisher information (as desired). Thus, it strengthens our\nconfidencethatifthesamplesarealmostfromtheposterior,\nthe learned pre-conditioner converges to something sensi-\nble. What we do know is that if we anneal the stepsizes\naccording to a certain polynomial schedule, and we keep\nthe pre-conditioner fixed, then SGFS is a version of SGLD\nwhich was shown to converge to the correct equilibrium\ndistribution (Welling & Teh, 2011). We believe the adap-\ntation of the Fisher information through an online average\nis slow enough for the resulting Markov chain to still be\nvalid, but a proof is currently lacking. The theory of adap-\ntive MCMC (Andrieu & Thoms, 2009) or two time scale\nstochastic approximations (Borkar, 1997) might hold the\nkey to such a proof which we leave for future work. Putting\nit all together, we arrive at algorithm 1 below.\nThe general method still has a free symmetric positive-\ndefinite matrix, B, which may be chosen according to our\nconvenience. Examine the limit ? \u2192 0. In this case our\nmethod becomes SGLD with preconditioning matrix B\u22121\nand step size ?.\nIftheposteriorisGaussian, asisusuallythecasewhenN is\nlarge, the proposed SGFS algorithm will sample correctly\nfor arbitrary choice of B even when the step size ? is large.\nAlgorithm 1: Stochastic Gradient Fisher Scoring (SGFS)\nInput: n, B, {\u03bat}t=1:T\nOutput: {\u03b8t}t=1:T\n1: Initialize \u03b81,\u02c6I1,0\n2: \u03b3 \u2190n+N\n3: for t = 1 : T do\n4:\nChoose random minibatch Xt\n5:\ngn(\u03b8t) \u21901\n6:\nV (\u03b8t) \u2190\n1\nn\u22121\n7:\n\u02c6I1,t\u2190 (1 \u2212 \u03bat)\u02c6I1,t\u22121+ \u03batV (\u03b8t)\n8:\nDraw \u03b7 \u223c N[0,4B\n9:\n\u03b8t+1\u2190 \u03b8t+\n2 \u03b3N\u02c6I1,t+4B\n?\n{\u2207logp(\u03b8t) + Ngn(\u03b8t) + \u03b7}\n10: end for\nn\nn= {xt1...xtn}\nn\n?n\ni=1gti(\u03b8t)\n?n\ni=1{gti(\u03b8t) \u2212 gn(\u03b8t)}{gti(\u03b8t) \u2212 gn(\u03b8t)}T\n?]\n?\u22121\n?\nHowever, for some models the conditions of the Bernstein-\nvon Mises theorem are violated and the posterior may not\nbe well approximated by a Gaussian. This is the case for\ne.g. neural networks and discriminative RBMs, where the\nidentifiability condition of the parameters do not hold. In\nthis case, we have to choose a small ? to achieve accurate\nsampling (see section 5). These two extremes can be com-\nbined in a single \u201canytime\u201d algorithm by slowly annealing\nthe stepsize. For a non-adaptive version of our algorithm\n(i.e. where we would stop changing\u02c6I1) after a fixed num-\nber of iterations) this would according to the results from\nWelling and Teh (2011) lead to a valid Markov chain for\nposterior sampling.\nWe recommend choosing B \u221d IN. With this choice, our\nmethod is highly reminiscent of \u201cFisher scoring\u201d which\nis why we named it \u201cStochastic Gradient Fisher Scoring\u201d\n(SGFS). In fact we can think of the proposed updates as a\nstochastic version of Fisher scoring based on small mini-\nbatches of gradients. But remarkably, the proposed algo-\nrithm is not only much faster than Fisher scoring (because\nit only requires small minibatches to compute an update), it\nalso samples approximately from the posterior distribution.\nSo the knife cuts on both sides: SGFS is a faster optimiza-\ntion algorithm but also doesn\u2019t overfit due to the fact that\nit switches to sampling when the right statistical scale of\nprecision is reached.\n4. Computational Efficiency\nClearly, the main computational benefit relative to stan-\ndard MCMC algorithms comes from the fact that we use\nstochastic minibatches instead of the entire dataset at every\niteration. However, for a model with a large number of pa-\nrameters another source of significant computational effort\nis the computation of the D \u00d7 D matrix \u03b3N\u02c6I1,t+4B\n?and"},{"page":5,"text":"Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring\nmultiplying its inverse with the mean gradient resulting in a\ntotal computational complexity of O(D3) per iteration. In\nthe case n < D the computational complexity per iteration\ncan be brought down to O(nD2) by using the Sherman-\nMorrison-Woodbury equation. A more numerically stable\nalternative is to update Cholesky factors (Seeger, 2004).\nIn case even this is infeasible one can factor the Fisher in-\nformation into k independent blocks of variables of, say\nsize d, in which case we have brought down the complexity\nto O(kd3). The extreme case of this is when we treat every\nparameter as independent which boils down to replacing\nthe Fisher information by a diagonal matrix with the vari-\nances of the individual parameters populating the diagonal.\nWhile for a large stepsize this algorithm will not sample\nfrom the correct Gaussian approximation, it will still sam-\nple correctly from the posterior for very small stepsizes. In\nfact, it is expected to do this more efficiently than SGLD\nwhich does not rescale its stepsizes at all. We have used\nthefullcovariancealgorithm(SGFS-f)andthediagonalco-\nvariance algorithm (SGFS-d) in the experiments section.\n5. Experiments\nBelow we report experimental results where we test SGFS-\nf, SGFS-d, SGLD, SGD and HMC on three different mod-\nels: logistic regression, neural networks and discriminative\nRBMs. The experiments share the following practice in\ncommon. Stepsizes for SGD and SGLD are always se-\nlected through cross-validation for at least five settings.\nThe minibatch size n is set to either 300 or 500, but the\nresults are not sensitive to the precise value as long as it\nis large enough for the central limit theorem to hold (typi-\ncally, n > 100 is recommended). Also, we used \u03bat=1\nt.\n5.1. Logistic Regression\nA logistic regression model (LR) was trained on the\nMNIST dataset for binary classification of two digits 7 and\n9 using a total of 10,000 data-items. We used a 50 dimen-\nsional random projection of the original features and ran\nSGFS with \u03bb = 1. We used B = \u03b3INand tested the algo-\nrithm for a number of \u03b1 values (where \u03b1 =\nthealgorithmfor3,000burn-initerationsandthencollected\n100,000 samples. We compare the algorithm to Hamil-\ntonian Monte Carlo sampling (Neal, 1993) and to SGLD\n(Welling & Teh, 2011). For HMC, the \u201cleapfrogstep\u201d size\nwas adapted during burn-in so that the acceptance ratio was\naround 0.8. For SGLD we also used a range of fixed step-\nsizes.\n2\n\u221a?). We ran\nIn figure 1 we show 2-d marginal distributions of SGFS\ncompared to the ground truth from a long HMC run where\nwe used \u03b1 = 0 for SGFS. From this we conclude that even\nfor the largest possible stepsize the fit for SGFS-f is al-\n0.50.6\nSGFS\u2212f (BEST)\n0.70.8\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n\u22120.4\u22120.3 \u22120.2 \u22120.1\n\u22120.75\n\u22120.7\n\u22120.65\n\u22120.6\n\u22120.55\n\u22120.5\n\u22120.45\n\u22120.4\nSGFS\u2212f (WORST)\n0.5 0.6\nSGFS\u2212d (BEST)\n0.7 0.8\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n\u22120.4\u22120.3 \u22120.2\u22120.1\n\u22120.75\n\u22120.7\n\u22120.65\n\u22120.6\n\u22120.55\n\u22120.5\n\u22120.45\n\u22120.4\nSGFS\u2212d (WORST)\nFigure 1. 2-d marginal posterior distributions for logistic regres-\nsion. Grey colors correspond to samples from SGFS. Red solid\nand blue dotted ellipses represent iso-probability contours at two\nstandard deviations away from the mean computed from HMC\nand SGFS, respectively. Top plots are the results for SGFS-f and\nbottom plots represent SGFS-d. Plots on the left represent the 2-d\nmarginals with the smallest difference between HMC and SGFS\nwhile the plots on the right represent the 2-d marginals with the\nlargest difference. Value for \u03b1 is 0 meaning that no additional\nnoise was added.\nmost perfect while SGFS-d underestimates the variance in\nthis case (note however that for smaller stepsizes (larger \u03b1)\nSGFS-d becomes very similar to SGLD and is thus guaran-\nteed to sample correctly albeit with a low mixing rate).\nNext, we studied the inverse autocorrelation time per unit\ncomputation (ATUC)1averaged over the 51 parameters and\ncompared this with the relative error after a fixed amount of\ncomputation time. The relative error is computed as fol-\nlows: first we compute the mean and covariance of the\nparameter samples up to time t : \u03b8t=\nCt=\nt\nthe long HMC run which we indicate with \u03b8\u221eand C\u221e.\nFinally we compute\n1\nt\n?t\nt?=1\u03b8t? and\n1\n?t\nt?=1(\u03b8t? \u2212 \u03b8t)(\u03b8t? \u2212 \u03b8t)T. We do the same for\nE1t=\n?\ni|\u03b8t\n?\ni\u2212 \u03b8\u221e\ni|\u03b8\u221e\ni|\ni|\n,E2t=\n?\nij|Ct\n?\nij\u2212 C\u221e\nij|C\u221e\nij|\nij|\n(14)\nIn Figure 2 we plot the \u201cError at time T\u201d for two val-\nues of T (T=100, T=3000) as a function of the inverse\nATUC, which is a measure of the mixing rate. Top plots\nshow the results for the mean and bottom plots for the\ncovariance. Each point denoted by a cross is obtained\n1ATUC = Autocorrelation Time \u00d7 Time per Sample. Auto-\ncorrelation time is defined as 1 + 2?\u221e\ns=1\u03c1(s) with \u03c1(s) the au-\ntocorrelation at lag s Neal (1993)."},{"page":6,"text":"Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring\n10\n\u22122\n10\n\u22121\n10\n0\n10\n1\n10\n\u22123\n10\n\u22122\n10\n\u22121\nRelative Error in Mean at 100 sec.\nMixing Rate (1\/ATUC)\nSGLD\nHMC\nSGFS\u2212f\nSGFS\u2212d\n10\n\u22122\n10\n\u22121\n10\n0\n10\n1\n10\n\u22123\n10\n\u22122\n10\n\u22121\nRelative Error in Mean at 3000 sec.\nMixing Rate (1\/ATUC)\nHMC\nSGLD\nSGFS\u2212f\nSGFS\u2212d\n10\n\u22122\n10\n0\n10\n\u22121\n10\n0\nRelative Error in Cov at 100 sec.\nMixing Rate (1\/ATUC)\nHMC\nSGLD\nSGFS\u2212d\nSGFS\u2212f\n10\n\u22122\n10\n0\n10\n\u22121\n10\n0\nRelative Error in Cov at 3000 sec.\nMixing Rate (1\/ATUC)\nHMC\nSGLD\nSGFS\u2212d\nSGFS\u2212f\nFigure 2. Final error of logistic regression at time T versus mixing\nrate for the mean (top) and covariance (bottom) estimates after\n100 (left) and 3000 (right) seconds of computation. See main text\nfor detailed explanation.\nfrom a different setting of parameters that control the mix-\ning rate: \u03b1 = [0,1,2,3,4,5,6] for SGFS, stepsizes ? =\n[1e\u22123,5e\u22124,1e\u22124,5e\u22125,1e\u22125,5e\u22126,1e\u22126]forSGLD,\nand number of leapfrog steps s = [50,40,30,20,10,1] for\nHMC. The circle is the result for the fastest mixing chain.\nFor SGFS and SGLD, if the slope of the curve is nega-\ntive (downward trend) then the corresponding algorithm\nwas still in the phase of reducing error by reducing sam-\npling variance at time T. However, when the curve bends\nupwards and develops a positive slope the algorithm has\nreached its error floor corresponding to the approximation\nbias. The situation is different for HMC, (which has no\nbias) but where the bending occurs because the number of\nleapfrog steps has become so large that it is turning back\non itself. HMC is not faring well because it is computa-\ntionally expensive to run (which hurts both its mixing rate\nand error at time T). We also observe that in the allowed\nrunning time SGFS-f has not reached its error floor (both\nfor the mean and the covariance). SGFS-d is reaching its\nerror floor only for the covariance (which is consistent with\nFigure 1 bottom) but still fares well in terms of the mean.\nFinally, for SGLD we clearly see that in order to obtain a\nhigh mixing rate (low ATUC) it has to pay the price of a\nlarge bias. These plots clearly illustrate the advantage of\nSGFS over both HMC as well as SGLD.\n5.2. SGFS on Neural Networks\nWe also applied our methods to a 3 layer neural network\n(NN) with logistic activation functions. Below we describe\nclassification results for two datasets.\n104\n105\n10-0.35\n10-0.34\n10-0.33\n10-0.32\nIteration Number (Log)\nClassification Error (Log)\n \n \nSGD-Avg (\uf06c = 0)\nSGD-Avg (\uf06c = 0.1)\nSGLD-Avg (\uf06c = 0)\nSGLD-Avg (\uf06c = 0.1)\nSGFS-d-Avg (\uf06c = 0, \uf061 = 6)\nSGFS-d-Avg (\uf06c = 0.1, \uf061 = 6)\nSGD\nSGFS-d\nSGLD\n10\n0\n10\n2\n10\n4\n10\n-0.7\n10\n-0.5\n10\n-0.3\n10\n-0.1\nIteration Number (Log)\nClassification Error (Log)\n \n \nSGFS-f-Avg (\uf06c = 0.001, \uf061 = 2)\nSGFS-d-Avg (\uf06c = 0.001, \uf061 = 2)\nSGLD-Avg (\uf06c = 0.001, \uf065 = [10-3,10-7])\nSGD-Avg (\uf06c = 0.001, \uf065 = [10-3,10-7])\nSGLD\nSGD\nSGFS-f\nSGFS-d\nFigure 3. Test-set classification error of NNs trained with SGFS-\nf, SGFS-d, SGLD and SGD on the HHP dataset (left) and the\nMNIST dataset (right)\n5.2.1. HERITAGE HEALTH PRIZE (HHP)\nThe goal of this competition is to predict how many days\nbetween [0 \u2212 15] a person will stay in a hospital given\nhis\/her past three years of hospitalization records2. We\nused the same features as the team market makers that won\nthe first milestone prize. Integrating the first and second\nyear data, we obtained 147,473 data-items with 139 fea-\nture dimensions and then used a randomly selected 70%\nfor training and the remainder for testing. NNs with 30\nhidden units were used because more hidden units did not\nnoticeably improve the results. Although we used \u03b1 = 6\nfor SGFS-d, there was no significant difference for values\nin the range 3 \u2264 \u03b1 \u2264 6. However, \u03b1 < 3 did not work for\nthis dataset due to the fact that many features had values 0.\nFor SGD, we used stepsizes from a polynomial anneal-\ning schedule a(b + t)\u2212\u03b4. Because the training error de-\ncreased slowly in a valid range \u03b4 = [0.5,1], we used \u03b4 = 3,\na = 1014, b = 2.2 \u00d7 105instead which was found optimal\nthrough cross-validation. (This setting reduced the stepsize\nfrom 10\u22122to 10\u22126during 1e+7 iterations). For SGLD,\na = 1, b = 104, and \u03b4 = 1 reducing the step size from\n10\u22124to 10\u22126was used. Figure 3 (left) shows the classi-\nfication errors averaged over the posterior samples for two\nregularizer values, \u03bb = 0 and the best regularizer value \u03bb\nfound through cross-validation. First, we clearly see that\nSGD severely overfits without a regularizer while SGLD\nand SGFS prevent it because they average predictions over\nsamples from a posterior mode. Furthermore, we see that\nwhen the best regularizer is used, SGFS (marginally) out-\nperforms both SGD and SGLD. The result from SGFS-d\nsubmitted to the actual competition leaderboard gave us an\nerror of 0.4635 which is comparable to 0.4632 obtained by\nthe milestone winner with a fine-tuned Gradient Boosting\nMachine.\n2http:\/\/www.heritagehealthprize.com"},{"page":7,"text":"Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring\n\u22122.2\u22122\u22121.8\nSGFS\u2212f (BEST)\n\u22121.6\u22121.4 \u22121.2\u22121\u22120.8\u22120.6\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n\u22120.25\u22120.2\nSGFS\u2212f (WORST)\n\u22120.15\u22120.1\u22120.0500.05 0.10.15\n\u22120.2\n\u22120.15\n\u22120.1\n\u22120.05\n0\n0.05\n0.1\n0.15\n0.2\n\u22122.2\u22122\u22121.8\u22121.6\u22121.4\u22121.2\u22121\u22120.8\u22120.6\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nSGLD (BEST)\n\u22122\u22121.5\u22121\u22120.50 0.5\n\u22124\n\u22123.5\n\u22123\n\u22122.5\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\nSGLD (WORST)\nFigure 4. 2-d marginal posterior distributions of DRBM. Grey\ncolors correspond to samples from SGFS\/SGLD. Thick red solid\nlines correspond to iso-probability contours at two standard de-\nviations away from the mean computed from HMC samples.\nThin red solid lines correspond to HMC results based on sub-\nsets of the samples. The thick blue dashed lines correspond to\nSGFS-f (top) and SGLD (bottom) runs. Plots on the left repre-\nsent the 2-d marginals with the smallest difference between HMC\nand SGFS\/SGLD while the plots on the right represent the 2-d\nmarginals with the largest difference.\n5.2.2. CHARACTER RECOGNITION\nWe also tested our methods on the MNIST dataset for 10\ndigit classification which has 60,000 training instances and\n10,000 test instances. In order to test with SGFS-f, we used\ninputs from 20 dimensional random projections and 30 hid-\nden units so that the number of parameters equals 940.\nMoreover, we increased the mini-batch size to 2,000 to re-\nducethe timerequiredtoreach agoodapproximation ofthe\n940\u00d7940 covariance matrix. The classification error aver-\naged over the samples is shown in Figure 3 (right). Here,\nwe used a small regularization parameter of \u03bb = 0.001\nfor all methods as overfitting was not an issue. For SGFS,\n\u03b1 = 2 is used while for both SGD and SGLD the stepsizes\nwere annealed from 10\u22123to 10\u22127using a = 1, b = 1000,\nand \u03b3 = 1.\n5.3. Discriminative Restricted Boltzmann Machine\n(DRBM)\nWe trained a DRBM (Larochelle & Bengio, 2008) on the\nKDD99 dataset which consists of 4,898,430 datapoints\nwith 40 features, belonging to a total of 23 classes. We\nfirst tested the classification performance by training the\nDRBM using SGLD, SGFS-f, SGFS-d and SGD. For this\nexperiment the dataset was divided into a 90% training set,\n5% validation and 5% test set. We used 41 hidden units\ngiving us a total of 2647 parameters in the model. We used\n0.51 1.52 2.53 3.544.5\n?5\nx 10\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\nRel. Err. in Mean at 6790 sec.\nMixing Rate (1\/ATUC)\nSGFS?f\nSGLD\n0.51 1.522.53 3.54 4.5\n?5\nx 10\n1\n2\n3\n4\n5\n6\n7\nRel. Err. in Cov at 6790 sec.\nMixing Rate (1\/ATUC)\nSGFS?f\nSGLD\nFigure 5. Final error for DRBM at time T versus mixing rate for\nthe mean (left) and covariance (right) estimates after 6790 sec-\nonds of computation on a subset of KDD99.\nSGD\n8.010\u22124\nSGLD\n6.610\u22124\nSGFS-d\n4.210\u22124\nSGFS-f\n4.410\u22124\nTable 1. Final test error rate on the KDD99 dataset.\n\u03bb = 10 and B = \u03b3IN. We tried 6 different (\u03b1,?) com-\nbinations for SGFS-f and SGFS-d and tried 18 annealing\nschedules for SGD and SGLD, and used the validation set\nto pick the best one. The best results were obtained with an\n\u03b1 value of 8.95 for SGFS-f and SGFS-d, and [a = 0.1, b =\n100000, \u03b4 = 0.9] for SGD and SGLD. We ran all algorithms\nfor 100,000 iterations. Although we experimented with dif-\nferent burn-in iterations, the algorithms were insensitive to\nthis choice. The final error rates are given in table 1 from\nwhich we conclude that the samplers based on stochastic\ngradients can act as effective optimizers whereas HMC on\nthe full dataset becomes completely impractical because it\nhas to compute 11.7 billion gradients per iteration which\ntakes around 7.5 minutes per sample (4408587 datapoints\n\u00d7 2647 parameters).\nTo compare the quality of the samples drawn after burn-in,\nwe created a 10% subset of the original dataset. This time\nwe picked only the 6 most populous classes. We tested all\nalgorithms with 41, 10 and 5 hidden units, but since the\nposterior is highly multi-modal, the different algorithms\nended up sampling from different modes. In an attempt\nto get a meaningful comparison, we therefore reduced the\nnumber of hidden units to 2. This improved the situation to\nsome degree, but did not entirely get rid of the multi-modal\nand non-Gaussian structure of the posterior. We compare\nresults of SGFS-f\/SGLD with 30 independent HMC runs,\neach providing 4000 samples for a total of 120,000 sam-\nples. Since HMC was very slow (even on the reduced set)\nwe initialized at a mode and used the Fisher information\nat the mode as a pre-conditioner. We used 1 leapfrog step\nand tuned the step-size to get an acceptance rate of 0.8.\nWe ran SGFS-f with \u03b1 = [2,3,4,5,10] and SGLD with\nfixed step sizes of [5e-4, 1e-4, 5e-5, 1e-5, 5e-6]. Both\nalgorithms were initialized at the same mode and ran for\n1 million iterations. We looked at the marginal distribu-"},{"page":8,"text":"Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring\ntions of the top 25 pairs of variables which had the highest\ncorrelation coefficient. In Figure 4 (top-left and bottom-\nleft) we show a set of parameters where both SGFS-f and\nSGLD obtained an accurate estimate of the marginal poste-\nrior. In 4 (top-right and bottom-right) we show an example\nwhere SGLD failed. The thin solid red lines correspond\nto HMC runs computed from various subsets of the sam-\nples, whereas the thick solid red line is computed using the\nall samples from all HMC runs. We have shown marginal\nposterior estimates of the SGFS-f\/SGLD algorithms with a\nthick dashed blue ellipse. After inspection, it seemed that\nthe posterior structure was highly non-Gaussian with re-\ngions where the probability very sharply decreased. SGLD\nregularly stepped into these regions and then got catapulted\naway due to the large gradients there. SGFS-f presumably\navoided those regions by adapting to the local covariance\nstructure. We found that in this region even the HMC runs\nare not consistent with one another. Note that the SGFS-f\ncontours seem to agree with the HMC contours as much as\nthe HMC contours agree with the results of its own subsets,\nin both the easy and the hard case.\nFinally, we plot the error after 6790 seconds of computa-\ntion versus the mixing rate. Figure 5-left shows the results\nfor the mean and the right for the covariance (for an ex-\nplanation of the various quantities see discussion in section\n5.1). We note again that SGLD incurs a significantly larger\napproximation bias at the same mixing rate as SGFS-f.\n6. Conclusions\nWe have introduced a novel method, \u201cStochastic Gradient\nFisher Scoring\u201d (SGFS) for approximate Bayesian learn-\ning. The main idea is to use stochastic gradients in the\nLangevin equation and leverage the central limit theorem\nto estimate the noise induced by the subsampling process.\nThis subsampling noise is combined with artificially in-\njected noise and multiplied by the estimated inverse Fisher\ninformation matrix to approximately sample from the pos-\nterior. This leads to the following desirable properties.\n\u2022 Unlike regular MCMC methods, SGFS is fast because it\nuses only stochastic gradients based on small mini-batches\nto draw samples.\n\u2022 Unlike stochastic gradient descent, SGFS samples (ap-\nproximately) from the posterior distribution.\n\u2022 Unlike SGLD, SGFS samples from a Gaussian approx-\nimation of the posterior distribution (that is correct for\nN \u2192 \u221e) for large stepsizes.\n\u2022 By annealing the stepsize, SGFS becomes an any-\ntime method capturing more non-Gaussian structure with\nsmaller stepsizes but at the cost of slower mixing.\n\u2022 During its burn-in phase, SGFS is an efficient optimizer\nbecause like Fisher scoring and Gauss-Newton methods, it\nis based on the natural gradient.\nFor an appropriate annealing schedule, SGFS thus goes\nthrough three distinct phases: 1) during burn-in we use\na large stepsize and the method is similar to a stochastic\ngradient version of Fisher scoring, 2) when the stepsize\nis still large, but when we have reached the mode of the\ndistribution, SGFS samples from the asymptotic Gaussian\napproximation of the posterior, and 3) when the stepsize\nis further annealed, SGFS will behave like SGLD with a\npre-conditioning matrix and generate increasingly accurate\nsamples from the true posterior.\nAcknowledgements\nThis material is based upon work supported by the National Sci-\nence Foundation under Grant No. 0447903, 0914783, 0928427.\nReferences\nAndrieu, C. and Thoms, J. A tutorial on adaptive mcmc. Statistics\nand Computing, 18(4):343\u2013373, 2009.\nBorkar, V.S. Stochastic approximation with two time scales. Sys-\ntems and Control Letters, 29(5):291\u2013294, 1997.\nBottou, L. and Bousquet, O. The tradeoffs of large scale learn-\ning. In Advances in Neural Information Processing Systems,\nvolume 20, pp. 161\u2013168, 2008.\nGirolami, M. and Calderhead, B. Riemann manifold langevin\nand hamiltonian monte carlo. Journal of the Royal Statistical\nSociety B, 73 (2):1\u201337, 2010.\nLarochelle, H. and Bengio, Y. Classification using discriminative\nRestricted Boltzmann Machines. In Proceedings of the 25th\nInternational Conference on Machine learning, pp. 536\u2013543.\nACM, 2008.\nLe Cam, L.M. Asymptotic methods in statistical decision theory.\nSpringer, 1986.\nNeal, R.M. Probabilistic inference using markov chain monte\ncarlo methods. Technical Report CRG-TR-93-1, University of\nToronto, Computer Science, 1993.\nSchraudolph, N. N., Yu, J., and G\u00a8 unter, S. A stochastic quasi-\nNewton method for online convex optimization. In Meila, Ma-\nrina and Shen, Xiaotong (eds.), Proc. 11thIntl. Conf. Artificial\nIntelligence and Statistics (AIstats), pp. 436\u2013443, San Juan,\nPuerto Rico, 2007.\nScott, W.A. Maximum likelihood estimation using the empirical\nfisher information matrix. Journal of Statistical Computation\nand Simulation, 72(8):599\u2013611, 2002.\nSeeger, M.\nsition.\nley, 2004.\ncholupdate.shtml.\nLow rank updates for the cholesky decompo-\nTechnical report, University of California Berke-\nURL http:\/\/lapmal.epfl.ch\/papers\/\nWelling, M. and Teh, Y.W. Bayesian learning via stochastic gra-\ndient langevin dynamics. In Proceedings of the 28th Interna-\ntional Conference on Machine Learning (ICML), pp. 681\u2013688,\n2011."}],"widgetId":"rgw26_56ab1c85d5e07"},"id":"rgw26_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=228095582&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw27_56ab1c85d5e07"},"id":"rgw27_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=228095582&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":228095582,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":228095582,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithSlurp","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2081618664,"url":"researcher\/2081618664_Chunyuan_Li","fullname":"Chunyuan Li","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2075738569,"url":"researcher\/2075738569_Changyou_Chen","fullname":"Changyou Chen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":71191078,"url":"researcher\/71191078_David_Carlson","fullname":"David Carlson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10135830,"url":"researcher\/10135830_Lawrence_Carin","fullname":"Lawrence Carin","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":false,"isSlurp":true,"isNoText":false,"publicationType":"Article","publicationDate":"Dec 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/288059869_Preconditioned_Stochastic_Gradient_Langevin_Dynamics_for_Deep_Neural_Networks","usePlainButton":true,"publicationUid":288059869,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/288059869_Preconditioned_Stochastic_Gradient_Langevin_Dynamics_for_Deep_Neural_Networks","title":"Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks","displayTitleAsLink":true,"authors":[{"id":2081618664,"url":"researcher\/2081618664_Chunyuan_Li","fullname":"Chunyuan Li","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2075738569,"url":"researcher\/2075738569_Changyou_Chen","fullname":"Changyou Chen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":71191078,"url":"researcher\/71191078_David_Carlson","fullname":"David Carlson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10135830,"url":"researcher\/10135830_Lawrence_Carin","fullname":"Lawrence Carin","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Effective training of deep neural networks suffers from two main issues. The\nfirst is that the parameter spaces of these models exhibit pathological\ncurvature. Recent methods address this problem by using adaptive\npreconditioning for Stochastic Gradient Descent (SGD). These methods improve\nconvergence by adapting to the local geometry of parameter space. A second\nissue is overfitting, which is typically addressed by early stopping. However,\nrecent work has demonstrated that Bayesian model averaging mitigates this\nproblem. The posterior can be sampled by using Stochastic Gradient Langevin\nDynamics (SGLD). However, the rapidly changing curvature renders default SGLD\nmethods inefficient. Here, we propose combining adaptive preconditioners with\nSGLD. In support of this idea, we give theoretical properties on asymptotic\nconvergence and predictive risk. We also provide empirical results for Logistic\nRegression, Feedforward Neural Nets, and Convolutional Neural Nets,\ndemonstrating that our preconditioned SGLD method gives state-of-the-art\nperformance on these models.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/288059869_Preconditioned_Stochastic_Gradient_Langevin_Dynamics_for_Deep_Neural_Networks","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1512.07666","sourceName":"de.arxiv.org","hasSourceUrl":true},"publicationUid":288059869,"publicationUrl":"publication\/288059869_Preconditioned_Stochastic_Gradient_Langevin_Dynamics_for_Deep_Neural_Networks","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/288059869_Preconditioned_Stochastic_Gradient_Langevin_Dynamics_for_Deep_Neural_Networks\/links\/5681cf0108aebccc4e0bebee\/smallpreview.png","linkId":"5681cf0108aebccc4e0bebee","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=288059869&reference=5681cf0108aebccc4e0bebee&eventCode=&origin=publication_list","widgetId":"rgw31_56ab1c85d5e07"},"id":"rgw31_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=288059869&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":228095582,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/288059869_Preconditioned_Stochastic_Gradient_Langevin_Dynamics_for_Deep_Neural_Networks\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["This encourages the algorithm to explore the full posterior, instead of simply converging to a maximum a posterior (MAP) solution. Later, SGLD was extended by (Ahn, Korattikara, and Welling 2012), (Patterson and Teh 2013) and (Korattikara et al. 2015). Furthermore , higher-order versions of the SGLD with momentum have also been proposed, including stochastic "],"widgetId":"rgw32_56ab1c85d5e07"},"id":"rgw32_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw30_56ab1c85d5e07"},"id":"rgw30_56ab1c85d5e07","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=288059869&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithSlurp","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2083133647,"url":"researcher\/2083133647_Chong_Wang","fullname":"Chong Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2064238818,"url":"researcher\/2064238818_David_M_Blei","fullname":"David M. Blei","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":false,"isSlurp":true,"isNoText":false,"publicationType":"Article","publicationDate":"Oct 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":1,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283043210_A_General_Method_for_Robust_Bayesian_Modeling","usePlainButton":true,"publicationUid":283043210,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/283043210_A_General_Method_for_Robust_Bayesian_Modeling","title":"A General Method for Robust Bayesian Modeling","displayTitleAsLink":true,"authors":[{"id":2083133647,"url":"researcher\/2083133647_Chong_Wang","fullname":"Chong Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2064238818,"url":"researcher\/2064238818_David_M_Blei","fullname":"David M. Blei","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Robust Bayesian models are appealing alternatives to standard models,\nproviding protection from data that contains outliers or other departures from\nthe model assumptions. Historically, robust models were mostly developed on a\ncase-by-case basis; examples include robust linear regression, robust mixture\nmodels, and bursty topic models. In this paper we develop a general approach to\nrobust Bayesian modeling. We show how to turn an existing Bayesian model into a\nrobust model, and then develop a generic strategy for computing with it. We use\nour method to study robust variants of several models, including linear\nregression, Poisson regression, logistic regression, and probabilistic topic\nmodels. We discuss the connections between our methods and existing approaches,\nespecially empirical Bayes and James-Stein estimation.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283043210_A_General_Method_for_Robust_Bayesian_Modeling","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1510.05078","sourceName":"de.arxiv.org","hasSourceUrl":true},"publicationUid":283043210,"publicationUrl":"publication\/283043210_A_General_Method_for_Robust_Bayesian_Modeling","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/283043210_A_General_Method_for_Robust_Bayesian_Modeling\/links\/562ec76508ae518e348386b2\/smallpreview.png","linkId":"562ec76508ae518e348386b2","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=283043210&reference=562ec76508ae518e348386b2&eventCode=&origin=publication_list","widgetId":"rgw34_56ab1c85d5e07"},"id":"rgw34_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=283043210&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":228095582,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283043210_A_General_Method_for_Robust_Bayesian_Modeling\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["We use a model to encode the types of patterns we want to discover in the data\u2014either to predict about future data or explore existing data\u2014and then use a posterior inference algorithm to uncover the realization of those patterns that underlie the observations. Innovations in scalable inference allow us to use Bayesian models to analyze massive data (Hoffman et al., 2013; Welling and Teh, 2011; Ahn et al., 2012; Xing et al., 2013); innovations in generic inference allow us to easily explore a wide variety of models (Ranganath et al., 2014; Wood et al., 2014; Hoffman and Gelman, 2014). Consequently, modern Bayesian modeling has had an impact on many fields, including natural language processing (Teh, 2006), computer vision (Fei-Fei and Perona, 2005), the natural sciences (Pritchard et al., 2000), and the social sciences (Grimmer, 2009). "],"widgetId":"rgw35_56ab1c85d5e07"},"id":"rgw35_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw33_56ab1c85d5e07"},"id":"rgw33_56ab1c85d5e07","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=283043210&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2045181100,"url":"researcher\/2045181100_Sanvesh_Srivastava","fullname":"Sanvesh Srivastava","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084332535,"url":"researcher\/2084332535_Cheng_Li","fullname":"Cheng Li","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9530149,"url":"researcher\/9530149_David_B_Dunson","fullname":"David B. Dunson","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Aug 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space","usePlainButton":true,"publicationUid":281262237,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space","title":"Scalable Bayes via Barycenter in Wasserstein Space","displayTitleAsLink":true,"authors":[{"id":2045181100,"url":"researcher\/2045181100_Sanvesh_Srivastava","fullname":"Sanvesh Srivastava","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084332535,"url":"researcher\/2084332535_Cheng_Li","fullname":"Cheng Li","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9530149,"url":"researcher\/9530149_David_B_Dunson","fullname":"David B. Dunson","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"We propose a novel approach WASP for Bayesian inference when massive size of\nthe data prohibits posterior computations. WASP is estimated in three steps.\nFirst, data are divided into smaller computationally tractable subsets. Second,\nposterior draws of parameters are obtained for every subset after modifying\nsubset posteriors using stochastic approximation. Finally, the empirical\nmeasures of samples from each subset posterior are combined through their\nbarycenter in the Wasserstein space of probability measures. Stochastic\napproximation ensures that posterior uncertainty quantification of the\nbarycenter matches with that of the full data posterior distribution. The\ncombining step can be conducted efficiently through a sparse linear program,\nwhich takes negligible time relative to sampling from subset posteriors,\nfacilitating scaling to massive data. WASP is very general and allows\napplication of existing sampling algorithms to massive data with minimal\nmodifications. We provide theoretical conditions under which rate of\nconvergence of WASP to the delta measure centered at the true parameter\ncoincides with the optimal parametric rate up to a logarithmic factor. WASP is\napplied for scalable Bayesian computations in a nonparametric mixture model and\na movie recommender database containing tens of millions of ratings.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/David_Dunson\/publication\/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space\/links\/55ffe48708ae07629e51e264.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/David_Dunson","sourceName":"David B Dunson","hasSourceUrl":true},"publicationUid":281262237,"publicationUrl":"publication\/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space\/links\/55ffe48708ae07629e51e264\/smallpreview.png","linkId":"55ffe48708ae07629e51e264","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=281262237&reference=55ffe48708ae07629e51e264&eventCode=&origin=publication_list","widgetId":"rgw37_56ab1c85d5e07"},"id":"rgw37_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=281262237&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"55ffe48708ae07629e51e264","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":228095582,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/281262237_Scalable_Bayes_via_Barycenter_in_Wasserstein_Space\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Relying on stochastic approximation, most of these approaches use stochastic gradient descent for optimization (Welling and Teh, 2011; Broderick et al., 2013; Hoffman et al., 2013). Optimization updates are also coupled with sampling using modified Hamiltonian or Langevin Dynamics to improve posterior exploration (Ahn et al., 2012; Korattikara et al., 2014). Recent work in this area has focused on using different factorizations and parametrizations to improve the efficiency of existing methods (Wang and Blei, 2013; Tan and Nott, 2013; Wand, 2014). "],"widgetId":"rgw38_56ab1c85d5e07"},"id":"rgw38_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw36_56ab1c85d5e07"},"id":"rgw36_56ab1c85d5e07","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=281262237&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":228095582,"publicationLink":"publication\/228095582_Bayesian_Posterior_Sampling_via_Stochastic_Gradient_Fisher_Scoring","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw29_56ab1c85d5e07"},"id":"rgw29_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=228095582&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=31","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":31,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw28_56ab1c85d5e07"},"id":"rgw28_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=228095582&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/228095582_Bayesian_Posterior_Sampling_via_Stochastic_Gradient_Fisher_Scoring","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1c85d5e07"},"id":"rgw2_56ab1c85d5e07","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":228095582},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=228095582&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1c85d5e07"},"id":"rgw1_56ab1c85d5e07","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"fQOh\/hA6jLeDe4DjDqSi2p2cQc1bwvCzcEs78\/5UGH3pMYS73d0NdPWiVei57E3AjXIjE4jfCdtSS7SEKDT7y1yfPp20onqIh53KgjOGu5D2aqOLE2EL6ruET3kJtXNNoj7p6EgLCM2fJnmaS+mMA97p5jEBp7KIsPvyKxoB9KfVTwoWJaKTPJlgOF5v\/kjjwcMIj\/JWNUHpb9HysXn0Z33D82xbCnG+brENbRI\/kdfrPY8DtfAsWxJxGh2j5tdzzdPacLxo9jEjypECm2r0cYnh9Iusk6LEkBJTtd9EUsY=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/228095582_Bayesian_Posterior_Sampling_via_Stochastic_Gradient_Fisher_Scoring\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring\" \/>\n<meta property=\"og:description\" content=\"In this paper we address the following question: Can we approximately sample\nfrom a Bayesian posterior distribution if we are only allowed to touch a small\nmini-batch of data-items for every...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/228095582_Bayesian_Posterior_Sampling_via_Stochastic_Gradient_Fisher_Scoring\/links\/02b0457d0cf27908e9dda46b\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/228095582_Bayesian_Posterior_Sampling_via_Stochastic_Gradient_Fisher_Scoring\" \/>\n<meta property=\"rg:id\" content=\"PB:228095582\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring\" \/>\n<meta name=\"citation_author\" content=\"Sungjin Ahn\" \/>\n<meta name=\"citation_author\" content=\"Anoop Korattikara\" \/>\n<meta name=\"citation_author\" content=\"Max Welling\" \/>\n<meta name=\"citation_publication_date\" content=\"2012\/06\/27\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/228095582_Bayesian_Posterior_Sampling_via_Stochastic_Gradient_Fisher_Scoring\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/228095582_Bayesian_Posterior_Sampling_via_Stochastic_Gradient_Fisher_Scoring\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-b7a3a1a3-9b58-44a7-95cb-6a96d2cfc7eb","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":944,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw39_56ab1c85d5e07"},"id":"rgw39_56ab1c85d5e07","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-b7a3a1a3-9b58-44a7-95cb-6a96d2cfc7eb", "ce86d24d847c6e8c79f53652d42f23ccf884e470");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-b7a3a1a3-9b58-44a7-95cb-6a96d2cfc7eb", "ce86d24d847c6e8c79f53652d42f23ccf884e470");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw40_56ab1c85d5e07"},"id":"rgw40_56ab1c85d5e07","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/228095582_Bayesian_Posterior_Sampling_via_Stochastic_Gradient_Fisher_Scoring","requestToken":"ceSWGSoYFYcYBU2LTcgPy+QAJ0EkiaTiukggyZOufVgeo3Tg1S7vWd2sF+ENv42FuJQV8YEpnx7jEK7Rn5zXbjhHKaixBthXNpxMC3X+Wp7\/5YIUZPI5ftgfPi5Fd3IjV4YOYL80KEyC6zwmOqXP8zGMjpDoy5BlU6G9HOqERqyrq9jiaCsL+fjRiEgywvQCepdoi\/lqZkSbkOk5Tdc0OSaCbGOFl6QQpKs0LxeDoHH5UIAdE2EdxLya6mXmMeLAwyJT9rIe\/JstCYeJGglj\/g0NLV7+hj4zYW63UPNzhQY=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=Fxu9Sza8I_OYmU-03QaUFHLXvgqpCYSWE4aqhgk85SEEt6Kh0572WUQV0fxwNJcg","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjI4MDk1NTgyX0JheWVzaWFuX1Bvc3Rlcmlvcl9TYW1wbGluZ192aWFfU3RvY2hhc3RpY19HcmFkaWVudF9GaXNoZXJfU2NvcmluZw%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw42_56ab1c85d5e07"},"id":"rgw42_56ab1c85d5e07","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw41_56ab1c85d5e07"},"id":"rgw41_56ab1c85d5e07","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw43_56ab1c85d5e07"},"id":"rgw43_56ab1c85d5e07","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
