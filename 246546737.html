<!DOCTYPE html> <html lang="en" class="" id="rgw39_56aba1423863e"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="AxBtX5K5NO6BV2DKQ4vqUmqER05fOLKsaSfPLqXcvaJyR1JC02SB77OojcohPb0+oi2mSkMQYZmiyoR/0w4rYuRvQj2iuk7RsdUlCJhkSO68UPUxNnmt3UPqxHEZNR1Hono7j0VCnXqjpQBfTa5mWMgg7D06VXQsBZoa5oLyB8SJC9RbQ+49FvEnM/gafSsKt/SbYaxLsoSr54xGuFuLWGTFR0cQZNDZiNe15KD0xFuq/GAbvGZ9uZlO28gFP1YFjtkzYyXkKFft5JluRvp7BTMbKc6ipDUHKYTx6RwROHI="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-56658b32-e9fa-4c00-a1d2-f18df45dda5a",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/246546737_Dropout_Training_as_Adaptive_Regularization" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Dropout Training as Adaptive Regularization" />
<meta property="og:description" content="Dropout and other feature noising schemes control overfitting by artificially
corrupting the training data. For generalized linear models, dropout performs a
form of adaptive regularization. Using..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/246546737_Dropout_Training_as_Adaptive_Regularization/links/54361a580cf2dc341db2dc01/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/246546737_Dropout_Training_as_Adaptive_Regularization" />
<meta property="rg:id" content="PB:246546737" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Dropout Training as Adaptive Regularization" />
<meta name="citation_author" content="Stefan Wager" />
<meta name="citation_author" content="Sida Wang" />
<meta name="citation_author" content="Percy Liang" />
<meta name="citation_publication_date" content="2013/07/04" />
<meta name="citation_journal_title" content="Advances in neural information processing systems" />
<meta name="citation_issn" content="1049-5258" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Sida_Wang4/publication/246546737_Dropout_Training_as_Adaptive_Regularization/links/54361a580cf2dc341db2dc01.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/246546737_Dropout_Training_as_Adaptive_Regularization" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/246546737_Dropout_Training_as_Adaptive_Regularization" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Dropout Training as Adaptive Regularization (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Dropout Training as Adaptive Regularization on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56aba1423863e" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56aba1423863e" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56aba1423863e">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Dropout%20Training%20as%20Adaptive%20Regularization&rft.title=Advances%20in%20Neural%20Information%20Processing%20Systems&rft.jtitle=Advances%20in%20Neural%20Information%20Processing%20Systems&rft.date=2013&rft.issn=1049-5258&rft.au=Stefan%20Wager%2CSida%20Wang%2CPercy%20Liang&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Dropout Training as Adaptive Regularization</h1> <meta itemprop="headline" content="Dropout Training as Adaptive Regularization">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/246546737_Dropout_Training_as_Adaptive_Regularization/links/54361a580cf2dc341db2dc01/smallpreview.png">  <div id="rgw8_56aba1423863e" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56aba1423863e" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Stefan_Wager" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Stefan Wager" alt="Stefan Wager" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Stefan Wager</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw10_56aba1423863e" data-account-key="Stefan_Wager">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Stefan_Wager"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Stefan Wager" alt="Stefan Wager" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Stefan_Wager" class="display-name">Stefan Wager</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Stanford_University" title="Stanford University">Stanford University</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56aba1423863e" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Sida_Wang4" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Sida Wang" alt="Sida Wang" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Sida Wang</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw12_56aba1423863e" data-account-key="Sida_Wang4">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Sida_Wang4"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Sida Wang" alt="Sida Wang" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Sida_Wang4" class="display-name">Sida Wang</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Stanford_University" title="Stanford University">Stanford University</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw13_56aba1423863e"> <a href="researcher/2019310164_Percy_Liang" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Percy Liang" alt="Percy Liang" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Percy Liang</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw14_56aba1423863e">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2019310164_Percy_Liang"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Percy Liang" alt="Percy Liang" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2019310164_Percy_Liang" class="display-name">Percy Liang</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/1049-5258_Advances_in_neural_information_processing_systems"><span itemprop="name">Advances in neural information processing systems</span></a> </span>        <meta itemprop="datePublished" content="2013-07">  07/2013;               <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1307.1493" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw15_56aba1423863e" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>Dropout and other feature noising schemes control overfitting by artificially<br />
corrupting the training data. For generalized linear models, dropout performs a<br />
form of adaptive regularization. Using this viewpoint, we show that the dropout<br />
regularizer is first-order equivalent to an L2 regularizer applied after<br />
scaling the features by an estimate of the inverse diagonal Fisher information<br />
matrix. We also establish a connection to AdaGrad, an online learner, and find<br />
that a close relative of AdaGrad operates by repeatedly solving linear<br />
dropout-regularized problems. By casting dropout as regularization, we develop<br />
a natural semi-supervised algorithm that uses unlabeled data to create a better<br />
adaptive regularizer. We apply this idea to document classification tasks, and<br />
show that it consistently boosts the performance of dropout training, improving<br />
on state-of-the-art results on the IMDB reviews dataset.</div> </p>  </div>   </div>      <div class="action-container"> <div id="rgw16_56aba1423863e" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw30_56aba1423863e">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw31_56aba1423863e">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Sida_Wang4/publication/246546737_Dropout_Training_as_Adaptive_Regularization/links/54361a580cf2dc341db2dc01.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Sida_Wang4">Sida Wang</a>, <span class="js-publication-date"> Oct 09, 2014 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw33_56aba1423863e" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw34_56aba1423863e" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw35_56aba1423863e" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw36_56aba1423863e" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw37_56aba1423863e" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw38_56aba1423863e" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw32_56aba1423863e" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FSida_Wang4%2Fpublication%2F246546737_Dropout_Training_as_Adaptive_Regularization%2Flinks%2F54361a580cf2dc341db2dc01.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw29_56aba1423863e"  itemprop="articleBody">  <p>Page 1</p> <p>Dropout Training as Adaptive Regularization<br />Stefan Wager∗, Sida Wang†, and Percy Liang†<br />Departments of Statistics∗and Computer Science†<br />Stanford University, Stanford, CA-94305<br />swager@stanford.edu, {sidaw, pliang}@cs.stanford.edu<br />Abstract<br />Dropout and other feature noising schemes control overfitting by artificially cor-<br />rupting the training data. For generalized linear models, dropout performs a form<br />of adaptive regularization. Using this viewpoint, we show that the dropout regular-<br />izer is first-order equivalent to an L2regularizer applied after scaling the features<br />byanestimateoftheinversediagonalFisherinformationmatrix. Wealsoestablish<br />a connection to AdaGrad, an online learning algorithm, and find that a close rel-<br />ative of AdaGrad operates by repeatedly solving linear dropout-regularized prob-<br />lems. By casting dropout as regularization, we develop a natural semi-supervised<br />algorithm that uses unlabeled data to create a better adaptive regularizer. We ap-<br />ply this idea to document classification tasks, and show that it consistently boosts<br />the performance of dropout training, improving on state-of-the-art results on the<br />IMDB reviews dataset.<br />1 Introduction<br />Dropout training was introduced by Hinton et al. [1] as a way to control overfitting by randomly<br />omitting subsets of features at each iteration of a training procedure.1Although dropout has proved<br />to be a very successful technique, the reasons for its success are not yet well understood at a theo-<br />retical level.<br />Dropout training falls into the broader category of learning methods that artificially corrupt train-<br />ing data to stabilize predictions [2, 4, 5, 6, 7]. There is a well-known connection between artificial<br />feature corruption and regularization [8, 9, 10]. For example, Bishop [9] showed that the effect of<br />training with features that have been corrupted with additive Gaussian noise is equivalent to a form<br />of L2-type regularization in the low noise limit. In this paper, we take a step towards understand-<br />ing how dropout training works by analyzing it as a regularizer. We focus on generalized linear<br />models (GLMs), a class of models for which feature dropout reduces to a form of adaptive model<br />regularization.<br />Using this framework, we show that dropout training is first-order equivalent to L2-regularization af-<br />ter transforming the input by diag(ˆI)−1/2, whereˆI is an estimate of the Fisher information matrix.<br />This transformation effectively makes the level curves of the objective more spherical, and so bal-<br />ances out the regularization applied to different features. In the case of logistic regression, dropout<br />can be interpreted as a form of adaptive L2-regularization that favors rare but useful features.<br />The problem of learning with rare but useful features is discussed in the context of online learning<br />by Duchi et al. [11], who show that their AdaGrad adaptive descent procedure achieves better regret<br />bounds than regular stochastic gradient descent (SGD) in this setting. Here, we show that AdaGrad<br />S.W. is supported by a B.C. and E.J. Eaves Stanford Graduate Fellowship.<br />1Hinton et al. introduced dropout training in the context of neural networks specifically, and also advocated<br />omitting random hidden layers during training. In this paper, we follow [2, 3] and study feature dropout as a<br />generic training method that can be applied to any learning algorithm.<br />1<br />arXiv:1307.1493v2  [stat.ML]  1 Nov 2013</p>  <p>Page 2</p> <p>and dropout training have an intimate connection: Just as SGD progresses by repeatedly solving<br />linearized L2-regularized problems, a close relative of AdaGrad advances by solving linearized<br />dropout-regularized problems.<br />Our formulation of dropout training as adaptive regularization also leads to a simple semi-supervised<br />learning scheme, where we use unlabeled data to learn a better dropout regularizer. The approach<br />is fully discriminative and does not require fitting a generative model. We apply this idea to several<br />document classification problems, and find that it consistently improves the performance of dropout<br />training. On the benchmark IMDB reviews dataset introduced by [12], dropout logistic regression<br />with a regularizer tuned on unlabeled data outperforms previous state-of-the-art. In follow-up re-<br />search [13], we extend the results from this paper to more complicated structured prediction, such<br />as multi-class logistic regression and linear chain conditional random fields.<br />2<br />We begin by discussing the general connections between feature noising and regularization in gen-<br />eralized linear models (GLMs). We will apply the machinery developed here to dropout training in<br />Section 4.<br />A GLM defines a conditional distribution over a response y ∈ Y given an input feature vector<br />x ∈ Rd:<br />pβ(y | x)<br />Here, h(y) is a quantity independent of x and β, A(·) is the log-partition function, and ?x,y(β) is the<br />loss function (i.e., the negative log likelihood); Table 1 contains a summary of notation. Common<br />examples of GLMs include linear (Y = R), logistic (Y = {0,1}), and Poisson (Y = {0,1,2,...})<br />regression.<br />Artificial Feature Noising as Regularization<br />def<br />= h(y)exp{y x · β − A(x · β)},?x,y(β)<br />def<br />= −logpβ(y | x).<br />(1)<br />Given n training examples (xi,yi), the standard maximum likelihood estimateˆβ ∈ Rdminimizes<br />the empirical loss over the training examples:<br />ˆβ<br />def<br />= arg min<br />β∈Rd<br />n<br />?<br />i=1<br />?xi,yi(β).<br />(2)<br />With artificial feature noising, we replace the observed feature vectors xiwith noisy versions ˜ xi=<br />ν(xi,ξi), where ν is our noising function and ξiis an independent random variable. We first create<br />many noisy copies of the dataset, and then average out the auxiliary noise. In this paper, we will<br />consider two types of noise:<br />• Additive Gaussian noise: ν(xi, ξi) = xi+ ξi, where ξi∼ N(0, σ2Id×d).<br />• Dropout noise: ν(xi, ξi) = xi? ξi, where ? is the elementwise product of two vec-<br />tors. Each component of ξi ∈ {0, (1 − δ)−1}dis an independent draw from a scaled<br />Bernoulli(1−δ) random variable. In other words, dropout noise corresponds to setting ˜ xij<br />to 0 with probability δ and to xij/(1 − δ) else.2<br />Integrating over the feature noise gives us a noised maximum likelihood parameter estimate:<br />ˆβ = arg min<br />β∈Rd<br />n<br />?<br />i=1<br />Eξ[?˜ xi,yi(β)], where Eξ[Z]<br />def<br />= E[Z | {xi, yi}]<br />(3)<br />is the expectation taken with respect to the artificial feature noise ξ = (ξ1, ..., ξn). Similar expres-<br />sions have been studied by [9, 10].<br />For GLMs, the noised empirical loss takes on a simpler form:<br />n<br />?<br />i=1<br />Eξ[?˜ xi,yi(β)] =<br />n<br />?<br />i=1<br />−(y xi· β − Eξ[A(˜ xi· β)]) =<br />n<br />?<br />i=1<br />?xi,yi(β) + R(β).<br />(4)<br />2Artificial noise of the form xi? ξiis also called blankout noise. For GLMs, blankout noise is equivalent<br />to dropout noise as defined by [1].<br />2</p>  <p>Page 3</p> <p>Table 1: Summary of notation.<br />xi<br />˜ xi<br />A(x · β)<br />Observed feature vector<br />Noised feature vector<br />Log-partition function<br />R(β)<br />Rq(β)<br />?(β)<br />Noising penalty (5)<br />Quadratic approximation (6)<br />Negative log-likelihood (loss)<br />The first equality holds provided that Eξ[˜ xi] = xi, and the second is true with the following defini-<br />tion:<br />n<br />?<br />Here, R(β)actsasaregularizerthatincorporatestheeffectofartificialfeaturenoising. InGLMs, the<br />log-partition function A must always be convex, and so R is always positive by Jensen’s inequality.<br />The key observation here is that the effect of artificial feature noising reduces to a penalty R(β)<br />that does not depend on the labels {yi}. Because of this, artificial feature noising penalizes the<br />complexity of a classifier in a way that does not depend on the accuracy of a classifier. Thus, for<br />GLMs, artificial feature noising is a regularization scheme on the model itself that can be compared<br />with other forms of regularization such as ridge (L2) or lasso (L1) penalization. In Section 6, we<br />exploit the label-independence of the noising penalty and use unlabeled data to tune our estimate of<br />R(β).<br />The fact that R does not depend on the labels has another useful consequence that relates to predic-<br />tion. The natural prediction rule with artificially noised features is to select ˆ y to minimize expected<br />loss over the added noise: ˆ y = argminyEξ[?˜ x,y(ˆβ)]. It is common practice, however, not to noise<br />the inputs and just to output classification decisions based on the original feature vector [1, 3, 14]:<br />ˆ y = argminy?x,y(ˆβ). It is easy to verify that these expressions are in general not equivalent, but<br />they are equivalent when the effect of feature noising reduces to a label-independent penalty on the<br />likelihood. Thus, the common practice of predicting with clean features is formally justified for<br />GLMs.<br />R(β)<br />def<br />=<br />i=1<br />Eξ[A(˜ xi· β)] − A(xi· β).<br />(5)<br />2.1<br />Although the noising penalty R yields an explicit regularizer that does not depend on the labels<br />{yi}, the form of R can be difficult to interpret. To gain more insight, we will work with a quadratic<br />approximation of the type used by [9, 10]. By taking a second-order Taylor expansion of A around<br />x · β, we get that Eξ[A(˜ x · β)] − A(x · β) ≈<br />Eξ[A?(x · β)(˜ x − x)] vanishes because Eξ[˜ x] = x. Applying this quadratic approximation to (5)<br />yields the following quadratic noising regularizer, which will play a pivotal role in the rest of the<br />paper:<br />=1<br />2<br />i=1<br />This regularizer penalizes two types of variance over the training examples: (i) A??(xi· β), which<br />corresponds to the variance of the response yiin the GLM, and (ii) Varξ[˜ xi· β], the variance of the<br />estimated GLM parameter due to noising.3<br />A Quadratic Approximation to the Noising Penalty<br />1<br />2A??(x · β) Varξ[˜ x · β]. Here the first-order term<br />Rq(β)<br />def<br />n<br />?<br />A??(xi· β) Varξ[˜ xi· β].<br />(6)<br />Accuracy of approximation<br />gression in the case that ˜ x·β is Gaussian;4we vary the mean parameter p<br />noise level σ. We see that Rqis generally very accurate, although it tends to overestimate the true<br />penalty for p ≈ 0.5 and tends to underestimate it for very confident predictions. We give a graphical<br />explanation for this phenomenon in the Appendix (Figure A.1).<br />The quadratic approximation also appears to hold up on real datasets. In Figure 1b, we com-<br />pare the evolution during training of both R and Rqon the 20 newsgroups alt.atheism vs<br />Figure 1a compares the noising penalties R and Rqfor logistic re-<br />def<br />= (1+e−x·β)−1and the<br />3Although Rqis not convex, we were still able (using an L-BFGS algorithm) to train logistic regression<br />with Rqas a surrogate for the dropout regularizer without running into any major issues with local optima.<br />4This assumption holds a priori for additive Gaussian noise, and can be reasonable for dropout by the central<br />limit theorem.<br />3</p>  <p>Page 4</p> <p>0.00.5 1.01.5<br />0.00<br />0.05<br />0.10<br />0.15<br />0.20<br />0.25<br />0.30<br />Sigma<br />Noising Penalty<br />p = 0.5<br />p = 0.73<br />p = 0.82<br />p = 0.88<br />p = 0.95<br />(a) Comparison of noising penalties R and Rqfor<br />logistic regression with Gaussian perturbations,<br />i.e., (˜ x − x) · β ∼ N(0, σ2). The solid line<br />indicates the true penalty and the dashed one is<br />our quadratic approximation thereof; p = (1 +<br />e−x·β)−1is the mean parameter for the logistic<br />model.<br />050 100150<br />10<br />20<br />50<br />100<br />200<br />500<br />Training Iteration<br />Loss<br />Dropout Penalty<br />Quadratic Penalty<br />Negative Log−Likelihood<br />(b) Comparing the evolution of the exact dropout<br />penalty R and our quadratic approximation Rq<br />for logistic regression on the AthR classification<br />task in [15] with 22K features and n = 1000<br />examples. The horizontal axis is the number of<br />quasi-Newton steps taken while training with ex-<br />act dropout.<br />Figure 1: Validating the quadratic approximation.<br />soc.religion.christian classification task described in [15]. We see that the quadratic ap-<br />proximation is accurate most of the way through the learning procedure, only deteriorating slightly<br />as the model converges to highly confident predictions.<br />In practice, we have found that fitting logistic regression with the quadratic surrogate Rqgives<br />similar results to actual dropout-regularized logistic regression. We use this technique for our ex-<br />periments in Section 6.<br />3<br />Having established the general quadratic noising regularizer Rq, we now turn to studying the ef-<br />fects of Rqfor various likelihoods (linear and logistic regression) and noising models (additive and<br />dropout). In this section, we warm up with additive noise; in Section 4 we turn to our main target of<br />interest, namely dropout noise.<br />Regularization based on Additive Noise<br />Linear regression<br />the original feature vector x. Note that Varξ[˜ x · β] = σ2?β?2<br />A(z) =1<br />noising penalty:<br />Suppose ˜ x = x + ε is generated by by adding noise with Var[ε] = σ2Id×dto<br />2, and in the case of linear regression<br />2z2, so A??(z) = 1. Applying these facts to (6) yields a simplified form for the quadratic<br />Rq(β) =1<br />2σ2n?β?2<br />2.<br />(7)<br />Thus, we recover the well-known result that linear regression with additive feature noising is equiv-<br />alent to ridge regression [2, 9]. Note that, with linear regression, the quadratic approximation Rqis<br />exact and so the correspondence with L2-regularization is also exact.<br />Logistic regression<br />The situation gets more interesting when we move beyond linear regression.<br />For logistic regression, A??(xi· β) = pi(1 − pi) where pi= (1 + exp(−xi· β))−1is the predicted<br />probability of yi= 1. The quadratic noising penalty is then<br />Rq(β) =1<br />2σ2?β?2<br />2<br />n<br />?<br />i=1<br />pi(1 − pi).<br />(8)<br />In other words, the noising penalty now simultaneously encourages parsimonious modeling as be-<br />fore (by encouraging ?β?2<br />move away from1<br />2to be small) as well as confident predictions (by encouraging the pi’s to<br />2).<br />4</p>  <p>Page 5</p> <p>Table 2: Form of the different regularization schemes. These expressions assume that the design<br />matrix has been normalized, i.e., that?<br />Linear RegressionLogistic Regression<br />L2-penalization<br />?β?2<br />Additive Noising<br />?β?2<br />Dropout Training<br />?β?2<br />4Regularization based on Dropout Noise<br />Recall that dropout training corresponds to applying dropout noise to training examples, where<br />the noised features ˜ xiare obtained by setting ˜ xijto 0 with some “dropout probability” δ and to<br />xij/(1 − δ) with probability (1 − δ), independently for each coordinate j of the feature vector. We<br />can check that:<br />Varξ[˜ xi· β] =1<br />2<br />and so the quadratic dropout penalty is<br />n<br />?<br />Letting X ∈ Rn×dbe the design matrix with rows xiand V (β) ∈ Rn×nbe a diagonal matrix with<br />entries A??(xi· β), we can re-write this penalty as<br />Rq(β) =1<br />2<br />Let β∗be the maximum likelihood estimate given infinite data. When computed at β∗, the matrix<br />1<br />nX?V (β∗)X =<br />n<br />dropout can be seen as an attempt to apply an L2penalty after normalizing the feature vector by<br />diag(I)−1/2. The Fisher information is linked to the shape of the level surfaces of ?(β) around β∗.<br />If I were a multiple of the identity matrix, then these level surfaces would be perfectly spherical<br />around β∗. Dropout, by normalizing the problem by diag(I)−1/2, ensures that while the level<br />surfaces of ?(β) may not be spherical, the L2-penalty is applied in a basis where the features have<br />been balanced out. We give a graphical illustration of this phenomenon in Figure A.2.<br />Linear Regression<br />For linear regression, V is the identity matrix, so the dropout objective is<br />equivalent to a form of ridge regression where each column of the design matrix is normalized<br />before applying the L2penalty.5This connection has been noted previously by [3].<br />Logistic Regression<br />The form of dropout penalties becomes much more intriguing once we move<br />beyond the realm of linear regression. The case of logistic regression is particularly interesting.<br />Here, we can write the quadratic dropout penalty from (10) as<br />?<br />Thus, just like additive noising, dropout generally gives an advantage to confident predictions and<br />small β. However, unlike all the other methods considered so far, dropout may allow for some large<br />pi(1 − pi) and some large β2<br />Our analysis shows that dropout regularization should be better than L2-regularization for learning<br />weights for features that are rare (i.e., often 0) but highly discriminative, because dropout effectively<br />does not penalize βjover observations for which xij= 0. Thus, in order for a feature to earn a large<br />β2<br />is active.6Dropout training has been empirically found to perform well on tasks such as document<br />ix2<br />ij= 1 for all j. The pi = (1 + e−xi·β)−1are mean<br />parameters for the logistic model.<br />GLM<br />?β?2<br />2tr(V (β))<br />2<br />?β?2<br />?<br />22<br />2<br />?β?2<br />?<br />2<br />ipi(1 − pi)<br />?β?2<br />2<br />i,jpi(1 − pi)x2<br />ijβ2<br />j<br />β?diag(X?V (β)X)β<br />δ<br />1 − δ<br />d<br />?<br />j=1<br />x2<br />ijβ2<br />j,<br />(9)<br />Rq(β) =1<br />2<br />δ<br />1 − δ<br />i=1<br />A??(xi· β)<br />d<br />?<br />j=1<br />x2<br />ijβ2<br />j.<br />(10)<br />δ<br />1 − δβ?diag(X?V (β)X)β.<br />(11)<br />1<br />?n<br />i=1∇2?xi,yi(β∗) is an estimate of the Fisher information matrix I. Thus,<br />Rq(β) =1<br />2<br />δ<br />1 − δ<br />n<br />i=1<br />d<br />?<br />j=1<br />pi(1 − pi)x2<br />ijβ2<br />j.<br />(12)<br />j, provided that the corresponding cross-term x2<br />ijis small.<br />j, it suffices for it to contribute to a confident prediction with small pi(1 − pi) each time that it<br />5Normalizing the columns of the design matrix before performing penalized regression is standard practice,<br />and is implemented by default in software like glmnet for R [16].<br />6To be precise, dropout does not reward all rare but discriminative features. Rather, dropout rewards those<br />features that are rare and positively co-adapted with other features in a way that enables the model to make<br />confident predictions whenever the feature of interest is active.<br />5</p>  <p>Page 6</p> <p>Table 3: Accuracy of L2and dropout regularized logistic regression on a simulated example. The<br />first row indicates results over test examples where some of the rare useful features are active (i.e.,<br />where there is some signal that can be exploited), while the second row indicates accuracy over the<br />full test set. These results are averaged over 100 simulation runs, with 75 training examples in each.<br />All tuning parameters were set to optimal values. The sampling error on all reported values is within<br />±0.01.<br />Accuracy<br />L2-regularization<br />Active Instances0.66<br />All Instances0.53<br />Dropout training<br />0.73<br />0.55<br />classification where rare but discriminative features are prevalent [3]. Our result suggests that this is<br />no mere coincidence.<br />We summarize the relationship between L2-penalization, additive noising and dropout in Table 2.<br />Additive noising introduces a product-form penalty depending on both β and A??. However, the full<br />potential of artificial feature noising only emerges with dropout, which allows the penalty terms due<br />to β and A??to interact in a non-trivial way through the design matrix X (except for linear regression,<br />in which all the noising schemes we consider collapse to ridge regression).<br />4.1<br />The above discussion suggests that dropout logistic regression should perform well with rare but<br />useful features. To test this intuition empirically, we designed a simulation study where all the<br />signal is grouped in 50 rare features, each of which is active only 4% of the time. We then added<br />1000 nuisance features that are always active to the design matrix, for a total of d = 1050 features.<br />To make sure that our experiment was picking up the effect of dropout training specifically and not<br />just normalization of X, we ensured that the columns of X were normalized in expectation.<br />The dropout penalty for logistic regression can be written as a matrix product<br />A Simulation Example<br />Rq(β) =1<br />2<br />δ<br />1 − δ(···<br />pi(1 − pi)<br />···)<br /><br /><br />···<br />x2<br />···<br />···<br />ij<br />···<br /><br /><br /><br /><br />···<br />β2<br />···<br />j<br /><br />.<br />(13)<br />We designed the simulation study in such a way that, at the optimal β, the dropout penalty should<br />have structure<br />Small<br />(confident prediction)<br />Big<br />(weak prediction)<br />??<br />··· ···<br />0<br />···<br /><br /><br /><br /><br /><br /><br />Big<br />(useful feature)<br />Small<br />(nuisance feature)<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />.<br />(14)<br />A dropout penalty with such a structure should be small. Although there are some uncertain pre-<br />dictions with large pi(1 − pi) and some big weights β2<br />corresponding terms x2<br />tures and thus have no signal). Meanwhile, L2penalization has no natural way of penalizing some<br />βjmore and others less. Our simulation results, given in Table 3, confirm that dropout training<br />outperforms L2-regularization here as expected. See Appendix A.1 for details.<br />j, these terms cannot interact because the<br />ijare all 0 (these are examples without any of the rare discriminative fea-<br />5<br />There is a well-known connection between L2-regularization and stochastic gradient descent (SGD).<br />In SGD, the weight vectorˆβ is updated withˆβt+1 =ˆβt− ηtgt, where gt = ∇?xt,yt(ˆβt) is the<br />gradient of the loss due to the t-th training example. We can also write this update as a linear<br />L2-penalized problem<br />?<br />where the first two terms form a linear approximation to the loss and the third term is an L2-<br />regularizer. Thus, SGD progresses by repeatedly solving linearized L2-regularized problems.<br />Dropout Regularization in Online Learning<br />ˆβt+1= argminβ<br />?xt,yt(ˆβt) + gt· (β −ˆβt) +<br />1<br />2ηt?β −ˆβt?2<br />2<br />?<br />,<br />(15)<br />6</p>  <p>Page 7</p> <p>0 10000<br />size of unlabeled data<br />200003000040000<br />0.8<br />0.82<br />0.84<br />0.86<br />0.88<br />0.9<br />accuracy<br /> <br /> <br />dropout+unlabeled<br />dropout<br />L2<br />50001000015000<br />0.8<br />0.82<br />0.84<br />0.86<br />0.88<br />0.9<br />size of labeled data<br />accuracy<br /> <br /> <br />dropout+unlabeled<br />dropout<br />L2<br />Figure 2: Test set accuracy on the IMDB dataset [12] with unigram features. Left: 10000 labeled<br />training examples, and up to 40000 unlabeled examples. Right: 3000-15000 labeled training exam-<br />ples, and 25000 unlabeled examples. The unlabeled data is discounted by a factor α = 0.4.<br />As discussed by Duchi et al. [11], a problem with classic SGD is that it can be slow at learning<br />weights corresponding to rare but highly discriminative features. This problem can be alleviated<br />by running a modified form of SGD withˆβt+1 =ˆβt− η A−1<br />also learned online; this leads to the AdaGrad family of stochastic descent rules. Duchi et al. use<br />At = diag(Gt)1/2where Gt =<br />bounds in the presence of rare but useful features. At least superficially, AdaGrad and dropout seem<br />to have similar goals: For logistic regression, they can both be understood as adaptive alternatives<br />to methods based on L2-regularization that favor learning rare, useful features. As it turns out, they<br />have a deeper connection.<br />The natural way to incorporate dropout regularization into SGD is to replace the penalty term ?β −<br />ˆβ?2<br />ˆβt+1= argminβ<br />?xt,yt(ˆβt) + gt· (β −ˆβt) + Rq(β −ˆβt;ˆβt)<br />where, Rq(·;ˆβt) is the quadratic noising regularizer centered atˆβt:7<br />Rq(β −ˆβt;ˆβt) =1<br />t gt, where the transformation Atis<br />?t<br />i=1gig?<br />i and show that this choice achieves desirable regret<br />2/2η in (15) with the dropout regularizer, giving us an update rule<br />??<br />(16)<br />2(β −ˆβt)?diag(Ht)(β −ˆβt),where Ht=<br />t<br />?<br />i=1<br />∇2?xi,yi(ˆβt).<br />(17)<br />This implies that dropout descent is first-order equivalent to an adaptive SGD procedure with At=<br />diag(Ht). To see the connection between AdaGrad and this dropout-based online procedure, recall<br />that for GLMs both of the expressions<br />Eβ∗?∇2?x,y(β∗)?= Eβ∗?∇?x,y(β∗)∇?x,y(β∗)??<br />are equal to the Fisher information I [17]. In other words, asˆβtconverges to β∗, Gtand Htare both<br />consistent estimates of the Fisher information. Thus, by using dropout instead of L2-regularization<br />to solve linearized problems in online learning, we end up with an AdaGrad-like algorithm.<br />Of course, the connection between AdaGrad and dropout is not perfect. In particular, AdaGrad<br />allows for a more aggressive learning rate by using At = diag(Gt)−1/2instead of diag(Gt)−1.<br />But, at a high level, AdaGrad and dropout appear to both be aiming for the same goal: scaling<br />the features by the Fisher information to make the level-curves of the objective more circular. In<br />contrast, L2-regularization makes no attempt to sphere the level curves, and AROW [18]—another<br />popular adaptive method for online learning—only attempts to normalize the effective feature matrix<br />but does not consider the sensitivity of the loss to changes in the model weights. In the case of<br />logistic regression, AROW also favors learning rare features, but unlike dropout and AdaGrad does<br />not privilege confident predictions.<br />(18)<br />7This expression is equivalent to (11) except that we usedˆβtand not β −ˆβtto compute Ht.<br />7</p>  <p>Page 8</p> <p>Table 4: Performance of semi-supervised dropout training for document classification.<br />(a) Test accuracy with and without unlabeled data on<br />different datasets. Each dataset is split into 3 parts<br />of equal sizes: train, unlabeled, and test. Log. Reg.:<br />logistic regression with L2 regularization; Dropout:<br />dropout trained with quadratic surrogate; +Unla-<br />beled: using unlabeled data.<br />Datasets Log. Reg. Dropout +Unlabeled<br />Subj 88.96<br />RT73.49<br />IMDB-2k80.63<br />XGraph83.10<br />BbCrypt97.28<br />IMDB 87.14<br />90.85<br />75.18<br />81.23<br />84.64<br />98.49<br />88.70<br />91.48<br />76.56<br />80.33<br />85.41<br />99.24<br />89.21<br />(b) Test accuracy on the IMDB dataset [12]. Labeled:<br />usingjustlabeleddatafromeachpaper/method, +Un-<br />labeled: useadditionalunlabeleddata. Drop: dropout<br />with Rq, MNB: multionomial naive Bayes with semi-<br />supervised frequency estimate from [19],8-Uni: uni-<br />gram features, -Bi: bigram features.<br />Methods Labeled +Unlabeled<br />MNB-Uni [19]83.62<br />MNB-Bi [19] 86.63<br />Vect.Sent[12]88.33<br />NBSVM[15]-Bi91.22<br />Drop-Uni 87.78<br />Drop-Bi91.31<br />84.13<br />86.98<br />88.89<br />–<br />89.52<br />91.98<br />6<br />Recall that the regularizer R(β) in (5) is independent of the labels {yi}. As a result, we can use<br />additional unlabeled training examples to estimate it more accurately. Suppose we have an unlabeled<br />dataset {zi} of size m, and let α ∈ (0,1] be a discount factor for the unlabeled data. Then we can<br />define a semi-supervised penalty estimate<br />n<br />n + αm<br />where R(β) is the original penalty estimate and RUnlabeled(β) =?<br />validation; empirically, α ∈ [0.1,0.4] works well. For convenience, we optimize the quadratic<br />surrogate Rq<br />from [3] for estimating R∗(β).<br />Most approaches to semi-supervised learning either rely on using a generative model [19, 20, 21, 22,<br />23] or various assumptions on the relationship between the predictor and the marginal distribution<br />over inputs. Our semi-supervised approach is based on a different intuition: we’d like to set weights<br />to make confident predictions on unlabeled data as well as the labeled data, an intuition shared by<br />entropy regularization [24] and transductive SVMs [25].<br />Semi-Supervised Dropout Training<br />R∗(β)<br />def<br />=<br />?<br />R(β) + αRUnlabeled(β)<br />?<br />,<br />(19)<br />iEξ[A(zi· β)] − A(zi· β) is<br />computed using (5) over the unlabeled examples zi. We select the discount parameter by cross-<br />∗instead of R∗. Another practical option would be to use the Gaussian approximation<br />Experiments<br />datasets described in [15] are shown in Table 4a; Figure 2 illustrates how the use of unlabeled data<br />improves the performance of our classifier on a single dataset. Overall, we see that using unlabeled<br />data to learn a better regularizer R∗(β) consistently improves the performance of dropout training.<br />Table 4b shows our results on the IMDB dataset of [12]. The dataset contains 50,000 unlabeled<br />examples in addition to the labeled train and test sets of size 25,000 each. Whereas the train and<br />test examples are either positive or negative, the unlabeled examples contain neutral reviews as well.<br />We train a dropout-regularized logistic regression classifier on unigram/bigram features, and use the<br />unlabeled data to tune our regularizer. Our method benefits from unlabeled data even in the presence<br />of a large amount of labeled data, and achieves state-of-the-art accuracy on this dataset.<br />We apply this semi-supervised technique to text classification. Results on several<br />7<br />We analyzed dropout training as a form of adaptive regularization. This framework enabled us<br />to uncover close connections between dropout training, adaptively balanced L2-regularization, and<br />AdaGrad; and led to a simple yet effective method for semi-supervised training. There seem to be<br />multiple opportunities for digging deeper into the connection between dropout training and adaptive<br />regularization. In particular, it would be interesting to see whether the dropout regularizer takes<br />on a tractable and/or interpretable form in neural networks, and whether similar semi-supervised<br />schemes could be used to improve on the results presented in [1].<br />Conclusion<br />8Our implementation of semi-supervised MNB. MNB with EM [20] failed to give an improvement.<br />8</p>  <p>Page 9</p> <p>References<br />[1] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-<br />nov. Improving neural networks by preventing co-adaptation of feature detectors.<br />arXiv:1207.0580, 2012.<br />[2] Laurens van der Maaten, Minmin Chen, Stephen Tyree, and Kilian Q Weinberger.<br />marginalized corrupted features. In Proceedings of the International Conference on Machine Learning,<br />2013.<br />[3] Sida I Wang and Christopher D Manning. Fast dropout training. In Proceedings of the International<br />Conference on Machine Learning, 2013.<br />[4] Yaser S Abu-Mostafa. Learning from hints in neural networks. Journal of Complexity, 6(2):192–198,<br />1990.<br />[5] Chris J.C. Burges and Bernhard Schlkopf. Improving the accuracy and speed of support vector machines.<br />In Advances in Neural Information Processing Systems, pages 375–381, 1997.<br />[6] Patrice Y Simard, Yann A Le Cun, John S Denker, and Bernard Victorri. Transformation invariance in<br />pattern recognition: Tangent distance and propagation. International Journal of Imaging Systems and<br />Technology, 11(3):181–197, 2000.<br />[7] Salah Rifai, Yann Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. The manifold tangent<br />classifier. Advances in Neural Information Processing Systems, 24:2294–2302, 2011.<br />[8] Kiyotoshi Matsuoka. Noise injection into inputs in back-propagation learning. Systems, Man and Cyber-<br />netics, IEEE Transactions on, 22(3):436–440, 1992.<br />[9] Chris M Bishop. Training with noise is equivalent to Tikhonov regularization. Neural computation,<br />7(1):108–116, 1995.<br />[10] Salah Rifai, Xavier Glorot, Yoshua Bengio, and Pascal Vincent. Adding noise to the input of a model<br />trained with a regularized objective. arXiv preprint arXiv:1104.3250, 2011.<br />[11] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and<br />stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, 2010.<br />[12] Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.<br />Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Associa-<br />tion for Computational Linguistics, pages 142–150. Association for Computational Linguistics, 2011.<br />[13] Sida I Wang, Mengqiu Wang, Stefan Wager, Percy Liang, and Christopher D Manning. Feature noising<br />for log-linear structured prediction. In Empirical Methods in Natural Language Processing, 2013.<br />[14] Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout<br />networks. In Proceedings of the International Conference on Machine Learning, 2013.<br />[15] Sida Wang and Christopher D Manning. Baselines and bigrams: Simple, good sentiment and topic clas-<br />sification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,<br />pages 90–94. Association for Computational Linguistics, 2012.<br />[16] Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Regularization paths for generalized linear models<br />via coordinate descent. Journal of Statistical Software, 33(1):1, 2010.<br />[17] Erich Leo Lehmann and George Casella. Theory of Point Estimation. Springer, 1998.<br />[18] Koby Crammer, Alex Kulesza, Mark Dredze, et al. Adaptive regularization of weight vectors. Advances<br />in Neural Information Processing Systems, 22:414–422, 2009.<br />[19] Jiang Su, Jelber Sayyad Shirab, and Stan Matwin. Large scale text classification using semi-supervised<br />multinomial naive Bayes. In Proceedings of the International Conference on Machine Learning, 2011.<br />[20] Kamal Nigam, Andrew Kachites McCallum, Sebastian Thrun, and Tom Mitchell. Text classification from<br />labeled and unlabeled documents using EM. Machine Learning, 39(2-3):103–134, May 2000.<br />[21] G. Bouchard and B. Triggs. The trade-off between generative and discriminative classifiers. In Interna-<br />tional Conference on Computational Statistics, pages 721–728, 2004.<br />[22] R. Raina, Y. Shen, A. Ng, and A. McCallum. Classification with hybrid generative/discriminative models.<br />In Advances in Neural Information Processing Systems, Cambridge, MA, 2004. MIT Press.<br />[23] J. Suzuki, A. Fujino, and H. Isozaki. Semi-supervised structured output learning based on a hybrid<br />generative and discriminative approach. In Empirical Methods in Natural Language Processing and<br />Computational Natural Language Learning, 2007.<br />[24] Y. Grandvalet and Y. Bengio. Entropy regularization. In Semi-Supervised Learning, United Kingdom,<br />2005. Springer.<br />[25] Thorsten Joachims. Transductive inference for text classification using support vector machines. In<br />Proceedings of the International Conference on Machine Learning, pages 200–209, 1999.<br />arXiv preprint<br />Learning with<br />9</p>  <p>Page 10</p> <p>A Appendix<br />Figure A.1: Quadratic approximations to the logistic loss. We see that the red curve, namely the<br />quadratic approximation taken at η = 0, p = 1/(1 + eη) = 0.5 is always above the actual loss<br />curve. Meanwhile, quadratic approximations taken at the more extreme locations of p = 0.05 and<br />p = 0.95 undershoot the true loss over a large range. Note that the curvature of the loss is symmetric<br />in the natural parameter η and so the performance of the quadratic approximation is equivalent at p<br />and 1 − p for all p ∈ (0, 1).<br />A.1Description of Simulation Study<br />Section 4.1 gives the motivation for and a high-level description of our simulation study. Here, we<br />give a detailed description of the study.<br />Generating features.<br />5 groups of 10; the last 1000 features are nuisance terms. Each xiwas independently generated as<br />follows:<br />Our simulation has 1050 features. The first 50 discriminative features form<br />1. Pick a group number g ∈ 1, ..., 25, and a sign sgn = ±1.<br />2. If g ≤ 5, draw the entries of xiwith index between 10(g − 1) + 1 and 10(g − 1) + 10<br />uniformly from sgn · Exp(C), where C is selected such that E[(xi)2<br />the other discriminative features to 0. If g &gt; 5, set all the discriminative features to 0.<br />3. Draw the last 1000 entries of xiindependently from N(0,1).<br />Notice that this procedure guarantees that the columns of X all have the same expected second<br />moments.<br />j] = 1 for all j. Set all<br />Generating labels.<br />σ(xi· β), where the first 50 coordinates of β are 0.057 and the remaining 1000 coordinates are 0.<br />The value 0.057 was selected to make the average value of |xi· β| in the presence of signal be 2.<br />Training.<br />Foreach simulationrun, wegenerated atrainingset ofsizen = 75. For thispurpose, we<br />cycled over the group number g deterministically. The penalization parameters were set to roughly<br />optimal values. For dropout, we used δ = 0.9 while from L2-penalization we used λ = 32.<br />Given an xi, we generate yifrom the Bernoulli distribution with parameter<br />10</p>  <p>Page 11</p> <p>Figure A.2: Comparison of two L2regularizers. In both cases, the black solid ellipses are level sur-<br />faces of the likelihood and the blue dashed curves are level surfaces of the regularizer; the optimum<br />of the regularized objective is denoted by OPT. The left panel shows a classic spherical L2regulizer<br />?β?2<br />of the likelihood (I is the Fisher information matrix). The second regularizer is still aligned with<br />the axes, but the relative importance of each axis is now scaled using the curvature of the likelihood<br />function. As argued in (11), dropout training is comparable to the setup depicted in the right panel.<br />2, whereas the right panel has an L2regularizer β?diag(I)β that has been adapted to the shape<br />11</p>  <a href="https://www.researchgate.net/profile/Sida_Wang4/publication/246546737_Dropout_Training_as_Adaptive_Regularization/links/54361a580cf2dc341db2dc01.pdf">Download full-text</a> </div> <div id="rgw21_56aba1423863e" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw22_56aba1423863e">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw23_56aba1423863e"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Sida_Wang4/publication/246546737_Dropout_Training_as_Adaptive_Regularization/links/54361a580cf2dc341db2dc01.pdf" class="publication-viewer" title="54361a580cf2dc341db2dc01.pdf">54361a580cf2dc341db2dc01.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Sida_Wang4">Sida Wang</a> &middot; Oct 9, 2014 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw24_56aba1423863e"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://export.arxiv.org/pdf/1307.1493" target="_blank" rel="nofollow" class="publication-viewer" title="Dropout Training as Adaptive Regularization">Dropout Training as Adaptive Regularization</a> </div>  <div class="details">   Available from <a href="http://export.arxiv.org/pdf/1307.1493" target="_blank" rel="nofollow">export.arxiv.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw26_56aba1423863e" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw27_56aba1423863e">  </ul> </div> </div>   <div id="rgw17_56aba1423863e" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw18_56aba1423863e"> <div> <h5> <a href="publication/251044571_Adaptive_regularization_for_super-resolution-image_reconstruction_based_on_local_structures_Adaptive_regularization_for_super-resolution-image_reconstruction_based_on_local_structures" class="color-inherit ga-similar-publication-title"><span class="publication-title">Adaptive regularization for super-resolution-image reconstruction based on local structures: Adaptive regularization for super-resolution-image reconstruction based on local structures</span></a>  </h5>  <div class="authors"> <a href="researcher/2023882643_Jian-hua_YUAN" class="authors ga-similar-publication-author">Jian-hua YUAN</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56aba1423863e"> <div> <h5> <a href="publication/287250378_Dropout_Training_of_Matrix_Factorization_and_Autoencoder_for_Link_Prediction_in_Sparse_Graphs" class="color-inherit ga-similar-publication-title"><span class="publication-title">Dropout Training of Matrix Factorization and Autoencoder for Link Prediction in Sparse Graphs</span></a>  </h5>  <div class="authors"> <a href="researcher/2089511288_Shuangfei_Zhai" class="authors ga-similar-publication-author">Shuangfei Zhai</a>, <a href="researcher/2089489597_Zhongfei_Zhang" class="authors ga-similar-publication-author">Zhongfei Zhang</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw20_56aba1423863e"> <div> <h5> <a href="publication/287034172_Fast_dropout_training" class="color-inherit ga-similar-publication-title"><span class="publication-title">Fast dropout training</span></a>  </h5>  <div class="authors"> <a href="researcher/2088292542_SI_Wang" class="authors ga-similar-publication-author">S.I. Wang</a>, <a href="researcher/2088297253_CD_Manning" class="authors ga-similar-publication-author">C.D. Manning</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw40_56aba1423863e" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw41_56aba1423863e">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw42_56aba1423863e" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=pi_X2_I_nqH5y9X8R2o4dszxuGGcFAwpFKRXkPEIY3Ll2yHCIwdEtklCdjuHv1Ps" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="q93MvbLIrURpG4k0KwjcyTyYPVBF0cr8g2vD0L1aqezt5MSCGnVHfJQc6BbM1zwsLmiY5af57JMIQrDoWiWSyfRR7auB2duQhc15IL4GM6lJ8Hu8csQHJpFiEm9w1etmPWrQjREmBElG3itiyGkNyZc922INl1lCCxEI5S8v9muEaahrhsAXbjfyCcgzvfu/dOmw8mdBS63GSh5KLhCEcfe8aJIBYsS80qh13d6+7GMY2MvpJTXcUD0CTjr9q/WA4ul+5mpMqykvLZcaFIA5ZJVfRjXtpVlPm4vcCQXaVlo="/> <input type="hidden" name="urlAfterLogin" value="publication/246546737_Dropout_Training_as_Adaptive_Regularization"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjQ2NTQ2NzM3X0Ryb3BvdXRfVHJhaW5pbmdfYXNfQWRhcHRpdmVfUmVndWxhcml6YXRpb24%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjQ2NTQ2NzM3X0Ryb3BvdXRfVHJhaW5pbmdfYXNfQWRhcHRpdmVfUmVndWxhcml6YXRpb24%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjQ2NTQ2NzM3X0Ryb3BvdXRfVHJhaW5pbmdfYXNfQWRhcHRpdmVfUmVndWxhcml6YXRpb24%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw43_56aba1423863e"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 406;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Stefan Wager","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Stefan_Wager","institution":"Stanford University","institutionUrl":false,"widgetId":"rgw4_56aba1423863e"},"id":"rgw4_56aba1423863e","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=3270934","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56aba1423863e"},"id":"rgw3_56aba1423863e","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=246546737","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":246546737,"title":"Dropout Training as Adaptive Regularization","journalTitle":"Advances in neural information processing systems","journalDetailsTooltip":{"data":{"journalTitle":"Advances in neural information processing systems","journalAbbrev":"Adv Neural Inform Process Syst","publisher":"IEEE Conference on Neural Information Processing Systems--Natural and Synthetic, Massachusetts Institute of Technology Press","issn":"1049-5258","impactFactor":"0.00","fiveYearImpactFactor":"0.00","citedHalfLife":"0.00","immediacyIndex":"0.00","eigenFactor":"0.00","articleInfluence":"0.00","widgetId":"rgw6_56aba1423863e"},"id":"rgw6_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=1049-5258","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"07\/2013;","publicationDateRobot":"2013-07","article":"","journalTitle":"Advances in neural information processing systems","journalUrl":"journal\/1049-5258_Advances_in_neural_information_processing_systems"}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1307.1493","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Dropout Training as Adaptive Regularization"},{"key":"rft.title","value":"Advances in Neural Information Processing Systems"},{"key":"rft.jtitle","value":"Advances in Neural Information Processing Systems"},{"key":"rft.date","value":"2013"},{"key":"rft.issn","value":"1049-5258"},{"key":"rft.au","value":"Stefan Wager,Sida Wang,Percy Liang"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56aba1423863e"},"id":"rgw7_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=246546737","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":246546737,"peopleItems":[{"data":{"authorNameOnPublication":"Stefan Wager","accountUrl":"profile\/Stefan_Wager","accountKey":"Stefan_Wager","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Stefan Wager","profile":{"professionalInstitution":{"professionalInstitutionName":"Stanford University","professionalInstitutionUrl":"institution\/Stanford_University"}},"professionalInstitutionName":"Stanford University","professionalInstitutionUrl":"institution\/Stanford_University","url":"profile\/Stefan_Wager","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Stefan_Wager","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw10_56aba1423863e"},"id":"rgw10_56aba1423863e","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=3270934&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Stanford University","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":2,"publicationUid":246546737,"widgetId":"rgw9_56aba1423863e"},"id":"rgw9_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=3270934&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=2&publicationUid=246546737","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Sida Wang","accountUrl":"profile\/Sida_Wang4","accountKey":"Sida_Wang4","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Sida Wang","profile":{"professionalInstitution":{"professionalInstitutionName":"Stanford University","professionalInstitutionUrl":"institution\/Stanford_University"}},"professionalInstitutionName":"Stanford University","professionalInstitutionUrl":"institution\/Stanford_University","url":"profile\/Sida_Wang4","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Sida_Wang4","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw12_56aba1423863e"},"id":"rgw12_56aba1423863e","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=5897080&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Stanford University","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":2,"publicationUid":246546737,"widgetId":"rgw11_56aba1423863e"},"id":"rgw11_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=5897080&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=2&publicationUid=246546737","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/2019310164_Percy_Liang","authorNameOnPublication":"Percy Liang","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Percy Liang","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2019310164_Percy_Liang","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw14_56aba1423863e"},"id":"rgw14_56aba1423863e","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2019310164&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw13_56aba1423863e"},"id":"rgw13_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2019310164&authorNameOnPublication=Percy%20Liang","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56aba1423863e"},"id":"rgw8_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=246546737&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":246546737,"abstract":"<noscript><\/noscript><div>Dropout and other feature noising schemes control overfitting by artificially<br \/>\ncorrupting the training data. For generalized linear models, dropout performs a<br \/>\nform of adaptive regularization. Using this viewpoint, we show that the dropout<br \/>\nregularizer is first-order equivalent to an L2 regularizer applied after<br \/>\nscaling the features by an estimate of the inverse diagonal Fisher information<br \/>\nmatrix. We also establish a connection to AdaGrad, an online learner, and find<br \/>\nthat a close relative of AdaGrad operates by repeatedly solving linear<br \/>\ndropout-regularized problems. By casting dropout as regularization, we develop<br \/>\na natural semi-supervised algorithm that uses unlabeled data to create a better<br \/>\nadaptive regularizer. We apply this idea to document classification tasks, and<br \/>\nshow that it consistently boosts the performance of dropout training, improving<br \/>\non state-of-the-art results on the IMDB reviews dataset.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw15_56aba1423863e"},"id":"rgw15_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=246546737","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/246546737_Dropout_Training_as_Adaptive_Regularization\/links\/54361a580cf2dc341db2dc01\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw16_56aba1423863e"},"id":"rgw16_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56aba1423863e"},"id":"rgw5_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=246546737&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2023882643,"url":"researcher\/2023882643_Jian-hua_YUAN","fullname":"Jian-hua YUAN","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Dec 2009","journal":"Journal of Computer Applications","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/251044571_Adaptive_regularization_for_super-resolution-image_reconstruction_based_on_local_structures_Adaptive_regularization_for_super-resolution-image_reconstruction_based_on_local_structures","usePlainButton":true,"publicationUid":251044571,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/251044571_Adaptive_regularization_for_super-resolution-image_reconstruction_based_on_local_structures_Adaptive_regularization_for_super-resolution-image_reconstruction_based_on_local_structures","title":"Adaptive regularization for super-resolution-image reconstruction based on local structures: Adaptive regularization for super-resolution-image reconstruction based on local structures","displayTitleAsLink":true,"authors":[{"id":2023882643,"url":"researcher\/2023882643_Jian-hua_YUAN","fullname":"Jian-hua YUAN","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of Computer Applications 12\/2009; 29(11):3008-3010. DOI:10.3724\/SP.J.1087.2009.03008"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/251044571_Adaptive_regularization_for_super-resolution-image_reconstruction_based_on_local_structures_Adaptive_regularization_for_super-resolution-image_reconstruction_based_on_local_structures","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/251044571_Adaptive_regularization_for_super-resolution-image_reconstruction_based_on_local_structures_Adaptive_regularization_for_super-resolution-image_reconstruction_based_on_local_structures\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56aba1423863e"},"id":"rgw18_56aba1423863e","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=251044571","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2089511288,"url":"researcher\/2089511288_Shuangfei_Zhai","fullname":"Shuangfei Zhai","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089489597,"url":"researcher\/2089489597_Zhongfei_Zhang","fullname":"Zhongfei Zhang","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Dec 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/287250378_Dropout_Training_of_Matrix_Factorization_and_Autoencoder_for_Link_Prediction_in_Sparse_Graphs","usePlainButton":true,"publicationUid":287250378,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/287250378_Dropout_Training_of_Matrix_Factorization_and_Autoencoder_for_Link_Prediction_in_Sparse_Graphs","title":"Dropout Training of Matrix Factorization and Autoencoder for Link Prediction in Sparse Graphs","displayTitleAsLink":true,"authors":[{"id":2089511288,"url":"researcher\/2089511288_Shuangfei_Zhai","fullname":"Shuangfei Zhai","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089489597,"url":"researcher\/2089489597_Zhongfei_Zhang","fullname":"Zhongfei Zhang","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/287250378_Dropout_Training_of_Matrix_Factorization_and_Autoencoder_for_Link_Prediction_in_Sparse_Graphs","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/287250378_Dropout_Training_of_Matrix_Factorization_and_Autoencoder_for_Link_Prediction_in_Sparse_Graphs\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56aba1423863e"},"id":"rgw19_56aba1423863e","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=287250378","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2088292542,"url":"researcher\/2088292542_SI_Wang","fullname":"S.I. Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2088297253,"url":"researcher\/2088297253_CD_Manning","fullname":"C.D. Manning","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2013","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/287034172_Fast_dropout_training","usePlainButton":true,"publicationUid":287034172,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/287034172_Fast_dropout_training","title":"Fast dropout training","displayTitleAsLink":true,"authors":[{"id":2088292542,"url":"researcher\/2088292542_SI_Wang","fullname":"S.I. Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2088297253,"url":"researcher\/2088297253_CD_Manning","fullname":"C.D. Manning","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/287034172_Fast_dropout_training","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/287034172_Fast_dropout_training\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56aba1423863e"},"id":"rgw20_56aba1423863e","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=287034172","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw17_56aba1423863e"},"id":"rgw17_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=246546737&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":246546737,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":246546737,"publicationType":"article","linkId":"54361a580cf2dc341db2dc01","fileName":"54361a580cf2dc341db2dc01.pdf","fileUrl":"profile\/Sida_Wang4\/publication\/246546737_Dropout_Training_as_Adaptive_Regularization\/links\/54361a580cf2dc341db2dc01.pdf","name":"Sida Wang","nameUrl":"profile\/Sida_Wang4","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Oct 9, 2014","fileSize":"381.09 KB","widgetId":"rgw23_56aba1423863e"},"id":"rgw23_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=246546737&linkId=54361a580cf2dc341db2dc01&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":246546737,"publicationType":"article","linkId":"02f487170cf21189773be7cf","fileName":"Dropout Training as Adaptive Regularization","fileUrl":"http:\/\/export.arxiv.org\/pdf\/1307.1493","name":"export.arxiv.org","nameUrl":"http:\/\/export.arxiv.org\/pdf\/1307.1493","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw24_56aba1423863e"},"id":"rgw24_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=246546737&linkId=02f487170cf21189773be7cf&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw22_56aba1423863e"},"id":"rgw22_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=246546737&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":31,"valueFormatted":"31","widgetId":"rgw25_56aba1423863e"},"id":"rgw25_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=246546737","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw21_56aba1423863e"},"id":"rgw21_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=246546737&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":246546737,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw27_56aba1423863e"},"id":"rgw27_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=246546737&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":31,"valueFormatted":"31","widgetId":"rgw28_56aba1423863e"},"id":"rgw28_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=246546737","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw26_56aba1423863e"},"id":"rgw26_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=246546737&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Dropout Training as Adaptive Regularization\nStefan Wager\u2217, Sida Wang\u2020, and Percy Liang\u2020\nDepartments of Statistics\u2217and Computer Science\u2020\nStanford University, Stanford, CA-94305\nswager@stanford.edu, {sidaw, pliang}@cs.stanford.edu\nAbstract\nDropout and other feature noising schemes control overfitting by artificially cor-\nrupting the training data. For generalized linear models, dropout performs a form\nof adaptive regularization. Using this viewpoint, we show that the dropout regular-\nizer is first-order equivalent to an L2regularizer applied after scaling the features\nbyanestimateoftheinversediagonalFisherinformationmatrix. Wealsoestablish\na connection to AdaGrad, an online learning algorithm, and find that a close rel-\native of AdaGrad operates by repeatedly solving linear dropout-regularized prob-\nlems. By casting dropout as regularization, we develop a natural semi-supervised\nalgorithm that uses unlabeled data to create a better adaptive regularizer. We ap-\nply this idea to document classification tasks, and show that it consistently boosts\nthe performance of dropout training, improving on state-of-the-art results on the\nIMDB reviews dataset.\n1 Introduction\nDropout training was introduced by Hinton et al. [1] as a way to control overfitting by randomly\nomitting subsets of features at each iteration of a training procedure.1Although dropout has proved\nto be a very successful technique, the reasons for its success are not yet well understood at a theo-\nretical level.\nDropout training falls into the broader category of learning methods that artificially corrupt train-\ning data to stabilize predictions [2, 4, 5, 6, 7]. There is a well-known connection between artificial\nfeature corruption and regularization [8, 9, 10]. For example, Bishop [9] showed that the effect of\ntraining with features that have been corrupted with additive Gaussian noise is equivalent to a form\nof L2-type regularization in the low noise limit. In this paper, we take a step towards understand-\ning how dropout training works by analyzing it as a regularizer. We focus on generalized linear\nmodels (GLMs), a class of models for which feature dropout reduces to a form of adaptive model\nregularization.\nUsing this framework, we show that dropout training is first-order equivalent to L2-regularization af-\nter transforming the input by diag(\u02c6I)\u22121\/2, where\u02c6I is an estimate of the Fisher information matrix.\nThis transformation effectively makes the level curves of the objective more spherical, and so bal-\nances out the regularization applied to different features. In the case of logistic regression, dropout\ncan be interpreted as a form of adaptive L2-regularization that favors rare but useful features.\nThe problem of learning with rare but useful features is discussed in the context of online learning\nby Duchi et al. [11], who show that their AdaGrad adaptive descent procedure achieves better regret\nbounds than regular stochastic gradient descent (SGD) in this setting. Here, we show that AdaGrad\nS.W. is supported by a B.C. and E.J. Eaves Stanford Graduate Fellowship.\n1Hinton et al. introduced dropout training in the context of neural networks specifically, and also advocated\nomitting random hidden layers during training. In this paper, we follow [2, 3] and study feature dropout as a\ngeneric training method that can be applied to any learning algorithm.\n1\narXiv:1307.1493v2  [stat.ML]  1 Nov 2013"},{"page":2,"text":"and dropout training have an intimate connection: Just as SGD progresses by repeatedly solving\nlinearized L2-regularized problems, a close relative of AdaGrad advances by solving linearized\ndropout-regularized problems.\nOur formulation of dropout training as adaptive regularization also leads to a simple semi-supervised\nlearning scheme, where we use unlabeled data to learn a better dropout regularizer. The approach\nis fully discriminative and does not require fitting a generative model. We apply this idea to several\ndocument classification problems, and find that it consistently improves the performance of dropout\ntraining. On the benchmark IMDB reviews dataset introduced by [12], dropout logistic regression\nwith a regularizer tuned on unlabeled data outperforms previous state-of-the-art. In follow-up re-\nsearch [13], we extend the results from this paper to more complicated structured prediction, such\nas multi-class logistic regression and linear chain conditional random fields.\n2\nWe begin by discussing the general connections between feature noising and regularization in gen-\neralized linear models (GLMs). We will apply the machinery developed here to dropout training in\nSection 4.\nA GLM defines a conditional distribution over a response y \u2208 Y given an input feature vector\nx \u2208 Rd:\np\u03b2(y | x)\nHere, h(y) is a quantity independent of x and \u03b2, A(\u00b7) is the log-partition function, and ?x,y(\u03b2) is the\nloss function (i.e., the negative log likelihood); Table 1 contains a summary of notation. Common\nexamples of GLMs include linear (Y = R), logistic (Y = {0,1}), and Poisson (Y = {0,1,2,...})\nregression.\nArtificial Feature Noising as Regularization\ndef\n= h(y)exp{y x \u00b7 \u03b2 \u2212 A(x \u00b7 \u03b2)},?x,y(\u03b2)\ndef\n= \u2212logp\u03b2(y | x).\n(1)\nGiven n training examples (xi,yi), the standard maximum likelihood estimate\u02c6\u03b2 \u2208 Rdminimizes\nthe empirical loss over the training examples:\n\u02c6\u03b2\ndef\n= arg min\n\u03b2\u2208Rd\nn\n?\ni=1\n?xi,yi(\u03b2).\n(2)\nWith artificial feature noising, we replace the observed feature vectors xiwith noisy versions \u02dc xi=\n\u03bd(xi,\u03bei), where \u03bd is our noising function and \u03beiis an independent random variable. We first create\nmany noisy copies of the dataset, and then average out the auxiliary noise. In this paper, we will\nconsider two types of noise:\n\u2022 Additive Gaussian noise: \u03bd(xi, \u03bei) = xi+ \u03bei, where \u03bei\u223c N(0, \u03c32Id\u00d7d).\n\u2022 Dropout noise: \u03bd(xi, \u03bei) = xi? \u03bei, where ? is the elementwise product of two vec-\ntors. Each component of \u03bei \u2208 {0, (1 \u2212 \u03b4)\u22121}dis an independent draw from a scaled\nBernoulli(1\u2212\u03b4) random variable. In other words, dropout noise corresponds to setting \u02dc xij\nto 0 with probability \u03b4 and to xij\/(1 \u2212 \u03b4) else.2\nIntegrating over the feature noise gives us a noised maximum likelihood parameter estimate:\n\u02c6\u03b2 = arg min\n\u03b2\u2208Rd\nn\n?\ni=1\nE\u03be[?\u02dc xi,yi(\u03b2)], where E\u03be[Z]\ndef\n= E[Z | {xi, yi}]\n(3)\nis the expectation taken with respect to the artificial feature noise \u03be = (\u03be1, ..., \u03ben). Similar expres-\nsions have been studied by [9, 10].\nFor GLMs, the noised empirical loss takes on a simpler form:\nn\n?\ni=1\nE\u03be[?\u02dc xi,yi(\u03b2)] =\nn\n?\ni=1\n\u2212(y xi\u00b7 \u03b2 \u2212 E\u03be[A(\u02dc xi\u00b7 \u03b2)]) =\nn\n?\ni=1\n?xi,yi(\u03b2) + R(\u03b2).\n(4)\n2Artificial noise of the form xi? \u03beiis also called blankout noise. For GLMs, blankout noise is equivalent\nto dropout noise as defined by [1].\n2"},{"page":3,"text":"Table 1: Summary of notation.\nxi\n\u02dc xi\nA(x \u00b7 \u03b2)\nObserved feature vector\nNoised feature vector\nLog-partition function\nR(\u03b2)\nRq(\u03b2)\n?(\u03b2)\nNoising penalty (5)\nQuadratic approximation (6)\nNegative log-likelihood (loss)\nThe first equality holds provided that E\u03be[\u02dc xi] = xi, and the second is true with the following defini-\ntion:\nn\n?\nHere, R(\u03b2)actsasaregularizerthatincorporatestheeffectofartificialfeaturenoising. InGLMs, the\nlog-partition function A must always be convex, and so R is always positive by Jensen\u2019s inequality.\nThe key observation here is that the effect of artificial feature noising reduces to a penalty R(\u03b2)\nthat does not depend on the labels {yi}. Because of this, artificial feature noising penalizes the\ncomplexity of a classifier in a way that does not depend on the accuracy of a classifier. Thus, for\nGLMs, artificial feature noising is a regularization scheme on the model itself that can be compared\nwith other forms of regularization such as ridge (L2) or lasso (L1) penalization. In Section 6, we\nexploit the label-independence of the noising penalty and use unlabeled data to tune our estimate of\nR(\u03b2).\nThe fact that R does not depend on the labels has another useful consequence that relates to predic-\ntion. The natural prediction rule with artificially noised features is to select \u02c6 y to minimize expected\nloss over the added noise: \u02c6 y = argminyE\u03be[?\u02dc x,y(\u02c6\u03b2)]. It is common practice, however, not to noise\nthe inputs and just to output classification decisions based on the original feature vector [1, 3, 14]:\n\u02c6 y = argminy?x,y(\u02c6\u03b2). It is easy to verify that these expressions are in general not equivalent, but\nthey are equivalent when the effect of feature noising reduces to a label-independent penalty on the\nlikelihood. Thus, the common practice of predicting with clean features is formally justified for\nGLMs.\nR(\u03b2)\ndef\n=\ni=1\nE\u03be[A(\u02dc xi\u00b7 \u03b2)] \u2212 A(xi\u00b7 \u03b2).\n(5)\n2.1\nAlthough the noising penalty R yields an explicit regularizer that does not depend on the labels\n{yi}, the form of R can be difficult to interpret. To gain more insight, we will work with a quadratic\napproximation of the type used by [9, 10]. By taking a second-order Taylor expansion of A around\nx \u00b7 \u03b2, we get that E\u03be[A(\u02dc x \u00b7 \u03b2)] \u2212 A(x \u00b7 \u03b2) \u2248\nE\u03be[A?(x \u00b7 \u03b2)(\u02dc x \u2212 x)] vanishes because E\u03be[\u02dc x] = x. Applying this quadratic approximation to (5)\nyields the following quadratic noising regularizer, which will play a pivotal role in the rest of the\npaper:\n=1\n2\ni=1\nThis regularizer penalizes two types of variance over the training examples: (i) A??(xi\u00b7 \u03b2), which\ncorresponds to the variance of the response yiin the GLM, and (ii) Var\u03be[\u02dc xi\u00b7 \u03b2], the variance of the\nestimated GLM parameter due to noising.3\nA Quadratic Approximation to the Noising Penalty\n1\n2A??(x \u00b7 \u03b2) Var\u03be[\u02dc x \u00b7 \u03b2]. Here the first-order term\nRq(\u03b2)\ndef\nn\n?\nA??(xi\u00b7 \u03b2) Var\u03be[\u02dc xi\u00b7 \u03b2].\n(6)\nAccuracy of approximation\ngression in the case that \u02dc x\u00b7\u03b2 is Gaussian;4we vary the mean parameter p\nnoise level \u03c3. We see that Rqis generally very accurate, although it tends to overestimate the true\npenalty for p \u2248 0.5 and tends to underestimate it for very confident predictions. We give a graphical\nexplanation for this phenomenon in the Appendix (Figure A.1).\nThe quadratic approximation also appears to hold up on real datasets. In Figure 1b, we com-\npare the evolution during training of both R and Rqon the 20 newsgroups alt.atheism vs\nFigure 1a compares the noising penalties R and Rqfor logistic re-\ndef\n= (1+e\u2212x\u00b7\u03b2)\u22121and the\n3Although Rqis not convex, we were still able (using an L-BFGS algorithm) to train logistic regression\nwith Rqas a surrogate for the dropout regularizer without running into any major issues with local optima.\n4This assumption holds a priori for additive Gaussian noise, and can be reasonable for dropout by the central\nlimit theorem.\n3"},{"page":4,"text":"0.00.5 1.01.5\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nSigma\nNoising Penalty\np = 0.5\np = 0.73\np = 0.82\np = 0.88\np = 0.95\n(a) Comparison of noising penalties R and Rqfor\nlogistic regression with Gaussian perturbations,\ni.e., (\u02dc x \u2212 x) \u00b7 \u03b2 \u223c N(0, \u03c32). The solid line\nindicates the true penalty and the dashed one is\nour quadratic approximation thereof; p = (1 +\ne\u2212x\u00b7\u03b2)\u22121is the mean parameter for the logistic\nmodel.\n050 100150\n10\n20\n50\n100\n200\n500\nTraining Iteration\nLoss\nDropout Penalty\nQuadratic Penalty\nNegative Log\u2212Likelihood\n(b) Comparing the evolution of the exact dropout\npenalty R and our quadratic approximation Rq\nfor logistic regression on the AthR classification\ntask in [15] with 22K features and n = 1000\nexamples. The horizontal axis is the number of\nquasi-Newton steps taken while training with ex-\nact dropout.\nFigure 1: Validating the quadratic approximation.\nsoc.religion.christian classification task described in [15]. We see that the quadratic ap-\nproximation is accurate most of the way through the learning procedure, only deteriorating slightly\nas the model converges to highly confident predictions.\nIn practice, we have found that fitting logistic regression with the quadratic surrogate Rqgives\nsimilar results to actual dropout-regularized logistic regression. We use this technique for our ex-\nperiments in Section 6.\n3\nHaving established the general quadratic noising regularizer Rq, we now turn to studying the ef-\nfects of Rqfor various likelihoods (linear and logistic regression) and noising models (additive and\ndropout). In this section, we warm up with additive noise; in Section 4 we turn to our main target of\ninterest, namely dropout noise.\nRegularization based on Additive Noise\nLinear regression\nthe original feature vector x. Note that Var\u03be[\u02dc x \u00b7 \u03b2] = \u03c32?\u03b2?2\nA(z) =1\nnoising penalty:\nSuppose \u02dc x = x + \u03b5 is generated by by adding noise with Var[\u03b5] = \u03c32Id\u00d7dto\n2, and in the case of linear regression\n2z2, so A??(z) = 1. Applying these facts to (6) yields a simplified form for the quadratic\nRq(\u03b2) =1\n2\u03c32n?\u03b2?2\n2.\n(7)\nThus, we recover the well-known result that linear regression with additive feature noising is equiv-\nalent to ridge regression [2, 9]. Note that, with linear regression, the quadratic approximation Rqis\nexact and so the correspondence with L2-regularization is also exact.\nLogistic regression\nThe situation gets more interesting when we move beyond linear regression.\nFor logistic regression, A??(xi\u00b7 \u03b2) = pi(1 \u2212 pi) where pi= (1 + exp(\u2212xi\u00b7 \u03b2))\u22121is the predicted\nprobability of yi= 1. The quadratic noising penalty is then\nRq(\u03b2) =1\n2\u03c32?\u03b2?2\n2\nn\n?\ni=1\npi(1 \u2212 pi).\n(8)\nIn other words, the noising penalty now simultaneously encourages parsimonious modeling as be-\nfore (by encouraging ?\u03b2?2\nmove away from1\n2to be small) as well as confident predictions (by encouraging the pi\u2019s to\n2).\n4"},{"page":5,"text":"Table 2: Form of the different regularization schemes. These expressions assume that the design\nmatrix has been normalized, i.e., that?\nLinear RegressionLogistic Regression\nL2-penalization\n?\u03b2?2\nAdditive Noising\n?\u03b2?2\nDropout Training\n?\u03b2?2\n4Regularization based on Dropout Noise\nRecall that dropout training corresponds to applying dropout noise to training examples, where\nthe noised features \u02dc xiare obtained by setting \u02dc xijto 0 with some \u201cdropout probability\u201d \u03b4 and to\nxij\/(1 \u2212 \u03b4) with probability (1 \u2212 \u03b4), independently for each coordinate j of the feature vector. We\ncan check that:\nVar\u03be[\u02dc xi\u00b7 \u03b2] =1\n2\nand so the quadratic dropout penalty is\nn\n?\nLetting X \u2208 Rn\u00d7dbe the design matrix with rows xiand V (\u03b2) \u2208 Rn\u00d7nbe a diagonal matrix with\nentries A??(xi\u00b7 \u03b2), we can re-write this penalty as\nRq(\u03b2) =1\n2\nLet \u03b2\u2217be the maximum likelihood estimate given infinite data. When computed at \u03b2\u2217, the matrix\n1\nnX?V (\u03b2\u2217)X =\nn\ndropout can be seen as an attempt to apply an L2penalty after normalizing the feature vector by\ndiag(I)\u22121\/2. The Fisher information is linked to the shape of the level surfaces of ?(\u03b2) around \u03b2\u2217.\nIf I were a multiple of the identity matrix, then these level surfaces would be perfectly spherical\naround \u03b2\u2217. Dropout, by normalizing the problem by diag(I)\u22121\/2, ensures that while the level\nsurfaces of ?(\u03b2) may not be spherical, the L2-penalty is applied in a basis where the features have\nbeen balanced out. We give a graphical illustration of this phenomenon in Figure A.2.\nLinear Regression\nFor linear regression, V is the identity matrix, so the dropout objective is\nequivalent to a form of ridge regression where each column of the design matrix is normalized\nbefore applying the L2penalty.5This connection has been noted previously by [3].\nLogistic Regression\nThe form of dropout penalties becomes much more intriguing once we move\nbeyond the realm of linear regression. The case of logistic regression is particularly interesting.\nHere, we can write the quadratic dropout penalty from (10) as\n?\nThus, just like additive noising, dropout generally gives an advantage to confident predictions and\nsmall \u03b2. However, unlike all the other methods considered so far, dropout may allow for some large\npi(1 \u2212 pi) and some large \u03b22\nOur analysis shows that dropout regularization should be better than L2-regularization for learning\nweights for features that are rare (i.e., often 0) but highly discriminative, because dropout effectively\ndoes not penalize \u03b2jover observations for which xij= 0. Thus, in order for a feature to earn a large\n\u03b22\nis active.6Dropout training has been empirically found to perform well on tasks such as document\nix2\nij= 1 for all j. The pi = (1 + e\u2212xi\u00b7\u03b2)\u22121are mean\nparameters for the logistic model.\nGLM\n?\u03b2?2\n2tr(V (\u03b2))\n2\n?\u03b2?2\n?\n22\n2\n?\u03b2?2\n?\n2\nipi(1 \u2212 pi)\n?\u03b2?2\n2\ni,jpi(1 \u2212 pi)x2\nij\u03b22\nj\n\u03b2?diag(X?V (\u03b2)X)\u03b2\n\u03b4\n1 \u2212 \u03b4\nd\n?\nj=1\nx2\nij\u03b22\nj,\n(9)\nRq(\u03b2) =1\n2\n\u03b4\n1 \u2212 \u03b4\ni=1\nA??(xi\u00b7 \u03b2)\nd\n?\nj=1\nx2\nij\u03b22\nj.\n(10)\n\u03b4\n1 \u2212 \u03b4\u03b2?diag(X?V (\u03b2)X)\u03b2.\n(11)\n1\n?n\ni=1\u22072?xi,yi(\u03b2\u2217) is an estimate of the Fisher information matrix I. Thus,\nRq(\u03b2) =1\n2\n\u03b4\n1 \u2212 \u03b4\nn\ni=1\nd\n?\nj=1\npi(1 \u2212 pi)x2\nij\u03b22\nj.\n(12)\nj, provided that the corresponding cross-term x2\nijis small.\nj, it suffices for it to contribute to a confident prediction with small pi(1 \u2212 pi) each time that it\n5Normalizing the columns of the design matrix before performing penalized regression is standard practice,\nand is implemented by default in software like glmnet for R [16].\n6To be precise, dropout does not reward all rare but discriminative features. Rather, dropout rewards those\nfeatures that are rare and positively co-adapted with other features in a way that enables the model to make\nconfident predictions whenever the feature of interest is active.\n5"},{"page":6,"text":"Table 3: Accuracy of L2and dropout regularized logistic regression on a simulated example. The\nfirst row indicates results over test examples where some of the rare useful features are active (i.e.,\nwhere there is some signal that can be exploited), while the second row indicates accuracy over the\nfull test set. These results are averaged over 100 simulation runs, with 75 training examples in each.\nAll tuning parameters were set to optimal values. The sampling error on all reported values is within\n\u00b10.01.\nAccuracy\nL2-regularization\nActive Instances0.66\nAll Instances0.53\nDropout training\n0.73\n0.55\nclassification where rare but discriminative features are prevalent [3]. Our result suggests that this is\nno mere coincidence.\nWe summarize the relationship between L2-penalization, additive noising and dropout in Table 2.\nAdditive noising introduces a product-form penalty depending on both \u03b2 and A??. However, the full\npotential of artificial feature noising only emerges with dropout, which allows the penalty terms due\nto \u03b2 and A??to interact in a non-trivial way through the design matrix X (except for linear regression,\nin which all the noising schemes we consider collapse to ridge regression).\n4.1\nThe above discussion suggests that dropout logistic regression should perform well with rare but\nuseful features. To test this intuition empirically, we designed a simulation study where all the\nsignal is grouped in 50 rare features, each of which is active only 4% of the time. We then added\n1000 nuisance features that are always active to the design matrix, for a total of d = 1050 features.\nTo make sure that our experiment was picking up the effect of dropout training specifically and not\njust normalization of X, we ensured that the columns of X were normalized in expectation.\nThe dropout penalty for logistic regression can be written as a matrix product\nA Simulation Example\nRq(\u03b2) =1\n2\n\u03b4\n1 \u2212 \u03b4(\u00b7\u00b7\u00b7\npi(1 \u2212 pi)\n\u00b7\u00b7\u00b7)\n\uf8eb\n\uf8ed\n\u00b7\u00b7\u00b7\nx2\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\nij\n\u00b7\u00b7\u00b7\n\uf8f6\n\uf8f8\n\uf8eb\n\uf8ed\n\u00b7\u00b7\u00b7\n\u03b22\n\u00b7\u00b7\u00b7\nj\n\uf8f6\n\uf8f8.\n(13)\nWe designed the simulation study in such a way that, at the optimal \u03b2, the dropout penalty should\nhave structure\nSmall\n(confident prediction)\nBig\n(weak prediction)\n??\n\u00b7\u00b7\u00b7 \u00b7\u00b7\u00b7\n0\n\u00b7\u00b7\u00b7\n\uf8eb\n\uf8ed\n\uf8ec\n\uf8f6\n\uf8f8\n\uf8f7\nBig\n(useful feature)\nSmall\n(nuisance feature)\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n\uf8ec\n\uf8ec\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8\n\uf8f7\n\uf8f7\n.\n(14)\nA dropout penalty with such a structure should be small. Although there are some uncertain pre-\ndictions with large pi(1 \u2212 pi) and some big weights \u03b22\ncorresponding terms x2\ntures and thus have no signal). Meanwhile, L2penalization has no natural way of penalizing some\n\u03b2jmore and others less. Our simulation results, given in Table 3, confirm that dropout training\noutperforms L2-regularization here as expected. See Appendix A.1 for details.\nj, these terms cannot interact because the\nijare all 0 (these are examples without any of the rare discriminative fea-\n5\nThere is a well-known connection between L2-regularization and stochastic gradient descent (SGD).\nIn SGD, the weight vector\u02c6\u03b2 is updated with\u02c6\u03b2t+1 =\u02c6\u03b2t\u2212 \u03b7tgt, where gt = \u2207?xt,yt(\u02c6\u03b2t) is the\ngradient of the loss due to the t-th training example. We can also write this update as a linear\nL2-penalized problem\n?\nwhere the first two terms form a linear approximation to the loss and the third term is an L2-\nregularizer. Thus, SGD progresses by repeatedly solving linearized L2-regularized problems.\nDropout Regularization in Online Learning\n\u02c6\u03b2t+1= argmin\u03b2\n?xt,yt(\u02c6\u03b2t) + gt\u00b7 (\u03b2 \u2212\u02c6\u03b2t) +\n1\n2\u03b7t?\u03b2 \u2212\u02c6\u03b2t?2\n2\n?\n,\n(15)\n6"},{"page":7,"text":"0 10000\nsize of unlabeled data\n200003000040000\n0.8\n0.82\n0.84\n0.86\n0.88\n0.9\naccuracy\n \n \ndropout+unlabeled\ndropout\nL2\n50001000015000\n0.8\n0.82\n0.84\n0.86\n0.88\n0.9\nsize of labeled data\naccuracy\n \n \ndropout+unlabeled\ndropout\nL2\nFigure 2: Test set accuracy on the IMDB dataset [12] with unigram features. Left: 10000 labeled\ntraining examples, and up to 40000 unlabeled examples. Right: 3000-15000 labeled training exam-\nples, and 25000 unlabeled examples. The unlabeled data is discounted by a factor \u03b1 = 0.4.\nAs discussed by Duchi et al. [11], a problem with classic SGD is that it can be slow at learning\nweights corresponding to rare but highly discriminative features. This problem can be alleviated\nby running a modified form of SGD with\u02c6\u03b2t+1 =\u02c6\u03b2t\u2212 \u03b7 A\u22121\nalso learned online; this leads to the AdaGrad family of stochastic descent rules. Duchi et al. use\nAt = diag(Gt)1\/2where Gt =\nbounds in the presence of rare but useful features. At least superficially, AdaGrad and dropout seem\nto have similar goals: For logistic regression, they can both be understood as adaptive alternatives\nto methods based on L2-regularization that favor learning rare, useful features. As it turns out, they\nhave a deeper connection.\nThe natural way to incorporate dropout regularization into SGD is to replace the penalty term ?\u03b2 \u2212\n\u02c6\u03b2?2\n\u02c6\u03b2t+1= argmin\u03b2\n?xt,yt(\u02c6\u03b2t) + gt\u00b7 (\u03b2 \u2212\u02c6\u03b2t) + Rq(\u03b2 \u2212\u02c6\u03b2t;\u02c6\u03b2t)\nwhere, Rq(\u00b7;\u02c6\u03b2t) is the quadratic noising regularizer centered at\u02c6\u03b2t:7\nRq(\u03b2 \u2212\u02c6\u03b2t;\u02c6\u03b2t) =1\nt gt, where the transformation Atis\n?t\ni=1gig?\ni and show that this choice achieves desirable regret\n2\/2\u03b7 in (15) with the dropout regularizer, giving us an update rule\n??\n(16)\n2(\u03b2 \u2212\u02c6\u03b2t)?diag(Ht)(\u03b2 \u2212\u02c6\u03b2t),where Ht=\nt\n?\ni=1\n\u22072?xi,yi(\u02c6\u03b2t).\n(17)\nThis implies that dropout descent is first-order equivalent to an adaptive SGD procedure with At=\ndiag(Ht). To see the connection between AdaGrad and this dropout-based online procedure, recall\nthat for GLMs both of the expressions\nE\u03b2\u2217?\u22072?x,y(\u03b2\u2217)?= E\u03b2\u2217?\u2207?x,y(\u03b2\u2217)\u2207?x,y(\u03b2\u2217)??\nare equal to the Fisher information I [17]. In other words, as\u02c6\u03b2tconverges to \u03b2\u2217, Gtand Htare both\nconsistent estimates of the Fisher information. Thus, by using dropout instead of L2-regularization\nto solve linearized problems in online learning, we end up with an AdaGrad-like algorithm.\nOf course, the connection between AdaGrad and dropout is not perfect. In particular, AdaGrad\nallows for a more aggressive learning rate by using At = diag(Gt)\u22121\/2instead of diag(Gt)\u22121.\nBut, at a high level, AdaGrad and dropout appear to both be aiming for the same goal: scaling\nthe features by the Fisher information to make the level-curves of the objective more circular. In\ncontrast, L2-regularization makes no attempt to sphere the level curves, and AROW [18]\u2014another\npopular adaptive method for online learning\u2014only attempts to normalize the effective feature matrix\nbut does not consider the sensitivity of the loss to changes in the model weights. In the case of\nlogistic regression, AROW also favors learning rare features, but unlike dropout and AdaGrad does\nnot privilege confident predictions.\n(18)\n7This expression is equivalent to (11) except that we used\u02c6\u03b2tand not \u03b2 \u2212\u02c6\u03b2tto compute Ht.\n7"},{"page":8,"text":"Table 4: Performance of semi-supervised dropout training for document classification.\n(a) Test accuracy with and without unlabeled data on\ndifferent datasets. Each dataset is split into 3 parts\nof equal sizes: train, unlabeled, and test. Log. Reg.:\nlogistic regression with L2 regularization; Dropout:\ndropout trained with quadratic surrogate; +Unla-\nbeled: using unlabeled data.\nDatasets Log. Reg. Dropout +Unlabeled\nSubj 88.96\nRT73.49\nIMDB-2k80.63\nXGraph83.10\nBbCrypt97.28\nIMDB 87.14\n90.85\n75.18\n81.23\n84.64\n98.49\n88.70\n91.48\n76.56\n80.33\n85.41\n99.24\n89.21\n(b) Test accuracy on the IMDB dataset [12]. Labeled:\nusingjustlabeleddatafromeachpaper\/method, +Un-\nlabeled: useadditionalunlabeleddata. Drop: dropout\nwith Rq, MNB: multionomial naive Bayes with semi-\nsupervised frequency estimate from [19],8-Uni: uni-\ngram features, -Bi: bigram features.\nMethods Labeled +Unlabeled\nMNB-Uni [19]83.62\nMNB-Bi [19] 86.63\nVect.Sent[12]88.33\nNBSVM[15]-Bi91.22\nDrop-Uni 87.78\nDrop-Bi91.31\n84.13\n86.98\n88.89\n\u2013\n89.52\n91.98\n6\nRecall that the regularizer R(\u03b2) in (5) is independent of the labels {yi}. As a result, we can use\nadditional unlabeled training examples to estimate it more accurately. Suppose we have an unlabeled\ndataset {zi} of size m, and let \u03b1 \u2208 (0,1] be a discount factor for the unlabeled data. Then we can\ndefine a semi-supervised penalty estimate\nn\nn + \u03b1m\nwhere R(\u03b2) is the original penalty estimate and RUnlabeled(\u03b2) =?\nvalidation; empirically, \u03b1 \u2208 [0.1,0.4] works well. For convenience, we optimize the quadratic\nsurrogate Rq\nfrom [3] for estimating R\u2217(\u03b2).\nMost approaches to semi-supervised learning either rely on using a generative model [19, 20, 21, 22,\n23] or various assumptions on the relationship between the predictor and the marginal distribution\nover inputs. Our semi-supervised approach is based on a different intuition: we\u2019d like to set weights\nto make confident predictions on unlabeled data as well as the labeled data, an intuition shared by\nentropy regularization [24] and transductive SVMs [25].\nSemi-Supervised Dropout Training\nR\u2217(\u03b2)\ndef\n=\n?\nR(\u03b2) + \u03b1RUnlabeled(\u03b2)\n?\n,\n(19)\niE\u03be[A(zi\u00b7 \u03b2)] \u2212 A(zi\u00b7 \u03b2) is\ncomputed using (5) over the unlabeled examples zi. We select the discount parameter by cross-\n\u2217instead of R\u2217. Another practical option would be to use the Gaussian approximation\nExperiments\ndatasets described in [15] are shown in Table 4a; Figure 2 illustrates how the use of unlabeled data\nimproves the performance of our classifier on a single dataset. Overall, we see that using unlabeled\ndata to learn a better regularizer R\u2217(\u03b2) consistently improves the performance of dropout training.\nTable 4b shows our results on the IMDB dataset of [12]. The dataset contains 50,000 unlabeled\nexamples in addition to the labeled train and test sets of size 25,000 each. Whereas the train and\ntest examples are either positive or negative, the unlabeled examples contain neutral reviews as well.\nWe train a dropout-regularized logistic regression classifier on unigram\/bigram features, and use the\nunlabeled data to tune our regularizer. Our method benefits from unlabeled data even in the presence\nof a large amount of labeled data, and achieves state-of-the-art accuracy on this dataset.\nWe apply this semi-supervised technique to text classification. Results on several\n7\nWe analyzed dropout training as a form of adaptive regularization. This framework enabled us\nto uncover close connections between dropout training, adaptively balanced L2-regularization, and\nAdaGrad; and led to a simple yet effective method for semi-supervised training. There seem to be\nmultiple opportunities for digging deeper into the connection between dropout training and adaptive\nregularization. In particular, it would be interesting to see whether the dropout regularizer takes\non a tractable and\/or interpretable form in neural networks, and whether similar semi-supervised\nschemes could be used to improve on the results presented in [1].\nConclusion\n8Our implementation of semi-supervised MNB. MNB with EM [20] failed to give an improvement.\n8"},{"page":9,"text":"References\n[1] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdi-\nnov. Improving neural networks by preventing co-adaptation of feature detectors.\narXiv:1207.0580, 2012.\n[2] Laurens van der Maaten, Minmin Chen, Stephen Tyree, and Kilian Q Weinberger.\nmarginalized corrupted features. In Proceedings of the International Conference on Machine Learning,\n2013.\n[3] Sida I Wang and Christopher D Manning. Fast dropout training. In Proceedings of the International\nConference on Machine Learning, 2013.\n[4] Yaser S Abu-Mostafa. Learning from hints in neural networks. Journal of Complexity, 6(2):192\u2013198,\n1990.\n[5] Chris J.C. Burges and Bernhard Schlkopf. Improving the accuracy and speed of support vector machines.\nIn Advances in Neural Information Processing Systems, pages 375\u2013381, 1997.\n[6] Patrice Y Simard, Yann A Le Cun, John S Denker, and Bernard Victorri. Transformation invariance in\npattern recognition: Tangent distance and propagation. International Journal of Imaging Systems and\nTechnology, 11(3):181\u2013197, 2000.\n[7] Salah Rifai, Yann Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier Muller. The manifold tangent\nclassifier. Advances in Neural Information Processing Systems, 24:2294\u20132302, 2011.\n[8] Kiyotoshi Matsuoka. Noise injection into inputs in back-propagation learning. Systems, Man and Cyber-\nnetics, IEEE Transactions on, 22(3):436\u2013440, 1992.\n[9] Chris M Bishop. Training with noise is equivalent to Tikhonov regularization. Neural computation,\n7(1):108\u2013116, 1995.\n[10] Salah Rifai, Xavier Glorot, Yoshua Bengio, and Pascal Vincent. Adding noise to the input of a model\ntrained with a regularized objective. arXiv preprint arXiv:1104.3250, 2011.\n[11] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\nstochastic optimization. Journal of Machine Learning Research, 12:2121\u20132159, 2010.\n[12] Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.\nLearning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 142\u2013150. Association for Computational Linguistics, 2011.\n[13] Sida I Wang, Mengqiu Wang, Stefan Wager, Percy Liang, and Christopher D Manning. Feature noising\nfor log-linear structured prediction. In Empirical Methods in Natural Language Processing, 2013.\n[14] Ian J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout\nnetworks. In Proceedings of the International Conference on Machine Learning, 2013.\n[15] Sida Wang and Christopher D Manning. Baselines and bigrams: Simple, good sentiment and topic clas-\nsification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,\npages 90\u201394. Association for Computational Linguistics, 2012.\n[16] Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Regularization paths for generalized linear models\nvia coordinate descent. Journal of Statistical Software, 33(1):1, 2010.\n[17] Erich Leo Lehmann and George Casella. Theory of Point Estimation. Springer, 1998.\n[18] Koby Crammer, Alex Kulesza, Mark Dredze, et al. Adaptive regularization of weight vectors. Advances\nin Neural Information Processing Systems, 22:414\u2013422, 2009.\n[19] Jiang Su, Jelber Sayyad Shirab, and Stan Matwin. Large scale text classification using semi-supervised\nmultinomial naive Bayes. In Proceedings of the International Conference on Machine Learning, 2011.\n[20] Kamal Nigam, Andrew Kachites McCallum, Sebastian Thrun, and Tom Mitchell. Text classification from\nlabeled and unlabeled documents using EM. Machine Learning, 39(2-3):103\u2013134, May 2000.\n[21] G. Bouchard and B. Triggs. The trade-off between generative and discriminative classifiers. In Interna-\ntional Conference on Computational Statistics, pages 721\u2013728, 2004.\n[22] R. Raina, Y. Shen, A. Ng, and A. McCallum. Classification with hybrid generative\/discriminative models.\nIn Advances in Neural Information Processing Systems, Cambridge, MA, 2004. MIT Press.\n[23] J. Suzuki, A. Fujino, and H. Isozaki. Semi-supervised structured output learning based on a hybrid\ngenerative and discriminative approach. In Empirical Methods in Natural Language Processing and\nComputational Natural Language Learning, 2007.\n[24] Y. Grandvalet and Y. Bengio. Entropy regularization. In Semi-Supervised Learning, United Kingdom,\n2005. Springer.\n[25] Thorsten Joachims. Transductive inference for text classification using support vector machines. In\nProceedings of the International Conference on Machine Learning, pages 200\u2013209, 1999.\narXiv preprint\nLearning with\n9"},{"page":10,"text":"A Appendix\nFigure A.1: Quadratic approximations to the logistic loss. We see that the red curve, namely the\nquadratic approximation taken at \u03b7 = 0, p = 1\/(1 + e\u03b7) = 0.5 is always above the actual loss\ncurve. Meanwhile, quadratic approximations taken at the more extreme locations of p = 0.05 and\np = 0.95 undershoot the true loss over a large range. Note that the curvature of the loss is symmetric\nin the natural parameter \u03b7 and so the performance of the quadratic approximation is equivalent at p\nand 1 \u2212 p for all p \u2208 (0, 1).\nA.1Description of Simulation Study\nSection 4.1 gives the motivation for and a high-level description of our simulation study. Here, we\ngive a detailed description of the study.\nGenerating features.\n5 groups of 10; the last 1000 features are nuisance terms. Each xiwas independently generated as\nfollows:\nOur simulation has 1050 features. The first 50 discriminative features form\n1. Pick a group number g \u2208 1, ..., 25, and a sign sgn = \u00b11.\n2. If g \u2264 5, draw the entries of xiwith index between 10(g \u2212 1) + 1 and 10(g \u2212 1) + 10\nuniformly from sgn \u00b7 Exp(C), where C is selected such that E[(xi)2\nthe other discriminative features to 0. If g > 5, set all the discriminative features to 0.\n3. Draw the last 1000 entries of xiindependently from N(0,1).\nNotice that this procedure guarantees that the columns of X all have the same expected second\nmoments.\nj] = 1 for all j. Set all\nGenerating labels.\n\u03c3(xi\u00b7 \u03b2), where the first 50 coordinates of \u03b2 are 0.057 and the remaining 1000 coordinates are 0.\nThe value 0.057 was selected to make the average value of |xi\u00b7 \u03b2| in the presence of signal be 2.\nTraining.\nForeach simulationrun, wegenerated atrainingset ofsizen = 75. For thispurpose, we\ncycled over the group number g deterministically. The penalization parameters were set to roughly\noptimal values. For dropout, we used \u03b4 = 0.9 while from L2-penalization we used \u03bb = 32.\nGiven an xi, we generate yifrom the Bernoulli distribution with parameter\n10"},{"page":11,"text":"Figure A.2: Comparison of two L2regularizers. In both cases, the black solid ellipses are level sur-\nfaces of the likelihood and the blue dashed curves are level surfaces of the regularizer; the optimum\nof the regularized objective is denoted by OPT. The left panel shows a classic spherical L2regulizer\n?\u03b2?2\nof the likelihood (I is the Fisher information matrix). The second regularizer is still aligned with\nthe axes, but the relative importance of each axis is now scaled using the curvature of the likelihood\nfunction. As argued in (11), dropout training is comparable to the setup depicted in the right panel.\n2, whereas the right panel has an L2regularizer \u03b2?diag(I)\u03b2 that has been adapted to the shape\n11"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Sida_Wang4\/publication\/246546737_Dropout_Training_as_Adaptive_Regularization\/links\/54361a580cf2dc341db2dc01.pdf","widgetId":"rgw29_56aba1423863e"},"id":"rgw29_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=246546737&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw30_56aba1423863e"},"id":"rgw30_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=246546737&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":246546737,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"54361a580cf2dc341db2dc01","name":"Sida Wang","date":"Oct 09, 2014 ","nameLink":"profile\/Sida_Wang4","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Sida_Wang4\/publication\/246546737_Dropout_Training_as_Adaptive_Regularization\/links\/54361a580cf2dc341db2dc01.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Sida_Wang4\/publication\/246546737_Dropout_Training_as_Adaptive_Regularization\/links\/54361a580cf2dc341db2dc01.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"8213e78ce75372effd497abb35033a7f","showFileSizeNote":false,"fileSize":"381.09 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"54361a580cf2dc341db2dc01","name":"Sida Wang","date":"Oct 09, 2014 ","nameLink":"profile\/Sida_Wang4","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Sida_Wang4\/publication\/246546737_Dropout_Training_as_Adaptive_Regularization\/links\/54361a580cf2dc341db2dc01.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Sida_Wang4\/publication\/246546737_Dropout_Training_as_Adaptive_Regularization\/links\/54361a580cf2dc341db2dc01.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"8213e78ce75372effd497abb35033a7f","showFileSizeNote":false,"fileSize":"381.09 KB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=BuvJjFdj8gzWX76UpmtQ21ElZwVIDjKbjJI4uNLOfd7GpZQL-BvpcoEpVDLvLJ4p-6I43iNCTqMfNG9IPPl4uw.g1mopgZePK036ITOvG-bqRnkJ9Eo1bkkpoy4H_O0vi2fsW91IUVK8YLdAuZZ_N9iRH4H-b-Q6Ti91lJZE6fTCw","clickOnPill":"publication.PublicationFigures.html?_sg=lGWZSoySTEG3UkTJGAVx-y7pLIbn3jMDeN-bUJkq3Zn7wIeIKYs9ULluYQz2xfSVGvPROTqyVSF1SCIKqMhZFw.U0ppVydTKWgrFDISaa_feFuV-oGw006wTdsdOohG9CCcQRvhDSDH_Nhcr8CHSAb02DTyGEW6AI5PDfQlK_iISg"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FSida_Wang4%2Fpublication%2F246546737_Dropout_Training_as_Adaptive_Regularization%2Flinks%2F54361a580cf2dc341db2dc01.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=PhDVkwrpyX2RZYqFjirF-GquLyn_LlFAfylKLIVTkdHFsnJzfAJaT_hvCTg4aJE01x_u1JWiHPg0BdH5xNlAWw","urlHash":"e912727b4b43482507863e6b7e3e1086","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=ek0UgMi51iKyeOQEQPt_cXzuFRTWFf0z3Ja39CZak65xHFawreXlwZmjgghEoLUnPLKHNoa526cy43YfDmEu2IPzSDCsyj7rIOMxtzVbn9k.JQ6bss6K2zqA8XADYAp42f-tfa2C73m0T95ev5DKdY2uwAeyH0cMEkm9MXUH0RkurxoabxRnLtYQtlKLmGnUxA.fLIqojjcjrYT1N7wIWYLHGbyRMEk_Ab2likzeFb5RFM63fTJtcuFPoDTN8Cloj6zyPDO2n4ZPlHBEJoNirU-8A","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"54361a580cf2dc341db2dc01","trackedDownloads":{"54361a580cf2dc341db2dc01":{"v":false,"d":false}},"assetId":"AS:150241747673089@1412831832912","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":246546737,"commentCursorPromo":null,"widgetId":"rgw32_56aba1423863e"},"id":"rgw32_56aba1423863e","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FSida_Wang4%2Fpublication%2F246546737_Dropout_Training_as_Adaptive_Regularization%2Flinks%2F54361a580cf2dc341db2dc01.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A150241747673089%401412831832912&publicationUid=246546737&linkId=54361a580cf2dc341db2dc01&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Dropout Training as Adaptive Regularization","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=7BiIEGmy5bpnthDQodhKjOl3AzpquffKabc6UcWHXM1BWnueMjdv097DgIIY4yeLX1Di2AqlCpxvfF0sDpGmaMnWVQVoZVfOO8Hg1YJSUFU.1wb9S9r4_Ow6Plcb4VvOZDx9d-jenomdMdziDxIhWqac3uzeLQoyRdS0oRbtmFA7RMXcRRtcoYu41rXmXpKQow.IxYtvSgVNRiqgjLvGnreSeL6Ex24JpIeE-dnJZBF-Cjm7pwQoFWlcnORSDZikTiK-P90Qq1UIKAPlLHXJN6E6A","publicationUid":246546737,"trackedDownloads":{"54361a580cf2dc341db2dc01":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw34_56aba1423863e"},"id":"rgw34_56aba1423863e","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw35_56aba1423863e"},"id":"rgw35_56aba1423863e","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw36_56aba1423863e"},"id":"rgw36_56aba1423863e","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw37_56aba1423863e"},"id":"rgw37_56aba1423863e","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw38_56aba1423863e"},"id":"rgw38_56aba1423863e","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw33_56aba1423863e"},"id":"rgw33_56aba1423863e","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw31_56aba1423863e"},"id":"rgw31_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/246546737_Dropout_Training_as_Adaptive_Regularization","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56aba1423863e"},"id":"rgw2_56aba1423863e","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":246546737},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=246546737&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56aba1423863e"},"id":"rgw1_56aba1423863e","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"AxBtX5K5NO6BV2DKQ4vqUmqER05fOLKsaSfPLqXcvaJyR1JC02SB77OojcohPb0+oi2mSkMQYZmiyoR\/0w4rYuRvQj2iuk7RsdUlCJhkSO68UPUxNnmt3UPqxHEZNR1Hono7j0VCnXqjpQBfTa5mWMgg7D06VXQsBZoa5oLyB8SJC9RbQ+49FvEnM\/gafSsKt\/SbYaxLsoSr54xGuFuLWGTFR0cQZNDZiNe15KD0xFuq\/GAbvGZ9uZlO28gFP1YFjtkzYyXkKFft5JluRvp7BTMbKc6ipDUHKYTx6RwROHI=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/246546737_Dropout_Training_as_Adaptive_Regularization\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Dropout Training as Adaptive Regularization\" \/>\n<meta property=\"og:description\" content=\"Dropout and other feature noising schemes control overfitting by artificially\ncorrupting the training data. For generalized linear models, dropout performs a\nform of adaptive regularization. Using...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/246546737_Dropout_Training_as_Adaptive_Regularization\/links\/54361a580cf2dc341db2dc01\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/246546737_Dropout_Training_as_Adaptive_Regularization\" \/>\n<meta property=\"rg:id\" content=\"PB:246546737\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Dropout Training as Adaptive Regularization\" \/>\n<meta name=\"citation_author\" content=\"Stefan Wager\" \/>\n<meta name=\"citation_author\" content=\"Sida Wang\" \/>\n<meta name=\"citation_author\" content=\"Percy Liang\" \/>\n<meta name=\"citation_publication_date\" content=\"2013\/07\/04\" \/>\n<meta name=\"citation_journal_title\" content=\"Advances in neural information processing systems\" \/>\n<meta name=\"citation_issn\" content=\"1049-5258\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Sida_Wang4\/publication\/246546737_Dropout_Training_as_Adaptive_Regularization\/links\/54361a580cf2dc341db2dc01.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/246546737_Dropout_Training_as_Adaptive_Regularization\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/246546737_Dropout_Training_as_Adaptive_Regularization\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-56658b32-e9fa-4c00-a1d2-f18df45dda5a","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":387,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw39_56aba1423863e"},"id":"rgw39_56aba1423863e","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-56658b32-e9fa-4c00-a1d2-f18df45dda5a", "f304313801f863fb31c00d47550ea1468dcd108a");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-56658b32-e9fa-4c00-a1d2-f18df45dda5a", "f304313801f863fb31c00d47550ea1468dcd108a");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw40_56aba1423863e"},"id":"rgw40_56aba1423863e","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/246546737_Dropout_Training_as_Adaptive_Regularization","requestToken":"q93MvbLIrURpG4k0KwjcyTyYPVBF0cr8g2vD0L1aqezt5MSCGnVHfJQc6BbM1zwsLmiY5af57JMIQrDoWiWSyfRR7auB2duQhc15IL4GM6lJ8Hu8csQHJpFiEm9w1etmPWrQjREmBElG3itiyGkNyZc922INl1lCCxEI5S8v9muEaahrhsAXbjfyCcgzvfu\/dOmw8mdBS63GSh5KLhCEcfe8aJIBYsS80qh13d6+7GMY2MvpJTXcUD0CTjr9q\/WA4ul+5mpMqykvLZcaFIA5ZJVfRjXtpVlPm4vcCQXaVlo=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=pi_X2_I_nqH5y9X8R2o4dszxuGGcFAwpFKRXkPEIY3Ll2yHCIwdEtklCdjuHv1Ps","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjQ2NTQ2NzM3X0Ryb3BvdXRfVHJhaW5pbmdfYXNfQWRhcHRpdmVfUmVndWxhcml6YXRpb24%3D","signupCallToAction":"Join for free","widgetId":"rgw42_56aba1423863e"},"id":"rgw42_56aba1423863e","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw41_56aba1423863e"},"id":"rgw41_56aba1423863e","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw43_56aba1423863e"},"id":"rgw43_56aba1423863e","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
