<!DOCTYPE html> <html lang="en" class="" id="rgw36_56aba1ca6d0ef"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="ltMUYAiEwi4w5JZBAUtZg9QmzrH0OkZ8vYm2Sl9P2zZywzTGaJYHyaSXGJCRS81DdQUQARigwzAtVd8rAxTkqHNwmHUjfA89tD2MbgNvDmdDk+o4iIEFa8FkW/0LtbgGQhifFlSVYxdaa7MQHqk/5i8Id9X0l7XfG1a2htYzg4n3eWNAQmy0nJ4B7VWn+2rfGI8WrKvhc9bjFi1zJioYxYmjepaqBrSm/ExvYF9pCgqn8lk5TOwPGGQtpOGtnKGYB4BMlT7VqDFPlr+0P0qUcKH8Rh4vxvjrX4hU0SUNOoQ="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-93d1d800-dc49-4c5e-bb09-9409e51a8341",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/232805135_Deep_Gaussian_Processes" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Deep Gaussian Processes" />
<meta property="og:description" content="In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a
deep belief network based on Gaussian process mappings. The data is modeled as
the output of a multivariate GP. The..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/232805135_Deep_Gaussian_Processes/links/0912f509b1437973f0000000/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/232805135_Deep_Gaussian_Processes" />
<meta property="rg:id" content="PB:232805135" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Deep Gaussian Processes" />
<meta name="citation_author" content="Andreas C. Damianou" />
<meta name="citation_author" content="Neil D. Lawrence" />
<meta name="citation_publication_date" content="2012/11/01" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Andreas_Damianou/publication/232805135_Deep_Gaussian_Processes/links/0912f509b1437973f0000000.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/232805135_Deep_Gaussian_Processes" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/232805135_Deep_Gaussian_Processes" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/215868066921738/styles/pow/publicliterature/FigureList.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Deep Gaussian Processes (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Deep Gaussian Processes on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56aba1ca6d0ef" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56aba1ca6d0ef" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56aba1ca6d0ef">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Deep%20Gaussian%20Processes&rft.date=2012&rft.au=Andreas%20C.%20Damianou%2CNeil%20D.%20Lawrence&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Deep Gaussian Processes</h1> <meta itemprop="headline" content="Deep Gaussian Processes">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/232805135_Deep_Gaussian_Processes/links/0912f509b1437973f0000000/smallpreview.png">  <div id="rgw7_56aba1ca6d0ef" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56aba1ca6d0ef" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Andreas_Damianou" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272454912311306%401441969721269_m/Andreas_Damianou.png" title="Andreas Damianou" alt="Andreas Damianou" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Andreas Damianou</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw9_56aba1ca6d0ef" data-account-key="Andreas_Damianou">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Andreas_Damianou"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272454912311306%401441969721269_l/Andreas_Damianou.png" title="Andreas Damianou" alt="Andreas Damianou" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Andreas_Damianou" class="display-name">Andreas Damianou</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/The_University_of_Sheffield" title="The University of Sheffield">The University of Sheffield</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56aba1ca6d0ef"> <a href="researcher/39663468_Neil_D_Lawrence" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Neil D. Lawrence" alt="Neil D. Lawrence" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Neil D. Lawrence</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw11_56aba1ca6d0ef">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/39663468_Neil_D_Lawrence"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Neil D. Lawrence" alt="Neil D. Lawrence" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/39663468_Neil_D_Lawrence" class="display-name">Neil D. Lawrence</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">        <meta itemprop="datePublished" content="2012-11">  11/2012;               <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1211.0358" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw12_56aba1ca6d0ef" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a<br />
deep belief network based on Gaussian process mappings. The data is modeled as<br />
the output of a multivariate GP. The inputs to that Gaussian process are then<br />
governed by another GP. A single layer model is equivalent to a standard GP or<br />
the GP latent variable model (GPLVM). We perform inference in the model by<br />
approximate variational marginalization. This results in a strict lower bound<br />
on the marginal likelihood of the model which we use for model selection<br />
(number of layers and nodes per layer). Deep belief networks are typically<br />
applied to relatively large data sets using stochastic gradient descent for<br />
optimization. Our fully Bayesian treatment allows for the application of deep<br />
models even when data is scarce. Model selection by our variational bound shows<br />
that a five layer hierarchy is justified even when modelling a digit data set<br />
containing only 150 examples.</div> </p>  </div>   </div>     <div id="rgw13_56aba1ca6d0ef" class="figure-carousel"> <div class="carousel-hd"> Figures in this publication </div> <div class="carousel-bd"> <ul class="clearfix">  <li> <a href="/figure/232805135_fig1_Figure-3-Figure-a-shows-the-deep-GP-model-employed-Figure-b-shows-the-ARD-weights" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 3: Figure (a) shows the deep GP model employed. Figure (b) shows..." data-key="232805135_fig1_Figure-3-Figure-a-shows-the-deep-GP-model-employed-Figure-b-shows-the-ARD-weights"> <img class="fig" src="https://www.researchgate.net/profile/Andreas_Damianou/publication/232805135/figure/fig1/Figure-3-Figure-a-shows-the-deep-GP-model-employed-Figure-b-shows-the-ARD-weights_small.png" alt="Figure 3: Figure (a) shows the deep GP model employed. Figure (b) shows..." title="Figure 3: Figure (a) shows the deep GP model employed. Figure (b) shows..."/> </a> </li>  </ul> </div> </div> <div class="action-container"> <div id="rgw14_56aba1ca6d0ef" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw27_56aba1ca6d0ef">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw28_56aba1ca6d0ef">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Andreas_Damianou/publication/232805135_Deep_Gaussian_Processes/links/0912f509b1437973f0000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Andreas_Damianou">Andreas Damianou</a>   </span>  </div>  <div class="social-share-container"><div id="rgw30_56aba1ca6d0ef" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw31_56aba1ca6d0ef" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw32_56aba1ca6d0ef" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw33_56aba1ca6d0ef" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw34_56aba1ca6d0ef" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw35_56aba1ca6d0ef" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw29_56aba1ca6d0ef" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FAndreas_Damianou%2Fpublication%2F232805135_Deep_Gaussian_Processes%2Flinks%2F0912f509b1437973f0000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw26_56aba1ca6d0ef"  itemprop="articleBody">  <p>Page 1</p> <p>Deep Gaussian Processes<br />Andreas C. Damianou<br />Dept. of Computer Science &amp; Sheffield Institute for Translational Neuroscience,<br />University of Sheffield, UK<br />{andreas.damianou, n.lawrence}@sheffield.ac.uk<br />Neil D. Lawrence<br />Abstract<br />In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a<br />deep belief network based on Gaussian process mappings. The data is modeled<br />as the output of a multivariate GP. The inputs to that Gaussian process are then<br />governed by another GP. A single layer model is equivalent to a standard GP or<br />the GP latent variable model (GPLVM). We perform inference in the model by<br />approximate variational marginalization. This results in a strict lower bound on<br />the marginal likelihood of the model which we use for model selection (number<br />of layers and nodes per layer). Deep belief networks are typically applied to rela-<br />tively large data sets using stochastic gradient descent for optimization. Our fully<br />Bayesian treatment allows for the application of deep models even when data is<br />scarce. Model selection by our variational bound shows that a five layer hierarchy<br />is justified even when modelling a digit data set containing only 150 examples.<br />1Introduction<br />Probabilistic modelling with neural network architectures constitute a well studied area of machine<br />learning. The recent advances in the domain of deep learning [Hinton and Osindero, 2006, Bengio<br />et al., 2012] have brought this kind of models again in popularity. Empirically, deep models seem<br />to have structural advantages that have been shown to improve learning in complicated datasets<br />associated with abstract information [Bengio, 2009]. Many interesting variants of the traditional<br />neural networks have emerged since the development of efficient training methodologies for deep<br />architectures: deep belief networks, (stacked) restricted Boltzmann machines [Hinton, 2010] etc.<br />Gaussianprocessmodelswereintroducedinthemachinelearningcommunity asafullyprobabilistic<br />substitute for the multilayer perceptron, inspired by the observation [Neal, 1996] that a GP is a mul-<br />tilayer perceptron with infinite basis functions. It seems prudent, therefore, to investigate how these<br />models will perform in deep hierarchies. Inference in deep GP models is analytically intractable.<br />Lawrence and Moore [2007] investigated a maximum a posteriori (MAP) approach for estimation<br />of latent variables. For the MAP approach to work, however, a strong prior was required on the top<br />level of the hierarchy. There are two main contributions in this paper. Firstly, we show how we can<br />approximately marginalize the latent variables variationally. This gives a rigorous lower bound on<br />the marginal log likelihood of a deep GP. We test the resulting approach on a toy problem and then<br />recreate results of Lawrence and Moore [2007] on a motion capture example. Our second contri-<br />bution is to demonstrate the applicability of deep models even when data is scarce. The variational<br />lower bound gives us an objective measure from which we can select different structures for our<br />deep hierarchy (number of layers, number of nodes per layer). In a simple digits example we find<br />that the best lower bound is given by the model with the deepest hierarchy we applied (5 layers).<br />The deep GP consists of a cascade of hidden layers of latent variables where each node acts as output<br />for the layer above and as input for the layer below—with the observed outputs being placed in the<br />leaves of the hierarchy. Gaussian processes govern the mappings between the layers.<br />1</p>  <p>Page 2</p> <p>A single layer of the deep GP is a Gaussian process latent variable model (GPLVM). Inference over<br />the latent variables even in this model is intractable. However, Titsias and Lawrence [2010] have<br />shown that latent variables can be approximately marginalized allowing a variational lower bound<br />on the likelihood to be computed. The appropriate size of the latent space can be computed using au-<br />tomatic relevance determination (ARD) priors [Neal, 1996, MacKay et al., 1992]. Damianou et al.<br />[2011] extended this approach by placing a Gaussian process prior over the latent space giving a<br />Bayesian dynamical GPLVM. Here we extend that approach to allow us to approximately marginal-<br />ize any number of hidden layers. We demonstrate how a deep hierarchy of Gaussian processes can<br />be obtained by marginalising out the latent variables in the structure, obtaining a fully Bayesian<br />training procedure along with a variational approximation to the true posterior of all the latent vari-<br />ables given the outputs. The resulting model is very flexible, and we discuss a number of potential<br />applications in further work.<br />2The Model<br />We first consider standard approaches to modeling with GPs. We then extend these ideas to deep<br />GPs by considering Gaussian process priors over the inputs to the GP model.<br />2.1Standard GP Modelling<br />In the traditional probabilistic inference framework, we are given a set of training input-output pairs,<br />stored in matrices X ∈ RN×Qand Y ∈ RN×Drespectively, and seek to estimate the unobserved,<br />latent function f = f(x), responsible for generating Y given X. In this setting, Gaussian processes<br />(GP’s) [Rasmussen and Williams, 2006] can be employed as nonparametric prior distributions over<br />the latent function f. More formally, we assume that each datapoint ynis generated from the<br />corresponding f(xn) by adding independent Gaussian noise, i.e.<br />yn= f(xn) + ?n, ? ∼ N(0,σ?I),<br />(1)<br />and f is drawn from a Gaussian process, i.e. f(x) ∼ GP (0,k(x,x?)). This (zero-mean) Gaus-<br />sian process prior only depends on the covariance function k operating on the inputs X. As<br />we wish to obtain a flexible model, we only make very general assumptions about the form of<br />the generative mapping f and this is reflected in the choice of the covariance function which de-<br />fines the properties of this mapping. For example, an exponentiated quadratic covariance function<br />k(xi,xj) = (σse)2exp<br />2l2<br />, forces the latent functions to be infinitely smooth. We de-<br />note any covariance function hyperparameters (such as (σse,l) of the aforementioned covariance<br />function) by θ. The collection of latent function instantiations, denoted by F = {fn}N<br />distributed, allowing us to compute analytically the marginal likelihood1<br />?<br />n=1<br />Gaussian processes have also been used with success in unsupervised learning scenarios, where the<br />input data X are not directly observed. The Gaussian process latent variable model (GP-LVM)<br />[Lawrence, 2005, 2004] provides an elegant solution to this problem by treating the unobserved<br />inputs X as latent variables, while employing a product of D independent GPs as prior for the latent<br />mapping. The assumed generative procedure now takes the form:<br />?<br />−(xi−xj)2<br />?<br />n, is normally<br />p(Y |X) =<br />N<br />?<br />p(yn|fn)p(fn|xn)dF = N(Y |0,KNN+ σ2<br />?I),KNN= k(X,X).<br />(2)<br />ynd= fd(xn) + ?nd,<br />(3)<br />where ? is again random Gaussian noise and F = {fd}D<br />dataset, the Gaussian process priors take the form of a Gaussian distribution<br />d=1with fnd = fd(xn). Given a finite<br />p(F|X) =<br />D<br />?<br />d=1<br />N(fd|0,KNN)<br />(4)<br />which, in turn, allows for general non-linear mappings to be marginalised out analytically to obtain<br />the likelihood p(Y |X) =?D<br />d=1N(yd|X), analogously to equation (2).<br />1All probabilities involving f should also have θ in the conditioning set, but here we omit it for clarity.<br />2</p>  <p>Page 3</p> <p>2.2Deep Gaussian Processes<br />Our deep Gaussian process architecture corresponds to a graphical model with three kinds of nodes,<br />illustrated in figure 1(a): the leaf nodes Y ∈ RN×Dwhich are the only observed ones, the interme-<br />diate latent spaces Xh∈ RN×Qh,h = 1,...,H − 1, where H is the number of hidden layers, and<br />the parent latent node Z = XH ∈ RN×QZ. In our notation the parent node is separated from the<br />rest of the hidden layers for notational clarity and because it can be constrained with a prior of our<br />choice and, therefore, be involved in different mathematical expressions. In this deep architecture,<br />all intermediate nodes Xhact as inputs for the layer below (including the leaves) and as outputs for<br />the layer above. For simplicity, consider a structure with only two hidden units, as the one depicted<br />in figure 1(b). The generative process takes the form:<br />ynd=fY<br />xnq=fX<br />d(xn) + ?Y<br />q(zn) + ?X<br />nd, d = 1,...,D, xn∈ RQ<br />nq, q = 1,...,Q, zn∈ RQZ<br />(5)<br />and the intermediate node is involved in two Gaussian processes, fYand fX, playing the role of an<br />input and an output respectively: fY∼ GP(0,kY(X,X))<br />structure can be naturally extended vertically (i.e. deeper hierarchies) or horizontally (i.e. segmen-<br />tation of each layer into different partitions of the output space), as we will see later in the paper.<br />However, it is already obvious how each layer adds a significant number of model parameters (Xh)<br />as well as a regularization challenge, since the size of each latent layer is crucial but has to be a priori<br />defined. For this reason, unlike Lawrence and Moore [2007], we seek to variationally marginalise<br />out the whole latent space. Not only this will allow us to obtain an automatic Occam’s razor due to<br />the Bayesian training, but also we will end up with a significantly lower number of model parame-<br />ters, since the variational procedure only adds variational parameters. The first step to this approach,<br />is to define automatic relevance determination (ARD) covariance functions for the GPs:<br />and<br />fX∼ GP(0,kX(Z,Z)). This<br />k(xi,xj) = σ2<br />arde−1<br />2<br />?Q<br />q=1wq(xi,q−xj,q)2.<br />(6)<br />This covariance function assumes a different weight wqfor each latent dimension and this can be<br />exploited in a Bayesian training framework in order to “switch off” irrelevant dimensions by driv-<br />ing their corresponding weight to zero, thus helping towards automatically finding the structure of<br />complex models [Damianou et al., 2012]. However, the nonlinearities introduced by this covariance<br />function make the Bayesian treatment of this model challenging. Nevertheless, following recent<br />non-standard variational inference methods we can define analytically an approximate Bayesian<br />training procedure, as will be explained in the next section.<br />2.3 Bayesian Training<br />A Bayesian training procedure requires optimisation of the model evidence:<br />logp(Y ) = log<br />?<br />p(Y |X)p(X|Z)p(Z)dXdZ.<br />(7)<br />When prior information is available regarding the observed data (e.g. their dynamical nature is<br />known a priori), the prior distribution on the parent latent node can be selected so as to constrain the<br />whole latent space through propagation of the prior density through the cascade. Here we take the<br />general case where p(Z) = N(Z|0,I). However, the integral of equation (7) is intractable due to<br />the nonlinear way in which X and Z are treated through the GP priors fYand fX. As a first step,<br />we apply Jensen’s inequality to find a variational lower bound on logp(Y ):<br />?<br />where we expanded the likelihood and, additionally, we introduced a variational distribution Q, the<br />form of which will be defined later on. The above integral is still intractable because X and Z<br />still appear nonlinearly in the p(FY|X) and p(FX|Z) terms respectively. A key result of Titsias<br />and Lawrence [2010] is that expanding the probability space of the GP prior p(F|X) with extra<br />variables allows for priors on the latent space to be propagated through the nonlinear mapping f.<br />More precisely, we augment the probability space of equation (4) with K auxiliary pseudo-inputs<br />logp(Y ) ≥ Fv=<br />Qlogp(Y |FY)p(FY|X)p(X|FX)p(FX|Z)p(Z)<br />Q<br />dXdZdFYdFX<br />(8)<br />3</p>  <p>Page 4</p> <p>˜ X ∈ RK×Qand˜Z ∈ RK×QZthat correspond to a collection of function values UY∈ RK×Dand<br />UX∈ RK×Qrespectively2. Following this approach, we obtain the augmented GP likelihood:<br />p(Y |X,˜ X) = p(Y |FY)p(FY|UY,X)p(UY|˜ X)dFYdUY<br />and similarly for the p(X|Z) term. The pseudo-inputs˜ X and˜Z are known as inducing points, and<br />will be dropped from our expressions from now on, for clarity. Note that FYand UYare draws<br />from the same GP so that p(UY) and p(FY|UY,X) are also Gaussian distributions.<br />We are now able to define a variational distribution Q which, when combined with the new expres-<br />sions for the augmented GP priors, results in a tractable variational bound. Specifically, we have:<br />(9)<br />Q = p(FY|UY,X)q(UY)q(X)p(FX|UX,Z)q(UX)q(Z).<br />We select q(UY) and q(UX) to be free-form variational distributions, while q(X) and q(Z) are<br />chosen to be Gaussian, factorised with respect to dimensions:<br />(10)<br />q(X) =<br />Q<br />?<br />q=1<br />N(µX<br />q,SX<br />q) and q(Z) =<br />QZ<br />?<br />q=1<br />N(µZ<br />q,SZ<br />q).<br />(11)<br />By substituting equations (10) and (9) back to (8), we see that the “difficult” terms p(FY|UY,X)<br />and p(FX|UX,Z) cancel out in the fraction, leaving a quantity that can be computed analytically:<br />?<br />More specifically, we can break the logarithm in equation (12) by grouping the variables of the<br />fraction in such a way that the bound can be written as:<br />Fv=<br />Qlogp(Y |FY)p(UY)p(X|FX,UX)p(UX)p(Z)<br />Q<br />dXdZdFYdFXdUYdUX.<br />(12)<br />Fv= gY+ rX+ Hq(X)− KL[q(Z) ? p(Z)]<br />(13)<br />where H represents the entropy with respect to a distribution, KL denotes the Kullback – Leibler<br />divergence and, using ?·? to denote expectations,<br />?<br />?<br />Both terms only involve known Gaussian densities and are, thus, tractable. The gY term is the same<br />as the bound found for the Bayesian GP-LVM [Titsias and Lawrence, 2010]. Since it it only involves<br />expectations with respect to Gaussian distributions, the GP output variables are only involved in a<br />quantity of the form Y Y?. Further, as can be seen from the above equations, the function r(·) is<br />similar to g(·) but it involves expectations with respect to densities of all of its variables. Therefore,<br />rXwill involve X (i.e. the outputs of the top layer) in a term?XX??<br />3 Extending the hierarchy<br />gY = g(Y,FY,UY,X) =logp(Y |FY) + logp(UY)<br />q(UY)<br />?<br />?<br />p(FY|UY,X)q(UY)q(X)<br />(14)<br />rX= r(X,FX,UX,Z) = logp(X|FX) + logp(UX)<br />q(UX)<br />p(FX|UX,Z)q(UX)q(X)q(Z)<br />.<br />(15)<br />q(X)=?Q<br />q=1µX<br />qµX<br />q+ SX<br />q.<br />Although the main calculations were demonstrated in a simple hierarchy, it is easy to extend the<br />model vertically, i.e. by adding more hidden layers, or horizontally, i.e. by considering conditional<br />independencies of the latent variables belonging to the same layer. The first case only requires<br />adding more rXfunctions to the variational bound, i.e. instead of a single rXterm we will now<br />have the a sum:?H−1<br />Now consider the horizontal expansion scenario and assume that we wish to break the single la-<br />tent space Xh, of layer h, to Mhconditionally independent subsets. As long as the variational<br />distribution q(Xh) of equation (11) is chosen to be factorised in a consistent way, this is feasi-<br />ble by just breaking the original rXhterm of equation (15) into the sum?Mh<br />2The number of inducing points K does not need to be the same for all GPs<br />h=1rXh, where rXh= r(Xh,FXh,UXh,Xh+1), with XH= Z.<br />m=1r(m)<br />Xh. This fol-<br />lows just from the fact that, due to the independence assumption, it holds that logp(Xh|Xh+1) =<br />4</p>  <p>Page 5</p> <p>?Mh<br />different observation spaces which, however, we believe they have some commonality. For example,<br />when the observed data are coming from a video and an audio recording of the same event. Given<br />the above, the variational bound for the most general version of the model takes the form:<br />m=1logp(X(m)<br />ing the gYterm of the bound. This scenario arises when, for example we are presented with multiple<br />h<br />|Xh+1). Notice that the same principle can also be applied to the leaves by break-<br />Fv=<br />MY<br />?<br />m=1<br />g(m)<br />Y<br />+<br />H−1<br />?<br />h=1<br />Mh<br />?<br />m=1<br />r(m)<br />Xh+<br />H−1<br />?<br />h=1<br />Hq(Xh)− KL(q(Z) ? p(Z)).<br />(16)<br />Figure 1(c) shows the association of this objective function’s terms with each layer of the hierarchy.<br />Recall that each r(m)<br />Y<br />term is associated with a different GP and, thus, is coming with its<br />own set of automatic relevance determination (ARD) weights, which were described in equation (6).<br />Xhand g(m)<br />3.1 Deep multiple-output Gaussian processes<br />The particular way of extending the hierarchies horizontally, as presented above, can be seen as<br />a means of performing unsupervised multiple-output GP learning. This only requires assigning<br />a different gY term (and, thus, associated ARD weights) to each vector yd, where d indexes the<br />output dimensions. After training our model, we hope that the columns of Y that encode similar<br />informationwillbeassignedrelevanceweightvectorsthatarealsosimilar. Thisideacanbeextended<br />to all levels of the hierarchy, thus obtaining a fully factorised deep GP model.<br />This special case of our model makes the connection between our model’s structure and neural<br />network architectures more obvious: the ARD parameters play a role similar to the weights of neural<br />networks, while the latent variables play the role of neurons which learn hierarchies of features.<br />...<br />(a)<br />(b) (c)<br />Figure 1: Different graphical representations of the Deep GP model: (a) shows the general architec-<br />ture with a cascade of H hidden layers, (b) depicts a simplification of a two hidden layer hierarchy<br />also demonstrating the corresponding GP mappings and (c) illustrates the most general case where<br />the leaves and all intermediate nodes are allowed to form conditionally independent groups. The<br />terms of the objective (16) corresponding to each layer are included on the left of each level.<br />3.2 Parameters and complexity<br />In all graphical model variants shown in figure 1, every arrow represents a generative procedure with<br />a GP prior, corresponding to a set of parameters {˜ X,θ,σ?}. Further, each layer of latent variables<br />corresponds to a variational distribution q(X) which is associated with a set of variational means<br />and covariances, as shown in equation (11). The parent node can have the same form as equation<br />(11) or can be constrained with a more informative prior which would couple the points of q(Z).<br />For example, a dynamical prior would introduce Q × N2parameters which can, nevertheless, be<br />5</p>  <p>Page 6</p> <p>reparametrized using less variables [Damianou et al., 2011]. However, as is evident from equations<br />(10) and (12), the inducing points and the parameters of q(X) and q(Z) are variational rather than<br />model parameters, something which significantly helps in regularizing the problem. Therefore,<br />adding more layers to the hierarchy does not introduce many more model parameters. Moreover, as<br />in common sparse methods for Gaussian processes [Titsias, 2009], the complexity of each generative<br />GP mapping is reduced from the typical O(N3) to O(NM2).<br />4 Demonstration<br />In this section we demonstrate the deep GP model in toy and real-world datasets. The model is<br />initialised by performing dimensionality reduction in the observations to obtain the first hidden<br />layer and then repeating this process for the next layers. To obtain these stacked initial spaces we<br />experimented with PCA and the Bayesian GP-LVM, but the end result did not vary significantly.<br />4.1Toy data<br />We first test our model in toy data, created by sampling from a three-level hierarchy of GPs. Figure 2<br />(a) depicts the true hierarchy: from the top latent layer two intermediate latent signals are generated.<br />These, in turn, together generate 10−dimensional observations (not depicted here) through sampling<br />of another GP. These observations are then used to train the following models: a deep GP, a simple<br />stacked Isomap and a simple stacked PCA method, the results of which are shown in figures 2 (b, c,<br />d) respectively. Note that only our method marginalises the latent space and, in contrast to the other<br />two, it is not given any information about the dimensionality of each true signal in the hierarchy.<br />Further, as can be seen in figure 2, our model not only finds the correct dimensionality for each<br />hidden layer, but it also discovers latent signals which are closer to the real ones.<br />(a)(b)(c) (d)<br />Figure 2: Attempts to reconstruct the real data (fig. (a)) with our model (b), stacked Isomap (c) and<br />stacked PCA (d). Our model can also find the correct dimensionalities automatically.<br />4.2 Modeling human motion<br />For our second demonstration we consider the same motion capture dataset used from Lawrence<br />and Moore [2007], taken from the CMU MOCAP database3.<br />62−dimensional sets of 78 points each, represent two subjects walking towards each other and<br />performing a ‘high-five’. We applied our method with a two-level hierarchy where the two observa-<br />tion sets were taken to be conditionally independent given their parent latent layer. Therefore, we<br />obtained three optimised sets of ARD parameters: one for each modality of the bottom layer (shown<br />with bar graphs having bins of different colours/widths for each modality, in figure 3(b), and one<br />ARD weight set for the top node, as shown in figure 3(c). Our model discovered a common subspace<br />in the intermediate layer, since for dimensions 2 and 6 both ARD sets have a non-zero value. This<br />is expected, as the two subjects perform very similar motions with opposite directions. The ARD<br />weights are also a means of automatically selecting the dimensionality of each layer and subspace.<br />This kind of modelling is impossible for a MAP method like [Lawrence and Moore, 2007] which<br />requires the exact latent structure to be given a priori. The full latent space learned by the afore-<br />mentioned MAP method is plotted in figure 4 (d,e,f), where fig. (d) corresponds to the top latent<br />space and each of the other two encodes information for each of the two interacting subjects. Our<br />method is not constrained to two dimensional spaces, so for comparison we plot two-dimensional<br />The data, summarised in two<br />3http://mocap.cs.cmu.edu.<br />6</p>  <p>Page 7</p> <p>projections of the dominant dimensions of each subspace in figure 4 (a,b,c). The similarity of the<br />latent spaces is obvious. In contrast to Lawrence and Moore [2007], we did not have to constrain<br />the latent space with dynamics in order to obtain results of good quality.<br />Further, we can sample from these spaces to see what kind of information they encode. Indeed, we<br />observed that the top layer generates outputs which correspond to different variations of the whole<br />sequence, while when sampling from the first layer we obtain outputs which only differ in a small<br />subset of the output dimensions, e.g. those corresponding to the subject’s hand.<br />(a) (b)<br />12345<br />(c)<br />678910<br />Figure 3: Figure (a) shows the deep GP model employed. Figure (b) shows the ARD weights for<br />fY1and fY2and figure (c) those for fX.<br />(a) (b)(c)(d)(e)(f)<br />Figure 4: Left (a,b,c): projections of the latent spaces discovered by our model, Right (d,e,f): the<br />full latent space learned for the model of Lawrence and Moore [2007].<br />4.3Deep learning of digit images<br />In our last experiment we demonstrate the ability of our model to learn latent features of increas-<br />ing abstraction and we demonstrate the usefulness of an analytic bound on the model evidence as a<br />means of evaluating the quality of the data fit for different choices of the overall depth of the hier-<br />archy. We built a dataset consisting of 50 examples for each of the digits 0,1 and 6 taken from the<br />USPS handwritten digit database. Each digit is represented as an image in 16×16 pixels. We exper-<br />imented with models of depth ranging from 1 (equivalent to Bayesian GP-LVM) to 5 hidden layers<br />and evaluated each model by measuring the nearest neighbour error in the latent features discovered<br />in each hierarchy. Our finding was that the approximate evidence of the model was increasing with<br />the number of layers and so was the quality of the model in terms of nearest neighbour errors4. In-<br />deed, the single-layer model made 5 mistakes even though it automatically decided to use 10 latent<br />dimensions and the quality of the trained models was increasing with the number of hidden layers.<br />Finally, only one point had a nearest neighbour of a different class in the 4−dimensional top level’s<br />feature space of a model with depth 5. A 2D projection of this space is plotted in figure 6(a). The<br />ARD weights for this model are depicted in figure 5.<br />To further test the nature of the discovered latent features, we generated outputs by sampling from<br />each hidden layer. As can be seen in figure 6(b), the first hierarchical layers encode very local<br />features whereas the higher ones encode much more abstract information.<br />5Discussion and future work<br />We have introduced a framework for efficient Bayesian training of hierarchical Gaussian process<br />mappings for unsupervised learning. Our approach approximately marginalises out the latent space,<br />thus allowing for automatic structure discovery in the hierarchy. The method was able to success-<br />fullylearnahierarchyoffeatureswhichdescribenaturalhumanmotionandthepixelsofhandwritten<br />4The results were the same when we took into account the Bayesian Information Criterion.<br />7</p>  <p>Page 8</p> <p>Figure 5: The ARD weights of a deep GP with 5 hidden layers. The top layer is on the far left.<br />x 10<br />−5<br />0<br />5<br />10<br />15<br />(a)(b)<br />Figure 6: Fig. (a) demonstrates the nearest neighbour class separation test on a deep GP model<br />with depth 5. Fig. (b) shows outputs obtained when sampling from this model. The first two rows<br />(top-down), which were sampled from layers 1 and 2 respectively, encode very local features, e.g.<br />explaining if a zero is a closed circle or not, or how big the circle of a 6 is. We discovered many more<br />local features when we sampled from different dimensions. On the other hand, when we sampled<br />from the two dominant dimensions of the parent latent node (two rows in the bottom) we obtained<br />much more varying outputs. Thus, the higher levels indeed encode much more abstract information.<br />digits. Our variational lower bound selected a deep hierarchical representation for handwritten digits<br />even though the data in our experiment was relatively scarce (150 data points).<br />In the future, we wish to test our model on various inference tasks, such as class conditional density<br />estimation. Our method can also be used to improve existing deep algorithms, something which we<br />plan to further investigate by incorporating ideas from past approaches. Indeed, previous efforts to<br />combine GPs with deep structures were successful at unsupervised pre-training [Erhan et al., 2010]<br />or guiding [Snoek et al., 2012] of traditional deep models.<br />Although the experiments presented here considered only up to five layers in the hierarchy, the<br />methodology is directly applicable to deeper architectures, with which we intend to experiment in<br />the future. The marginalisation of the latent space allows for such an expansion with simultaneous<br />regularisation. The variational lower bound allows us to make a principled choice between models<br />trained using different initializations and with different numbers of layers.<br />The deep hierarchy we have proposed could also be used with inputs governing the top layer of the<br />hierarchy, leading to a powerful model for regression based on Gaussian processes, but which is<br />not itself a Gaussian process. We expect such a model to have applications in multitask learning<br />(where intermediate layers could learn representations shared across the tasks) and in modelling<br />nonstationary data or data involving jumps. These are both areas where a single layer GP struggles.<br />Acknowledgments<br />Research was partially supported by the University of Sheffield Moody endowment fund and the<br />Greek State Scholarships Foundation.<br />References<br />Y. Bengio.<br />(1):1–127,<br />http://dx.doi.org/10.1561/2200000006.<br />Learning Deep Architectures for AI.<br />Jan. 2009. ISSN 1935-8237.<br />Found. Trends Mach. Learn., 2<br />10.1561/2200000006.doi:URL<br />8</p>  <p>Page 9</p> <p>Y. Bengio, A. C. Courville, and P. Vincent. Unsupervised feature learning and deep learning: A<br />review and new perspectives. CoRR, abs/1206.5538, 2012.<br />A. C. Damianou, M. Titsias, and N. D. Lawrence. Variational gaussian process dynamical systems.<br />In Advances in Neural Information Processing Systems (NIPS) 24, pages 2510–2518, 2011.<br />A. C. Damianou, C. H. Ek, M. K. Titsias, and N. D. Lawrence. Manifold relevance determination.<br />In J. Langford and J. Pineau, editors, Proceedings of the International Conference in Machine<br />Learning, volume 29, San Francisco, CA, 2012. Morgan Kauffman.<br />D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and S. Bengio. Why does unsu-<br />pervised pre-training help deep learning? J. Mach. Learn. Res., 11:625–660, Mar. 2010. ISSN<br />1532-4435. URL http://dl.acm.org/citation.cfm?id=1756006.1756025.<br />G. Hinton. A Practical Guide to Training Restricted Boltzmann Machines. Technical report, 2010.<br />URL http://www.cs.toronto.edu/˜hinton/absps/guideTR.pdf.<br />G. E. Hinton and S. Osindero. A fast learning algorithm for deep belief nets. Neural Computation,<br />18:2006, 2006.<br />N. D. Lawrence. Gaussian process latent variable models for visualisation of high dimensional data.<br />In In NIPS, page 2004, 2004.<br />N. D. Lawrence. Probabilistic non-linear principal component analysis with Gaussian process latent<br />variable models. Journal of Machine Learning Research, 6:1783–1816, 2005.<br />N. D. Lawrence and A. J. Moore.Hierarchical Gaussian process latent variable models.<br />Z. Ghahramani, editor, Proceedings of the International Conference in Machine Learning, vol-<br />ume 24, pages 481–488. Omnipress, 2007. ISBN 1-59593-793-3.<br />D. J. MacKay, J. Bridle, K. Rose, S. Sibisi, J. Skilling, H. Sompolinsky, and N. W. T. Comments.<br />Bayesian methods for adaptive models, 1992.<br />R. M. Neal. Bayesian Learning for Neural Networks. Springer, 1996. Lecture Notes in Statistics<br />118.<br />C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. Cambridge,<br />MA, 2006. ISBN 0-262-18253-X.<br />J. Snoek, R. P. Adams, and H. Larochelle. On nonparametric guidance for learning autoencoder<br />representations. In Fifteenth International Conference on Artificial Intelligence and Statistics<br />(AISTATS), 2012.<br />M. Titsias. Variational learning of inducing variables in sparse Gaussian processes. JMLR W&amp;CP,<br />5:567–574, 2009.<br />M. Titsias and N. D. Lawrence. Bayesian Gaussian process latent variable model. Journal of<br />Machine Learning Research - Proceedings Track, 9:844–851, 2010.<br />In<br />9</p>  <a href="https://www.researchgate.net/profile/Andreas_Damianou/publication/232805135_Deep_Gaussian_Processes/links/0912f509b1437973f0000000.pdf">Download full-text</a> </div> <div id="rgw19_56aba1ca6d0ef" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw20_56aba1ca6d0ef">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw21_56aba1ca6d0ef"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Andreas_Damianou/publication/232805135_Deep_Gaussian_Processes/links/0912f509b1437973f0000000.pdf" class="publication-viewer" title="DeepGPs_CameraReady.pdf">DeepGPs_CameraReady.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Andreas_Damianou">Andreas Damianou</a> &middot; May 16, 2014 </span>   </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw23_56aba1ca6d0ef" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw24_56aba1ca6d0ef">  </ul> </div> </div>   <div id="rgw15_56aba1ca6d0ef" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw16_56aba1ca6d0ef"> <div> <h5> <a href="publication/291229154_Constancy_of_the_Cluster_Gas_Mass_Fraction_in_the_R_hct_Universe" class="color-inherit ga-similar-publication-title"><span class="publication-title">Constancy of the Cluster Gas Mass Fraction in the R_h=ct Universe</span></a>  </h5>  <div class="authors"> <a href="researcher/5141427_Fulvio_Melia" class="authors ga-similar-publication-author">Fulvio Melia</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56aba1ca6d0ef"> <div> <h5> <a href="publication/283666618_Selecting_the_Regularization_Parameters_in_High-dimensional_Panel_Data_Models_Consistency_and_Efficiency" class="color-inherit ga-similar-publication-title"><span class="publication-title">Selecting the Regularization Parameters in High-dimensional Panel Data Models: Consistency and Efficiency</span></a>  </h5>  <div class="authors"> <a href="researcher/2060647719_Tomohiro_Ando" class="authors ga-similar-publication-author">Tomohiro Ando</a>, <a href="researcher/7821268_Jushan_Bai" class="authors ga-similar-publication-author">Jushan Bai</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56aba1ca6d0ef"> <div> <h5> <a href="publication/282663386_Independent_predictors_of_tuberculosis_mortality_in_a_high_HIV_prevalence_setting_A_retrospective_cohort_study" class="color-inherit ga-similar-publication-title"><span class="publication-title">Independent predictors of tuberculosis mortality in a high HIV prevalence setting: A retrospective cohort study</span></a>  </h5>  <div class="authors"> <a href="researcher/38908205_Dominique_J_Pepper" class="authors ga-similar-publication-author">Dominique J. Pepper</a>, <a href="researcher/75635434_Michael_Schomaker" class="authors ga-similar-publication-author">Michael Schomaker</a>, <a href="researcher/39635770_Robert_J_Wilkinson" class="authors ga-similar-publication-author">Robert J. Wilkinson</a>, <a href="researcher/50849905_Virginia_de_Azevedo" class="authors ga-similar-publication-author">Virginia de Azevedo</a>, <a href="researcher/38990957_Gary_Maartens" class="authors ga-similar-publication-author">Gary Maartens</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw37_56aba1ca6d0ef" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw38_56aba1ca6d0ef">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw39_56aba1ca6d0ef" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=0SLFNlQ85uA3bJMCpDwHI2yHcGY-tIv4gCVU2FO1FZgscATmsY20j8sRUPCyCGEw" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="YcorPdiUacMzYD9wMg+KmsyIT3MFeUw1x1f5gNk89iOkacPHcsks8M3FFK9Dac3RPcH02KYgPRIKfDm2aa8+D9Ra0wIY9kZiHMtJIN8823eUFm76bvjom+Zu++/b0VsCZG71KIvoK0KK01xTXHkb/KGE2Shn6NTQZWecLPR0aTWivfLcJ9Y7YQ6M3YPK4fC7hGzb/1/A3wIyQDlCjn5rQAEkoTG6B/Q/Wo/zfXIZ4EW63UyoS5tZAjT8F/PgCiQMOE4+9ZgixaUjWjWkcW29s6ctBRk2qm8v8oKz1d41QvA="/> <input type="hidden" name="urlAfterLogin" value="publication/232805135_Deep_Gaussian_Processes"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjMyODA1MTM1X0RlZXBfR2F1c3NpYW5fUHJvY2Vzc2Vz"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjMyODA1MTM1X0RlZXBfR2F1c3NpYW5fUHJvY2Vzc2Vz"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjMyODA1MTM1X0RlZXBfR2F1c3NpYW5fUHJvY2Vzc2Vz"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw40_56aba1ca6d0ef"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 614;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FigureList","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Andreas Damianou","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272454912311306%401441969721269_m\/Andreas_Damianou.png","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Andreas_Damianou","institution":"The University of Sheffield","institutionUrl":false,"widgetId":"rgw4_56aba1ca6d0ef"},"id":"rgw4_56aba1ca6d0ef","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=1981895","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56aba1ca6d0ef"},"id":"rgw3_56aba1ca6d0ef","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=232805135","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":232805135,"title":"Deep Gaussian Processes","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"11\/2012;","publicationDateRobot":"2012-11","article":""}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1211.0358","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Deep Gaussian Processes"},{"key":"rft.date","value":"2012"},{"key":"rft.au","value":"Andreas C. Damianou,Neil D. Lawrence"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56aba1ca6d0ef"},"id":"rgw6_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=232805135","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":232805135,"peopleItems":[{"data":{"authorNameOnPublication":"Andreas Damianou","accountUrl":"profile\/Andreas_Damianou","accountKey":"Andreas_Damianou","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272454912311306%401441969721269_m\/Andreas_Damianou.png","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Andreas Damianou","profile":{"professionalInstitution":{"professionalInstitutionName":"The University of Sheffield","professionalInstitutionUrl":"institution\/The_University_of_Sheffield"}},"professionalInstitutionName":"The University of Sheffield","professionalInstitutionUrl":"institution\/The_University_of_Sheffield","url":"profile\/Andreas_Damianou","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272454912311306%401441969721269_l\/Andreas_Damianou.png","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Andreas_Damianou","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw9_56aba1ca6d0ef"},"id":"rgw9_56aba1ca6d0ef","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1981895&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"The University of Sheffield","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":2,"accountCount":1,"publicationUid":232805135,"widgetId":"rgw8_56aba1ca6d0ef"},"id":"rgw8_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1981895&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=2&accountCount=1&publicationUid=232805135","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/39663468_Neil_D_Lawrence","authorNameOnPublication":"Neil D. Lawrence","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Neil D. Lawrence","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/39663468_Neil_D_Lawrence","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw11_56aba1ca6d0ef"},"id":"rgw11_56aba1ca6d0ef","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=39663468&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw10_56aba1ca6d0ef"},"id":"rgw10_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=39663468&authorNameOnPublication=Neil%20D.%20Lawrence","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56aba1ca6d0ef"},"id":"rgw7_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=232805135&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":232805135,"abstract":"<noscript><\/noscript><div>In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a<br \/>\ndeep belief network based on Gaussian process mappings. The data is modeled as<br \/>\nthe output of a multivariate GP. The inputs to that Gaussian process are then<br \/>\ngoverned by another GP. A single layer model is equivalent to a standard GP or<br \/>\nthe GP latent variable model (GPLVM). We perform inference in the model by<br \/>\napproximate variational marginalization. This results in a strict lower bound<br \/>\non the marginal likelihood of the model which we use for model selection<br \/>\n(number of layers and nodes per layer). Deep belief networks are typically<br \/>\napplied to relatively large data sets using stochastic gradient descent for<br \/>\noptimization. Our fully Bayesian treatment allows for the application of deep<br \/>\nmodels even when data is scarce. Model selection by our variational bound shows<br \/>\nthat a five layer hierarchy is justified even when modelling a digit data set<br \/>\ncontaining only 150 examples.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw12_56aba1ca6d0ef"},"id":"rgw12_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=232805135","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":{"data":{"figures":[{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Andreas_Damianou\/publication\/232805135\/figure\/fig1\/Figure-3-Figure-a-shows-the-deep-GP-model-employed-Figure-b-shows-the-ARD-weights.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Andreas_Damianou\/publication\/232805135\/figure\/fig1\/Figure-3-Figure-a-shows-the-deep-GP-model-employed-Figure-b-shows-the-ARD-weights_small.png","figureUrl":"\/figure\/232805135_fig1_Figure-3-Figure-a-shows-the-deep-GP-model-employed-Figure-b-shows-the-ARD-weights","selected":false,"title":"Figure 3: Figure (a) shows the deep GP model employed. Figure (b) shows...","key":"232805135_fig1_Figure-3-Figure-a-shows-the-deep-GP-model-employed-Figure-b-shows-the-ARD-weights"}],"readerDocId":"6306035","linkBehaviour":"dialog","isDialog":true,"headerText":"Figures in this publication","isNewPublicationDesign":false,"widgetId":"rgw13_56aba1ca6d0ef"},"id":"rgw13_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/FigureList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FigureList.html?readerDocId=6306035&isDialog=1&linkBehaviour=dialog","viewClass":"views.publicliterature.FigureListView","yuiModules":["rg.views.publicliterature.FigureListView","css-pow-publicliterature-FigureList"],"stylesheets":["pow\/publicliterature\/FigureList.css"],"_isYUI":true},"previewImage":"https:\/\/i1.rgstatic.net\/publication\/232805135_Deep_Gaussian_Processes\/links\/0912f509b1437973f0000000\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw14_56aba1ca6d0ef"},"id":"rgw14_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56aba1ca6d0ef"},"id":"rgw5_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=232805135&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":5141427,"url":"researcher\/5141427_Fulvio_Melia","fullname":"Fulvio Melia","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291229154_Constancy_of_the_Cluster_Gas_Mass_Fraction_in_the_R_hct_Universe","usePlainButton":true,"publicationUid":291229154,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/291229154_Constancy_of_the_Cluster_Gas_Mass_Fraction_in_the_R_hct_Universe","title":"Constancy of the Cluster Gas Mass Fraction in the R_h=ct Universe","displayTitleAsLink":true,"authors":[{"id":5141427,"url":"researcher\/5141427_Fulvio_Melia","fullname":"Fulvio Melia","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291229154_Constancy_of_the_Cluster_Gas_Mass_Fraction_in_the_R_hct_Universe","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291229154_Constancy_of_the_Cluster_Gas_Mass_Fraction_in_the_R_hct_Universe\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56aba1ca6d0ef"},"id":"rgw16_56aba1ca6d0ef","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291229154","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2060647719,"url":"researcher\/2060647719_Tomohiro_Ando","fullname":"Tomohiro Ando","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7821268,"url":"researcher\/7821268_Jushan_Bai","fullname":"Jushan Bai","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Oct 2015","journal":"Econometric Reviews","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283666618_Selecting_the_Regularization_Parameters_in_High-dimensional_Panel_Data_Models_Consistency_and_Efficiency","usePlainButton":true,"publicationUid":283666618,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.19","url":"publication\/283666618_Selecting_the_Regularization_Parameters_in_High-dimensional_Panel_Data_Models_Consistency_and_Efficiency","title":"Selecting the Regularization Parameters in High-dimensional Panel Data Models: Consistency and Efficiency","displayTitleAsLink":true,"authors":[{"id":2060647719,"url":"researcher\/2060647719_Tomohiro_Ando","fullname":"Tomohiro Ando","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7821268,"url":"researcher\/7821268_Jushan_Bai","fullname":"Jushan Bai","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Econometric Reviews 10\/2015;  DOI:10.1080\/07474938.2015.1092822"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283666618_Selecting_the_Regularization_Parameters_in_High-dimensional_Panel_Data_Models_Consistency_and_Efficiency","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283666618_Selecting_the_Regularization_Parameters_in_High-dimensional_Panel_Data_Models_Consistency_and_Efficiency\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56aba1ca6d0ef"},"id":"rgw17_56aba1ca6d0ef","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=283666618","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":38908205,"url":"researcher\/38908205_Dominique_J_Pepper","fullname":"Dominique J. Pepper","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":75635434,"url":"researcher\/75635434_Michael_Schomaker","fullname":"Michael Schomaker","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39635770,"url":"researcher\/39635770_Robert_J_Wilkinson","fullname":"Robert J. Wilkinson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":50849905,"url":"researcher\/50849905_Virginia_de_Azevedo","fullname":"Virginia de Azevedo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Oct 2015","journal":"AIDS Research and Therapy","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/282663386_Independent_predictors_of_tuberculosis_mortality_in_a_high_HIV_prevalence_setting_A_retrospective_cohort_study","usePlainButton":true,"publicationUid":282663386,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.46","url":"publication\/282663386_Independent_predictors_of_tuberculosis_mortality_in_a_high_HIV_prevalence_setting_A_retrospective_cohort_study","title":"Independent predictors of tuberculosis mortality in a high HIV prevalence setting: A retrospective cohort study","displayTitleAsLink":true,"authors":[{"id":38908205,"url":"researcher\/38908205_Dominique_J_Pepper","fullname":"Dominique J. Pepper","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":75635434,"url":"researcher\/75635434_Michael_Schomaker","fullname":"Michael Schomaker","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39635770,"url":"researcher\/39635770_Robert_J_Wilkinson","fullname":"Robert J. Wilkinson","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":50849905,"url":"researcher\/50849905_Virginia_de_Azevedo","fullname":"Virginia de Azevedo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38990957,"url":"researcher\/38990957_Gary_Maartens","fullname":"Gary Maartens","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["AIDS Research and Therapy 10\/2015; 12(1). DOI:10.1186\/s12981-015-0076-5"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/282663386_Independent_predictors_of_tuberculosis_mortality_in_a_high_HIV_prevalence_setting_A_retrospective_cohort_study","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/282663386_Independent_predictors_of_tuberculosis_mortality_in_a_high_HIV_prevalence_setting_A_retrospective_cohort_study\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56aba1ca6d0ef"},"id":"rgw18_56aba1ca6d0ef","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=282663386","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw15_56aba1ca6d0ef"},"id":"rgw15_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=232805135&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":232805135,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":232805135,"publicationType":"article","linkId":"0912f509b1437973f0000000","fileName":"DeepGPs_CameraReady.pdf","fileUrl":"profile\/Andreas_Damianou\/publication\/232805135_Deep_Gaussian_Processes\/links\/0912f509b1437973f0000000.pdf","name":"Andreas Damianou","nameUrl":"profile\/Andreas_Damianou","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"May 16, 2014","fileSize":"790.59 KB","widgetId":"rgw21_56aba1ca6d0ef"},"id":"rgw21_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=232805135&linkId=0912f509b1437973f0000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw20_56aba1ca6d0ef"},"id":"rgw20_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=232805135&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":246,"valueFormatted":"246","widgetId":"rgw22_56aba1ca6d0ef"},"id":"rgw22_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=232805135","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw19_56aba1ca6d0ef"},"id":"rgw19_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=232805135&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":232805135,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw24_56aba1ca6d0ef"},"id":"rgw24_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=232805135&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":246,"valueFormatted":"246","widgetId":"rgw25_56aba1ca6d0ef"},"id":"rgw25_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=232805135","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw23_56aba1ca6d0ef"},"id":"rgw23_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=232805135&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Deep Gaussian Processes\nAndreas C. Damianou\nDept. of Computer Science & Sheffield Institute for Translational Neuroscience,\nUniversity of Sheffield, UK\n{andreas.damianou, n.lawrence}@sheffield.ac.uk\nNeil D. Lawrence\nAbstract\nIn this paper we introduce deep Gaussian process (GP) models. Deep GPs are a\ndeep belief network based on Gaussian process mappings. The data is modeled\nas the output of a multivariate GP. The inputs to that Gaussian process are then\ngoverned by another GP. A single layer model is equivalent to a standard GP or\nthe GP latent variable model (GPLVM). We perform inference in the model by\napproximate variational marginalization. This results in a strict lower bound on\nthe marginal likelihood of the model which we use for model selection (number\nof layers and nodes per layer). Deep belief networks are typically applied to rela-\ntively large data sets using stochastic gradient descent for optimization. Our fully\nBayesian treatment allows for the application of deep models even when data is\nscarce. Model selection by our variational bound shows that a five layer hierarchy\nis justified even when modelling a digit data set containing only 150 examples.\n1Introduction\nProbabilistic modelling with neural network architectures constitute a well studied area of machine\nlearning. The recent advances in the domain of deep learning [Hinton and Osindero, 2006, Bengio\net al., 2012] have brought this kind of models again in popularity. Empirically, deep models seem\nto have structural advantages that have been shown to improve learning in complicated datasets\nassociated with abstract information [Bengio, 2009]. Many interesting variants of the traditional\nneural networks have emerged since the development of efficient training methodologies for deep\narchitectures: deep belief networks, (stacked) restricted Boltzmann machines [Hinton, 2010] etc.\nGaussianprocessmodelswereintroducedinthemachinelearningcommunity asafullyprobabilistic\nsubstitute for the multilayer perceptron, inspired by the observation [Neal, 1996] that a GP is a mul-\ntilayer perceptron with infinite basis functions. It seems prudent, therefore, to investigate how these\nmodels will perform in deep hierarchies. Inference in deep GP models is analytically intractable.\nLawrence and Moore [2007] investigated a maximum a posteriori (MAP) approach for estimation\nof latent variables. For the MAP approach to work, however, a strong prior was required on the top\nlevel of the hierarchy. There are two main contributions in this paper. Firstly, we show how we can\napproximately marginalize the latent variables variationally. This gives a rigorous lower bound on\nthe marginal log likelihood of a deep GP. We test the resulting approach on a toy problem and then\nrecreate results of Lawrence and Moore [2007] on a motion capture example. Our second contri-\nbution is to demonstrate the applicability of deep models even when data is scarce. The variational\nlower bound gives us an objective measure from which we can select different structures for our\ndeep hierarchy (number of layers, number of nodes per layer). In a simple digits example we find\nthat the best lower bound is given by the model with the deepest hierarchy we applied (5 layers).\nThe deep GP consists of a cascade of hidden layers of latent variables where each node acts as output\nfor the layer above and as input for the layer below\u2014with the observed outputs being placed in the\nleaves of the hierarchy. Gaussian processes govern the mappings between the layers.\n1"},{"page":2,"text":"A single layer of the deep GP is a Gaussian process latent variable model (GPLVM). Inference over\nthe latent variables even in this model is intractable. However, Titsias and Lawrence [2010] have\nshown that latent variables can be approximately marginalized allowing a variational lower bound\non the likelihood to be computed. The appropriate size of the latent space can be computed using au-\ntomatic relevance determination (ARD) priors [Neal, 1996, MacKay et al., 1992]. Damianou et al.\n[2011] extended this approach by placing a Gaussian process prior over the latent space giving a\nBayesian dynamical GPLVM. Here we extend that approach to allow us to approximately marginal-\nize any number of hidden layers. We demonstrate how a deep hierarchy of Gaussian processes can\nbe obtained by marginalising out the latent variables in the structure, obtaining a fully Bayesian\ntraining procedure along with a variational approximation to the true posterior of all the latent vari-\nables given the outputs. The resulting model is very flexible, and we discuss a number of potential\napplications in further work.\n2The Model\nWe first consider standard approaches to modeling with GPs. We then extend these ideas to deep\nGPs by considering Gaussian process priors over the inputs to the GP model.\n2.1Standard GP Modelling\nIn the traditional probabilistic inference framework, we are given a set of training input-output pairs,\nstored in matrices X \u2208 RN\u00d7Qand Y \u2208 RN\u00d7Drespectively, and seek to estimate the unobserved,\nlatent function f = f(x), responsible for generating Y given X. In this setting, Gaussian processes\n(GP\u2019s) [Rasmussen and Williams, 2006] can be employed as nonparametric prior distributions over\nthe latent function f. More formally, we assume that each datapoint ynis generated from the\ncorresponding f(xn) by adding independent Gaussian noise, i.e.\nyn= f(xn) + ?n, ? \u223c N(0,\u03c3?I),\n(1)\nand f is drawn from a Gaussian process, i.e. f(x) \u223c GP (0,k(x,x?)). This (zero-mean) Gaus-\nsian process prior only depends on the covariance function k operating on the inputs X. As\nwe wish to obtain a flexible model, we only make very general assumptions about the form of\nthe generative mapping f and this is reflected in the choice of the covariance function which de-\nfines the properties of this mapping. For example, an exponentiated quadratic covariance function\nk(xi,xj) = (\u03c3se)2exp\n2l2\n, forces the latent functions to be infinitely smooth. We de-\nnote any covariance function hyperparameters (such as (\u03c3se,l) of the aforementioned covariance\nfunction) by \u03b8. The collection of latent function instantiations, denoted by F = {fn}N\ndistributed, allowing us to compute analytically the marginal likelihood1\n?\nn=1\nGaussian processes have also been used with success in unsupervised learning scenarios, where the\ninput data X are not directly observed. The Gaussian process latent variable model (GP-LVM)\n[Lawrence, 2005, 2004] provides an elegant solution to this problem by treating the unobserved\ninputs X as latent variables, while employing a product of D independent GPs as prior for the latent\nmapping. The assumed generative procedure now takes the form:\n?\n\u2212(xi\u2212xj)2\n?\nn, is normally\np(Y |X) =\nN\n?\np(yn|fn)p(fn|xn)dF = N(Y |0,KNN+ \u03c32\n?I),KNN= k(X,X).\n(2)\nynd= fd(xn) + ?nd,\n(3)\nwhere ? is again random Gaussian noise and F = {fd}D\ndataset, the Gaussian process priors take the form of a Gaussian distribution\nd=1with fnd = fd(xn). Given a finite\np(F|X) =\nD\n?\nd=1\nN(fd|0,KNN)\n(4)\nwhich, in turn, allows for general non-linear mappings to be marginalised out analytically to obtain\nthe likelihood p(Y |X) =?D\nd=1N(yd|X), analogously to equation (2).\n1All probabilities involving f should also have \u03b8 in the conditioning set, but here we omit it for clarity.\n2"},{"page":3,"text":"2.2Deep Gaussian Processes\nOur deep Gaussian process architecture corresponds to a graphical model with three kinds of nodes,\nillustrated in figure 1(a): the leaf nodes Y \u2208 RN\u00d7Dwhich are the only observed ones, the interme-\ndiate latent spaces Xh\u2208 RN\u00d7Qh,h = 1,...,H \u2212 1, where H is the number of hidden layers, and\nthe parent latent node Z = XH \u2208 RN\u00d7QZ. In our notation the parent node is separated from the\nrest of the hidden layers for notational clarity and because it can be constrained with a prior of our\nchoice and, therefore, be involved in different mathematical expressions. In this deep architecture,\nall intermediate nodes Xhact as inputs for the layer below (including the leaves) and as outputs for\nthe layer above. For simplicity, consider a structure with only two hidden units, as the one depicted\nin figure 1(b). The generative process takes the form:\nynd=fY\nxnq=fX\nd(xn) + ?Y\nq(zn) + ?X\nnd, d = 1,...,D, xn\u2208 RQ\nnq, q = 1,...,Q, zn\u2208 RQZ\n(5)\nand the intermediate node is involved in two Gaussian processes, fYand fX, playing the role of an\ninput and an output respectively: fY\u223c GP(0,kY(X,X))\nstructure can be naturally extended vertically (i.e. deeper hierarchies) or horizontally (i.e. segmen-\ntation of each layer into different partitions of the output space), as we will see later in the paper.\nHowever, it is already obvious how each layer adds a significant number of model parameters (Xh)\nas well as a regularization challenge, since the size of each latent layer is crucial but has to be a priori\ndefined. For this reason, unlike Lawrence and Moore [2007], we seek to variationally marginalise\nout the whole latent space. Not only this will allow us to obtain an automatic Occam\u2019s razor due to\nthe Bayesian training, but also we will end up with a significantly lower number of model parame-\nters, since the variational procedure only adds variational parameters. The first step to this approach,\nis to define automatic relevance determination (ARD) covariance functions for the GPs:\nand\nfX\u223c GP(0,kX(Z,Z)). This\nk(xi,xj) = \u03c32\narde\u22121\n2\n?Q\nq=1wq(xi,q\u2212xj,q)2.\n(6)\nThis covariance function assumes a different weight wqfor each latent dimension and this can be\nexploited in a Bayesian training framework in order to \u201cswitch off\u201d irrelevant dimensions by driv-\ning their corresponding weight to zero, thus helping towards automatically finding the structure of\ncomplex models [Damianou et al., 2012]. However, the nonlinearities introduced by this covariance\nfunction make the Bayesian treatment of this model challenging. Nevertheless, following recent\nnon-standard variational inference methods we can define analytically an approximate Bayesian\ntraining procedure, as will be explained in the next section.\n2.3 Bayesian Training\nA Bayesian training procedure requires optimisation of the model evidence:\nlogp(Y ) = log\n?\np(Y |X)p(X|Z)p(Z)dXdZ.\n(7)\nWhen prior information is available regarding the observed data (e.g. their dynamical nature is\nknown a priori), the prior distribution on the parent latent node can be selected so as to constrain the\nwhole latent space through propagation of the prior density through the cascade. Here we take the\ngeneral case where p(Z) = N(Z|0,I). However, the integral of equation (7) is intractable due to\nthe nonlinear way in which X and Z are treated through the GP priors fYand fX. As a first step,\nwe apply Jensen\u2019s inequality to find a variational lower bound on logp(Y ):\n?\nwhere we expanded the likelihood and, additionally, we introduced a variational distribution Q, the\nform of which will be defined later on. The above integral is still intractable because X and Z\nstill appear nonlinearly in the p(FY|X) and p(FX|Z) terms respectively. A key result of Titsias\nand Lawrence [2010] is that expanding the probability space of the GP prior p(F|X) with extra\nvariables allows for priors on the latent space to be propagated through the nonlinear mapping f.\nMore precisely, we augment the probability space of equation (4) with K auxiliary pseudo-inputs\nlogp(Y ) \u2265 Fv=\nQlogp(Y |FY)p(FY|X)p(X|FX)p(FX|Z)p(Z)\nQ\ndXdZdFYdFX\n(8)\n3"},{"page":4,"text":"\u02dc X \u2208 RK\u00d7Qand\u02dcZ \u2208 RK\u00d7QZthat correspond to a collection of function values UY\u2208 RK\u00d7Dand\nUX\u2208 RK\u00d7Qrespectively2. Following this approach, we obtain the augmented GP likelihood:\np(Y |X,\u02dc X) = p(Y |FY)p(FY|UY,X)p(UY|\u02dc X)dFYdUY\nand similarly for the p(X|Z) term. The pseudo-inputs\u02dc X and\u02dcZ are known as inducing points, and\nwill be dropped from our expressions from now on, for clarity. Note that FYand UYare draws\nfrom the same GP so that p(UY) and p(FY|UY,X) are also Gaussian distributions.\nWe are now able to define a variational distribution Q which, when combined with the new expres-\nsions for the augmented GP priors, results in a tractable variational bound. Specifically, we have:\n(9)\nQ = p(FY|UY,X)q(UY)q(X)p(FX|UX,Z)q(UX)q(Z).\nWe select q(UY) and q(UX) to be free-form variational distributions, while q(X) and q(Z) are\nchosen to be Gaussian, factorised with respect to dimensions:\n(10)\nq(X) =\nQ\n?\nq=1\nN(\u00b5X\nq,SX\nq) and q(Z) =\nQZ\n?\nq=1\nN(\u00b5Z\nq,SZ\nq).\n(11)\nBy substituting equations (10) and (9) back to (8), we see that the \u201cdifficult\u201d terms p(FY|UY,X)\nand p(FX|UX,Z) cancel out in the fraction, leaving a quantity that can be computed analytically:\n?\nMore specifically, we can break the logarithm in equation (12) by grouping the variables of the\nfraction in such a way that the bound can be written as:\nFv=\nQlogp(Y |FY)p(UY)p(X|FX,UX)p(UX)p(Z)\nQ\ndXdZdFYdFXdUYdUX.\n(12)\nFv= gY+ rX+ Hq(X)\u2212 KL[q(Z) ? p(Z)]\n(13)\nwhere H represents the entropy with respect to a distribution, KL denotes the Kullback \u2013 Leibler\ndivergence and, using ?\u00b7? to denote expectations,\n?\n?\nBoth terms only involve known Gaussian densities and are, thus, tractable. The gY term is the same\nas the bound found for the Bayesian GP-LVM [Titsias and Lawrence, 2010]. Since it it only involves\nexpectations with respect to Gaussian distributions, the GP output variables are only involved in a\nquantity of the form Y Y?. Further, as can be seen from the above equations, the function r(\u00b7) is\nsimilar to g(\u00b7) but it involves expectations with respect to densities of all of its variables. Therefore,\nrXwill involve X (i.e. the outputs of the top layer) in a term?XX??\n3 Extending the hierarchy\ngY = g(Y,FY,UY,X) =logp(Y |FY) + logp(UY)\nq(UY)\n?\n?\np(FY|UY,X)q(UY)q(X)\n(14)\nrX= r(X,FX,UX,Z) = logp(X|FX) + logp(UX)\nq(UX)\np(FX|UX,Z)q(UX)q(X)q(Z)\n.\n(15)\nq(X)=?Q\nq=1\u00b5X\nq\u00b5X\nq+ SX\nq.\nAlthough the main calculations were demonstrated in a simple hierarchy, it is easy to extend the\nmodel vertically, i.e. by adding more hidden layers, or horizontally, i.e. by considering conditional\nindependencies of the latent variables belonging to the same layer. The first case only requires\nadding more rXfunctions to the variational bound, i.e. instead of a single rXterm we will now\nhave the a sum:?H\u22121\nNow consider the horizontal expansion scenario and assume that we wish to break the single la-\ntent space Xh, of layer h, to Mhconditionally independent subsets. As long as the variational\ndistribution q(Xh) of equation (11) is chosen to be factorised in a consistent way, this is feasi-\nble by just breaking the original rXhterm of equation (15) into the sum?Mh\n2The number of inducing points K does not need to be the same for all GPs\nh=1rXh, where rXh= r(Xh,FXh,UXh,Xh+1), with XH= Z.\nm=1r(m)\nXh. This fol-\nlows just from the fact that, due to the independence assumption, it holds that logp(Xh|Xh+1) =\n4"},{"page":5,"text":"?Mh\ndifferent observation spaces which, however, we believe they have some commonality. For example,\nwhen the observed data are coming from a video and an audio recording of the same event. Given\nthe above, the variational bound for the most general version of the model takes the form:\nm=1logp(X(m)\ning the gYterm of the bound. This scenario arises when, for example we are presented with multiple\nh\n|Xh+1). Notice that the same principle can also be applied to the leaves by break-\nFv=\nMY\n?\nm=1\ng(m)\nY\n+\nH\u22121\n?\nh=1\nMh\n?\nm=1\nr(m)\nXh+\nH\u22121\n?\nh=1\nHq(Xh)\u2212 KL(q(Z) ? p(Z)).\n(16)\nFigure 1(c) shows the association of this objective function\u2019s terms with each layer of the hierarchy.\nRecall that each r(m)\nY\nterm is associated with a different GP and, thus, is coming with its\nown set of automatic relevance determination (ARD) weights, which were described in equation (6).\nXhand g(m)\n3.1 Deep multiple-output Gaussian processes\nThe particular way of extending the hierarchies horizontally, as presented above, can be seen as\na means of performing unsupervised multiple-output GP learning. This only requires assigning\na different gY term (and, thus, associated ARD weights) to each vector yd, where d indexes the\noutput dimensions. After training our model, we hope that the columns of Y that encode similar\ninformationwillbeassignedrelevanceweightvectorsthatarealsosimilar. Thisideacanbeextended\nto all levels of the hierarchy, thus obtaining a fully factorised deep GP model.\nThis special case of our model makes the connection between our model\u2019s structure and neural\nnetwork architectures more obvious: the ARD parameters play a role similar to the weights of neural\nnetworks, while the latent variables play the role of neurons which learn hierarchies of features.\n...\n(a)\n(b) (c)\nFigure 1: Different graphical representations of the Deep GP model: (a) shows the general architec-\nture with a cascade of H hidden layers, (b) depicts a simplification of a two hidden layer hierarchy\nalso demonstrating the corresponding GP mappings and (c) illustrates the most general case where\nthe leaves and all intermediate nodes are allowed to form conditionally independent groups. The\nterms of the objective (16) corresponding to each layer are included on the left of each level.\n3.2 Parameters and complexity\nIn all graphical model variants shown in figure 1, every arrow represents a generative procedure with\na GP prior, corresponding to a set of parameters {\u02dc X,\u03b8,\u03c3?}. Further, each layer of latent variables\ncorresponds to a variational distribution q(X) which is associated with a set of variational means\nand covariances, as shown in equation (11). The parent node can have the same form as equation\n(11) or can be constrained with a more informative prior which would couple the points of q(Z).\nFor example, a dynamical prior would introduce Q \u00d7 N2parameters which can, nevertheless, be\n5"},{"page":6,"text":"reparametrized using less variables [Damianou et al., 2011]. However, as is evident from equations\n(10) and (12), the inducing points and the parameters of q(X) and q(Z) are variational rather than\nmodel parameters, something which significantly helps in regularizing the problem. Therefore,\nadding more layers to the hierarchy does not introduce many more model parameters. Moreover, as\nin common sparse methods for Gaussian processes [Titsias, 2009], the complexity of each generative\nGP mapping is reduced from the typical O(N3) to O(NM2).\n4 Demonstration\nIn this section we demonstrate the deep GP model in toy and real-world datasets. The model is\ninitialised by performing dimensionality reduction in the observations to obtain the first hidden\nlayer and then repeating this process for the next layers. To obtain these stacked initial spaces we\nexperimented with PCA and the Bayesian GP-LVM, but the end result did not vary significantly.\n4.1Toy data\nWe first test our model in toy data, created by sampling from a three-level hierarchy of GPs. Figure 2\n(a) depicts the true hierarchy: from the top latent layer two intermediate latent signals are generated.\nThese, in turn, together generate 10\u2212dimensional observations (not depicted here) through sampling\nof another GP. These observations are then used to train the following models: a deep GP, a simple\nstacked Isomap and a simple stacked PCA method, the results of which are shown in figures 2 (b, c,\nd) respectively. Note that only our method marginalises the latent space and, in contrast to the other\ntwo, it is not given any information about the dimensionality of each true signal in the hierarchy.\nFurther, as can be seen in figure 2, our model not only finds the correct dimensionality for each\nhidden layer, but it also discovers latent signals which are closer to the real ones.\n(a)(b)(c) (d)\nFigure 2: Attempts to reconstruct the real data (fig. (a)) with our model (b), stacked Isomap (c) and\nstacked PCA (d). Our model can also find the correct dimensionalities automatically.\n4.2 Modeling human motion\nFor our second demonstration we consider the same motion capture dataset used from Lawrence\nand Moore [2007], taken from the CMU MOCAP database3.\n62\u2212dimensional sets of 78 points each, represent two subjects walking towards each other and\nperforming a \u2018high-five\u2019. We applied our method with a two-level hierarchy where the two observa-\ntion sets were taken to be conditionally independent given their parent latent layer. Therefore, we\nobtained three optimised sets of ARD parameters: one for each modality of the bottom layer (shown\nwith bar graphs having bins of different colours\/widths for each modality, in figure 3(b), and one\nARD weight set for the top node, as shown in figure 3(c). Our model discovered a common subspace\nin the intermediate layer, since for dimensions 2 and 6 both ARD sets have a non-zero value. This\nis expected, as the two subjects perform very similar motions with opposite directions. The ARD\nweights are also a means of automatically selecting the dimensionality of each layer and subspace.\nThis kind of modelling is impossible for a MAP method like [Lawrence and Moore, 2007] which\nrequires the exact latent structure to be given a priori. The full latent space learned by the afore-\nmentioned MAP method is plotted in figure 4 (d,e,f), where fig. (d) corresponds to the top latent\nspace and each of the other two encodes information for each of the two interacting subjects. Our\nmethod is not constrained to two dimensional spaces, so for comparison we plot two-dimensional\nThe data, summarised in two\n3http:\/\/mocap.cs.cmu.edu.\n6"},{"page":7,"text":"projections of the dominant dimensions of each subspace in figure 4 (a,b,c). The similarity of the\nlatent spaces is obvious. In contrast to Lawrence and Moore [2007], we did not have to constrain\nthe latent space with dynamics in order to obtain results of good quality.\nFurther, we can sample from these spaces to see what kind of information they encode. Indeed, we\nobserved that the top layer generates outputs which correspond to different variations of the whole\nsequence, while when sampling from the first layer we obtain outputs which only differ in a small\nsubset of the output dimensions, e.g. those corresponding to the subject\u2019s hand.\n(a) (b)\n12345\n(c)\n678910\nFigure 3: Figure (a) shows the deep GP model employed. Figure (b) shows the ARD weights for\nfY1and fY2and figure (c) those for fX.\n(a) (b)(c)(d)(e)(f)\nFigure 4: Left (a,b,c): projections of the latent spaces discovered by our model, Right (d,e,f): the\nfull latent space learned for the model of Lawrence and Moore [2007].\n4.3Deep learning of digit images\nIn our last experiment we demonstrate the ability of our model to learn latent features of increas-\ning abstraction and we demonstrate the usefulness of an analytic bound on the model evidence as a\nmeans of evaluating the quality of the data fit for different choices of the overall depth of the hier-\narchy. We built a dataset consisting of 50 examples for each of the digits 0,1 and 6 taken from the\nUSPS handwritten digit database. Each digit is represented as an image in 16\u00d716 pixels. We exper-\nimented with models of depth ranging from 1 (equivalent to Bayesian GP-LVM) to 5 hidden layers\nand evaluated each model by measuring the nearest neighbour error in the latent features discovered\nin each hierarchy. Our finding was that the approximate evidence of the model was increasing with\nthe number of layers and so was the quality of the model in terms of nearest neighbour errors4. In-\ndeed, the single-layer model made 5 mistakes even though it automatically decided to use 10 latent\ndimensions and the quality of the trained models was increasing with the number of hidden layers.\nFinally, only one point had a nearest neighbour of a different class in the 4\u2212dimensional top level\u2019s\nfeature space of a model with depth 5. A 2D projection of this space is plotted in figure 6(a). The\nARD weights for this model are depicted in figure 5.\nTo further test the nature of the discovered latent features, we generated outputs by sampling from\neach hidden layer. As can be seen in figure 6(b), the first hierarchical layers encode very local\nfeatures whereas the higher ones encode much more abstract information.\n5Discussion and future work\nWe have introduced a framework for efficient Bayesian training of hierarchical Gaussian process\nmappings for unsupervised learning. Our approach approximately marginalises out the latent space,\nthus allowing for automatic structure discovery in the hierarchy. The method was able to success-\nfullylearnahierarchyoffeatureswhichdescribenaturalhumanmotionandthepixelsofhandwritten\n4The results were the same when we took into account the Bayesian Information Criterion.\n7"},{"page":8,"text":"Figure 5: The ARD weights of a deep GP with 5 hidden layers. The top layer is on the far left.\nx 10\n\u22125\n0\n5\n10\n15\n(a)(b)\nFigure 6: Fig. (a) demonstrates the nearest neighbour class separation test on a deep GP model\nwith depth 5. Fig. (b) shows outputs obtained when sampling from this model. The first two rows\n(top-down), which were sampled from layers 1 and 2 respectively, encode very local features, e.g.\nexplaining if a zero is a closed circle or not, or how big the circle of a 6 is. We discovered many more\nlocal features when we sampled from different dimensions. On the other hand, when we sampled\nfrom the two dominant dimensions of the parent latent node (two rows in the bottom) we obtained\nmuch more varying outputs. Thus, the higher levels indeed encode much more abstract information.\ndigits. Our variational lower bound selected a deep hierarchical representation for handwritten digits\neven though the data in our experiment was relatively scarce (150 data points).\nIn the future, we wish to test our model on various inference tasks, such as class conditional density\nestimation. Our method can also be used to improve existing deep algorithms, something which we\nplan to further investigate by incorporating ideas from past approaches. Indeed, previous efforts to\ncombine GPs with deep structures were successful at unsupervised pre-training [Erhan et al., 2010]\nor guiding [Snoek et al., 2012] of traditional deep models.\nAlthough the experiments presented here considered only up to five layers in the hierarchy, the\nmethodology is directly applicable to deeper architectures, with which we intend to experiment in\nthe future. The marginalisation of the latent space allows for such an expansion with simultaneous\nregularisation. The variational lower bound allows us to make a principled choice between models\ntrained using different initializations and with different numbers of layers.\nThe deep hierarchy we have proposed could also be used with inputs governing the top layer of the\nhierarchy, leading to a powerful model for regression based on Gaussian processes, but which is\nnot itself a Gaussian process. We expect such a model to have applications in multitask learning\n(where intermediate layers could learn representations shared across the tasks) and in modelling\nnonstationary data or data involving jumps. These are both areas where a single layer GP struggles.\nAcknowledgments\nResearch was partially supported by the University of Sheffield Moody endowment fund and the\nGreek State Scholarships Foundation.\nReferences\nY. Bengio.\n(1):1\u2013127,\nhttp:\/\/dx.doi.org\/10.1561\/2200000006.\nLearning Deep Architectures for AI.\nJan. 2009. ISSN 1935-8237.\nFound. Trends Mach. Learn., 2\n10.1561\/2200000006.doi:URL\n8"},{"page":9,"text":"Y. Bengio, A. C. Courville, and P. Vincent. Unsupervised feature learning and deep learning: A\nreview and new perspectives. CoRR, abs\/1206.5538, 2012.\nA. C. Damianou, M. Titsias, and N. D. Lawrence. Variational gaussian process dynamical systems.\nIn Advances in Neural Information Processing Systems (NIPS) 24, pages 2510\u20132518, 2011.\nA. C. Damianou, C. H. Ek, M. K. Titsias, and N. D. Lawrence. Manifold relevance determination.\nIn J. Langford and J. Pineau, editors, Proceedings of the International Conference in Machine\nLearning, volume 29, San Francisco, CA, 2012. Morgan Kauffman.\nD. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and S. Bengio. Why does unsu-\npervised pre-training help deep learning? J. Mach. Learn. Res., 11:625\u2013660, Mar. 2010. ISSN\n1532-4435. URL http:\/\/dl.acm.org\/citation.cfm?id=1756006.1756025.\nG. Hinton. A Practical Guide to Training Restricted Boltzmann Machines. Technical report, 2010.\nURL http:\/\/www.cs.toronto.edu\/\u02dchinton\/absps\/guideTR.pdf.\nG. E. Hinton and S. Osindero. A fast learning algorithm for deep belief nets. Neural Computation,\n18:2006, 2006.\nN. D. Lawrence. Gaussian process latent variable models for visualisation of high dimensional data.\nIn In NIPS, page 2004, 2004.\nN. D. Lawrence. Probabilistic non-linear principal component analysis with Gaussian process latent\nvariable models. Journal of Machine Learning Research, 6:1783\u20131816, 2005.\nN. D. Lawrence and A. J. Moore.Hierarchical Gaussian process latent variable models.\nZ. Ghahramani, editor, Proceedings of the International Conference in Machine Learning, vol-\nume 24, pages 481\u2013488. Omnipress, 2007. ISBN 1-59593-793-3.\nD. J. MacKay, J. Bridle, K. Rose, S. Sibisi, J. Skilling, H. Sompolinsky, and N. W. T. Comments.\nBayesian methods for adaptive models, 1992.\nR. M. Neal. Bayesian Learning for Neural Networks. Springer, 1996. Lecture Notes in Statistics\n118.\nC. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. Cambridge,\nMA, 2006. ISBN 0-262-18253-X.\nJ. Snoek, R. P. Adams, and H. Larochelle. On nonparametric guidance for learning autoencoder\nrepresentations. In Fifteenth International Conference on Artificial Intelligence and Statistics\n(AISTATS), 2012.\nM. Titsias. Variational learning of inducing variables in sparse Gaussian processes. JMLR W&CP,\n5:567\u2013574, 2009.\nM. Titsias and N. D. Lawrence. Bayesian Gaussian process latent variable model. Journal of\nMachine Learning Research - Proceedings Track, 9:844\u2013851, 2010.\nIn\n9"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Andreas_Damianou\/publication\/232805135_Deep_Gaussian_Processes\/links\/0912f509b1437973f0000000.pdf","widgetId":"rgw26_56aba1ca6d0ef"},"id":"rgw26_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=232805135&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw27_56aba1ca6d0ef"},"id":"rgw27_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=232805135&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":232805135,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"0912f509b1437973f0000000","name":"Andreas Damianou","date":null,"nameLink":"profile\/Andreas_Damianou","filename":"DeepGPs_CameraReady.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Andreas_Damianou\/publication\/232805135_Deep_Gaussian_Processes\/links\/0912f509b1437973f0000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Andreas_Damianou\/publication\/232805135_Deep_Gaussian_Processes\/links\/0912f509b1437973f0000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"9e9a37f50b4694c9976f4b6b81beca32","showFileSizeNote":false,"fileSize":"790.59 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"0912f509b1437973f0000000","name":"Andreas Damianou","date":null,"nameLink":"profile\/Andreas_Damianou","filename":"DeepGPs_CameraReady.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Andreas_Damianou\/publication\/232805135_Deep_Gaussian_Processes\/links\/0912f509b1437973f0000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Andreas_Damianou\/publication\/232805135_Deep_Gaussian_Processes\/links\/0912f509b1437973f0000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"9e9a37f50b4694c9976f4b6b81beca32","showFileSizeNote":false,"fileSize":"790.59 KB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[{"props":{"position":"float","orientation":"portrait","coords":"pag:7:rect:108.00,289.20,396.00,19.87","ordinal":"3"},"assetId":"AS:300606355984397@1448681548307"}],"figureAssetIds":["AS:300606355984397@1448681548307"],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=GVQCk3vjbk4Pt-Q7E7Z1MMlB-VI_XRgQFHC0p_JT7NkwwkPOXAB1Kp7Ou--moR5E2wXJzstsZ_H9ahjylFsEPA.xPVfk3ZQnICxqZtLBgoEEEsnvMUc4Z4Zcz2ubD4VLamcrhllXGROpBxXZgYaWo3jRH79o37JxUNlrMgi6QEgHQ","clickOnPill":"publication.PublicationFigures.html?_sg=1x0lLdO2aNOjLcThN2Y5iQwiZRMZhd4lw0c3l9yWiw70vbAJ6T6_vfCg4T4vV-KMRjfuiW8YiNPhMHX7AZN7Nw.fw_29J3x5GaikEf0764OhJ7NiR7O2KojaNyI27iW7Y7AdieIMT0cVGxwGdGpifKXGp0Hl-CGiif3UsHQ-T2cDw"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FAndreas_Damianou%2Fpublication%2F232805135_Deep_Gaussian_Processes%2Flinks%2F0912f509b1437973f0000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=GKyyCJNeBf7hIJCTEdit1bZpsAY-qB2VWXvMzbyX2Fi3xn_2AyZmYlvGc9WVHYWL_zWOwWhKYjrBn9SUMspl3Q","urlHash":"df199b1f96b994af18520f6feaaf76ec","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=O79CZflLBWXUtMhTAh-3tnhq5DOMOW7vWTaeF2P7Sv0LxPyMufrY2QGYh_zv9fij-NSODcSU1xa1oj3DnsFqh93w8z2zadTrdWSD4xmv9_Q.MJRqeHu8xHMpZm0phEyY73oGEC1YpzJ4q80szKSm4xEcADvtj3P-pnkoBgX8VsM-k197HSO9cUoR_Lu_SqLbXQ.yo-d2VPclu3iS-d9djOzIvunoGOhWHT90wY5xuaEtb5nh6lAH4oGyyItn00miKYx98wYAs78KUiB8hXx9cynQg","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"0912f509b1437973f0000000","trackedDownloads":{"0912f509b1437973f0000000":{"v":false,"d":false}},"assetId":"AS:97259307929612@1400199834861","readerDocId":"6306035","assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":232805135,"commentCursorPromo":null,"widgetId":"rgw29_56aba1ca6d0ef"},"id":"rgw29_56aba1ca6d0ef","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FAndreas_Damianou%2Fpublication%2F232805135_Deep_Gaussian_Processes%2Flinks%2F0912f509b1437973f0000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A97259307929612%401400199834861&publicationUid=232805135&linkId=0912f509b1437973f0000000&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Deep Gaussian Processes","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=tbsXoLOHxTS_9gssp_DkSFmCxUswAx0Tl9LDrtlP8305wmtse48nn0ohUZM91sg7_MNei3YWVYlailGxJCwpVTHeTXAAmyJbzx0UqxOx3Hc.SG3E_hI6D8H0Xqit9DR5Ohf5BIAHFaREHKdryhUAc5OfzQLHnCcTP4YalGIoRmzLgTFIBnGOdnhriApOEXiyUA.cAQQNk5UH62fl-Zr1u9fqTiWYyxs4EB_q5Mslh9C4ohNSzIfxNaDQs64VdWO-fM8TlXH8fQJjvPCBOUrn3OOVQ","publicationUid":232805135,"trackedDownloads":{"0912f509b1437973f0000000":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw31_56aba1ca6d0ef"},"id":"rgw31_56aba1ca6d0ef","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw32_56aba1ca6d0ef"},"id":"rgw32_56aba1ca6d0ef","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw33_56aba1ca6d0ef"},"id":"rgw33_56aba1ca6d0ef","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw34_56aba1ca6d0ef"},"id":"rgw34_56aba1ca6d0ef","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw35_56aba1ca6d0ef"},"id":"rgw35_56aba1ca6d0ef","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw30_56aba1ca6d0ef"},"id":"rgw30_56aba1ca6d0ef","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw28_56aba1ca6d0ef"},"id":"rgw28_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/232805135_Deep_Gaussian_Processes","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56aba1ca6d0ef"},"id":"rgw2_56aba1ca6d0ef","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":232805135},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=232805135&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56aba1ca6d0ef"},"id":"rgw1_56aba1ca6d0ef","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"ltMUYAiEwi4w5JZBAUtZg9QmzrH0OkZ8vYm2Sl9P2zZywzTGaJYHyaSXGJCRS81DdQUQARigwzAtVd8rAxTkqHNwmHUjfA89tD2MbgNvDmdDk+o4iIEFa8FkW\/0LtbgGQhifFlSVYxdaa7MQHqk\/5i8Id9X0l7XfG1a2htYzg4n3eWNAQmy0nJ4B7VWn+2rfGI8WrKvhc9bjFi1zJioYxYmjepaqBrSm\/ExvYF9pCgqn8lk5TOwPGGQtpOGtnKGYB4BMlT7VqDFPlr+0P0qUcKH8Rh4vxvjrX4hU0SUNOoQ=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/232805135_Deep_Gaussian_Processes\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Deep Gaussian Processes\" \/>\n<meta property=\"og:description\" content=\"In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a\ndeep belief network based on Gaussian process mappings. The data is modeled as\nthe output of a multivariate GP. The...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/232805135_Deep_Gaussian_Processes\/links\/0912f509b1437973f0000000\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/232805135_Deep_Gaussian_Processes\" \/>\n<meta property=\"rg:id\" content=\"PB:232805135\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Deep Gaussian Processes\" \/>\n<meta name=\"citation_author\" content=\"Andreas C. Damianou\" \/>\n<meta name=\"citation_author\" content=\"Neil D. Lawrence\" \/>\n<meta name=\"citation_publication_date\" content=\"2012\/11\/01\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Andreas_Damianou\/publication\/232805135_Deep_Gaussian_Processes\/links\/0912f509b1437973f0000000.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/232805135_Deep_Gaussian_Processes\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/232805135_Deep_Gaussian_Processes\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/215868066921738\/styles\/pow\/publicliterature\/FigureList.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-93d1d800-dc49-4c5e-bb09-9409e51a8341","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":596,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw36_56aba1ca6d0ef"},"id":"rgw36_56aba1ca6d0ef","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-93d1d800-dc49-4c5e-bb09-9409e51a8341", "d2b78cd8322a38ef5bece0514f68fb1c21d04f3a");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-93d1d800-dc49-4c5e-bb09-9409e51a8341", "d2b78cd8322a38ef5bece0514f68fb1c21d04f3a");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw37_56aba1ca6d0ef"},"id":"rgw37_56aba1ca6d0ef","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/232805135_Deep_Gaussian_Processes","requestToken":"YcorPdiUacMzYD9wMg+KmsyIT3MFeUw1x1f5gNk89iOkacPHcsks8M3FFK9Dac3RPcH02KYgPRIKfDm2aa8+D9Ra0wIY9kZiHMtJIN8823eUFm76bvjom+Zu++\/b0VsCZG71KIvoK0KK01xTXHkb\/KGE2Shn6NTQZWecLPR0aTWivfLcJ9Y7YQ6M3YPK4fC7hGzb\/1\/A3wIyQDlCjn5rQAEkoTG6B\/Q\/Wo\/zfXIZ4EW63UyoS5tZAjT8F\/PgCiQMOE4+9ZgixaUjWjWkcW29s6ctBRk2qm8v8oKz1d41QvA=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=0SLFNlQ85uA3bJMCpDwHI2yHcGY-tIv4gCVU2FO1FZgscATmsY20j8sRUPCyCGEw","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjMyODA1MTM1X0RlZXBfR2F1c3NpYW5fUHJvY2Vzc2Vz","signupCallToAction":"Join for free","widgetId":"rgw39_56aba1ca6d0ef"},"id":"rgw39_56aba1ca6d0ef","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw38_56aba1ca6d0ef"},"id":"rgw38_56aba1ca6d0ef","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw40_56aba1ca6d0ef"},"id":"rgw40_56aba1ca6d0ef","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
