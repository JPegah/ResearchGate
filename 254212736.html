<!DOCTYPE html> <html lang="en" class="" id="rgw47_56ab9d690670d"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="syH3EtjmQLfTJ9+jS5lje46HbeLYeCYoBIlv7bVer6Cs4qH53lK+0Jf5Py7outXNdc1RPoo8TjEm0ck5bYfdS1ZpPde5qZOVLNt2e+29TNao9p8pLxDd+o5tMEwQya5+YC0CDO+s4uA9E/1JArpLPxzMSirBhLs9p6pytmpGlWfk0MpOz7ESZ4QPwjTKUyiavzIV/1FVggDhryUonnF6T12ptRT7XGrfQgPa3aDvvRs+JsgkvPCcva2K1hChazRHlB/uKGlFrNUO89rauXAKBQs87D7gIc1lOt4lk2EnU2Y="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-c1ed37e9-a1c1-42bc-b3f2-34b9efd6406b",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Variational inference for Dirichlet process mixtures. Bayesian Anal 1:121-144" />
<meta property="og:description" content="Dirichlet process (DP) mixture models are the cornerstone of nonparametric Bayesian
statistics, and the development of Monte-Carlo Markov chain (MCMC) sampling methods for DP
mixtures has enabled..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144/links/53d6746a0cf2a7fbb2eaa5f3/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144" />
<meta property="rg:id" content="PB:254212736" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1214/06-BA104" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Variational inference for Dirichlet process mixtures. Bayesian Anal 1:121-144" />
<meta name="citation_author" content="David M. Blei" />
<meta name="citation_author" content="Michael I. Jordan" />
<meta name="citation_publication_date" content="2006/03/01" />
<meta name="citation_issn" content="1936-0975" />
<meta name="citation_volume" content="1" />
<meta name="citation_issue" content="1" />
<meta name="citation_doi" content="10.1214/06-BA104" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Michael_Jordan13/publication/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144/links/53d6746a0cf2a7fbb2eaa5f3.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Variational inference for Dirichlet process mixtures. Bayesian Anal 1:121-144 (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Variational inference for Dirichlet process mixtures. Bayesian Anal 1:121-144 on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab9d690670d" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab9d690670d" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab9d690670d">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1214%2F06-BA104&rft.atitle=Variational%20inference%20for%20Dirichlet%20process%20mixtures.%20Bayesian%20Anal%201%3A121-144&rft.title=Bayesian%20Analysis&rft.jtitle=Bayesian%20Analysis&rft.volume=1&rft.issue=1&rft.date=2006&rft.issn=1936-0975&rft.au=David%20M.%20Blei%2CMichael%20I.%20Jordan&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Variational inference for Dirichlet process mixtures. Bayesian Anal 1:121-144</h1> <meta itemprop="headline" content="Variational inference for Dirichlet process mixtures. Bayesian Anal 1:121-144">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144/links/53d6746a0cf2a7fbb2eaa5f3/smallpreview.png">  <div id="rgw7_56ab9d690670d" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab9d690670d"> <a href="researcher/2064238818_David_M_Blei" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="David M. Blei" alt="David M. Blei" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">David M. Blei</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw9_56ab9d690670d">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2064238818_David_M_Blei"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="David M. Blei" alt="David M. Blei" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2064238818_David_M_Blei" class="display-name">David M. Blei</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56ab9d690670d" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Michael_Jordan13" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A278648436346888%401443446372379_m" title="Michael Jordan" alt="Michael Jordan" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Michael Jordan</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw11_56ab9d690670d" data-account-key="Michael_Jordan13">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Michael_Jordan13"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A278648436346888%401443446372379_l" title="Michael Jordan" alt="Michael Jordan" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Michael_Jordan13" class="display-name">Michael Jordan</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_of_California_Berkeley" title="University of California, Berkeley">University of California, Berkeley</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">     Bayesian Analysis   <meta itemprop="datePublished" content="2006-03">  03/2006;  1(1).    DOI:&nbsp;10.1214/06-BA104           </div> <div id="rgw12_56ab9d690670d" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>Dirichlet process (DP) mixture models are the cornerstone of nonparametric Bayesian<br />
statistics, and the development of Monte-Carlo Markov chain (MCMC) sampling methods for DP<br />
mixtures has enabled the application of nonparametric Bayesian methods to a variety of<br />
practical data analysis problems. However, MCMC sampling can be prohibitively slow, and it<br />
is important to explore alternatives. One class of alternatives is provided by variational<br />
methods, a class of deterministic algorithms that convert inference problems into<br />
optimization problems (Opper and Saad 2001; Wainwright and Jordan 2003). Thus far,<br />
variational methods have mainly been explored in the parametric setting, in particular<br />
within the formalism of the exponential family (Attias 2000; Ghahramani and Beal 2001;<br />
Blei et al. 2003). In this paper, we present a variational inference algorithm for DP<br />
mixtures. We present experiments that compare the algorithm to Gibbs sampling algorithms<br />
for DP mixtures of Gaussians and present an application to a large-scale image analysis <br />
problem.</div> </p>  </div>  </div>   </div>      <div class="action-container"> <div id="rgw13_56ab9d690670d" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw27_56ab9d690670d">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw39_56ab9d690670d">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Michael_Jordan13/publication/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144/links/53d6746a0cf2a7fbb2eaa5f3.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Michael_Jordan13">Michael Jordan</a>, <span class="js-publication-date"> Jul 28, 2014 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw41_56ab9d690670d" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw42_56ab9d690670d" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw43_56ab9d690670d" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw44_56ab9d690670d" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw45_56ab9d690670d" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw46_56ab9d690670d" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw40_56ab9d690670d" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMichael_Jordan13%2Fpublication%2F254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144%2Flinks%2F53d6746a0cf2a7fbb2eaa5f3.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw26_56ab9d690670d"  itemprop="articleBody">  <p>Page 1</p> <p>Bayesian Analysis (2006)<br />1, Number 1, pp. 121–144<br />Variational Inference for Dirichlet Process<br />Mixtures<br />David M. Blei∗<br />Michael I. Jordan†<br />Abstract.<br />parametric Bayesian statistics, and the development of Monte-Carlo Markov chain<br />(MCMC) sampling methods for DP mixtures has enabled the application of non-<br />parametric Bayesian methods to a variety of practical data analysis problems.<br />However, MCMC sampling can be prohibitively slow, and it is important to ex-<br />plore alternatives. One class of alternatives is provided by variational methods, a<br />class of deterministic algorithms that convert inference problems into optimization<br />problems (Opper and Saad 2001; Wainwright and Jordan 2003). Thus far, varia-<br />tional methods have mainly been explored in the parametric setting, in particular<br />within the formalism of the exponential family (Attias 2000; Ghahramani and Beal<br />2001; Blei et al. 2003). In this paper, we present a variational inference algorithm<br />for DP mixtures. We present experiments that compare the algorithm to Gibbs<br />sampling algorithms for DP mixtures of Gaussians and present an application to<br />a large-scale image analysis problem.<br />Dirichlet process (DP) mixture models are the cornerstone of non-<br />Keywords: Dirichlet processes, hierarchical models, variational inference, image<br />processing, Bayesian computation<br />1 Introduction<br />The methodology of Monte Carlo Markov chain (MCMC) sampling has energized Bayesian<br />statistics for more than a decade, providing a systematic approach to the computation<br />of likelihoods and posterior distributions, and permitting the deployment of Bayesian<br />methods in a rapidly growing number of applied problems. However, while an unques-<br />tioned success story, MCMC is not an unqualified one—MCMC methods can be slow<br />to converge and their convergence can be difficult to diagnose. While further research<br />on sampling is needed, it is also important to explore alternatives, particularly in the<br />context of large-scale problems.<br />One such class of alternatives is provided by variational inference methods<br />(Ghahramani and Beal 2001; Jordan et al. 1999; Opper and Saad 2001; Wainwright and Jordan<br />2003; Wiegerinck 2000). Like MCMC, variational inference methods have their roots in<br />statistical physics, and, in contradistinction to MCMC methods, they are deterministic.<br />The basic idea of variational inference is to formulate the computation of a marginal<br />or conditional probability in terms of an optimization problem. This (generally in-<br />tractable) problem is then “relaxed,” yielding a simplified optimization problem that<br />∗School<br />http://www.cs.berkeley.edu/~blei/<br />†Department of Statistics and Computer Science Division, University of California, Berkeley, CA,<br />http://www.cs.berkeley.edu/~jordan/<br />ofComputer Science,CarnegieMellon University,Pittsburgh, PA,<br />c ? 2006 International Society for Bayesian Analysis ba0001</p>  <p>Page 2</p> <p>122<br />Variational inference for Dirichlet process mixtures<br />depends on a number of free parameters, known as variational parameters. Solving<br />for the variational parameters gives an approximation to the marginal or conditional<br />probabilities of interest.<br />Variational inference methods have been developed principally in the context of the<br />exponential family, where the convexity properties of the natural parameter space and<br />the cumulant function yield an elegant general variational formalism<br />(Wainwright and Jordan 2003). For example, variational methods have been developed<br />for parametric hierarchical Bayesian models based on general exponential family spec-<br />ifications (Ghahramani and Beal 2001). MCMC methods have seen much wider appli-<br />cation. In particular, the development of MCMC algorithms for nonparametric models<br />such as the Dirichlet process has led to increased interest in nonparametric Bayesian<br />methods. In the current paper, we aim to close this gap by developing variational<br />methods for Dirichlet process mixtures.<br />The Dirichlet process (DP), introduced in Ferguson (1973), is a measure on measures.<br />The DP is parameterized by a base distribution G0and a positive scaling parameter<br />α.1Suppose we draw a random measure G from a Dirichlet process, and independently<br />draw N random variables ηnfrom G:<br />G|{G0,α}∼<br />∼<br />DP(G0,α)<br />G,n ∈ {1,...,N}.ηn<br />Marginalizing out the random measure G, the joint distribution of {η1,...,ηN} follows<br />a P´ olya urn scheme (Blackwell and MacQueen 1973). Positive probability is assigned to<br />configurations in which different ηntake on identical values; moreover, the underlying<br />random measure G is discrete with probability one. This is seen most directly in the<br />stick-breaking representation of the DP, in which G is represented explicitly as an infinite<br />sum of atomic measures (Sethuraman 1994).<br />The Dirichlet process mixture model (Antoniak 1974) adds a level to the hierarchy<br />by treating ηnas the parameter of the distribution of the nth observation. Given the<br />discreteness of G, the DP mixture has an interpretation as a mixture model with an<br />unbounded number of mixture components.<br />Given a sample {x1,...,xN} from a DP mixture, our goal is to compute the predic-<br />tive density:<br />p(x|x1,...,xN,α,G0) =<br />?<br />p(x|η)p(η |x1,...,xN,α,G0)dη,(1)<br />As in many hierarchical Bayesian models, the posterior distribution p(η |x1,...,xN,G0,α)<br />is complicated and is not available in a closed form. MCMC provides one class of approx-<br />imations for this posterior and the predictive density (MacEachern 1994; Escobar and West<br />1995; Neal 2000).<br />1Ferguson (1973) parameterizes the Dirichlet process by a single base measure, which is αG0in our<br />notation.</p>  <p>Page 3</p> <p>D. M. Blei and M. I. Jordan123<br />In this paper, we present a variational inference algorithm for DP mixtures based<br />on the stick-breaking representation of the underlying DP. The algorithm involves two<br />probability distributions—the posterior distribution p and a variational distribution q.<br />The latter is endowed with free variational parameters, and the algorithmic problem<br />is to adjust these parameters so that q approximates p. We also use a stick-breaking<br />representation for q, but in this case we truncate the representation to yield a finite-<br />dimensional representation. While in principle we could also truncate p, turning the<br />model into a finite-dimensional model, it is important to emphasize at the outset that<br />this is not our approach—we truncate only the variational distribution.<br />The paper is organized as follows. In Section 2 we provide basic background on<br />DP mixture models, focusing on the case of exponential family mixtures. In Section 3<br />we present a variational inference algorithms for DP mixtures. Section 4 overviews<br />MCMC algorithms for the DP mixture, discussing algorithms based both on the P´ olya<br />urn representation and the stick-breaking representation. Section 5 presents the results<br />of experimental comparisons, Section 6 presents an analysis of natural image data, and<br />Section 7 presents our conclusions.<br />2 Dirichlet process mixture models<br />Let η be a continuous random variable, let G0be a non-atomic probability distribution<br />for η, and let α be a positive, real-valued scalar. A random measure G is distributed<br />according to a Dirichlet process (DP) (Ferguson 1973), with scaling parameter α and<br />base distribution G0, if for all natural numbers k and k-partitions {B1,...,Bk},<br />(G(B1),G(B2),...,G(Bk)) ∼ Dir(αG0(B1),αG0(B2),...,αG0(Bk)). (2)<br />Integrating out G, the joint distribution of the collection of variables {η1,...,ηn} ex-<br />hibits a clustering effect; conditioning on n − 1 draws, the nth value is, with positive<br />probability, exactly equal to one of those draws:<br />p(·|η1,...,ηn−1) ∝ αG0(·) +<br />n−1<br />?<br />i=1<br />δηi(·). (3)<br />Thus, the variables {η1,...,ηn−1} are randomly partitioned according to which vari-<br />ables are equal to the same value, with the distribution of the partition obtained from<br />a P´ olya urn scheme (Blackwell and MacQueen 1973). Let {η∗<br />tinct values of {η1,...,ηn−1}, let c = {c1,...,cn−1} be assignment variables such that<br />ηi= η∗<br />follows the urn distribution:<br />1,...,η∗<br />|c|} denote the dis-<br />ci, and let |c| denote the number of cells in the partition. The distribution of ηn<br />ηn=<br />?<br />η∗<br />i<br />with prob.<br />with prob.<br />|{j :cj=i}|<br />n−1+α<br />α<br />n−1+α, η,η ∼ G0<br />(4)<br />where |{j : cj= i}| is the number of times the value η∗<br />ioccurs in {η1,...,ηn−1}.</p>  <p>Page 4</p> <p>124<br />Variational inference for Dirichlet process mixtures<br />In the Dirichlet process mixture model, the DP is used as a nonparametric prior in<br />a hierarchical Bayesian specification (Antoniak 1974):<br />G|{α,G0}<br />ηn|G<br />Xn|ηn<br />∼<br />∼<br />∼<br />DP(α,G0)<br />G<br />p(xn|ηn).<br />Data generated from this model can be partitioned according to the distinct values of<br />the parameter. Taking this view, the DP mixture has a natural interpretation as a<br />flexible mixture model in which the number of components (i.e., the number of cells in<br />the partition) is random and grows as new data are observed.<br />The definition of the DP via its finite dimensional distributions in Equation (2)<br />reposes on the Kolmogorov consistency theorem (Ferguson 1973). Sethuraman (1994)<br />provides a more explicit characterization of the DP in terms of a stick-breaking construc-<br />tion. Consider two infinite collections of independent random variables, Vi∼ Beta(1,α)<br />and η∗<br />i∼ G0, for i = {1,2,...}. The stick-breaking representation of G is as follows:<br />πi(v) = vi<br />i−1<br />?<br />j=1<br />(1 − vj)(5)<br />G =<br />∞<br />?<br />i=1<br />πi(v)δη∗<br />i. (6)<br />This representation of the DP makes clear that G is discrete (with probability one); the<br />support of G consists of a countably infinite set of atoms, drawn independently from G0.<br />The mixing proportions πi(v) are given by successively breaking a unit length “stick”<br />into an infinite number of pieces. The size of each successive piece, proportional to the<br />rest of the stick, is given by an independent draw from a Beta(1,α) distribution.<br />In the DP mixture, the vector π(v) comprises the infinite vector of mixing pro-<br />portions and {η∗<br />2,...} are the atoms representing the mixture components. Let Zn<br />be an assignment variable of the mixture component with which the data point xnis<br />associated. The data can be described as arising from the following process:<br />1,η∗<br />1. Draw Vi|α ∼ Beta(1,α),i = {1,2,...}<br />2. Draw η∗<br />i|G0∼ G0,i = {1,2,...}<br />3. For the nth data point:<br />(a) Draw Zn|{v1,v2,...} ∼ Mult(π(v)).<br />(b) Draw Xn|zn∼ p(xn|η∗<br />zn).<br />In this paper, we restrict ourselves to DP mixtures for which the observable data<br />are drawn from an exponential family distribution, and where the base distribution for<br />the DP is the corresponding conjugate prior.</p>  <p>Page 5</p> <p>D. M. Blei and M. I. Jordan 125<br />λ<br />α<br />N<br />8<br />Z<br />X<br />V<br />η*<br />n<br />n<br />k<br />k<br />Figure 1: Graphical model representation of an exponential family DP mixture. Nodes<br />denote random variables, edges denote possible dependence, and plates denote replica-<br />tion.<br />The stick-breaking construction for the DP mixture is depicted as a graphical model<br />in Figure 1. The conditional distributions of Vkand Znare as described above. The<br />distribution of Xnconditional on Znand {η∗<br />1,η∗<br />2,...} is<br />p(xn|zn,η∗<br />1,η∗<br />2,...) =<br />∞<br />?<br />i=1<br />?<br />h(xn)exp{η∗<br />i<br />Txn− a(η∗<br />i)}<br />?1[zn=i]<br />,<br />where a(η∗<br />is the sufficient statistic for the natural parameter η.<br />i) is the appropriate cumulant function and we assume for simplicity that x<br />The vector of sufficient statistics of the corresponding conjugate family is (η∗T,−a(η∗))T.<br />The base distribution is<br />p(η∗|λ) = h(η∗)exp{λT<br />1η∗+ λ2(−a(η∗)) − a(λ)}, (7)<br />where we decompose the hyperparameter λ such that λ1 contains the first dim(η∗)<br />components and λ2is a scalar.<br />3 Variational inference for DP mixtures<br />There is no direct way to compute the posterior distribution under a DP mixture prior.<br />Approximate inference methods are required for DP mixtures and Markov chain Monte<br />Carlo (MCMC) sampling methods have become the methodology of choice (MacEachern<br />1994; Escobar and West 1995; MacEachern 1998; Neal 2000; Ishwaran and James 2001).<br />Variational inference provides an alternative, deterministic methodology for approx-<br />imating likelihoods and posteriors (Wainwright and Jordan 2003). Consider a model<br />with hyperparameters θ, latent variables W = {W1,...,WM}, and observations x =<br />{x1,...,xN}. The posterior distribution of the latent variables is:<br />p(w|x,θ) = exp{logp(x,w|θ) − logp(x|θ)}. (8)</p>  <p>Page 6</p> <p>126<br />Variational inference for Dirichlet process mixtures<br />Working directly with this posterior is typically precluded by the need to compute<br />the normalizing constant. The log marginal probability of the observations is:<br />logp(x|θ) = log<br />?<br />p(w,x|θ)dw, (9)<br />which may be difficult to compute given that the latent variables become dependent<br />when conditioning on observed data.<br />MCMC algorithms circumvent this computation by constructing an approximate<br />posterior based on samples from a Markov chain whose stationary distribution is the<br />posterior of interest. Gibbs sampling is the simplest MCMC algorithm; one iteratively<br />samples each latent variable conditioned on the previously sampled values of the other<br />latent variables:<br />p(wi|w−i,x,θ) = exp{logp(w,x|θ) − logp(w−i,x|θ)}. (10)<br />The normalizing constants for these conditional distributions are assumed to be available<br />analytically for settings in which Gibbs sampling is appropriate.<br />Variational inference is based on reformulating the problem of computing the poste-<br />rior distribution as an optimization problem, perturbing (or, “relaxing”) that problem,<br />and finding solutions to the perturbed problem (Wainwright and Jordan 2003). In this<br />paper, we work with a particular class of variational methods known as mean-field meth-<br />ods. These are based on optimizing Kullback-Leibler (KL) divergence with respect to<br />a so-called variational distribution. In particular, let qν(w) be a family of distributions<br />indexed by a variational parameter ν. We aim to minimize the KL divergence between<br />qν(w) and p(w|x,θ):<br />D(qν(w)||p(w|x,θ)) = Eq[logqν(W)] − Eq[logp(W,x|θ)] + logp(x|θ), (11)<br />where here and elsewhere in the paper we omit the variational parameters ν when using<br />q as a subscript of an expectation. Notice that the problematic marginal probability<br />does not depend on the variational parameters; it can be ignored in the optimization.<br />The minimization in Equation (11) can be cast alternatively as the maximization of<br />a lower bound on the log marginal likelihood:<br />logp(x|θ) ≥ Eq[logp(W,x|θ)] − Eq[logqν(W)]. (12)<br />The gap in this bound is the divergence between qν(w) and the true posterior.<br />For the mean-field framework to yield a computationally effective inference method,<br />it is necessary to choose a family of distributions qν(w) such that we can tractably<br />optimize Equation (11). In constructing that family, one typically breaks some of the<br />dependencies between latent variables that make the true posterior difficult to compute.<br />In the next sections, we consider fully-factorized variational distributions which break<br />all of the dependencies.</p>  <p>Page 7</p> <p>D. M. Blei and M. I. Jordan127<br />3.1 Mean field variational inference in exponential families<br />For each latent variable, let us assume that the conditional distribution p(wi|w−i,x,θ)<br />is a member of the exponential family2:<br />p(wi|w−i,x,θ) = h(wi)exp{gi(w−i,x,θ)Twi− a(gi(w−i,x,θ))}, (13)<br />where gi(w−i,x,θ) is the natural parameter for wiwhen conditioning on the remaining<br />latent variables and the observations.<br />In this setting it is natural to consider the following family of distributions as mean-<br />field variational approximations (Ghahramani and Beal 2001):<br />qν(w) =<br />M<br />?<br />i=1<br />exp{νT<br />iwi− a(wi)}, (14)<br />where ν = {ν1,ν2,...,νM} are variational parameters. Indeed, it turns out that the<br />variational algorithm that we obtain using this fully-factorized family is reminiscent<br />of Gibbs sampling. In particular, as we show in Appendix 7, the optimization of KL<br />divergence with respect to a single variational parameter νiis achieved by computing<br />the following expectation:<br />νi= Eq[gi(W−i,x,θ)]. (15)<br />Repeatedly updating each parameter in turn by computing this expectation amounts<br />to performing coordinate ascent in the KL divergence.<br />Notice the interesting relationship of this algorithm to the Gibbs sampler. In Gibbs<br />sampling, we iteratively draw the latent variables wifrom the distribution p(wi|w−i,x,θ).<br />In mean-field variational inference, we iteratively update the variational parameter νi<br />by setting it equal to the expected value of gi(w−i,x,θ). This expectation is computed<br />under the variational distribution.<br />3.2 DP mixtures<br />In this section we develop a mean-field variational algorithm for the DP mixture. Our<br />algorithm is based on the stick-breaking representation of the DP mixture (see Figure 1).<br />In this representation the latent variables are the stick lengths, the atoms, and the cluster<br />assignments: W = {V,η∗,Z}. The hyperparameters are the scaling parameter and the<br />parameter of the conjugate base distribution: θ = {α,λ}.<br />Following the general recipe in Equation (12), we write the variational bound on the<br />2Examples of models in which p(wi|w−i,x,θ) is an exponential family distribution include hidden<br />Markov models, mixture models, state space models, and hierarchical Bayesian models with conjugate<br />and mixture of conjugate priors.</p>  <p>Page 8</p> <p>128<br />Variational inference for Dirichlet process mixtures<br />log marginal probability of the data:<br />logp(x|α,λ) ≥ Eq[logp(V|α)] + Eq[logp(η∗|λ)]<br />+<br />N<br />?<br />n=1<br />(Eq[logp(Zn|V)] + Eq[logp(xn|Zn)])<br />− Eq[logq(V,η∗,Z)].<br />(16)<br />To exploit this bound, we must find a family of variational distributions that approxi-<br />mates the distribution of the infinite-dimensional random measure G, where the random<br />measure is expressed in terms of the infinite sets V = {V1,V2,...} and η∗= {η∗<br />We do this by considering truncated stick-breaking representations. Thus, we fix a value<br />T and let q(vT = 1) = 1; this implies that the mixture proportions πt(v) are equal to<br />zero for t &gt; T (see Equation 5).<br />1,η∗<br />2,...}.<br />Truncated stick-breaking representations have been considered previously by<br />Ishwaran and James (2001) in the context of sampling-based inference for an approxi-<br />mation to the DP mixture model. Note that our use of truncation is rather different. In<br />our case, the model is a full Dirichlet process and is not truncated; only the variational<br />distribution is truncated. The truncation level T is a variational parameter which can<br />be freely set; it is not a part of the prior model specification (see Section 5).<br />We thus propose the following factorized family of variational distributions for mean-<br />field variational inference:<br />q(v,η∗,z) =<br />T−1<br />?<br />t=1<br />qγt(vt)<br />T?<br />t=1<br />qτt(η∗<br />t)<br />N<br />?<br />n=1<br />qφn(zn) (17)<br />where qγt(vt) are beta distributions, qτt(η∗<br />natural parameters τt, and qφn(zn) are multinomial distributions. In the notation of<br />Section 3.1, the free variational parameters are<br />t) are exponential family distributions with<br />ν = {γ1,...,γT−1,τ1,...,τT,φ1,...,φN}.<br />It is important to note that there is a different variational parameter for each latent<br />variable under the variational distribution. For example, the choice of the mixture com-<br />ponent znfor the nth data point is governed by a multinomial distribution indexed by<br />a variational parameter φn. This reflects the conditional nature of variational inference.<br />Coordinate ascent algorithm<br />In this section we present an explicit coordinate ascent algorithm for optimizing the<br />bound in Equation (16) with respect to the variational parameters.<br />All of the terms in the bound involve standard computations in the exponential<br />family, except for the third term. We rewrite the third term using indicator random</p>  <p>Page 9</p> <p>D. M. Blei and M. I. Jordan129<br />variables:<br />Eq[logp(Zn|V)]=Eq<br />?<br />i=1q(zn&gt; i)Eq[log(1 − Vi)] + q(zn= i)Eq[logVi].<br />log<br />??∞<br />i=1(1 − Vi)1[Zn&gt;i]V1[Zn=i]<br />i<br />??<br />=<br />?∞<br />Recall that Eq[log(1 − VT)] = 0 and q(zn&gt; T) = 0. Consequently, we can truncate this<br />summation at t = T:<br />Eq[logp(Zn|V)] =<br />T<br />?<br />i=1<br />q(zn&gt; i)Eq[log(1 − Vi)] + q(zn= i)Eq[logVi],<br />where<br />q(zn= i)=φn,i<br />?T<br />Ψ(γi,1) − Ψ(γi,1+ γi,2)<br />Ψ(γi,2) − Ψ(γi,1+ γi,2).<br />q(zn&gt; i)<br />Eq[logVi]<br />=<br />=<br />j=i+1φn,j<br />Eq[log(1 − Vi)]=<br />The digamma function, denoted by Ψ, arises from the derivative of the log normalization<br />factor in the beta distribution.<br />We now use the general expression in Equation (15) to derive a mean-field coordinate<br />ascent algorithm. This yields:<br />γt,1<br />= 1 +?<br />α +?<br />λ1+?<br />λ2+?<br />exp(St),<br />nφn,t<br />?T<br />nφn,t.<br />(18)<br />γt,2<br />τt,1<br />τt,2<br />φn,t<br />=<br />=<br />n<br />j=t+1φn,j<br />nφn,txn<br />(19)<br />(20)<br />=<br />∝<br />(21)<br />(22)<br />for t ∈ {1,...,T} and n ∈ {1,...,N}, where<br />St= Eq[logVt] +?t−1<br />Iterating these updates optimizes Equation (16) with respect to the variational param-<br />eters defined in Equation (17).<br />i=1Eq[log(1 − Vi)] + Eq[η∗<br />t]TXn− Eq[a(η∗<br />t)].<br />Practical applications of variational methods must address initialization of the vari-<br />ational distribution. While the algorithm yields a bound for any starting values of the<br />variational parameters, poor choices of initialization can lead to local maxima that yield<br />poor bounds. We initialize the variational distribution by incrementally updating the<br />parameters according to a random permutation of the data points. (This can be viewed<br />as a variational version of sequential importance sampling). We run the algorithm mul-<br />tiple times and choose the final parameter settings that give the best bound on the<br />marginal likelihood.</p>  <p>Page 10</p> <p>130<br />Variational inference for Dirichlet process mixtures<br />To compute the predictive distribution, we use the variational posterior in a manner<br />analogous to the way that the empirical approximation is used by an MCMC sampling<br />algorithm. The predictive distribution is:<br />p(xN+1|x,α,λ) =<br />??∞<br />?<br />t=1<br />πt(v)p(xN+1|η∗<br />t)<br />?<br />dP(v,η∗|x,λ,α).<br />Under the factorized variational approximation to the posterior, the distribution of<br />the atoms and the stick lengths are decoupled and the infinite sum is truncated. Conse-<br />quently, we can approximate the predictive distribution with a product of expectations<br />which are straightforward to compute under the variational approximation,<br />p(xN+1|x,α,λ) ≈<br />T<br />?<br />t=1<br />Eq[πt(V)]Eq[p(xN+1|η∗<br />t)], (23)<br />where q depends implicitly on x, α, and λ.<br />Finally, we remark on two possible extensions. First, when G0is not conjugate, a<br />simple coordinate ascent update for τimay not be available, particularly when p(η∗<br />is not in the exponential family. However, such an update is available for the special<br />case of G0being a mixture of conjugate distributions. Second, it is often important in<br />applications to integrate over a diffuse prior on the scaling parameter α. As we show<br />in Appendix 7, it is straightforward to extend the variational algorithm to include a<br />gamma prior on α.<br />i|z,x,λ)<br />4 Gibbs sampling<br />For comparison to variational inference, we review the collapsed Gibbs sampler and<br />blocked Gibbs sampler for DP mixtures.<br />4.1 Collapsed Gibbs sampling<br />The collapsed Gibbs sampler for a DP mixture with conjugate base distribution<br />(MacEachern 1994) integrates out the random measure G and distinct parameter val-<br />ues {η∗<br />|c|}. The Markov chain is thus defined only on the latent partition<br />c = {c1,...,cN}. (Recall that |c| denotes the number of cells in the partition.)<br />1,...,η∗<br />The algorithm iteratively samples each assignment variable Cn, for n ∈ {1,...,N},<br />conditional on the other cells in the partition, c−n. The assignment Cncan be one of<br />|c−n| + 1 values: either the nth data point is in a cell with other data points, or in a<br />cell by itself.<br />Exchangeability implies that Cnhas the following multinomial distribution:<br />p(cn= k|x,c−n,λ,α) ∝ p(xn|x−n,c−n,cn= k,λ)p(cn= k|c−n,α). (24)</p>  <p>Page 11</p> <p>D. M. Blei and M. I. Jordan131<br />The first term is a ratio of normalizing constants of the posterior distribution of the kth<br />parameter, one including and one excluding the nth data point:<br />p(xn|x−n,c−n,cn= k,λ) =<br />?<br />exp<br />?<br />The second term is given by the P´ olya urn scheme:<br />expa(λ1+?<br />a(λ1+?<br />m?=n1[cm= k]xm+ xn,λ2+?<br />m?=n1[cm= k]xm,λ2+?<br />m?=n1[cm= k] + 1)<br />?<br />m?=n1[cm= k])<br />?<br />.<br />(25)<br />p(cn= k|c−n) ∝<br />?<br />|{j : c−n,j= k}|<br />α<br />if k is an existing cell in the partition<br />if k is a new cell in the partition,<br />(26)<br />where |{j : c−n,j= k}| denotes the number of data points in the kth cell of the partition<br />c−n.<br />Once this chain has reached its stationary distribution, we collect B samples {c1,...,cB}<br />to approximate the posterior. The approximate predictive distribution is an average of<br />the predictive distributions across the Monte Carlo samples:<br />p(xN+1|x1,...,xN,α,λ) =1<br />B<br />B<br />?<br />b=1<br />p(xN+1|cb,x,α,λ).<br />For a given sample, that distribution is<br />p(xN+1|cb,x,α,λ) =<br />|cb|+1<br />?<br />k=1<br />p(cN+1= k|cb,α)p(xN+1|cb,x,cN+1= k,λ).<br />When G0is not conjugate, the distribution in Equation (25) does not have a simple<br />closed form. Effective algorithms for handling this case are given in Neal (2000).<br />4.2 Blocked Gibbs sampling<br />In the collapsed Gibbs sampler, the assignment variable Cnis drawn from a distribu-<br />tion that depends on the most recently sampled values of the other assignment vari-<br />ables. Consequently, these variables must be updated one at a time which can poten-<br />tially slow down the algorithm when compared to a blocking strategy. To this end,<br />Ishwaran and James (2001) developed a blocked Gibbs sampling algorithm based on<br />the stick-breaking representation of Figure 1.<br />The main issue to face in developing a blocked Gibbs sampler for the stick-breaking<br />DP mixture is that one needs to sample the infinite collection of stick lengths V before<br />sampling the finite collection of cluster assignments Z. Ishwaran and James (2001) face<br />this issue by defining a truncated Dirichlet process (TDP) in which VK−1is set equal to<br />one for some fixed value K. This yields πi(V) = 0 for i ≥ K, and converts the infinite<br />sum in Equation (5) into a finite sum. Ishwaran and James (2001) justify substituting a</p>  <p>Page 12</p> <p>132<br />Variational inference for Dirichlet process mixtures<br />TDP mixture model for a full DP mixture model by showing that the truncated process<br />closely approximates a true Dirichlet process when the truncation level is chosen large<br />relative to the number of data points.<br />In the TDP mixture, the state of the Markov chain consists of the beta variables<br />V = {V1,...,VK−1}, the mixture component parameters η∗= {η∗<br />indicator variables Z = {Z1,...,ZN}. The blocked Gibbs sampler iterates between the<br />following three steps:<br />1,...,η∗<br />K}, and the<br />1. For n ∈ {1,...,N}, independently sample Znfrom<br />p(zn= k|v,η∗,x) = πk(v)p(xn|η∗<br />k),<br />2. For k ∈ {1,...,K}, independently sample Vkfrom Beta(γk,1,γk,2), where<br />γk,1<br />= 1 +?N<br />α +?K<br />n=11[zn= k]<br />γk,2<br />=<br />i=k+1<br />?N<br />n=11[zn= i].<br />This step follows from the conjugacy between the multinomial distribution and<br />the truncated stick-breaking construction, which is a generalized Dirichlet distri-<br />bution (Connor and Mosimann 1969).<br />3. For k ∈ {1,...,K}, independently sample η∗<br />in the same family as the base distribution, with parameters<br />kfrom p(η∗<br />k|τk). This distribution is<br />τk,1<br />τk,2<br />=<br />=<br />λ1+?<br />i?=n1[zi= k]xi<br />i?=n1[zi= k].λ2+?<br />(27)<br />After the chain has reached its stationary distribution, we collect B samples and<br />construct an approximate predictive distribution. Again, this distribution is an aver-<br />age of the predictive distributions for each of the collected samples. The predictive<br />distribution for a particular sample is<br />p(xN+1|z,x,α,λ) =<br />K<br />?<br />k=1<br />E[πi(V)|γ1,...,γk]p(xN+1|τk),(28)<br />where E[πi|γ1,...,γk] is the expectation of the product of independent beta variables<br />given in Equation (5). This distribution only depends on z; the other variables are<br />needed in the Gibbs sampling procedure, but can be integrated out here. Note that this<br />approximation has a form similar to the approximate predictive distribution under the<br />variational distribution in Equation (23). In the variational case, however, the averaging<br />is done parametrically via the variational distribution rather than by a Monte Carlo<br />integral.<br />The TDP sampler readily handles non-conjugacy of G0, provided that there is a<br />method of sampling η∗<br />ifrom its posterior.</p>  <p>Page 13</p> <p>D. M. Blei and M. I. Jordan133<br />−40<br />−20<br />0<br />20<br />40<br />60<br />−20−100 1020<br />−40<br />−20<br />0<br />20<br />40<br />60<br />initial<br />−40<br />−20<br />0<br />20<br />40<br />60<br />−20−1001020<br />−40<br />−20<br />0<br />20<br />40<br />60<br />iteration 2<br />−40<br />−20<br />0<br />20<br />40<br />60<br />−20−10 010 20<br />−40<br />−20<br />0<br />20<br />40<br />60<br />iteration 5<br />Figure 2: The approximate predictive distribution given by variational inference at<br />different stages of the algorithm. The data are 100 points generated by a Gaussian DP<br />mixture model with fixed diagonal covariance.<br />5Empirical comparison<br />Qualitatively, variational methods offer several potential advantages over Gibbs sam-<br />pling.They are deterministic, and have an optimization criterion given by Equa-<br />tion (16) that can be used to assess convergence. In contrast, assessing convergence<br />of a Gibbs sampler—namely, determining when the Markov chain has reached its sta-<br />tionary distribution—is an active field of research. Theoretical bounds on the mixing<br />time are of little practical use, and there is no consensus on how to choose among the<br />several empirical methods developed for this purpose (Robert and Casella 2004).<br />But there are several potential disadvantages of variational methods as well. First,<br />the optimization procedure can fall prey to local maxima in the variational parameter<br />space. Local maxima can be mitigated with restarts, or removed via the incorporation<br />of additional variational parameters, but these strategies may slow the overall conver-<br />gence of the procedure. Second, any given fixed variational representation yields only<br />an approximation to the posterior. There are methods for considering hierarchies of<br />variational representations that approach the posterior in the limit, but these methods<br />may again incur serious computational costs. Lacking a theory by which these issues can<br />be evaluated in the general setting of DP mixtures, we turn to experimental evaluation.<br />We studied the performance of the variational algorithm of Section 3 and the Gibbs<br />samplers of Section 4 in the setting of DP mixtures of Gaussians with fixed inverse<br />covariance matrix Λ (i.e., the DP mixes over the mean of the Gaussian). The natural<br />conjugate base distribution for the DP is Gaussian, with covariance given by Λ/λ2(see<br />Equation 7).<br />Figure 2 provides an illustrative example of variational inference on a small problem<br />involving 100 data points sampled from a two-dimensional DP mixture of Gaussians<br />with diagonal covariance. Each panel in the figure plots the data and presents the</p>  <p>Page 14</p> <p>134<br />Variational inference for Dirichlet process mixtures<br />Figure 3: Mean convergence time and standard error across ten data sets per dimension<br />for variational inference, TDP Gibbs sampling, and the collapsed Gibbs sampler.<br />predictive distribution given by the variational inference algorithm at a given iteration<br />(see Equation (23)). The truncation level was set to 20. As seen in the first panel, the<br />initialization of the variational parameters yields a largely flat distribution. After one<br />iteration, the algorithm has found the modes of the predictive distribution and, after<br />convergence, it has further refined those modes. Even though 20 mixture components<br />are represented in the variational distribution, the fitted approximate posterior only<br />uses five of them.<br />To compare the variational inference algorithm to the Gibbs sampling algorithms, we<br />conducted a systematic set of simulation experiments in which the dimensionality of the<br />data was varied from 5 to 50. The covariance matrix was given by the autocorrelation<br />matrix for a first-order autoregressive process, chosen so that the components are highly<br />dependent (ρ = 0.9). The base distribution was a zero-mean Gaussian with covariance<br />appropriately scaled for comparison across dimensions. The scaling parameter α was<br />set equal to one.<br />In each case, we generated 100 data points from a DP mixture of Gaussians model<br />of the chosen dimensionality and generated 100 additional points as held-out data. In<br />testing on the held-out data, we treated each point as the 101st data point in the<br />collection and computed its conditional probability using each algorithm’s approximate<br />predictive distribution.</p>  <p>Page 15</p> <p>D. M. Blei and M. I. Jordan135<br />DimMean held out log probability (Std err)<br />Variational Collapsed Gibbs<br />-147.96 (4.12)-148.08 (3.93)<br />-266.59 (7.69)-266.29 (7.64)<br />-494.12 (7.31)-492.32 (7.54)<br />-721.55 (8.18)-720.05 (7.92)<br />-943.39 (10.65) -941.04 (10.15)<br />-1151.01 (15.23)-1148.51 (14.78)<br />Truncated Gibbs<br />-147.93 (3.88)<br />-265.89 (7.66)<br />-491.96 (7.59)<br />-720.02 (7.96)<br />-940.71 (10.23)<br />-1147.48 (14.55)<br />5<br />10<br />20<br />30<br />40<br />50<br />Table 1: Average held-out log probability for the predictive distributions given by vari-<br />ational inference, TDP Gibbs sampling, and the collapsed Gibbs sampler.<br />246810<br />−1420<br />−1380<br />−1340<br />−1300<br />Truncation level<br />Log marginal probability bound<br />010203040506070<br />−380<br />−360<br />−340<br />−320<br />−300<br />Iteration<br />Held−out score<br />2.88e−15<br />1.46e−10<br />9.81e−05<br />Figure 4: The optimal bound on the log probability as a function of the truncation<br />level (left). There are five clusters in the simulated 20-dimensional DP mixture of<br />Gaussians data set which was used. Held-out probability as a function of iteration of<br />variational inference for the same simulated data set (right). The relative change in the<br />log probability bound of the observations is labeled at selected iterations.</p>  <p>Page 16</p> <p>136<br />Variational inference for Dirichlet process mixtures<br />0 50 100150<br />0.0<br />0.4<br />0.8<br />Lag<br />Autocorrelation<br />050100150<br />0.0<br />0.4<br />0.8<br />Lag<br />Autocorrelation<br />Figure 5: Autocorrelation plots on the size of the largest component for the truncated<br />DP Gibbs sampler (left) and collapsed Gibbs sampler (right) in an example dataset of<br />50-dimensional Gaussian data.<br />The TDP approximation was truncated at K = 20 components. For the variational<br />algorithm, the truncation level was also T = 20 components. Note that in the latter<br />case, the truncation level is simply another variational parameter. While we held T fixed<br />in our simulations, it is also possible to optimize T with respect to the KL divergence.<br />Indeed, Figure 4 (left) shows how the optimal KL divergence changes as a function of<br />the truncation level for one of the simulated data sets.<br />We ran all algorithms to convergence and measured the computation time.3For the<br />collapsed Gibbs sampler, we assessed convergence to the stationary distribution with<br />the diagnostic given by Raftery and Lewis (1992), and collected 25 additional samples<br />to estimate the predictive distribution (the same diagnostic provides an appropriate<br />lag at which to collect uncorrelated samples). We assessed convergence of the blocked<br />Gibbs sampler using the same statistic as for the collapsed Gibbs sampler and used the<br />same number of samples to form the approximate predictive distribution.4<br />Finally, for variational inference, we measured convergence using the relative change<br />in the log marginal probability bound (Equation 16), stopping the algorithm when it<br />was less than 1e−10.<br />There is a certain inevitable arbitrariness in these choices; in general it is difficult<br />to envisage measures of computation time that allow stochastic MCMC algorithms and<br />deterministic variational algorithms to be compared in a standardized way. Nonetheless,<br />we have made what we consider to be reasonable, pragmatic choices. In particular, our<br />choice of stopping time for the variational algorithm is quite conservative, as illustrated<br />in Figure 4 (right).<br />Figure 3 illustrates the average convergence time across ten datasets per dimension.<br />With the caveats in mind regarding convergence time measurement, it appears that the<br />variational algorithm is quite competitive with the MCMC algorithms. The variational<br />3All timing computations were made on a Pentium III 1GHZ desktop machine.<br />4Typically, hundreds or thousands of samples are used in MCMC algorithms to form the approxi-<br />mate posterior. However, we found that such approximations did not offer any additional predictive<br />performance in the simulated data. To be fair to MCMC in the timing comparisons, we used a small<br />number of samples to estimate the predictive distributions.</p>  <p>Page 17</p> <p>D. M. Blei and M. I. Jordan 137<br />Figure 6: Four sample clusters from a DP mixture analysis of 5000 images from the<br />Associated Press. The left-most column is the posterior mean of each cluster followed<br />by the top ten images associated with it. These clusters capture patterns in the data,<br />such as basketball shots, outdoor scenes on gray days, faces, and pictures with blue<br />backgrounds.<br />algorithm was faster and exhibited significantly less variance in its convergence time.<br />Moreover, there is little evidence of an increase in convergence time across dimensionality<br />for the variational algorithm over the range tested.<br />Note that the collapsed Gibbs sampler converged faster than the TDP Gibbs sampler.<br />Though an iteration of collapsed Gibbs is slower than an iteration of TDP Gibbs, the<br />TDP Gibbs sampler required a longer burn-in and greater lag to obtain uncorrelated<br />samples. This is illustrated in the autocorrelation plots of Figure 5. Comparing the two<br />MCMC algorithms, we found no advantage to the truncated approximation.<br />Table 1 illustrates the average log likelihood assigned to the held-out data by the<br />approximate predictive distributions. First, notice that the collapsed DP Gibbs sam-<br />pler assigned the same likelihood as the posterior from the TDP Gibbs sampler—an<br />indication of the quality of a TDP for approximating a DP. More importantly, however,<br />the predictive distribution based on the variational posterior assigned a similar score as<br />those based on samples from the true posterior. Though it is based on an approximation<br />to the posterior, the resulting predictive distributions are very accurate for this class of<br />DP mixtures.<br />6Image analysis<br />Finite Gaussian mixture models are widely used in computer vision to model natural im-<br />ages for the purposes of automatic clustering, retrieval, and classification (Barnard et al.<br />2003; Jeon et al. 2003). These applications are often large-scale data analysis problems,<br />involving thousands of data points (images) in hundreds of dimensions (pixels). The ap-</p>  <p>Page 18</p> <p>138<br />Variational inference for Dirichlet process mixtures<br />051015<br />α<br />2025 30<br />0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0<br />0<br />20<br />40<br />60<br />80<br />100<br />120<br />prior<br />posterior<br />Factor<br />Expected number of images<br />Figure 7: The expected number of images allocated to each component in the variational<br />posterior (left). The posterior uses 79 components to describe the data. The prior for the<br />scaling parameter α and the approximate posterior given by its variational distribution<br />(right).<br />propriate number of mixture components to use in these problems is generally unknown,<br />and DP mixtures provide an attractive alternative to current methods. However, a de-<br />ployment of DP mixtures in such problems crucially requires inferential methods that<br />are computationally efficient. To demonstrate the applicability of our variational ap-<br />proach to DP mixtures in the setting of large datasets, we analyzed a collection of 5000<br />images from the Associated Press under the assumptions of a DP mixture of Gaussians<br />model.<br />Each image was reduced to a 192-dimensional real-valued vector given by an 8×8 grid<br />of average red, green, and blue values. We fit a DP mixture model in which the mixture<br />components are Gaussian with mean µ and covariance matrix σ2I. The base distribution<br />G0was a product measure—Gamma(4,2) for 1/σ2and N(0,5σ2) for µ. Furthermore, we<br />placed a Gamma(1,1) prior on the DP scaling parameter α, as described in Appendix 7.<br />We used a truncation level of 150 for the variational distribution.<br />The variational algorithm required approximately four hours to converge. The re-<br />sulting approximate posterior used 79 mixture components to describe the collection.<br />For a rough comparison to Gibbs sampling, an iteration of collapsed Gibbs takes 15<br />minutes with this data set. In the same four hours, one could perform only 16 itera-<br />tions. This is not enough for a chain to converge to its stationary distribution, let alone<br />provide a sufficient number of uncorrelated samples to construct an empirical estimate<br />of the posterior.<br />Figure 7 (left) illustrates the expected number of images allocated to each compo-<br />nent under the variational approximation to the posterior. Figure 6 illustrates the ten<br />pictures with highest approximate posterior probability associated with each of four of<br />the components. These clusters appear to capture basketball shots, outdoor scenes on<br />gray days, faces, and blue backgrounds.<br />Figure 7 (right) illustrates the prior for the scaling parameter α as well as the<br />approximate posterior given by the fitted variational distribution. We see that the</p>  <p>Page 19</p> <p>D. M. Blei and M. I. Jordan139<br />approximate posterior is peaked and rather different from the prior, indicating that the<br />data have provided information regarding α.<br />7 Conclusions<br />We have developed a mean-field variational inference algorithm for the Dirichlet pro-<br />cess mixture model and demonstrated its applicability to the kinds of multivariate data<br />for which Gibbs sampling algorithms can exhibit slow convergence. Variational infer-<br />ence was faster than Gibbs sampling in our simulations, and its convergence time was<br />independent of dimensionality for the range which we tested.<br />Both variational and MCMC methods have strengths and weaknesses, and it is un-<br />likely that one methodology will dominate the other in general. While MCMC sampling<br />provides theoretical guarantees of accuracy, variational inference provides a fast, deter-<br />ministic approximation to otherwise unattainable posteriors. Moreover, both MCMC<br />and variational methods are computational paradigms, providing a wide variety of spe-<br />cific algorithmic approaches which trade off speed, accuracy and ease of implementation<br />in different ways. We have investigated the deployment of the simplest form of varia-<br />tional method for DP mixtures—a mean-field variational algorithm—but it worth noting<br />that other variational approaches, such as those described in Wainwright and Jordan<br />(2003), are also worthy of consideration in the nonparametric context.<br />Appendix-A Variational inference in exponential families<br />In this appendix, we derive the coordinate ascent algorithm for variational inference<br />described in Section 3.2. Recall that we are considering a latent variable model with<br />hyperparameters θ, observed variables x = {x1,...,xN}, and latent variables W =<br />{W1,...,WM}. The posterior can be written as<br />p(w|x,θ) = exp{logp(w,x|θ) − logp(x|θ)}.(29)<br />The variational bound on the log marginal probability is<br />logp(x|θ) ≥ Eq[logp(x,W|θ)] − Eq[logq(W)]. (30)<br />This bound holds for any distribution q(w).<br />For the optimization of this bound to be computationally tractable, we restrict our-<br />selves to fully-factorized variational distributions of the form qν(w) =?M<br />exponential family (Ghahramani and Beal 2001). We derive a coordinate ascent algo-<br />rithm in which we iteratively maximize the bound with respect to each νi, holding the<br />other variational parameters fixed.<br />i=1qνi(wi),<br />where ν = {ν1,ν2,...,νM} are variational parameters and each distribution is in the</p>  <p>Page 20</p> <p>140<br />Variational inference for Dirichlet process mixtures<br />Let us rewrite the bound in Equation (30) using the chain rule:<br />logp(x|θ) ≥ logp(x|θ)+<br />M<br />?<br />m=1<br />Eq[logp(Wm|x,W1,...,Wm−1,θ)]−<br />M<br />?<br />m=1<br />Eq[logqνm(Wm)].<br />(31)<br />To optimize with respect to νi, reorder w such that wiis last in the list. The portion<br />of Equation (31) depending on νiis<br />?i= Eq[logp(Wi|W−i,x,θ)] − Eq[logqνi(Wi)].(32)<br />The variational distribution qνi(wi) is in the exponential family,<br />qνi(wi) = h(wi)exp{νT<br />iwi− a(νi)},<br />and Equation (32) simplifies as follows:<br />?i<br />=Eq<br />Eq[logp(Wi|W−i,x,θ)] − Eq[logh(Wi)] − νT<br />?logp(Wi|W−i,x,θ) − logh(Wi) − νT<br />iWi+ a(νi)?<br />=<br />ia?(νi) + a(νi),<br />because Eq[Wi] = a?(νi).<br />The derivative with respect to νiis<br />∂<br />∂νi?i=<br />∂<br />∂νi<br />(Eq[logp(Wi|W−i,x,θ)] − Eq[logh(Wi)]) − νT<br />ia??(νi).(33)<br />The optimal νisatisfies<br />νi= [a??(νi)]−1<br />?<br />∂<br />∂νiEq[logp(Wi|W−i,x,θ)] −<br />∂<br />∂νiEq[logh(Wi)]<br />?<br />.(34)<br />The result in Equation (34) is general. In many applications of mean field methods,<br />including those in the current paper, a further simplification is achieved. In particular,<br />if the conditional distribution p(wi|w−i,x,θ) is an exponential family distribution then<br />p(wi|w−i,x,θ) = h(wi)exp{gi(w−i,x,θ)Twi− a(gi(w−i,x,θ))},<br />where gi(w−i,x,θ) denotes the natural parameter for wi when conditioning on the<br />remaining latent variables and the observations. This yields simplified expressions for<br />the expected log probability of Wiand its first derivative:<br />Eq[logp(Wi|W−i,x,θ)]=Eq[logh(Wi)] + Eq[gi(W−i,x,θ)]Ta?(νi) − Eq[a(gi(W−i,x,θ))]<br />∂<br />∂νiEq[logh(Wi)] + Eq[gi(W−i,x,θ)]Ta??(νi).<br />∂<br />∂νiEq[logp(Wi|W−i,x,θ)]=<br />Using the first derivative in Equation (34), the maximum is attained at<br />νi= Eq[gi(W−i,x,θ)].(35)</p>  <p>Page 21</p> <p>D. M. Blei and M. I. Jordan141<br />We define a coordinate ascent algorithm based on Equation (35) by iteratively updating<br />νifor i ∈ {1,...,M}. Such an algorithm finds a local maximum of Equation (30) by<br />Proposition 2.7.1 of Bertsekas (1999), under the condition that the right-hand side of<br />Equation (32) is strictly convex.<br />Relaxing the two assumptions complicates the algorithm, but the basic idea re-<br />mains the same. If p(wi|w−i,x,θ) is not in the exponential family, then there may<br />not be an analytic expression for the update in Equation (34). If q(w) is not a fully<br />factorized distribution, then the second term of the bound in Equation (32) becomes<br />Eq[logq(wi|w−i)] and the subsequent simplifications may not be applicable.<br />Further perspectives on algorithms of this kind can be found in Xing et al. (2003),<br />Ghahramani and Beal (2001), and Wiegerinck (2000). For a more general treatment of<br />variational methods for statistical inference, see Wainwright and Jordan (2003).<br />Appendix-BPlacing a prior on the scaling parameter<br />The scaling parameter α can have a significant effect on the growth of the number of<br />components grows with the data, and it is generally important to consider extended<br />models which integrate over α. For the urn-based samplers, Escobar and West (1995)<br />place a Gamma(s1,s2) prior on α and implement the corresponding Gibbs updates with<br />auxiliary variable methods.<br />In the stick-breaking representation, the gamma distribution is convenient because<br />it is conjugate to the stick lengths. We write the gamma distribution in its canonical<br />form:<br />p(α|s1,s2) = (1/α)exp{−s2α + s1logα − a(s1,s2)},<br />where s1is the shape parameter and s2is the inverse scale parameter. This distribution<br />is conjugate to Beta(1,α). The log normalizer is<br />a(s1,s2) = logΓ(s1) − s1logs2,<br />and the posterior parameters conditional on data {v1,...,vK} are<br />ˆ s2<br />ˆ s1<br />=<br />=<br />s2−?K<br />s1+ K.<br />i=1log(1 − vi)<br />We extend the variational inference algorithm to include posterior updates for the<br />scaling parameter α. The variational distribution is Gamma(w1,w2). The variational<br />parameters are updated as follows:<br />w1<br />=s1+ T − 1<br />T−1<br />?<br />w2<br />=s2−<br />i=1<br />Eq[log(1 − Vi)]),<br />and we replace α with its expectation Eq[α] = w1/w2in the updates for γt,2in Equa-<br />tion (19).</p>  <p>Page 22</p> <p>142<br />Variational inference for Dirichlet process mixtures<br />Bibliography<br />Antoniak, C. (1974). “Mixtures of Dirichlet processes with applications to Bayesian<br />nonparametric problems.” The Annals of Statistics, 2(6):1152–1174. 122, 124<br />Attias, H. (2000). “A variational Bayesian framework for graphical models.” In Solla, S.,<br />Leen, T., and Muller, K. (eds.), Advances in Neural Information Processing Systems<br />12, 209–215. Cambridge, MA: MIT Press. 121<br />Barnard, K., Duygulu, P., de Freitas, N., Forsyth, D., Blei, D., and Jordan, M. (2003).<br />“Matching words and pictures.” Journal of Machine Learning Research, 3:1107–1135.<br />137<br />Bertsekas, D. (1999). Nonlinear Programming. Nashua, NH: Athena Scientific. 141<br />Blackwell, D. and MacQueen, J. (1973). “Ferguson distributions via P´ olya urn schemes.”<br />The Annals of Statistics, 1(2):353–355. 122, 123<br />Blei, D., Ng, A., and Jordan, M. (2003). “Latent Dirichlet allocation.” Journal of<br />Machine Learning Research, 3:993–1022. 121<br />Connor, R. and Mosimann, J. (1969). “Concepts of independence for proportions with<br />a generalization of the Dirichlet distribution.” Journal of the American Statistical<br />Association, 64(325):194–206. 132<br />Escobar, M. and West, M. (1995). “Bayesian density estimation and inference using<br />mixtures.” Journal of the American Statistical Association, 90:577–588.<br />141<br />122, 125,<br />Ferguson, T. (1973). “A Bayesian analysis of some nonparametric problems.” The<br />Annals of Statistics, 1:209–230. 122, 123, 124<br />Ghahramani, Z. and Beal, M. (2001). “Propagation algorithms for variational Bayesian<br />learning.” In Leen, T., Dietterich, T., and Tresp, V. (eds.), Advances in Neural<br />Information Processing Systems 13, 507–513. Cambridge, MA: MIT Press. 121, 122,<br />127, 139, 141<br />Ishwaran, J. and James, L. (2001). “Gibbs sampling methods for stick-breaking priors.”<br />Journal of the American Statistical Association, 96:161–174. 125, 128, 131<br />Jeon, J., Lavrenko, V., and Manmatha, R. (2003). “Automatic image annotation and<br />retrieval using cross-media relevance models.” In Proceedings of the 26th Annual<br />International ACM SIGIR conference on Research and Development in Information<br />Retrieval, 119–126. ACM Press. 137<br />Jordan, M., Ghahramani, Z., Jaakkola, T., and Saul, L. (1999). “Introduction to vari-<br />ational methods for graphical models.” Machine Learning, 37:183–233. 121<br />MacEachern, S. (1994). “Estimating normal means with a conjugate style Dirichlet<br />process prior.” Communications in Statistics B, 23:727–741. 122, 125, 130</p>  <p>Page 23</p> <p>D. M. Blei and M. I. Jordan143<br />— (1998). “Computational methods for mixture of Dirichlet process models.” In Dey,<br />D., Muller, P., and Sinha, D. (eds.), Practical Nonparametric and Semiparametric<br />Bayesian Statistics, 23–44. Springer. 125<br />Neal, R. (2000). “Markov chain sampling methods for Dirichlet process mixture models.”<br />Journal of Computational and Graphical Statistics, 9(2):249–265. 122, 125, 131<br />Opper, M. and Saad, D. (2001). Advanced Mean Field Methods: Theory and Practice.<br />Cambridge, MA: MIT Press. 121<br />Raftery, A. and Lewis, S. (1992). “One long run with diagnostics: Implementation<br />strategies for Markov chain Monte Carlo.” Statistical Science, 7:493–497. 136<br />Robert, C. and Casella, G. (2004). Monte Carlo Statistical Methods. New York, NY:<br />Springer-Verlag. 133<br />Sethuraman, J. (1994). “A constructive definition of Dirichlet priors.” Statistica Sinica,<br />4:639–650. 122, 124<br />Wainwright, M. and Jordan, M. (2003). “Graphical models, exponential families, and<br />variational inference.” Technical Report 649, U.C. Berkeley, Dept. of Statistics. 121,<br />122, 125, 126, 139, 141<br />Wiegerinck, W. (2000). “Variational approximations between mean field theory and the<br />junction tree algorithm.” In Boutilier, C. and Goldszmidt, M. (eds.), Proceedings<br />of the 16th Annual Conference on Uncertainty in Artificial Intelligence (UAI-00),<br />626–633. San Francisco, CA: Morgan Kaufmann Publishers. 121, 141<br />Xing, E., Jordan, M., and Russell, S. (2003). “A generalized mean field algorithm for<br />variational inference in exponential families.” In Meek, C. and Kjærulff, U. (eds.),<br />Proceedings of the 19th Annual Conference on Uncertainty in Artificial Intelligence<br />(UAI-03), 583–591. San Francisco, CA: Morgan Kaufmann Publishers. 141<br />Acknowledgments<br />We thank Jaety Edwards for providing the AP image data. We want to acknowledge support<br />from Intel Corporation, Microsoft Research, and a grant from DARPA in support of the CALO<br />project.</p>  <p>Page 24</p> <p>144<br />Variational inference for Dirichlet process mixtures</p>  <a href="https://www.researchgate.net/profile/Michael_Jordan13/publication/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144/links/53d6746a0cf2a7fbb2eaa5f3.pdf">Download full-text</a> </div> <div id="rgw18_56ab9d690670d" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw19_56ab9d690670d">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw20_56ab9d690670d"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Michael_Jordan13/publication/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144/links/53d6746a0cf2a7fbb2eaa5f3.pdf" class="publication-viewer" title="53d6746a0cf2a7fbb2eaa5f3.pdf">53d6746a0cf2a7fbb2eaa5f3.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Michael_Jordan13">Michael Jordan</a> &middot; Jul 28, 2014 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw21_56ab9d690670d"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://projecteuclid.org//DPubS/Repository/1.0/Disseminate?view=body&amp;id=pdf_1&amp;handle=euclid.ba/1340371077" target="_blank" rel="nofollow" class="publication-viewer" title="Variational inference for Dirichlet process mixtures. Bayesian Anal 1:121-144">Variational inference for Dirichlet process mixtur...</a> </div>  <div class="details">   Available from <a href="http://projecteuclid.org//DPubS/Repository/1.0/Disseminate?view=body&amp;id=pdf_1&amp;handle=euclid.ba/1340371077" target="_blank" rel="nofollow">projecteuclid.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw28_56ab9d690670d" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  <small> (4)  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (290) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw29_56ab9d690670d" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw30_56ab9d690670d" >  <div class="indent-left">  <div id="rgw31_56ab9d690670d" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/266261649_Adaptive_Low-Complexity_Sequential_Inference_for_Dirichlet_Process_Mixture_Models">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Theodoros_Tsiligkaridis" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Theodoros Tsiligkaridis </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw32_56ab9d690670d">  <li class="citation-context-item"> "While most fast DPMM algorithms use a fixed α [7] [4] [8], imposing a prior distribution on α and sampling from it provides more flexibility, but this approach still heavily relies on experimentation and prior knowledge. Thus, many fast inference methods for Dirichlet process mixture models have been proposed that can adapt α to the data, including the works [6] where learning of α is incorporated in the Gibbs sampling analysis, [3] where a Gamma prior is used in a conjugate manner directly in the variational inference algorithm. [14] also account for model uncertainty on the concentration parameter α in a Bayesian manner directly in the sequential inference procedure. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Conference Paper:</span>    <a class="js-publication-title-link ga-publication-item" href="publication/266261649_Adaptive_Low-Complexity_Sequential_Inference_for_Dirichlet_Process_Mixture_Models"> <span class="publication-title js-publication-title">Adaptive Low-Complexity Sequential Inference for Dirichlet Process Mixture Models</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/71996886_Theodoros_Tsiligkaridis" class="authors js-author-name ga-publications-authors">Theodoros Tsiligkaridis</a> &middot;     <a href="researcher/8027653_Keith_W_Forsythe" class="authors js-author-name ga-publications-authors">Keith W. Forsythe</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> We develop a sequential low-complexity inference procedure for Dirichlet process mixtures of Gaussians for online clustering and parameter estimation when the number of clusters are unknown a-priori. We present an easily computable, closed form parametric expression for the conditional likelihood, in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors. Motivated by large-sample asymptotics, we propose a novel adaptive low-complexity design for the Dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate. We further prove that in the large-sample limit, the conditional likelihood and data predictive distribution become asymptotically Gaussian. We demonstrate through experiments on synthetic and real data sets that our approach is superior to other online state-of-the-art methods. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Conference Paper &middot; Dec 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Theodoros_Tsiligkaridis/publication/266261649_Adaptive_Low-Complexity_Sequential_Inference_for_Dirichlet_Process_Mixture_Models/links/55f8107d08aec948c477aa3a.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw33_56ab9d690670d" >  <div class="indent-left">  <div id="rgw34_56ab9d690670d" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/283117782_Multiple_co-clustering_based_on_nonparametric_mixture_models_with_heterogeneous_marginal_distributions">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Yu_Shimizu3" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Yu Shimizu </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw35_56ab9d690670d">  <li class="citation-context-item"> "Note that we truncate the number of views with sufficient large V and the number of feature clusters with G [2]. When Y (m) j,v,g = 1, feature j belongs to feature cluster g at view v. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link ga-publication-item" href="publication/283117782_Multiple_co-clustering_based_on_nonparametric_mixture_models_with_heterogeneous_marginal_distributions"> <span class="publication-title js-publication-title">Multiple co-clustering based on nonparametric mixture models with heterogeneous marginal distributions</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2083267847_Tomoki_Tokuda" class="authors js-author-name ga-publications-authors">Tomoki Tokuda</a> &middot;     <a href="researcher/9834630_Junichiro_Yoshimoto" class="authors js-author-name ga-publications-authors">Junichiro Yoshimoto</a> &middot;     <a href="researcher/2073058086_Yu_Shimizu" class="authors js-author-name ga-publications-authors">Yu Shimizu</a> &middot;     <a href="researcher/39446016_Shigeru_Toki" class="authors js-author-name ga-publications-authors">Shigeru Toki</a> &middot;     <a href="researcher/38184049_Go_Okada" class="authors js-author-name ga-publications-authors">Go Okada</a> &middot;     <a href="researcher/2073055749_Masahiro_Takamura" class="authors js-author-name ga-publications-authors">Masahiro Takamura</a> &middot;     <a href="researcher/2083310578_Tetsuya_Yamamoto" class="authors js-author-name ga-publications-authors">Tetsuya Yamamoto</a> &middot;     <a href="researcher/38423292_Shinpei_Yoshimura" class="authors js-author-name ga-publications-authors">Shinpei Yoshimura</a> &middot;     <a href="researcher/39906000_Yasumasa_Okamoto" class="authors js-author-name ga-publications-authors">Yasumasa Okamoto</a> &middot;     <a href="researcher/38476423_Shigeto_Yamawaki" class="authors js-author-name ga-publications-authors">Shigeto Yamawaki</a> &middot;     <a href="researcher/38620681_Kenji_Doya" class="authors js-author-name ga-publications-authors">Kenji Doya</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> We propose a novel method for multiple clustering that assumes a
co-clustering structure (partitions in both rows and columns of the data
matrix) in each view. The new method is applicable to high-dimensional data. It
is based on a nonparametric Bayesian approach in which the number of views and
the number of feature-/subject clusters are inferred in a data-driven manner.
We simultaneously model different distribution families, such as Gaussian,
Poisson, and multinomial distributions in each cluster block. This makes our
method applicable to datasets consisting of both numerical and categorical
variables, which biomedical data typically do. Clustering solutions are based
on variational inference with mean field approximation. We apply the proposed
method to synthetic and real data, and show that our method outperforms other
multiple clustering methods both in recovering true cluster structures and in
computation time. Finally, we apply our method to a depression dataset with no
true cluster structure available, from which useful inferences are drawn about
possible clustering structures of the data. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Oct 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Yu_Shimizu3/publication/283117782_Multiple_co-clustering_based_on_nonparametric_mixture_models_with_heterogeneous_marginal_distributions/links/564d33e208ae4988a7a42ec1.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw36_56ab9d690670d" >  <div class="indent-left">  <div id="rgw37_56ab9d690670d" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/266735007_Robust_Frequency-Hopping_Spectrum_Estimation_Based_on_Sparse_Bayesian_Method">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Haijian_Zhang" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Haijian Zhang </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw38_56ab9d690670d">  <li class="citation-context-item"> "In this process [21], the mixing weight π k (t i ) in (8) is constructed in a stick-breaking manner [25] " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link ga-publication-item" href="publication/266735007_Robust_Frequency-Hopping_Spectrum_Estimation_Based_on_Sparse_Bayesian_Method"> <span class="publication-title js-publication-title">Robust Frequency-Hopping Spectrum Estimation Based on Sparse Bayesian Method</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2055865637_Lifan_Zhao" class="authors js-author-name ga-publications-authors">Lifan Zhao</a> &middot;     <a href="researcher/2044854251_Lu_Wang" class="authors js-author-name ga-publications-authors">Lu Wang</a> &middot;     <a href="researcher/6634118_Guoan_Bi" class="authors js-author-name ga-publications-authors">Guoan Bi</a> &middot;     <a href="researcher/7493547_Liren_Zhang" class="authors js-author-name ga-publications-authors">Liren Zhang</a> &middot;     <a href="researcher/29918066_Haijian_Zhang" class="authors js-author-name ga-publications-authors">Haijian Zhang</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> This paper considers the problem of estimating multiple frequency hopping signals with unknown hopping pattern. By segmenting the received signals into overlapped measurements and leveraging the property that frequency content at each time instant is intrinsically parsimonious, a sparsity-inspired high-resolution time-frequency representation (TFR) is developed to achieve robust estimation. Inspired by the sparse Bayesian learning algorithm, the problem is formulated hierarchically to induce sparsity. In addition to the sparsity, the hopping pattern is exploited via temporal-aware clustering by exerting a dependent Dirichlet process prior over the latent parametric space. The estimation accuracy of the parameters can be greatly improved by this particular information-sharing scheme, and sharp boundary of the hopping time estimation is manifested. Moreover, the proposed algorithm is further extended to multi-channel cases, where task-relation is utilized to obtain robust clustering of the latent parameters for better estimation performance. Since the problem is formulated in a full Bayesian framework, laborintensive parameter tuning process can be avoided. Another superiority of the approach is that high-resolution instantaneous frequency estimation can be directly obtained without further refinement of the TFR. Results of numerical experiments show that the proposed algorithm can achieve superior performance particularly in low signal-to-noise ratio scenarios compared with other recently reported ones. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Feb 2015  &middot; IEEE Transactions on Wireless Communications  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Haijian_Zhang/publication/266735007_Robust_Frequency-Hopping_Spectrum_Estimation_Based_on_Sparse_Bayesian_Method/links/54f00c390cf2432ba657dd86.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw23_56ab9d690670d" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw24_56ab9d690670d">  </ul> </div> </div>   <div id="rgw14_56ab9d690670d" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw15_56ab9d690670d"> <div> <h5> <a href="publication/286413478_Estimating_size_and_scope_economies_in_the_Portuguese_water_sector_using_the_Bayesian_stochastic_frontier_analysis" class="color-inherit ga-similar-publication-title"><span class="publication-title">Estimating size and scope economies in the Portuguese water sector using the Bayesian stochastic frontier analysis</span></a>  </h5>  <div class="authors"> <a href="researcher/58482171_Pedro_Carvalho" class="authors ga-similar-publication-author">Pedro Carvalho</a>, <a href="researcher/15534211_Rui_Cunha_Marques" class="authors ga-similar-publication-author">Rui Cunha Marques</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw16_56ab9d690670d"> <div> <h5> <a href="publication/292102780_On_the_use_of_Bayesian_statistics_for_pair-wise_comparison_of_mega-variate_data_sets_extracting_meaningful_differences_between_GCxGC-MS_chromatograms_using_Jensen-Shannon_divergence" class="color-inherit ga-similar-publication-title"><span class="publication-title">On the use of Bayesian statistics for pair-wise comparison of mega-variate data sets: extracting meaningful differences between GCxGC-MS chromatograms using Jensen-Shannon divergence</span></a>  </h5>  <div class="authors"> <a href="researcher/2096152192_Andrei_Barcaru" class="authors ga-similar-publication-author">Andrei Barcaru</a>, <a href="researcher/12616090_Gabriel_Vivo-Truyols" class="authors ga-similar-publication-author">Gabriel Vivó-Truyols</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56ab9d690670d"> <div> <h5> <a href="publication/291517974_Application_of_hurdle_model_with_random_effects_for_evaluating_the_balance_improvement_in_stroke_patients" class="color-inherit ga-similar-publication-title"><span class="publication-title">Application of hurdle model with random effects for evaluating the balance improvement in stroke patients</span></a>  </h5>  <div class="authors"> <a href="researcher/2095273190_Alireza_Akbarzadeh_Baghban" class="authors ga-similar-publication-author">Alireza Akbarzadeh Baghban</a>, <a href="researcher/2085259874_Somayeh_Ahmadi_Gooraji" class="authors ga-similar-publication-author">Somayeh Ahmadi Gooraji</a>, <a href="researcher/2095308769_Amir_Kavousi" class="authors ga-similar-publication-author">Amir Kavousi</a>, <a href="researcher/2095411942_Navid_Mirzakhani_Araghi" class="authors ga-similar-publication-author">Navid Mirzakhani Araghi</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw48_56ab9d690670d" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw49_56ab9d690670d">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw50_56ab9d690670d" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=nZWqeLWr7Dkp70UU1St34rfHrXaJmLWQz0PXH6eCg1QDIFN2jb_7o5-W_W3gmuce" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="Ot94uDfElj+yvFdxBYNKJd79i3uC/PMVGRqb8vCWaYDsJFebMQhP16YlruLEUboJKgaGb4q7x/71i54vvmqj8NPJ5EPlXI3vrYiALLkUSQoEe0kH8GA2OgxzUv+YLpaXSE/ddQecqWH/uOGDdh2kJ4/cwrAHjXOZMd4gUvZncdth048BNjUaAdu3g35Z0XNKLAV8ksBtdeZNd5w3T+r3+N8rsYJnbK6/5Fd48jzEQMh/qzLkPjP6YgPXEBeYMkv6ZJj+kYykLy0cSlH52A9kJMBtVHOX4kDMgjzy228iXNo="/> <input type="hidden" name="urlAfterLogin" value="publication/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjU0MjEyNzM2X1ZhcmlhdGlvbmFsX2luZmVyZW5jZV9mb3JfRGlyaWNobGV0X3Byb2Nlc3NfbWl4dHVyZXNfQmF5ZXNpYW5fQW5hbF8xMTIxLTE0NA%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjU0MjEyNzM2X1ZhcmlhdGlvbmFsX2luZmVyZW5jZV9mb3JfRGlyaWNobGV0X3Byb2Nlc3NfbWl4dHVyZXNfQmF5ZXNpYW5fQW5hbF8xMTIxLTE0NA%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjU0MjEyNzM2X1ZhcmlhdGlvbmFsX2luZmVyZW5jZV9mb3JfRGlyaWNobGV0X3Byb2Nlc3NfbWl4dHVyZXNfQmF5ZXNpYW5fQW5hbF8xMTIxLTE0NA%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw51_56ab9d690670d"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 469;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Michael Jordan","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278648436346888%401443446372379_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Michael_Jordan13","institution":"University of California, Berkeley","institutionUrl":false,"widgetId":"rgw4_56ab9d690670d"},"id":"rgw4_56ab9d690670d","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=5451072","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab9d690670d"},"id":"rgw3_56ab9d690670d","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=254212736","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":254212736,"title":"Variational inference for Dirichlet process mixtures. Bayesian Anal 1:121-144","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"doi":"10.1214\/06-BA104","journalInfos":{"journal":"Bayesian Analysis","publicationDate":"03\/2006;","publicationDateRobot":"2006-03","article":"1(1)."}},"source":false,"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1214\/06-BA104"},{"key":"rft.atitle","value":"Variational inference for Dirichlet process mixtures. Bayesian Anal 1:121-144"},{"key":"rft.title","value":"Bayesian Analysis"},{"key":"rft.jtitle","value":"Bayesian Analysis"},{"key":"rft.volume","value":"1"},{"key":"rft.issue","value":"1"},{"key":"rft.date","value":"2006"},{"key":"rft.issn","value":"1936-0975"},{"key":"rft.au","value":"David M. Blei,Michael I. Jordan"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56ab9d690670d"},"id":"rgw6_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=254212736","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":254212736,"peopleItems":[{"data":{"authorUrl":"researcher\/2064238818_David_M_Blei","authorNameOnPublication":"David M. Blei","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"David M. Blei","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2064238818_David_M_Blei","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw9_56ab9d690670d"},"id":"rgw9_56ab9d690670d","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2064238818&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab9d690670d"},"id":"rgw8_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2064238818&authorNameOnPublication=David%20M.%20Blei","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Michael Jordan","accountUrl":"profile\/Michael_Jordan13","accountKey":"Michael_Jordan13","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278648436346888%401443446372379_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Michael Jordan","profile":{"professionalInstitution":{"professionalInstitutionName":"University of California, Berkeley","professionalInstitutionUrl":"institution\/University_of_California_Berkeley"}},"professionalInstitutionName":"University of California, Berkeley","professionalInstitutionUrl":"institution\/University_of_California_Berkeley","url":"profile\/Michael_Jordan13","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278648436346888%401443446372379_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Michael_Jordan13","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw11_56ab9d690670d"},"id":"rgw11_56ab9d690670d","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=5451072&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University of California, Berkeley","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":2,"accountCount":1,"publicationUid":254212736,"widgetId":"rgw10_56ab9d690670d"},"id":"rgw10_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=5451072&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=2&accountCount=1&publicationUid=254212736","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab9d690670d"},"id":"rgw7_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=254212736&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":254212736,"abstract":"<noscript><\/noscript><div>Dirichlet process (DP) mixture models are the cornerstone of nonparametric Bayesian<br \/>\nstatistics, and the development of Monte-Carlo Markov chain (MCMC) sampling methods for DP<br \/>\nmixtures has enabled the application of nonparametric Bayesian methods to a variety of<br \/>\npractical data analysis problems. However, MCMC sampling can be prohibitively slow, and it<br \/>\nis important to explore alternatives. One class of alternatives is provided by variational<br \/>\nmethods, a class of deterministic algorithms that convert inference problems into<br \/>\noptimization problems (Opper and Saad 2001; Wainwright and Jordan 2003). Thus far,<br \/>\nvariational methods have mainly been explored in the parametric setting, in particular<br \/>\nwithin the formalism of the exponential family (Attias 2000; Ghahramani and Beal 2001;<br \/>\nBlei et al. 2003). In this paper, we present a variational inference algorithm for DP<br \/>\nmixtures. We present experiments that compare the algorithm to Gibbs sampling algorithms<br \/>\nfor DP mixtures of Gaussians and present an application to a large-scale image analysis <br \/>\nproblem.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw12_56ab9d690670d"},"id":"rgw12_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=254212736","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144\/links\/53d6746a0cf2a7fbb2eaa5f3\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw13_56ab9d690670d"},"id":"rgw13_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab9d690670d"},"id":"rgw5_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=254212736&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":58482171,"url":"researcher\/58482171_Pedro_Carvalho","fullname":"Pedro Carvalho","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":15534211,"url":"researcher\/15534211_Rui_Cunha_Marques","fullname":"Rui Cunha Marques","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2016","journal":"Science of The Total Environment","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/286413478_Estimating_size_and_scope_economies_in_the_Portuguese_water_sector_using_the_Bayesian_stochastic_frontier_analysis","usePlainButton":true,"publicationUid":286413478,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"4.10","url":"publication\/286413478_Estimating_size_and_scope_economies_in_the_Portuguese_water_sector_using_the_Bayesian_stochastic_frontier_analysis","title":"Estimating size and scope economies in the Portuguese water sector using the Bayesian stochastic frontier analysis","displayTitleAsLink":true,"authors":[{"id":58482171,"url":"researcher\/58482171_Pedro_Carvalho","fullname":"Pedro Carvalho","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":15534211,"url":"researcher\/15534211_Rui_Cunha_Marques","fullname":"Rui Cunha Marques","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Science of The Total Environment 02\/2016; 544:574-586. DOI:10.1016\/j.scitotenv.2015.11.169"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/286413478_Estimating_size_and_scope_economies_in_the_Portuguese_water_sector_using_the_Bayesian_stochastic_frontier_analysis","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/286413478_Estimating_size_and_scope_economies_in_the_Portuguese_water_sector_using_the_Bayesian_stochastic_frontier_analysis\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw15_56ab9d690670d"},"id":"rgw15_56ab9d690670d","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=286413478","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2096152192,"url":"researcher\/2096152192_Andrei_Barcaru","fullname":"Andrei Barcaru","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":12616090,"url":"researcher\/12616090_Gabriel_Vivo-Truyols","fullname":"Gabriel Viv\u00f3-Truyols","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Analytical Chemistry","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/292102780_On_the_use_of_Bayesian_statistics_for_pair-wise_comparison_of_mega-variate_data_sets_extracting_meaningful_differences_between_GCxGC-MS_chromatograms_using_Jensen-Shannon_divergence","usePlainButton":true,"publicationUid":292102780,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"5.64","url":"publication\/292102780_On_the_use_of_Bayesian_statistics_for_pair-wise_comparison_of_mega-variate_data_sets_extracting_meaningful_differences_between_GCxGC-MS_chromatograms_using_Jensen-Shannon_divergence","title":"On the use of Bayesian statistics for pair-wise comparison of mega-variate data sets: extracting meaningful differences between GCxGC-MS chromatograms using Jensen-Shannon divergence","displayTitleAsLink":true,"authors":[{"id":2096152192,"url":"researcher\/2096152192_Andrei_Barcaru","fullname":"Andrei Barcaru","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":12616090,"url":"researcher\/12616090_Gabriel_Vivo-Truyols","fullname":"Gabriel Viv\u00f3-Truyols","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Analytical Chemistry 01\/2016;  DOI:10.1021\/acs.analchem.5b03506"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/292102780_On_the_use_of_Bayesian_statistics_for_pair-wise_comparison_of_mega-variate_data_sets_extracting_meaningful_differences_between_GCxGC-MS_chromatograms_using_Jensen-Shannon_divergence","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/292102780_On_the_use_of_Bayesian_statistics_for_pair-wise_comparison_of_mega-variate_data_sets_extracting_meaningful_differences_between_GCxGC-MS_chromatograms_using_Jensen-Shannon_divergence\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56ab9d690670d"},"id":"rgw16_56ab9d690670d","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=292102780","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2095273190,"url":"researcher\/2095273190_Alireza_Akbarzadeh_Baghban","fullname":"Alireza Akbarzadeh Baghban","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2085259874,"url":"researcher\/2085259874_Somayeh_Ahmadi_Gooraji","fullname":"Somayeh Ahmadi Gooraji","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095308769,"url":"researcher\/2095308769_Amir_Kavousi","fullname":"Amir Kavousi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095411942,"url":"researcher\/2095411942_Navid_Mirzakhani_Araghi","fullname":"Navid Mirzakhani Araghi","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Medical journal of the Islamic Republic of Iran","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291517974_Application_of_hurdle_model_with_random_effects_for_evaluating_the_balance_improvement_in_stroke_patients","usePlainButton":true,"publicationUid":291517974,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/291517974_Application_of_hurdle_model_with_random_effects_for_evaluating_the_balance_improvement_in_stroke_patients","title":"Application of hurdle model with random effects for evaluating the balance improvement in stroke patients","displayTitleAsLink":true,"authors":[{"id":2095273190,"url":"researcher\/2095273190_Alireza_Akbarzadeh_Baghban","fullname":"Alireza Akbarzadeh Baghban","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2085259874,"url":"researcher\/2085259874_Somayeh_Ahmadi_Gooraji","fullname":"Somayeh Ahmadi Gooraji","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095308769,"url":"researcher\/2095308769_Amir_Kavousi","fullname":"Amir Kavousi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095411942,"url":"researcher\/2095411942_Navid_Mirzakhani_Araghi","fullname":"Navid Mirzakhani Araghi","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Medical journal of the Islamic Republic of Iran 01\/2016; 29:244."],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291517974_Application_of_hurdle_model_with_random_effects_for_evaluating_the_balance_improvement_in_stroke_patients","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291517974_Application_of_hurdle_model_with_random_effects_for_evaluating_the_balance_improvement_in_stroke_patients\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab9d690670d"},"id":"rgw17_56ab9d690670d","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291517974","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw14_56ab9d690670d"},"id":"rgw14_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=254212736&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":254212736,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":254212736,"publicationType":"article","linkId":"53d6746a0cf2a7fbb2eaa5f3","fileName":"53d6746a0cf2a7fbb2eaa5f3.pdf","fileUrl":"profile\/Michael_Jordan13\/publication\/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144\/links\/53d6746a0cf2a7fbb2eaa5f3.pdf","name":"Michael Jordan","nameUrl":"profile\/Michael_Jordan13","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Jul 28, 2014","fileSize":"1.43 MB","widgetId":"rgw20_56ab9d690670d"},"id":"rgw20_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=254212736&linkId=53d6746a0cf2a7fbb2eaa5f3&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":254212736,"publicationType":"article","linkId":"02833f680cf274c8f342d530","fileName":"Variational inference for Dirichlet process mixtures. Bayesian Anal 1:121-144","fileUrl":"http:\/\/projecteuclid.org\/\/DPubS\/Repository\/1.0\/Disseminate?view=body&id=pdf_1&handle=euclid.ba\/1340371077","name":"projecteuclid.org","nameUrl":"http:\/\/projecteuclid.org\/\/DPubS\/Repository\/1.0\/Disseminate?view=body&id=pdf_1&handle=euclid.ba\/1340371077","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw21_56ab9d690670d"},"id":"rgw21_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=254212736&linkId=02833f680cf274c8f342d530&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw19_56ab9d690670d"},"id":"rgw19_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=254212736&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":35,"valueFormatted":"35","widgetId":"rgw22_56ab9d690670d"},"id":"rgw22_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=254212736","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw18_56ab9d690670d"},"id":"rgw18_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=254212736&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":254212736,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw24_56ab9d690670d"},"id":"rgw24_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=254212736&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":35,"valueFormatted":"35","widgetId":"rgw25_56ab9d690670d"},"id":"rgw25_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=254212736","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw23_56ab9d690670d"},"id":"rgw23_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=254212736&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Bayesian Analysis (2006)\n1, Number 1, pp. 121\u2013144\nVariational Inference for Dirichlet Process\nMixtures\nDavid M. Blei\u2217\nMichael I. Jordan\u2020\nAbstract.\nparametric Bayesian statistics, and the development of Monte-Carlo Markov chain\n(MCMC) sampling methods for DP mixtures has enabled the application of non-\nparametric Bayesian methods to a variety of practical data analysis problems.\nHowever, MCMC sampling can be prohibitively slow, and it is important to ex-\nplore alternatives. One class of alternatives is provided by variational methods, a\nclass of deterministic algorithms that convert inference problems into optimization\nproblems (Opper and Saad 2001; Wainwright and Jordan 2003). Thus far, varia-\ntional methods have mainly been explored in the parametric setting, in particular\nwithin the formalism of the exponential family (Attias 2000; Ghahramani and Beal\n2001; Blei et al. 2003). In this paper, we present a variational inference algorithm\nfor DP mixtures. We present experiments that compare the algorithm to Gibbs\nsampling algorithms for DP mixtures of Gaussians and present an application to\na large-scale image analysis problem.\nDirichlet process (DP) mixture models are the cornerstone of non-\nKeywords: Dirichlet processes, hierarchical models, variational inference, image\nprocessing, Bayesian computation\n1 Introduction\nThe methodology of Monte Carlo Markov chain (MCMC) sampling has energized Bayesian\nstatistics for more than a decade, providing a systematic approach to the computation\nof likelihoods and posterior distributions, and permitting the deployment of Bayesian\nmethods in a rapidly growing number of applied problems. However, while an unques-\ntioned success story, MCMC is not an unqualified one\u2014MCMC methods can be slow\nto converge and their convergence can be difficult to diagnose. While further research\non sampling is needed, it is also important to explore alternatives, particularly in the\ncontext of large-scale problems.\nOne such class of alternatives is provided by variational inference methods\n(Ghahramani and Beal 2001; Jordan et al. 1999; Opper and Saad 2001; Wainwright and Jordan\n2003; Wiegerinck 2000). Like MCMC, variational inference methods have their roots in\nstatistical physics, and, in contradistinction to MCMC methods, they are deterministic.\nThe basic idea of variational inference is to formulate the computation of a marginal\nor conditional probability in terms of an optimization problem. This (generally in-\ntractable) problem is then \u201crelaxed,\u201d yielding a simplified optimization problem that\n\u2217School\nhttp:\/\/www.cs.berkeley.edu\/~blei\/\n\u2020Department of Statistics and Computer Science Division, University of California, Berkeley, CA,\nhttp:\/\/www.cs.berkeley.edu\/~jordan\/\nofComputer Science,CarnegieMellon University,Pittsburgh, PA,\nc ? 2006 International Society for Bayesian Analysis ba0001"},{"page":2,"text":"122\nVariational inference for Dirichlet process mixtures\ndepends on a number of free parameters, known as variational parameters. Solving\nfor the variational parameters gives an approximation to the marginal or conditional\nprobabilities of interest.\nVariational inference methods have been developed principally in the context of the\nexponential family, where the convexity properties of the natural parameter space and\nthe cumulant function yield an elegant general variational formalism\n(Wainwright and Jordan 2003). For example, variational methods have been developed\nfor parametric hierarchical Bayesian models based on general exponential family spec-\nifications (Ghahramani and Beal 2001). MCMC methods have seen much wider appli-\ncation. In particular, the development of MCMC algorithms for nonparametric models\nsuch as the Dirichlet process has led to increased interest in nonparametric Bayesian\nmethods. In the current paper, we aim to close this gap by developing variational\nmethods for Dirichlet process mixtures.\nThe Dirichlet process (DP), introduced in Ferguson (1973), is a measure on measures.\nThe DP is parameterized by a base distribution G0and a positive scaling parameter\n\u03b1.1Suppose we draw a random measure G from a Dirichlet process, and independently\ndraw N random variables \u03b7nfrom G:\nG|{G0,\u03b1}\u223c\n\u223c\nDP(G0,\u03b1)\nG,n \u2208 {1,...,N}.\u03b7n\nMarginalizing out the random measure G, the joint distribution of {\u03b71,...,\u03b7N} follows\na P\u00b4 olya urn scheme (Blackwell and MacQueen 1973). Positive probability is assigned to\nconfigurations in which different \u03b7ntake on identical values; moreover, the underlying\nrandom measure G is discrete with probability one. This is seen most directly in the\nstick-breaking representation of the DP, in which G is represented explicitly as an infinite\nsum of atomic measures (Sethuraman 1994).\nThe Dirichlet process mixture model (Antoniak 1974) adds a level to the hierarchy\nby treating \u03b7nas the parameter of the distribution of the nth observation. Given the\ndiscreteness of G, the DP mixture has an interpretation as a mixture model with an\nunbounded number of mixture components.\nGiven a sample {x1,...,xN} from a DP mixture, our goal is to compute the predic-\ntive density:\np(x|x1,...,xN,\u03b1,G0) =\n?\np(x|\u03b7)p(\u03b7 |x1,...,xN,\u03b1,G0)d\u03b7,(1)\nAs in many hierarchical Bayesian models, the posterior distribution p(\u03b7 |x1,...,xN,G0,\u03b1)\nis complicated and is not available in a closed form. MCMC provides one class of approx-\nimations for this posterior and the predictive density (MacEachern 1994; Escobar and West\n1995; Neal 2000).\n1Ferguson (1973) parameterizes the Dirichlet process by a single base measure, which is \u03b1G0in our\nnotation."},{"page":3,"text":"D. M. Blei and M. I. Jordan123\nIn this paper, we present a variational inference algorithm for DP mixtures based\non the stick-breaking representation of the underlying DP. The algorithm involves two\nprobability distributions\u2014the posterior distribution p and a variational distribution q.\nThe latter is endowed with free variational parameters, and the algorithmic problem\nis to adjust these parameters so that q approximates p. We also use a stick-breaking\nrepresentation for q, but in this case we truncate the representation to yield a finite-\ndimensional representation. While in principle we could also truncate p, turning the\nmodel into a finite-dimensional model, it is important to emphasize at the outset that\nthis is not our approach\u2014we truncate only the variational distribution.\nThe paper is organized as follows. In Section 2 we provide basic background on\nDP mixture models, focusing on the case of exponential family mixtures. In Section 3\nwe present a variational inference algorithms for DP mixtures. Section 4 overviews\nMCMC algorithms for the DP mixture, discussing algorithms based both on the P\u00b4 olya\nurn representation and the stick-breaking representation. Section 5 presents the results\nof experimental comparisons, Section 6 presents an analysis of natural image data, and\nSection 7 presents our conclusions.\n2 Dirichlet process mixture models\nLet \u03b7 be a continuous random variable, let G0be a non-atomic probability distribution\nfor \u03b7, and let \u03b1 be a positive, real-valued scalar. A random measure G is distributed\naccording to a Dirichlet process (DP) (Ferguson 1973), with scaling parameter \u03b1 and\nbase distribution G0, if for all natural numbers k and k-partitions {B1,...,Bk},\n(G(B1),G(B2),...,G(Bk)) \u223c Dir(\u03b1G0(B1),\u03b1G0(B2),...,\u03b1G0(Bk)). (2)\nIntegrating out G, the joint distribution of the collection of variables {\u03b71,...,\u03b7n} ex-\nhibits a clustering effect; conditioning on n \u2212 1 draws, the nth value is, with positive\nprobability, exactly equal to one of those draws:\np(\u00b7|\u03b71,...,\u03b7n\u22121) \u221d \u03b1G0(\u00b7) +\nn\u22121\n?\ni=1\n\u03b4\u03b7i(\u00b7). (3)\nThus, the variables {\u03b71,...,\u03b7n\u22121} are randomly partitioned according to which vari-\nables are equal to the same value, with the distribution of the partition obtained from\na P\u00b4 olya urn scheme (Blackwell and MacQueen 1973). Let {\u03b7\u2217\ntinct values of {\u03b71,...,\u03b7n\u22121}, let c = {c1,...,cn\u22121} be assignment variables such that\n\u03b7i= \u03b7\u2217\nfollows the urn distribution:\n1,...,\u03b7\u2217\n|c|} denote the dis-\nci, and let |c| denote the number of cells in the partition. The distribution of \u03b7n\n\u03b7n=\n?\n\u03b7\u2217\ni\nwith prob.\nwith prob.\n|{j :cj=i}|\nn\u22121+\u03b1\n\u03b1\nn\u22121+\u03b1, \u03b7,\u03b7 \u223c G0\n(4)\nwhere |{j : cj= i}| is the number of times the value \u03b7\u2217\nioccurs in {\u03b71,...,\u03b7n\u22121}."},{"page":4,"text":"124\nVariational inference for Dirichlet process mixtures\nIn the Dirichlet process mixture model, the DP is used as a nonparametric prior in\na hierarchical Bayesian specification (Antoniak 1974):\nG|{\u03b1,G0}\n\u03b7n|G\nXn|\u03b7n\n\u223c\n\u223c\n\u223c\nDP(\u03b1,G0)\nG\np(xn|\u03b7n).\nData generated from this model can be partitioned according to the distinct values of\nthe parameter. Taking this view, the DP mixture has a natural interpretation as a\nflexible mixture model in which the number of components (i.e., the number of cells in\nthe partition) is random and grows as new data are observed.\nThe definition of the DP via its finite dimensional distributions in Equation (2)\nreposes on the Kolmogorov consistency theorem (Ferguson 1973). Sethuraman (1994)\nprovides a more explicit characterization of the DP in terms of a stick-breaking construc-\ntion. Consider two infinite collections of independent random variables, Vi\u223c Beta(1,\u03b1)\nand \u03b7\u2217\ni\u223c G0, for i = {1,2,...}. The stick-breaking representation of G is as follows:\n\u03c0i(v) = vi\ni\u22121\n?\nj=1\n(1 \u2212 vj)(5)\nG =\n\u221e\n?\ni=1\n\u03c0i(v)\u03b4\u03b7\u2217\ni. (6)\nThis representation of the DP makes clear that G is discrete (with probability one); the\nsupport of G consists of a countably infinite set of atoms, drawn independently from G0.\nThe mixing proportions \u03c0i(v) are given by successively breaking a unit length \u201cstick\u201d\ninto an infinite number of pieces. The size of each successive piece, proportional to the\nrest of the stick, is given by an independent draw from a Beta(1,\u03b1) distribution.\nIn the DP mixture, the vector \u03c0(v) comprises the infinite vector of mixing pro-\nportions and {\u03b7\u2217\n2,...} are the atoms representing the mixture components. Let Zn\nbe an assignment variable of the mixture component with which the data point xnis\nassociated. The data can be described as arising from the following process:\n1,\u03b7\u2217\n1. Draw Vi|\u03b1 \u223c Beta(1,\u03b1),i = {1,2,...}\n2. Draw \u03b7\u2217\ni|G0\u223c G0,i = {1,2,...}\n3. For the nth data point:\n(a) Draw Zn|{v1,v2,...} \u223c Mult(\u03c0(v)).\n(b) Draw Xn|zn\u223c p(xn|\u03b7\u2217\nzn).\nIn this paper, we restrict ourselves to DP mixtures for which the observable data\nare drawn from an exponential family distribution, and where the base distribution for\nthe DP is the corresponding conjugate prior."},{"page":5,"text":"D. M. Blei and M. I. Jordan 125\n\u03bb\n\u03b1\nN\n8\nZ\nX\nV\n\u03b7*\nn\nn\nk\nk\nFigure 1: Graphical model representation of an exponential family DP mixture. Nodes\ndenote random variables, edges denote possible dependence, and plates denote replica-\ntion.\nThe stick-breaking construction for the DP mixture is depicted as a graphical model\nin Figure 1. The conditional distributions of Vkand Znare as described above. The\ndistribution of Xnconditional on Znand {\u03b7\u2217\n1,\u03b7\u2217\n2,...} is\np(xn|zn,\u03b7\u2217\n1,\u03b7\u2217\n2,...) =\n\u221e\n?\ni=1\n?\nh(xn)exp{\u03b7\u2217\ni\nTxn\u2212 a(\u03b7\u2217\ni)}\n?1[zn=i]\n,\nwhere a(\u03b7\u2217\nis the sufficient statistic for the natural parameter \u03b7.\ni) is the appropriate cumulant function and we assume for simplicity that x\nThe vector of sufficient statistics of the corresponding conjugate family is (\u03b7\u2217T,\u2212a(\u03b7\u2217))T.\nThe base distribution is\np(\u03b7\u2217|\u03bb) = h(\u03b7\u2217)exp{\u03bbT\n1\u03b7\u2217+ \u03bb2(\u2212a(\u03b7\u2217)) \u2212 a(\u03bb)}, (7)\nwhere we decompose the hyperparameter \u03bb such that \u03bb1 contains the first dim(\u03b7\u2217)\ncomponents and \u03bb2is a scalar.\n3 Variational inference for DP mixtures\nThere is no direct way to compute the posterior distribution under a DP mixture prior.\nApproximate inference methods are required for DP mixtures and Markov chain Monte\nCarlo (MCMC) sampling methods have become the methodology of choice (MacEachern\n1994; Escobar and West 1995; MacEachern 1998; Neal 2000; Ishwaran and James 2001).\nVariational inference provides an alternative, deterministic methodology for approx-\nimating likelihoods and posteriors (Wainwright and Jordan 2003). Consider a model\nwith hyperparameters \u03b8, latent variables W = {W1,...,WM}, and observations x =\n{x1,...,xN}. The posterior distribution of the latent variables is:\np(w|x,\u03b8) = exp{logp(x,w|\u03b8) \u2212 logp(x|\u03b8)}. (8)"},{"page":6,"text":"126\nVariational inference for Dirichlet process mixtures\nWorking directly with this posterior is typically precluded by the need to compute\nthe normalizing constant. The log marginal probability of the observations is:\nlogp(x|\u03b8) = log\n?\np(w,x|\u03b8)dw, (9)\nwhich may be difficult to compute given that the latent variables become dependent\nwhen conditioning on observed data.\nMCMC algorithms circumvent this computation by constructing an approximate\nposterior based on samples from a Markov chain whose stationary distribution is the\nposterior of interest. Gibbs sampling is the simplest MCMC algorithm; one iteratively\nsamples each latent variable conditioned on the previously sampled values of the other\nlatent variables:\np(wi|w\u2212i,x,\u03b8) = exp{logp(w,x|\u03b8) \u2212 logp(w\u2212i,x|\u03b8)}. (10)\nThe normalizing constants for these conditional distributions are assumed to be available\nanalytically for settings in which Gibbs sampling is appropriate.\nVariational inference is based on reformulating the problem of computing the poste-\nrior distribution as an optimization problem, perturbing (or, \u201crelaxing\u201d) that problem,\nand finding solutions to the perturbed problem (Wainwright and Jordan 2003). In this\npaper, we work with a particular class of variational methods known as mean-field meth-\nods. These are based on optimizing Kullback-Leibler (KL) divergence with respect to\na so-called variational distribution. In particular, let q\u03bd(w) be a family of distributions\nindexed by a variational parameter \u03bd. We aim to minimize the KL divergence between\nq\u03bd(w) and p(w|x,\u03b8):\nD(q\u03bd(w)||p(w|x,\u03b8)) = Eq[logq\u03bd(W)] \u2212 Eq[logp(W,x|\u03b8)] + logp(x|\u03b8), (11)\nwhere here and elsewhere in the paper we omit the variational parameters \u03bd when using\nq as a subscript of an expectation. Notice that the problematic marginal probability\ndoes not depend on the variational parameters; it can be ignored in the optimization.\nThe minimization in Equation (11) can be cast alternatively as the maximization of\na lower bound on the log marginal likelihood:\nlogp(x|\u03b8) \u2265 Eq[logp(W,x|\u03b8)] \u2212 Eq[logq\u03bd(W)]. (12)\nThe gap in this bound is the divergence between q\u03bd(w) and the true posterior.\nFor the mean-field framework to yield a computationally effective inference method,\nit is necessary to choose a family of distributions q\u03bd(w) such that we can tractably\noptimize Equation (11). In constructing that family, one typically breaks some of the\ndependencies between latent variables that make the true posterior difficult to compute.\nIn the next sections, we consider fully-factorized variational distributions which break\nall of the dependencies."},{"page":7,"text":"D. M. Blei and M. I. Jordan127\n3.1 Mean field variational inference in exponential families\nFor each latent variable, let us assume that the conditional distribution p(wi|w\u2212i,x,\u03b8)\nis a member of the exponential family2:\np(wi|w\u2212i,x,\u03b8) = h(wi)exp{gi(w\u2212i,x,\u03b8)Twi\u2212 a(gi(w\u2212i,x,\u03b8))}, (13)\nwhere gi(w\u2212i,x,\u03b8) is the natural parameter for wiwhen conditioning on the remaining\nlatent variables and the observations.\nIn this setting it is natural to consider the following family of distributions as mean-\nfield variational approximations (Ghahramani and Beal 2001):\nq\u03bd(w) =\nM\n?\ni=1\nexp{\u03bdT\niwi\u2212 a(wi)}, (14)\nwhere \u03bd = {\u03bd1,\u03bd2,...,\u03bdM} are variational parameters. Indeed, it turns out that the\nvariational algorithm that we obtain using this fully-factorized family is reminiscent\nof Gibbs sampling. In particular, as we show in Appendix 7, the optimization of KL\ndivergence with respect to a single variational parameter \u03bdiis achieved by computing\nthe following expectation:\n\u03bdi= Eq[gi(W\u2212i,x,\u03b8)]. (15)\nRepeatedly updating each parameter in turn by computing this expectation amounts\nto performing coordinate ascent in the KL divergence.\nNotice the interesting relationship of this algorithm to the Gibbs sampler. In Gibbs\nsampling, we iteratively draw the latent variables wifrom the distribution p(wi|w\u2212i,x,\u03b8).\nIn mean-field variational inference, we iteratively update the variational parameter \u03bdi\nby setting it equal to the expected value of gi(w\u2212i,x,\u03b8). This expectation is computed\nunder the variational distribution.\n3.2 DP mixtures\nIn this section we develop a mean-field variational algorithm for the DP mixture. Our\nalgorithm is based on the stick-breaking representation of the DP mixture (see Figure 1).\nIn this representation the latent variables are the stick lengths, the atoms, and the cluster\nassignments: W = {V,\u03b7\u2217,Z}. The hyperparameters are the scaling parameter and the\nparameter of the conjugate base distribution: \u03b8 = {\u03b1,\u03bb}.\nFollowing the general recipe in Equation (12), we write the variational bound on the\n2Examples of models in which p(wi|w\u2212i,x,\u03b8) is an exponential family distribution include hidden\nMarkov models, mixture models, state space models, and hierarchical Bayesian models with conjugate\nand mixture of conjugate priors."},{"page":8,"text":"128\nVariational inference for Dirichlet process mixtures\nlog marginal probability of the data:\nlogp(x|\u03b1,\u03bb) \u2265 Eq[logp(V|\u03b1)] + Eq[logp(\u03b7\u2217|\u03bb)]\n+\nN\n?\nn=1\n(Eq[logp(Zn|V)] + Eq[logp(xn|Zn)])\n\u2212 Eq[logq(V,\u03b7\u2217,Z)].\n(16)\nTo exploit this bound, we must find a family of variational distributions that approxi-\nmates the distribution of the infinite-dimensional random measure G, where the random\nmeasure is expressed in terms of the infinite sets V = {V1,V2,...} and \u03b7\u2217= {\u03b7\u2217\nWe do this by considering truncated stick-breaking representations. Thus, we fix a value\nT and let q(vT = 1) = 1; this implies that the mixture proportions \u03c0t(v) are equal to\nzero for t > T (see Equation 5).\n1,\u03b7\u2217\n2,...}.\nTruncated stick-breaking representations have been considered previously by\nIshwaran and James (2001) in the context of sampling-based inference for an approxi-\nmation to the DP mixture model. Note that our use of truncation is rather different. In\nour case, the model is a full Dirichlet process and is not truncated; only the variational\ndistribution is truncated. The truncation level T is a variational parameter which can\nbe freely set; it is not a part of the prior model specification (see Section 5).\nWe thus propose the following factorized family of variational distributions for mean-\nfield variational inference:\nq(v,\u03b7\u2217,z) =\nT\u22121\n?\nt=1\nq\u03b3t(vt)\nT?\nt=1\nq\u03c4t(\u03b7\u2217\nt)\nN\n?\nn=1\nq\u03c6n(zn) (17)\nwhere q\u03b3t(vt) are beta distributions, q\u03c4t(\u03b7\u2217\nnatural parameters \u03c4t, and q\u03c6n(zn) are multinomial distributions. In the notation of\nSection 3.1, the free variational parameters are\nt) are exponential family distributions with\n\u03bd = {\u03b31,...,\u03b3T\u22121,\u03c41,...,\u03c4T,\u03c61,...,\u03c6N}.\nIt is important to note that there is a different variational parameter for each latent\nvariable under the variational distribution. For example, the choice of the mixture com-\nponent znfor the nth data point is governed by a multinomial distribution indexed by\na variational parameter \u03c6n. This reflects the conditional nature of variational inference.\nCoordinate ascent algorithm\nIn this section we present an explicit coordinate ascent algorithm for optimizing the\nbound in Equation (16) with respect to the variational parameters.\nAll of the terms in the bound involve standard computations in the exponential\nfamily, except for the third term. We rewrite the third term using indicator random"},{"page":9,"text":"D. M. Blei and M. I. Jordan129\nvariables:\nEq[logp(Zn|V)]=Eq\n?\ni=1q(zn> i)Eq[log(1 \u2212 Vi)] + q(zn= i)Eq[logVi].\nlog\n??\u221e\ni=1(1 \u2212 Vi)1[Zn>i]V1[Zn=i]\ni\n??\n=\n?\u221e\nRecall that Eq[log(1 \u2212 VT)] = 0 and q(zn> T) = 0. Consequently, we can truncate this\nsummation at t = T:\nEq[logp(Zn|V)] =\nT\n?\ni=1\nq(zn> i)Eq[log(1 \u2212 Vi)] + q(zn= i)Eq[logVi],\nwhere\nq(zn= i)=\u03c6n,i\n?T\n\u03a8(\u03b3i,1) \u2212 \u03a8(\u03b3i,1+ \u03b3i,2)\n\u03a8(\u03b3i,2) \u2212 \u03a8(\u03b3i,1+ \u03b3i,2).\nq(zn> i)\nEq[logVi]\n=\n=\nj=i+1\u03c6n,j\nEq[log(1 \u2212 Vi)]=\nThe digamma function, denoted by \u03a8, arises from the derivative of the log normalization\nfactor in the beta distribution.\nWe now use the general expression in Equation (15) to derive a mean-field coordinate\nascent algorithm. This yields:\n\u03b3t,1\n= 1 +?\n\u03b1 +?\n\u03bb1+?\n\u03bb2+?\nexp(St),\nn\u03c6n,t\n?T\nn\u03c6n,t.\n(18)\n\u03b3t,2\n\u03c4t,1\n\u03c4t,2\n\u03c6n,t\n=\n=\nn\nj=t+1\u03c6n,j\nn\u03c6n,txn\n(19)\n(20)\n=\n\u221d\n(21)\n(22)\nfor t \u2208 {1,...,T} and n \u2208 {1,...,N}, where\nSt= Eq[logVt] +?t\u22121\nIterating these updates optimizes Equation (16) with respect to the variational param-\neters defined in Equation (17).\ni=1Eq[log(1 \u2212 Vi)] + Eq[\u03b7\u2217\nt]TXn\u2212 Eq[a(\u03b7\u2217\nt)].\nPractical applications of variational methods must address initialization of the vari-\national distribution. While the algorithm yields a bound for any starting values of the\nvariational parameters, poor choices of initialization can lead to local maxima that yield\npoor bounds. We initialize the variational distribution by incrementally updating the\nparameters according to a random permutation of the data points. (This can be viewed\nas a variational version of sequential importance sampling). We run the algorithm mul-\ntiple times and choose the final parameter settings that give the best bound on the\nmarginal likelihood."},{"page":10,"text":"130\nVariational inference for Dirichlet process mixtures\nTo compute the predictive distribution, we use the variational posterior in a manner\nanalogous to the way that the empirical approximation is used by an MCMC sampling\nalgorithm. The predictive distribution is:\np(xN+1|x,\u03b1,\u03bb) =\n??\u221e\n?\nt=1\n\u03c0t(v)p(xN+1|\u03b7\u2217\nt)\n?\ndP(v,\u03b7\u2217|x,\u03bb,\u03b1).\nUnder the factorized variational approximation to the posterior, the distribution of\nthe atoms and the stick lengths are decoupled and the infinite sum is truncated. Conse-\nquently, we can approximate the predictive distribution with a product of expectations\nwhich are straightforward to compute under the variational approximation,\np(xN+1|x,\u03b1,\u03bb) \u2248\nT\n?\nt=1\nEq[\u03c0t(V)]Eq[p(xN+1|\u03b7\u2217\nt)], (23)\nwhere q depends implicitly on x, \u03b1, and \u03bb.\nFinally, we remark on two possible extensions. First, when G0is not conjugate, a\nsimple coordinate ascent update for \u03c4imay not be available, particularly when p(\u03b7\u2217\nis not in the exponential family. However, such an update is available for the special\ncase of G0being a mixture of conjugate distributions. Second, it is often important in\napplications to integrate over a diffuse prior on the scaling parameter \u03b1. As we show\nin Appendix 7, it is straightforward to extend the variational algorithm to include a\ngamma prior on \u03b1.\ni|z,x,\u03bb)\n4 Gibbs sampling\nFor comparison to variational inference, we review the collapsed Gibbs sampler and\nblocked Gibbs sampler for DP mixtures.\n4.1 Collapsed Gibbs sampling\nThe collapsed Gibbs sampler for a DP mixture with conjugate base distribution\n(MacEachern 1994) integrates out the random measure G and distinct parameter val-\nues {\u03b7\u2217\n|c|}. The Markov chain is thus defined only on the latent partition\nc = {c1,...,cN}. (Recall that |c| denotes the number of cells in the partition.)\n1,...,\u03b7\u2217\nThe algorithm iteratively samples each assignment variable Cn, for n \u2208 {1,...,N},\nconditional on the other cells in the partition, c\u2212n. The assignment Cncan be one of\n|c\u2212n| + 1 values: either the nth data point is in a cell with other data points, or in a\ncell by itself.\nExchangeability implies that Cnhas the following multinomial distribution:\np(cn= k|x,c\u2212n,\u03bb,\u03b1) \u221d p(xn|x\u2212n,c\u2212n,cn= k,\u03bb)p(cn= k|c\u2212n,\u03b1). (24)"},{"page":11,"text":"D. M. Blei and M. I. Jordan131\nThe first term is a ratio of normalizing constants of the posterior distribution of the kth\nparameter, one including and one excluding the nth data point:\np(xn|x\u2212n,c\u2212n,cn= k,\u03bb) =\n?\nexp\n?\nThe second term is given by the P\u00b4 olya urn scheme:\nexpa(\u03bb1+?\na(\u03bb1+?\nm?=n1[cm= k]xm+ xn,\u03bb2+?\nm?=n1[cm= k]xm,\u03bb2+?\nm?=n1[cm= k] + 1)\n?\nm?=n1[cm= k])\n?\n.\n(25)\np(cn= k|c\u2212n) \u221d\n?\n|{j : c\u2212n,j= k}|\n\u03b1\nif k is an existing cell in the partition\nif k is a new cell in the partition,\n(26)\nwhere |{j : c\u2212n,j= k}| denotes the number of data points in the kth cell of the partition\nc\u2212n.\nOnce this chain has reached its stationary distribution, we collect B samples {c1,...,cB}\nto approximate the posterior. The approximate predictive distribution is an average of\nthe predictive distributions across the Monte Carlo samples:\np(xN+1|x1,...,xN,\u03b1,\u03bb) =1\nB\nB\n?\nb=1\np(xN+1|cb,x,\u03b1,\u03bb).\nFor a given sample, that distribution is\np(xN+1|cb,x,\u03b1,\u03bb) =\n|cb|+1\n?\nk=1\np(cN+1= k|cb,\u03b1)p(xN+1|cb,x,cN+1= k,\u03bb).\nWhen G0is not conjugate, the distribution in Equation (25) does not have a simple\nclosed form. Effective algorithms for handling this case are given in Neal (2000).\n4.2 Blocked Gibbs sampling\nIn the collapsed Gibbs sampler, the assignment variable Cnis drawn from a distribu-\ntion that depends on the most recently sampled values of the other assignment vari-\nables. Consequently, these variables must be updated one at a time which can poten-\ntially slow down the algorithm when compared to a blocking strategy. To this end,\nIshwaran and James (2001) developed a blocked Gibbs sampling algorithm based on\nthe stick-breaking representation of Figure 1.\nThe main issue to face in developing a blocked Gibbs sampler for the stick-breaking\nDP mixture is that one needs to sample the infinite collection of stick lengths V before\nsampling the finite collection of cluster assignments Z. Ishwaran and James (2001) face\nthis issue by defining a truncated Dirichlet process (TDP) in which VK\u22121is set equal to\none for some fixed value K. This yields \u03c0i(V) = 0 for i \u2265 K, and converts the infinite\nsum in Equation (5) into a finite sum. Ishwaran and James (2001) justify substituting a"},{"page":12,"text":"132\nVariational inference for Dirichlet process mixtures\nTDP mixture model for a full DP mixture model by showing that the truncated process\nclosely approximates a true Dirichlet process when the truncation level is chosen large\nrelative to the number of data points.\nIn the TDP mixture, the state of the Markov chain consists of the beta variables\nV = {V1,...,VK\u22121}, the mixture component parameters \u03b7\u2217= {\u03b7\u2217\nindicator variables Z = {Z1,...,ZN}. The blocked Gibbs sampler iterates between the\nfollowing three steps:\n1,...,\u03b7\u2217\nK}, and the\n1. For n \u2208 {1,...,N}, independently sample Znfrom\np(zn= k|v,\u03b7\u2217,x) = \u03c0k(v)p(xn|\u03b7\u2217\nk),\n2. For k \u2208 {1,...,K}, independently sample Vkfrom Beta(\u03b3k,1,\u03b3k,2), where\n\u03b3k,1\n= 1 +?N\n\u03b1 +?K\nn=11[zn= k]\n\u03b3k,2\n=\ni=k+1\n?N\nn=11[zn= i].\nThis step follows from the conjugacy between the multinomial distribution and\nthe truncated stick-breaking construction, which is a generalized Dirichlet distri-\nbution (Connor and Mosimann 1969).\n3. For k \u2208 {1,...,K}, independently sample \u03b7\u2217\nin the same family as the base distribution, with parameters\nkfrom p(\u03b7\u2217\nk|\u03c4k). This distribution is\n\u03c4k,1\n\u03c4k,2\n=\n=\n\u03bb1+?\ni?=n1[zi= k]xi\ni?=n1[zi= k].\u03bb2+?\n(27)\nAfter the chain has reached its stationary distribution, we collect B samples and\nconstruct an approximate predictive distribution. Again, this distribution is an aver-\nage of the predictive distributions for each of the collected samples. The predictive\ndistribution for a particular sample is\np(xN+1|z,x,\u03b1,\u03bb) =\nK\n?\nk=1\nE[\u03c0i(V)|\u03b31,...,\u03b3k]p(xN+1|\u03c4k),(28)\nwhere E[\u03c0i|\u03b31,...,\u03b3k] is the expectation of the product of independent beta variables\ngiven in Equation (5). This distribution only depends on z; the other variables are\nneeded in the Gibbs sampling procedure, but can be integrated out here. Note that this\napproximation has a form similar to the approximate predictive distribution under the\nvariational distribution in Equation (23). In the variational case, however, the averaging\nis done parametrically via the variational distribution rather than by a Monte Carlo\nintegral.\nThe TDP sampler readily handles non-conjugacy of G0, provided that there is a\nmethod of sampling \u03b7\u2217\nifrom its posterior."},{"page":13,"text":"D. M. Blei and M. I. Jordan133\n\u221240\n\u221220\n0\n20\n40\n60\n\u221220\u2212100 1020\n\u221240\n\u221220\n0\n20\n40\n60\ninitial\n\u221240\n\u221220\n0\n20\n40\n60\n\u221220\u22121001020\n\u221240\n\u221220\n0\n20\n40\n60\niteration 2\n\u221240\n\u221220\n0\n20\n40\n60\n\u221220\u221210 010 20\n\u221240\n\u221220\n0\n20\n40\n60\niteration 5\nFigure 2: The approximate predictive distribution given by variational inference at\ndifferent stages of the algorithm. The data are 100 points generated by a Gaussian DP\nmixture model with fixed diagonal covariance.\n5Empirical comparison\nQualitatively, variational methods offer several potential advantages over Gibbs sam-\npling.They are deterministic, and have an optimization criterion given by Equa-\ntion (16) that can be used to assess convergence. In contrast, assessing convergence\nof a Gibbs sampler\u2014namely, determining when the Markov chain has reached its sta-\ntionary distribution\u2014is an active field of research. Theoretical bounds on the mixing\ntime are of little practical use, and there is no consensus on how to choose among the\nseveral empirical methods developed for this purpose (Robert and Casella 2004).\nBut there are several potential disadvantages of variational methods as well. First,\nthe optimization procedure can fall prey to local maxima in the variational parameter\nspace. Local maxima can be mitigated with restarts, or removed via the incorporation\nof additional variational parameters, but these strategies may slow the overall conver-\ngence of the procedure. Second, any given fixed variational representation yields only\nan approximation to the posterior. There are methods for considering hierarchies of\nvariational representations that approach the posterior in the limit, but these methods\nmay again incur serious computational costs. Lacking a theory by which these issues can\nbe evaluated in the general setting of DP mixtures, we turn to experimental evaluation.\nWe studied the performance of the variational algorithm of Section 3 and the Gibbs\nsamplers of Section 4 in the setting of DP mixtures of Gaussians with fixed inverse\ncovariance matrix \u039b (i.e., the DP mixes over the mean of the Gaussian). The natural\nconjugate base distribution for the DP is Gaussian, with covariance given by \u039b\/\u03bb2(see\nEquation 7).\nFigure 2 provides an illustrative example of variational inference on a small problem\ninvolving 100 data points sampled from a two-dimensional DP mixture of Gaussians\nwith diagonal covariance. Each panel in the figure plots the data and presents the"},{"page":14,"text":"134\nVariational inference for Dirichlet process mixtures\nFigure 3: Mean convergence time and standard error across ten data sets per dimension\nfor variational inference, TDP Gibbs sampling, and the collapsed Gibbs sampler.\npredictive distribution given by the variational inference algorithm at a given iteration\n(see Equation (23)). The truncation level was set to 20. As seen in the first panel, the\ninitialization of the variational parameters yields a largely flat distribution. After one\niteration, the algorithm has found the modes of the predictive distribution and, after\nconvergence, it has further refined those modes. Even though 20 mixture components\nare represented in the variational distribution, the fitted approximate posterior only\nuses five of them.\nTo compare the variational inference algorithm to the Gibbs sampling algorithms, we\nconducted a systematic set of simulation experiments in which the dimensionality of the\ndata was varied from 5 to 50. The covariance matrix was given by the autocorrelation\nmatrix for a first-order autoregressive process, chosen so that the components are highly\ndependent (\u03c1 = 0.9). The base distribution was a zero-mean Gaussian with covariance\nappropriately scaled for comparison across dimensions. The scaling parameter \u03b1 was\nset equal to one.\nIn each case, we generated 100 data points from a DP mixture of Gaussians model\nof the chosen dimensionality and generated 100 additional points as held-out data. In\ntesting on the held-out data, we treated each point as the 101st data point in the\ncollection and computed its conditional probability using each algorithm\u2019s approximate\npredictive distribution."},{"page":15,"text":"D. M. Blei and M. I. Jordan135\nDimMean held out log probability (Std err)\nVariational Collapsed Gibbs\n-147.96 (4.12)-148.08 (3.93)\n-266.59 (7.69)-266.29 (7.64)\n-494.12 (7.31)-492.32 (7.54)\n-721.55 (8.18)-720.05 (7.92)\n-943.39 (10.65) -941.04 (10.15)\n-1151.01 (15.23)-1148.51 (14.78)\nTruncated Gibbs\n-147.93 (3.88)\n-265.89 (7.66)\n-491.96 (7.59)\n-720.02 (7.96)\n-940.71 (10.23)\n-1147.48 (14.55)\n5\n10\n20\n30\n40\n50\nTable 1: Average held-out log probability for the predictive distributions given by vari-\national inference, TDP Gibbs sampling, and the collapsed Gibbs sampler.\n246810\n\u22121420\n\u22121380\n\u22121340\n\u22121300\nTruncation level\nLog marginal probability bound\n010203040506070\n\u2212380\n\u2212360\n\u2212340\n\u2212320\n\u2212300\nIteration\nHeld\u2212out score\n2.88e\u221215\n1.46e\u221210\n9.81e\u221205\nFigure 4: The optimal bound on the log probability as a function of the truncation\nlevel (left). There are five clusters in the simulated 20-dimensional DP mixture of\nGaussians data set which was used. Held-out probability as a function of iteration of\nvariational inference for the same simulated data set (right). The relative change in the\nlog probability bound of the observations is labeled at selected iterations."},{"page":16,"text":"136\nVariational inference for Dirichlet process mixtures\n0 50 100150\n0.0\n0.4\n0.8\nLag\nAutocorrelation\n050100150\n0.0\n0.4\n0.8\nLag\nAutocorrelation\nFigure 5: Autocorrelation plots on the size of the largest component for the truncated\nDP Gibbs sampler (left) and collapsed Gibbs sampler (right) in an example dataset of\n50-dimensional Gaussian data.\nThe TDP approximation was truncated at K = 20 components. For the variational\nalgorithm, the truncation level was also T = 20 components. Note that in the latter\ncase, the truncation level is simply another variational parameter. While we held T fixed\nin our simulations, it is also possible to optimize T with respect to the KL divergence.\nIndeed, Figure 4 (left) shows how the optimal KL divergence changes as a function of\nthe truncation level for one of the simulated data sets.\nWe ran all algorithms to convergence and measured the computation time.3For the\ncollapsed Gibbs sampler, we assessed convergence to the stationary distribution with\nthe diagnostic given by Raftery and Lewis (1992), and collected 25 additional samples\nto estimate the predictive distribution (the same diagnostic provides an appropriate\nlag at which to collect uncorrelated samples). We assessed convergence of the blocked\nGibbs sampler using the same statistic as for the collapsed Gibbs sampler and used the\nsame number of samples to form the approximate predictive distribution.4\nFinally, for variational inference, we measured convergence using the relative change\nin the log marginal probability bound (Equation 16), stopping the algorithm when it\nwas less than 1e\u221210.\nThere is a certain inevitable arbitrariness in these choices; in general it is difficult\nto envisage measures of computation time that allow stochastic MCMC algorithms and\ndeterministic variational algorithms to be compared in a standardized way. Nonetheless,\nwe have made what we consider to be reasonable, pragmatic choices. In particular, our\nchoice of stopping time for the variational algorithm is quite conservative, as illustrated\nin Figure 4 (right).\nFigure 3 illustrates the average convergence time across ten datasets per dimension.\nWith the caveats in mind regarding convergence time measurement, it appears that the\nvariational algorithm is quite competitive with the MCMC algorithms. The variational\n3All timing computations were made on a Pentium III 1GHZ desktop machine.\n4Typically, hundreds or thousands of samples are used in MCMC algorithms to form the approxi-\nmate posterior. However, we found that such approximations did not offer any additional predictive\nperformance in the simulated data. To be fair to MCMC in the timing comparisons, we used a small\nnumber of samples to estimate the predictive distributions."},{"page":17,"text":"D. M. Blei and M. I. Jordan 137\nFigure 6: Four sample clusters from a DP mixture analysis of 5000 images from the\nAssociated Press. The left-most column is the posterior mean of each cluster followed\nby the top ten images associated with it. These clusters capture patterns in the data,\nsuch as basketball shots, outdoor scenes on gray days, faces, and pictures with blue\nbackgrounds.\nalgorithm was faster and exhibited significantly less variance in its convergence time.\nMoreover, there is little evidence of an increase in convergence time across dimensionality\nfor the variational algorithm over the range tested.\nNote that the collapsed Gibbs sampler converged faster than the TDP Gibbs sampler.\nThough an iteration of collapsed Gibbs is slower than an iteration of TDP Gibbs, the\nTDP Gibbs sampler required a longer burn-in and greater lag to obtain uncorrelated\nsamples. This is illustrated in the autocorrelation plots of Figure 5. Comparing the two\nMCMC algorithms, we found no advantage to the truncated approximation.\nTable 1 illustrates the average log likelihood assigned to the held-out data by the\napproximate predictive distributions. First, notice that the collapsed DP Gibbs sam-\npler assigned the same likelihood as the posterior from the TDP Gibbs sampler\u2014an\nindication of the quality of a TDP for approximating a DP. More importantly, however,\nthe predictive distribution based on the variational posterior assigned a similar score as\nthose based on samples from the true posterior. Though it is based on an approximation\nto the posterior, the resulting predictive distributions are very accurate for this class of\nDP mixtures.\n6Image analysis\nFinite Gaussian mixture models are widely used in computer vision to model natural im-\nages for the purposes of automatic clustering, retrieval, and classification (Barnard et al.\n2003; Jeon et al. 2003). These applications are often large-scale data analysis problems,\ninvolving thousands of data points (images) in hundreds of dimensions (pixels). The ap-"},{"page":18,"text":"138\nVariational inference for Dirichlet process mixtures\n051015\n\u03b1\n2025 30\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n20\n40\n60\n80\n100\n120\nprior\nposterior\nFactor\nExpected number of images\nFigure 7: The expected number of images allocated to each component in the variational\nposterior (left). The posterior uses 79 components to describe the data. The prior for the\nscaling parameter \u03b1 and the approximate posterior given by its variational distribution\n(right).\npropriate number of mixture components to use in these problems is generally unknown,\nand DP mixtures provide an attractive alternative to current methods. However, a de-\nployment of DP mixtures in such problems crucially requires inferential methods that\nare computationally efficient. To demonstrate the applicability of our variational ap-\nproach to DP mixtures in the setting of large datasets, we analyzed a collection of 5000\nimages from the Associated Press under the assumptions of a DP mixture of Gaussians\nmodel.\nEach image was reduced to a 192-dimensional real-valued vector given by an 8\u00d78 grid\nof average red, green, and blue values. We fit a DP mixture model in which the mixture\ncomponents are Gaussian with mean \u00b5 and covariance matrix \u03c32I. The base distribution\nG0was a product measure\u2014Gamma(4,2) for 1\/\u03c32and N(0,5\u03c32) for \u00b5. Furthermore, we\nplaced a Gamma(1,1) prior on the DP scaling parameter \u03b1, as described in Appendix 7.\nWe used a truncation level of 150 for the variational distribution.\nThe variational algorithm required approximately four hours to converge. The re-\nsulting approximate posterior used 79 mixture components to describe the collection.\nFor a rough comparison to Gibbs sampling, an iteration of collapsed Gibbs takes 15\nminutes with this data set. In the same four hours, one could perform only 16 itera-\ntions. This is not enough for a chain to converge to its stationary distribution, let alone\nprovide a sufficient number of uncorrelated samples to construct an empirical estimate\nof the posterior.\nFigure 7 (left) illustrates the expected number of images allocated to each compo-\nnent under the variational approximation to the posterior. Figure 6 illustrates the ten\npictures with highest approximate posterior probability associated with each of four of\nthe components. These clusters appear to capture basketball shots, outdoor scenes on\ngray days, faces, and blue backgrounds.\nFigure 7 (right) illustrates the prior for the scaling parameter \u03b1 as well as the\napproximate posterior given by the fitted variational distribution. We see that the"},{"page":19,"text":"D. M. Blei and M. I. Jordan139\napproximate posterior is peaked and rather different from the prior, indicating that the\ndata have provided information regarding \u03b1.\n7 Conclusions\nWe have developed a mean-field variational inference algorithm for the Dirichlet pro-\ncess mixture model and demonstrated its applicability to the kinds of multivariate data\nfor which Gibbs sampling algorithms can exhibit slow convergence. Variational infer-\nence was faster than Gibbs sampling in our simulations, and its convergence time was\nindependent of dimensionality for the range which we tested.\nBoth variational and MCMC methods have strengths and weaknesses, and it is un-\nlikely that one methodology will dominate the other in general. While MCMC sampling\nprovides theoretical guarantees of accuracy, variational inference provides a fast, deter-\nministic approximation to otherwise unattainable posteriors. Moreover, both MCMC\nand variational methods are computational paradigms, providing a wide variety of spe-\ncific algorithmic approaches which trade off speed, accuracy and ease of implementation\nin different ways. We have investigated the deployment of the simplest form of varia-\ntional method for DP mixtures\u2014a mean-field variational algorithm\u2014but it worth noting\nthat other variational approaches, such as those described in Wainwright and Jordan\n(2003), are also worthy of consideration in the nonparametric context.\nAppendix-A Variational inference in exponential families\nIn this appendix, we derive the coordinate ascent algorithm for variational inference\ndescribed in Section 3.2. Recall that we are considering a latent variable model with\nhyperparameters \u03b8, observed variables x = {x1,...,xN}, and latent variables W =\n{W1,...,WM}. The posterior can be written as\np(w|x,\u03b8) = exp{logp(w,x|\u03b8) \u2212 logp(x|\u03b8)}.(29)\nThe variational bound on the log marginal probability is\nlogp(x|\u03b8) \u2265 Eq[logp(x,W|\u03b8)] \u2212 Eq[logq(W)]. (30)\nThis bound holds for any distribution q(w).\nFor the optimization of this bound to be computationally tractable, we restrict our-\nselves to fully-factorized variational distributions of the form q\u03bd(w) =?M\nexponential family (Ghahramani and Beal 2001). We derive a coordinate ascent algo-\nrithm in which we iteratively maximize the bound with respect to each \u03bdi, holding the\nother variational parameters fixed.\ni=1q\u03bdi(wi),\nwhere \u03bd = {\u03bd1,\u03bd2,...,\u03bdM} are variational parameters and each distribution is in the"},{"page":20,"text":"140\nVariational inference for Dirichlet process mixtures\nLet us rewrite the bound in Equation (30) using the chain rule:\nlogp(x|\u03b8) \u2265 logp(x|\u03b8)+\nM\n?\nm=1\nEq[logp(Wm|x,W1,...,Wm\u22121,\u03b8)]\u2212\nM\n?\nm=1\nEq[logq\u03bdm(Wm)].\n(31)\nTo optimize with respect to \u03bdi, reorder w such that wiis last in the list. The portion\nof Equation (31) depending on \u03bdiis\n?i= Eq[logp(Wi|W\u2212i,x,\u03b8)] \u2212 Eq[logq\u03bdi(Wi)].(32)\nThe variational distribution q\u03bdi(wi) is in the exponential family,\nq\u03bdi(wi) = h(wi)exp{\u03bdT\niwi\u2212 a(\u03bdi)},\nand Equation (32) simplifies as follows:\n?i\n=Eq\nEq[logp(Wi|W\u2212i,x,\u03b8)] \u2212 Eq[logh(Wi)] \u2212 \u03bdT\n?logp(Wi|W\u2212i,x,\u03b8) \u2212 logh(Wi) \u2212 \u03bdT\niWi+ a(\u03bdi)?\n=\nia?(\u03bdi) + a(\u03bdi),\nbecause Eq[Wi] = a?(\u03bdi).\nThe derivative with respect to \u03bdiis\n\u2202\n\u2202\u03bdi?i=\n\u2202\n\u2202\u03bdi\n(Eq[logp(Wi|W\u2212i,x,\u03b8)] \u2212 Eq[logh(Wi)]) \u2212 \u03bdT\nia??(\u03bdi).(33)\nThe optimal \u03bdisatisfies\n\u03bdi= [a??(\u03bdi)]\u22121\n?\n\u2202\n\u2202\u03bdiEq[logp(Wi|W\u2212i,x,\u03b8)] \u2212\n\u2202\n\u2202\u03bdiEq[logh(Wi)]\n?\n.(34)\nThe result in Equation (34) is general. In many applications of mean field methods,\nincluding those in the current paper, a further simplification is achieved. In particular,\nif the conditional distribution p(wi|w\u2212i,x,\u03b8) is an exponential family distribution then\np(wi|w\u2212i,x,\u03b8) = h(wi)exp{gi(w\u2212i,x,\u03b8)Twi\u2212 a(gi(w\u2212i,x,\u03b8))},\nwhere gi(w\u2212i,x,\u03b8) denotes the natural parameter for wi when conditioning on the\nremaining latent variables and the observations. This yields simplified expressions for\nthe expected log probability of Wiand its first derivative:\nEq[logp(Wi|W\u2212i,x,\u03b8)]=Eq[logh(Wi)] + Eq[gi(W\u2212i,x,\u03b8)]Ta?(\u03bdi) \u2212 Eq[a(gi(W\u2212i,x,\u03b8))]\n\u2202\n\u2202\u03bdiEq[logh(Wi)] + Eq[gi(W\u2212i,x,\u03b8)]Ta??(\u03bdi).\n\u2202\n\u2202\u03bdiEq[logp(Wi|W\u2212i,x,\u03b8)]=\nUsing the first derivative in Equation (34), the maximum is attained at\n\u03bdi= Eq[gi(W\u2212i,x,\u03b8)].(35)"},{"page":21,"text":"D. M. Blei and M. I. Jordan141\nWe define a coordinate ascent algorithm based on Equation (35) by iteratively updating\n\u03bdifor i \u2208 {1,...,M}. Such an algorithm finds a local maximum of Equation (30) by\nProposition 2.7.1 of Bertsekas (1999), under the condition that the right-hand side of\nEquation (32) is strictly convex.\nRelaxing the two assumptions complicates the algorithm, but the basic idea re-\nmains the same. If p(wi|w\u2212i,x,\u03b8) is not in the exponential family, then there may\nnot be an analytic expression for the update in Equation (34). If q(w) is not a fully\nfactorized distribution, then the second term of the bound in Equation (32) becomes\nEq[logq(wi|w\u2212i)] and the subsequent simplifications may not be applicable.\nFurther perspectives on algorithms of this kind can be found in Xing et al. (2003),\nGhahramani and Beal (2001), and Wiegerinck (2000). For a more general treatment of\nvariational methods for statistical inference, see Wainwright and Jordan (2003).\nAppendix-BPlacing a prior on the scaling parameter\nThe scaling parameter \u03b1 can have a significant effect on the growth of the number of\ncomponents grows with the data, and it is generally important to consider extended\nmodels which integrate over \u03b1. For the urn-based samplers, Escobar and West (1995)\nplace a Gamma(s1,s2) prior on \u03b1 and implement the corresponding Gibbs updates with\nauxiliary variable methods.\nIn the stick-breaking representation, the gamma distribution is convenient because\nit is conjugate to the stick lengths. We write the gamma distribution in its canonical\nform:\np(\u03b1|s1,s2) = (1\/\u03b1)exp{\u2212s2\u03b1 + s1log\u03b1 \u2212 a(s1,s2)},\nwhere s1is the shape parameter and s2is the inverse scale parameter. This distribution\nis conjugate to Beta(1,\u03b1). The log normalizer is\na(s1,s2) = log\u0393(s1) \u2212 s1logs2,\nand the posterior parameters conditional on data {v1,...,vK} are\n\u02c6 s2\n\u02c6 s1\n=\n=\ns2\u2212?K\ns1+ K.\ni=1log(1 \u2212 vi)\nWe extend the variational inference algorithm to include posterior updates for the\nscaling parameter \u03b1. The variational distribution is Gamma(w1,w2). The variational\nparameters are updated as follows:\nw1\n=s1+ T \u2212 1\nT\u22121\n?\nw2\n=s2\u2212\ni=1\nEq[log(1 \u2212 Vi)]),\nand we replace \u03b1 with its expectation Eq[\u03b1] = w1\/w2in the updates for \u03b3t,2in Equa-\ntion (19)."},{"page":22,"text":"142\nVariational inference for Dirichlet process mixtures\nBibliography\nAntoniak, C. (1974). \u201cMixtures of Dirichlet processes with applications to Bayesian\nnonparametric problems.\u201d The Annals of Statistics, 2(6):1152\u20131174. 122, 124\nAttias, H. (2000). \u201cA variational Bayesian framework for graphical models.\u201d In Solla, S.,\nLeen, T., and Muller, K. (eds.), Advances in Neural Information Processing Systems\n12, 209\u2013215. Cambridge, MA: MIT Press. 121\nBarnard, K., Duygulu, P., de Freitas, N., Forsyth, D., Blei, D., and Jordan, M. (2003).\n\u201cMatching words and pictures.\u201d Journal of Machine Learning Research, 3:1107\u20131135.\n137\nBertsekas, D. (1999). Nonlinear Programming. Nashua, NH: Athena Scientific. 141\nBlackwell, D. and MacQueen, J. (1973). \u201cFerguson distributions via P\u00b4 olya urn schemes.\u201d\nThe Annals of Statistics, 1(2):353\u2013355. 122, 123\nBlei, D., Ng, A., and Jordan, M. (2003). \u201cLatent Dirichlet allocation.\u201d Journal of\nMachine Learning Research, 3:993\u20131022. 121\nConnor, R. and Mosimann, J. (1969). \u201cConcepts of independence for proportions with\na generalization of the Dirichlet distribution.\u201d Journal of the American Statistical\nAssociation, 64(325):194\u2013206. 132\nEscobar, M. and West, M. (1995). \u201cBayesian density estimation and inference using\nmixtures.\u201d Journal of the American Statistical Association, 90:577\u2013588.\n141\n122, 125,\nFerguson, T. (1973). \u201cA Bayesian analysis of some nonparametric problems.\u201d The\nAnnals of Statistics, 1:209\u2013230. 122, 123, 124\nGhahramani, Z. and Beal, M. (2001). \u201cPropagation algorithms for variational Bayesian\nlearning.\u201d In Leen, T., Dietterich, T., and Tresp, V. (eds.), Advances in Neural\nInformation Processing Systems 13, 507\u2013513. Cambridge, MA: MIT Press. 121, 122,\n127, 139, 141\nIshwaran, J. and James, L. (2001). \u201cGibbs sampling methods for stick-breaking priors.\u201d\nJournal of the American Statistical Association, 96:161\u2013174. 125, 128, 131\nJeon, J., Lavrenko, V., and Manmatha, R. (2003). \u201cAutomatic image annotation and\nretrieval using cross-media relevance models.\u201d In Proceedings of the 26th Annual\nInternational ACM SIGIR conference on Research and Development in Information\nRetrieval, 119\u2013126. ACM Press. 137\nJordan, M., Ghahramani, Z., Jaakkola, T., and Saul, L. (1999). \u201cIntroduction to vari-\national methods for graphical models.\u201d Machine Learning, 37:183\u2013233. 121\nMacEachern, S. (1994). \u201cEstimating normal means with a conjugate style Dirichlet\nprocess prior.\u201d Communications in Statistics B, 23:727\u2013741. 122, 125, 130"},{"page":23,"text":"D. M. Blei and M. I. Jordan143\n\u2014 (1998). \u201cComputational methods for mixture of Dirichlet process models.\u201d In Dey,\nD., Muller, P., and Sinha, D. (eds.), Practical Nonparametric and Semiparametric\nBayesian Statistics, 23\u201344. Springer. 125\nNeal, R. (2000). \u201cMarkov chain sampling methods for Dirichlet process mixture models.\u201d\nJournal of Computational and Graphical Statistics, 9(2):249\u2013265. 122, 125, 131\nOpper, M. and Saad, D. (2001). Advanced Mean Field Methods: Theory and Practice.\nCambridge, MA: MIT Press. 121\nRaftery, A. and Lewis, S. (1992). \u201cOne long run with diagnostics: Implementation\nstrategies for Markov chain Monte Carlo.\u201d Statistical Science, 7:493\u2013497. 136\nRobert, C. and Casella, G. (2004). Monte Carlo Statistical Methods. New York, NY:\nSpringer-Verlag. 133\nSethuraman, J. (1994). \u201cA constructive definition of Dirichlet priors.\u201d Statistica Sinica,\n4:639\u2013650. 122, 124\nWainwright, M. and Jordan, M. (2003). \u201cGraphical models, exponential families, and\nvariational inference.\u201d Technical Report 649, U.C. Berkeley, Dept. of Statistics. 121,\n122, 125, 126, 139, 141\nWiegerinck, W. (2000). \u201cVariational approximations between mean field theory and the\njunction tree algorithm.\u201d In Boutilier, C. and Goldszmidt, M. (eds.), Proceedings\nof the 16th Annual Conference on Uncertainty in Artificial Intelligence (UAI-00),\n626\u2013633. San Francisco, CA: Morgan Kaufmann Publishers. 121, 141\nXing, E., Jordan, M., and Russell, S. (2003). \u201cA generalized mean field algorithm for\nvariational inference in exponential families.\u201d In Meek, C. and Kj\u00e6rulff, U. (eds.),\nProceedings of the 19th Annual Conference on Uncertainty in Artificial Intelligence\n(UAI-03), 583\u2013591. San Francisco, CA: Morgan Kaufmann Publishers. 141\nAcknowledgments\nWe thank Jaety Edwards for providing the AP image data. We want to acknowledge support\nfrom Intel Corporation, Microsoft Research, and a grant from DARPA in support of the CALO\nproject."},{"page":24,"text":"144\nVariational inference for Dirichlet process mixtures"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Jordan13\/publication\/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144\/links\/53d6746a0cf2a7fbb2eaa5f3.pdf","widgetId":"rgw26_56ab9d690670d"},"id":"rgw26_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=254212736&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw27_56ab9d690670d"},"id":"rgw27_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=254212736&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":254212736,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":254212736,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":71996886,"url":"researcher\/71996886_Theodoros_Tsiligkaridis","fullname":"Theodoros Tsiligkaridis","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278903538110475%401443507193589_m"},{"id":8027653,"url":"researcher\/8027653_Keith_W_Forsythe","fullname":"Keith W. Forsythe","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Conference Paper","publicationDate":"Dec 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":1,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/266261649_Adaptive_Low-Complexity_Sequential_Inference_for_Dirichlet_Process_Mixture_Models","usePlainButton":true,"publicationUid":266261649,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/266261649_Adaptive_Low-Complexity_Sequential_Inference_for_Dirichlet_Process_Mixture_Models","title":"Adaptive Low-Complexity Sequential Inference for Dirichlet Process Mixture Models","displayTitleAsLink":true,"authors":[{"id":71996886,"url":"researcher\/71996886_Theodoros_Tsiligkaridis","fullname":"Theodoros Tsiligkaridis","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278903538110475%401443507193589_m"},{"id":8027653,"url":"researcher\/8027653_Keith_W_Forsythe","fullname":"Keith W. Forsythe","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Advances in Neural Information Processing Systems (NIPS), Montreal, Quebec; 12\/2015"],"abstract":"We develop a sequential low-complexity inference procedure for Dirichlet process mixtures of Gaussians for online clustering and parameter estimation when the number of clusters are unknown a-priori. We present an easily computable, closed form parametric expression for the conditional likelihood, in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors. Motivated by large-sample asymptotics, we propose a novel adaptive low-complexity design for the Dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate. We further prove that in the large-sample limit, the conditional likelihood and data predictive distribution become asymptotically Gaussian. We demonstrate through experiments on synthetic and real data sets that our approach is superior to other online state-of-the-art methods.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/266261649_Adaptive_Low-Complexity_Sequential_Inference_for_Dirichlet_Process_Mixture_Models","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Theodoros_Tsiligkaridis\/publication\/266261649_Adaptive_Low-Complexity_Sequential_Inference_for_Dirichlet_Process_Mixture_Models\/links\/55f8107d08aec948c477aa3a.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Theodoros_Tsiligkaridis","sourceName":"Theodoros Tsiligkaridis","hasSourceUrl":true},"publicationUid":266261649,"publicationUrl":"publication\/266261649_Adaptive_Low-Complexity_Sequential_Inference_for_Dirichlet_Process_Mixture_Models","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/266261649_Adaptive_Low-Complexity_Sequential_Inference_for_Dirichlet_Process_Mixture_Models\/links\/55f8107d08aec948c477aa3a\/smallpreview.png","linkId":"55f8107d08aec948c477aa3a","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=266261649&reference=55f8107d08aec948c477aa3a&eventCode=&origin=publication_list","widgetId":"rgw31_56ab9d690670d"},"id":"rgw31_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=266261649&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"55f8107d08aec948c477aa3a","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":254212736,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/266261649_Adaptive_Low-Complexity_Sequential_Inference_for_Dirichlet_Process_Mixture_Models\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["While most fast DPMM algorithms use a fixed \u03b1 [7] [4] [8], imposing a prior distribution on \u03b1 and sampling from it provides more flexibility, but this approach still heavily relies on experimentation and prior knowledge. Thus, many fast inference methods for Dirichlet process mixture models have been proposed that can adapt \u03b1 to the data, including the works [6] where learning of \u03b1 is incorporated in the Gibbs sampling analysis, [3] where a Gamma prior is used in a conjugate manner directly in the variational inference algorithm. [14] also account for model uncertainty on the concentration parameter \u03b1 in a Bayesian manner directly in the sequential inference procedure. "],"widgetId":"rgw32_56ab9d690670d"},"id":"rgw32_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw30_56ab9d690670d"},"id":"rgw30_56ab9d690670d","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=266261649&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2083267847,"url":"researcher\/2083267847_Tomoki_Tokuda","fullname":"Tomoki Tokuda","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9834630,"url":"researcher\/9834630_Junichiro_Yoshimoto","fullname":"Junichiro Yoshimoto","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278615926296602%401443438621747_m"},{"id":2073058086,"url":"researcher\/2073058086_Yu_Shimizu","fullname":"Yu Shimizu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":39446016,"url":"researcher\/39446016_Shigeru_Toki","fullname":"Shigeru Toki","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":7,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Oct 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283117782_Multiple_co-clustering_based_on_nonparametric_mixture_models_with_heterogeneous_marginal_distributions","usePlainButton":true,"publicationUid":283117782,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/283117782_Multiple_co-clustering_based_on_nonparametric_mixture_models_with_heterogeneous_marginal_distributions","title":"Multiple co-clustering based on nonparametric mixture models with heterogeneous marginal distributions","displayTitleAsLink":true,"authors":[{"id":2083267847,"url":"researcher\/2083267847_Tomoki_Tokuda","fullname":"Tomoki Tokuda","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9834630,"url":"researcher\/9834630_Junichiro_Yoshimoto","fullname":"Junichiro Yoshimoto","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278615926296602%401443438621747_m"},{"id":2073058086,"url":"researcher\/2073058086_Yu_Shimizu","fullname":"Yu Shimizu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39446016,"url":"researcher\/39446016_Shigeru_Toki","fullname":"Shigeru Toki","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38184049,"url":"researcher\/38184049_Go_Okada","fullname":"Go Okada","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2073055749,"url":"researcher\/2073055749_Masahiro_Takamura","fullname":"Masahiro Takamura","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083310578,"url":"researcher\/2083310578_Tetsuya_Yamamoto","fullname":"Tetsuya Yamamoto","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38423292,"url":"researcher\/38423292_Shinpei_Yoshimura","fullname":"Shinpei Yoshimura","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39906000,"url":"researcher\/39906000_Yasumasa_Okamoto","fullname":"Yasumasa Okamoto","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38476423,"url":"researcher\/38476423_Shigeto_Yamawaki","fullname":"Shigeto Yamawaki","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38620681,"url":"researcher\/38620681_Kenji_Doya","fullname":"Kenji Doya","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"We propose a novel method for multiple clustering that assumes a\nco-clustering structure (partitions in both rows and columns of the data\nmatrix) in each view. The new method is applicable to high-dimensional data. It\nis based on a nonparametric Bayesian approach in which the number of views and\nthe number of feature-\/subject clusters are inferred in a data-driven manner.\nWe simultaneously model different distribution families, such as Gaussian,\nPoisson, and multinomial distributions in each cluster block. This makes our\nmethod applicable to datasets consisting of both numerical and categorical\nvariables, which biomedical data typically do. Clustering solutions are based\non variational inference with mean field approximation. We apply the proposed\nmethod to synthetic and real data, and show that our method outperforms other\nmultiple clustering methods both in recovering true cluster structures and in\ncomputation time. Finally, we apply our method to a depression dataset with no\ntrue cluster structure available, from which useful inferences are drawn about\npossible clustering structures of the data.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283117782_Multiple_co-clustering_based_on_nonparametric_mixture_models_with_heterogeneous_marginal_distributions","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Yu_Shimizu3\/publication\/283117782_Multiple_co-clustering_based_on_nonparametric_mixture_models_with_heterogeneous_marginal_distributions\/links\/564d33e208ae4988a7a42ec1.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Yu_Shimizu3","sourceName":"Yu Shimizu","hasSourceUrl":true},"publicationUid":283117782,"publicationUrl":"publication\/283117782_Multiple_co-clustering_based_on_nonparametric_mixture_models_with_heterogeneous_marginal_distributions","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/283117782_Multiple_co-clustering_based_on_nonparametric_mixture_models_with_heterogeneous_marginal_distributions\/links\/564d33e208ae4988a7a42ec1\/smallpreview.png","linkId":"564d33e208ae4988a7a42ec1","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=283117782&reference=564d33e208ae4988a7a42ec1&eventCode=&origin=publication_list","widgetId":"rgw34_56ab9d690670d"},"id":"rgw34_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=283117782&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"564d33e208ae4988a7a42ec1","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":254212736,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283117782_Multiple_co-clustering_based_on_nonparametric_mixture_models_with_heterogeneous_marginal_distributions\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Note that we truncate the number of views with sufficient large V and the number of feature clusters with G [2]. When Y (m) j,v,g = 1, feature j belongs to feature cluster g at view v. "],"widgetId":"rgw35_56ab9d690670d"},"id":"rgw35_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw33_56ab9d690670d"},"id":"rgw33_56ab9d690670d","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=283117782&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2055865637,"url":"researcher\/2055865637_Lifan_Zhao","fullname":"Lifan Zhao","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A279329389989895%401443608724265_m"},{"id":2044854251,"url":"researcher\/2044854251_Lu_Wang","fullname":"Lu Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":6634118,"url":"researcher\/6634118_Guoan_Bi","fullname":"Guoan Bi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":7493547,"url":"researcher\/7493547_Liren_Zhang","fullname":"Liren Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Feb 2015","journal":"IEEE Transactions on Wireless Communications","showEnrichedPublicationItem":false,"citationCount":3,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/266735007_Robust_Frequency-Hopping_Spectrum_Estimation_Based_on_Sparse_Bayesian_Method","usePlainButton":true,"publicationUid":266735007,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.50","url":"publication\/266735007_Robust_Frequency-Hopping_Spectrum_Estimation_Based_on_Sparse_Bayesian_Method","title":"Robust Frequency-Hopping Spectrum Estimation Based on Sparse Bayesian Method","displayTitleAsLink":true,"authors":[{"id":2055865637,"url":"researcher\/2055865637_Lifan_Zhao","fullname":"Lifan Zhao","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A279329389989895%401443608724265_m"},{"id":2044854251,"url":"researcher\/2044854251_Lu_Wang","fullname":"Lu Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":6634118,"url":"researcher\/6634118_Guoan_Bi","fullname":"Guoan Bi","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7493547,"url":"researcher\/7493547_Liren_Zhang","fullname":"Liren Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":29918066,"url":"researcher\/29918066_Haijian_Zhang","fullname":"Haijian Zhang","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["IEEE Transactions on Wireless Communications 02\/2015; 14(2). DOI:10.1109\/TWC.2014.2360191"],"abstract":"This paper considers the problem of estimating multiple frequency hopping signals with unknown hopping pattern. By segmenting the received signals into overlapped measurements and leveraging the property that frequency content at each time instant is intrinsically parsimonious, a sparsity-inspired high-resolution time-frequency representation (TFR) is developed to achieve robust estimation. Inspired by the sparse Bayesian learning algorithm, the problem is formulated hierarchically to induce sparsity. In addition to the sparsity, the hopping pattern is exploited via temporal-aware clustering by exerting a dependent Dirichlet process prior over the latent parametric space. The estimation accuracy of the parameters can be greatly improved by this particular information-sharing scheme, and sharp boundary of the hopping time estimation is manifested. Moreover, the proposed algorithm is further extended to multi-channel cases, where task-relation is utilized to obtain robust clustering of the latent parameters for better estimation performance. Since the problem is formulated in a full Bayesian framework, laborintensive parameter tuning process can be avoided. Another superiority of the approach is that high-resolution instantaneous frequency estimation can be directly obtained without further refinement of the TFR. Results of numerical experiments show that the proposed algorithm can achieve superior performance particularly in low signal-to-noise ratio scenarios compared with other recently reported ones.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/266735007_Robust_Frequency-Hopping_Spectrum_Estimation_Based_on_Sparse_Bayesian_Method","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Haijian_Zhang\/publication\/266735007_Robust_Frequency-Hopping_Spectrum_Estimation_Based_on_Sparse_Bayesian_Method\/links\/54f00c390cf2432ba657dd86.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Haijian_Zhang","sourceName":"Haijian Zhang","hasSourceUrl":true},"publicationUid":266735007,"publicationUrl":"publication\/266735007_Robust_Frequency-Hopping_Spectrum_Estimation_Based_on_Sparse_Bayesian_Method","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/266735007_Robust_Frequency-Hopping_Spectrum_Estimation_Based_on_Sparse_Bayesian_Method\/links\/54f00c390cf2432ba657dd86\/smallpreview.png","linkId":"54f00c390cf2432ba657dd86","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=266735007&reference=54f00c390cf2432ba657dd86&eventCode=&origin=publication_list","widgetId":"rgw37_56ab9d690670d"},"id":"rgw37_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=266735007&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"54f00c390cf2432ba657dd86","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":254212736,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/266735007_Robust_Frequency-Hopping_Spectrum_Estimation_Based_on_Sparse_Bayesian_Method\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["In this process [21], the mixing weight \u03c0 k (t i ) in (8) is constructed in a stick-breaking manner [25] "],"widgetId":"rgw38_56ab9d690670d"},"id":"rgw38_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw36_56ab9d690670d"},"id":"rgw36_56ab9d690670d","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=266735007&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":254212736,"publicationLink":"publication\/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw29_56ab9d690670d"},"id":"rgw29_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=254212736&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=290","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":true,"citationsCount":4,"hasIncomingCitations":true,"incomingCitationsCount":290,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw28_56ab9d690670d"},"id":"rgw28_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=254212736&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"53d6746a0cf2a7fbb2eaa5f3","name":"Michael Jordan","date":"Jul 28, 2014 ","nameLink":"profile\/Michael_Jordan13","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Michael_Jordan13\/publication\/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144\/links\/53d6746a0cf2a7fbb2eaa5f3.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Jordan13\/publication\/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144\/links\/53d6746a0cf2a7fbb2eaa5f3.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"5c40003127cb3de89938e5198a55568b","showFileSizeNote":false,"fileSize":"1.43 MB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"53d6746a0cf2a7fbb2eaa5f3","name":"Michael Jordan","date":"Jul 28, 2014 ","nameLink":"profile\/Michael_Jordan13","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Michael_Jordan13\/publication\/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144\/links\/53d6746a0cf2a7fbb2eaa5f3.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Jordan13\/publication\/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144\/links\/53d6746a0cf2a7fbb2eaa5f3.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"5c40003127cb3de89938e5198a55568b","showFileSizeNote":false,"fileSize":"1.43 MB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=VU-GCq04eQr7LhCFnCYqRWmAYTsbSMVDOkiA68mIo8cX9TguMExmXodcZDLOcQjy220tnLhVIfsee2939dkv4w.ebbwM1aLR9o3qHAumM8FITow6UCbK8G2qdwio1_uND407I9NnmxqMw0BEgD6ieRt2xw_TVe2t2K3cw5S7AWsqw","clickOnPill":"publication.PublicationFigures.html?_sg=oU2QojxK-XaR6VpWxcNZ5M8USDeYWlmRJ4ih84yQrB7mHFgjql22pwhMYbbRgE4Qt7JcsuQP7_78wynb6Ql1zQ.YGXrelOeaggPnyl3ebWZSun5vMnrkKHycTTbLjym9ZHUFaDamR7-g85DO6pLzhMgLLXaPdM3sVsnCGmCVynUwQ"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMichael_Jordan13%2Fpublication%2F254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144%2Flinks%2F53d6746a0cf2a7fbb2eaa5f3.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=fx_2Xsin1r75nUXuoRbdG7jZ6o-nie_aO-cCWYJZpDLakkcv_NwWXNCJ3a-aFHUjjss4KLDrggnEQ2Xj9FGrJw","urlHash":"ff86632b98cbb5b96abfa772de1aa9db","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=_LdI4T6R7wo6ZMFfLBnu_D4xz3BLCQd-jz1vUZyHQ_07SCbnTppKCmETcww5Y-41_L6hj3kG81ctwAHxft69s9j5mn0QOWAb5XtwkWKadzU.rfO7sHScuBXSED4UFp_OJ3scq6MdNJPKOVxPcnd_ApC4aAeC3VS-xEVdUubdgawCssgN6QIvqYg_d3fANwrJQA.SBDRIRIZ1zDaRUDPC9oNqnBJKt54blpO23ys_i59hk7BYjZH1NR0k8ZL9dPn5_VIccuc4iND92VujxqrN3ZKeg","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"53d6746a0cf2a7fbb2eaa5f3","trackedDownloads":{"53d6746a0cf2a7fbb2eaa5f3":{"v":false,"d":false}},"assetId":"AS:123950180868096@1406563434126","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":254212736,"commentCursorPromo":null,"widgetId":"rgw40_56ab9d690670d"},"id":"rgw40_56ab9d690670d","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMichael_Jordan13%2Fpublication%2F254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144%2Flinks%2F53d6746a0cf2a7fbb2eaa5f3.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A123950180868096%401406563434126&publicationUid=254212736&linkId=53d6746a0cf2a7fbb2eaa5f3&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Variational inference for Dirichlet process mixtures. Bayesian Anal 1:121-144","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=vUVq9qNrufZfkHQUOnNWS2lMVZ1LaBIeXoAfeDw614szHRFqUH63PqLkj6dKHE0RZiJQ3kye4hFqgq0j7xigvuxsGhmFVNb-yHyJJJS3PcE.EUaoxLgPGzojlfzgsQvUxtnMg-T45QYoAreQa70q1Baml8wxYnEXBI3K7PxLCMHkoci0xtiTOSWmKKPzC5VWbw.Ek0QSo7J3pYmctwuE0u5_kKfSPp_wM_cstS6Glv3fCCqhyDaMWswg_aSUw9ZTFZxQYxh6c8Dd2jyYJL9S0rXQw","publicationUid":254212736,"trackedDownloads":{"53d6746a0cf2a7fbb2eaa5f3":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw42_56ab9d690670d"},"id":"rgw42_56ab9d690670d","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw43_56ab9d690670d"},"id":"rgw43_56ab9d690670d","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw44_56ab9d690670d"},"id":"rgw44_56ab9d690670d","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw45_56ab9d690670d"},"id":"rgw45_56ab9d690670d","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw46_56ab9d690670d"},"id":"rgw46_56ab9d690670d","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw41_56ab9d690670d"},"id":"rgw41_56ab9d690670d","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw39_56ab9d690670d"},"id":"rgw39_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab9d690670d"},"id":"rgw2_56ab9d690670d","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":254212736},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=254212736&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab9d690670d"},"id":"rgw1_56ab9d690670d","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"syH3EtjmQLfTJ9+jS5lje46HbeLYeCYoBIlv7bVer6Cs4qH53lK+0Jf5Py7outXNdc1RPoo8TjEm0ck5bYfdS1ZpPde5qZOVLNt2e+29TNao9p8pLxDd+o5tMEwQya5+YC0CDO+s4uA9E\/1JArpLPxzMSirBhLs9p6pytmpGlWfk0MpOz7ESZ4QPwjTKUyiavzIV\/1FVggDhryUonnF6T12ptRT7XGrfQgPa3aDvvRs+JsgkvPCcva2K1hChazRHlB\/uKGlFrNUO89rauXAKBQs87D7gIc1lOt4lk2EnU2Y=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Variational inference for Dirichlet process mixtures. Bayesian Anal 1:121-144\" \/>\n<meta property=\"og:description\" content=\"Dirichlet process (DP) mixture models are the cornerstone of nonparametric Bayesian\nstatistics, and the development of Monte-Carlo Markov chain (MCMC) sampling methods for DP\nmixtures has enabled...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144\/links\/53d6746a0cf2a7fbb2eaa5f3\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144\" \/>\n<meta property=\"rg:id\" content=\"PB:254212736\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1214\/06-BA104\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Variational inference for Dirichlet process mixtures. Bayesian Anal 1:121-144\" \/>\n<meta name=\"citation_author\" content=\"David M. Blei\" \/>\n<meta name=\"citation_author\" content=\"Michael I. Jordan\" \/>\n<meta name=\"citation_publication_date\" content=\"2006\/03\/01\" \/>\n<meta name=\"citation_issn\" content=\"1936-0975\" \/>\n<meta name=\"citation_volume\" content=\"1\" \/>\n<meta name=\"citation_issue\" content=\"1\" \/>\n<meta name=\"citation_doi\" content=\"10.1214\/06-BA104\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Michael_Jordan13\/publication\/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144\/links\/53d6746a0cf2a7fbb2eaa5f3.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-c1ed37e9-a1c1-42bc-b3f2-34b9efd6406b","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":451,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw47_56ab9d690670d"},"id":"rgw47_56ab9d690670d","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-c1ed37e9-a1c1-42bc-b3f2-34b9efd6406b", "f7b67d734ac547c034f78ac433173dd456b7d8e9");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-c1ed37e9-a1c1-42bc-b3f2-34b9efd6406b", "f7b67d734ac547c034f78ac433173dd456b7d8e9");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw48_56ab9d690670d"},"id":"rgw48_56ab9d690670d","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/254212736_Variational_inference_for_Dirichlet_process_mixtures_Bayesian_Anal_1121-144","requestToken":"Ot94uDfElj+yvFdxBYNKJd79i3uC\/PMVGRqb8vCWaYDsJFebMQhP16YlruLEUboJKgaGb4q7x\/71i54vvmqj8NPJ5EPlXI3vrYiALLkUSQoEe0kH8GA2OgxzUv+YLpaXSE\/ddQecqWH\/uOGDdh2kJ4\/cwrAHjXOZMd4gUvZncdth048BNjUaAdu3g35Z0XNKLAV8ksBtdeZNd5w3T+r3+N8rsYJnbK6\/5Fd48jzEQMh\/qzLkPjP6YgPXEBeYMkv6ZJj+kYykLy0cSlH52A9kJMBtVHOX4kDMgjzy228iXNo=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=nZWqeLWr7Dkp70UU1St34rfHrXaJmLWQz0PXH6eCg1QDIFN2jb_7o5-W_W3gmuce","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjU0MjEyNzM2X1ZhcmlhdGlvbmFsX2luZmVyZW5jZV9mb3JfRGlyaWNobGV0X3Byb2Nlc3NfbWl4dHVyZXNfQmF5ZXNpYW5fQW5hbF8xMTIxLTE0NA%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw50_56ab9d690670d"},"id":"rgw50_56ab9d690670d","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw49_56ab9d690670d"},"id":"rgw49_56ab9d690670d","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw51_56ab9d690670d"},"id":"rgw51_56ab9d690670d","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
