<!DOCTYPE html> <html lang="en" class="" id="rgw39_56ab9eb3b965f"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="2oUGVx1klRbMiUgTpGydSQm/8J5GLPYvZbKnL4TUeoMET+NHOQpu1Hg5zriM4eJ03LUGLirKQqRyY9Ofl4/u4yu6bEsgXHtLy74zNaJtR1bM3useXStMOVlYXsrly3M581JtdTFFnWryhKO5wXGW1Ulbnruv5n4iR6YtSRBnHXEDOjz+J3HF5R5ufzMK3a59DxVRCp1/nvew4DSsVA3IFswwd017aaLQvmGls8+OsUnho9Eab+EHAWL3drdduuFwNc/1rLKE2VQftF5liPKW4WFkTalqs7Ej+IU1cwdUzc8="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-ae852297-22e3-4faf-ac98-afb9080b2f2f",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/266660385_Reducing_the_sampling_complexity_of_topic_models" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Reducing the sampling complexity of topic models" />
<meta property="og:description" content="Inference in topic models typically involves a sampling step to associate latent variables with observations. Unfortunately the generative model loses sparsity as the amount of data increases,..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/266660385_Reducing_the_sampling_complexity_of_topic_models/links/5583a5c508aefa35fe30c780/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/266660385_Reducing_the_sampling_complexity_of_topic_models" />
<meta property="rg:id" content="PB:266660385" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1145/2623330.2623756" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Reducing the sampling complexity of topic models" />
<meta name="citation_author" content="Aaron Q. Li" />
<meta name="citation_author" content="Amr Ahmed" />
<meta name="citation_author" content="Sujith Ravi" />
<meta name="citation_author" content="Alexander J. Smola" />
<meta name="citation_publication_date" content="2014/08/24" />
<meta name="citation_isbn" content="978-1-4503-2956-9" />
<meta name="citation_doi" content="10.1145/2623330.2623756" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Sujith_Ravi/publication/266660385_Reducing_the_sampling_complexity_of_topic_models/links/5583a5c508aefa35fe30c780.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/266660385_Reducing_the_sampling_complexity_of_topic_models" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/266660385_Reducing_the_sampling_complexity_of_topic_models" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Reducing the sampling complexity of topic models (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Reducing the sampling complexity of topic models on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab9eb3b965f" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab9eb3b965f" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab9eb3b965f">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1145%2F2623330.2623756&rft.atitle=Reducing%20the%20sampling%20complexity%20of%20topic%20models&rft.title=Proceedings%20of%20the%20ACM%20SIGKDD%20International%20Conference%20on%20Knowledge%20Discovery%20and%20Data%20Mining&rft.jtitle=Proceedings%20of%20the%20ACM%20SIGKDD%20International%20Conference%20on%20Knowledge%20Discovery%20and%20Data%20Mining&rft.date=2014&rft.au=Aaron%20Q.%20Li%2CAmr%20Ahmed%2CSujith%20Ravi%2CAlexander%20J.%20Smola&rft.isbn=978-1-4503-2956-9&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Reducing the sampling complexity of topic models</h1> <meta itemprop="headline" content="Reducing the sampling complexity of topic models">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/266660385_Reducing_the_sampling_complexity_of_topic_models/links/5583a5c508aefa35fe30c780/smallpreview.png">  <div id="rgw7_56ab9eb3b965f" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab9eb3b965f" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Aaron_Li3" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A278485756071938%401443407586376_m" title="Aaron Li" alt="Aaron Li" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Aaron Li</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw9_56ab9eb3b965f" data-account-key="Aaron_Li3">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Aaron_Li3"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A278485756071938%401443407586376_l" title="Aaron Li" alt="Aaron Li" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Aaron_Li3" class="display-name">Aaron Li</a>    </h5> <div class="truncate-single-line meta">    <span class="meta">Scaled Inference</span>    </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56ab9eb3b965f"> <a href="researcher/2055577116_Amr_Ahmed" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Amr Ahmed" alt="Amr Ahmed" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Amr Ahmed</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw11_56ab9eb3b965f">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2055577116_Amr_Ahmed"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Amr Ahmed" alt="Amr Ahmed" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2055577116_Amr_Ahmed" class="display-name">Amr Ahmed</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56ab9eb3b965f" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Sujith_Ravi" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A282136889315329%401444278084290_m/Sujith_Ravi.png" title="Sujith Ravi" alt="Sujith Ravi" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Sujith Ravi</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw13_56ab9eb3b965f" data-account-key="Sujith_Ravi">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Sujith_Ravi"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A282136889315329%401444278084290_l/Sujith_Ravi.png" title="Sujith Ravi" alt="Sujith Ravi" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Sujith_Ravi" class="display-name">Sujith Ravi</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Google_Inc" title="Google Inc.">Google Inc.</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw14_56ab9eb3b965f" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Alexander_Smola" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Alexander J. Smola" alt="Alexander J. Smola" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Alexander J. Smola</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw15_56ab9eb3b965f" data-account-key="Alexander_Smola">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Alexander_Smola"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Alexander J. Smola" alt="Alexander J. Smola" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Alexander_Smola" class="display-name">Alexander J. Smola</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Carnegie_Mellon_University" title="Carnegie Mellon University">Carnegie Mellon University</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">     Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining   <meta itemprop="datePublished" content="2014-08">  08/2014;      DOI:&nbsp;10.1145/2623330.2623756           </div> <div id="rgw16_56ab9eb3b965f" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>Inference in topic models typically involves a sampling step to associate latent variables with observations. Unfortunately the generative model loses sparsity as the amount of data increases, requiring O(k) operations per word for k topics. In this paper we propose an algorithm which scales linearly with the number of actually instantiated topics kd in the document. For large document collections and in structured hierarchical models kd ll k. This yields an order of magnitude speedup. Our method applies to a wide variety of statistical models such as PDP [16,4] and HDP [19]. At its core is the idea that dense, slowly changing distributions can be approximated efficiently by the combination of a Metropolis-Hastings step, use of sparsity, and amortized constant time sampling via Walker's alias method.</div> </p>  </div>   </div>      <div class="action-container"> <div id="rgw17_56ab9eb3b965f" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw30_56ab9eb3b965f">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw31_56ab9eb3b965f">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Sujith_Ravi/publication/266660385_Reducing_the_sampling_complexity_of_topic_models/links/5583a5c508aefa35fe30c780.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Sujith_Ravi">Sujith Ravi</a>, <span class="js-publication-date"> Jun 19, 2015 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw33_56ab9eb3b965f" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw34_56ab9eb3b965f" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw35_56ab9eb3b965f" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw36_56ab9eb3b965f" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw37_56ab9eb3b965f" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw38_56ab9eb3b965f" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw32_56ab9eb3b965f" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FSujith_Ravi%2Fpublication%2F266660385_Reducing_the_sampling_complexity_of_topic_models%2Flinks%2F5583a5c508aefa35fe30c780.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw29_56ab9eb3b965f"  itemprop="articleBody">  <p>Page 1</p> <p>Reducing the Sampling Complexity of Topic Models<br />Aaron Q. Li<br />CMU Language Technologies<br />Pittsburgh, PA<br />aaronli@cmu.edu<br />Sujith Ravi<br />Google Strategic Technologies<br />Mountain View, CA<br />sravi@google.com<br />Amr Ahmed<br />Google Strategic Technologies<br />Mountain View, CA<br />amra@google.com<br />Alexander J. Smola<br />CMU MLD and Google ST<br />Pittsburgh PA<br />alex@smola.org<br />ABSTRACT<br />Inference in topic models typically involves a sampling step<br />to associate latent variables with observations. Unfortu-<br />nately the generative model loses sparsity as the amount<br />of data increases, requiring O(k) operations per word for k<br />topics. In this paper we propose an algorithm which scales<br />linearly with the number of actually instantiated topics kdin<br />the document. For large document collections and in struc-<br />tured hierarchical models kd ? k. This yields an order of<br />magnitude speedup. Our method applies to a wide variety<br />of statistical models such as PDP [16, 4] and HDP [19].<br />At its core is the idea that dense, slowly changing distri-<br />butions can be approximated efficiently by the combination<br />of a Metropolis-Hastings step, use of sparsity, and amortized<br />constant time sampling via Walker’s alias method.<br />Keywords<br />Sampling; Scalability; Topic Models; Alias Method<br />1.INTRODUCTION<br />Topic models are some of the most versatile tools for mod-<br />eling statistical dependencies. Given a set of observations<br />xi ∈ X, such as documents, logs of user activity, or com-<br />munications patterns, we want to infer the hidden causes<br />motivating this behavior. A key property in topic models<br />is that they model p(x) via a discrete hidden factor, z via<br />p(x|z) and p(z). For instance, z may be the cluster of a docu-<br />ment. In this case it leads to Gaussian and Dirichlet mixture<br />models [14]. When z is a vector of topics associated with in-<br />dividual words, this leads to Latent Dirichlet Allocation [3].<br />Likewise, whenever z indicates a term in a hierarchy, it leads<br />to structured and mixed-content annotations [19, 2, 4, 12].<br />1.1Sparsity in Topic Models<br />One of the key obstacles in performing scalable inference is<br />to draw p(z|x) from the discrete state distribution associated<br />Permission to make digital or hard copies of all or part of this work for personal or<br />classroom use is granted without fee provided that copies are not made or distributed<br />for profit or commercial advantage and that copies bear this notice and the full citation<br />on the first page. Copyrights for components of this work owned by others than the<br />author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or<br />republish, to post on servers or to redistribute to lists, requires prior specific permission<br />and/or a fee. Request permissions from permissions@acm.org.<br />KDD’14, August 24–27, 2014, New York, NY, USA.<br />Copyright is held by the owner/author(s). Publication rights licensed to ACM.<br />ACM 978-1-4503-2956-9/14/08 ...$15.00.<br />http://dx.doi.org/10.1145/2623330.2623756.<br />with the data. A substantial improvement in this context<br />was provided by [22] who exploited sparsity to decompose<br />the collapsed sampler [9] for Latent Dirichlet Allocation. As<br />a result the sampling cost can be reduced from O(k), the<br />total number of topics to O(kd+ kw), i.e. the number kw of<br />topics occurring for a particular word w and kd for a partic-<br />ular document d. This insight led to an order of magnitude<br />improvement for sampling topic models, thus making their<br />implementation feasible at a large scale. In fact, the strat-<br />egy is sufficiently robust that it can be extended to settings<br />where the topic smoother depends on the words [15].<br />For small datasets the assumption kd+kw ? k is well sat-<br />isfied. Unfortunately, as the number of documents grows, so<br />does the number of topics in which a particular word occurs.<br />In particular kw → k, since the probability of observing any<br />particular topic for a given word is rarely nonzero: Assume<br />that the probability of occurrence for a given topic for a<br />word is bounded from below by δ. Then the probability of<br />the topic occurring at least once in a collection of n docu-<br />ments is given by<br />1 − (1 − δ)n≥ 1 − e−nδ→ 1 for n → ∞.<br />From this it follows that kw = O(k) for n = O(δ−1logk).<br />In other words, for large numbers documents the efficiencies<br />discussed in [22] vanish. This is troubling, since in many in-<br />dustrial settings n can be in the order of billions to trillions.<br />Consequently, with increasing amounts of data, the time to<br />process individual documents increases due to loss of spar-<br />sity, thus leading to a superlinear increase in runtime.<br />On the other hand, the topic-sparsity for a given document<br />essentially remains unchanged, regardless of the total num-<br />ber of related documents that are available. This is due to<br />the fact that the number of tokens per document is typically<br />less than O(k). For instance, microblogs contain only dozens<br />of words, yet admit to thousands of topics.1This situation<br />is exacerbated when it comes to hierarchical and structured<br />topic models, since there the number of (sub)topics can grow<br />considerably more rapidly. Hence the use of sparsity is cru-<br />cial in designing efficient samplers.<br />1.2Metropolis-Hastings-Walker Sampling<br />The present paper proposes a new decomposition of the<br />collapsed conditional probability, in conjunction with a<br />1Obviously, this approach would not work to infer topics for<br />Dostojevski’s War and Peace. That said, a plain topic model<br />is an unlikely candidate to represent very long documents.</p>  <p>Page 2</p> <p>Metropolis-Hastings [7] scheme and the use of the alias<br />method, introduced by Walker [20, 13], to amortize dense<br />updates for random variables. This method is highly ver-<br />satile. It defers corrections to the model and avoids renor-<br />malization. This allows us to apply it to both flat and hier-<br />archical models. Experimental evaluation demonstrates the<br />efficacy of our approach, yielding orders of magnitude accel-<br />eration and a simplified algorithm.<br />While we introduce our algorithm in the context of topic<br />models, it is entirely general and applies to a much richer<br />class of models. At its heart lies the insight that in many<br />inference problems the model parameters only change rela-<br />tively slowly during sampling. For instance, the location of<br />cluster centers, the definition of topics, or the shape of au-<br />toregressive functions, only change relatively slowly. Hence,<br />if we could draw from a distribution over k outcomes k times,<br />Walker’s alias method would allow us to generate samples in<br />amortized constant time. At the same time, the Metropolis<br />Hastings algorithm allows us to use approximations of the<br />correct probability distribution, provided that we compute<br />ratios between successive states correctly. Our approach is<br />to draw from the stale distribution in constant time and to<br />accept the transition based on the ratio between successive<br />states. This step takes constant time. Moreover, the pro-<br />posal is independent of the current state. Once k samples<br />have been drawn, we simply update the alias table. In honor<br />of the constitutent algorithms we refer to our technique as<br />the Metropolis Hastings Walker (MHW) sampler.<br />2.TOPIC MODELS<br />We begin with a brief introduction to topic models and the<br />associated inference problems. This includes a short motiva-<br />tion of sampling schemes in the context collapsed samplers<br />[9, 18] and of stochastic variational models [21]. It is followed<br />by a description of extensions to hierarchical models.<br />2.1 Latent Dirichlet Allocation<br />In LDA [3] one assumes that documents are mixture dis-<br />tributions of language models associated with individual<br />topics. That is, the documents are generated following the<br />graphical model below:<br />for all i<br />for all d<br />for all k<br />αθd<br />zdi<br />wdi<br />ψk<br />β<br />For each document d draw a topic distribution θd from a<br />Dirichlet distribution with concentration parameter α<br />θd∼ Dir(α).(1)<br />For each topic t draw a word distribution from a Dirichlet<br />distribution with concentration parameter β<br />ψt ∼ Dir(β).(2)<br />For each word i ∈ {1...nd} in document d draw a topic<br />from the multinomial θd via<br />zdi∼ Discrete(θd).(3)<br />Draw a word from the multinomial ψzdivia<br />wdi∼ Discrete(ψzdi).(4)<br />The beauty of the Dirichlet-multinomial design is that the<br />distributions are conjugate. This means that the multino-<br />mial distributions θd and ψk can be integrated out, thus<br />allowing one to express p(w,z|α,β,nd) in closed-form [9].<br />This yields a Gibbs sampler to draw p(zdi|rest) efficiently.<br />The conditional probability is given by<br />p(zdi|rest) ∝(n−di<br />td<br />+ αt)(n−di<br />n−di<br />t<br />tw + βw)<br />+¯β<br />.(5)<br />Here the count variables ntd,ntw and nt denote the num-<br />ber of occurrences of a particular (topic,document) and<br />(topic,word) pair, or of a particular topic respectively. More-<br />over, the superscript ·−didenotes said count when ignoring<br />the pair (zdi,wdi). For instance, n−di<br />ing the (topic,word) combination at position (d,i). Finally,<br />¯β :=?<br />time since we have k nonzero terms in a sum that needs to be<br />normalized. [22] devised an ingenious strategy for exploiting<br />sparsity by decomposing terms into<br />tw is obtained when ignor-<br />wβw denotes the joint normalization.<br />At first glance, sampling from (5) appears to cost O(k)<br />p(zdi|rest) ∝ βw<br />αt<br />n−di<br />t<br />+¯β+ n−di<br />td<br />βw<br />n−di<br />t<br />+¯β+ n−di<br />tw<br />n−di<br />td<br />n−di<br />t<br />+ αt<br />+¯β<br />As can be seen, for small collections of documents only the<br />first term is dense, and more specifically,?<br />is, whenever both ntd and ntw are sparse, sampling from<br />p(zdi|rest) can be accomplished efficiently. The use of packed<br />index variables and a clever reordering of (topic,count) pairs<br />further improve efficient sampling to O(kw+ kd).<br />Stochastic variational inference [11] requires an analogous<br />sampling step. The main difference being that rather than<br />usingntw+βw<br />nt+¯β<br />to capture p(w|t) one uses a natural parameter<br />ηtw associated with the conjugate variational distribution.<br />Unfortunately this renders the model dense, unless rather<br />careful precautions are undertaken [11] to separate residual<br />dense and sparse components.<br />Instead, we devise a sampler to draw from p(zdi|rest) in<br />amortized O(kd) time. We accomplish this by using<br />tαt/(n−di<br />t<br />+¯β)<br />can be computed from?<br />tαt/(nt+¯β) in O(1) time. That<br />p(zdi|rest) ∝ n−di<br />td<br />n−di<br />tw + βw<br />n−di<br />t<br />+¯β<br />+αt(n−di<br />tw + βw)<br />n−di<br />t<br />+¯β<br />(6)<br />Here the first term is sparse in kd and we can draw from it<br />in O(kd) time. The second term is dense, regardless of the<br />number of documents (this holds true for stochastic varia-<br />tional samplers, too). However, the ’language model’ p(w|t)<br />does not change too drastically whenever we resample a sin-<br />gle word. The number of words is huge, hence the amount of<br />change per word is concomitantly small. This insight forms<br />the basis for applying Metropolis-Hastings-Walker sampling.<br />2.2Poisson Dirichlet Process<br />To illustrate the fact that the MHW sampler also works<br />with models containing a dense generative part, we describe<br />its application to the Poisson Dirichlet Process [4, 16]. The<br />model is given by the following variant of the LDA model:</p>  <p>Page 3</p> <p>for all i<br />for all d<br />for all k<br />αθd<br />zdi<br />wdi<br />ψt<br />ψ0<br />β<br />In a conventional topic model the language model is sim-<br />ply given by a multinomial draw from a Dirichlet distribu-<br />tion. This fails to exploit distribution information between<br />topics, such as the fact that all topics have the same common<br />underlying language. A means for addressing this problem<br />is to add a level of hierarchy to model the distribution over<br />ψt via?<br />The ingredients for a refined language model are a Pitman-<br />Yor Topic Model (PYTM) [17] that is more appropriate to<br />deal with natural languages. This is then combined with<br />the Poisson Dirichlet Process (PDP) [16, 4] to capture the<br />fact that the number of occurences of a word in a natu-<br />ral language corpus follows power-law. Within a corpus, the<br />frequency of a word is approximately inversely proportional<br />to its ranking in number of occurences. Each draw from a<br />Poisson Dirichlet Process PDP(b,a,ψ0) is a probability dis-<br />tribution. The base distribution ψ0 defines the common un-<br />derlying distribution shared across the generated distribu-<br />tions. Under the settings of Pitman-Yor Topic Model, each<br />topic defines a distribution over words, and the base dis-<br />tribution defines the common underlying common language<br />model shared by the topics. The concentration parameter<br />b controls how likely a word is to occur again while being<br />sampled from the generated distribution. The discount pa-<br />rameter a prevents a word to be sampled too often by im-<br />posing a penalty on its probability based on its frequency.<br />The combined model described explicityly in [5]:<br />tp(ψt|ψ0)p(ψ0|β) rather than?<br />tp(ψt|β). Such a<br />model is depicted above.<br />θd∼ Dir(α)<br />zdi∼ Discrete(θd)<br />wdi∼ Discrete(ψzdi)<br />As can be seen, the document-specific part is identical to<br />LDA whereas the language model is rather more sophisti-<br />cated. Likewise, the collapsed inference scheme is analogous<br />to a Chinese Restaurant Process [6, 5]. The technical diffi-<br />culty arises from the fact that we are dealing with distribu-<br />tions over countable domains. Hence, we need to keep track<br />of multiplicities, i.e. whether any given token is drawn from<br />βi or β0. This will require the introduction of additional<br />count variables in the collapsed inference algorithm.<br />Each topic is equivalent to a restaurant. Each token in the<br />document is equivalent to a customer. Each type of word<br />corresponds each type of dish served by the restaurant. The<br />same results in [6] can be used to derive the conditional<br />probability by introducing axillary variables:<br />ψ0 ∼ Dir(β)<br />ψt ∼ PDP(b,a,ψ0)<br />• stw denotes the number of tables serving dish w in<br />restaurant t. Here t is the equivalent of a topic.<br />• rdi indicates whether wdi opens a new table in the<br />restaurant or not (to deal with multiplicities).<br />• mtw denotes the number of times dish w has been<br />served in restaurant t (analogously to nwk in LDA).<br />The conditional probability is given by:<br />p(zdi= t,rdi= 0|rest) ∝αt+ ndt<br />bt+ mt<br />mtw+ 1 − stw<br />mtw+ 1<br />Smtw+1<br />stw,at<br />Smtw<br />stw,at<br />(7)<br />if no additional ’table’ is opened by word wdi. Otherwise<br />p(zdi= t,rdi= 1|rest)<br />∝(αt+ ndt)bt+ atst<br />(8)<br />bt+ mt<br />stw+ 1<br />mtw+ 1<br />γ + stw<br />¯ γ + st<br />Smtw+1<br />stw+1,at<br />Smtw<br />stw,at<br />Here SN<br />M,ais the generalized Stirling number. It is given by<br />SN+1<br />M,a= SN<br />M−1,a+ (N − Ma)SN<br />M,aand SN<br />M,a= 0<br />for M &gt; N, and SN<br />[4]. Moreover we have mt =?<br />these two expressions can be written as a combination of<br />a sparse term and a dense term, simply by splitting the<br />factor (αt+ndt) into its sparse component ndtand its dense<br />counterpart αt. Hence we can apply the same strategy as<br />before when sampling topics from LDA, albeit now using a<br />twice as large space of state variables.<br />2.3Hierarchical Dirichlet Process<br />To illustrate the efficacy and generality of our approach we<br />discuss a third case where the document model itself is more<br />sophisticated than a simple collapsed Dirichlet-multinomial.<br />We demonstrate that there, too, inference can be performed<br />efficiently. Consider the two-level topic model based on the<br />Hierarchical Dirichlet Process [19] (HDP-LDA). In it, the<br />topic distribution for each document θd is drawn from a<br />Dirichlet process DP(b1,θ0). In turn, θ0 is drawn from a<br />Dirichlet process DP(b0,H(·)) governing the distribution<br />over topics. In other words, we add an extra level of hierar-<br />chy on the document side (compared to the extra hierarchy<br />on the language model used in the PDP).<br />0,a= δN,0. A detailed analysis is given in<br />wmtw, and st =?<br />tstw.<br />Similar to the conditional probability expression in LDA,<br />for all i<br />for all d<br />for all k<br />Hθ0<br />θd<br />zdi<br />wdi<br />ψk<br />β<br />More formally, the joint distribution is as follows:<br />θ0 ∼ DP(b0,H(·))<br />θd∼ DP(b1,θ0)<br />zdi∼ Discrete(θd)<br />wdi∼ Discrete(ψzdi)<br />By construction, DP(b0,H(·)) is a Dirichlet Process, equiva-<br />lent to a Poisson Dirichlet Process PDP(b0,a,H(·)) with the<br />discount parameter a set to 0. The base distribution H(.) is<br />often assumed to be a uniform distribution in most cases.<br />At first, a base θ0 is drawn from DP(b0,H(·)). This gov-<br />erns how many topics there are in general, and what their<br />overall prevalence is. The latter is then used in the next level<br />of the hierarchy to draw a document-specific distribution θd<br />that serves the same role as in LDA. The main difference is<br />that unlike in LDA, we use θ0 to infer which topics are more<br />popular than others.<br />It is also possible to extend the model to more than two<br />levels of hierarchy, such as the infinite mixture model [19].<br />Similar to Poisson Dirichlet Process, an equivalent Chinese<br />Restaurant Franchise analogy [6, 19] exists for Hierarchi-<br />cal Dirichlet Process with multiple levels. In this analogy,<br />each Dirichlet Process is mapped to a single Chinese Restau-<br />ψt ∼ Dir(β)</p>  <p>Page 4</p> <p>rant Process, and the hierarchical structure is constructed<br />to identify the parent and children of each restaurant.<br />The general (collapsed) structure is as follows: let Nj be<br />the total number of customers in restaurant j and njt be<br />the number of customers in restaurant j served with dish<br />t. When a new customer (a token) enters restaurant j with<br />the corresponding Dirichlet Process DP(bj,Hj(·)), there are<br />two types of seating arrangement for the customer:<br />• With probability<br />dish (topic) t and sits at an existing table.<br />• With probability<br />table served with a new dish t drawn from Hj(·).<br />In the event that the customer sits at a new table, a phan-<br />tom customer is sent upstream the hierarchy to the parent<br />restaurant of j, denoted by j?, with corresponding Dirichlet<br />Process DP(bj?,Hj?(·)). The parent restaurant then decides<br />the seating arrangement of the phantom customer under the<br />same rules. This process repeats, until there is no more par-<br />ent restaurant or any of phantom customer decides to sit in<br />an existing table in any parent restaurant along the path.<br />We use the block Gibbs sampler given in [6] as it allows us<br />to extend our approach for multi-level Hierarchical Dirichlet<br />Process, and performs better than the samplers given in [19]<br />and the collapsed Gibbs sampler given in [4], as measured<br />in convergence speed, running time, and topic quality.<br />The key difference of [6] relative to [19] is that rather<br />than keeping track of relative assignments of tables to each<br />other (and the resulting multiplicities and infrequent block<br />moves) it simply keeps track of the level within the hierarchy<br />of restaurants at which an individual customer opens a new<br />table. The advantage is that this allows us to factor out<br />the relative assignment of customers to specific tables but<br />rather only keep track of the dishes that they consume. The<br />obvious downside being that a small number of customers<br />can be blocked from moves if they opened a table at a high<br />position of the hierarchy that other customers depend upon.<br />Improving mixing in this context is subject to future work.<br />In the setting studied above we only have a two-level HDP:<br />that of the parent DP tying all documents together and the<br />DP within each document, governing its topic distribution.<br />We use zdi ∈ N to denote the topic indicator of word i at<br />position d and udi ∈ {0,1} to indicate whether a new table<br />is opened at the root level (i.e. udi = 1). Moreover, define<br />std to be the table counter for document d, i.e. the number<br />of times a table serving topic t has been opened, and let st<br />be the associated counter for the base DP, associated with<br />tables opened at the parent level. Finally, let s :=?<br />Clearly the situation where st = 0 and udi= 0 is impossi-<br />ble since this would imply that we are opening a new table<br />at document d while there is no matching table available at<br />the root level. Hence for the collapsed sampler we only need<br />to consider the following cases:<br />njt<br />bj+Njthe customer is served with<br />bj<br />bj+Njthe customer sits at a new<br />tst be<br />the total number of tables opened at the root level.<br />• A new root topic is generated. That is, we currently<br />have st and need to set udi= 1.<br />• A new table is added at document d. In this case we<br />require that st, i.e. that the topic exists at the root<br />level. Moreover, obviously it requires that std= 0 since<br />we are adding the first table to serve dish t.<br />• No additional topic is introduced but we may be in-<br />troducing an additional table.<br />This amounts to the following (unnormalized) conditional<br />probabilities. See [6] for further details.<br />p(zdi= t,udi= u|rest)<br /><br /><br /><br />Expressions for the generalized form are analogous. Both<br />forms contain a fraction with its numerator being the sum<br />of a sparse term mtw and a dense term βw. Therefore, the<br />conditional probability can be decomposed to a dense term<br />multiplied by βw, and a sparse term multiplied by mtw. Ap-<br />plying the same methodology, the sampling complexity of a<br />multi-level HDP can be reduced to O(kw).<br />(9)<br />∝βw+ mtw<br />¯β + mt<br /><br /><br /><br /><br /><br /><br /><br /><br /><br /><br />b0b1<br />b0+s<br />if st = 0 and udi= 1<br />b1s2<br />t<br />(st+1)(s+b0)<br />if st ?= 0 and std= 0<br />Sndt+1<br />sdt+1,0<br />Sndt<br />sdt,0<br />sdt+1<br />ndk+1<br />if st ?= 0 and std?= 0<br />3.METROPOLIS-HASTINGS-WALKER<br />We now introduce the key components for the MHW al-<br />gorithm and how to use it in sampling topics. They consist<br />of the alias method [20, 13] and a simplified version of the<br />Metropolis-Hastings sampler [7].<br />3.1Walker’s Alias Method<br />Typically, when drawing from a distribution over l out-<br />comes, it is accepted that one would need to perform O(l)<br />work to generate a sample. In fact, this is a lower bound,<br />since we need to inspect each probability at least once before<br />we can construct the sampler. However, what is commonly<br />overlooked is that there exist algorithms that allow us to<br />draw subsequent samples from the same distribution in O(1)<br />time. This means that drawing l samples from a distribution<br />over l outcomes can be accomplished in O(1) amortized time<br />per draw. We make extensive use of this fact.<br />Denote by piwith i ∈ {1...l} the probabilities of a distri-<br />bution over l outcomes from which we would like to sample.<br />The algorithm works by decomposing a distribution over l<br />events into l bins of equal probability by pairing at most two<br />events per bin. Since it ’robs’ from the probabilities pi &gt; 1/l<br />and adds to those with pi &lt; 1/l it is also referred to as ’Robin<br />Hood’ method [13]. The algorithm proceeds as follows:<br />1: GenerateAlias(p,l)<br />2: Initialize L = H = ∅ and A = [].<br />3: for i = 1 to l do<br />4:if pi ≤ l−1then<br />5:L ← L ∪ {(i,pi)}<br />6: else<br />7:H ← H ∪ {(i,pi)}<br />8:end if<br />9: end for<br />10: while L ?= ∅ do<br />11: Extract (i,pi) from L and (h,ph) from H<br />12:A ← [A,(i,h,pi)]<br />13:if ph− pi &gt; l−1then<br />14:H ← H ∪ {(h,ph− pi)}<br />15: else<br />16:L ← L ∪ {(h,ph− pi)}<br />17:end if<br />18: end while<br />19: return A</p>  <p>Page 5</p> <p>This yields an array A containing triples (i,h,ph) with<br />ph &lt; l−1. It runs in O(l) time since at each step one event<br />is removed from the list. And the probabilities remain un-<br />changed, as can be seen by induction. All we need to do now<br />is to draw a random element from A and flip a biased coin<br />to accept h or i with probability lphand 1−lphrespectively.<br />1: SampleAlias(A,l)<br />2: bin = RandInt(l)<br />3: (i,h,p) = A[bin]<br />4: if lp &gt; RandUnif(1) then<br />5: return h<br />6: else<br />7:return i<br />8: end if<br />Note that the alias method works since we are implicitly<br />exploiting parallelism inherent in CPUs: as long as l does<br />not exceed 264are guaranteed that even an information the-<br />oretically inefficient code will not require more than 64 bit,<br />which can be generated in constant time.<br />3.2Sampling with Proposal Distributions<br />Whenever we draw l identical samples from p it is clear<br />that the above algorithm provides an O(1) sampler. How-<br />ever, if p changes, it is difficult to apply the alias sampler<br />directly. To address this, we use rejection sampling and<br />Metropolis-Hastings procedures. Rejection sampling pro-<br />ceeds as follows:<br />1: Rejection(p,q,c)<br />2: repeat<br />3:Draw i ∼ q(i)<br />4: until p(i) ≥ cq(i)RandUnif(1)<br />5: return i<br />Here p is the distribution we would like to draw from, q is a<br />reference distribution that makes sampling easy, and c ≥ 1<br />is chosen such that cq(i) ≥ p(i) for all i. We then accept with<br />probability<br />of samples to draw via Rejection(p,q,c) is c, provided that<br />a good bound c exists. In this case we have the following:<br />p(i)<br />cq(i). It is well known that the expected number<br />Lemma 1 Given l distributions pi and q over l outcomes<br />satisfying ciq ≥ pi, the expected amortized runtime complex-<br />ity for drawing using SampleAlias(A,l) and rejecting using<br />Rejection(pi,q,ci) is given by O<br />?<br />1<br />l<br />?l<br />i=1ci<br />?<br />.<br />Proof. Preprocessing costs amortized O(1) time. Each<br />rejection sampler costs O(ci) work. Averaging over the draws<br />proves the claim.<br />In many cases, unfortunately, we do not know ci, or comput-<br />ing ci is essentially as costly as drawing from pi itself. More-<br />over, in some cases cimay be unreasonably large. In this sit-<br />uation we resort to Metropolis Hastings sampling [7] using<br />a stationary proposal distribution. As in rejection sampling,<br />we use a proposal distribution q and correct the effect of sam-<br />pling from the ’wrong’ distribution by a subsequent accep-<br />tance step. The main difference is that Metropolis Hastings<br />can be considerably more efficient than Rejection sampling<br />since it only requires that the ratios of probabilities are close<br />rather than requiring knowledge of a uniform upper bound<br />on the ratio. The drawback is that instead of drawing iid<br />samples from p we end up with a chain of dependent sam-<br />ples from p, as governed by q.<br />For the purpose of the current method we only need to<br />concern ourselves with stationary distributions p and q, i.e.<br />p(i) = p(i|j) and q(i) = q(i|j), hence we only discuss this<br />special case below. For a more general discussion see e.g. [8].<br />1: StationaryMetropolisHastings(p,q,n)<br />2: if no initial state exists then i ∼ q(i)<br />3: for l = 1 to n do<br />4: Draw j ∼ q(j)<br />5: if RandUnif(1) &lt; min<br />6:i ← j<br />7: end if<br />8: end for<br />9: return i<br />As a result, provided that p and q are sufficiently similar, the<br />sampler accepts most of the time. This is the case, e.g. when-<br />ever we use a stale variant of p as the proposal q. Obviously,<br />a necessary requirement is that q(i) &gt; 0 whenever p(i) &gt; 0,<br />which holds, e.g. whenever we incorporate a smoother.<br />3.3MHW Sampling<br />In combining both methods we arrive at, what we believe<br />is a significant improvement over each component individu-<br />ally. It works as follows:<br />1: Initialize A ← GenerateAlias(p,l)<br />2: for i = 1 to<br />3:Update q as needed<br />4:Sample j ∼ StationaryMetropolisHastings(p,A,n)<br />5: end for<br />Provided that the sampler mixes within n rounds of the MH-<br />procedure, this generates draws from up-to-date versions of<br />p. Note that a further improvement is possible whenever we<br />can start with a more up-to-date draw from p, e.g. in the<br />case of revisiting a document in a topic model. After burn-in<br />the previous topic assignment for a given word is likely to<br />be still pertinent for the current sampling pass.<br />?<br />1,p(j)q(i)<br />p(i)q(j)<br />?<br />then<br />N<br />ndo<br />Lemma 2 If the Metropolis Hastings sampler over N out-<br />comes using q instead of p mixes well in n steps, the amor-<br />tized cost of drawing n samples from q is O(n) per sample.<br />This follows directly from the construction of the sampler<br />and the fact that we can amortize generating the alias table.<br />Note that by choosing a good starting point and after burn-<br />in we can effectively set n = 1.<br />4.APPLICATIONS<br />We now have all components necessary for an accelerated<br />sampler. The trick is to recycle old values for p(wdi|zdi) even<br />when they change slightly and then to correct this via a<br />Metropolis-Hastings scheme. Since the values change only<br />slightly, we can therefore amortize the values efficiently. We<br />begin by discussing this for the case of ’flat’ topic models<br />and extend it to hierarchical models subsequently.<br />4.1Sampler for LDA<br />We now design a proposal distribution for (6). It involves<br />computing the document-specific sparse term exactly and<br />approximating the remainder with slightly stale data. Fur-<br />thermore, to avoid the need to store a stale alias table A,<br />we simply draw from the distribution and keep the samples.<br />Once this supply is exhausted we compute a new table.</p>  <p>Page 6</p> <p>Alias table: Denote by<br />Qw :=<br />?<br />t<br />αtntw+ βw<br />nt+¯β<br />and qw(t) :=<br />αt<br />Qw<br />ntw+ βw<br />nt+¯β<br />the alias normalization and the associated probability dis-<br />tribution. Then we perform the following steps:<br />1. Generate the alias table A using qw.<br />2. Draw k samples from qw and store them in Sw.<br />3. Discard A and only retain Qw and the array Sw.<br />Generating Sw and computing Qw costs O(k) time. In par-<br />ticular, storage of Sw requires at most O(k log2k) bits, thus<br />it is much more compact than A. Note, however, that we<br />need to store Qw and qw(t).<br />Metropolis Hastings proposal: Denote by<br />Pdw:=<br />?<br />t<br />n−di<br />td<br />n−di<br />n−di<br />t<br />tw + βw<br />+¯β<br />and pdw(t) :=n−di<br />td<br />Pdw<br />n−di<br />n−di<br />t<br />tw + βw<br />+¯β<br />the sparse document-dependent topic contribution. Com-<br />puting it costs O(kd) time. This allows us to construct a<br />proposal distribution<br />q(t) :=<br />Pdw<br />Pdw+ Qwpdw(t) +<br />Qw<br />Pdw+ Qwqw(t)(10)<br />To perform an MH-step we then draw from q(t) in O(kd)<br />amortized time. The step from topic s to topic t is accepted<br />with probability min(1,π) where<br />π =n−di<br />n−di<br />td<br />sd+ αs<br />+ αt<br />·n−di<br />n−di<br />tw + βw<br />sw + βw<br />·n−di<br />n−di<br />t<br />s<br />+¯β<br />+¯β·Pdwpdw(s) + Qwqw(s)<br />Pdwpdw(t) + Qwqw(t)<br />Note that the last fraction effectively removes the normal-<br />ization in pdw and qw respectively, that is, we take ratios of<br />unnormalized probabilities.<br />Complexity: To draw from q costs O(kd) time. This is so<br />since computing Pdw has this time complexity, and so does<br />the sampler for pdw. Moreover, drawing from qw(t) is O(1),<br />hence it does not change the order of the algorithm. Note<br />that repeated draws from q are only O(1) since we can use<br />the very same alias sampler also for draws from pdw. Finally,<br />evaluating π costs only O(1) time. We have the following:<br />Lemma 3 Drawing up to k steps in a Metropolis-Hastings<br />proposal from p(zdi|rest) can be accomplished in O(kd) amor-<br />tized time per sample and O(k) space.<br />4.2Sampler for the Poisson Dirichlet Process<br />Following the same steps as above, the basic Poisson<br />Dirichlet Process topic model can be decomposed by ex-<br />ploiting the sparsity of ndt. The main difference to before is<br />that we need to account for the auxiliary variable r ∈ {0,1}<br />rather than just the topic indicator t. The alias table is:<br /><br /><br />Qw :=qw(t,r)<br />qw(t,r) :=<br /><br /><br />r,t<br /><br />?<br />αk<br />bt+mtw<br />mtw−stw+1<br />mtw+1<br />Smtw+1<br />stw,at<br />Smtw<br />stw,at<br />if r = 1<br />αkbt+atst<br />bt+mt<br />stw+1<br />mtw+1<br />β+stw<br />¯β+st<br />Smtw+1<br />stw+1,at<br />Smtw<br />stw,at<br />otherwise<br />Likewise, the sparse document-specific contribution is<br /><br /><br />Pdw:=pdw(t,r)<br />pdw(t,r) :=ndt<br /><br /><br /><br />1<br />bt+mt<br />mtw−stw+1<br />mtw+1<br />Smtw+1<br />stw,at<br />Smtw<br />stw,at<br />if r = 1<br />bt+atstw<br />bt+mtw<br />stw+1<br />mtw+1<br />β+stw<br />¯β+st<br />Smtw+1<br />stw+1,at<br />Smtw<br />stw,at<br />otherwise<br />?<br />r,t<br />As previously, computing pdw(t,r) only costs O(kd) time,<br />which allows a proposal distribution very similar to the case<br />in LDA to be constructed:<br />Pdw<br />Pdw+ Qwpdw(t,r) +<br />q(t,r) :=<br />Qw<br />Pdw+ Qwqw(t,r)<br />As before, we use a Metropolis-Hastings sampler, although<br />this time for the state pair (s,t) → (s?,t?) and accept as<br />before by using the ratio of current and stale probabilities<br />(the latter given by q). As before in the context of LDA, the<br />time complexity of this sampler is amortized O(kd).<br />4.3Sampler for the HDP<br />Due to slight differences in the nature of the sparse term<br />and the dense term, we demonstrate the efficacy of our ap-<br />proach for sparse language models here. That is, we show<br />that whenever the document model is dense but the lan-<br />guage model sparse, our strategy still applies. In other<br />words, this sampler works at O(kw) cost which is beneficial<br />for infrequent words.<br />For brevity, we only discuss the derivation for the two level<br />HDP-LDA, given that the general multi-level HDP can be<br />easily extended from the derivation. Recall (9). Now the alias<br />table is now given by:<br />qw(t,u) :=p(zdi= t,udi= u|rest)?¯β + mt<br />Qw :=qw(t,u)<br />?βw<br />?<br />t,u<br />and the exact term is given by<br />pdw(t,u) :=γwp(zdi= t,udi= u|rest)(¯ γ + mt)mtw<br />Pdw:=pdw(t,u)<br />?<br />t,u<br />As before, we engineer the proposal distribution to be a com-<br />bination of stale and fresh counts. It is given by<br />q(t,u) :=<br />Pdw<br />Pdw+ Qwpdw(t,u) +<br />Qw<br />Pdw+ Qwqw(t,u)<br />Subsequently, the state transition (t,u) → (t?,u?) is accepted<br />using straightforward Metropolis-Hastings acceptance ra-<br />tios. We omitted the subscript wdi= w for brevity. The same<br />argument as above shows that the time complexity of our<br />sampler for drawing from HDP-LDA is amortized O(kw).<br />5.EXPERIMENTS<br />To demonstrate empirically the performance of the alias<br />method we implemented the aforementioned samplers in<br />both their base forms that have O(k) time complexity,<br />as well as our alias variants which have amortized O(kd)<br />time complexity. In addition to this, we implemented the<br />SparseLDA [22] algorithm with the full set of features includ-<br />ing the sorted list containing a compact encoding of ntw and<br />ndt, as well as dynamic O(1) update of bucket values. Be-<br />yond the standard implementation provided in MalletLDA</p>  <p>Page 7</p> <p>RedState<br />(4.5s per LDA iteration)<br />GPOL<br />(36s per LDA iteration)<br />Enron<br />(85s per LDA iteration)<br />Figure 1: Runtime time comparison between LDA, HDP, PDP and their Alias sampled counterparts<br />AliasLDA, AliasHDP and AliasPDP.<br />Figure 2: Perplexity as a function of runtime (in sec-<br />onds) for PDP, AliasPDP, HDP, and AliasHDP on<br />GPOL (left) and Enron (right).<br />Figure 3: Runtimes of SparseLDA and AliasLDA on<br />PubMedSmall (left) and NyTimes (right).<br />by [22], we made two major improvements: we accelerated<br />the sorting algorithm for the compact list of encoded values<br />to amortized O(1); and we avoided hash maps which sub-<br />stantially improved the speed in general with small sacrifice<br />of memory efficiency (we need an inverted list of the indices<br />and an inverted list of the indices of the inverted lists).<br />In this section these implementations will be referred as<br />LDA which is O(k), SparseLDA which is O(kw + kd),</p>  <p>Page 8</p> <p>AliasLDA which is O(kd), PDP at O(k) [5], AliasPDP<br />at O(kd), HDP at O(k) [6], and AliasHDP at O(kw).<br />5.1 Environment and Datasets<br />All our implementations are written in C++11 in a way<br />that maximise runtime speed, compiled with gcc 4.8 with<br />-O3 compiler optimisation in amd64 architecture. All our ex-<br />periments are conducted on a laptop with 12GB memory<br />and an Intel i7-740QM processor with 1.73GHz clock rate,<br />4×256KB L2 Cache and 6MB L3 Cache. Furthermore, we<br />only use one single sampling thread across all experiments.<br />Therefore, only one CPU core is active throughout and only<br />256KB L2 cache is available. We further disabled Turbo<br />Boost to ensure all experiment are run at exactly 1.73GHz<br />clock rate. Ubuntu 13.10 64bit served as runtime.<br />We use 5 datasets with a variety in sizes, vocabulary<br />length, and document lengths for evaluation, as shown in Ta-<br />ble 1 . RedState dataset contains American political blogs<br />crawled from redstate.com in year 2011. GPOL contains a<br />subset of political news articles from Reuters RCV1 collec-<br />tion.2We also included the Enron Email Dataset,3. NY-<br />Times contains articles published by New York Times be-<br />tween year 1987 and 2007. PubMedSmall is a subset of ap-<br />proximately 1% of the biomedical literature abstracts from<br />PubMed. Stopwords are removed from all datasets. Further-<br />more, words occurring less than 10 times are removed from<br />NYTimes, Enron, and PubMedSmall. NYTimes, En-<br />ron, and PUBMED datasets are available at [1].<br />5.2Evaluation Metrics and Parameters<br />We evaluate the algorithms based on two metrics: the<br />amount of time elapsed for one Gibbs sampling iteration<br />and perplexity. The perplexity is evaluated after every 5 it-<br />erations, beginning with the first evaluation at the ending<br />of the first Gibbs sampling iteration. We use the standard<br />held-out method [10] to evaluate test perplexity, in which a<br />small set of test documents originating from the same collec-<br />tion is set to query the model being trained. This produces<br />an estimate of the document-topic mixtures˜θdtfor each test<br />document d. From there the perplexity is then evaluated as:<br />?<br />d=1d=1<br />?<br />Here we obtain the estimate of p(wi = w|zdi= t,rest) from<br />the model being trained. To avoid effects due to variation in<br />the number of topics, we hardcoded k = 1024 for all experi-<br />ments except one (GPOL) where we vary k and observe the<br />effect on speed per iteration. We use fixed values for hyper-<br />parameters in all our models, setting α = β = 0.1 for LDA,<br />a = 0.1, b = 10, and γ = 0.1 for the PDP, and b0 = b1 = 10,<br />γ = 0.1 for the HDP. For alias implementations, we fix the<br />number of Metropolis-Hasting sampling steps at 2, as we<br />observed a satisfactory acceptance rate (over 90%) at these<br />settings. Only a negligible improvement in perplexity was<br />observed by raising this value. Furthermore, we did not ob-<br />serve degraded topic quality even when Metropolis-Hasting<br />π(W|rest) :=<br />D<br />?<br />nd<br />Nd<br />?−1<br />D<br />?<br />logp(wd|rest) where<br />p(wd|rest) =<br />i=1<br />k<br />?<br />t=1<br />p(wi = w|zdi= t,rest)p(zdi= t|rest)<br />2Reuters Vol. 1, English language, 1996-08-20 to 1997-08-19<br />3Source: www.cs.cmu.edu/˜enron<br />sampling step was reduced to n = 1, and in all our experi-<br />ments the perplexity almost perfectly converges at the same<br />pace (i.e. along number of iterations) with the same algo-<br />rithm without applying alias method (albeit with much less<br />time per iteration).<br />Dataset<br />RedState<br />GPOL<br />Enron<br />PubMedSmall<br />NYTimes<br />VLDTL/V<br />26.21<br />35.9<br />218<br />337<br />970<br />L/D<br />157<br />183<br />165<br />66<br />331<br />12,272<br />73,444<br />28,099<br />106,797<br />101,636<br />321,699<br />2,638,750<br />6,125,138<br />35,980,539<br />98,607,383<br />2,045<br />14,377<br />36,999<br />546,665<br />297,253<br />231<br />1,596<br />2,860<br />2,002<br />2,497<br />Table 1: Datasets and their statistics. V: vocabulary<br />size; L: total number of training tokens, D: number<br />of training documents; T: number of test documents.<br />L/V is the average number occurrences of a word.<br />L/D is the average document length.<br />5.3 Performance Summary<br />Figure 6 shows the overall performance of perplexity as<br />a function of time elapsed when comparing SparseLDA vs<br />AliasLDA on the four larger datasets. When k is fixed<br />to 1024, substantial performance in perplexity over run-<br />ning time on all problems with the exception of the Enron<br />dataset, most likely due to its uniquely small vocabulary<br />size. The gap in performance is increased as the datasets<br />become larger and more realistic in size. The gain in per-<br />formance is noted in particular when the average document<br />length is smaller since our sampler scales with O(kd) which<br />is likely to be smaller for short documents.<br />Figure 2 gives the comparison between PDP, HDP and<br />their aliased variants on GPOL and Enron. By the time<br />AliasPDP and AliasHDP are converged, the straightforward<br />sampler are still at their first few iterations.<br />5.4 Performance as a Function of Iterations<br />In the following we evaluate the performance in two sep-<br />arate parts: perplexity as a function of iterations and run-<br />time vs. iterations. We first establish that the acceleration<br />comes at no cost of degraded topic quality, as shown in<br />Figure 6. The convergence speed and converged perplex-<br />ity of AliasLDA, AliasPDP, and AliasHDP almost perfectly<br />match the non-alias counterparts. This further shows that<br />our choice of relatively small number of Metropolis-Hasting<br />steps (2 per sample) is adequate.<br />The improved performance in running time of our alias<br />implementations can be seen in all phases of sampling when<br />compared to non-alias standard implementations (LDA,<br />PDP, HDP). When compared to SparseLDA (Figure 3),<br />the performance gain is salient during all phases on larger<br />datasets (except for the early stage in Enron dataset), and<br />the performance is very close on small datasets (0.35s per it-<br />eration on AliasLDA vs. 0.25s per iteration on SparseLDA).<br />As the size of the data grows AliasLDA outperforms<br />SparseLDA without degrading topic quality, reducing the<br />amount of time for each Gibbs iteration on NYTimes corpus<br />by around 12% to 38% overall, on Enron corpus by around<br />30% after 30 iterations, and on PubMedSmall corpus by<br />27%-60% throughout the first 50 iterations. Compared to<br />SparseLDA, the time required for each Gibbs iteration with<br />AliasLDA grows at a much slower rate, and the benefits of<br />reduced sampling complexity is particularly clear when the<br />average length of each document is small.</p>  <p>Page 9</p> <p>Figure 4: Comparison of SparseLDA and AliasLDA<br />on GPOL when varying the number of topics for<br />k ∈ {256,1024,2048,4096}.<br />Percentage of full PubMedSmall collection<br />Seconds per iteration<br />Figure 5: Average runtime per iteration when com-<br />pared on {10%,20%,40%,75%,100%} of the PubMedS-<br />mall dataset for SparseLDA and AliasLDA.<br />The gap in performance is especially large for more so-<br />phisticated language modelsl such as PDP and HDP. The<br />running time for each Gibbs iteration is reduced by 60% to<br />80% for PDP, and 80% to 95% for HDP, an order of magni-<br />tude on improvement.<br />5.5Varying the number of topics<br />When the number of topics k increases, the running time<br />for an iteration of AliasLDA increases at a much lower<br />rate than SparseLDA, as seen from Figure 4 on dataset<br />GPOL since kd is almost constant. Even though the gap<br />between SparseLDA and AliasLDA may seem insignificant<br />at k = 1024, it becomes very pronounced at k = 2048<br />(45% improvement) and at k = 4096 (over 100%) This con-<br />firms the observation above that shorter documents benefits<br />more from AliasLDA in the sense that the average docu-<br />ments length L/D relative to the number of topics k be-<br />comes“shorter”as k increases. This yields a more sparse ndt<br />and lower kd for a document d on average.<br />5.6 Varying the corpus size<br />Figure 5 demonstrates how the gap in running time speed<br />scales with growing number of documents in the same do-<br />main. We measure the average runtime for the first 50 Gibbs<br />iterations on 10%, 20%, 40%, 75%, and 100% of PubMedS-<br />mall dataset. The speedup ratio for each subset is at 31%,<br />34%, 37%, 41%, 43% respectively. In other words, it in-<br />creases with the amount of data, which conforms our in-<br />tuition that adding new documents increases the density of<br />ntw, thus slowing down the sparse sampler much more than<br />the alias sampler, since the latter only depends on kdrather<br />than kd+ kw.<br />Perplexity vs. Runtime<br />GPOL<br />Enron<br />PubMedSmall<br />NYTimes<br />Perplexity vs. Iterations<br />Figure 6: Perplexity as a function of runtime<br />(left) and number of iterations (right) for LDA,<br />SparseLDA, and LDA, PDP and HDP, both with<br />and without using the Alias method. We see consid-<br />erable acceleration at unchanged perplexity.<br />6.CONCLUSION<br />In this paper, we described an approach that effectively<br />reduces sampling complexity of topic models from O(k) to<br />O(kd) in general, and from O(kw+kd) (SparseLDA) to O(kd)<br />(AliasLDA) for LDA topic model. Empirically, we showed<br />that our approach scales better than existing state-of-the-<br />art method when the number of topics and the number of<br />documents become large. This enables many large scale ap-<br />plications, and many existing applications which require a</p>  <p>Page 10</p> <p>scalable distributed approach. In many industrial applica-<br />tions where the number of tokens easily reaches billions,<br />these properties are crucial and often desirable in design-<br />ing a scalable and responsive service. We also demonstrated<br />an order of magnitude improvement when our approach is<br />applied to complex models such as PDP and HDP. With an<br />order of magnitude gain in speed, PDP and HDP may be-<br />come much more appealing to many applications for their<br />superior convergence performance, and more sophisticated<br />representation of topic distributions and language models.<br />For k = 1024 topics the number of tokens processed per<br />second in our implementation is beyond 1 million for all<br />datasets except one (NYTimes), of which contains substan-<br />tially more lengthy documents. This is substantially faster<br />than many known implementations when measured in num-<br />ber of tokens processed per computing second per core, such<br />as YahooLDA [18], and GraphLab, given that we only utilise<br />a single thread on a single laptop CPU core.<br />Acknowledgments: This work was supported in part by<br />a resource grant from amazon.com, a Faculty Research Grant<br />from Google, and Intel.<br />7.REFERENCES<br />[1] K. Bache and M. Lichman. UCI machine learning<br />repository, 2013.<br />[2] D. Blei, T. Griffiths, and M. Jordan. The nested<br />chinese restaurant process and Bayesian<br />nonparametric inference of topic hierarchies. Journal<br />of the ACM, 57(2):1–30, 2010.<br />[3] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet<br />allocation. JMLR, 3:993–1022, Jan. 2003.<br />[4] W. Buntine and M. Hutter. A bayesian review of the<br />poisson-dirichlet process, 2010.<br />[5] C. Chen, W. Buntine, N. Ding, L. Xie, and L. Du.<br />Differential topic models. In IEEE Pattern Analysis<br />and Machine Intelligence, 2014.<br />[6] C. Chen, L. Du, and W. Buntine. Sampling table<br />configurations for the hierarchical poisson-dirichlet<br />process. In ECML, pages 296–311, 2011.<br />[7] J. Geweke and H. Tanizaki. Bayesian estimation of<br />state-space model using the metropolis-hastings<br />algorithm within gibbs sampling. Computational<br />Statistics and Data Analysis, 37(2):151–170, 2001.<br />[8] W. R. Gilks, S. Richardson, and D. J. Spiegelhalter.<br />Markov Chain Monte Carlo in Practice. 1995.<br />[9] T. Griffiths and M. Steyvers. Finding scientific topics.<br />PNAS, 101:5228–5235, 2004.<br />[10] G. Heinrich. Parameter estimation for text analysis.<br />Technical report, Fraunhofer IGD, 2004.<br />[11] M. Hoffman, D. M. Blei, C. Wang, and J. Paisley.<br />Stochastic variational inference. In ICML, 2012.<br />[12] W. Li, D. Blei, and A. McCallum. Nonparametric<br />bayes pachinko allocation. In UAI, 2007.<br />[13] G. Marsaglia, W. W. Tsang, and J. Wang. Fast<br />generation of discrete random variables. Journal of<br />Statistical Software, 11(3):1–8, 2004.<br />[14] R. M. Neal. Markov chain sampling methods for<br />Dirichlet process mixture models. University of<br />Toronto, Technical Report 9815, 1998.<br />[15] J. Petterson, A. Smola, T. Caetano, W. Buntine, and<br />S. Narayanamurthy. Word features for latent dirichlet<br />allocation. In NIPS, pages 1921–1929, 2010.<br />[16] J. Pitman and M. Yor. The two-parameter<br />poisson-dirichlet distribution derived from a stable<br />subordinator. A. of Probability, 25(2):855–900, 1997.<br />[17] I. Sato and H. Nakagawa. Topic models with<br />power-law using Pitman-Yor process. In KDD, pages<br />673–682. ACM, 2010.<br />[18] A. J. Smola and S. Narayanamurthy. An architecture<br />for parallel topic models. In PVLDB, 2010.<br />[19] Y. Teh, M. Jordan, M. Beal, and D. Blei. Hierarchical<br />dirichlet processes. JASA, 101(576):1566–1581, 2006.<br />[20] A. J. Walker. An efficient method for generating<br />discrete random variables with general distributions.<br />ACM TOMS, 3(3):253–256, 1977.<br />[21] C. Wang, J. Paisley, and D. M. Blei. Online<br />variational inference for the hierarchical Dirichlet<br />process. In Conference on Artificial Intelligence and<br />Statistics, 2011.<br />[22] L. Yao, D. Mimno, and A. McCallum. Efficient<br />methods for topic model inference on streaming<br />document collections. In KDD’09, 2009.</p>  <a href="https://www.researchgate.net/profile/Sujith_Ravi/publication/266660385_Reducing_the_sampling_complexity_of_topic_models/links/5583a5c508aefa35fe30c780.pdf">Download full-text</a> </div> <div id="rgw22_56ab9eb3b965f" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw23_56ab9eb3b965f">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw24_56ab9eb3b965f"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Sujith_Ravi/publication/266660385_Reducing_the_sampling_complexity_of_topic_models/links/5583a5c508aefa35fe30c780.pdf" class="publication-viewer" title="fastlda-kdd2014.pdf">fastlda-kdd2014.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Sujith_Ravi">Sujith Ravi</a> &middot; Jun 19, 2015 </span>   </div>   <div class="details"> Available from <a href="http://sravi.org/pubs/fastlda-kdd2014.pdf" target="_blank" rel="nofollow">sravi.org</a> </div>   </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw26_56ab9eb3b965f" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw27_56ab9eb3b965f">  </ul> </div> </div>   <div id="rgw18_56ab9eb3b965f" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw19_56ab9eb3b965f"> <div> <h5> <a href="publication/257319241_Reducing_the_Sampling_Complexity_of_Energy_Detection_in_Cognitive_Radio_Networks_under_Low_SNR_by_Using_the_Optimal_Stochastic_Resonance_Technique" class="color-inherit ga-similar-publication-title"><span class="publication-title">Reducing the Sampling Complexity of Energy Detection in Cognitive Radio Networks under Low SNR by Using the Optimal Stochastic Resonance Technique</span></a>  </h5>  <div class="authors"> <a href="researcher/8372861_Di_He" class="authors ga-similar-publication-author">Di He</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw20_56ab9eb3b965f"> <div> <h5> <a href="publication/2839732_Population_Size_and_Sampling_Complexity_in_Genetic_Algorithms" class="color-inherit ga-similar-publication-title"><span class="publication-title">Population Size and Sampling Complexity in Genetic Algorithms</span></a>  </h5>  <div class="authors"> <a href="researcher/9095638_Yong_Gao" class="authors ga-similar-publication-author">Yong Gao</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw21_56ab9eb3b965f"> <div> <h5> <a href="publication/262347211_VC_dimension_and_sampling_complexity_of_learning_sparse_polynomials_and_rational_functions" class="color-inherit ga-similar-publication-title"><span class="publication-title">VC dimension and sampling complexity of learning sparse polynomials and rational functions</span></a>  </h5>  <div class="authors"> <a href="researcher/6929841_Marek_Karpinski" class="authors ga-similar-publication-author">Marek Karpinski</a>, <a href="researcher/21230937_Thorsten_Werther" class="authors ga-similar-publication-author">Thorsten Werther</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw40_56ab9eb3b965f" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw41_56ab9eb3b965f">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw42_56ab9eb3b965f" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=dkhoVW_poTSz9rhw6oRY11HzetRYI90YkwUkvOVrNRuuentvWD4-7mvLBgOoU7T8" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="gKZ3JCWbnh5EgLmtaeskIdh+szs1qnpDCzMXEwSwYFsLulDb1OHBE8WHZQGmAgFaon7aE3drEGNr+9/YnrRxzwMlMr8Bp1+U17Ir4vjiLqIosn2VyZ5dpIu6QnEtEZCgIoGG0/n0D4XGNr4I9PyrzhtP+Cuw7Wgo524/8kV9REBX1Wr7JIBf7wT7O+SN4+9FyWHHPJK5n0S1O/zrzLCrfcDrc9Ys1QseK2vNHVD5RUbAyyUTg0IVLQbrxQqeqboUqGS8EARft+vI+njZ3ghc7u93Z8VUwCqYdv3Vq8ELdVA="/> <input type="hidden" name="urlAfterLogin" value="publication/266660385_Reducing_the_sampling_complexity_of_topic_models"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjY2NjYwMzg1X1JlZHVjaW5nX3RoZV9zYW1wbGluZ19jb21wbGV4aXR5X29mX3RvcGljX21vZGVscw%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjY2NjYwMzg1X1JlZHVjaW5nX3RoZV9zYW1wbGluZ19jb21wbGV4aXR5X29mX3RvcGljX21vZGVscw%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjY2NjYwMzg1X1JlZHVjaW5nX3RoZV9zYW1wbGluZ19jb21wbGV4aXR5X29mX3RvcGljX21vZGVscw%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw43_56ab9eb3b965f"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 913;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Aaron Li","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278485756071938%401443407586376_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Aaron_Li3","institution":"Scaled Inference","institutionUrl":false,"widgetId":"rgw4_56ab9eb3b965f"},"id":"rgw4_56ab9eb3b965f","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=5213710","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab9eb3b965f"},"id":"rgw3_56ab9eb3b965f","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=266660385","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":266660385,"title":"Reducing the sampling complexity of topic models","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"doi":"10.1145\/2623330.2623756","journalInfos":{"journal":"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","publicationDate":"08\/2014;","publicationDateRobot":"2014-08","article":""}},"source":false,"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1145\/2623330.2623756"},{"key":"rft.atitle","value":"Reducing the sampling complexity of topic models"},{"key":"rft.title","value":"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"},{"key":"rft.jtitle","value":"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining"},{"key":"rft.date","value":"2014"},{"key":"rft.au","value":"Aaron Q. Li,Amr Ahmed,Sujith Ravi,Alexander J. Smola"},{"key":"rft.isbn","value":"978-1-4503-2956-9"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56ab9eb3b965f"},"id":"rgw6_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=266660385","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":266660385,"peopleItems":[{"data":{"authorNameOnPublication":"Aaron Li","accountUrl":"profile\/Aaron_Li3","accountKey":"Aaron_Li3","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278485756071938%401443407586376_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Aaron Li","profile":{"professionalInstitution":{"professionalInstitutionName":"Scaled Inference","professionalInstitutionUrl":false}},"professionalInstitutionName":"Scaled Inference","professionalInstitutionUrl":false,"url":"profile\/Aaron_Li3","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278485756071938%401443407586376_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Aaron_Li3","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw9_56ab9eb3b965f"},"id":"rgw9_56ab9eb3b965f","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=5213710&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Scaled Inference","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":4,"accountCount":3,"publicationUid":266660385,"widgetId":"rgw8_56ab9eb3b965f"},"id":"rgw8_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=5213710&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=4&accountCount=3&publicationUid=266660385","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/2055577116_Amr_Ahmed","authorNameOnPublication":"Amr Ahmed","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Amr Ahmed","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2055577116_Amr_Ahmed","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw11_56ab9eb3b965f"},"id":"rgw11_56ab9eb3b965f","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2055577116&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw10_56ab9eb3b965f"},"id":"rgw10_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2055577116&authorNameOnPublication=Amr%20Ahmed","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Sujith Ravi","accountUrl":"profile\/Sujith_Ravi","accountKey":"Sujith_Ravi","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A282136889315329%401444278084290_m\/Sujith_Ravi.png","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Sujith Ravi","profile":{"professionalInstitution":{"professionalInstitutionName":"Google Inc.","professionalInstitutionUrl":"institution\/Google_Inc"}},"professionalInstitutionName":"Google Inc.","professionalInstitutionUrl":"institution\/Google_Inc","url":"profile\/Sujith_Ravi","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A282136889315329%401444278084290_l\/Sujith_Ravi.png","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Sujith_Ravi","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw13_56ab9eb3b965f"},"id":"rgw13_56ab9eb3b965f","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1898916&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Google Inc.","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":4,"accountCount":3,"publicationUid":266660385,"widgetId":"rgw12_56ab9eb3b965f"},"id":"rgw12_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1898916&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=4&accountCount=3&publicationUid=266660385","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Alexander J. Smola","accountUrl":"profile\/Alexander_Smola","accountKey":"Alexander_Smola","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Alexander J. Smola","profile":{"professionalInstitution":{"professionalInstitutionName":"Carnegie Mellon University","professionalInstitutionUrl":"institution\/Carnegie_Mellon_University"}},"professionalInstitutionName":"Carnegie Mellon University","professionalInstitutionUrl":"institution\/Carnegie_Mellon_University","url":"profile\/Alexander_Smola","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Alexander_Smola","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw15_56ab9eb3b965f"},"id":"rgw15_56ab9eb3b965f","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=2624028&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Carnegie Mellon University","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":4,"accountCount":3,"publicationUid":266660385,"widgetId":"rgw14_56ab9eb3b965f"},"id":"rgw14_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=2624028&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=4&accountCount=3&publicationUid=266660385","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab9eb3b965f"},"id":"rgw7_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=266660385&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":266660385,"abstract":"<noscript><\/noscript><div>Inference in topic models typically involves a sampling step to associate latent variables with observations. Unfortunately the generative model loses sparsity as the amount of data increases, requiring O(k) operations per word for k topics. In this paper we propose an algorithm which scales linearly with the number of actually instantiated topics kd in the document. For large document collections and in structured hierarchical models kd ll k. This yields an order of magnitude speedup. Our method applies to a wide variety of statistical models such as PDP [16,4] and HDP [19]. At its core is the idea that dense, slowly changing distributions can be approximated efficiently by the combination of a Metropolis-Hastings step, use of sparsity, and amortized constant time sampling via Walker's alias method.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw16_56ab9eb3b965f"},"id":"rgw16_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=266660385","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/266660385_Reducing_the_sampling_complexity_of_topic_models\/links\/5583a5c508aefa35fe30c780\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw17_56ab9eb3b965f"},"id":"rgw17_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab9eb3b965f"},"id":"rgw5_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=266660385&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":8372861,"url":"researcher\/8372861_Di_He","fullname":"Di He","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Aug 2013","journal":"Circuits Systems and Signal Processing","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/257319241_Reducing_the_Sampling_Complexity_of_Energy_Detection_in_Cognitive_Radio_Networks_under_Low_SNR_by_Using_the_Optimal_Stochastic_Resonance_Technique","usePlainButton":true,"publicationUid":257319241,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.12","url":"publication\/257319241_Reducing_the_Sampling_Complexity_of_Energy_Detection_in_Cognitive_Radio_Networks_under_Low_SNR_by_Using_the_Optimal_Stochastic_Resonance_Technique","title":"Reducing the Sampling Complexity of Energy Detection in Cognitive Radio Networks under Low SNR by Using the Optimal Stochastic Resonance Technique","displayTitleAsLink":true,"authors":[{"id":8372861,"url":"researcher\/8372861_Di_He","fullname":"Di He","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Circuits Systems and Signal Processing 08\/2013; 32(4). DOI:10.1007\/s00034-013-9552-0"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/257319241_Reducing_the_Sampling_Complexity_of_Energy_Detection_in_Cognitive_Radio_Networks_under_Low_SNR_by_Using_the_Optimal_Stochastic_Resonance_Technique","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/257319241_Reducing_the_Sampling_Complexity_of_Energy_Detection_in_Cognitive_Radio_Networks_under_Low_SNR_by_Using_the_Optimal_Stochastic_Resonance_Technique\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab9eb3b965f"},"id":"rgw19_56ab9eb3b965f","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=257319241","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":9095638,"url":"researcher\/9095638_Yong_Gao","fullname":"Yong Gao","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"May 2003","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/2839732_Population_Size_and_Sampling_Complexity_in_Genetic_Algorithms","usePlainButton":true,"publicationUid":2839732,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/2839732_Population_Size_and_Sampling_Complexity_in_Genetic_Algorithms","title":"Population Size and Sampling Complexity in Genetic Algorithms","displayTitleAsLink":true,"authors":[{"id":9095638,"url":"researcher\/9095638_Yong_Gao","fullname":"Yong Gao","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/2839732_Population_Size_and_Sampling_Complexity_in_Genetic_Algorithms","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/2839732_Population_Size_and_Sampling_Complexity_in_Genetic_Algorithms\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56ab9eb3b965f"},"id":"rgw20_56ab9eb3b965f","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=2839732","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":6929841,"url":"researcher\/6929841_Marek_Karpinski","fullname":"Marek Karpinski","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":21230937,"url":"researcher\/21230937_Thorsten_Werther","fullname":"Thorsten Werther","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Conference Paper","publicationDate":"Aug 1994","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/262347211_VC_dimension_and_sampling_complexity_of_learning_sparse_polynomials_and_rational_functions","usePlainButton":true,"publicationUid":262347211,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/262347211_VC_dimension_and_sampling_complexity_of_learning_sparse_polynomials_and_rational_functions","title":"VC dimension and sampling complexity of learning sparse polynomials and rational functions","displayTitleAsLink":true,"authors":[{"id":6929841,"url":"researcher\/6929841_Marek_Karpinski","fullname":"Marek Karpinski","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":21230937,"url":"researcher\/21230937_Thorsten_Werther","fullname":"Thorsten Werther","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Proceedings of a workshop on Computational learning theory and natural learning systems (vol. 1) : constraints and prospects: constraints and prospects; 08\/1994"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/262347211_VC_dimension_and_sampling_complexity_of_learning_sparse_polynomials_and_rational_functions","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/262347211_VC_dimension_and_sampling_complexity_of_learning_sparse_polynomials_and_rational_functions\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw21_56ab9eb3b965f"},"id":"rgw21_56ab9eb3b965f","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=262347211","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw18_56ab9eb3b965f"},"id":"rgw18_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=266660385&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":266660385,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":266660385,"publicationType":"article","linkId":"5583a5c508aefa35fe30c780","fileName":"fastlda-kdd2014.pdf","fileUrl":"profile\/Sujith_Ravi\/publication\/266660385_Reducing_the_sampling_complexity_of_topic_models\/links\/5583a5c508aefa35fe30c780.pdf","name":"Sujith Ravi","nameUrl":"profile\/Sujith_Ravi","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":"http:\/\/sravi.org\/pubs\/fastlda-kdd2014.pdf","hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"publisherLink":"http:\/\/sravi.org\/pubs\/fastlda-kdd2014.pdf","publisherLinkDisplay":"sravi.org","isUserLink":true,"uploadDate":"Jun 19, 2015","fileSize":"1.26 MB","widgetId":"rgw24_56ab9eb3b965f"},"id":"rgw24_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=266660385&linkId=5583a5c508aefa35fe30c780&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw23_56ab9eb3b965f"},"id":"rgw23_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=266660385&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":33,"valueFormatted":"33","widgetId":"rgw25_56ab9eb3b965f"},"id":"rgw25_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=266660385","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw22_56ab9eb3b965f"},"id":"rgw22_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=266660385&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":266660385,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw27_56ab9eb3b965f"},"id":"rgw27_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=266660385&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":33,"valueFormatted":"33","widgetId":"rgw28_56ab9eb3b965f"},"id":"rgw28_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=266660385","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw26_56ab9eb3b965f"},"id":"rgw26_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=266660385&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Reducing the Sampling Complexity of Topic Models\nAaron Q. Li\nCMU Language Technologies\nPittsburgh, PA\naaronli@cmu.edu\nSujith Ravi\nGoogle Strategic Technologies\nMountain View, CA\nsravi@google.com\nAmr Ahmed\nGoogle Strategic Technologies\nMountain View, CA\namra@google.com\nAlexander J. Smola\nCMU MLD and Google ST\nPittsburgh PA\nalex@smola.org\nABSTRACT\nInference in topic models typically involves a sampling step\nto associate latent variables with observations. Unfortu-\nnately the generative model loses sparsity as the amount\nof data increases, requiring O(k) operations per word for k\ntopics. In this paper we propose an algorithm which scales\nlinearly with the number of actually instantiated topics kdin\nthe document. For large document collections and in struc-\ntured hierarchical models kd ? k. This yields an order of\nmagnitude speedup. Our method applies to a wide variety\nof statistical models such as PDP [16, 4] and HDP [19].\nAt its core is the idea that dense, slowly changing distri-\nbutions can be approximated efficiently by the combination\nof a Metropolis-Hastings step, use of sparsity, and amortized\nconstant time sampling via Walker\u2019s alias method.\nKeywords\nSampling; Scalability; Topic Models; Alias Method\n1.INTRODUCTION\nTopic models are some of the most versatile tools for mod-\neling statistical dependencies. Given a set of observations\nxi \u2208 X, such as documents, logs of user activity, or com-\nmunications patterns, we want to infer the hidden causes\nmotivating this behavior. A key property in topic models\nis that they model p(x) via a discrete hidden factor, z via\np(x|z) and p(z). For instance, z may be the cluster of a docu-\nment. In this case it leads to Gaussian and Dirichlet mixture\nmodels [14]. When z is a vector of topics associated with in-\ndividual words, this leads to Latent Dirichlet Allocation [3].\nLikewise, whenever z indicates a term in a hierarchy, it leads\nto structured and mixed-content annotations [19, 2, 4, 12].\n1.1Sparsity in Topic Models\nOne of the key obstacles in performing scalable inference is\nto draw p(z|x) from the discrete state distribution associated\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand\/or a fee. Request permissions from permissions@acm.org.\nKDD\u201914, August 24\u201327, 2014, New York, NY, USA.\nCopyright is held by the owner\/author(s). Publication rights licensed to ACM.\nACM 978-1-4503-2956-9\/14\/08 ...$15.00.\nhttp:\/\/dx.doi.org\/10.1145\/2623330.2623756.\nwith the data. A substantial improvement in this context\nwas provided by [22] who exploited sparsity to decompose\nthe collapsed sampler [9] for Latent Dirichlet Allocation. As\na result the sampling cost can be reduced from O(k), the\ntotal number of topics to O(kd+ kw), i.e. the number kw of\ntopics occurring for a particular word w and kd for a partic-\nular document d. This insight led to an order of magnitude\nimprovement for sampling topic models, thus making their\nimplementation feasible at a large scale. In fact, the strat-\negy is sufficiently robust that it can be extended to settings\nwhere the topic smoother depends on the words [15].\nFor small datasets the assumption kd+kw ? k is well sat-\nisfied. Unfortunately, as the number of documents grows, so\ndoes the number of topics in which a particular word occurs.\nIn particular kw \u2192 k, since the probability of observing any\nparticular topic for a given word is rarely nonzero: Assume\nthat the probability of occurrence for a given topic for a\nword is bounded from below by \u03b4. Then the probability of\nthe topic occurring at least once in a collection of n docu-\nments is given by\n1 \u2212 (1 \u2212 \u03b4)n\u2265 1 \u2212 e\u2212n\u03b4\u2192 1 for n \u2192 \u221e.\nFrom this it follows that kw = O(k) for n = O(\u03b4\u22121logk).\nIn other words, for large numbers documents the efficiencies\ndiscussed in [22] vanish. This is troubling, since in many in-\ndustrial settings n can be in the order of billions to trillions.\nConsequently, with increasing amounts of data, the time to\nprocess individual documents increases due to loss of spar-\nsity, thus leading to a superlinear increase in runtime.\nOn the other hand, the topic-sparsity for a given document\nessentially remains unchanged, regardless of the total num-\nber of related documents that are available. This is due to\nthe fact that the number of tokens per document is typically\nless than O(k). For instance, microblogs contain only dozens\nof words, yet admit to thousands of topics.1This situation\nis exacerbated when it comes to hierarchical and structured\ntopic models, since there the number of (sub)topics can grow\nconsiderably more rapidly. Hence the use of sparsity is cru-\ncial in designing efficient samplers.\n1.2Metropolis-Hastings-Walker Sampling\nThe present paper proposes a new decomposition of the\ncollapsed conditional probability, in conjunction with a\n1Obviously, this approach would not work to infer topics for\nDostojevski\u2019s War and Peace. That said, a plain topic model\nis an unlikely candidate to represent very long documents."},{"page":2,"text":"Metropolis-Hastings [7] scheme and the use of the alias\nmethod, introduced by Walker [20, 13], to amortize dense\nupdates for random variables. This method is highly ver-\nsatile. It defers corrections to the model and avoids renor-\nmalization. This allows us to apply it to both flat and hier-\narchical models. Experimental evaluation demonstrates the\nefficacy of our approach, yielding orders of magnitude accel-\neration and a simplified algorithm.\nWhile we introduce our algorithm in the context of topic\nmodels, it is entirely general and applies to a much richer\nclass of models. At its heart lies the insight that in many\ninference problems the model parameters only change rela-\ntively slowly during sampling. For instance, the location of\ncluster centers, the definition of topics, or the shape of au-\ntoregressive functions, only change relatively slowly. Hence,\nif we could draw from a distribution over k outcomes k times,\nWalker\u2019s alias method would allow us to generate samples in\namortized constant time. At the same time, the Metropolis\nHastings algorithm allows us to use approximations of the\ncorrect probability distribution, provided that we compute\nratios between successive states correctly. Our approach is\nto draw from the stale distribution in constant time and to\naccept the transition based on the ratio between successive\nstates. This step takes constant time. Moreover, the pro-\nposal is independent of the current state. Once k samples\nhave been drawn, we simply update the alias table. In honor\nof the constitutent algorithms we refer to our technique as\nthe Metropolis Hastings Walker (MHW) sampler.\n2.TOPIC MODELS\nWe begin with a brief introduction to topic models and the\nassociated inference problems. This includes a short motiva-\ntion of sampling schemes in the context collapsed samplers\n[9, 18] and of stochastic variational models [21]. It is followed\nby a description of extensions to hierarchical models.\n2.1 Latent Dirichlet Allocation\nIn LDA [3] one assumes that documents are mixture dis-\ntributions of language models associated with individual\ntopics. That is, the documents are generated following the\ngraphical model below:\nfor all i\nfor all d\nfor all k\n\u03b1\u03b8d\nzdi\nwdi\n\u03c8k\n\u03b2\nFor each document d draw a topic distribution \u03b8d from a\nDirichlet distribution with concentration parameter \u03b1\n\u03b8d\u223c Dir(\u03b1).(1)\nFor each topic t draw a word distribution from a Dirichlet\ndistribution with concentration parameter \u03b2\n\u03c8t \u223c Dir(\u03b2).(2)\nFor each word i \u2208 {1...nd} in document d draw a topic\nfrom the multinomial \u03b8d via\nzdi\u223c Discrete(\u03b8d).(3)\nDraw a word from the multinomial \u03c8zdivia\nwdi\u223c Discrete(\u03c8zdi).(4)\nThe beauty of the Dirichlet-multinomial design is that the\ndistributions are conjugate. This means that the multino-\nmial distributions \u03b8d and \u03c8k can be integrated out, thus\nallowing one to express p(w,z|\u03b1,\u03b2,nd) in closed-form [9].\nThis yields a Gibbs sampler to draw p(zdi|rest) efficiently.\nThe conditional probability is given by\np(zdi|rest) \u221d(n\u2212di\ntd\n+ \u03b1t)(n\u2212di\nn\u2212di\nt\ntw + \u03b2w)\n+\u00af\u03b2\n.(5)\nHere the count variables ntd,ntw and nt denote the num-\nber of occurrences of a particular (topic,document) and\n(topic,word) pair, or of a particular topic respectively. More-\nover, the superscript \u00b7\u2212didenotes said count when ignoring\nthe pair (zdi,wdi). For instance, n\u2212di\ning the (topic,word) combination at position (d,i). Finally,\n\u00af\u03b2 :=?\ntime since we have k nonzero terms in a sum that needs to be\nnormalized. [22] devised an ingenious strategy for exploiting\nsparsity by decomposing terms into\ntw is obtained when ignor-\nw\u03b2w denotes the joint normalization.\nAt first glance, sampling from (5) appears to cost O(k)\np(zdi|rest) \u221d \u03b2w\n\u03b1t\nn\u2212di\nt\n+\u00af\u03b2+ n\u2212di\ntd\n\u03b2w\nn\u2212di\nt\n+\u00af\u03b2+ n\u2212di\ntw\nn\u2212di\ntd\nn\u2212di\nt\n+ \u03b1t\n+\u00af\u03b2\nAs can be seen, for small collections of documents only the\nfirst term is dense, and more specifically,?\nis, whenever both ntd and ntw are sparse, sampling from\np(zdi|rest) can be accomplished efficiently. The use of packed\nindex variables and a clever reordering of (topic,count) pairs\nfurther improve efficient sampling to O(kw+ kd).\nStochastic variational inference [11] requires an analogous\nsampling step. The main difference being that rather than\nusingntw+\u03b2w\nnt+\u00af\u03b2\nto capture p(w|t) one uses a natural parameter\n\u03b7tw associated with the conjugate variational distribution.\nUnfortunately this renders the model dense, unless rather\ncareful precautions are undertaken [11] to separate residual\ndense and sparse components.\nInstead, we devise a sampler to draw from p(zdi|rest) in\namortized O(kd) time. We accomplish this by using\nt\u03b1t\/(n\u2212di\nt\n+\u00af\u03b2)\ncan be computed from?\nt\u03b1t\/(nt+\u00af\u03b2) in O(1) time. That\np(zdi|rest) \u221d n\u2212di\ntd\nn\u2212di\ntw + \u03b2w\nn\u2212di\nt\n+\u00af\u03b2\n+\u03b1t(n\u2212di\ntw + \u03b2w)\nn\u2212di\nt\n+\u00af\u03b2\n(6)\nHere the first term is sparse in kd and we can draw from it\nin O(kd) time. The second term is dense, regardless of the\nnumber of documents (this holds true for stochastic varia-\ntional samplers, too). However, the \u2019language model\u2019 p(w|t)\ndoes not change too drastically whenever we resample a sin-\ngle word. The number of words is huge, hence the amount of\nchange per word is concomitantly small. This insight forms\nthe basis for applying Metropolis-Hastings-Walker sampling.\n2.2Poisson Dirichlet Process\nTo illustrate the fact that the MHW sampler also works\nwith models containing a dense generative part, we describe\nits application to the Poisson Dirichlet Process [4, 16]. The\nmodel is given by the following variant of the LDA model:"},{"page":3,"text":"for all i\nfor all d\nfor all k\n\u03b1\u03b8d\nzdi\nwdi\n\u03c8t\n\u03c80\n\u03b2\nIn a conventional topic model the language model is sim-\nply given by a multinomial draw from a Dirichlet distribu-\ntion. This fails to exploit distribution information between\ntopics, such as the fact that all topics have the same common\nunderlying language. A means for addressing this problem\nis to add a level of hierarchy to model the distribution over\n\u03c8t via?\nThe ingredients for a refined language model are a Pitman-\nYor Topic Model (PYTM) [17] that is more appropriate to\ndeal with natural languages. This is then combined with\nthe Poisson Dirichlet Process (PDP) [16, 4] to capture the\nfact that the number of occurences of a word in a natu-\nral language corpus follows power-law. Within a corpus, the\nfrequency of a word is approximately inversely proportional\nto its ranking in number of occurences. Each draw from a\nPoisson Dirichlet Process PDP(b,a,\u03c80) is a probability dis-\ntribution. The base distribution \u03c80 defines the common un-\nderlying distribution shared across the generated distribu-\ntions. Under the settings of Pitman-Yor Topic Model, each\ntopic defines a distribution over words, and the base dis-\ntribution defines the common underlying common language\nmodel shared by the topics. The concentration parameter\nb controls how likely a word is to occur again while being\nsampled from the generated distribution. The discount pa-\nrameter a prevents a word to be sampled too often by im-\nposing a penalty on its probability based on its frequency.\nThe combined model described explicityly in [5]:\ntp(\u03c8t|\u03c80)p(\u03c80|\u03b2) rather than?\ntp(\u03c8t|\u03b2). Such a\nmodel is depicted above.\n\u03b8d\u223c Dir(\u03b1)\nzdi\u223c Discrete(\u03b8d)\nwdi\u223c Discrete(\u03c8zdi)\nAs can be seen, the document-specific part is identical to\nLDA whereas the language model is rather more sophisti-\ncated. Likewise, the collapsed inference scheme is analogous\nto a Chinese Restaurant Process [6, 5]. The technical diffi-\nculty arises from the fact that we are dealing with distribu-\ntions over countable domains. Hence, we need to keep track\nof multiplicities, i.e. whether any given token is drawn from\n\u03b2i or \u03b20. This will require the introduction of additional\ncount variables in the collapsed inference algorithm.\nEach topic is equivalent to a restaurant. Each token in the\ndocument is equivalent to a customer. Each type of word\ncorresponds each type of dish served by the restaurant. The\nsame results in [6] can be used to derive the conditional\nprobability by introducing axillary variables:\n\u03c80 \u223c Dir(\u03b2)\n\u03c8t \u223c PDP(b,a,\u03c80)\n\u2022 stw denotes the number of tables serving dish w in\nrestaurant t. Here t is the equivalent of a topic.\n\u2022 rdi indicates whether wdi opens a new table in the\nrestaurant or not (to deal with multiplicities).\n\u2022 mtw denotes the number of times dish w has been\nserved in restaurant t (analogously to nwk in LDA).\nThe conditional probability is given by:\np(zdi= t,rdi= 0|rest) \u221d\u03b1t+ ndt\nbt+ mt\nmtw+ 1 \u2212 stw\nmtw+ 1\nSmtw+1\nstw,at\nSmtw\nstw,at\n(7)\nif no additional \u2019table\u2019 is opened by word wdi. Otherwise\np(zdi= t,rdi= 1|rest)\n\u221d(\u03b1t+ ndt)bt+ atst\n(8)\nbt+ mt\nstw+ 1\nmtw+ 1\n\u03b3 + stw\n\u00af \u03b3 + st\nSmtw+1\nstw+1,at\nSmtw\nstw,at\nHere SN\nM,ais the generalized Stirling number. It is given by\nSN+1\nM,a= SN\nM\u22121,a+ (N \u2212 Ma)SN\nM,aand SN\nM,a= 0\nfor M > N, and SN\n[4]. Moreover we have mt =?\nthese two expressions can be written as a combination of\na sparse term and a dense term, simply by splitting the\nfactor (\u03b1t+ndt) into its sparse component ndtand its dense\ncounterpart \u03b1t. Hence we can apply the same strategy as\nbefore when sampling topics from LDA, albeit now using a\ntwice as large space of state variables.\n2.3Hierarchical Dirichlet Process\nTo illustrate the efficacy and generality of our approach we\ndiscuss a third case where the document model itself is more\nsophisticated than a simple collapsed Dirichlet-multinomial.\nWe demonstrate that there, too, inference can be performed\nefficiently. Consider the two-level topic model based on the\nHierarchical Dirichlet Process [19] (HDP-LDA). In it, the\ntopic distribution for each document \u03b8d is drawn from a\nDirichlet process DP(b1,\u03b80). In turn, \u03b80 is drawn from a\nDirichlet process DP(b0,H(\u00b7)) governing the distribution\nover topics. In other words, we add an extra level of hierar-\nchy on the document side (compared to the extra hierarchy\non the language model used in the PDP).\n0,a= \u03b4N,0. A detailed analysis is given in\nwmtw, and st =?\ntstw.\nSimilar to the conditional probability expression in LDA,\nfor all i\nfor all d\nfor all k\nH\u03b80\n\u03b8d\nzdi\nwdi\n\u03c8k\n\u03b2\nMore formally, the joint distribution is as follows:\n\u03b80 \u223c DP(b0,H(\u00b7))\n\u03b8d\u223c DP(b1,\u03b80)\nzdi\u223c Discrete(\u03b8d)\nwdi\u223c Discrete(\u03c8zdi)\nBy construction, DP(b0,H(\u00b7)) is a Dirichlet Process, equiva-\nlent to a Poisson Dirichlet Process PDP(b0,a,H(\u00b7)) with the\ndiscount parameter a set to 0. The base distribution H(.) is\noften assumed to be a uniform distribution in most cases.\nAt first, a base \u03b80 is drawn from DP(b0,H(\u00b7)). This gov-\nerns how many topics there are in general, and what their\noverall prevalence is. The latter is then used in the next level\nof the hierarchy to draw a document-specific distribution \u03b8d\nthat serves the same role as in LDA. The main difference is\nthat unlike in LDA, we use \u03b80 to infer which topics are more\npopular than others.\nIt is also possible to extend the model to more than two\nlevels of hierarchy, such as the infinite mixture model [19].\nSimilar to Poisson Dirichlet Process, an equivalent Chinese\nRestaurant Franchise analogy [6, 19] exists for Hierarchi-\ncal Dirichlet Process with multiple levels. In this analogy,\neach Dirichlet Process is mapped to a single Chinese Restau-\n\u03c8t \u223c Dir(\u03b2)"},{"page":4,"text":"rant Process, and the hierarchical structure is constructed\nto identify the parent and children of each restaurant.\nThe general (collapsed) structure is as follows: let Nj be\nthe total number of customers in restaurant j and njt be\nthe number of customers in restaurant j served with dish\nt. When a new customer (a token) enters restaurant j with\nthe corresponding Dirichlet Process DP(bj,Hj(\u00b7)), there are\ntwo types of seating arrangement for the customer:\n\u2022 With probability\ndish (topic) t and sits at an existing table.\n\u2022 With probability\ntable served with a new dish t drawn from Hj(\u00b7).\nIn the event that the customer sits at a new table, a phan-\ntom customer is sent upstream the hierarchy to the parent\nrestaurant of j, denoted by j?, with corresponding Dirichlet\nProcess DP(bj?,Hj?(\u00b7)). The parent restaurant then decides\nthe seating arrangement of the phantom customer under the\nsame rules. This process repeats, until there is no more par-\nent restaurant or any of phantom customer decides to sit in\nan existing table in any parent restaurant along the path.\nWe use the block Gibbs sampler given in [6] as it allows us\nto extend our approach for multi-level Hierarchical Dirichlet\nProcess, and performs better than the samplers given in [19]\nand the collapsed Gibbs sampler given in [4], as measured\nin convergence speed, running time, and topic quality.\nThe key difference of [6] relative to [19] is that rather\nthan keeping track of relative assignments of tables to each\nother (and the resulting multiplicities and infrequent block\nmoves) it simply keeps track of the level within the hierarchy\nof restaurants at which an individual customer opens a new\ntable. The advantage is that this allows us to factor out\nthe relative assignment of customers to specific tables but\nrather only keep track of the dishes that they consume. The\nobvious downside being that a small number of customers\ncan be blocked from moves if they opened a table at a high\nposition of the hierarchy that other customers depend upon.\nImproving mixing in this context is subject to future work.\nIn the setting studied above we only have a two-level HDP:\nthat of the parent DP tying all documents together and the\nDP within each document, governing its topic distribution.\nWe use zdi \u2208 N to denote the topic indicator of word i at\nposition d and udi \u2208 {0,1} to indicate whether a new table\nis opened at the root level (i.e. udi = 1). Moreover, define\nstd to be the table counter for document d, i.e. the number\nof times a table serving topic t has been opened, and let st\nbe the associated counter for the base DP, associated with\ntables opened at the parent level. Finally, let s :=?\nClearly the situation where st = 0 and udi= 0 is impossi-\nble since this would imply that we are opening a new table\nat document d while there is no matching table available at\nthe root level. Hence for the collapsed sampler we only need\nto consider the following cases:\nnjt\nbj+Njthe customer is served with\nbj\nbj+Njthe customer sits at a new\ntst be\nthe total number of tables opened at the root level.\n\u2022 A new root topic is generated. That is, we currently\nhave st and need to set udi= 1.\n\u2022 A new table is added at document d. In this case we\nrequire that st, i.e. that the topic exists at the root\nlevel. Moreover, obviously it requires that std= 0 since\nwe are adding the first table to serve dish t.\n\u2022 No additional topic is introduced but we may be in-\ntroducing an additional table.\nThis amounts to the following (unnormalized) conditional\nprobabilities. See [6] for further details.\np(zdi= t,udi= u|rest)\n\uf8f1\n\uf8f2\n\uf8f4\nExpressions for the generalized form are analogous. Both\nforms contain a fraction with its numerator being the sum\nof a sparse term mtw and a dense term \u03b2w. Therefore, the\nconditional probability can be decomposed to a dense term\nmultiplied by \u03b2w, and a sparse term multiplied by mtw. Ap-\nplying the same methodology, the sampling complexity of a\nmulti-level HDP can be reduced to O(kw).\n(9)\n\u221d\u03b2w+ mtw\n\u00af\u03b2 + mt\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\nb0b1\nb0+s\nif st = 0 and udi= 1\nb1s2\nt\n(st+1)(s+b0)\nif st ?= 0 and std= 0\nSndt+1\nsdt+1,0\nSndt\nsdt,0\nsdt+1\nndk+1\nif st ?= 0 and std?= 0\n3.METROPOLIS-HASTINGS-WALKER\nWe now introduce the key components for the MHW al-\ngorithm and how to use it in sampling topics. They consist\nof the alias method [20, 13] and a simplified version of the\nMetropolis-Hastings sampler [7].\n3.1Walker\u2019s Alias Method\nTypically, when drawing from a distribution over l out-\ncomes, it is accepted that one would need to perform O(l)\nwork to generate a sample. In fact, this is a lower bound,\nsince we need to inspect each probability at least once before\nwe can construct the sampler. However, what is commonly\noverlooked is that there exist algorithms that allow us to\ndraw subsequent samples from the same distribution in O(1)\ntime. This means that drawing l samples from a distribution\nover l outcomes can be accomplished in O(1) amortized time\nper draw. We make extensive use of this fact.\nDenote by piwith i \u2208 {1...l} the probabilities of a distri-\nbution over l outcomes from which we would like to sample.\nThe algorithm works by decomposing a distribution over l\nevents into l bins of equal probability by pairing at most two\nevents per bin. Since it \u2019robs\u2019 from the probabilities pi > 1\/l\nand adds to those with pi < 1\/l it is also referred to as \u2019Robin\nHood\u2019 method [13]. The algorithm proceeds as follows:\n1: GenerateAlias(p,l)\n2: Initialize L = H = \u2205 and A = [].\n3: for i = 1 to l do\n4:if pi \u2264 l\u22121then\n5:L \u2190 L \u222a {(i,pi)}\n6: else\n7:H \u2190 H \u222a {(i,pi)}\n8:end if\n9: end for\n10: while L ?= \u2205 do\n11: Extract (i,pi) from L and (h,ph) from H\n12:A \u2190 [A,(i,h,pi)]\n13:if ph\u2212 pi > l\u22121then\n14:H \u2190 H \u222a {(h,ph\u2212 pi)}\n15: else\n16:L \u2190 L \u222a {(h,ph\u2212 pi)}\n17:end if\n18: end while\n19: return A"},{"page":5,"text":"This yields an array A containing triples (i,h,ph) with\nph < l\u22121. It runs in O(l) time since at each step one event\nis removed from the list. And the probabilities remain un-\nchanged, as can be seen by induction. All we need to do now\nis to draw a random element from A and flip a biased coin\nto accept h or i with probability lphand 1\u2212lphrespectively.\n1: SampleAlias(A,l)\n2: bin = RandInt(l)\n3: (i,h,p) = A[bin]\n4: if lp > RandUnif(1) then\n5: return h\n6: else\n7:return i\n8: end if\nNote that the alias method works since we are implicitly\nexploiting parallelism inherent in CPUs: as long as l does\nnot exceed 264are guaranteed that even an information the-\noretically inefficient code will not require more than 64 bit,\nwhich can be generated in constant time.\n3.2Sampling with Proposal Distributions\nWhenever we draw l identical samples from p it is clear\nthat the above algorithm provides an O(1) sampler. How-\never, if p changes, it is difficult to apply the alias sampler\ndirectly. To address this, we use rejection sampling and\nMetropolis-Hastings procedures. Rejection sampling pro-\nceeds as follows:\n1: Rejection(p,q,c)\n2: repeat\n3:Draw i \u223c q(i)\n4: until p(i) \u2265 cq(i)RandUnif(1)\n5: return i\nHere p is the distribution we would like to draw from, q is a\nreference distribution that makes sampling easy, and c \u2265 1\nis chosen such that cq(i) \u2265 p(i) for all i. We then accept with\nprobability\nof samples to draw via Rejection(p,q,c) is c, provided that\na good bound c exists. In this case we have the following:\np(i)\ncq(i). It is well known that the expected number\nLemma 1 Given l distributions pi and q over l outcomes\nsatisfying ciq \u2265 pi, the expected amortized runtime complex-\nity for drawing using SampleAlias(A,l) and rejecting using\nRejection(pi,q,ci) is given by O\n?\n1\nl\n?l\ni=1ci\n?\n.\nProof. Preprocessing costs amortized O(1) time. Each\nrejection sampler costs O(ci) work. Averaging over the draws\nproves the claim.\nIn many cases, unfortunately, we do not know ci, or comput-\ning ci is essentially as costly as drawing from pi itself. More-\nover, in some cases cimay be unreasonably large. In this sit-\nuation we resort to Metropolis Hastings sampling [7] using\na stationary proposal distribution. As in rejection sampling,\nwe use a proposal distribution q and correct the effect of sam-\npling from the \u2019wrong\u2019 distribution by a subsequent accep-\ntance step. The main difference is that Metropolis Hastings\ncan be considerably more efficient than Rejection sampling\nsince it only requires that the ratios of probabilities are close\nrather than requiring knowledge of a uniform upper bound\non the ratio. The drawback is that instead of drawing iid\nsamples from p we end up with a chain of dependent sam-\nples from p, as governed by q.\nFor the purpose of the current method we only need to\nconcern ourselves with stationary distributions p and q, i.e.\np(i) = p(i|j) and q(i) = q(i|j), hence we only discuss this\nspecial case below. For a more general discussion see e.g. [8].\n1: StationaryMetropolisHastings(p,q,n)\n2: if no initial state exists then i \u223c q(i)\n3: for l = 1 to n do\n4: Draw j \u223c q(j)\n5: if RandUnif(1) < min\n6:i \u2190 j\n7: end if\n8: end for\n9: return i\nAs a result, provided that p and q are sufficiently similar, the\nsampler accepts most of the time. This is the case, e.g. when-\never we use a stale variant of p as the proposal q. Obviously,\na necessary requirement is that q(i) > 0 whenever p(i) > 0,\nwhich holds, e.g. whenever we incorporate a smoother.\n3.3MHW Sampling\nIn combining both methods we arrive at, what we believe\nis a significant improvement over each component individu-\nally. It works as follows:\n1: Initialize A \u2190 GenerateAlias(p,l)\n2: for i = 1 to\n3:Update q as needed\n4:Sample j \u223c StationaryMetropolisHastings(p,A,n)\n5: end for\nProvided that the sampler mixes within n rounds of the MH-\nprocedure, this generates draws from up-to-date versions of\np. Note that a further improvement is possible whenever we\ncan start with a more up-to-date draw from p, e.g. in the\ncase of revisiting a document in a topic model. After burn-in\nthe previous topic assignment for a given word is likely to\nbe still pertinent for the current sampling pass.\n?\n1,p(j)q(i)\np(i)q(j)\n?\nthen\nN\nndo\nLemma 2 If the Metropolis Hastings sampler over N out-\ncomes using q instead of p mixes well in n steps, the amor-\ntized cost of drawing n samples from q is O(n) per sample.\nThis follows directly from the construction of the sampler\nand the fact that we can amortize generating the alias table.\nNote that by choosing a good starting point and after burn-\nin we can effectively set n = 1.\n4.APPLICATIONS\nWe now have all components necessary for an accelerated\nsampler. The trick is to recycle old values for p(wdi|zdi) even\nwhen they change slightly and then to correct this via a\nMetropolis-Hastings scheme. Since the values change only\nslightly, we can therefore amortize the values efficiently. We\nbegin by discussing this for the case of \u2019flat\u2019 topic models\nand extend it to hierarchical models subsequently.\n4.1Sampler for LDA\nWe now design a proposal distribution for (6). It involves\ncomputing the document-specific sparse term exactly and\napproximating the remainder with slightly stale data. Fur-\nthermore, to avoid the need to store a stale alias table A,\nwe simply draw from the distribution and keep the samples.\nOnce this supply is exhausted we compute a new table."},{"page":6,"text":"Alias table: Denote by\nQw :=\n?\nt\n\u03b1tntw+ \u03b2w\nnt+\u00af\u03b2\nand qw(t) :=\n\u03b1t\nQw\nntw+ \u03b2w\nnt+\u00af\u03b2\nthe alias normalization and the associated probability dis-\ntribution. Then we perform the following steps:\n1. Generate the alias table A using qw.\n2. Draw k samples from qw and store them in Sw.\n3. Discard A and only retain Qw and the array Sw.\nGenerating Sw and computing Qw costs O(k) time. In par-\nticular, storage of Sw requires at most O(k log2k) bits, thus\nit is much more compact than A. Note, however, that we\nneed to store Qw and qw(t).\nMetropolis Hastings proposal: Denote by\nPdw:=\n?\nt\nn\u2212di\ntd\nn\u2212di\nn\u2212di\nt\ntw + \u03b2w\n+\u00af\u03b2\nand pdw(t) :=n\u2212di\ntd\nPdw\nn\u2212di\nn\u2212di\nt\ntw + \u03b2w\n+\u00af\u03b2\nthe sparse document-dependent topic contribution. Com-\nputing it costs O(kd) time. This allows us to construct a\nproposal distribution\nq(t) :=\nPdw\nPdw+ Qwpdw(t) +\nQw\nPdw+ Qwqw(t)(10)\nTo perform an MH-step we then draw from q(t) in O(kd)\namortized time. The step from topic s to topic t is accepted\nwith probability min(1,\u03c0) where\n\u03c0 =n\u2212di\nn\u2212di\ntd\nsd+ \u03b1s\n+ \u03b1t\n\u00b7n\u2212di\nn\u2212di\ntw + \u03b2w\nsw + \u03b2w\n\u00b7n\u2212di\nn\u2212di\nt\ns\n+\u00af\u03b2\n+\u00af\u03b2\u00b7Pdwpdw(s) + Qwqw(s)\nPdwpdw(t) + Qwqw(t)\nNote that the last fraction effectively removes the normal-\nization in pdw and qw respectively, that is, we take ratios of\nunnormalized probabilities.\nComplexity: To draw from q costs O(kd) time. This is so\nsince computing Pdw has this time complexity, and so does\nthe sampler for pdw. Moreover, drawing from qw(t) is O(1),\nhence it does not change the order of the algorithm. Note\nthat repeated draws from q are only O(1) since we can use\nthe very same alias sampler also for draws from pdw. Finally,\nevaluating \u03c0 costs only O(1) time. We have the following:\nLemma 3 Drawing up to k steps in a Metropolis-Hastings\nproposal from p(zdi|rest) can be accomplished in O(kd) amor-\ntized time per sample and O(k) space.\n4.2Sampler for the Poisson Dirichlet Process\nFollowing the same steps as above, the basic Poisson\nDirichlet Process topic model can be decomposed by ex-\nploiting the sparsity of ndt. The main difference to before is\nthat we need to account for the auxiliary variable r \u2208 {0,1}\nrather than just the topic indicator t. The alias table is:\n\uf8f1\n\uf8f4\nQw :=qw(t,r)\nqw(t,r) :=\n\uf8f4\n\uf8f3\nr,t\n\uf8f2\n?\n\u03b1k\nbt+mtw\nmtw\u2212stw+1\nmtw+1\nSmtw+1\nstw,at\nSmtw\nstw,at\nif r = 1\n\u03b1kbt+atst\nbt+mt\nstw+1\nmtw+1\n\u03b2+stw\n\u00af\u03b2+st\nSmtw+1\nstw+1,at\nSmtw\nstw,at\notherwise\nLikewise, the sparse document-specific contribution is\n\uf8f1\n\uf8f4\nPdw:=pdw(t,r)\npdw(t,r) :=ndt\n\uf8f4\n\uf8f3\n\uf8f2\n1\nbt+mt\nmtw\u2212stw+1\nmtw+1\nSmtw+1\nstw,at\nSmtw\nstw,at\nif r = 1\nbt+atstw\nbt+mtw\nstw+1\nmtw+1\n\u03b2+stw\n\u00af\u03b2+st\nSmtw+1\nstw+1,at\nSmtw\nstw,at\notherwise\n?\nr,t\nAs previously, computing pdw(t,r) only costs O(kd) time,\nwhich allows a proposal distribution very similar to the case\nin LDA to be constructed:\nPdw\nPdw+ Qwpdw(t,r) +\nq(t,r) :=\nQw\nPdw+ Qwqw(t,r)\nAs before, we use a Metropolis-Hastings sampler, although\nthis time for the state pair (s,t) \u2192 (s?,t?) and accept as\nbefore by using the ratio of current and stale probabilities\n(the latter given by q). As before in the context of LDA, the\ntime complexity of this sampler is amortized O(kd).\n4.3Sampler for the HDP\nDue to slight differences in the nature of the sparse term\nand the dense term, we demonstrate the efficacy of our ap-\nproach for sparse language models here. That is, we show\nthat whenever the document model is dense but the lan-\nguage model sparse, our strategy still applies. In other\nwords, this sampler works at O(kw) cost which is beneficial\nfor infrequent words.\nFor brevity, we only discuss the derivation for the two level\nHDP-LDA, given that the general multi-level HDP can be\neasily extended from the derivation. Recall (9). Now the alias\ntable is now given by:\nqw(t,u) :=p(zdi= t,udi= u|rest)?\u00af\u03b2 + mt\nQw :=qw(t,u)\n?\u03b2w\n?\nt,u\nand the exact term is given by\npdw(t,u) :=\u03b3wp(zdi= t,udi= u|rest)(\u00af \u03b3 + mt)mtw\nPdw:=pdw(t,u)\n?\nt,u\nAs before, we engineer the proposal distribution to be a com-\nbination of stale and fresh counts. It is given by\nq(t,u) :=\nPdw\nPdw+ Qwpdw(t,u) +\nQw\nPdw+ Qwqw(t,u)\nSubsequently, the state transition (t,u) \u2192 (t?,u?) is accepted\nusing straightforward Metropolis-Hastings acceptance ra-\ntios. We omitted the subscript wdi= w for brevity. The same\nargument as above shows that the time complexity of our\nsampler for drawing from HDP-LDA is amortized O(kw).\n5.EXPERIMENTS\nTo demonstrate empirically the performance of the alias\nmethod we implemented the aforementioned samplers in\nboth their base forms that have O(k) time complexity,\nas well as our alias variants which have amortized O(kd)\ntime complexity. In addition to this, we implemented the\nSparseLDA [22] algorithm with the full set of features includ-\ning the sorted list containing a compact encoding of ntw and\nndt, as well as dynamic O(1) update of bucket values. Be-\nyond the standard implementation provided in MalletLDA"},{"page":7,"text":"RedState\n(4.5s per LDA iteration)\nGPOL\n(36s per LDA iteration)\nEnron\n(85s per LDA iteration)\nFigure 1: Runtime time comparison between LDA, HDP, PDP and their Alias sampled counterparts\nAliasLDA, AliasHDP and AliasPDP.\nFigure 2: Perplexity as a function of runtime (in sec-\nonds) for PDP, AliasPDP, HDP, and AliasHDP on\nGPOL (left) and Enron (right).\nFigure 3: Runtimes of SparseLDA and AliasLDA on\nPubMedSmall (left) and NyTimes (right).\nby [22], we made two major improvements: we accelerated\nthe sorting algorithm for the compact list of encoded values\nto amortized O(1); and we avoided hash maps which sub-\nstantially improved the speed in general with small sacrifice\nof memory efficiency (we need an inverted list of the indices\nand an inverted list of the indices of the inverted lists).\nIn this section these implementations will be referred as\nLDA which is O(k), SparseLDA which is O(kw + kd),"},{"page":8,"text":"AliasLDA which is O(kd), PDP at O(k) [5], AliasPDP\nat O(kd), HDP at O(k) [6], and AliasHDP at O(kw).\n5.1 Environment and Datasets\nAll our implementations are written in C++11 in a way\nthat maximise runtime speed, compiled with gcc 4.8 with\n-O3 compiler optimisation in amd64 architecture. All our ex-\nperiments are conducted on a laptop with 12GB memory\nand an Intel i7-740QM processor with 1.73GHz clock rate,\n4\u00d7256KB L2 Cache and 6MB L3 Cache. Furthermore, we\nonly use one single sampling thread across all experiments.\nTherefore, only one CPU core is active throughout and only\n256KB L2 cache is available. We further disabled Turbo\nBoost to ensure all experiment are run at exactly 1.73GHz\nclock rate. Ubuntu 13.10 64bit served as runtime.\nWe use 5 datasets with a variety in sizes, vocabulary\nlength, and document lengths for evaluation, as shown in Ta-\nble 1 . RedState dataset contains American political blogs\ncrawled from redstate.com in year 2011. GPOL contains a\nsubset of political news articles from Reuters RCV1 collec-\ntion.2We also included the Enron Email Dataset,3. NY-\nTimes contains articles published by New York Times be-\ntween year 1987 and 2007. PubMedSmall is a subset of ap-\nproximately 1% of the biomedical literature abstracts from\nPubMed. Stopwords are removed from all datasets. Further-\nmore, words occurring less than 10 times are removed from\nNYTimes, Enron, and PubMedSmall. NYTimes, En-\nron, and PUBMED datasets are available at [1].\n5.2Evaluation Metrics and Parameters\nWe evaluate the algorithms based on two metrics: the\namount of time elapsed for one Gibbs sampling iteration\nand perplexity. The perplexity is evaluated after every 5 it-\nerations, beginning with the first evaluation at the ending\nof the first Gibbs sampling iteration. We use the standard\nheld-out method [10] to evaluate test perplexity, in which a\nsmall set of test documents originating from the same collec-\ntion is set to query the model being trained. This produces\nan estimate of the document-topic mixtures\u02dc\u03b8dtfor each test\ndocument d. From there the perplexity is then evaluated as:\n?\nd=1d=1\n?\nHere we obtain the estimate of p(wi = w|zdi= t,rest) from\nthe model being trained. To avoid effects due to variation in\nthe number of topics, we hardcoded k = 1024 for all experi-\nments except one (GPOL) where we vary k and observe the\neffect on speed per iteration. We use fixed values for hyper-\nparameters in all our models, setting \u03b1 = \u03b2 = 0.1 for LDA,\na = 0.1, b = 10, and \u03b3 = 0.1 for the PDP, and b0 = b1 = 10,\n\u03b3 = 0.1 for the HDP. For alias implementations, we fix the\nnumber of Metropolis-Hasting sampling steps at 2, as we\nobserved a satisfactory acceptance rate (over 90%) at these\nsettings. Only a negligible improvement in perplexity was\nobserved by raising this value. Furthermore, we did not ob-\nserve degraded topic quality even when Metropolis-Hasting\n\u03c0(W|rest) :=\nD\n?\nnd\nNd\n?\u22121\nD\n?\nlogp(wd|rest) where\np(wd|rest) =\ni=1\nk\n?\nt=1\np(wi = w|zdi= t,rest)p(zdi= t|rest)\n2Reuters Vol. 1, English language, 1996-08-20 to 1997-08-19\n3Source: www.cs.cmu.edu\/\u02dcenron\nsampling step was reduced to n = 1, and in all our experi-\nments the perplexity almost perfectly converges at the same\npace (i.e. along number of iterations) with the same algo-\nrithm without applying alias method (albeit with much less\ntime per iteration).\nDataset\nRedState\nGPOL\nEnron\nPubMedSmall\nNYTimes\nVLDTL\/V\n26.21\n35.9\n218\n337\n970\nL\/D\n157\n183\n165\n66\n331\n12,272\n73,444\n28,099\n106,797\n101,636\n321,699\n2,638,750\n6,125,138\n35,980,539\n98,607,383\n2,045\n14,377\n36,999\n546,665\n297,253\n231\n1,596\n2,860\n2,002\n2,497\nTable 1: Datasets and their statistics. V: vocabulary\nsize; L: total number of training tokens, D: number\nof training documents; T: number of test documents.\nL\/V is the average number occurrences of a word.\nL\/D is the average document length.\n5.3 Performance Summary\nFigure 6 shows the overall performance of perplexity as\na function of time elapsed when comparing SparseLDA vs\nAliasLDA on the four larger datasets. When k is fixed\nto 1024, substantial performance in perplexity over run-\nning time on all problems with the exception of the Enron\ndataset, most likely due to its uniquely small vocabulary\nsize. The gap in performance is increased as the datasets\nbecome larger and more realistic in size. The gain in per-\nformance is noted in particular when the average document\nlength is smaller since our sampler scales with O(kd) which\nis likely to be smaller for short documents.\nFigure 2 gives the comparison between PDP, HDP and\ntheir aliased variants on GPOL and Enron. By the time\nAliasPDP and AliasHDP are converged, the straightforward\nsampler are still at their first few iterations.\n5.4 Performance as a Function of Iterations\nIn the following we evaluate the performance in two sep-\narate parts: perplexity as a function of iterations and run-\ntime vs. iterations. We first establish that the acceleration\ncomes at no cost of degraded topic quality, as shown in\nFigure 6. The convergence speed and converged perplex-\nity of AliasLDA, AliasPDP, and AliasHDP almost perfectly\nmatch the non-alias counterparts. This further shows that\nour choice of relatively small number of Metropolis-Hasting\nsteps (2 per sample) is adequate.\nThe improved performance in running time of our alias\nimplementations can be seen in all phases of sampling when\ncompared to non-alias standard implementations (LDA,\nPDP, HDP). When compared to SparseLDA (Figure 3),\nthe performance gain is salient during all phases on larger\ndatasets (except for the early stage in Enron dataset), and\nthe performance is very close on small datasets (0.35s per it-\neration on AliasLDA vs. 0.25s per iteration on SparseLDA).\nAs the size of the data grows AliasLDA outperforms\nSparseLDA without degrading topic quality, reducing the\namount of time for each Gibbs iteration on NYTimes corpus\nby around 12% to 38% overall, on Enron corpus by around\n30% after 30 iterations, and on PubMedSmall corpus by\n27%-60% throughout the first 50 iterations. Compared to\nSparseLDA, the time required for each Gibbs iteration with\nAliasLDA grows at a much slower rate, and the benefits of\nreduced sampling complexity is particularly clear when the\naverage length of each document is small."},{"page":9,"text":"Figure 4: Comparison of SparseLDA and AliasLDA\non GPOL when varying the number of topics for\nk \u2208 {256,1024,2048,4096}.\nPercentage of full PubMedSmall collection\nSeconds per iteration\nFigure 5: Average runtime per iteration when com-\npared on {10%,20%,40%,75%,100%} of the PubMedS-\nmall dataset for SparseLDA and AliasLDA.\nThe gap in performance is especially large for more so-\nphisticated language modelsl such as PDP and HDP. The\nrunning time for each Gibbs iteration is reduced by 60% to\n80% for PDP, and 80% to 95% for HDP, an order of magni-\ntude on improvement.\n5.5Varying the number of topics\nWhen the number of topics k increases, the running time\nfor an iteration of AliasLDA increases at a much lower\nrate than SparseLDA, as seen from Figure 4 on dataset\nGPOL since kd is almost constant. Even though the gap\nbetween SparseLDA and AliasLDA may seem insignificant\nat k = 1024, it becomes very pronounced at k = 2048\n(45% improvement) and at k = 4096 (over 100%) This con-\nfirms the observation above that shorter documents benefits\nmore from AliasLDA in the sense that the average docu-\nments length L\/D relative to the number of topics k be-\ncomes\u201cshorter\u201das k increases. This yields a more sparse ndt\nand lower kd for a document d on average.\n5.6 Varying the corpus size\nFigure 5 demonstrates how the gap in running time speed\nscales with growing number of documents in the same do-\nmain. We measure the average runtime for the first 50 Gibbs\niterations on 10%, 20%, 40%, 75%, and 100% of PubMedS-\nmall dataset. The speedup ratio for each subset is at 31%,\n34%, 37%, 41%, 43% respectively. In other words, it in-\ncreases with the amount of data, which conforms our in-\ntuition that adding new documents increases the density of\nntw, thus slowing down the sparse sampler much more than\nthe alias sampler, since the latter only depends on kdrather\nthan kd+ kw.\nPerplexity vs. Runtime\nGPOL\nEnron\nPubMedSmall\nNYTimes\nPerplexity vs. Iterations\nFigure 6: Perplexity as a function of runtime\n(left) and number of iterations (right) for LDA,\nSparseLDA, and LDA, PDP and HDP, both with\nand without using the Alias method. We see consid-\nerable acceleration at unchanged perplexity.\n6.CONCLUSION\nIn this paper, we described an approach that effectively\nreduces sampling complexity of topic models from O(k) to\nO(kd) in general, and from O(kw+kd) (SparseLDA) to O(kd)\n(AliasLDA) for LDA topic model. Empirically, we showed\nthat our approach scales better than existing state-of-the-\nart method when the number of topics and the number of\ndocuments become large. This enables many large scale ap-\nplications, and many existing applications which require a"},{"page":10,"text":"scalable distributed approach. In many industrial applica-\ntions where the number of tokens easily reaches billions,\nthese properties are crucial and often desirable in design-\ning a scalable and responsive service. We also demonstrated\nan order of magnitude improvement when our approach is\napplied to complex models such as PDP and HDP. With an\norder of magnitude gain in speed, PDP and HDP may be-\ncome much more appealing to many applications for their\nsuperior convergence performance, and more sophisticated\nrepresentation of topic distributions and language models.\nFor k = 1024 topics the number of tokens processed per\nsecond in our implementation is beyond 1 million for all\ndatasets except one (NYTimes), of which contains substan-\ntially more lengthy documents. This is substantially faster\nthan many known implementations when measured in num-\nber of tokens processed per computing second per core, such\nas YahooLDA [18], and GraphLab, given that we only utilise\na single thread on a single laptop CPU core.\nAcknowledgments: This work was supported in part by\na resource grant from amazon.com, a Faculty Research Grant\nfrom Google, and Intel.\n7.REFERENCES\n[1] K. Bache and M. Lichman. UCI machine learning\nrepository, 2013.\n[2] D. Blei, T. Griffiths, and M. Jordan. The nested\nchinese restaurant process and Bayesian\nnonparametric inference of topic hierarchies. Journal\nof the ACM, 57(2):1\u201330, 2010.\n[3] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet\nallocation. JMLR, 3:993\u20131022, Jan. 2003.\n[4] W. Buntine and M. Hutter. A bayesian review of the\npoisson-dirichlet process, 2010.\n[5] C. Chen, W. Buntine, N. Ding, L. Xie, and L. Du.\nDifferential topic models. In IEEE Pattern Analysis\nand Machine Intelligence, 2014.\n[6] C. Chen, L. Du, and W. Buntine. Sampling table\nconfigurations for the hierarchical poisson-dirichlet\nprocess. In ECML, pages 296\u2013311, 2011.\n[7] J. Geweke and H. Tanizaki. Bayesian estimation of\nstate-space model using the metropolis-hastings\nalgorithm within gibbs sampling. Computational\nStatistics and Data Analysis, 37(2):151\u2013170, 2001.\n[8] W. R. Gilks, S. Richardson, and D. J. Spiegelhalter.\nMarkov Chain Monte Carlo in Practice. 1995.\n[9] T. Griffiths and M. Steyvers. Finding scientific topics.\nPNAS, 101:5228\u20135235, 2004.\n[10] G. Heinrich. Parameter estimation for text analysis.\nTechnical report, Fraunhofer IGD, 2004.\n[11] M. Hoffman, D. M. Blei, C. Wang, and J. Paisley.\nStochastic variational inference. In ICML, 2012.\n[12] W. Li, D. Blei, and A. McCallum. Nonparametric\nbayes pachinko allocation. In UAI, 2007.\n[13] G. Marsaglia, W. W. Tsang, and J. Wang. Fast\ngeneration of discrete random variables. Journal of\nStatistical Software, 11(3):1\u20138, 2004.\n[14] R. M. Neal. Markov chain sampling methods for\nDirichlet process mixture models. University of\nToronto, Technical Report 9815, 1998.\n[15] J. Petterson, A. Smola, T. Caetano, W. Buntine, and\nS. Narayanamurthy. Word features for latent dirichlet\nallocation. In NIPS, pages 1921\u20131929, 2010.\n[16] J. Pitman and M. Yor. The two-parameter\npoisson-dirichlet distribution derived from a stable\nsubordinator. A. of Probability, 25(2):855\u2013900, 1997.\n[17] I. Sato and H. Nakagawa. Topic models with\npower-law using Pitman-Yor process. In KDD, pages\n673\u2013682. ACM, 2010.\n[18] A. J. Smola and S. Narayanamurthy. An architecture\nfor parallel topic models. In PVLDB, 2010.\n[19] Y. Teh, M. Jordan, M. Beal, and D. Blei. Hierarchical\ndirichlet processes. JASA, 101(576):1566\u20131581, 2006.\n[20] A. J. Walker. An efficient method for generating\ndiscrete random variables with general distributions.\nACM TOMS, 3(3):253\u2013256, 1977.\n[21] C. Wang, J. Paisley, and D. M. Blei. Online\nvariational inference for the hierarchical Dirichlet\nprocess. In Conference on Artificial Intelligence and\nStatistics, 2011.\n[22] L. Yao, D. Mimno, and A. McCallum. Efficient\nmethods for topic model inference on streaming\ndocument collections. In KDD\u201909, 2009."}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Sujith_Ravi\/publication\/266660385_Reducing_the_sampling_complexity_of_topic_models\/links\/5583a5c508aefa35fe30c780.pdf","widgetId":"rgw29_56ab9eb3b965f"},"id":"rgw29_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=266660385&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw30_56ab9eb3b965f"},"id":"rgw30_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=266660385&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":266660385,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"5583a5c508aefa35fe30c780","name":"Sujith Ravi","date":"Jun 19, 2015 ","nameLink":"profile\/Sujith_Ravi","filename":"fastlda-kdd2014.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Sujith_Ravi\/publication\/266660385_Reducing_the_sampling_complexity_of_topic_models\/links\/5583a5c508aefa35fe30c780.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Sujith_Ravi\/publication\/266660385_Reducing_the_sampling_complexity_of_topic_models\/links\/5583a5c508aefa35fe30c780.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"f3f0b655a2fecfab1e87baf21378807b","showFileSizeNote":false,"fileSize":"1.26 MB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"5583a5c508aefa35fe30c780","name":"Sujith Ravi","date":"Jun 19, 2015 ","nameLink":"profile\/Sujith_Ravi","filename":"fastlda-kdd2014.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Sujith_Ravi\/publication\/266660385_Reducing_the_sampling_complexity_of_topic_models\/links\/5583a5c508aefa35fe30c780.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Sujith_Ravi\/publication\/266660385_Reducing_the_sampling_complexity_of_topic_models\/links\/5583a5c508aefa35fe30c780.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"f3f0b655a2fecfab1e87baf21378807b","showFileSizeNote":false,"fileSize":"1.26 MB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=S9gO6lCj61kOMuKhft-S7h8pd4-EkhO8VlsC3vMGB5ZQhupkuKVCTxV7RSdtucSX6Ya-eJvVBiMeA_Kil_tQSg.jUeYnXmshnwybjgClF7SbhJAsYb0dlV5PqRtQGxnOcIAxw3O3z57DAhys28mYpLPUUnyx5cn12QGjSyWemyWrg","clickOnPill":"publication.PublicationFigures.html?_sg=iGn99pW4dWyzAw-OEqXY0I4P1F6C_Xoc_hMIKKRcnv8BpY6u6nGfCfnBuMUIkGUecQ6FFp70kr7tzXmz2i5hTA.zOsW-3kYRDDZ2S4lE_hgX9Ui7L700X843onjWXHJS_y58SpIdln-T3r-9ym37pQold9DSTHPvlbHZOTnD5HFnQ"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FSujith_Ravi%2Fpublication%2F266660385_Reducing_the_sampling_complexity_of_topic_models%2Flinks%2F5583a5c508aefa35fe30c780.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=_WKOjxe0gCtk2z8LzQwnLJcWhcbL9GNq_Th3xVdKDdUU-eO1u9Iu08wuvVT_x6iSdKcePT2WfSfP-MKtdN0vCQ","urlHash":"74b919fd11295c3c82d5312e89a398fc","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=CrQ53SfwRIl3lgL5ZJPAYaN2FHDjE_umFM3IepeE-M6atNnz3pblXLLH0LVQm0_WxAwG6BsDkGXTefs5T6yarBsPpvZSTV_ozyRpImUwj6M.eMSnFsVEhzYt0bQBbNfBYd6pfZXIgnRPvb_V3ygShw27s88QH9jF9iVq-2nW0VmV-TyBu3WNiM0WVTGMLOEGgQ.aKHAB5cMzLnj23RaY9xYaYOc4qXQYQGl23nWvIimpLLZHxRx3D-khxb17fLPs4j7uhXMeJCXPn2wvdjZDuneIA","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"5583a5c508aefa35fe30c780","trackedDownloads":{"5583a5c508aefa35fe30c780":{"v":false,"d":false}},"assetId":"AS:241925798232064@1434691013195","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":266660385,"commentCursorPromo":null,"widgetId":"rgw32_56ab9eb3b965f"},"id":"rgw32_56ab9eb3b965f","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FSujith_Ravi%2Fpublication%2F266660385_Reducing_the_sampling_complexity_of_topic_models%2Flinks%2F5583a5c508aefa35fe30c780.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A241925798232064%401434691013195&publicationUid=266660385&linkId=5583a5c508aefa35fe30c780&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Reducing the sampling complexity of topic models","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=v01HiaPL7hLDzVLeIRsPqweVfsaB4odMMwk2IQC8TkVPMsKhh8pHFKkFCy13i4mtkkII9ZEdaERYc5DHH2-bDSjoVhI_tH8YUjM8JRpdJ3o.00SBKAVxXCXvaLVBM9kYKDTQ1fYFWaEF7Xg1awEga-TaGq2kvI-jRwO5u6-vECtdpUP7RihnWcD-nP-Mg_diqQ.cv23ST1qAbPFNcOycPVgkV8hywH_uIaklk24sYUOwu1UH3KJ8nUhN-JkXsoyz-tmKM7b2H3V0pIgoq_7SOGWwA","publicationUid":266660385,"trackedDownloads":{"5583a5c508aefa35fe30c780":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw34_56ab9eb3b965f"},"id":"rgw34_56ab9eb3b965f","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw35_56ab9eb3b965f"},"id":"rgw35_56ab9eb3b965f","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw36_56ab9eb3b965f"},"id":"rgw36_56ab9eb3b965f","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw37_56ab9eb3b965f"},"id":"rgw37_56ab9eb3b965f","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw38_56ab9eb3b965f"},"id":"rgw38_56ab9eb3b965f","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw33_56ab9eb3b965f"},"id":"rgw33_56ab9eb3b965f","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw31_56ab9eb3b965f"},"id":"rgw31_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/266660385_Reducing_the_sampling_complexity_of_topic_models","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab9eb3b965f"},"id":"rgw2_56ab9eb3b965f","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":266660385},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=266660385&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab9eb3b965f"},"id":"rgw1_56ab9eb3b965f","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"2oUGVx1klRbMiUgTpGydSQm\/8J5GLPYvZbKnL4TUeoMET+NHOQpu1Hg5zriM4eJ03LUGLirKQqRyY9Ofl4\/u4yu6bEsgXHtLy74zNaJtR1bM3useXStMOVlYXsrly3M581JtdTFFnWryhKO5wXGW1Ulbnruv5n4iR6YtSRBnHXEDOjz+J3HF5R5ufzMK3a59DxVRCp1\/nvew4DSsVA3IFswwd017aaLQvmGls8+OsUnho9Eab+EHAWL3drdduuFwNc\/1rLKE2VQftF5liPKW4WFkTalqs7Ej+IU1cwdUzc8=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/266660385_Reducing_the_sampling_complexity_of_topic_models\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Reducing the sampling complexity of topic models\" \/>\n<meta property=\"og:description\" content=\"Inference in topic models typically involves a sampling step to associate latent variables with observations. Unfortunately the generative model loses sparsity as the amount of data increases,...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/266660385_Reducing_the_sampling_complexity_of_topic_models\/links\/5583a5c508aefa35fe30c780\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/266660385_Reducing_the_sampling_complexity_of_topic_models\" \/>\n<meta property=\"rg:id\" content=\"PB:266660385\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1145\/2623330.2623756\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Reducing the sampling complexity of topic models\" \/>\n<meta name=\"citation_author\" content=\"Aaron Q. Li\" \/>\n<meta name=\"citation_author\" content=\"Amr Ahmed\" \/>\n<meta name=\"citation_author\" content=\"Sujith Ravi\" \/>\n<meta name=\"citation_author\" content=\"Alexander J. Smola\" \/>\n<meta name=\"citation_publication_date\" content=\"2014\/08\/24\" \/>\n<meta name=\"citation_isbn\" content=\"978-1-4503-2956-9\" \/>\n<meta name=\"citation_doi\" content=\"10.1145\/2623330.2623756\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Sujith_Ravi\/publication\/266660385_Reducing_the_sampling_complexity_of_topic_models\/links\/5583a5c508aefa35fe30c780.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/266660385_Reducing_the_sampling_complexity_of_topic_models\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/266660385_Reducing_the_sampling_complexity_of_topic_models\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-ae852297-22e3-4faf-ac98-afb9080b2f2f","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":893,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw39_56ab9eb3b965f"},"id":"rgw39_56ab9eb3b965f","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-ae852297-22e3-4faf-ac98-afb9080b2f2f", "1dd59e0c610b222ed87f7004e5a723bb5226295b");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-ae852297-22e3-4faf-ac98-afb9080b2f2f", "1dd59e0c610b222ed87f7004e5a723bb5226295b");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw40_56ab9eb3b965f"},"id":"rgw40_56ab9eb3b965f","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/266660385_Reducing_the_sampling_complexity_of_topic_models","requestToken":"gKZ3JCWbnh5EgLmtaeskIdh+szs1qnpDCzMXEwSwYFsLulDb1OHBE8WHZQGmAgFaon7aE3drEGNr+9\/YnrRxzwMlMr8Bp1+U17Ir4vjiLqIosn2VyZ5dpIu6QnEtEZCgIoGG0\/n0D4XGNr4I9PyrzhtP+Cuw7Wgo524\/8kV9REBX1Wr7JIBf7wT7O+SN4+9FyWHHPJK5n0S1O\/zrzLCrfcDrc9Ys1QseK2vNHVD5RUbAyyUTg0IVLQbrxQqeqboUqGS8EARft+vI+njZ3ghc7u93Z8VUwCqYdv3Vq8ELdVA=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=dkhoVW_poTSz9rhw6oRY11HzetRYI90YkwUkvOVrNRuuentvWD4-7mvLBgOoU7T8","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjY2NjYwMzg1X1JlZHVjaW5nX3RoZV9zYW1wbGluZ19jb21wbGV4aXR5X29mX3RvcGljX21vZGVscw%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw42_56ab9eb3b965f"},"id":"rgw42_56ab9eb3b965f","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw41_56ab9eb3b965f"},"id":"rgw41_56ab9eb3b965f","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw43_56ab9eb3b965f"},"id":"rgw43_56ab9eb3b965f","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
